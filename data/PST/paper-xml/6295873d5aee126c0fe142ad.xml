<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diffusion-LM Improves Controllable Text Generation</title>
				<funder ref="#_uH2vXCN #_u9carUa">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-27">27 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<persName><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Thickstun</surname></persName>
							<email>jthickst@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
							<email>pliang@cs.stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Stanford Univeristy</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Stanford Univeristy</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Diffusion-LM Improves Controllable Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-27">27 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.14217v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large autoregressive language models (LMs) are capable of generating high quality text <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b44">45]</ref>, but in order to reliably deploy these LMs in real world applications, the text generation process needs to be controllable: we need to generate text that satisfies desired requirements (e.g. topic, syntactic structure). A natural approach for controlling a LM would be to fine-tune the LM using supervised data of the form (control, text) <ref type="bibr" target="#b15">[16]</ref>. However, updating the LM parameters for each control task can be expensive and does not allow for compositions of multiple controls (e.g. generate text that is both positive sentiment and non-toxic). This motivates light-weight and modular plug-and-play approaches <ref type="bibr" target="#b5">[6]</ref> that keep the LM frozen and steer the generation process using an external classifier that measures how well the generated text satisfies the control. But steering a frozen autoregressive LM has been shown to be difficult, and existing successes have been limited to simple, attribute-level controls (e.g., sentiment or topic) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>In order to tackle more complex controls, we propose Diffusion-LM, a new language model based on continuous diffusions. Diffusion-LM starts with a sequence of Gaussian noise vectors and incrementally denoises them into vectors corresponding to words, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. These gradual denoising steps produce a hierarchy of continuous latent representations. We find that this hierarchical and continuous latent variable enables simple, gradient-based methods to perform complex control tasks such as constraining the parse tree of a generated sequence. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4</ref>], but they have not been applied to text because of the inherently discrete nature of text ( ?3). Adapting this class of models to text requires several modifications to standard diffusions: we add an embedding step and a rounding step to the standard diffusion process, design a training objective to learn the embedding, and propose techniques to improve rounding ( ?4). We control Diffusion-LM using a gradient-based method, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. This method enables us to steer the text generation process towards outputs that satisfy target structural and semantic controls. It iteratively performs gradient updates on the continuous latent variables of Diffusion-LM to balance fluency and control satisfaction ( ?5.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous diffusion models have been extremely successful in vision and audio domains</head><p>To demonstrate control of Diffusion-LM, we consider six control targets ranging from fine-grained attributes (e.g., semantic content) to complex structures (e.g., parse trees). Our method almost doubles the success rate of previous plug-and-play methods and matches or outperforms the fine-tuning oracle on all these classifier-guided control tasks ( ?7.1). In addition to these individual control tasks, we show that we can successfully compose multiple classifier-guided controls to generate sentences with both desired semantic content and syntactic structure ( ?7.2). Finally, we consider span-anchored controls, such as length control and infilling. Diffusion-LM allows us to perform these control tasks without a classifier, and our Diffusion-LM significantly outperforms prior plug-and-play methods and is on-par with an autoregressive LM trained from scratch for the infilling task ( ?7.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Diffusion Models for Text. Diffusion models <ref type="bibr" target="#b38">[39]</ref> have demonstrated great success in continuous data domains <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>, producing images and audio that have state-of-the-art sample quality. To handle discrete data, past works have studied text diffusion models on discrete state spaces, which defines a corruption process on discrete data (e.g., each token has some probability to be corrupted to an absorbing or random token) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. In this paper, we focus on continuous diffusion models for text and to the best of our knowledge, our work is the first to explore this setting. In contrast to discrete diffusion LMs, our continuous diffusion LMs induce continuous latent representations, which enables efficient gradient-based methods for controllable generation.</p><p>Autoregressive and Non-autoregressive LMs. Most large pre-trained LMs are left-to-right autoregressive (e.g., GPT-3 <ref type="bibr" target="#b2">[3]</ref>, PaLM <ref type="bibr" target="#b4">[5]</ref>). The fixed generation order limits the models' flexibility in many controllable generation settings, especially those that impose controls globally on both left and right contexts. One example is infilling, which imposes lexical control on the right contexts; another example is syntactic structure control, which controls global properties involving both left and right contexts. Since autoregressive LMs cannot directly condition on right contexts, prior works have developed specialized training and decoding techniques for these tasks <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30]</ref>. For example, Qin et al. <ref type="bibr" target="#b30">[31]</ref> proposed a decoding method that relaxes the discrete LM outputs to continuous variables and backpropagates gradient information from the right context. Diffusion-LM can condition on arbitrary classifiers that look at complex, global properties of the sentence. There are other non-autoregressive LMs that have been developed for machine translation and speech-to-text tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">37]</ref>. However these methods are specialized for speech and translation settings, where the entropy over valid outputs is low, and it has been shown that these approaches fail for language modeling <ref type="bibr" target="#b34">[35]</ref>.</p><p>Plug-and-Play Controllable Generation. Plug-and-play controllable generation aims to keep the LM frozen and steer its output using potential functions (e.g., classifiers). Given a probabilistic potential function that measures how well the generated text satisfies the desired control, the generated text should be optimized for both control satisfaction (measured by the potential function) and fluency (measured by LM probabilities) . There are several plug-and-play approaches based on autoregressive LMs: FUDGE <ref type="bibr" target="#b43">[44]</ref> reweights the LM prediction at each token with an estimate of control satisfaction for the partial sequence; GeDi <ref type="bibr" target="#b21">[22]</ref> and DExperts <ref type="bibr" target="#b23">[24]</ref> reweight the LM prediction at each token with a smaller LM finetuned/trained for the control task.</p><p>The closest work to ours is PPLM <ref type="bibr" target="#b5">[6]</ref>, which runs gradient ascent on an autoregressive LM's hidden activations to steer the next token to satisfy the control and maintain fluency. Because PPLM is based on autoregressive LMs, it can only generate left-to-right. This prevents PPLM from repairing and recovering errors made in previous generation steps. Despite their success on attribute (e.g., topic) controls, we will show these plug-and-play methods for autoregressive LMs fail on more complex control tasks such as controlling syntactic structure and semantic content in ?7.1. We demonstrate that Diffusion-LM is capable of plug-and-play controllable generation by applying classifier-guided gradient updates to the continuous sequence of latent variables induced by the Diffusion-LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Statement and Background</head><p>We first define controllable generation ( ?3.1) and then review continuous diffusion models ( ?3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generative Models and Controllable Generation for Text</head><p>Text generation is the task of sampling w from a trained language model p lm (w), where w = [w 1 ? ? ? w n ] is a sequence of discrete words and p lm (w) is a probability distribution over sequences of words. Controllable text generation is the task of sampling w from a conditional distribution p(w | c), where c denotes a control variable. For syntactic control, c can be a target syntax tree (Figure <ref type="figure" target="#fig_0">1</ref>), while for sentiment control, c could be a desired sentiment label. The goal of controllable generation is to generate w that satisfies the control target c.</p><p>Consider the plug-and-play controllable generation setting: we are given a language model p lm (w) trained from a large amount of unlabeled text data, and for each control task, we are given a classifier p(c | w) trained from smaller amount of labeled text data (e.g., for syntactic control, the classifier is a probabilistic parser). The goal is to utilize these two models to approximately sample from the posterior p(w | c) via Bayes rule p(w | c) ? p lm (w) ? p(c | w). Here, p lm (w) encourages w to be fluent, and the p(c | w) encourages w to fulfill the control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Autoregressive Language Models</head><p>The canonical approach to language modeling factors p lm in an autoregressive left-to-right mannar, p lm (w) = p lm (w 1 ) n i=2 p lm (x i | x &lt;i ). In this case, text generation is reduced to the task of repeatedly predicting the next token conditioned on the partial sequence generated so far. The next token prediction p lm (x i | x &lt;i ) is often parametrized by Transformer architecture <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Diffusion Models for Continuous Domains</head><p>A diffusion model <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref> is a latent variable model that models the data x 0 ? R d as a Markov chain x T . . . x 0 with each variable in R d , and x T is a Gaussian. The diffusion model incrementally denoises the sequence of latent variables x T :1 to approximate samples from the target data distribution (Figure <ref type="figure">2</ref>). The initial state p ? (x T ) ? N (0, I), and each denoising transition x t ? x t-1 is parametrized by the model p ? (x t-1 | x t ) = N (x t-1 ; ? ? (x t , t), ? ? (x t , t)). For example, ? ? and ? ? may be computed by a U-Net or a Tranformer.</p><p>To train the diffusion model, we define a forward process that constructs the intermediate latent variables x 1:T . The forward process incrementally adds Gaussian noise to data x 0 until, at diffusion step T , samples x T are approximately Gaussian. Each transition</p><formula xml:id="formula_0">x t-1 ? x t is parametrized by q(x t | x t-1 ) = N (x t ; ? 1 -? t x t-1 , ? t I)</formula><p>, where the hyperparameter ? t is the amount of noise added at diffusion step t. This parametrization of the forward process q contains no trainable parameters and Figure <ref type="figure">2</ref>: A graphical model representing the forward and reverse diffusion processes. In addition to the original diffusion models <ref type="bibr" target="#b11">[12]</ref>, we add a Markov transition between x 0 and w, and propose the embedding ?4.1 and rounding ?4.2 techniques. allows us to define a training objective that involves generating noisy data according to a pre-defined forward process q and training a model to reverse the process and reconstruct the data.</p><p>The diffusion model is trained to maximize the marginal likelihood of the data E x0?pdata [log p ? (x 0 )], and the canonical objective is the variational lower bound of log p ? (x 0 ) <ref type="bibr" target="#b38">[39]</ref>,</p><formula xml:id="formula_1">L vlb (x 0 ) = E q(x 1:T |x0) log q(x T |x 0 ) p ? (x T ) + T t=2 log q(x t-1 |x 0 , x t ) p ? (x t-1 |x t ) -log p ? (x 0 |x 1 ) .<label>(1)</label></formula><p>However, this objective can be unstable and require many optimization tricks to stabilize <ref type="bibr" target="#b26">[27]</ref>. To circumvent this issue, Ho et al. <ref type="bibr" target="#b11">[12]</ref> devised a simple surrogate objective that expands and reweights each KL-divergence term in L vlb to obtain a mean-squared error loss (derivation in Appendix E) which we will refer to as</p><formula xml:id="formula_2">L simple (x 0 ) = T t=1 E q(xt|x0) ||? ? (x t , t) -?(x t , x 0 )|| 2 ,</formula><p>where ?(x t , x 0 ) is the mean of the posterior q(x t-1 |x 0 , x t ) which is a closed from Gaussian, and ? ? (x t , t) is the predicted mean of p ? (x t-1 | x t ) computed by a neural network. While L simple is no longer a valid lower bound, prior work has found that it empirically made training more stable and improved sample quality<ref type="foot" target="#foot_1">2</ref> . We will make use of similar simplifications in Diffusion-LM to stabilize training and improve sample quality ( ?4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Diffusion-LM: Continuous Diffusion Language Modeling</head><p>Constructing Diffusion-LM requires several modifications to the standard diffusion model. First, we must define an embedding function that maps discrete text into a continuous space. To address this, we propose an end-to-end training objective for learning embeddings ( ?4.1). Second, we require a rounding method to map vectors in embedding space back to words. To address this, we propose training and decoding time methods to facilitate rounding ( ?4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">End-to-end Training</head><p>To apply a continuous diffusion model to discrete text, we define an embedding function EMB(w i ) that maps each word to a vector in R d . We define the embedding of a sequence w of length n to be:</p><formula xml:id="formula_3">EMB(w) = [EMB(w 1 ), . . . , EMB(w n )] ? R nd .</formula><p>We propose a modification of the diffusion model training objective (Equation <ref type="formula" target="#formula_1">1</ref>) that jointly learns the diffusion model's parameters and word embeddings. In preliminary experiments, we explored random Gaussian embeddings, as well as pre-trained word embeddings <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33]</ref>. We found that these fixed embeddings are suboptimal for Diffusion-LM compared to end-to-end training <ref type="foot" target="#foot_2">3</ref> .</p><p>As shown in Figure <ref type="figure">2</ref>, our approach adds a Markov transition from discrete words w to x 0 in the forward process, parametrized by q ? (x 0 |w) = N (EMB(w), ? 0 I). In the reverse process, we add a trainable rounding step, parametrized by p ? (w</p><formula xml:id="formula_4">| x 0 ) = n i=1 p ? (w i | x i ), where p ? (w i | x i ) is a softmax distribution.</formula><p>The training objectives introduced in ?3 now becomes We derive L e2e simple (w) from L e2e vlb (w) following the simplification in ?3.3 and our derivation details are in Appendix E. Since we are training the embedding function, q ? now contains trainable parameters and we use the reparametrization trick <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b17">18]</ref> to backpropagate through this sampling step. Empirically, we find the learned embeddings cluster meaningfully: words with the same part-of-speech tags (syntactic role) tend to be clustered, as shown in Figure <ref type="figure">3</ref>.</p><formula xml:id="formula_5">L e2e vlb (w) = E q ? (x0|w) [L vlb (x 0 ) + log q ? (x 0 |w) -log p ? (w|x 0 )]] , L e2e simple (w) = E q ? (x 0:T |w) L simple (x 0 ) + ||EMB(w) -? ? (x 1 , 1)|| 2 -log p ? (w|x 0 ) .<label>(2</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reducing Rounding Errors</head><p>The learned embeddings define a mapping from discrete text to the continuous x 0 . We now describe the inverse process of rounding a predicted x 0 back to discrete text. Rounding is achieved by choosing the most probable word for each position, according to argmax p ? (w</p><formula xml:id="formula_6">| x 0 ) = n i=1 p ? (w i | x i ).</formula><p>Ideally, this argmax-rounding would be sufficient to map back to discrete text, as the denoising steps should ensure that x 0 lies exactly on the embedding of some word. However, empirically, the model fails to generate x 0 that commits to a single word.</p><p>One explanation for this phenomenon is that the L simple (x 0 ) term in our objective 2 puts insufficient emphasis on modeling the structure of x 0 . Recall that we defined L simple (x 0 ) = T t=1 E xt ||? ? (x t , t) -?(x t , x 0 )|| 2 , where our model ? ? (x t , t) directly predicts the mean of p ? (x t-1 | x t ) for each denoising step t. In this objective, the constraint that x 0 has to commit to a single word embedding will only appear in the terms with t near 0, and we found that this parametrization required careful tuning to force the objective to emphasize those terms (see Appendix H).</p><p>Our approach re-parametrizes L simple to force Diffusion-LM to explicitly model x 0 in every term of the objective. Specifically, we derive an analogue to L simple which is parametrized via x 0 ,</p><formula xml:id="formula_7">L e2e x0-simple (x 0 ) = T t=1 E xt ||f ? (x t , t) -x 0 || 2</formula><p>, where our model f ? (x t , t) predicts x 0 directly<ref type="foot" target="#foot_3">4</ref> . This forces the neural network to predict x 0 in every term and we found that models trained with this objective quickly learn that x 0 should precisely centered at a word embedding.</p><p>We described how re-parametrization can be helpful for model training, but we also found that the same intuition could be used at decoding time in a technique that we call the clamping trick. In the standard generation approach for a x 0 -parametrized model, the model denoises x t to x t-1 by first computing an estimate of x 0 via f ? (x t , t) and then sampling x t-1 conditioned on this estimate:</p><formula xml:id="formula_8">x t-1 = ? ?f ? (x t , t)+ ? 1 -? , where ?t = t s=0</formula><p>(1-? s ) and ? N (0, I) <ref type="foot" target="#foot_4">5</ref> . In the clamping trick, the model additionally maps the predicted vector f ? (x t , t) to its nearest word embedding sequence. Now, the sampling step becomes</p><formula xml:id="formula_9">x t-1 = ? ? ? Clamp(f ? (x t , t)) + ? 1 -? .</formula><p>The clamping trick forces the predicted vector to commit to a word for intermediate diffusion steps, making the vector predictions more precise and reducing rounding errors. <ref type="foot" target="#foot_5">6</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Decoding and Controllable Generation with Diffusion-LM</head><p>Having described the Diffusion-LM, we now consider the problem of controllable text generation ( ?5.1) and decoding ( ?5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Controllable Text Generation</head><p>We now describe a procedure that enables plug-and-play control on Diffusion-LM. Our approach to control is inspired by the Bayesian formulation in ?3.1, but instead of performing control directly on the discrete text, we perform control on the sequence of continuous latent variables x 0:T defined by Diffusion-LM, and apply the rounding step to convert these latents into text. Controlling x 0:T is equivalent to decoding from the posterior p(x 0:T |c) = T t=1 p(x t-1 | x t , c), and we decompose this joint inference problem to a sequence of control problems at each diffusion step:</p><formula xml:id="formula_10">p(x t-1 | x t , c) ? p(x t-1 | x t ) ? p(c | x t-1 , x t ). We further simplify p(c | x t-1 , x t ) = p(c | x t-1</formula><p>) via conditional independence assumptions from prior work on controlling diffusions <ref type="bibr" target="#b39">[40]</ref>. Consequently, for the t-th step, we run gradient update on x t-1 :</p><formula xml:id="formula_11">? xt-1 log p(x t-1 | x t , c) = ? xt-1 log p(x t-1 | x t ) + ? xt-1 log p(c | x t-1 ),</formula><p>where both log p(x t-1 | x t ) and log p(c | x t-1 ) are differentiable: the first term is parametrized by Diffusion-LM, and the second term is parametrized by a neural network classifier.</p><p>Similar to work in the image setting <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b39">40]</ref>, we train the classifier on the diffusion latent variables and run gradient updates on the latent space x t-1 to steer it towards fulfilling the control. These image diffusion works take one gradient step towards ? xt-1 log p(c | x t-1 ) per diffusion steps. To improve performance on text and speed up decoding, we introduce two key modifications: fluency regularization and multiple gradient steps.</p><p>To generate fluent text, we run gradient updates on a control objective with fluency regularization:</p><formula xml:id="formula_12">? log p(x t-1 | x t ) + log p(c | x t-1 )</formula><p>, where ? is a hyperparameter that trades off fluency (the first term) and control (the second term). While existing controllable generation methods for diffusions do not include the ? log p(x t-1 | x t ) term in the objective, we found this term to be instrumental for generating fluent text. The resulting controllable generation process can be viewed as a stochastic decoding method that balances maximizing and sampling p(x t-1 | x t , c), much like popular text generation techniques such as nucleus sampling <ref type="bibr" target="#b12">[13]</ref> or sampling with low temperature. In order to improve the control quality, we take multiple gradient steps for each diffusion step: we run 3 steps of the Adagrad<ref type="foot" target="#foot_6">7</ref>  <ref type="bibr" target="#b9">[10]</ref> update for each diffusion steps. To mitigate for the increased computation cost, we downsample the diffusion steps from 2000 to 200, which speeds up our controllable generation algorithm without hurting sample quality much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Minimum Bayes Risk Decoding</head><p>Many conditional text generation tasks require a single high-quality output sequence, such as machine translation or sentence infilling. In these settings, we apply Minimum Bayes Risk (MBR) decoding <ref type="bibr" target="#b22">[23]</ref> to aggregate a set of samples S drawn from the Diffusion-LM , and select the sample that achieves the minimum expected risk under a loss function L (e.g., negative BLEU score): ? = argmin w?S w ?S 1 |S| L(w, w ). We found that MBR decoding often returned high quality outputs, since a low quality sample would be dissimilar from the remaining samples and penalized by the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Setup</head><p>With the above improvements on training ( ?4) and decoding ( ?5), we train Diffusion-LM for two language modeling tasks. We then apply the controllable generation method to 5 classifier-guided control tasks, and apply MBR decoding to a classifier-free control task (i.e. infilling). My dog had stolen every one and put it under there. output text One day, I found all of my lost tennis balls underneath the bed.</p><p>Table <ref type="table">1</ref>: Example input control and output text for each control tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets and Hyperparameters</head><p>We train Diffusion-LM on two datasets: E2E <ref type="bibr" target="#b27">[28]</ref> and ROCStories <ref type="bibr" target="#b25">[26]</ref>. The E2E dataset consists of 50K restaurant reviews labeled by 8 fields including food type, price, and customer rating. The ROCStories dataset consists of 98K five-sentence stories, capturing a rich set of causal and temporal commonsense relations between daily events. This dataset is more challenging to model than E2E, because the stories contain a larger vocabulary of 11K words and more diverse semantic content.</p><p>Our Diffusion-LM is based on Transformer <ref type="bibr" target="#b41">[42]</ref> architecture with 80M parameters, with a sequence length n = 64, diffusion steps T = 2000 and a square-root noise schedule (see Appendix A for details). We treat the embedding dimension as a hyperparameter, setting d = 16 for E2E and d = 128 for ROCStories. See Appendix B for hyperparameter details. At decoding time, we downsample to 200 diffusion steps for E2E and maintain 2000 steps for ROCStories. Decoding Diffusion-LM for 200 steps is still 7x slower than decoding autoregressive LMs. For controllable generation, our method based on Diffusion-LM is 1.5x slower than FUDGE but 60x faster than PPLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Control tasks</head><p>We consider 6 control tasks shown in Table <ref type="table">1</ref>: the first 4 tasks rely on a classifier, and the last 2 tasks are classifier free <ref type="foot" target="#foot_7">8</ref> . For each control task (e.g. semantic content), we sample 200 control targets c (e.g., rating=5 star) from the validation splits, and we generate 50 samples for each control target. To evaluate the fluency of the generated text, we follow the prior works <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b5">6]</ref> and feed the generated text to a teacher LM (i.e., a carefully fine-tuned GPT-2 model) and report the perplexity of generated text under the teacher LM. We call this metric lm-score (denoted as lm): a lower lm-score indicates better sample quality. <ref type="foot" target="#foot_8">9</ref> We define success metrics for each control task as follows:</p><p>Semantic Content. Given a field (e.g., rating) and value (e.g., 5 star), generate a sentence that covers field=value, and report the success rate by exact match of 'value'.</p><p>Parts-of-speech. Given a sequence of parts-of-speech (POS) tags (e.g., Pronoun Verb Determiner Noun), generate a sequence of words of the same length whose POS tags (under an oracle POS tagger) match the target (e.g., I ate an apple). We quantify success via word-level exact match.</p><p>Syntax Tree. Given a target syntactic parse tree (see Figure <ref type="figure" target="#fig_0">1</ref>), generate text whose syntactic parse matches the given parse. To evaluate the success, we parse the generated text by an off-the-shelf parser <ref type="bibr" target="#b19">[20]</ref>, and report F1 scores.</p><p>Syntax Spans. Given a target (span, syntactic category) pair, generate text whose parse tree over span [i, j] matches the target syntactic category (e.g. prepositional phrase).We quantify success via the fraction of spans that match exactly.</p><p>Length. Given a target length 10, . . . , 40, generate a sequence with a length within ?2 of the target.</p><p>In the case of Diffusion-LM, we treat this as a classifier-free control task.</p><p>Infilling. Given a left context (O 1 ) and a right context (O 2 ) from the aNLG dataset <ref type="bibr" target="#b1">[2]</ref>, and the goal is to generate a sentence that logically connects O 1 and O 2 . For evaluation, we report both automatic and human evaluation from the Genie leaderboard <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Classifier-Guided Control Baselines</head><p>For the first 5 control tasks, we compare our method with PPLM, FUDGE, and a fine-tuning oracle. Both PPLM and FUDGE are plug-and-play controllable generation approaches based on an autoregressive LM, which we train from scratch using the GPT-2 small architecture <ref type="bibr" target="#b32">[33]</ref>.</p><p>PPLM <ref type="bibr" target="#b5">[6]</ref>. This method runs gradient ascent on the LM activations to increase the classifier probabilities and language model probabilities, and has been successful on simple attribute control. We apply PPLM to control semantic content, but not the remaining 4 tasks which require positional information, as PPLM's classifier lacks positional information.</p><p>FUDGE <ref type="bibr" target="#b43">[44]</ref>. For each control task, FUDGE requires a future discriminator that takes in a prefix sequence and predicts whether the complete sequence would satisfy the constraint. At decoding time, FUDGE reweights the LM prediction by the discriminator scores.</p><p>FT. For each control task, we fine-tune GPT-2 on (control, text) pair, yielding an oracle conditional language model that's not plug-and-play. We report both the sampling (with temperature 1.0) and beam search (with beam size 4) outputs of the fine-tuned models, denoted as FT-sample and FT-search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Infilling Baselines</head><p>We compare to 3 specialized baseline methods developed in past work for the infilling task.</p><p>DELOREAN <ref type="bibr" target="#b29">[30]</ref>. This method continuously relaxes the output space of a left-to-right autoregressive LM, and iteratively performs gradient updates on the continuous space to enforce fluent connection to the right contexts. This yields a continuous vector which is rounded back to text.</p><p>COLD <ref type="bibr" target="#b30">[31]</ref>. COLD specifies an energy-based model that includes fluency (from left-to-right and right-to-left LM) and coherence constraints (from lexical overlap). It samples continuous vectors from this energy-based model and round them to text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AR-infilling.</head><p>We train an autoregressive LM from scratch to do sentence infilling task <ref type="bibr" target="#b8">[9]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Main Results</head><p>We train Diffusion-LMs on the E2E and ROCStories datasets. In terms of negative log-likelihood (NLL, lower is better), we find that the variational upper bound of Diffusion-LM NLL <ref type="foot" target="#foot_9">10</ref> underperforms the equivalent autoregressive Transformer model (2.28 vs. 1.77 for E2E, 3.88 vs 3.05 for ROCStories) although scaling up model and dataset size partially bridges the gap (3.88 -? 3.10 on ROCStories). Our best log-likelihoods required several modifications from ?4; we explain these and give detailed log-likelihood results in Appendix F. Despite worse likelihoods, controllable generation based on our Diffusion-LM results in significantly better outputs than systems based on autoregressive LMs, as we will show in ?7.1, ?7.2, and ?7.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Classifier-Guided Controllable Text Generation Results</head><p>As shown in Table <ref type="table" target="#tab_2">2</ref>, Diffusion-LM achieves high success and fluency across all classifier-guided control tasks. It significantly outperforms the PPLM and FUDGE baselines across all 5 tasks. Surprisingly, our method outperforms the fine-tuning oracle on controlling syntactic parse trees and spans, while achieving similar performance on the remaining 3 tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FUDGE</head><p>In the city near The Portland Arms is a coffee and fast food place named The Cricketers which is not family -friendly with a customer rating of 5 out of 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion-LM</head><p>Located on the riverside , The Rice Boat is a restaurant that serves Indian food . FT Located near The Sorrento, The Mill is a pub that serves Indian cuisine.</p><p>Table <ref type="table">3</ref>: Qualitative examples from the Syntax Tree control. The syntactic parse tree is linearized by nested brackets representing the constituents, and we use the standard PTB syntactic categories. Tokens within each span are represented as * . We color failing spans red and bold the spans of interest that we discuss in ?7.1.</p><p>Controlling syntactic parse trees and spans are challenging tasks for fine-tuning, because conditioning on the parse tree requires reasoning about the nested structure of the parse tree, and conditioning on spans requires lookahead planning to ensure the right constituent appears at the target position.</p><p>We observe that PPLM fails in semantic content controls and conjecture that this is because PPLM is designed to control coarse-grained attributes, and may not be useful for more targeted tasks such as enforcing that a restaurant review contains a reference to Starbucks.</p><p>FUDGE performs well on semantic content control but does not perform well on the remaining four tasks. Controlling a structured output (Parts-of-speech and Syntax Tree) is hard for FUDGE because making one mistake anywhere in the prefix makes the discriminator assign low probabilities to all continuations. In other control tasks that require planning (Length and Syntax Spans), the future discriminator is difficult to train, as it must implicitly perform lookahead planning.</p><p>The non-autoregressive nature of our Diffusion-LM allows it to easily solve all the tasks that require precise future planning (Syntax Spans and Length). We believe that it works well for complex controls that involve global structures (Parts-of-speech, Syntax Tree) because the coarse-to-fine representations allow the classifier to exert control on the entire sequence (near t = T ) as well as on individual tokens (near t = 0).</p><p>Qualitative Results. Table <ref type="table">3</ref> shows samples of Syntax Tree control. Our method and fine-tuning both provide fluent sentences that mostly satisfy controls, whereas FUDGE deviates from the constraints after the first few words. One key difference between our method and fine-tuning is that Diffusion-LM is able to correct for a failed span and have suffix spans match the target. In the first example, the generated span ("Family friendly Indian food") is wrong because it contains 1 more word than the target. Fortunately, this error doesn't propagate to later spans, since Diffusion-LM adjusts by dropping the conjunction. Analogously, in the second example, the FT model generates a failed span ("The Mill") that contains 1 fewer word. However, the FT model fails to adjust in the suffix, leading to many misaligned errors in the suffix. Table <ref type="table">5</ref>: For sentence infilling, Diffusion-LM significantly outperforms prior work COLD <ref type="bibr" target="#b30">[31]</ref> and Delorean <ref type="bibr" target="#b29">[30]</ref> (numbers taken from paper), and matches the performance of an autoregressive LM (AR) trained from scratch to do infilling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Composition of Controls</head><p>One unique capability of plug-and-play controllable generation is its modularity. Given classifiers for multiple independent tasks, gradient guided control makes it simple to generate from the intersection of multiple controls by taking gradients on the sum of the classifier log-probabilities.</p><p>We evaluate this setting on the combination of Semantic Content + Syntax Tree control and Semantic Content + Parts-of-speech control. As shown in Table <ref type="table" target="#tab_3">4</ref>, our Diffusion-LM achieves a high success rate for both of the two components, whereas FUDGE gives up on the more global syntactic control. This is expected because FUDGE fails to control syntax on its own.</p><p>Fine-tuned models are good at POS and semantic content control individually but do not compose these two controls well by product of experts (PoE), leading to a large drop in success rates for both constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Infilling Results</head><p>As shown in Table <ref type="table">5</ref>, our diffusion LM significantly outperforms continuous relaxation based methods for infilling (COLD and DELOREAN). Moreover, our method achieves comparable performance to fine-tuning a specialized model for this task. Our method has slightly better automatic evaluation scores and the human evaluation found no statistically significant improvement for either method. These results suggest that Diffusion LM can solve many types of controllable generation tasks that depend on generation order or lexical constraints (such as infilling) without specialized training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Ablation Studies</head><p>We verify the importance of our proposed design choices in ?4 through two ablation studies. We measure the sample quality of Diffusion-LM using the lm-score on 500 samples ?6.2.</p><p>Learned v.s. Random Embeddings ( ?4.1). Learned embeddings outperform random embeddings on the ROCStories, which is a harder language modeling task. The same trend holds for the E2E dataset but with a smaller margin.</p><p>Objective Parametrization ( ?4.2). We propose to let the diffusion model predict x 0 directly. Here, we compare this with standard parametrization in image generation which parametrizes by the noise term . Figure <ref type="figure" target="#fig_3">4</ref> (right) shows that parametrizing by x 0 consistently attains good performance across dimensions, whereas parametrizing by works fine for small dimensions, but quickly collapses for larger dimensions. We proposed Diffusion-LM, a novel and controllable language model based on continuous diffusions, which enables new forms of complex fine-grained control tasks. We demonstrate Diffusion-LM's success in 6 fine-grained control tasks: our method almost doubles the control success rate of prior methods and is competitive with baseline fine-tuning methods that require additional training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Limitations</head><p>We find the complex controls enabled by Diffusion-LM to be compelling, and we are excited by how Diffusion-LM is a substantial departure from the current paradigm of discrete autoregressive generation. As with any new technologies, there are drawbacks to the Diffusion-LMs that we constructed: (1) it has higher perplexity; (2) decoding is substantially slower; and (3) training converges more slowly. We believe that with more follow-up work and optimization, many of these issues can be addressed, and this approach will turn out to be a compelling way to do controllable generation at scale. Because a diffusion model shares parameters for all diffusion steps, the noise schedule (parametrized by ?1:T ) is an important hyperparameter that determines how much weight we assign to each denoising problem. We find that standard noise schedules for continuous diffusions are not robust for text data. We hypothesize that the discrete nature of text and the rounding step make the model insensitive to noise near t = 0. Concretely, adding small amount of Gaussian noise to a word embedding is unlikely to change its nearest neighbor in the embedding space, making denoising an easy task near t = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Diffusion Noise Schedule</head><p>To address this, we introduce a new sqrt noise schedule that is better suited for text, shown in Figure <ref type="figure">5</ref> defined by ?t = 1 -t/T + s, where s is a small constant that corresponds to the starting noise level <ref type="foot" target="#foot_10">11</ref> . Compared to standard linear and cosine schedules, our sqrt schedule starts with a higher noise level and increase noise rapidly for the first 50 steps. Then sqrt slows down injecting noise to avoid spending much steps in the high-noise problems, which may be too difficult to solve well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters</head><p>Diffusion-LM hyperparameters. The hyperparameters that are specific to Diffusion-LM include the number of diffusion steps, the architecture of the Diffusion-LM, the embedding dimension, and the noise schedule, . We set the diffusion steps to be 2000, the architecture to be BERT-base <ref type="bibr" target="#b6">[7]</ref>, and the sequence length to be 64. For the embedding dimensions, we select from d ? {16, 64, 128, 256} and select d = 16 for the E2E dataset and d = 128 for ROCStories. For the noise schedule, we design the sqrt schedule (Appendix A) that is more robust to different parametrizations and embedding dimensions as shown in Appendix H. However, once we picked the x 0 -parametrization ( ?4.2) the advantage of sqrt schedule is not salient.</p><p>Training hyperparameters. We train Diffusion-LMs using AdamW optimizer and a linearly decay learning rate starting at 1e-4, dropout of 0.1, batch size of 64, and the total number of training iteration is 200K for E2E dataset, and 800K for ROCStories dataset. Our Diffusion-LMs are trained on a single GPU: NVIDIA RTX A5000, NVIDIA GeForce RTX 3090, or NVIDIA A100. It takes approximately 5 hours to train for 200K iterations on a single A100 GPU.</p><p>To stablize the training under L e2e vlb objective, we find that we need to set gradient clipping to 1.0 and apply importance sampling to reweight each term in L vlb <ref type="bibr" target="#b26">[27]</ref>. Both tricks are not necessary for L e2e simple objective.</p><p>Controllable Generation hyperparameters. To achieve controllable generation, we run gradient update on the continuous latents of Diffusion-LM. We use the AdaGrad optimizer <ref type="bibr" target="#b9">[10]</ref> to update the latent variables, and we tune the learning rate, lr ? {0.05, 0.1, 0.15, 0.2} and the trade-off parameter ? ? {0.1, 0.01, 0.001, 0.0005}. Different plug-and-play controllable generation approaches tradeoff between fluency and control by tunning different hyperparameters: PPLM uses the number of gradient updates per token, denoted as k, and we tune k ? {10, 30}. FUDGE uses the tradeoff parameter ? FUDGE and we tune this ? FUDGE ? {16, 8, 4, 2}. Table <ref type="table" target="#tab_4">6</ref> contains all the selected hyperparameter for each control tasks. Both PPLM and FUDGE has additional hyperparameters and we follow the instruction from the original paper to set those. For PPLM, we set the learning rate to be 0.04 and KL-scale to be 0.01. For FUDGE, we set precondition top-K to be 200, post top-K to be 10. To speed up decoding, we tried skipping steps in the generative diffusion process and downsample 2000 steps to 200 steps. Concretely, we set T = 200 and downsample the noise schedule ?t = ?10t , which is equivalent to setting each unit transition as the transition x t ? x t+10 . We decode Diffusion-LM using this new noise schedule and discretization. We find that this naive approach doesn't hurt sample quality for simple language modeling tasks like E2E, but it hurts sample quality for harder language modeling tasks like ROCStories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Decoding Speed</head><p>For plug-and-play controllable generation tasks, extant approaches are even slower. PPLM takes around 80 minutes to generate 50 samples (without batching), because it needs to run 30 gradient updates for each token. FUDGE takes 50 seconds to generate 50 samples (with batching), because it needs to call the lightweight classifier for each partial sequence, requiring 200 classifier calls for each token, yielding 100? sequence length calls. We can batch the classifier calls, but it sometimes limits batching across samples due to limited GPU memory. Our Diffusion-LM takes around 80 seconds to generate 50 samples (with batching). Our method downsamples the number of diffusion steps to 200, and it takes 3 classifier calls per diffusion step, yielding 600 model calls in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Classifiers for Classifier-Guided Controls</head><p>Semantic Content. We train an autoregressive LM (GPT-2 small architecture) to predict the (field, value) pair conditioned on text. To parametrize log p(c | x t ), we compute the logprob of "value" per token.</p><p>Parts-of-speech. The classifier is parametrized by a parts-of-speech tagger, which estimates the probability of the target POS sequence conditioned on the latent variables. This tagger uses a BERTbase architecture: the input is the concantenated word embedding, and output a softmax distribution over all POS tags for each input word. log p(c | x t ) is the sum of POS log-probs for each word in the sequence.</p><p>Syntax Tree. We train a Transformer-based constituency parser <ref type="bibr" target="#b19">[20]</ref>. Our parser makes locally normalized prediction for each span, predicting either "not a constituent", or a label for the constituent (e.g., Noun Phrase). log p(c | x t ) is the sum of log-probs for each labeled and non-constituency span in the sequence.</p><p>Syntax Span. We use the same parser trained for the syntax tree. log p(c | x t ) is the log-probability that the target span is annotated with the target label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E End-to-end Objective Derivations</head><p>For continuous diffusion models ( ?3.3), L simple is derived from the canonical objective L vlb by reweighting each term. The first T terms in L vlb are all KL divergence between two Gaussian distributions, which has a closed form solution. Take the t-th term for example:</p><formula xml:id="formula_13">E q(x 1:T |x0) log q(x t-1 |x 0 , x t ) p ? (x t-1 |x t ) = E q(x 1:T |x0) 1 2? 2 t ||? ? (x t , t) -?(x t , x 0 )|| 2 + C,<label>(3)</label></formula><p>where C is a constant, ? is the mean of the posterior q(x t-1 |x 0 , x t ), and ? ? is the mean of p ? (x t-1 | x t ) predicted by the diffusion model. Intuitively, this simplification matches the predicted mean of x t-1 to its true posterior mean. The simplification involves removing the constant C and the scaling factor 1 2? 2 t , yielding one term in L simple : Eq(x</p><formula xml:id="formula_14">1:T |x0) ||? ? (x t , t) -?(x t , x 0 )|| 2 .</formula><p>To apply continuous diffusion to model discrete text, we design Diffusion-LM ( ?4.1) and propose a new end-to-end training objective (equation ( <ref type="formula" target="#formula_5">2</ref>)) that learns the diffusion model and the embedding parameters jointly. The L e2e vlb can be written out as</p><formula xml:id="formula_15">L e2e vlb (w) = E q ? (x0|w) [L vlb (x 0 ) + log q ? (x 0 |w) -log p ? (w|x 0 )]] = E q ? (x 0:T |w) ? ? ? ? ? log q(x T |x 0 ) p ? (x T ) L T + T t=2 log q(x t-1 |x 0 , x t ) p ? (x t-1 |x t ) Lt-1 - log q ? (x 0 |w) log p ? (x 0 |x 1 ) L0 -log p ? (w|x 0 ) Lround ? ? ? ? ?</formula><p>We apply the same simplification which transforms L vlb ? L simple to transform L e2e vlb ? L e2e simple :</p><formula xml:id="formula_16">E q ? (x 0:T |w) [L T ] ? E[|| E x T ?q [x T |x 0 ] -0|| 2 ] = E[||?(x T ; x 0 )]|| 2 ] E q ? (x 0:T |w) [L t-1 ] ? E[|| E xt-1?q [x t-1 |x 0 , x t ] -E xt-1?p ? [x t-1 |x t ]|| 2 ] = E[||?(x t , x 0 ) -? ? (x t , t)|| 2 ] E q ? (x 0:T |w) [L 0 ] ? E[|| E x0?q ? [x 0 | w] -E x0?p ? [x 0 | x 1 ]|| 2 ] = E[||EMB(w) -? ? (x 1 , 1)|| 2 ]</formula><p>It's worth noting that the first term is constant if the noise schedule satisfies ?T = 0, which guarantees x T is pure Gaussian noise. In contrast, if the noise schedule doesn't go all the way such that x T is pure Gaussian noise, we need to include this regularization term to prevent the embedding from learning too large norms. Embedding with large norms is a degenerate solution, because it is impossible to sample from p(x T ) accurately, even though it makes all the other denoising transitions easily predictable.</p><p>Combining these terms yield L e2e simple .</p><formula xml:id="formula_17">L e2e simple (w) = E q ? (x 0:T |w) ||?(x T ; x 0 )|| 2 + T t=2 [||?(x t , x 0 ) -? ? (x t , t)|| 2 ] + E q ? (x0:1|w) ||EMB(w) -? ? (x 1 , 1)|| 2 -log p ? (w|x 0 ) .</formula><p>Intuitively, we learn a Transformer model that that takes as input (x t , t) ? (R nd , R) and the goal is to predict the distribution of x t-1 ? R nd . It's worth noting that this Transformer model is shared across all the diffusion steps t = 1 . . . T . As we demonstrated in the derivation of L e2e simple , the most natural thing is to directly parametrize the neural network to predict the mean of x t-1 | x t , we call this ? ? -parametrization.</p><p>There are other parametrizations that are equivalent to ? ? -parametrization up to a scaling constant. For example in ?4.2, we can train the Transformer model to directly predict x 0 via f ? (x t , t), and use the tractable Gaussian posterior q(x t-1 | x 0 , x t ) to compute the mean of x t-1 , which has a closed form solution, conditioned on predicted x 0 and observed x t : These two parametrizations differ by a constant scaling, and we apply the x 0 -parametrization to all terms in L e2e simple to reduce rounding errors as discussed in ?4.2:</p><formula xml:id="formula_18">? ?t-1?t 1-?t x 0 + ? ?t(1-?t-1) 1-?t x t . ||?(x t , x 0 ) -? ? (x t , t)|| 2 =||( ? ?t-1 ? t 1 -?t x 0 + ? ? t (1 -?t-1 ) 1 -?t x t ) -( ? ?t-1 ? t 1 -?t f ? (x t , t) + ? ? t (1 -?t-1 ) 1 -?t x t )|| 2 =|| ? ?t-1 ? t 1 -?t (x 0 -f ? (x t , t))|| 2 ?||x 0 -f ? (x t ,</formula><formula xml:id="formula_19">L e2e x0-simple (w) = E q ? (x 0:T |w) ||?(x T ; x 0 )|| 2 + T t=2 [||x 0 -f ? (x t , t)|| 2 ] + E q ? (x0:1|w) ||EMB(w) -f ? (x 1 , 1)|| 2 -log p ? (w|x 0 ) .</formula><p>To generate samples from a Diffusion-LM with x 0 -parametrization, at each diffusion step, the model estimates the x 0 via f ? (x t , t) and then we sample x t-1 from q(x t-1 | f ? (x t , t), x t ), which is fed as input to the next diffusion step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Log-Likelihood Models and Results</head><p>To investigate Diffusion-LM's log-likelihood performance, we make several departures from the training procedure of ?4. Ultimately the log-likelihood improvements described in this section did not translate into better generation quality in our experiments and therefore we focus on the original method in the rest of the paper. Our likelihood models are trained as follows:</p><p>? Instead of training a diffusion model on sequences of low-dimensional token embeddings, we train a model directly sequences of on one-hot token vectors. ? Following the setup of Kingma et al. <ref type="bibr" target="#b18">[19]</ref>, we train a continuous-time diffusion model against the log-likelihood bound and learn the noise schedule simultaneously with the rest of the model to minimize the loss variance. ? Because our model predicts sequences of one-hot vectors, we use a softmax nonlinearity at its output and replace all squared-error terms in the loss function with cross-entropy terms. This choice of surrogate loss led to better optimization, even though we evaluate against the original loss with squared-error terms. ? The model applies the following transformation to its inputs before any Transformer layers:</p><p>x := softmax(?(t)x + ?(t)) where ?(t) ? R and ?(t) ? R v are learned functions of the diffusion timestep t parameterized by MLPs (v is the vocabulary size). ? At inference time, we omit the rounding procedure in ?4.2.</p><p>For exact model architecture and training hyperparameter details, please refer to our released code.</p><p>We train these diffusion models, as well as baseline autoregressive Transformers, on E2E and ROCStories and report log-likelihoods in Table <ref type="table" target="#tab_5">7</ref>. We train two sizes of Transformers: "small" models with roughly 100M parameters and "medium" models with roughly 300M parameters. Both E2E and ROCstories are small enough datasets that all of our models reach their minimum test loss early in training (and overfit after that). To additionally compare model performance in a large-dataset regime, we also present "ROCStories (+GPT-J)" experiments in which we generate 8M examples of synthetic ROCStories training data by finetuning GPT-J <ref type="bibr" target="#b42">[43]</ref> on the original ROCStories data, pretrain our models on the synthetic dataset, and then finetune and evaluate them on the original ROCStories data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Qualitative Examples</head><p>We show randomly sampled outputs of Diffusion-LM both for unconditional generation and for the 5 control tasks. Table <ref type="table">8</ref> shows the unconditional generation results. Table <ref type="table">9</ref>, Table <ref type="table" target="#tab_8">10</ref>, Table <ref type="table" target="#tab_2">12</ref>, and Table <ref type="table">3</ref> show the qualitative samples from span control, POS control, semantic content control, and syntax tree control, respectively. Table <ref type="table" target="#tab_9">11</ref> shows the results of length control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Additional Ablation Studies</head><p>In addition to the 2 ablation studies in ?7.4, we provide more ablation results in Figure <ref type="figure">6</ref> about architecture choices and noise schedule. Learned v.s. Random Embeddings ( ?4.1). Learned embeddings outperform random embeddings on both ROCStories and the E2E dataset by xx percent and xx percent respectively, as shown in the first row of Figure <ref type="figure">6</ref>.</p><p>Noise Schedule (Appendix A). We compare the sqrt schedule with cosine <ref type="bibr" target="#b26">[27]</ref> and linear <ref type="bibr" target="#b11">[12]</ref> schedules proposed for image modeling. The middle row of Figure <ref type="figure">6</ref> demonstrates that sqrt schedule attains consistently good and stable performance across all dimension and parametrization choices. While the sqrt schedule is less important with x 0 -parametrization, we see that it provides a substantially more robust noise schedule under alternative parametrizations such as . Transformer v.s. U-Net. The U-Net architecture in Ho et al. <ref type="bibr" target="#b11">[12]</ref> utilizes 2D-convolutional layers, and we imitate all the model architectures except changing 2D-conv to 1D-conv which is suitable for text data. Figure <ref type="figure">6</ref> (last row) shows that the Transformer architecture outperforms U-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Societal Impacts</head><p>On the one hand, having strong controllability in language models will help with mitigating toxicity, making the language models more reliable to deploy. Additionally, we can also control the model to be more truthful, reducing the inaccurate information generated by the language model by carefully controlling it to be truthful. On the other hand, however, one could also imagine more powerful targeted disinformation (e.g., narrative wedging) derived from the fine-grained controllability.</p><p>Towards this end, it might be worth considering generation methods that can watermark the generated outputs without affecting its fluency, and this type of watermark could also be framed as a controllable generation problem, with distinguish-ability and fluency as the constraints.</p><p>target span <ref type="bibr">[3, 5,</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FUDGE</head><p>The Golden Palace is a cheap , 5 -star coffee shop , located on the river in the north of the city centre . Diffusion-LM The Olive Grove is a pub that provides Indian food in the high price range . It is in the city centre .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FT</head><p>The Golden Curry is located in city centre near Caf? Rouge which provides English food . Its customer rating is average and is not family -friendly . target span <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13</ref>, NP] FUDGE The Waterman is a family friendly place with a good rating .[missing span] Diffusion-LM The Vaults is a high priced , family friendly restaurant that serves Italian food . FT Strada is a restaurant which costs less than ? 20 , but is not family -friendly and has an average rating .</p><p>Table <ref type="table">9</ref>: Qualitative output of the syntax span control tasks. The target span (i, j,label) means the span from position i to position j should be a constituent with a specific label: S is sentence, NP is noun phrase, VP is verb phrase, PP is prepositional phrase, etc. We color failed spans red and correct spans green.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FUDGE</head><p>The Golden Curry is an English food restaurant located near the Caf? Rouge in the Riverside area . The customer rating is average . Children are welcome . Diffusion-LM Strada is a fast food pub located near Yippee Noodle Bar and has a customer rating of 3 out of 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FT</head><p>There is an Italian kid friendly restaurant in the riverside area near The Sorrento named Browns Cambridge in the riverside area . target length 27</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FUDGE</head><p>The Olive Grove is an expensive , children friendly , Fast food restaurant in the city centre .</p><p>[missing 9 words] Diffusion-LM The Eagle is a family friendly coffee shop in the city centre near Burger King . It serves Italian food and has a low customer rating . FT A pub in the city centre near Yippee Noodle Bar is named Strada. It serves French food and has a customer rating of 3 out of 5 target length 32</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FUDGE</head><p>The Golden Curry is a Japanese food restaurant with a high customer Rating , kid friendly and located along the riverside near Caf? Rouge . [missing 7 words] Diffusion-LM There is a family -friendly coffee shop in the city centre , it is called Zizzi . It is cheap and has a customer rating of 5 out of 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FT</head><p>In the city centre is a kids friendly place called Green Man. It has Japanese food and is near All Bar One. It has a price range of ? 20 -25 target length 37 FUDGE There is a coffee shop called Fitzbillies which offers French food at cheap prices . It is not family -friendly and has a customer rating of 5 out of 5 . It is in riverside . Diffusion-LM The Rice Boat provides Indian food in the moderate price range . It is located in the city centre . It is near Express by Holiday Inn . Its customer rating is 3 out of 5 . FT For a family friendly coffee shop that serves Italian food, with a customer rating of 5 out of 5 and a cheap price range, try The Eagle. It is located in the riverside area . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FUDGE</head><p>The Vaults is a high priced Italian Pub with a customer rating of 3 out of 5 near Caf? Adriatic Diffusion-LM</p><p>The Punter is a French restaurant with a high price range . FT A fast food coffee shop that is not kid friendly is called Cocum. It is expensive and gets average ratings.</p><p>Table <ref type="table" target="#tab_2">12</ref>: Qualitative output of the semantic content control task. We mark the compliant spans as green, and the spans that violates the control target as red.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a intermediate latent variables of decreasing noise level x T ? ? ? x 0 . For controllable generation, we iteratively perform gradient updates on these continuous latents to optimize for fluency (parametrized by Diffusion-LM) and satisfy control requirements (parametrized by a classifier).</figDesc><graphic url="image-1.png" coords="2,147.60,72.00,316.78,100.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>input (Semantic Content) food : Japanese output text Browns Cambridge is good for Japanese food and also children friendly near The Sorrento . input (Parts-of-speech) PROPN AUX DET ADJ NOUN NOUN VERB ADP DET NOUN ADP DET NOUN PUNCT output text Zizzi is a local coffee shop located on the outskirts of the city . input (Syntax Tree) (TOP (S (NP (*) (*) (*)) (VP (*) (NP (NP (*) (*)))))) output text The Twenty Two has great food input (Syntax Spans) (7, 10, VP) output text Wildwood pub serves multicultural dishes and is ranked 3 stars input (Length) 14 output text Browns Cambridge offers Japanese food located near The Sorrento in the city centre . input (left context) My dog loved tennis balls. input (right context)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Syntactic</head><label></label><figDesc>Parse ( S ( S ( NP * ) ( VP * ( NP ( NP * * ) ( VP * ( NP ( ADJP * * ) * ) ) ) ) ) * ( S ( NP * * * ) ( VP * ( ADJP ( ADJP * ) ) ) ) ) FUDGE Zizzi is a cheap restaurant . [incomplete] Diffusion-LM Zizzi is a pub providing family friendly Indian food Its customer rating is low FT Cocum is a Pub serving moderately priced meals and the customer rating is high Syntactic Parse ( S ( S ( VP * ( PP * ( NP * * ) ) ) ) * ( NP * * * ) ( VP * ( NP ( NP * * ) ( SBAR ( WHNP * ) ( S ( VP * ( NP * * ) ) ) ) ) ) * )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: We measure the impact of our proposed design choices through lm-score. We find both learned embeddings and reparametrization substantially improves sample quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 5: Visualizing the noise schedule ? 1 -?t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>)</figDesc><table><row><cell>Learned Embeddings</cell></row><row><cell>NOUN PROPN AUX VERB ADP DET ADJ PRON ADV SCONJ NUM</cell></row><row><cell>Figure 3: A t-SNE [41] plot of the learned word</cell></row><row><cell>embeddings. Each word is colored by its POS.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Semantic Content Parts-of-speech Syntax Tree Syntax Spans Diffusion-LM achieves high success rate (ctrl ?) and good fluency (lm ?) across all 5 control tasks, outperforming the PPLM and FUDGE baselines. Our method even outperforms the fine-tuning oracle (FT) on controlling syntactic parse trees and spans.</figDesc><table><row><cell>Length</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Semantic Content + Syntax TreeSemantic Content + Parts-of-speech semantic ctrl ? syntax ctrl ? lm ? semantic ctrl ? POS ctrl ? lm ? In this experiment, we compose semantic control and syntactic control: Diffusion-LM achieves higher success rate (ctrl ?) at some cost of fluency (lm ?). Our method outperforms both FUDGE and FT-PoE (product of experts of two fine-tuned models) on control success rate, especially for the structured syntactic controls (i.e. syntactic parse tree and POS).</figDesc><table><row><cell>FUDGE</cell><cell>61.7</cell><cell>15.4</cell><cell>3.52</cell><cell>64.5</cell><cell>24.1</cell><cell>3.52</cell></row><row><cell>Diffusion-LM</cell><cell>69.8</cell><cell>74.8</cell><cell>5.92</cell><cell>63.7</cell><cell>69.1</cell><cell>3.46</cell></row><row><cell>FT-PoE</cell><cell>61.7</cell><cell>29.2</cell><cell>2.77</cell><cell>29.4</cell><cell>10.5</cell><cell>2.97</cell></row><row><cell></cell><cell></cell><cell cols="2">Automatic Eval</cell><cell></cell><cell cols="2">Human Eval</cell></row><row><cell></cell><cell cols="4">BLEU-4 ? ROUGE-L ? CIDEr ? BERTScore ?</cell><cell></cell><cell></cell></row><row><cell>Left-only</cell><cell>0.9</cell><cell>16.3</cell><cell>3.5</cell><cell>38.5</cell><cell>n/a</cell><cell></cell></row><row><cell>DELOREAN</cell><cell>1.6</cell><cell>19.1</cell><cell>7.9</cell><cell>41.7</cell><cell>n/a</cell><cell></cell></row><row><cell>COLD</cell><cell>1.8</cell><cell>19.5</cell><cell>10.7</cell><cell>42.7</cell><cell>n/a</cell><cell></cell></row><row><cell>Diffusion</cell><cell>7.1</cell><cell>28.3</cell><cell>30.7</cell><cell>89.0</cell><cell>0.37 +0.03 -0.02</cell><cell></cell></row><row><cell>AR</cell><cell>6.7</cell><cell>27.0</cell><cell>26.9</cell><cell>89.0</cell><cell>0.39 +0.02 -0.03</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters for controllable generation methods.</figDesc><table><row><cell></cell><cell cols="2">Semantic</cell><cell cols="2">Parts-of-speech</cell><cell cols="2">Syntax Tree</cell><cell cols="2">Syntax Spans</cell><cell>Length</cell><cell></cell></row><row><cell></cell><cell>tradeoff</cell><cell>lr</cell><cell>tradeoff</cell><cell>lr</cell><cell>tradeoff</cell><cell>lr</cell><cell>tradeoff</cell><cell>lr</cell><cell>tradeoff</cell><cell>lr</cell></row><row><cell>PPLM</cell><cell>30</cell><cell>0.04</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FUDGE</cell><cell>8.0</cell><cell>-</cell><cell>20.0</cell><cell>-</cell><cell>20.0</cell><cell>-</cell><cell>20.0</cell><cell>-</cell><cell>2.0</cell><cell>-</cell></row><row><cell>Diffusion-LM</cell><cell>0.01</cell><cell>0.1</cell><cell cols="4">0.0005 0.05 0.0005 0.2</cell><cell>0.1</cell><cell>0.15</cell><cell>0.01</cell><cell>0.1</cell></row><row><cell cols="11">medium-length sequence regimes. Concretely, it takes around 1 minute to decode 50 sequence of</cell></row><row><cell>length 64.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Sampling from Diffusion-LMs requires iterating through the 2000 diffusion steps, yielding O(2000) f ? model calls. In contrast, sampling from autoregressive LMs takes O(n) where n is the sequence length. Therefore, decoding Diffusion-LM is slower than decoding autoregressive LMs in short and</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>t)|| 2 Log-likelihood results (nats per token)</figDesc><table><row><cell>Dataset</cell><cell cols="3">Small AR Small Diffusion Medium Diffusion</cell></row><row><cell>E2E</cell><cell>1.77</cell><cell>2.28</cell><cell>-</cell></row><row><cell>ROCStories</cell><cell>3.05</cell><cell>3.88</cell><cell>-</cell></row><row><cell>ROCStories (+GPT-J)</cell><cell>2.41</cell><cell>3.59</cell><cell>3.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>PP] FUDGE UNK the UNK for Italian food , The Eagle coffee shop is near Burger King in the riverside area . The Eagle has a customer rating of 5 out of 5 , and isn ' t family -friendly . The Eagle has a cheap price range . Diffusion-LM The Plough , near Caf? Rouge , is a high priced fast food pub . FT Along the riverside near Caf? Rouge is The Golden Curry . It serves Italian food in a familyfriendly environment . It has a low customer rating . Italian food for adults only . It has been rated average by customers . Diffusion-LM There is a Chinese restaurant called The Eagle , it has an average customer rating . FT On the riverside area are located Alimentum , has a very good French food for adults and kids , UNK price range are over 20 to 25 ? .</figDesc><table><row><cell>target span</cell><cell>[10, 12, PP]</cell></row><row><cell>FUDGE</cell><cell>Blue Spice is a high price range Fast food located in city centre .</cell></row><row><cell cols="2">Diffusion-LM The Phoenix is a high priced food restaurant , located near the river .</cell></row><row><cell>FT</cell><cell>The Punter is a family restaurant with low prices and delicious sushi , located near the Caf?</cell></row><row><cell></cell><cell>Sicilia</cell></row><row><cell>target span</cell><cell>[9, 14, S]</cell></row><row><cell cols="2">FUDGE Zizzi pub serves target span [4, 16, VP]</cell></row><row><cell>FUDGE</cell><cell>The Cambridge Blue pub is near the Caf? Brazil and offers a high price range for their French</cell></row><row><cell></cell><cell>food .</cell></row><row><cell cols="2">Diffusion-LM On the Ranch there is a children friendly pub called The Cricketers with an average customer</cell></row><row><cell></cell><cell>rating .</cell></row><row><cell>FT</cell><cell>The Travellers Rest Beefeater is an average rated restaurant located in the riverside area near</cell></row><row><cell></cell><cell>Caf? Adriatic . Their price range is less than ? 20 .</cell></row><row><cell>target span</cell><cell>[0, 2, NP]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Cocum , offers fast food at an average price range of ? 20 -25 . Diffusion-LM The Waterman -friendly serves UNK and fast food and is located near the Crown Plaza Hotel . FT The wine -Strada serves fast and cheap food and is located near the Rainbow Vegetarian Caf?. target POS DET PROPN PROPN VERB ADJ NOUN ADP NOUN ADP SYM NUM PUNCT NOUN NOUN AUX ADJ PUNCT DET PROPN PROPN AUX VERB ADP DET PROPN CCONJ PROPN ADP PROPN PROPN PUNCT ADJ PUNCT DET NOUN PART AUX VERB PUNCT FUDGE The Midsummer House offers cheap English food near All Bar One . Rated 5 out of 5 . Diffusion-LM The Rice Boat provides Chinese food in ? 20 -25 . Price range is high . The Rice Boat is located near the Express by Holiday Inn and is kids friendly . The customer rating is high . FT The Rice Boat welcomes Japanese food with prices under ? 20. Customer ratings are low. The Rice Boat is located near the Express by Holiday Inn. Convenient. No children's are allowed. target POS PROPN PROPN AUX DET ADJ NOUN NOUN ADP DET NOUN NOUN ADP DET PROPN PUNCT PRON AUX NOUN PUNCT ADJ PUNCT FUDGE Loch Fyne is a Japanese restaurant with a moderate price range and kid -friendly atmosphere . Diffusion-LM Browns Cambridge is an Italian restaurant shop in the city centre near The Sorrento .</figDesc><table><row><cell>target POS</cell><cell>PROPN AUX DET ADJ NOUN NOUN VERB ADP DET NOUN ADP DET NOUN PUNCT</cell></row><row><cell>FUDGE</cell><cell>Aromi is a non family -friendly fast food coffee shop in the riverside area with a low Customer</cell></row><row><cell></cell><cell>Rating .</cell></row><row><cell cols="2">Diffusion-LM Fitzbillies is a cheap coffee shop located on the outskirts of the city .</cell></row><row><cell>FT</cell><cell>Aromi is a fast food pub located at the centre of the city.</cell></row><row><cell>target POS</cell><cell>PROPN AUX DET NOUN VERB NOUN ADJ NOUN PUNCT PRON NOUN NOUN AUX</cell></row><row><cell></cell><cell>ADJ</cell></row><row><cell>FUDGE</cell><cell>Cocum is a family -friendly coffee shop , that has a low price range and a low customer rating</cell></row><row><cell></cell><cell>.</cell></row><row><cell cols="2">Diffusion-LM Zizzi is a pub providing restaurant Chinese food . Its customer rating is low</cell></row><row><cell>FT</cell><cell>Zizzi is a pub providing kids friendly services. Its customer rating is average</cell></row><row><cell>target POS</cell><cell>DET NOUN PUNCT PROPN VERB ADJ CCONJ ADJ NOUN CCONJ AUX VERB ADP</cell></row><row><cell></cell><cell>DET PROPN ADJ PROPN PUNCT</cell></row><row><cell>FUDGE</cell><cell>A child -friendly coffee shop , It is</cell></row><row><cell></cell><cell>family -friendly .</cell></row><row><cell>FT</cell><cell>Browns Cambridge is a cheap coffee shop in the riverside area near The Sorrento, that is</cell></row><row><cell></cell><cell>family -friendly.</cell></row><row><cell>target POS</cell><cell>PROPN VERB DET ADJ NOUN NOUN PROPN PUNCT PRON AUX ADJ VERB CCONJ</cell></row><row><cell></cell><cell>VERB NOUN SCONJ VERB ADJ NOUN PUNCT</cell></row><row><cell>FUDGE</cell><cell>Fitzbillies coffee shop has a high price range , children friendly service and serves Japanese</cell></row><row><cell></cell><cell>food in riverside with high customer rating .</cell></row><row><cell cols="2">Diffusion-LM There has a high customer rating . It is kid friendly called The Golden Curry and serves Indian</cell></row><row><cell></cell><cell>food .</cell></row><row><cell>FT</cell><cell>Customers give the French coffee shop Fitzbillies ; it is average rated and offers families where</cell></row><row><cell></cell><cell>serving light meals.</cell></row><row><cell>target POS</cell><cell>DET NUM NUM VERB ADJ NOUN</cell></row><row><cell>FUDGE</cell><cell>The Twenty Two serves Fast food and is kids friendly .</cell></row><row><cell cols="2">Diffusion-LM The Twenty Two provides Chinese food</cell></row><row><cell>FT</cell><cell>The Twenty Two provides Indian food</cell></row><row><cell>target POS</cell><cell>ADV NOUN ADV ADP PROPN PROPN PUNCT DET PROPN NOUN NOUN VERB ADJ</cell></row><row><cell></cell><cell>NOUN NOUN CCONJ AUX PART VERB NOUN NOUN PUNCT</cell></row><row><cell>FUDGE</cell><cell>UNK your whole family to The Wrestlers , the best UNK the UNK UNK at the river</cell></row><row><cell cols="2">Diffusion-LM Located in riverside near The Sorrento , Browns Cambridge coffee shop serves Japanese food ,</cell></row><row><cell></cell><cell>and is not family -friendly .</cell></row><row><cell>FT</cell><cell>Even adults only at Loch Fyne, The Rice Boat coffee shop has moderate price range and does</cell></row><row><cell></cell><cell>not cater kids age.</cell></row><row><cell>target POS</cell><cell>DET PROPN AUX DET NUM NOUN NOUN NOUN VERB ADP DET PROPN PROPN</cell></row><row><cell></cell><cell>PUNCT</cell></row><row><cell>FUDGE</cell><cell>The Eagle is a 3 star coffee shop located near Burger King , north of the City centre that</cell></row><row><cell></cell><cell>provides low -cost fast food .</cell></row><row><cell cols="2">Diffusion-LM The Cricketers is a five star coffee shop located near The Portland Arms .</cell></row></table><note><p><p>FT</p>The Vaults is a one star coffee shop located near the Caf? Brazil.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Qualitative output of the POS control tasks. The target POS is the sequence of gold parts-of-speech tags the generated texts should match. an average Japanese restaurant that is in the City Centre .Diffusion-LM The Twenty Two serves Chinese food and is not family friendly . FT Green Man is an average priced restaurant located near All Bar One target length 17 FUDGE Fitzbillies is an expensive Italian coffee shop in the city centre . It is not child friendly. . Diffusion-LM The Twenty Two serves Indian food in the city centre . It is not family friendly . FT For low -priced food and a family -friendly atmosphere, visit Fitzbillies near Express by Holiday Inn target length 22</figDesc><table><row><cell>target length</cell><cell>7</cell></row><row><cell>FUDGE</cell><cell>Wildwood is a cheap Japanese pub . Low rating .</cell></row><row><cell cols="2">Diffusion-LM The Twenty Two serves Indian food .</cell></row><row><cell>FT</cell><cell>The Mill is an Indian restaurant .</cell></row><row><cell>target length</cell><cell>12</cell></row><row><cell>FUDGE</cell><cell>The Phoenix is</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>Qualitative output of the length control tasks, where all the generated texts tried to exactly match the target length. We mark the words exceeding the target length red. target semantic content name : Bibimbap House FUDGE Clare Hall , the Bibimbap House , serves high end Japanese food in the city centre . Diffusion-LM Bibimbap House in riverside near Clare Hall has a cheap price range . FT By Clare Hall is Bibimbap House which serves expensive noodles. target semantic content name : Travellers Rest Beefeater FUDGE Clowns near Clare Hall in riverside is a French coffee shop rated 5 out of 5 Diffusion-LM Green Man is an Italian pub located in the city centre near Caf? UNK . FT Travellers Rest Beefeater is a reasonably priced restaurant that is family friendly. target semantic content Type : coffee shop FUDGE Wildwood is a coffee shop located near Ranch . It is expensive and highly UNK . Diffusion-LM The Punter is a high priced coffee shop located near Caf? Sicilia that serves Japanese food . It is not family -friendly and has a customer rating of 3 out of 5 . FT Located in the riverside area is the coffee shop Fitzbillies. It has Indian food in the price Range of less than ? 20 and a low customer Rating. It is not family Friendly. target semantic content customer rating : low FUDGE The Waterman is a fast food restaurant that is family -friendly near the city centre . [missing content] Diffusion-LM The Rice Boat restaurant has a low customer rating and is located in riverside . It serves Italian food , and is not family -friendly . FT The Eagle is low customer rating coffee shop with Italian food in riverside near Burger King. Its price range is less than ? 20 and is family -friendly. target semantic content near : The Sorrento FUDGE Browns Cambridge provides Indian food in the less than ? 20 price range . Its customer rating is low . [missing content] Diffusion-LM Near The Sorrento on the riverside is a pub named Taste of Cambridge that serves Japanese food . FT Browns Cambridge sells Italian food and is also a coffee shop. It has an average customer rating. It is located in the riverside area near Crowne Plaza Hotel and yes it is child friendly. target semantic content food : Italian FUDGE A non family -friendly Italian pub is Zizzi . It has an average customer rating . Diffusion-LM Loch Fyne is Italian Japanese restaurant that is kid friendly . FT situated near All Bar One is a child friendly Italian eatery called Green Man costing more than ? 30 is a restaurant near the riverside target semantic content price : high</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code is available at https://github.com/XiangLi1999/Diffusion-LM.git Preprint. Under review.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Our definition of Lsimple here uses a different parametrization from Ho et al.<ref type="bibr" target="#b11">[12]</ref>. We define our squared loss in terms of ? ? (xt, t) while they express it in terms of ? (xt, t).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>While trainable embeddings perform best on control and generation tasks, we found that fixed embeddings onto the vocabulary simplex were helpful when optimizing for held-out perplexity. We leave discussion of this approach and perplexity results to Appendix F as the focus of this work is generation quality and not perplexity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Predicting x0 and xt-1 is equivalent up to scaling constants as the distribution of xt-1 can be obtained in closed form via the forward process xt-1 = ? ?x0 + ? 1 -? , see Appendix E for further details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>This follows from the marginal distribution q(xt | x0), which is a closed form Gaussian since all the Markov transitions are Gaussian.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Intuitively, applying the clamping trick to early diffusion steps with t near T may be sub-optimal, because the model hasn't figured out what words to commit to. Empirically, applying clamping trick for all diffusion steps doesn't hurt the performance much. But to follow this intuition, one could also set the starting step of the clamping trick as a hyperparameter.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>We tried ablations that replaced Adagrad with SGD, but we found Adagrad to be substantially less sensitive to hyperparameter tuning.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>Length is classifier-free for our Diffusion-LM based methods, but other methods still require a classifier.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>Prior works<ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b5">6]</ref> use GPT<ref type="bibr" target="#b31">[32]</ref> as the teacher LM whereas we use a fine-tuned GPT-2 model because our base autoregressive LM and Diffusion-LM both generate UNK tokens, which does not exist in pretrained vocabularies of GPT.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>Exact log-likelihoods are intractable for Diffusion-LM, so we report the lower bound L e2e vlb .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>We set s =1e-4, and T = 2000, which sets the initial standard deviation to 0.1.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>We thank <rs type="person">Yang Song</rs>, <rs type="person">Jason Eisner</rs>, <rs type="person">Tianyi Zhang</rs>, <rs type="person">Rohan Taori</rs>, <rs type="person">Xuechen Li</rs>, <rs type="person">Niladri Chatterji</rs>, and the members of p-lambda group for early discussions and feedbacks. We gratefully acknowledge the support of a <rs type="grantName">PECASE award. Xiang Lisa Li</rs> is supported by a <rs type="grantName">Stanford Graduate Fellowship</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_uH2vXCN">
					<orgName type="grant-name">PECASE award. Xiang Lisa Li</orgName>
				</org>
				<org type="funding" xml:id="_u9carUa">
					<orgName type="grant-name">Stanford Graduate Fellowship</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture Choice</head><p>Figure <ref type="figure">6</ref>: Additional ablation results. The first row shows Diffusion-LM with trainable embeddings outperform random embeddings on both datasets ( ?4.1). The second row demonstrates that sqrt schedule attains consistently good and stable performance across all dimension and parametrization choices. The last row shows that Transformer architecture outperforms U-Net architecture for language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROCStories+Aug</head><p>Matt was at the store . He was looking at a new toothbrush . He found the perfect one . When he got home , he bought it . It was bright and he loved it .</p><p>I and my friend were hungry . We were looking for some to eat . We went to the grocery store . We bought some snacks . We decided to pick up some snacks . I was at the store . I had no money to buy milk . I decided to use the restroom . I went to the register . I was late to work .</p><p>The man wanted to lose weight . He did n't know how to do weight . He decided to start walking . He ate healthy and ate less . He lost ten pounds in three months .</p><p>I went to the aquarium . I wanted to feed something . I ordered a fish . When it arrived I had to find something . I was disappointed .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROCStories</head><p>Tom was planning a trip to California . He had fun in the new apartment . He was driving , until it began to rain . Unfortunately , he was soaked . Tom stayed in the rain at the beach .</p><p>Carrie wanted a new dress . She did not have enough money . She went to the bank to get one , but saw the missed . Finally , she decided to call her mom . She could not wait to see her new dress .</p><p>Tina went to her first football game . She was excited about it . When she got into the car she realized she forgot her hand . She ended up getting too late . Tina had to start crying .</p><p>Michael was at the park . Suddenly he found a stray cat . He decided to keep the cat . He went to his parents and demanded a leg . His parents gave him medicine to get it safe .</p><p>Tim was eating out with friends . They were out of service . Tim decided to have a pizza sandwich . Tim searched for several hours . He was able to find it within minutes .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E2E</head><p>The Waterman is an expensive pub that serves Japanese food . It is located in Riverside and has a low customer rating .</p><p>A high priced pub in the city centre is The Olive Grove . It is a family friendly pub serving French food .</p><p>The Rice Boat offers moderate priced Chinese food with a customer rating of 3 out of 5 . It is near Express by Holiday Inn .</p><p>There is a fast food restaurant , The Phoenix , in the city centre . It has a price range of more than 00a3 30 and the customer ratings are low .</p><p>The Mill is a coffee shop based in the city centre area near to The Sorrento . It is in the high price range and serves Indian food . </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structured denoising diffusion models in discrete state-spaces</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Berg</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=h7-XixPCAL" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abductive commonsense reasoning</title>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Byg1v1HKDB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1457" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wavegrad: Estimating gradients for waveform generation</title>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=NsMLjcFaO8O" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benton</forename><forename type="middle">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garc?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanumalayan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Sankaranarayana Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Oliveira Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">Scaling language modeling with pathways</title>
		<meeting><address><addrLine>Jeff Dean, Slav Petrov; Palm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>and Noah Fiedel</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Plug and play language models: A simple approach to controlled text generation</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1edEyBKDS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>ArXiv, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diffusion models beat GANs on image synthesis</title>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nichol</forename></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=AAWuCvzaVt" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enabling language models to fill in the blanks</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mina</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.225</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.225" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="2492" to="2501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Nonautoregressive neural machine translation</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1l8BtlCb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rygGQyrFvH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didrik</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyank</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Forr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05379</idno>
		<title level="m">Argmax flows and multinomial diffusion: Towards non-autoregressive language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rianne van den Berg, and Tim Salimans</title>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><forename type="middle">A</forename><surname>Gritsenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmijn</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Lm8T39vLDTE" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Autoregressive diffusion models</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ctrl: A conditional transformer language model for controllable generation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.05858</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Genie: A leaderboard for human-in-the-loop evaluation of text generation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno>ArXiv, abs/2101.06561</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00630</idno>
		<title level="m">Variational diffusion models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Constituency parsing with a self-attentive encoder</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1249</idno>
		<ptr target="https://aclanthology.org/P18-1249" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2676" to="2686" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<title level="m">Diffwave: A versatile diffusion model for audio synthesis</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName><surname>Gedi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06367</idno>
		<title level="m">Generative Discriminator Guided Sequence Generation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Minimum Bayes-risk decoding for statistical machine translation</title>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Byrne</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/N04-1022" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004-05-07">May 2 -May 7 2004</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DExperts: Decoding-time controlled text generation with experts and anti-experts</title>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.522</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.522" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6691" to="6706" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Symbolic music generation with diffusion models</title>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16091</idno>
		<imprint>
			<date type="published" when="2021-03">March 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1098</idno>
		<ptr target="https://aclanthology.org/N16-1098" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09672</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The E2E dataset: New challenges for end-to-end generation</title>
		<author>
			<persName><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ond?ej</forename><surname>Du?ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5525</idno>
		<ptr target="https://aclanthology.org/W17-5525" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbr?cken, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08">August 2017</date>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://aclanthology.org/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10">October 2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Back to the future: Unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning</title>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.58</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.58" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="794" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cold decoding: Energy-based constrained text generation with langevin dynamics</title>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2202.11705" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/language-unsupervised/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/better-language-models/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022-04">April 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A study of nonautoregressive model for sequence generation</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.15</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.15" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="149" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Non-autoregressive machine translation with latent alignments</title>
		<author>
			<persName><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1098" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gradient-guided unsupervised lexically constrained text generation</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.701</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.701" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="8692" to="8703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v37/sohl-dickstein15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">Jul 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="7" to="09" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=PxTIG12RRHS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">? Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax" />
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">FUDGE: Controlled text generation with future discriminators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.276</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.276" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="3511" to="3535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.01068" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
