<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Field-aware Factorization Machines for CTR Prediction</title>
				<funder ref="#_5pKbz6n">
					<orgName type="full">MOE of Taiwan</orgName>
				</funder>
				<funder ref="#_D2H5VhY #_fYUKYpU">
					<orgName type="full">MOST</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
							<email>yc.juan@criteo.com</email>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
							<email>yong.zhuang22@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
							<email>cjlin@csie.ntu.edu.tw</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Criteo Research</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of ECE *</orgName>
								<orgName type="institution">Carnegie Mellon Univ</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">National Taiwan Univ</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">National Taiwan Univ</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Field-aware Factorization Machines for CTR Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2959100.2959134</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine learning</term>
					<term>Click-through rate prediction</term>
					<term>Computational advertising</term>
					<term>Factorization machines</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-through rate (CTR) prediction plays an important role in computational advertising. Models based on degree-2 polynomial mappings and factorization machines (FMs) are widely used for this task. Recently, a variant of FMs, fieldaware factorization machines (FFMs), outperforms existing models in some world-wide CTR-prediction competitions. Based on our experiences in winning two of them, in this paper we establish FFMs as an effective method for classifying large sparse data including those from CTR prediction. First, we propose efficient implementations for training FFMs. Then we comprehensively analyze FFMs and compare this approach with competing models. Experiments show that FFMs are very useful for certain classification problems. Finally, we have released a package of FFMs for public use.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Click-through rate (CTR) prediction plays an important role in advertising industry <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b3">3]</ref>. Logistic regression is probably the most widely used model for this task <ref type="bibr" target="#b3">[3]</ref>. Given a data set with m instances (yi, xi), i = 1, . . . , m, where yi is the label and xi is an n-dimensional feature vector, the model w is obtained by solving the following optimization problem. In problem <ref type="bibr" target="#b1">(1)</ref>, ? is the regularization parameter, and in the loss function we consider the linear model:</p><note type="other">min</note><formula xml:id="formula_0">?LM(w, x) = w ? x.</formula><p>Learning the effect of feature conjunctions seems to be crucial for CTR prediction; see, for example, <ref type="bibr" target="#b1">[1]</ref>. Here, we consider an artificial data set in Table <ref type="table" target="#tab_0">1</ref> to have a better understanding of feature conjunctions. An ad from Gucci has a particularly high CTR on Vogue. This information is however difficult for linear models to learn because they learn the two weights Gucci and Vogue separately. To address this problem, two models have been used to learn the effect of feature conjunction. The first model, degree-2 polynomial mappings (Poly2) <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref>, learns a dedicate weight for each feature conjunction. The second model, factorization machines (FMs) <ref type="bibr">[6]</ref>, learns the effect of feature conjunction by factorizing it into a product of two latent vectors. We will discuss details about Poly2 and FMs in Section 2.</p><p>A variant of FM called pairwise interaction tensor factorization (PITF) <ref type="bibr" target="#b7">[7]</ref> was proposed for personalized tag recommendation. In KDD Cup 2012, a generalization of PITF called "factor model" was proposed by "Team Opera Solutions" <ref type="bibr" target="#b8">[8]</ref>. Because this term is too general and may easily be confused with factorization machines, we refer to it as "field-aware factorization machines" (FFMs) in this paper. The difference between PITF and FFM is that PITF considers three special fields including "user,""item," and "tag," while FFM is more general. Because <ref type="bibr" target="#b8">[8]</ref> is about the overall solution for the competition, its discussion of FFM is limited. We can conclude the following results in <ref type="bibr" target="#b8">[8]</ref>:</p><p>? Though FFM is shown to be effective in <ref type="bibr" target="#b8">[8]</ref>, this work may be the only published study of applying FFMs on CTR prediction problems. To further demonstrate the effectiveness of FFMs on CTR prediction, we present the use of FFM as our major model to win two world-wide CTR competitions hosted by Criteo and Avazu. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">POLY2 AND FM</head><p>Chang et. al <ref type="bibr" target="#b4">[4]</ref> have shown that a degree-2 polynomial mapping can often effectively capture the information of feature conjunctions. Further, they show that by applying a linear model on the explicit form of degree-2 mappings, the training and test time can be much faster than using kernel methods. This approach, referred to as Poly2, learns a weight for each feature pair:</p><formula xml:id="formula_1">? Poly2 (w, x) = n j 1 =1 n j 2 =j 1 +1 w h(j 1 ,j 2 ) xj 1 xj 2 ,<label>(2)</label></formula><p>where h(j1, j2) is a function encoding j1 and j2 into a natural number. The complexity of computing (2) is O(n 2 ), where n is the average number of non-zero elements per instance. FMs proposed in <ref type="bibr">[6]</ref> implicitly learn a latent vector for each feature. Each latent vector contains k latent factors, where k is a user-specified parameter. Then, the effect of feature conjunction is modelled by the inner product of two latent vectors:</p><formula xml:id="formula_2">?FM(w, x) = n j 1 =1 n j 2 =j 1 +1 (wj 1 ? wj 2 )xj 1 xj 2 .</formula><p>(</p><formula xml:id="formula_3">)<label>3</label></formula><p>The number of variables is n ? k, so directly computing (3) costs O(n 2 k) time. Following <ref type="bibr">[6]</ref>, by re-writing (3) to</p><formula xml:id="formula_4">?FM(w, x) = 1 2 n j=1 (s -wjxj) ? wjxj, where s = n j =1 w j x j ,</formula><p>the complexity is reduced to O(nk).</p><p>Rendle <ref type="bibr">[6]</ref> explains why FMs can be better than Poly2 when the data set is sparse. Here we give a similar illustration using the data set in Table <ref type="table" target="#tab_0">1</ref>. For example, there is only one negative training data for the pair (ESPN, Adidas). For Poly2, a very negative weight w ESPN,Adidas might be learned for this pair. For FMs, because the prediction of (ESPN, Adidas) is determined by wESPN ? w Adidas , and because wESPN and w Adidas are also learned from other pairs (e.g., (ESPN, Nike), (NBC, Adidas)), the prediction may be more accurate. Another example is that there is no training data for the pair (NBC, Gucci). For Poly2, the prediction on this pair is trivial, but for FMs, because wNBC and wGucci can be learned from other pairs, it is still possible to do meaningful prediction.</p><p>Note that in Poly2, the naive way to implement h(j1, j2) is to consider every pair of features as a new feature <ref type="bibr" target="#b4">[4]</ref>. 1  This approach requires the model as large as O(n 2 ), which is usually impractical for CTR prediction because of very large n. Vowpal Wabbit (VW) <ref type="bibr" target="#b9">[9]</ref>, a widely used machine learning package, solves this problem by hashing j1 and j2. 2 Our implementation is similar to VW's approach. Specifically,</p><formula xml:id="formula_5">h(j1, j2) = ( 1 2 (j1 + j2)(j1 + j2 + 1) + j2) mod B,</formula><p>where the model size B is a user-specified parameter.</p><p>In this paper, for the simplicity of formulations, we do not include linear terms and bias term. However, in Section 4, we include them for some experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">FFM</head><p>The idea of FFM originates from PITF <ref type="bibr" target="#b7">[7]</ref> proposed for recommender systems with personalized tags. In PITF, they assume three available fields including User, Item, and Tag, and factorize (User, Item), (User, Tag), and (Item, Tag) in separate latent spaces. In <ref type="bibr" target="#b8">[8]</ref>, they generalize PITF for more fields (e.g., AdID, AdvertiserID, UserID, QueryID) and effectively apply it on CTR prediction. Because <ref type="bibr" target="#b7">[7]</ref> aims at recommender systems and is limited to three specific fields (User, Item, and Tag), and <ref type="bibr" target="#b8">[8]</ref> lacks detailed discussion on FFM, in this section we provide a more comprehensive study of FFMs on CTR prediction. For most CTR data sets like that in Table <ref type="table" target="#tab_0">1</ref>, "features" can be grouped into "fields." In our example, three features ESPN, Vogue, and NBC, belong to the field Publisher, and the other three features Nike, Gucci, and Adidas, belong to the field Advertiser. FFM is a variant of FM that utilizes this information. To explain how FFM works, we consider the following new example: In FMs, every feature has only one latent vector to learn the latent effect with any other features. Take ESPN as an example, wESPN is used to learn the latent effect with Nike (wESPN ? w Nike ) and Male (wESPN ? w Male ). However, because Nike and Male belong to different fields, the latent effects of (EPSN, Nike) and (EPSN, Male) may be different.</p><p>In FFMs, each feature has several latent vectors. Depending on the field of other features, one of them is used to do the inner product. In our example, ?FFM(w, x) is</p><formula xml:id="formula_6">wESPN,A ? w Nike,P + wESPN,G ? w Male,P + w Nike,G ? w Male,A .</formula><p>We see that to learn the latent effect of (ESPN, NIKE), wESPN,A is used because Nike belongs to the field Advertiser, and w Nike,P is used because ESPN belongs to the field Publisher. Again, to learn the latent effect of (EPSN, Male), wESPN,G is used because Male belongs to the field Gender, and w Male,P is used because ESPN belongs to the field Publisher. Mathematically,</p><formula xml:id="formula_7">?FFM(w, x) = n j 1 =1 n j 2 =j 1 +1 (w j 1 ,f 2 ? w j 2 ,f 1 )xj 1 xj 2 ,<label>(4)</label></formula><p>where f1 and f2 are respectively the fields of j1 and j2. If f is the number of fields, then the number of variables of FFMs is nf k, and the complexity to compute (4) is O(n 2 k).</p><p>It is worth noting that in FFMs because each latent vector only needs to learn the effect with a specific field, usually kFFM kFM.</p><p>Table <ref type="table" target="#tab_2">2</ref> compares the number of variables and the computational complexity of different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Solving the Optimization Problem</head><p>The optimization problem is the same as (1) except that ?LM(w, x) is replaced by ?FFM(w, x). Following <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8]</ref>, we use stochastic gradient methods (SG). Recently, some adaptive learning-rate schedules such as <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref> have been proposed to boost the training process of SG. We use AdaGrad <ref type="bibr" target="#b10">[10]</ref> because <ref type="bibr" target="#b12">[12]</ref> has shown its effectiveness on matrix factorization, which is a special case of FFMs.</p><p>At each step of SG a data point (y, x) is sampled for updating w j 1 ,f 2 and w j 2 ,f 1 in (4). Note that because x is highly sparse in our application, we only update dimensions with non-zero values. First, the sub-gradients are</p><formula xml:id="formula_8">g j 1 ,f 2 ? ?w j 1 ,f 2 f (w) = ? ? w j 1 ,f 2 + ? ? w j 2 ,f 1 xj 1 xj 2 , (5) g j 2 ,f 1 ? ?w j 2 ,f 1 f (w) = ? ? w j 2 ,f 1 + ? ? w j 1 ,f 2 xj 1 xj 2 ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">? = ?log(1 + exp(-y?FFM(w, x))) ??FFM(w, x) = -y 1 + exp(y?FFM(w, x))</formula><p>.</p><p>Algorithm 1 Training FFM using SG 1: Let G ? R n?f ?k be a tensor of all ones 2: Run the following loop for t epochs 3:</p><formula xml:id="formula_10">for i ? {1, ? ? ? , m} do 4: Sample a data point (y, x) 5: caclulate ? 6: for j1 ? non-zero terms in {1, ? ? ? , n} do 7: for j2 ? non-zero terms in {j1 + 1, ? ? ? , n} do 8:</formula><p>calculate sub-gradient by ( <ref type="formula">5</ref>) and (6) 9:</p><formula xml:id="formula_11">for d ? {1, ? ? ? , k} do 10:</formula><p>Update the gradient sum by ( <ref type="formula">7</ref>) and ( <ref type="formula">8</ref>) 11:</p><p>Update model by ( <ref type="formula">9</ref>) and ( <ref type="formula" target="#formula_13">10</ref>)</p><p>Second, for each coordinate d = 1, . . . , k, the sum of squared gradient is accumulated:</p><formula xml:id="formula_12">(G j 1 ,f 2 ) d ? (G j 1 ,f 2 ) d + (g j 1 ,f 2 ) 2 d (7) (G j 2 ,f 1 ) d ? (G j 2 ,f 1 ) d + (g j 2 ,f 1 ) 2 d (8)</formula><p>Finally, (w j 1 ,f 2 ) d and (w j 2 ,f 1 ) d are updated by:</p><formula xml:id="formula_13">(w j 1 ,f 2 ) d ? (w j 1 ,f 2 ) d - ? (G j 1 ,f 2 ) d (g j 1 ,f 2 ) d (9) (w j 2 ,f 1 ) d ? (w j 2 ,f 1 ) d - ? (G j 2 ,f 1 ) d (g j 2 ,f 1 ) d , (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where ? is a user-specified learning rate. The initial values of w are randomly sampled from a uniform distribution be-</p><formula xml:id="formula_15">tween [0, 1/ ? k].</formula><p>The initial values of G are set to one in order to prevent a large value of (G</p><formula xml:id="formula_16">j 1 ,f 2 ) -1 2 d . The overall procedure is presented in Algorithm 1.</formula><p>Empirically, we find that normalizing each instance to have the unit length makes the test accuracy slightly better and insensitive to parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parallelization on Shared-memory Systems</head><p>Modern computers are widely equipped with multi-core CPUs. If these cores are fully utilized, the training time can be significantly reduced. Many parallelization approaches for SG have been proposed. In this paper, we apply Hogwild! <ref type="bibr" target="#b13">[13]</ref>, which allows each thread to run independently without any locking. Specifically, the for loop at line 3 of Algorithm 1 is parallelized.</p><p>In Section 4.4 we run extensive experiments to investigate the effectiveness of parallelization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adding Field Information</head><p>Consider the widely used LIBSVM data format:</p><formula xml:id="formula_17">label feat1:val1 feat2:val2 ? ? ? ,</formula><p>where each (feat, val) pair indicates feature index and value. For FFMs, we extend the above format to label field1:feat1:val1 field2:feat2:val2 ? ? ? That is, we must assign the corresponding field to each feature. The assignment is easy on some kinds of features, but may not be possible for some others. We discuss this issue on three typical classes of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categorical Features</head><p>For linear models, a categorical feature is commonly transformed to several binary features. For a data instance Yes P:ESPN A:Nike G:Male, we generate the following LIBSVM format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yes P-ESPN:1 A-Nike:1 G-Male:1</head><p>Note that according to the number of possible values in a categorical feature, the same number of binary features are generated and every time only one of them has the value 1. In the LIBSVM format, features with zero values are not stored. We apply the same setting to all models, so in this paper, every categorical feature is transformed to several binary ones. To add the field information, we can consider each category as a field. Then the above instance becomes Yes P:P-ESPN:1 A:A-Nike:1 G:G-Male:1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Numerical Features</head><p>Consider the following example to predict if a paper will be accepted by a conference. We use three numerical features "accept rate of the conference (AR),""h-index of the author (Hidx)," and "number of citations of the author (Cite):" There are two possible ways to assign fields. A naive way is to treat each feature as a dummy field, so the generated data is:</p><formula xml:id="formula_18">Accepted</formula><p>Yes AR:AR:45.73 Hidx:Hidx:2 Cite:Cite:3</p><p>However, the dummy fields may not be informative because they are merely duplicates of features. Another possible way is to discretize each numerical feature to a categorical one. Then, we can use the same setting for categorical features to add field information. The generated data looks like:</p><p>Yes AR:45:1 Hidx:2:1 Cite:3:1, where the AR feature is rounded to an integer. The main drawback is that usually it is not easy to determine the best discretization setting. For example, we may transform 45.73 to "45.7," "45," "40," or even "int(log(45.73))." In addition, we may lose some information after discretization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-field Features</head><p>On some data sets, all features belong to a single field and hence it is meaningless to assign fields to features. Typically this situation happens on NLP data sets. Consider the following example of predicting if a sentence expresses a good mood or not: good mood sentence Yes Hooray! Our paper is accepted! No Well, our paper is rejected..</p><p>In this example the only field is "sentence." If we assign this field to all words, then FFMs is reduced to FMs. Readers may ask about assigning dummy fields as we do for numerical features. Recall that the model size of FFMs is O(nf k). The use of dummy fields is impractical because f = n and n is often huge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>In this section, we first provide the details about the experimental setting in Section 4.1. Then, we investigate the impact of parameters. We find that unlike LM or Poly2, FFM is sensitive to the number of epochs. Therefore, in Section 4.3, we discuss this issue in detail before proposing an early stopping trick. The speedup of parallelization is studied in Section 4.4.</p><p>After checking various properties of FFMs, in Sections 4.5-4.6, we compare FFMs with other models including Poly2 and FMs. They are all implemented by the same SG method, so besides accuracy we can fairly compare their training time. Further in the comparison we include state-of-theart packages LIBLINEAR <ref type="bibr" target="#b14">[14]</ref> and LIBFM <ref type="bibr" target="#b15">[15]</ref> for training LM/Poly2 and FMs, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Sets</head><p>We mainly consider two CTR sets Criteo and Avazu from Kaggle competitions,<ref type="foot" target="#foot_0">3</ref> though in Section 4.6 more sets are considered. For feature engineering, we mainly apply our winning solution but remove complicated components. <ref type="foot" target="#foot_1">4</ref> For example, our winning solution for Avazu includes the ensemble of 20 models, but here we only use the simplest one. For other details please check our experimental code. A hashing trick is applied to generate 10 For both data sets, the labels in the test sets are not publicly available, so we split the available data to two sets for training and validation. The data split follows from how test sets are obtained: For Criteo, the last 6,040,618 lines are used as the validation set; for Avazu, we select the last 4,218,938 lines. We use the following terms to represent different sets of a problem. ? Te: The original test set. The labels are not released, so we must submit our prediction to the original evaluation systems to get the score. To avoid over-fitting the test set, the competition organizers divide this data set to two subsets "public set" on which the score is visible during the competition and "private set" on which the score is available after the end of competition. The final rank is determined by the private set.</p><p>For example, CriteoVa means the validation set from Criteo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Platform</head><p>All experiments are conducted on a Linux workstation with 12 physical cores on two Intel Xeon E5-2620 2.0GHz processors and 128 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>Depending on the model, we change ?(w, x) in (1) to ?LM(w, x), ? Poly2 (w, x), ?FM(w, x), or ?FFM(w, x) introduced in Sections 1-3. For the evaluation criterion, we consider the logistic loss defined as</p><formula xml:id="formula_19">logloss = 1 m m i=1 log(1 + exp(-yi?(w, xi))),</formula><p>where m is the number of test instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation</head><p>We implement LMs, Poly2, FMs, and FFMs all in C++. For FMs and FFMs, we use SSE instructions to boost the efficiency of inner products. The parallelization discussed in Section 3.2 is implemented by OpenMP <ref type="bibr" target="#b16">[16]</ref>. Our implementations include linear terms and bias term as they improve performance in some data sets. These terms should be used in general as we seldom see them to be harmful. Note that for code extensibility the field information is stored regardless of the model used. For non-FFM models, the implementation may become slightly faster by a simpler data structure without field information, but our conclusions from experiments should remain the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Impact of Parameters</head><p>We conduct experiments to investigate the impact of k, ?, and ?. The results can be found in Figure <ref type="figure" target="#fig_1">1</ref>. Regarding the parameter k, results in Figure <ref type="figure" target="#fig_1">1a</ref> show that it does not affect the logloss much. In Figure <ref type="figure" target="#fig_1">1b</ref>, we present the relationship between ? and logloss. If ? is too large, the model is not able to achieve a good performance. On the contrary, with a small ?, the model gets better results, but it easily overfits the data. We observe that the training logloss keeps decreasing. For the parameter ?, Figure <ref type="figure" target="#fig_1">1c</ref> shows that if we apply a small ?, FFMs will obtain its best performance slowly. However, with a large ?, FFMs are able to quickly reduce the logloss, but then over-fitting occurs. From the results in Figures <ref type="figure" target="#fig_1">1b</ref> and<ref type="figure" target="#fig_1">1c</ref>, there is a need of early-stopping that will be discussed in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Early Stopping</head><p>Early stopping, which terminates the training process before reaching the best result on training data, can be used to avoid over-fitting for many machine learning problems <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref>. For FFM, the strategy we use is:  such as lazy update <ref type="foot" target="#foot_2">5</ref> and ALS-based optimization methods. However, results are not as successful as that by early stopping of using a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Speedup</head><p>Because the parallelization of SG may cause a different convergence behavior, we experiment with different numbers of threads in Figure <ref type="figure">2</ref>. Results show that our parallelization still leads to similar convergence behavior. With this property we can define the speedup as:</p><p>Running time of one epoch with a single thread Running time of one epoch with multiple threads .</p><p>The result in Figure <ref type="figure">3</ref> shows a good speedup when the number of threads is small. However, if many threads are used, the speedup does not improve much. An explanation is that if two or more threads attempt to access the same memory address, one must wait for its term. This kind of conflicts can happen more often when more threads are used.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with LMs, Poly2, and FMs on Two CTR Competition Data Sets</head><p>To have a fair comparison, we implement the same SG method for LMs, Poly2, FMs, and FFMs. Further, we compare with two state-of-the-art packages:</p><p>? LIBLINEAR: a widely used package for linear models. For L2-regularized logistic regression, it implements two optimization methods: Newton method to solve the primal problem, and coordinate descent (CD) method to solve the dual problem. We used both for checking how optimization methods affect the performance; see the discussion in the end of this sub-section. Further, the existing Poly2 extension of LIBLINEAR does not support the hashing trick,<ref type="foot" target="#foot_3">6</ref> so we conduct suitable modifications and denote it as LIBLINEAR-Hash in this paper. ? LIBFM: As a widely used library for factorization machines, it supports three optimization approaches including stochastic gradient method (SG), alternating least squares (ALS), and Markov Chain Monte Carlo (MCMC). We tried all of them and found that ALS is significantly better than the other two in terms of logloss. Therefore, we consider ALS in our experiments.</p><p>For the parameters in all models, from a grid of points we select those that lead to the best performance on the validation sets. Every optimization algorithm needs a stopping condition; we use the default setting for Newton method and coordinate descent (CD) method by LIBLINEAR. For each of the other models, we need a validation set to check which iteration leads to the best validation score. After we obtain the best number of iterations, we re-train the model up to that iteration. Results on Criteo and Avazu with the list of parameters used can be found in Table <ref type="table" target="#tab_7">3</ref>. Clearly, FFMs outperform other models in terms of logloss, but it also requires longer training time than LMs and FMs. On the other end, though the logloss of LMs is worse than other models, it is significantly faster. These results show a clear trade-off between logloss and speed. Poly2 is the slowest among all models. The reason might be the expensive computation of (2). FM is a good balance between logloss and speed.</p><p>For LIBFM, it performs closely to our implementation of FMs in terms of logloss on Criteo. <ref type="foot" target="#foot_4">7</ref> However, we see that our implementation is significantly faster. We provide three possible reasons:</p><p>? The ALS algorithm used by LIBFM is more complicated than the SG algorithm we use. ? We use an adaptive learning rate strategy in SG.</p><p>? We use SSE instructions to boost inner product operations.</p><p>Because logistic regression is a convex problem, ideally, for either LM or Poly2, the three optimization methods (SG, Newton, and CD) should generate exactly the same model if they converge to the global optimum. However, practically results are slightly different. In particular, LM by SG is better than the two LIBLINEAR-based models on Avazu. In our implementation, LM via SG only loosely solves the optimization problem. Our experiments therefore indicate that the stopping condition of optimization methods can affect the performance of the resulting model even if the problem is convex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison on More Data Sets</head><p>In the previous section we focus on two competition data sets, but it is important to see how FFMs perform on other data sets. To answer this question, we consider more data sets for the comparison, where most of them are not CTR data. Note that following the discussion in Section 3.3, we do not consider data sets with single-field features. The reason is that depending on how we assign fields, FFMs either become equivalent to FMs, or generate a huge model.</p><p>Here we briefly introduce the data sets used.</p><p>? KDD2010-bridge:  For all experiments, a single thread is used. The public set is around 20% of the test data, while the private set contains the rest. For Criteo, we do not list the result of Poly2-LIBLINEAR-Hash-Newton, because the experiment does not finish after more than 10 days. Note that the we use different stopping conditions for different algorithms, so the training time is only for reference.</p><p>? phishing: 11 This set contains only categorical features.</p><p>? adult: 12 This data set includes both numerical and categorical features.</p><p>For KDD2010-bridge, KDD2012, and adult, we simply discretize all numerical features into 29, 13, and 94 bins respectively. For cod-rna and ijcnn, where features are all numerical, we try both approaches mentioned in Section 3.3 to obtain field information: applying dummy fields and discretization.</p><p>For the parameter selection, we follow the same procedure in Section 4.5. We split each set into training, validation, and test sets; then for predicting the test set, we use the model trained with parameters that achieve the best logloss on the validation set.</p><p>The statistics and experimental results of each data set are described in Table <ref type="table" target="#tab_8">4</ref>. FFMs significantly outperform other models on KDD2010-bridge and KDD2012. The common characteristic among these data sets are:</p><p>? Most features are categorical. 11 http://archive.ics.uci.edu/ml/datasets/Phishing+ Websites 12 http://archive.ics.uci.edu/ml/datasets/Adult</p><p>? The resulting set is highly sparse after transforming categorical features into many binary features.</p><p>However, on phishing and adult, FFM is not significantly better. For phishing, the reason might be that the data is not sparse so FFM, FM, and Poly2 have close performance; for adult, it seems feature conjunction is not useful because all models perform similarly to the linear model. When a data set contains only numerical features, FFMs may not have an obvious advantage. If we use dummy fields, then FFMs do not out-perform FMs, a result indicating that the field information is not helpful. On the other hand, if we discretize numerical features, though FFMs is the best among all models, the performance is much worse than that of using dummy fields. We summarize a guideline of applying FFMs on different kinds of data sets:</p><p>? FFMs should be effective for data sets that contain categorical features and are transformed to binary features. ? If the transformed set is not sparse enough, FFMs seem to bring less benefit. ? It is more difficult to apply FFMs on numerical data sets. In this paper we discuss efficient implementations of FFMs. We demonstrate that for certain kinds of data sets, FFMs outperform three well-known models, LM, Poly2, and FM, in terms of logloss, with a cost of longer training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS AND FUTURE WORKS</head><p>For the future work, the over-fitting problem discussed in Section 4.3 is an issue that we plan to investigate. Besides, for the ease of implementation we use SG as the optimization method. It is interesting to see how other optimization methods (e.g., Newton method) work on FFMs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>Va: The validation set mentioned above. ? Tr: The new training set after excluding the validation set from the original training data. ? TrVa: The original training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The impact of ?, ?, and k on FFMs. To make experiments faster, we randomly select 10% instances from CriteoTr and CriteoVa as the training and the test sets, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Epochs</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The convergence of using different number of threads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>w</cell><cell>? 2</cell><cell>w 2 2 +</cell><cell>m i=1</cell><cell>log(1 + exp(-yi?LM(w, xi))). (1)</cell></row></table><note><p>An artificial CTR data set, where + (-) represents the number of clicked (unclicked) impressions.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>? We compare FFMs with two related models, Poly2 and FMs. We first discuss conceptually why FFMs might be better than them, and conduct experiments to see the difference in terms of accuracy and training time. ? We present techniques for training FFMs. They include an effective parallel optimization algorithm for FFMs and the use of early-stopping to avoid over-fitting. ? To make FFMs available for public use, we release an open source software.This paper is organized as follows. Before we present FFMs and its implementation in Section 3, we discuss the two existing models Poly2 and FMs in Section 2. Experiments comparing FFMs with other models are in Section 4. Finally, conclusions and future directions are in Section 5.Code used for experiments in this paper and the package LIBFFM are respectively available at:</figDesc><table /><note><p>http://www.csie.ntu.edu.tw/?cjlin/ffm/exps http://www.csie.ntu.edu.tw/?cjlin/libffm</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Nike + wESPN ? w Male + w Nike ? w Male . Comparison of the number of variables and the complexity for prediction among LM, Poly2, FM, and FFM.</figDesc><table><row><cell cols="4">Clicked Publisher (P) Advertiser (A) Gender (G)</cell></row><row><cell>Yes</cell><cell>ESPN</cell><cell>Nike</cell><cell>Male</cell></row><row><cell cols="3">Recall that for FMs, ?FM(w, x) is</cell><cell></cell></row><row><cell cols="4">wESPN ? w 1 More precisely, [4] includes the original features as well,</cell></row><row><cell cols="4">though we do not consider such a setting until the experi-</cell></row><row><cell>ments.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 See</cell><cell cols="3">http://github.com/JohnLangford/vowpal wabbit/</cell></row><row><cell cols="3">wiki/Feature-interactions for details.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>6 features. The statistics of the two data sets are:</figDesc><table><row><cell cols="4">Data Set # instances # features # fields</cell></row><row><cell>Criteo</cell><cell>45,840,617</cell><cell>10 7</cell><cell>39</cell></row><row><cell>Avazu</cell><cell>40,428,967</cell><cell>10 7</cell><cell>33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>8</head><label></label><figDesc>This data set includes both numerical and categorical features.? KDD2012: 9 This set contains both numerical and categorical features. Because our evaluation is logloss, we transform the original target value "number of clicks" to a binary value "clicked or not." ? cod-rna: 10 This set contains only numerical features.? ijcnn: 10 This set contains only numerical features.</figDesc><table><row><cell>Model and implementation</cell><cell>parameters</cell><cell>training time (seconds)</cell><cell cols="2">public set logloss rank</cell><cell>private set logloss rank</cell></row><row><cell>LM-SG</cell><cell>? = 0.2, ? = 0, t = 13</cell><cell cols="2">527 0.46262</cell><cell cols="2">93 0.46224</cell><cell>91</cell></row><row><cell>LM-LIBLINEAR-CD</cell><cell>s = 7, c = 2</cell><cell cols="2">1,417 0.46239</cell><cell cols="2">91 0.46201</cell><cell>89</cell></row><row><cell>LM-LIBLINEAR-Newton</cell><cell>s = 0, c = 2</cell><cell cols="2">7,164 0.46602</cell><cell cols="2">225 0.46581</cell><cell>222</cell></row><row><cell>Poly2-SG</cell><cell>? = 0.2, ? = 0, B = 10 7 , t = 10</cell><cell cols="2">12,064 0.44973</cell><cell cols="2">14 0.44956</cell><cell>14</cell></row><row><cell cols="2">Poly2-LIBLINEAR-Hash-CD s = 7, c = 2</cell><cell cols="2">24,771 0.44893</cell><cell cols="2">13 0.44873</cell><cell>13</cell></row><row><cell>FM</cell><cell>? = 0.05, ? = 2 ? 10 -5 , k = 40, t = 8</cell><cell cols="2">2,022 0.44930</cell><cell cols="2">14 0.44922</cell><cell>14</cell></row><row><cell>FM</cell><cell>? = 0.05, ? = 2 ? 10 -5 , k = 100, t = 9</cell><cell cols="2">4,020 0.44867</cell><cell cols="2">11 0.44847</cell><cell>11</cell></row><row><cell>LIBFM</cell><cell>? = 40, k = 40, t = 20</cell><cell cols="2">23,700 0.45012</cell><cell cols="2">14 0.45000</cell><cell>15</cell></row><row><cell>LIBFM</cell><cell>? = 40, k = 40, t = 50</cell><cell cols="2">131,000 0.44904</cell><cell cols="2">14 0.44887</cell><cell>14</cell></row><row><cell>LIBFM</cell><cell>? = 40, k = 100, t = 20</cell><cell cols="2">54,320 0.44853</cell><cell cols="2">11 0.44834</cell><cell>11</cell></row><row><cell>LIBFM</cell><cell>? = 40, k = 100, t = 50</cell><cell cols="2">398,800 0.44794</cell><cell cols="2">9 0.44778</cell><cell>8</cell></row><row><cell>FFM</cell><cell>? = 0.2, ? = 2 ? 10 -5 , k = 4, t = 9</cell><cell cols="2">6,587 0.44612</cell><cell cols="2">3 0.44603</cell><cell>3</cell></row><row><cell></cell><cell>(a) Criteo</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model and implementation</cell><cell>parameters</cell><cell>training time (seconds)</cell><cell cols="2">public set logloss rank</cell><cell>private set logloss rank</cell></row><row><cell>LM-SG</cell><cell>? = 0.2, ? = 0, t = 10</cell><cell cols="2">164 0.39018</cell><cell cols="2">57 0.38833</cell><cell>64</cell></row><row><cell>LM-LIBLINEAR-CD</cell><cell>s = 7, c = 1</cell><cell cols="2">417 0.39131</cell><cell cols="2">115 0.38944</cell><cell>119</cell></row><row><cell>LM-LIBLINEAR-Newton</cell><cell>s = 0, c = 1</cell><cell cols="2">650 0.39269</cell><cell cols="2">182 0.39079</cell><cell>183</cell></row><row><cell>Poly2-SG</cell><cell>? = 0.2, ? = 0, B = 10 7 , t = 10</cell><cell cols="2">911 0.38554</cell><cell cols="2">10 0.38347</cell><cell>10</cell></row><row><cell>Poly2-LIBLINEAR-Hash-CD</cell><cell>s = 7, c = 1</cell><cell cols="2">1,756 0.38516</cell><cell cols="2">10 0.38303</cell><cell>9</cell></row><row><cell cols="2">Poly2-LIBLINEAR-Hash-Newton s = 0, c = 1</cell><cell cols="2">27,292 0.38598</cell><cell cols="2">11 0.38393</cell><cell>11</cell></row><row><cell>FM</cell><cell>? = 0.05, ? = 2 ? 10 -5 , k = 40, t = 8</cell><cell cols="2">574 0.38621</cell><cell cols="2">11 0.38407</cell><cell>11</cell></row><row><cell>FM</cell><cell>? = 0.05, ? = 2 ? 10 -5 , k = 100, t = 9</cell><cell cols="2">1,277 0.38740</cell><cell cols="2">17 0.38531</cell><cell>15</cell></row><row><cell>LIBFM</cell><cell>? = 40, k = 40, t = 20</cell><cell cols="2">18,712 0.39137</cell><cell cols="2">122 0.38963</cell><cell>127</cell></row><row><cell>LIBFM</cell><cell>? = 40, k = 40, t = 50</cell><cell cols="2">41,720 0.39786</cell><cell cols="2">935 0.39635</cell><cell>943</cell></row><row><cell>LIBFM</cell><cell>? = 40, k = 100, t = 20</cell><cell cols="2">39,719 0.39644</cell><cell cols="2">747 0.39470</cell><cell>755</cell></row><row><cell>LIBFM</cell><cell>? = 40, k = 100, t = 50</cell><cell cols="4">91,210 0.40740 1,129 0.40585 1,126</cell></row><row><cell>FFM</cell><cell>? = 0.2, ? = 2 ? 10 -5 , k = 4, t = 4</cell><cell cols="2">340 0.38411</cell><cell cols="2">6 0.38223</cell><cell>6</cell></row></table><note><p>(b) Avazu</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Comparison among models and implementations on data sets Criteo and Avazu. The training sets used here are CriteoTrVa and AvazuTrVa, and the test sets used here are CriteoTe and AvazuTe.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Comparison between LM, Poly2, FM, and FFMs. The best logloss is underlined.</figDesc><table><row><cell></cell><cell></cell><cell>statistics</cell><cell></cell><cell></cell><cell>logloss</cell></row><row><cell>Data set</cell><cell cols="3"># instances # features # fields</cell><cell>LM</cell><cell>Poly2</cell><cell>FM</cell><cell>FFM</cell></row><row><cell>KDD2010-bridge</cell><cell>20,012,499</cell><cell>651,166</cell><cell cols="2">9 0.27947</cell><cell cols="2">0.2622 0.26372 0.25639</cell></row><row><cell>KDD2012</cell><cell cols="2">149,639,105 54,686,452</cell><cell cols="4">11 0.15069 0.15099 0.15004 0.14906</cell></row><row><cell>phishing</cell><cell>11,055</cell><cell>100</cell><cell cols="4">30 0.14211 0.11512 0.09229</cell><cell>0.1065</cell></row><row><cell>adult</cell><cell>48,842</cell><cell>308</cell><cell>14</cell><cell cols="3">0.3097 0.30655 0.30763 0.30565</cell></row><row><cell>cod-rna (dummy fields)</cell><cell>331,152</cell><cell>8</cell><cell cols="4">8 0.13829 0.12874 0.12580 0.12914</cell></row><row><cell>cod-rna (discretization)</cell><cell>331,152</cell><cell>2,296</cell><cell cols="4">8 0.16455 0.17576 0.16570 0.14993</cell></row><row><cell>ijcnn (dummy fields)</cell><cell>141,691</cell><cell>22</cell><cell cols="4">22 0.20093 0.08981 0.07087</cell><cell>0.0692</cell></row><row><cell>ijcnn (discretization)</cell><cell>141,691</cell><cell>69,867</cell><cell cols="4">22 0.21588 0.24578 0.20223 0.18608</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Criteo Display Advertising Challenge: http://www. kaggle.com/c/criteo-display-ad-challenge. Avazu Click-Through Rate Prediction: http://www.kaggle.com/c/ avazu-ctr-prediction.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p><ref type="bibr" target="#b4">4</ref> The code and documents of our winning solution to the two competitions can be found in http://github. com/guestwalk/kaggle-2014-criteo and http://github.com/ guestwalk/kaggle-avazu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>http://blog.smola.org/post/943941371/ lazy-updates-for-generic-regularization-in-sgd</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>https://www.csie.ntu.edu.tw/?cjlin/libsvmtools/#fast training testing for polynomial mappings of data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>The performance of LIBFM on AvazuVa is as good as the FM we have implemented, but the performance on AvazuTe is poor. It is not entirely clear what happened, so further investigation is needed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>http://pslcdatashop.web.cmu.edu/KDDCup/downloads. jsp</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>http://www.kddcup2012.org/c/kddcup2012-track2/data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>http://www.csie.ntu.edu.tw/?cjlin/libsvmtools/datasets/ binary.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported in part by <rs type="funder">MOST</rs> via grants <rs type="grantNumber">104-2221-E-002-047-MY3</rs> and <rs type="grantNumber">104-2622-E-002-012-CC2</rs> and <rs type="funder">MOE of Taiwan</rs> via grant <rs type="grantNumber">105R7872</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_D2H5VhY">
					<idno type="grant-number">104-2221-E-002-047-MY3</idno>
				</org>
				<org type="funding" xml:id="_fYUKYpU">
					<idno type="grant-number">104-2622-E-002-012-CC2</idno>
				</org>
				<org type="funding" xml:id="_5pKbz6n">
					<idno type="grant-number">105R7872</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simple and scalable response prediction for display advertising</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Manavoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chikkerur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Hrafnkelsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kubica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting clicks: estimating the click-through rate for new ADs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training and testing low-degree polynomial data mappings via linear SVM</title>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ringgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1471" to="1490" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast methods for kernel-based text analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association of Computational Linguistics (ACL)</title>
		<meeting>the 41st Annual Meeting of the Association of Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pairwise interaction tensor factorization for personalized tag recommendation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd ACM International Conference on Web Search and Data Mining (WSDM)</title>
		<meeting>the 3rd ACM International Conference on Web Search and Data Mining (WSDM)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ensemble of collaborative filtering and feature engineered model for click through rate prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jahrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>T?scher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Spoelstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Cup 2012 Workshop</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Vowpal Wabbit</title>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<ptr target="https://github.com/JohnLangford/vowpalwabbit/wiki" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A learning-rate schedule for stochastic gradient methods to matrix factorization</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)</title>
		<meeting>the Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HOGWILD!: a lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LIBLINEAR: a library for large linear classification</title>
		<author>
			<persName><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Factorization machines with libFM</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">OpenMP: an industry standard API for shared-memory programming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Menon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Early stopping and non-parametric regression: An optimal data-dependent stopping rule</title>
		<author>
			<persName><forename type="first">G</forename><surname>Raskutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="335" to="366" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Boosting with early stopping: convergence and consistency</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1538" to="1579" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
