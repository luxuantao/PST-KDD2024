<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PipeDream: Generalized Pipeline Parallelism for DNN Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ‡ Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ‡ Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ‡ Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ‡ Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nikhil</forename><forename type="middle">R</forename><surname>Devanur</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ‡ Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ‡ Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ‡ Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University ‡ Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PipeDream: Generalized Pipeline Parallelism for DNN Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3341301.3359646</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>DNN training is extremely time-consuming, necessitating efficient multi-accelerator parallelization. Current approaches to parallelizing training primarily use intra-batch parallelization, where a single iteration of training is split over the available workers, but suffer from diminishing returns at higher worker counts. We present PipeDream, a system that adds inter-batch pipelining to intra-batch parallelism to further improve parallel training throughput, helping to better overlap computation with communication and reduce the amount of communication when possible. Unlike traditional pipelining, DNN training is bi-directional, where a forward pass through the computation graph is followed by a backward pass that uses state and intermediate data computed during the forward pass. Naïve pipelining can thus result in mismatches in state versions used in the forward and backward passes, or excessive pipeline flushes and lower hardware efficiency. To address these challenges, PipeDream versions model parameters for numerically correct gradient computations, and schedules forward and backward passes of different minibatches concurrently on different workers with minimal pipeline stalls. PipeDream also automatically partitions DNN layers among workers to balance work and minimize communication. Extensive experimentation with a range of DNN tasks, models, and hardware configurations shows that PipeDream trains models to high accuracy up to 5.3× faster than commonly used intra-batch parallelism techniques. * Work started as part of MSR internship. Equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep Neural Networks (DNNs) have facilitated tremendous progress across a range of applications, including image classification <ref type="bibr" target="#b19">[26,</ref><ref type="bibr" target="#b30">37,</ref><ref type="bibr" target="#b41">48]</ref>, translation <ref type="bibr" target="#b48">[55]</ref>, language modeling <ref type="bibr" target="#b33">[40]</ref>, and video captioning <ref type="bibr" target="#b47">[54]</ref>. As DNNs have become more widely deployed, they have also become more computationally expensive to train, thus requiring parallel execution across multiple accelerators (e.g., GPUs).</p><p>DNN training proceeds in iterations of forward and backward pass computations. In each iteration, the training loop processes a minibatch of input data and performs an update to the model parameters. Current approaches focus on parallelizing each iteration of the optimization algorithm across a set of workers. For example, data parallelism partitions the input data across workers <ref type="bibr" target="#b30">[37]</ref>, model parallelism partitions operators across workers <ref type="bibr" target="#b10">[16,</ref><ref type="bibr" target="#b15">21]</ref>, and hybrid schemes partition both <ref type="bibr" target="#b26">[33,</ref><ref type="bibr" target="#b27">34,</ref><ref type="bibr" target="#b29">36]</ref>. Unfortunately, intrabatch parallelization can suffer from high communication costs at large scale. For example, Figure <ref type="figure" target="#fig_1">1</ref> shows the communication overhead for data parallelism across five different DNN models on three different types of multi-GPU servers. Over 32 GPUs, the communication overhead for some models, computed as the percentage of total time spent on communication stalls, is as high as 90% due to expensive cross-server all_reduce communication. Communication overheads are high even on servers where GPUs within the server are connected by dedicated interconnects like NVLink <ref type="bibr" target="#b0">[4]</ref>. Moreover, rapid increases in GPU compute capacity over time will further shift the bottleneck of training towards communication for all models.</p><p>In this paper, we propose PipeDream, a system that uses pipeline parallelism to enable faster DNN training by combining intra-batch parallelism with inter-batch parallelization. PipeDream divides the model among available workers, assigning a group of consecutive operators (called layers in DNN terminology) in the operator graph to each of them, and then overlaps the computation and communication of different inputs in a pipelined fashion. This process can greatly reduce inter-worker communication because it limits the communication to layer inputs and outputs (activations in the forward pass and gradients in the backward pass) solely across consecutive layers assigned to different workers, which for many models are much smaller than the size of the entire model. Moreover, this communication is peer-to-peer, as opposed to all-to-all.</p><p>While pipelining is a simple idea, DNN training poses an important challenge not present in traditional pipelining: DNN training is bi-directional-the forward pass is followed by a backward pass through the same layers in reverse order, using state and intermediate results from the forward pass. To keep the pipeline full and thus achieve high hardware efficiency, a naïve scheduling mechanism might inject all minibatches in an epoch into the pipeline, first completing forward passes for all input minibatches followed by backward passes. However, this approach suffers from low statistical efficiency <ref type="bibr" target="#b12">[18]</ref>, increasing the number of passes through the dataset needed to produce a high-quality model. Furthermore, this strategy could prevent the model from reaching the desired target accuracy, since gradients are averaged over all training samples <ref type="bibr" target="#b4">[10,</ref><ref type="bibr" target="#b32">39]</ref>. To improve statistical efficiency, one could inject only a subset of m minibatches into the pipeline, and apply weight updates every m minibatches, as recently proposed by GPipe <ref type="bibr" target="#b21">[28]</ref>. However, this reduces hardware efficiency due to more frequent pipeline flushes. Traditional model parallel training corresponds to an extreme case of this (m = 1).</p><p>PipeDream takes a more nuanced approach to pipelining that outperforms other solutions -it achieves high hardware efficiency with no pipeline stalls in steady state, and high statistical efficiency  comparable to data parallelism using the same number of workers. Given a pipeline of groups of consecutive layers executed on different workers (called a stage), PipeDream uses a scheduling algorithm called 1F1B to keep hardware well utilized while achieving semantics similar to data parallelism. In 1F1B's steady state, each worker strictly alternates between forward and backward passes for its stage, ensuring high resource utilization (negligible pipeline stalls, no pipeline flushes) even in the common case where the backward pass takes longer than the forward pass. 1F1B also uses different versions of model weights to maintain statistical efficiency comparable to data parallelism. Each backward pass in a stage results in weight updates; the next forward pass uses the latest version of weights available, and "stashes" a copy of these weights to use during the corresponding backward pass. Although the forward pass will not see updates from incomplete in-flight mini-batches, learning is still effective because model weights change relatively slowly and bounded staleness has been found effective in improving training speeds <ref type="bibr" target="#b13">[19,</ref><ref type="bibr" target="#b36">43]</ref>. However, for the backward pass to compute numerically correct gradients, the same weight version used during the forward pass must be used. PipeDream limits the number of "in-pipeline" minibatches to the minimum needed to keep the pipeline full, reducing memory overhead.</p><p>Operating the pipeline at peak throughput also requires that all stages in the pipeline take roughly the same amount of time, since the throughput of a pipeline is bottlenecked by the slowest stage. PipeDream automatically determines how to partition the operators of the DNN based on a short profiling run performed on a single GPU, balancing computational load among the different stages while minimizing communication for the target platform. PipeDream effectively load balances even in the presence of model diversity (computation and communication) and platform diversity (interconnect topologies and hierarchical bandwidths). As DNNs do not always divide evenly among available workers, PipeDream may decide to use data parallelism for some stages-multiple workers can be assigned to a given stage, processing different minibatches in parallel. Note that vanilla data parallelism corresponds to the pipeline having a single replicated stage. PipeDream extends 1F1B to incorporate round-robin scheduling across data-parallel stages, while making sure that gradients in a backward pass are routed to the corresponding worker from the forward pass since the same weight version and intermediate outputs need to be used for a correct gradient computation. The combined scheduling algorithm, 1F1B-RR, produces a static schedule of operators that each worker runs repeatedly, keeping utilization high across all workers. Thus, pipeline-parallel training can be thought of as a principled combination of inter-batch pipelining with intra-batch parallelism.</p><p>Our evaluation, encompassing many combinations of DNN models, datasets, and hardware configurations, confirms the training time benefits of PipeDream's pipeline parallelism. Compared to data-parallel training, PipeDream reaches a high target accuracy on multi-GPU machines up to 5.3× faster for image classification tasks, up to 3.1× faster for machine translation tasks, 4.3× faster for language modeling tasks, and 3× faster for video captioning models. PipeDream is also 2.6× -15× faster than model parallelism, up to 1.9× faster than hybrid parallelism, and 1.7× faster than simpler approaches to pipelining such as GPipe's approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>A DNN model is composed of many operators organized into layers. When parallelizing DNN training, these layers may be partitioned over the available workers in different ways. In this section, we cover two broad classes of parallel DNN training: intra-and interbatch. We also highlight the challenges posed by DNN model and hardware diversity for effective parallelization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Intra-batch Parallelism</head><p>The most common way to train DNN models today is intra-batch parallelization, where a single iteration of training is split across available workers. Data Parallelism. In data parallelism, inputs are partitioned across workers. Each worker maintains a local copy of the model weights and trains on its own partition of inputs while periodically synchronizing weights with other workers, using either collective communication primitives like all_reduce <ref type="bibr" target="#b17">[24]</ref> or parameter servers <ref type="bibr" target="#b31">[38]</ref>. The amount of data communicated is proportional to the number of model weights and the number of workers participating in training.</p><p>The most commonly used form of data parallelism, referred to as bulk synchronous parallel or BSP <ref type="bibr" target="#b45">[52]</ref> <ref type="foot" target="#foot_0">1</ref> , requires each worker to wait for gradients from other workers. Despite optimizations such as Wait-free Backpropagation <ref type="bibr" target="#b50">[57]</ref>, where weight gradients are sent as soon as they are available (common in modern frameworks), communication stalls are sometimes inevitable for large models where the time needed to synchronize gradients across workers can dominate computation time.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> quantitatively shows the fraction of training time spent in communication stalls with data parallelism for different classes of DNNs using three types of servers: 8-1080Ti GPU instances linked over PCIe within servers and 25Gbps interconnects across servers, 4-V100 GPU instances without NVLink and 10Gbps interconnects across servers, and 8-V100 GPU instances with NVLink interconnects within servers and 25Gbps interconnects across servers.</p><p>We focus on four key takeaways. First, the communication overhead for many of these models is high despite using multi-GPU servers and state-of-the-art communication libraries like NCCL. Data parallelism scales well for models like ResNet-50, which have a large number of convolutional layers with compact weight representations, but scales less well for other models with LSTM or fully-connected layers, which have more dense weight representations. Second, applications distributed across multi-GPU servers are bottlenecked by slower inter-server links, as evidenced by communication overheads spiking and then plateauing when training scales out to multiple servers. Data parallelism for such hierarchical networks can be a poor fit, since the same number of bytes are sent over both high-and low-bandwidth channels. Third, as the number of data-parallel workers increases, communication overheads increase for all models, even if training is performed on a multi-GPU instance with NVLink. Coleman et al. <ref type="bibr" target="#b11">[17]</ref> showed similar results. Fourth, as GPU compute speeds increase (1080Tis to V100s), communication overheads also increase for all models.</p><p>Other DP Optimizations. Asynchronous parallel training (ASP) allows each worker to proceed with the next input minibatch before receiving the gradients from the previous minibatch. This approach improves hardware efficiency (time needed per iteration) over BSP by overlapping computation with communication, but also introduces staleness and reduces statistical efficiency (number of iterations needed to reach a particular target accuracy) <ref type="bibr" target="#b6">[12,</ref><ref type="bibr" target="#b14">20]</ref>.</p><p>Seide et al. <ref type="bibr" target="#b38">[45,</ref><ref type="bibr" target="#b39">46]</ref> looked at quantizing gradients to decrease the amount of data needed to be communicated over the network. This approximation strategy is effective for limited scenarios but lacks generality; it does not hurt convergence for some speech models <ref type="bibr" target="#b40">[47]</ref>, but has not been shown to be effective for other types of models. Others have explored techniques from the HPC literature to reduce the overhead of communication <ref type="bibr" target="#b3">[9,</ref><ref type="bibr" target="#b17">24,</ref><ref type="bibr" target="#b43">50,</ref><ref type="bibr" target="#b44">51]</ref>, often using highly specialized networking hardware. Our work is complementary to these techniques and focuses mainly on improving the performance of parallel DNN training when using commodity accelerators and interconnects available in public clouds.</p><p>Recent work has demonstrated that using large minibatches is effective for training ResNet-50, especially when combined with Layer-wise Adaptive Rate Scaling (LARS) <ref type="bibr" target="#b17">[24,</ref><ref type="bibr" target="#b24">31,</ref><ref type="bibr" target="#b49">56]</ref>. Large minibatches reduce the communication overhead by exchanging parameters less frequently; however, our experiments show that such techniques lack generality beyond ResNet-50 and pipeline parallelism can outperform the fastest LARS data-parallel option.</p><p>Model Parallelism. Model parallelism is an intra-batch parallelism approach where the operators in a DNN model are partitioned across the available workers, with each worker evaluating and performing updates for only a subset of the model's parameters for all inputs. The amount of data communicated is the size of intermediate outputs (and corresponding gradients) that need to be sent across workers.</p><p>Although model parallelism enables training of very large models, vanilla model parallelism is rarely used to accelerate DNN training because it suffers from two major limitations. First, modelparallel training results in under-utilization of compute resources, as illustrated in Figure <ref type="figure" target="#fig_2">2</ref>. Each worker is responsible for a group of consecutive layers; in this regime, the intermediate outputs (activations and gradients) between these groups are the only data that need to be communicated across workers. <ref type="foot" target="#foot_1">2</ref>The second limitation for model-parallel training is that the burden of partitioning a model across multiple GPUs is left to the programmer <ref type="bibr" target="#b29">[36]</ref>, resulting in point solutions. Recent work explores the use of Reinforcement Learning to automatically determine device placement for model parallelism <ref type="bibr" target="#b35">[42]</ref>. However, these techniques are time-and resource-intensive, and do not leverage the fact that DNN training can be thought of as a computational pipeline consisting of groups of consecutive layers -these assumptions make the optimization problem more tractable, allowing for exact solutions in polynomial time as we show in § 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid Intra-batch Parallelism.</head><p>Recent work has proposed splitting a single iteration of the optimization algorithm among multiple dimensions. OWT <ref type="bibr" target="#b29">[36]</ref> split the then-popular AlexNet model by hand, using data parallelism for convolutional layers that have a small number of weight parameters and large outputs, while choosing to not replicate fully connected layers that have a large number of weight parameters and small outputs. OWT does not use pipelining. FlexFlow <ref type="bibr" target="#b26">[33]</ref> proposed splitting a single iteration along samples, operators, attributes, and parameters, and describes an algorithm to determine how to perform this splitting in an automated way. However, FlexFlow does not perform pipelining, and we show in our experiments ( § 5.3) that this leaves as much as 90% of performance on the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inter-batch Parallelism</head><p>Chen et al. <ref type="bibr" target="#b9">[15]</ref> briefly explored the potential benefits of pipelining minibatches in model-parallel training, but do not address the conditions for good statistical efficiency, scale, and generality as applicable to large real-world models. Huo et al. <ref type="bibr" target="#b22">[29]</ref> explored parallelizing the backward pass during training. Our proposed solution parallelizes both the forward and backward pass.</p><p>GPipe (concurrent work with an earlier PipeDream preprint <ref type="bibr" target="#b18">[25]</ref>) uses pipelining in the context of model-parallel training for very large models <ref type="bibr" target="#b21">[28]</ref>. GPipe does not specify an algorithm for partitioning a model, but assumes a partitioned model as input. GPipe further splits a minibatch into m microbatches, and performs forward passes followed by backward passes for these m microbatches (see Figure <ref type="figure" target="#fig_3">3</ref>, m = 4). With a focus on training a large model like AmoebaNet, GPipe optimizes for memory efficiency; it uses existing techniques such as weight gradient aggregation and trades computation for memory by discarding activation stashes between the forward and the backward pass, instead opting to re-compute them when needed in the backward pass <ref type="bibr" target="#b8">[14]</ref>. As a result, it can suffer from reduced hardware efficiency due to re-computation overheads and frequent pipeline flushes if m is small ( § 5.4).</p><p>In comparison, PipeDream addresses key issues ignored in prior work, offering a general solution that keeps workers well utilized, combining pipelining with intra-batch parallelism in a principled way, while also automating the partitioning of the model across the available workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">DNN Model and Hardware Diversity</head><p>DNN models are diverse, with convolutional layers, LSTMs <ref type="bibr" target="#b48">[55]</ref>, attention layers <ref type="bibr" target="#b46">[53]</ref>, and fully-connected layers commonly used. These different types of models exhibit vastly different performance characteristics with different parallelization strategies, making the optimal parallelization strategy highly model-dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 5: An example pipeline-parallel assignment with four GPUs and an example timeline at one of the GPUs (worker 3), highlighting the temporal overlap of computation and activation / gradient communication.</head><p>Picking an optimal parallelization scheme is challenging because the efficacy of such a scheme depends on the characteristics of the target deployment hardware as well; GPUs, ASICs, and FPGAs have very different compute capabilities. Moreover, interconnects linking these accelerators have different topologies and capacities; cloud servers are linked by tens to 100Gbps networks, accelerators within servers might be connected over shared PCIe trees (10 to 15GBps), and specialized expensive servers, such as the DGX-1 [23], use NVLink with point-to-point 30GBps bandwidth capabilities. This diversity in models and deployments makes it extremely hard to manually come up with an optimal parallelization strategy. PipeDream automates this process, as we discuss in § 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PIPELINE PARALLELISM</head><p>PipeDream uses pipeline parallelism (PP), a new parallelization strategy that combines intra-batch parallelism with inter-batch parallelism. Pipeline-parallel computation involves partitioning the layers of a DNN model into multiple stages, where each stage consists of a consecutive set of layers in the model. Each stage is mapped to a separate GPU that performs the forward pass (and backward pass) for all layers in that stage. <ref type="foot" target="#foot_2">3</ref>In the simplest case, only one minibatch is active in the system, as in traditional model-parallel training (Figure <ref type="figure" target="#fig_2">2</ref>); in this setup, at most one GPU is active at a time. Ideally, we would like all GPUs to be active. With this in mind, we inject multiple minibatches into the pipeline one after the other. On completing its forward pass for a minibatch, each stage asynchronously sends the output activations to the next stage, while simultaneously starting to process another minibatch. The last stage starts the backward pass on a minibatch immediately after the forward pass completes. On completing its backward pass, each stage asynchronously sends the gradient to the previous stage while starting computation for the next minibatch (Figure <ref type="figure" target="#fig_4">4</ref>).</p><p>Pipeline parallelism can outperform intra-batch parallelism methods for two reasons:</p><p>Pipelining communicates less. PP often can communicate far less than DP. Instead of having to aggregate gradients for all parameters and send the result to all workers, as is done in data-parallel approaches (using either collective communication or a parameter server), each worker in a PP execution has to communicate only subsets of the gradients and output activations, to only a single other worker. This can result in large reductions in communication for some models (e.g., &gt;85% reduction for VGG-16, AWD LM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pipelining overlaps computation and communication.</head><p>Asynchronous communication of forward activations and backward gradients across stages results in significant overlap of communication with the computation of a subsequent minibatch, as shown in Figure <ref type="figure">5</ref>. This computation and communication are completely independent with no dependency edges, since they operate on different inputs, leading to easier parallelization.</p><p>However, to realize the opportunity of PP, PipeDream must overcome three challenges. In discussing PipeDream's solutions to these challenges, we will refer to Figure <ref type="figure" target="#fig_5">6</ref>, which shows PipeDream's high-level workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Challenge 1: Work Partitioning</head><p>PipeDream treats model training as a computation pipeline, with each worker executing a subset of the model as a stage. Like with any pipeline, the steady state throughput of the resulting pipeline is the throughput of the slowest stage. Having each stage process minibatches at vastly different throughputs can lead to bubbles in the pipeline, starving faster stages of minibatches to work on and resulting in resource under-utilization. Excessive communication between workers can also lower the throughput of the training pipeline. Moreover, the allocation of stages to workers needs to be model-and hardware-aware to be effective, and there may be cases where no simple partitioning across the GPUs achieves both limited communication and perfect load balance.</p><p>Solution: PipeDream's optimizer outputs a balanced pipeline. Its algorithm partitions DNN layers into stages such that each stage completes at roughly the same rate, while trying to minimize communication across workers in a topology-aware way (for example, large outputs should be sent over higher bandwidth links if possible). To further improve load balancing, PipeDream goes beyond straight pipelines, allowing a stage to be replicated (i.e., data parallelism is used on the stage). This partitioning problem is equivalent to minimizing the time taken by the slowest stage of the pipeline, and has the optimal sub-problem property: a pipeline that maximizes throughput given a worker count is composed of sub-pipelines that maximize throughput for smaller worker counts. Consequently, we use dynamic programming to find the optimal solution.</p><p>PipeDream exploits the fact that DNN training shows little variance in computation time across inputs. PipeDream records the computation time taken by the forward and backward pass, the size of the layer outputs, and the size of the associated parameters for each layer as part of an initial profiling step; this profile is used as the input to the optimizer's partitioning algorithm (Figure <ref type="figure" target="#fig_5">6</ref>). The partitioning algorithm also takes into account other constraints such as hardware topology and bandwidth, number of workers, and memory capacity of the compute devices.</p><p>Profiler. PipeDream records three quantities for each layer l, using a short (few minutes) profiling run of 1000 minibatches on a single GPU: 1) T l , the total computation time across the forward and backward passes for layer l on the target GPU, 2) a l , the size of the output activations of layer l (and the size of input gradients in the backward pass) in bytes, and 3) w l , the size of weight parameters for layer l in bytes.</p><p>PipeDream estimates the communication time by dividing the amount of data that needs to be transferred by the network bandwidth of the communication link. Assuming efficient all_reduce collective communication, in data-parallel configurations with m workers, each worker sends ( m−1 m • |w l |) bytes to other workers, and receives the same amount; this is used to estimate the time for weight synchronization for layer l when using data parallelism with m workers.</p><p>Partitioning Algorithm. Our partitioning algorithm takes the output of the profiling step, and computes: 1) a partitioning of layers into stages, 2) the replication factor (number of workers) for each stage, and 3) optimal number of in-flight minibatches to keep the training pipeline busy.</p><p>PipeDream's optimizer assumes that the machine topology is hierarchical and can be organized into levels, as shown in Figure <ref type="figure" target="#fig_6">7</ref>. Bandwidths within a level are the same, while bandwidths across levels are different. We assume that level k is comprised of m k components of level (k − 1), connected by links of bandwidth B k . In Figure <ref type="figure" target="#fig_6">7</ref>, m 2 = 2 and m 1 = 4. In addition, we define m 0 to be 1; m 0 represents the number of compute devices within the first level (solid green boxes in Figure <ref type="figure" target="#fig_6">7</ref>).</p><p>PipeDream's optimizer solves dynamic programming problems progressively from the lowest to the highest level. Intuitively, this process finds the optimal partitioning within a server and then uses these partitions to split a model optimally across servers.</p><p>Notation. Let A k (i → j, m) denote the time taken by the slowest stage in the optimal pipeline between layers i and j using m workers at level k. The goal of our algorithm is to find A L (0 → N , m L ), and the corresponding partitioning, where L is the highest level and N is the total number of layers in the model.</p><p>Let T k (i → j, m) denote the total time taken by a single stage spanning layers i through j for both forward and backward passes, replicated over m workers using bandwidth B k .</p><p>Formulation. For all k from 1 to L,</p><formula xml:id="formula_0">T k (i → j, m) = 1 m max ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ A k −1 (i → j, m k −1 ) 2(m − 1) j l =i |w l | B k</formula><p>where the first term inside the max is the total computation time for all the layers in the stage using level k − 1 as the computation substrate, and the second term is the time for data-parallel communication among all layers in the stage. The result of the max expression above gives the effective time spent processing m inputs while performing compute and communication concurrently; thus, the effective time spent processing a single input is this term divided by m.</p><p>The optimal pipeline can now be broken into an optimal subpipeline consisting of layers from 1 through s with m − m workers followed by a single stage with layers s + 1 through j replicated over m workers. Then, using the optimal sub-problem property, we have:</p><formula xml:id="formula_1">A k (i → j, m) = min i ≤s &lt;j min 1≤m &lt;m max ⎧ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎩ A k (i → s, m − m ) 2a s /B k T k (s + 1 → j, m )</formula><p>where the first term inside the max is the time taken by the slowest stage of the optimal sub-pipeline between layers i and s with m −m workers, the second term is the time taken to communicate the activations and gradients of size a s between layers s and s + 1, and the third term is the time taken by the single stage containing layers s + 1 to j in a data-parallel configuration of m workers. When solving for level k, we use A k −1 (i → j, m k −1 ), which is the optimal total computation time for layers i through j using all workers available in a single component at level (k − 1) (in Figure <ref type="figure">8</ref>: An example PipeDream pipeline with 3 workers and 2 stages. We assume that forward and backward passes in the first stage take two time units, while forward and backward passes in the second stage take only a single time unit. The first stage in this pipeline is replicated twice so that each stage sustains roughly the same throughput. Here, we assume that forward and backward passes take equal time, but this is not a requirement of our approach. the expression T k (i → j, m)). In Figure <ref type="figure" target="#fig_6">7</ref>, this would represent determining how best to partition intermediate layers of the model using all workers in a yellow server.</p><p>Initialization. Level 0 uses the profiled computation times: A 0 (i → j, m 0 ) = j l =i T l . For k &gt; 0, optimal compute times with all compute devices in the previous level are used:</p><formula xml:id="formula_2">A k (i → j, 1) = A k −1 (i → j, m k −1 ).</formula><p>Runtime Analysis. For a given level k, the total number of sub-problems is</p><formula xml:id="formula_3">O (N 2 m k ). Time complexity per sub-problem is O (Nm k ), leading to a total time complexity of O (N 3 m 2 k ) for level k. Total time complexity is L k =1 O (N 3 m 2 k ).</formula><p>In our experiments, the running time is under 8 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Challenge 2: Work Scheduling</head><p>Unlike traditional uni-directional pipelines, training in PipeDream involves a bi-directional pipeline, where an input minibatch proceeds through the computation pipeline first forward and then backward. Each active minibatch in the pipeline may be in a different stage, either in the forward pass or backward pass. As a result, each worker in the system needs to determine whether it should i) perform its stage's forward pass for a minibatch, pushing the minibatch to downstream workers, or ii) perform its stage's backward pass for a different minibatch, pushing the minibatch to upstream workers. In addition, how should minibatches be routed with replicated stages? Solution: In the startup phase, the input stage admits enough minibatches to keep the pipeline full in steady state. Based on the partitioning generated by our algorithm, the optimal number of minibatches admitted per input stage replica to keep the pipeline full in steady state is given by: NUM_OPT_ACTIVE_MINIBATCHES (NOAM) = (# workers) / (# of replicas in the input stage) . Once in steady state, each stage alternates between performing its forward pass for a minibatch and its backward pass for an earlier minibatch. We call this the one-forward-one-backward (1F1B) schedule. 1F1B ensures that every GPU is occupied with a minibatch in a balanced pipeline, with each stage producing outputs in aggregate at roughly the same rate. It also ensures backward passes from inputs are applied at regular intervals of time.</p><p>Figure <ref type="figure" target="#fig_4">4</ref> shows the corresponding compute timeline for a pipeline with 4 stages. The NOAM for this configuration is 4. In the startup phase, the input stage admits exactly four minibatches that propagate their way to the output stage. As soon as the output stage completes its forward pass for the first minibatch, it performs its backward pass for the same minibatch, and then starts alternating between forward and backward passes for subsequent minibatches. As the backward pass starts propagating to earlier stages in the pipeline, every stage starts alternating between its forward and backward passes for different minibatches. As shown in the figure, every worker is performing either a forward or backward pass for some minibatch in steady state.</p><p>When a stage is run in a data-parallel configuration (replicated across multiple GPUs), we use deterministic round-robin load balancing based on a minibatch identifier to spread work across the replicas. Such deterministic load-balancing ensures that each minibatch is routed to the same worker for both the forward and backward passes of the stage, which is important since parameters and intermediate outputs from the forward pass are needed for the backward pass. This mechanism, which we call one-forward-onebackward-round-robin (1F1B-RR), is a static policy that is executed without expensive distributed coordination. Figure <ref type="figure">8</ref> shows this mechanism in action for a simple 2-1 configuration, with the first stage replicated twice, and the second stage un-replicated. In the first stage, all inputs with even minibatch IDs are processed by worker 1, while inputs with odd minibatch IDs are processed by worker 2. Worker 3 in the second stage processes all inputs. All workers perform a forward pass followed by a backward pass on a different input minibatch.</p><p>For 1F1B-RR to be effective, it is not necessary for the forward pass to take as long as the backward pass. In fact, we observe that the backward pass is always larger than the forward pass in practice. 1F1B-RR remains an effective scheduling mechanism, as highlighted in Figure <ref type="figure" target="#fig_4">4</ref>. 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Challenge 3: Effective Learning</head><p>In a naively pipelined system, each stage's forward pass for a minibatch is performed using one version of parameters and its backward pass is performed using a different version of parameters. Figure <ref type="figure" target="#fig_4">4</ref> illustrates this using a partitioning with four workers and no stage replication. In stage 1, the forward pass for minibatch 5 is performed after the updates from minibatch 1 are applied, whereas the backward pass for minibatch 5 is performed after updates from minibatches 2, 3, and 4 are applied. As a result, in the backward pass for minibatch 5 on stage 1, the gradient is computed using a different set of weights than the ones used in the corresponding forward pass; this discrepancy in weight versions results in invalid gradients and can prevent model convergence.</p><p>Solution: PipeDream uses a technique called weight stashing to avoid a fundamental mismatch between the version of weights used in the forward and backward pas. Weight stashing maintains multiple versions of the weights, one for each active minibatch. Each stage processes a minibatch using the latest version of weights available in the forward pass. After completing the forward pass, 4 1F1B-RR produces a full steady state pipeline even for cases where the ratio of backward-to forward-pass time is not an integer (e.g., 3 to 2). PipeDream stores the weights used for that minibatch. The same weight version is then used used to compute the weight update and upstream weight gradient in the minibatch's backward pass.</p><p>Weight stashing ensures that within a stage, the same version of model parameters are used for the forward and backward pass of a given minibatch. For example, in Figure <ref type="figure" target="#fig_7">9</ref>, minibatch 5 uses parameter updates from minibatch 1 on machine 1 and from 2 on machine 2. Weight stashing does not guarantee the consistency of parameter versions used for a given minibatch across stages. Vertical Sync. Vertical sync is an optional technique in PipeDream that eliminates the potential inconsistency across stages. For example, in Figure <ref type="figure" target="#fig_4">4</ref>, minibatch 5 uses parameters updated by minibatch 1 on all workers for both its forward and backward passes when using vertical sync. Each minibatch (b i ) that enters the pipeline is associated with the latest weight version (w (i−x ) ) seen at the input stage. This information is propagated along with the activations and gradients as the minibatch b i flows through the pipeline in the forward direction. Across all stages, the forward pass for b i uses the stashed weights w (i−x ) as opposed to the latest weight update. After performing the backward pass for b i (using stashed weights w (i−x ) ), each stage independently applies weight updates to create the latest weights (w (i ) ), and can then delete w (i−x ) . This coordination across stages is asynchronous.</p><p>The semantics of vertical sync are different from GPipe (and data parallelism). In particular, gradients are not aggregated over all in-flight minibatches in the system -vertical sync merely ensures that the same weight versions are used to compute gradients across different workers (but the weight versions to which gradients are applied are different from those used to compute the corresponding gradients).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Staleness.</head><p>We can now formalize the degree of staleness of weight updates for each of these techniques. For this discussion, we assume a straight pipeline (i.e., no stage replication) with the model split into n stages; the weights in each stage are represented as w 1 , w 2 , and so on. In addition, we denote w (t ) l as the weights w l after t minibatches. Now, after every minibatch, we compute f (w 1 , w 2 , . . . ,w n ), which is the gradient averaged over all samples in the minibatch. Vanilla minibatch SGD (f is the loss function, ν is the learning rate) has the following gradient update:</p><formula xml:id="formula_4">w (t +1) = w (t ) − ν • f (w (t ) 1 , w (t ) 2 , . . . ,w (t ) n )</formula><p>With weight stashing, gradients in stage 1 are computed with weights that are n steps delayed, gradients for stage 2 are computed with weights that are n − 1 steps delayed, etc. Mathematically, this means the weight update looks like:</p><formula xml:id="formula_5">w (t +1) = w (t ) − ν • f (w (t −n+1) 1 , w (t −n+2) 2 , . . . ,w (t ) n )</formula><p>Without weight stashing, the weight update is not a valid gradient of the loss function f for any vector w 1 , . . . ,w n .</p><p>Adding vertical sync alters the weight update to:</p><formula xml:id="formula_6">w (t +1) = w (t ) − ν • f (w (t −n+1) 1 , w (t −n+1) 2 , . . . ,w (t −n+1) n )</formula><p>This is semantically similar to data parallelism with BSP synchronization on n workers (with the same per-worker minibatch size), with the same staleness (but gradients averaged over a minibatch of size B instead of nB).</p><p>Memory Overhead. Pipelining does not significantly increase perworker memory usage relative to data parallelism, even with weight stashing. Consider a straight pipeline (no data-parallel stages), where a model is divided across n workers, with each worker holding 1/n of the weights. With non-pipelined model-parallel training, each worker would need 1/n of the memory compared to data parallel training. Admitting n inputs into the pipeline, as PipeDream does, increases this by at most a factor of n, because a version of &lt;weights, activations&gt; is needed for each in-flight minibatch. Thus, PipeDream's peak per-worker memory usage is on par with data parallelism.</p><p>PipeDream's memory footprint can be further reduced by using existing techniques: efficient encoding or compression of intermediate data <ref type="bibr" target="#b23">[30]</ref>, gradient aggregation where weight gradients are added to a single buffer at a stage for m minibatches before performing a weight update, and trading computation time for activation-stash memory by discarding them in the forward pass and recomputing them as needed during the backward pass <ref type="bibr" target="#b8">[14]</ref>.</p><p>PipeDream's default semantics exclude vertical sync as it requires more metadata to be stored at every stage in the pipeline. PipeDream's default semantics (weight stashing but no vertical sync) are between regular minibatched SGD on a single worker, and data parallelism with BSP synchronization <ref type="bibr" target="#b13">[19,</ref><ref type="bibr" target="#b20">27]</ref>. Our evaluation demonstrates its effectiveness across models, datasets, and hardware configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION</head><p>The interface to PipeDream is implemented as a standalone Python library of ∼3000 LOC that manages device memory, schedules work, and handles communication. PipeDream uses PyTorch [5] for autodifferentiation and to execute operators; however, PipeDream is extensible and can work with other ML frameworks such as Tensorflow <ref type="bibr" target="#b2">[8]</ref>, MXNet <ref type="bibr" target="#b7">[13]</ref>, and CNTK <ref type="bibr" target="#b38">[45]</ref>. As a proof of concept, we also integrated PipeDream with Caffe <ref type="bibr" target="#b25">[32]</ref>.</p><p>PipeDream first profiles the model on a single GPU with a subset of inputs from the training dataset (Figure <ref type="figure" target="#fig_5">6</ref>). It then runs the optimization algorithm described in § 3.1 to partition the DNN model into stages, with some stages possibly replicated.</p><p>PipeDream's optimizer returns an annotated operator graph, with each model layer mapped to a stage ID. PipeDream performs a BFS traversal of this graph and generates code for each stage as a separate torch.nn.Module, ordering operators in each stage to make sure their input-output dependencies from the original PyTorch model graph are respected. The PipeDream runtime then assigns each stage (including replicas for replicated stages) to a single worker according to its 1F1B-RR schedule.</p><p>Parameter State. PipeDream maintains all parameters associated with the layers assigned to the stage directly in GPU memory. PipeDream applies updates to the most recent parameter version when the weight update becomes available if the stage is not replicated. The weight updates are synchronized across replicas prior to being applied if the stage is replicated. When a newer version of the parameters becomes available, the prior version is not immediately discarded. Parameters are discarded only once a backward pass that uses fresher parameters is performed.</p><p>Intermediate State. Each stage's input and output data is assigned a unique blob ID. Upon receiving intermediate data from the prior stage (or from disk in the case of the input stage), PipeDream copies the intermediate data to GPU memory and places a pointer to the associated buffer in a work queue. Intermediate data from the forward pass is not discarded until the associated minibatch completes that stage's backward pass. Intermediate data from the backward pass is freed as soon as the worker finishes using it, and if necessary, after it is sent to the next stage.</p><p>Stage Replication. PipeDream uses PyTorch's Distributed-DataParallel library [6] to synchronize parameters for layers of data-parallel stages. Using wait-free back propagation, weight gradients are communicated to servers as soon as they are computed, rather than waiting for computation to finish for all layers. Since we support replication of individual stages, data-parallel training is effectively a special case in our framework -we represent this as a single stage that contains all the layers of the DNN model, and replicate the stage across all available GPUs. We use the NCCL communication backend [3] for data-parallel baselines as we find it to be faster than Gloo [1] for the large tensors exchanged in DP. We also find that Gloo is faster than NCCL for small tensors that are exchanged across the pipeline, such as activations and gradients. PipeDream defaults to using Gloo for all inter-GPU communication when performing pipeline-parallel training because we are unable to use both Gloo (across the pipeline) and NCCL (across replicated stages) at the same time in a stage.</p><p>Checkpointing. PipeDream supports periodic checkpointing of model parameters for fault tolerance, with default checkpoints made across stages at the end of every epoch. Checkpoints don't require expensive global coordination. Each stage dumps its model parameters locally when it performs the backward pass for the last minibatch in an epoch. Restarting a run due to failures entails starting from the last successfully created checkpoint for all stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>This section evaluates the effectiveness of PipeDream for seven different DNNs on three different clusters. The results of our experiments support a number of important findings: 1) PipeDream achieves significant speedups in time-to-target-accuracy across a wide range of different learning tasks on different hardware deployments, 2) PipeDream is more efficient than other recently proposed inter-batch approaches, 3) PipeDream greatly reduces overheads of communication and does not significantly increase memory footprint compared to data-parallel training, and 4) combining pipelining, model parallelism, and data parallelism outperforms model-, data-, or hybrid-parallelism in isolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Tasks and Datasets. We use four tasks and four datasets in our experiments: 1) Image Classification, using the ImageNet-1K (ILSVRC12) <ref type="bibr" target="#b37">[44]</ref> dataset; 2) Translation, using the WMT16 English to German dataset for training, and the "newstest2014" dataset for validation; 3) Language Modeling, using the Penn Treebank (PTB) <ref type="bibr" target="#b34">[41]</ref> dataset; and 4) Video Captioning (S2VT), using the Microsoft Video description corpus (MSVD) <ref type="bibr" target="#b5">[11]</ref>.</p><p>Clusters. We use three different clusters in our experiments, summarized in Cluster-C has servers with 1 NVIDIA Titan X GPU and 12 GB of GPU device memory, connected via 40 Gbps Ethernet. Unless otherwise stated, all our experiments are run on multi-GPU servers (Cluster-A and Cluster-B).</p><p>Models. We use seven different DNN models in our experiments across the four applications: 1) VGG-16 <ref type="bibr" target="#b41">[48]</ref>, 2) ResNet-50 <ref type="bibr" target="#b19">[26]</ref>, 3) AlexNet <ref type="bibr" target="#b30">[37]</ref>, 4) Google Neural server Translation (GNMT) with 8 LSTM layers <ref type="bibr" target="#b48">[55]</ref>, 5) GNMT with 16 LSTM layers, 6) AWD Language Model (LM) <ref type="bibr" target="#b33">[40]</ref>, and 7) the S2VT <ref type="bibr" target="#b47">[54]</ref> sequence-to-sequence model for video transcription.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch Sizes and Training Methodology.</head><p>We use the largest per-GPU minibatch that fits in one GPU's memory -anything larger yields out-of-memory exceptions. This ensures that we hit peak achievable FLOPs on a single device. Unless otherwise stated, we report per-GPU minibatch sizes (G); for data-parallel runs with n workers, the global minibatch size (BS) is n×G. The global minibatch sizes we use are consistent with those used by the ML community and reported in the literature for these models. We use a per-GPU minibatch size of 64 per GPU for VGG-16, 256 for AlexNet, 128 for ResNet-50 (e.g., BS = 1024 for 8 GPUs), 64 for GNMT, 80 for S2VT, and batch size of 80 for LM. We train the VGG-16, ResNet-50, Language Modeling, and S2VT models using SGD with an initial learning rate of 0.01, 0.1, 30.0, and 0.01 respectively. For GNMT, we  use the Adam optimizer <ref type="bibr" target="#b28">[35]</ref> with an initial learning rate of 0.0003. We use full (fp32) precision in all our experiments. For all experiments (other than AlexNet), we measure the time taken to train to a target validation accuracy: top-1 accuracy of 68% for VGG-16 <ref type="bibr" target="#b1">[7]</ref>, top-1 accuracy of 75.9% for ResNet-50, BLEU score of 21.8 for GNMT, a validation perplexity of 98 for LM, and a METEOR <ref type="bibr" target="#b16">[22]</ref> score of 0.294 for S2VT. Guided by prior work, we adjust the learning rate during training to converge to the desired result faster <ref type="bibr" target="#b28">[35,</ref><ref type="bibr" target="#b42">49]</ref> and utilize learning rate warm-up for large global batch sizes <ref type="bibr" target="#b17">[24]</ref>. We use the same learning rate schedules for PipeDream and data-parallel training. For AlexNet we use synthetic data (otherwise, data loading is the bottleneck) and measure throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison to Data Parallelism</head><p>Table <ref type="table" target="#tab_0">1</ref> summarizes results comparing PipeDream with data-parallel training (DP). The table shows PipeDream's auto-generated configurations and their speedups in training time-to-accuracy over corresponding data-parallel training configurations. 5 PipeDream Configurations. As described in § 3.1, given a DNN model and a set of servers, PipeDream's optimizer automatically chooses to partition the model into stages, while also deciding the optimal replication factor for each stage. Although most prior research has focused on improving data-parallel training, our results indicate that the best configurations for many models is not data parallelism, despite the use of many important optimizations such as wait-free back propagation. In all but one of our experiments, the best PipeDream configuration combines model parallelism, pipelining, and sometimes data parallelism; each of these configurations outperforms data-parallel training, highlighting the importance of combining inter-batch pipelining with intra-batch parallelism. PipeDream's optimizer recommends data parallelism for ResNet-50 because its weight representations are small and its outputs  are large. PipeDream's optimizer, besides determining the optimal configuration, also automatically decides where to partition the DNN training graph; these partitioning decisions are not shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Image Classification. We compare PipeDream and DP time-toaccuracy for VGG-16 using 4 servers in Cluster-A (4x4 (A) in Table <ref type="table" target="#tab_0">1</ref>. PipeDream reaches target accuracy 5.28× faster than DP on a single server, due to a reduction in inter-server communication. Figure <ref type="figure" target="#fig_9">10</ref> (a) shows this comparison as the DNN is trained over time. In the 4-server configuration, PipeDream's optimizer ( § 3.1) recommends a 15-1 configuration -in this case, VGG-16's convolutional layers are replicated, while the large fully connected layers are not, reducing communication overhead. Moreover, pipelining across the two stages helps keep all workers busy.</p><p>Compared to Cluster-A, which has 4 GPUs per server connected via PCIe, Cluster-B has 8 GPUs per server connected over faster peer-to-peer NVLink interconnects. On 2 servers on Cluster-B (16 GPUs total), PipeDream reaches target accuracy 2.98× faster than DP when training VGG-16. Due to the faster interconnects on Cluster-B, both PipeDream and DP reach target accuracy faster than on Cluster-A (see Figure <ref type="figure" target="#fig_9">10</ref>).</p><p>For training ResNet-50 on Cluster-A, PipeDream's partitioning algorithm recommends data parallelism as the optimal configuration (no pipelining or model parallelism). Later, in § 5.5, we show the reason for this recommendation: non data-parallel configurations incur higher communication overheads than DP for ResNet-50, since ResNet-50 is composed of convolutional layers which have compact weight representations but large output activations. For AlexNet, we compare throughput of PipeDream on Cluster-A and Cluster-B. On Cluster-A, PipeDream achieves a time-per-epoch speedup of 4.92× with 4 servers. On Cluster-B, PipeDream achieves a speedup of 2.04× when using 16 GPUs.</p><p>Translation. We show results for the GNMT model with 8 LSTM layers (GNMT-8) and 16 LSTM layers (GNMT-16) in Table <ref type="table" target="#tab_0">1</ref>). Using 1 server on Cluster-A, PipeDream reaches target accuracy ∼ 1.5×  faster than DP for GNMT-8 and GNMT-16. When using 4 servers (16 GPUs) on Cluster-A, PipeDream reaches target accuracy 2.92× (GNMT-8) and 2.95× (GNMT-16) faster than DP. We show in § 5.5 that PipeDream significantly reduces communication compared to DP, thus reducing its time to target accuracy. On 2 servers (16 GPUs) of Cluster-B, PipeDream reaches target accuracy 3.14× faster than DP for GNMT-16, choosing a "straight" configuration (no stage replication). For GNMT-8, PipeDream falls back to data parallelism, since the smaller model has lower communication overhead on servers with fast NVLink interconnects between GPUs on the same server, and GNMT-8 does not have enough layers for a 16-deep straight pipeline.</p><p>Language Modeling. This model is made up of six LSTM layers that contain a large number of model parameters (0.41GB), making data-parallel training inefficient. Using a single server on Cluster-A, PipeDream reaches target accuracy 4.25× faster than DP. PipeDream chooses a "straight" configuration that reduces communication by 88% compared to DP.</p><p>Video Captioning. PipeDream chooses to use a 2-1-1 configuration for the S2VT on Cluster-C, reducing communication by 85% compared to DP, which in turn allows it to reach target accuracy 3.01× faster than DP.</p><p>Comparison to MLPerf v0.5. For ResNet-50 and GNMT-8, we observe that our data-parallel baseline on a single server with 8 GPUs in Cluster-B is comparable to the MLPerf v0.5 entry that uses a similar hardware configuration. However, we observe that per-epoch times on public cloud servers are slower than official MLPerf v0.5 entries for multi-server DP deployments, since slower communication links on public cloud servers (compared to dedicated clusters used in the MLPerf entries) make all_reduce communication slower. We cannot measure this difference in time-to-accuracy at the scales used by the MLPerf entries as it is cost prohibitive, but  entries with data-parallel runs on p3.16xlarge instances using the same code. Coleman et al. observed similar results <ref type="bibr" target="#b11">[17]</ref>, both for official DAWNBench and MLPerf entries.</p><p>Furthermore, with 8 GPUs, for GNMT-8, while full precision is slower than the entry using mixed precision, we use a fp32 baseline to be consistent with the rest of the evaluation in this paper. Figure <ref type="figure" target="#fig_12">12</ref> shows that communication overheads for data parallelism with mixed precision are higher than with full precision, and thus the speedups we highlight with pipeline parallelism should carryover (or improve) with mixed precision training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to DP with large minibatches.</head><p>Recent work has demonstrated that using large minibatches is effective for training ResNet-50 and AlexNet models, especially when combined with Layer-wise Adaptive Rate Scaling (LARS). <ref type="bibr" target="#b17">[24,</ref><ref type="bibr" target="#b24">31,</ref><ref type="bibr" target="#b49">56]</ref>. LARS uses different learning rates for each layer based on the ratio of the weight norm to the gradient norm. Large minibatches decrease the frequency of communication, reducing the communication overhead for data parallelism. Figure <ref type="figure" target="#fig_13">13</ref> shows 8-server results for data-parallel training of VGG-16 using LARS and large minibatches on Cluster-C. Minibatches of 1024 had the fastest time-to-targetaccuracy, while minibatches of 4096 and 8192 failed to reach target accuracy, highlighting the lack of generality of such approaches. PipeDream still reaches target accuracy over 2.4× faster than the fastest data-parallel option (1024 with LARS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison to Asynchronous Parallelism (ASP).</head><p>ASP can reduce communication overhead in data-parallel training. Unlike BSP, which synchronizes parameters after every minibatch, ASP has no synchronization overheads, and workers use the most recent parameter data available. The result is often poor statistical efficiency. For example, when training VGG-16 on 4 Cluster-B servers, ASP data-parallel takes 7.4× longer than PipeDream to reach a 48% accuracy (when we terminate ASP for taking too long to converge), even though ASP has minimal communication delays. Similar results have been shown by Chen et al. <ref type="bibr" target="#b6">[12]</ref>. Statistical Efficiency. Figure <ref type="figure" target="#fig_11">11</ref> shows accuracy vs. epoch for VGG-16 and GNMT-16 on Cluster-B. We do not show accuracy vs. epoch graphs for other experiments due to space constraints. However, we consistently observe that PipeDream reaches target accuracy in a similar number of epochs as DP (as can be seen by the fact that TTA and epoch time speedups are the same for many rows in Table <ref type="table" target="#tab_0">1</ref>). This highlights the fact that PipeDream's weight stashing mechanism is able to achieve statistical efficiency comparable to data parallelism, and that PipeDream's speedups are due to better system performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison to Other Intra-batch Parallelism Schemes</head><p>This section compares PipeDream to other intra-batch parallelization techniques besides data parallelism.</p><p>Model Parallelism. Figure <ref type="figure" target="#fig_15">14a</ref> compares model parallelism (blue bars), straight pipelines without replication (green bars), and pipelining with stage replication (red bars). For all four models, pipelining alone increases throughput by 2× or more. For GNMT-8 and GNMT-16, PipeDream's optimizer chooses not to replicate any stages, resulting in identical configurations for the green and red bars. For VGG-16 and AlexNet, PipeDream replicates the first stage, leading to speedups of 14.9× and 6.5× compared to model parallelism.</p><p>Hybrid Parallelism. Figure <ref type="figure" target="#fig_15">14b</ref> shows that pipelining for a configuration that combines data and model parallelism (similar to those proposed by Krizhevsky et al. <ref type="bibr" target="#b29">[36]</ref> and FlexFlow <ref type="bibr" target="#b26">[33,</ref><ref type="bibr" target="#b27">34]</ref>) increases throughput by as much as 80%. In running FlexFlow for AlexNet on Cluster-B (not shown in Figure <ref type="figure" target="#fig_15">14b</ref>), we observe that PipeDream is 1.9× faster; a speedup due to pipelining over hybrid parallelism. Note that the same number of bytes are being communicated across workers with and without pipelining. Speedups are achieved by overlapping compute and communication, and consequently better utilization of compute resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison to Inter-batch Parallelism</head><p>We compare training GNMT-16 using PipeDream and our implementation of GPipe using 16 GPUs on Cluster-A and Cluster-B. GPipe does not provide an algorithm for partitioning work across stages, so we use the same partitions as PipeDream. GPipe also does not provide an algorithm for how many items should be permitted into the "pipeline" (pipeline depth). When we set the pipeline depth to be equivalent to "NOAM" in PipeDream ( § 3.2), GPipe experiences 55% and 71% throughput slowdowns compared to PipeDream on Cluster-A and Cluster-B, respectively. Setting the pipeline depth for GPipe to the largest number that does not cause an out-ofmemory exception, leads to throughput slowdowns of 35% and 42%  on Cluster-A and Cluster-B, respectively. These throughput slowdowns are largely due to more frequent pipeline flushes compared to PipeDream (Figures <ref type="figure" target="#fig_4">3 and 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Microbenchmarks</head><p>We evaluate PipeDream's optimizer, its communication overhead and memory footprint, and the effect of pipeline depth on throughput and memory footprint.</p><p>Optimizer. PipeDream's optimizer is efficient, generating optimal training configurations in under 8 seconds for all models and hardware deployments evaluated. As one example, Figure <ref type="figure" target="#fig_16">15</ref> shows real vs. predicted throughputs for various configurations for VGG-16 with 16 workers. Predicted and real throughputs are strongly linearly correlated, and the optimizer picks the best configuration among those tested.</p><p>Memory Footprint. Figure <ref type="figure" target="#fig_17">16</ref> shows the per-stage memory footprint of PipeDream for 4-stage configurations for three different models. PipeDream's worst-case memory footprint is on par with that of data parallelism, even though PipeDream stashes multiple weight and activation versions. This is because each stage in PipeDream is responsible for only a fraction of the total number of weights and activations in the model. As PipeDream scales to include more stages, the memory footprints remain consistent as discussed in § 3.3.   For ResNet-50, the amount of communication for the best nondata-parallel configuration is higher than the DP configuration, thus explaining why PipeDream's optimizer chooses to perform ResNet-50 training using a data-parallel configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Communication Overhead</head><note type="other">.</note><p>Effect of Pipeline Depth. Figure <ref type="figure" target="#fig_20">18</ref> shows the effect of varying pipeline depth on throughput and memory overhead for GNMT-8. We make three observations: 1) Memory footprint with no pipelining is different across stages, since PipeDream's optimizer tries to load balance compute and communication, and not memory footprint (working set still fits comfortably in GPU memory). 2) As the pipeline depth increases from 2 to 7, memory footprint increases because the number of weights and activations that need to be stashed increases proportionally. 3) In our experiments, a pipeline depths of 4 (NOAM) and 7 give the highest throughput. While the working set of stages fits in GPU memory (16 GB), if required, pipeline depth can be decreased to trade throughput for reduced memory footprint. Throughput increases as pipeline depth increases since communication can be more easily hidden as the number of inputs in the pipeline increases, reducing pipeline stalls and thus improving resource utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Pipeline-parallel DNN training helps reduce the communication overheads that can bottleneck intra-batch parallelism. PipeDream automatically partitions DNN training across workers, combining inter-batch pipelining with intra-batch parallelism to better overlap computation with communication while minimizing the amount of data communicated. Compared to state-of-the-art approaches, PipeDream completes training up to 5.3× faster across a range of DNNs and hardware configurations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Communication overhead of data-parallel training using different multi-GPU server instances using PyTorch 1.1, NCCL [3], and fp32 precision. We use the largest per-GPU minibatch size that fits in GPU memory, and keep the per-GPU minibatch size constant as the number of GPUs are scaled up (weak scaling).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model parallel training with 4 workers. Numbers indicate batch ID, and backward passes takes twice as long as forward passes. For simplicity, we assume that communicating activations/gradients across workers has no overhead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: GPipe's inter-batch parallelism approach. Frequent pipeline flushes lead to increased idle time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example PipeDream pipeline with 4 workers, showing startup and steady states. In this example, the backward pass takes twice as long as the forward pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: PipeDream's automated mechanism to partition DNN layers into stages. PipeDream first profiles the input DNN, to get estimates for each layer's compute time and output size. Using these estimates, PipeDream's optimizer partitions layers across available machines, which is then executed by PipeDream's runtime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: An example 2-level hardware topology. Solid green boxes represent GPUs. Each server (dashed yellow boxes) has 4 GPUs connected internally by links of bandwidth B 1 ; each server is connected by links of bandwidth B 2 . In real systems, B 1 &gt; B 2 . Figure best seen in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Weight stashing as minibatch 5 flows across stages. Arrows point to weight versions used for forward and backward passes for minibatch 5 at the first and third stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Accuracy vs. time for VGG-16 using 16 GPUs. Each circle or triangle represents two epochs of training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>5 A configuration indicates how layers are partitioned into stages amongst workers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Accuracy vs. epoch using 16 GPUs on Cluster-B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Communication overhead of data-parallel training using different server instances using PyTorch 1.1 and NCCL [3] for a GNMT-8 model with fp16 and fp32 precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>13 :</head><label>13</label><figDesc>Statistical efficiency (accuracy vs. epoch) using LARS (VGG-16, 8 GPUs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Comparison of PipeDream (red) to non-DP intrabatch techniques for 4-GPU configurations on Cluster-A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Real vs. optimizer's predicted throughput for VGG-16 with 16 workers. Each symbol represents a different partition, including the triangle for vanilla data-parallelism and the diamond for the optimizer's selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Memory footprint for various models using 4 GPUs. Per-GPU memory footprint is shown for data parallelism, and is identical on all GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 17</head><label>17</label><figDesc>shows the amount of communication performed per training sample in the best non-DP configuration compared to the amount of communication performed in data-parallel training. For GNMT-8, GNMT-16, and VGG-16, the communication overhead for the best non-DP configuration is far less than the communication overhead for the DP configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Bytes communicated per training sample by dataparallel (DP) and the best non-DP configurations for 4 GPUs on Cluster-A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Effect of pipeline depth on throughput and memory overhead for GNMT-8 on 4 V100s in Cluster-A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Summary of results comparing PipeDream with data parallelism (DP) when training models to advertised final accu- racy. A PipeDream config of "2-1-1" means the model is split into three stages with the first stage replicated across 2 workers, and</head><label>1</label><figDesc></figDesc><table><row><cell>Task</cell><cell></cell><cell></cell><cell>Model</cell><cell>Dataset</cell><cell cols="3">Accuracy # Servers × # GPUs PipeDream</cell><cell>Speedup over DP</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Threshold per server (Cluster)</cell><cell>Config</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Epoch time</cell><cell>TTA</cell></row><row><cell></cell><cell></cell><cell cols="2">VGG-16 [48]</cell><cell>ImageNet [44]</cell><cell>68% top-1</cell><cell>4x4 (A) 2x8 (B)</cell><cell>15-1 15-1</cell><cell>5.28× 5.28× 2.98× 2.46×</cell></row><row><cell cols="2">Image Classification</cell><cell cols="2">ResNet-50 [26] AlexNet [37]</cell><cell>ImageNet [44] Synthetic Data</cell><cell>75.9% top-1 N/A</cell><cell>4x4 (A) 2x8 (B) 4x4 (A) 2x8 (B)</cell><cell>16 16 15-1 15-1</cell><cell>1× 1× 4.92× 2.04×</cell><cell>1× 1× N/A N/A</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1x4 (A)</cell><cell>Straight</cell><cell>1.46×</cell><cell>2.2×</cell></row><row><cell></cell><cell></cell><cell cols="2">GNMT-16 [55]</cell><cell>WMT16 EN-De</cell><cell>21.8 BLEU</cell><cell>4x4 (A)</cell><cell>Straight</cell><cell>2.34× 2.92×</cell></row><row><cell>Translation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2x8 (B)</cell><cell>Straight</cell><cell>3.14× 3.14×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1x4 (A)</cell><cell>Straight</cell><cell>1.5×</cell><cell>1.5×</cell></row><row><cell></cell><cell></cell><cell cols="2">GNMT-8 [55]</cell><cell>WMT16 EN-De</cell><cell>21.8 BLEU</cell><cell>3x4 (A)</cell><cell>Straight</cell><cell>2.95× 2.95×</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2x8 (B)</cell><cell>16</cell><cell>1×</cell><cell>1×</cell></row><row><cell cols="2">Language Model</cell><cell cols="3">AWD LM [40] Penn Treebank [41]</cell><cell>98 perplexity</cell><cell>1x4 (A)</cell><cell>Straight</cell><cell>4.25× 4.25×</cell></row><row><cell cols="2">Video Captioning</cell><cell cols="2">S2VT [54]</cell><cell cols="2">MSVD [11] 0.294 METEOR</cell><cell>4x1 (C)</cell><cell>2-1-1</cell><cell>3.01× 3.01×</cell></row><row><cell>Cluster</cell><cell cols="2">Server SKU</cell><cell>GPUs per</cell><cell>Interconnects</cell><cell></cell><cell></cell><cell></cell></row><row><cell>name</cell><cell></cell><cell></cell><cell>server</cell><cell>Intra-, Inter-server</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Cluster-A Azure NC24 v3</cell><cell>4x V100</cell><cell>PCIe, 10 Gbps</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Cluster-B AWS p3.16xlarge 8x V100</cell><cell>NVLink, 25 Gbps</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cluster-C</cell><cell cols="2">Private Cluster</cell><cell>1 Titan X</cell><cell>N/A, 40 Gbps</cell><cell></cell><cell></cell><cell></cell></row></table><note>a "straight" configuration is a pipeline with no replicated stages-e.g., "1-1-1-1" on 4 workers. Batch sizes used to train these models are reported in § 5.1.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Characteristics of servers used in experiments.</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note>. Cluster-A has servers with 4 NVIDIA V100 GPUs each (Microsoft Azure NCv3 instances), with 16 GB of GPU device memory, and a 10 Gbps Ethernet interface. Cluster-B has servers with 8 V100s each (AWS EC2 p3.16xlarge instances), with 16 GB of GPU device memory, and a 25 Gbps Ethernet interface. GPUs within servers are connected via a shared PCIe interconnect on Cluster-A, and via point-to-point NVLink on Cluster-B. All servers run 64-bit Ubuntu 16.04 with CUDA toolkit 10.0 and cuDNN v7.4.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 : Increase in per-epoch times for data-parallel train- ing when moving from dedicated clusters used in official MLPerf v0.5 entries to public clouds like Cluster-B. The same code is used for both sets of runs.</head><label>3</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Scale (# V100s)</cell><cell>Cluster-B /</cell></row><row><cell></cell><cell></cell><cell>official MLPerf v0.5</cell></row><row><cell>GNMT-8</cell><cell>256</cell><cell>1.94×</cell></row><row><cell>SSD</cell><cell>64</cell><cell>3.29×</cell></row><row><cell>Mask R-CNN</cell><cell>64</cell><cell>2.32×</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 3 compares the advertised training throughput of official MLPerf v0.5 [2]</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In this paper, we use DP to refer to data-parallelism with BSP.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">While other partitioning schemes are possible, this is the most common, and the one we will use in this paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We use GPUs as a concrete instance of accelerators and use the terms "GPU" and "worker" interchangeably.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>PipeDream is part of Project Fiddle at MSR. We thank the MSR Lab LT, especially Ricardo Bianchini and Donald Kossmann, for their enthusiastic and unwavering support of Project Fiddle, and for their generous support in procuring the many resources required to develop and evaluate PipeDream.</p><p>We also thank our shepherd, Christopher J. Rossbach, the anonymous SOSP reviewers, Zeyuan Allen-Zhu, Jakub Tarnawski, Shivaram Venkataraman, Jack Kosaian, Keshav Santhanam, Sahaana Suri, James Thomas, Shoumik Palkar, and many of our MSR colleagues for their invaluable feedback that made this work better. We acknowledge the support of the affiliate members of the Stanford DAWN project (Ant Financial, Facebook, Google, Infosys, Intel, Microsoft, NEC, SAP, Teradata, and VMware), Amazon Web Services, and Cisco. We also thank the members and companies of the CMU PDL Consortium (Alibaba, Amazon, Datrium, Facebook, Google, HPE, Hitachi, IBM, Intel, Micron, Microsoft, NetApp, Oracle, Salesforce, Samsung, Seagate, Two Sigma) and VMware for their feedback and support. This research was also supported in part by NSF CAREER grant CNS-1651570, NSF Graduate Research Fellowship grant DGE-1656518, NSF grant CCF-1725663, and the Intel STC on Visual Cloud Systems (ISTC-VCS). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://www.nvidia.com/en-us/data-center/nvlink/" />
		<title level="m">NVLink</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">VGG-16 target accuracy using Caffe model</title>
		<ptr target="https://gist.github.com/ksimonyan/211839e770f7b538e2d8#gistcomment-1403727" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ten-sorFlow: A System for Large-Scale Machine Learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
				<imprint>
			<publisher>GA</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bringing HPC Techniques to Deep Learning</title>
		<ptr target="http://research.baidu.com/bringing-hpc-techniques-deep-learning/" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Baidu Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Tradeoffs of Large Scale Learning</title>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collecting Highly Parallel Data for Paraphrase Evaluation</title>
		<author>
			<persName><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.00981</idno>
		<title level="m">Revisiting Distributed Synchronous SGD</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR abs/1512.01274</idno>
		<ptr target="http://arxiv.org/abs/1512.01274" />
		<title level="m">MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<title level="m">Training Deep Nets with Sublinear Memory Cost</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pipelined Back-Propagation for Context-dependent Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Eversole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Project Adam: Building an Efficient and Scalable Deep Learning Training System</title>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Trishul M Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson</forename><surname>Suzue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Apacible</surname></persName>
		</author>
		<author>
			<persName><surname>Kalyanaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Operating Systems Design and Implementation (OSDI &apos;14)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analysis of DAWNBench, a Time-to-Accuracy Machine Learning Performance Benchmark</title>
		<author>
			<persName><forename type="first">Cody</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Nardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunle</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="14" to="25" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Cody</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Nardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunle</forename><surname>Olukotun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<title level="m">DAWNBench: An End-to-End Deep Learning Benchmark and Competition. NIPS ML Systems Workshop</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploiting Bounded Staleness to Speed Up Big Data Analytics</title>
		<author>
			<persName><forename type="first">Henggang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cipar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qirong</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Kyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhimanu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">GeePS: Scalable Deep Learning on Distributed GPUs with a GPU-Specialized Parameter Server</title>
		<author>
			<persName><forename type="first">Henggang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh European Conference on Computer Systems. ACM, 4</title>
				<meeting>the Eleventh European Conference on Computer Systems. ACM, 4</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large Scale Distributed Deep Networks</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Quoc V Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Meteor Universal: Language Specific Translation Evaluation for Any Target Language</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
				<meeting>the Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Gibbons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03377</idno>
		<title level="m">PipeDream: Fast and Efficient Pipeline Parallel DNN Training</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>CoRR abs/1512.03385</idno>
		<ptr target="http://arxiv.org/abs/1512.03385" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server</title>
		<author>
			<persName><forename type="first">Qirong</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cipar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henggang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Kyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garth</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1223" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06965</idno>
		<title level="m">GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Zhouyuan</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10574</idno>
		<title level="m">Decoupled Parallel Backpropagation with Convergence Guarantee. ICML-18</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gist: Efficient Data Encoding for Deep Neural Network Training</title>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gennady</forename><surname>Pekhimenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA &apos;18)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shutao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangzihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haidong</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feihu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhou</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11205</idno>
		<title level="m">Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional Architecture for Fast Feature Embedding</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML &apos;18)</title>
				<meeting>the 28th International Conference on Machine Learning (ICML &apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond Data and Model Parallelism for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd SysML Conference, SysML &apos;19</title>
				<meeting>the 2nd SysML Conference, SysML &apos;19<address><addrLine>Palo Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A Method for Stochastic Optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">One Weird Trick for Parallelizing Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ima-geNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scaling Distributed Machine Learning with the Parameter Server</title>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><forename type="middle">Woo</forename><surname>David G Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amr</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanja</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><forename type="middle">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bor-Yiing</forename><surname>Shekita</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Operating Systems Design and Implementation (OSDI &apos;14)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Masters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Luschi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07612</idno>
		<title level="m">Revisiting Small Batch Training for Deep Neural Networks</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02182</idno>
		<title level="m">Regularizing and Optimizing LSTM Language Models</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent Neural Network Based Language Model</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2010-01">Jan Černockỳ, and Sanjeev Khudanpur. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Device Placement Optimization with Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuefeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.04972" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CNTK: Microsoft&apos;s Open-Source Deep-Learning Toolkit</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">1-Bit Stochastic Gradient Descent and its Application to Data-Parallel Distributed Training of Speech DNNs</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On Parallelizability of Stochastic Gradient Descent for Speech DNNs</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>IEEE SPS</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automating Model Search for Large Scale Machine Learning</title>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">R</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kraska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth ACM Symposium on Cloud Computing</title>
				<meeting>the Sixth ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="368" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optimization of Collective Communication Operations in MPICH</title>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Rabenseifner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="49" to="66" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Meet Horovod: Uber&apos;s Open Source Distributed Deep Learning Framework for TensorFlow</title>
		<ptr target="https://eng.uber.com/horovod/" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Uber Technologies Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A Bridging Model for Parallel Computation</title>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="1990-08">1990. Aug. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4534" to="4542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<title level="m">Large Batch Training of Convolutional Networks</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qirong</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 USENIX Annual Technical Conference (USENIX ATC 17). USENIX Association</title>
				<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="181" to="193" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
