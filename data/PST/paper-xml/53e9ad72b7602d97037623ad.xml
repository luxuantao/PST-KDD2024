<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relational Joins on Graphics Processors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bingsheng</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ke</forename><surname>Yang #</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Fang $</surname></persName>
							<email>rui.fang@highbridge.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Highbridge Capital Management LLC</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mian</forename><surname>Lu</surname></persName>
							<email>lumian@cse.ust.hk</email>
						</author>
						<author>
							<persName><forename type="first">Naga</forename><forename type="middle">K</forename><surname>Govindaraju</surname></persName>
							<email>nagag@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Qiong</forename><surname>Luo</surname></persName>
							<email>luo@cse.ust.hk</email>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
							<email>psander@cse.ust.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Relational Joins on Graphics Processors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9484E8818ED336173E4CAD47BA70F035</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.4 Systems</term>
					<term>Query processing; Relational databases Algorithms</term>
					<term>Measurement</term>
					<term>Performance relational database</term>
					<term>join</term>
					<term>sort</term>
					<term>primitive</term>
					<term>parallel processing</term>
					<term>graphics processors</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel design and implementation of relational join algorithms for new-generation graphics processing units (GPUs). The most recent GPU features include support for writing to random memory locations, efficient inter-processor communication, and a programming model for general-purpose computing. Taking advantage of these new features, we design a set of data-parallel primitives such as split and sort, and use these primitives to implement indexed or non-indexed nested-loop, sortmerge and hash joins. Our algorithms utilize the high parallelism as well as the high memory bandwidth of the GPU, and use parallel computation and memory optimizations to effectively reduce memory stalls. We have implemented our algorithms on a PC with an NVIDIA G80 GPU and an Intel quad-core CPU. Our GPU-based join algorithms are able to achieve a performance improvement of 2-7X over their optimized CPU-based counterparts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Graphics processing units (GPUs) are specialized architectures traditionally designed for gaming applications. Recent research has shown that they can significantly speed up database query processing <ref type="bibr" target="#b4">[5]</ref>[14] <ref type="bibr" target="#b14">[15]</ref>[16] <ref type="bibr" target="#b35">[36]</ref>.</p><p>Moreover, new generation GPUs, such as AMD R600 and NVIDIA G80, have transformed into powerful co-processors for general-purpose computing (GPGPU). In particular, they provide general parallel processing capabilities, including support for scatter operations and interprocessor communication, as well as general-purpose programming languages such as NVIDIA CUDA <ref type="bibr" target="#b26">[27]</ref>. In this paper, we investigate the design and implementation of common relational join algorithms on such GPUs.</p><p>Joins are the cornerstone operator in relational database systems and CPU-based join algorithms have been studied extensively in the literature. Basic join algorithms include non-indexed and indexed nested-loop joins (NINLJ and INLJ respectively), the sort-merge join (SMJ) and the hash join (HJ). Many variants have been designed for in-memory databases <ref type="bibr" target="#b9">[10]</ref>[31] <ref type="bibr" target="#b33">[34]</ref> and for parallel databases <ref type="bibr" target="#b12">[13]</ref>[24] <ref type="bibr" target="#b31">[32]</ref>. These studies have shown that the implementation techniques, as well as the design, have a great impact on the join performance on CPU-based architectures. In general, memory stalls are a major performance factor for CPUbased relational joins <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b33">[34]</ref>.</p><p>Similar to CPUs, in particular multi-core CPUs, GPUs are commodity hardware consisting of multiple processors. However, these two types of processors differ significantly in their hardware architecture. Specifically, GPUs provide parallel lower-clocked execution capabilities on over a hundred SIMD (Single Instruction Multiple Data) processors whereas current multi-core CPUs typically offer out-of-order execution capabilities on a much smaller number of cores. Moreover, the majority of GPU transistors are devoted to computation units rather than caches, and GPU cache sizes are 10X smaller than CPU cache sizes. These GPU hardware design choices provide higher computational capabilities, better latency tolerance and higher memory bandwidth.</p><p>We explore how relational joins can utilize hardware features of the GPU. In particular, the SIMD design and the massively multithreaded capability in GPUs require our algorithms to achieve good load balancing across processors to hide the latency effectively. Moreover, most GPUs lack hardware support for handling read/write conflicts among concurrent threads. On one hand, this design choice reduces the hardware complexity. On the other hand, high-level abstractions and carefully designed patterns in the software are necessary for correctness and efficiency.</p><p>Considering the characteristics of GPUs and individual join algorithms, we design a set of data-parallel primitives that are used as building blocks for our join algorithms. Most of these primitives can find their functionally-equivalent CPU-based counterparts in traditional databases, but our design and implementation are highly optimized for the GPU. In particular, our algorithms for these primitives take advantage of three advanced features of current GPUs: (1) the massive thread parallelism, (2) the fast inter-processor communication through local memory, and (3) the coalesced access. Specifically, our map primitive employs the coalesced accesses among GPU threads to fully utilize the video memory bandwidth; our split operation avoids the read/write conflicts by aligning histograms to the GPU threading architecture efficiently; our scatter and gather operations work in multiple passes for improved spatial locality in the memory access; and our sort algorithm uses the map primitive to implement a sorting network, or uses the split primitive to implement a quick sort.</p><p>Utilizing this small set of data-parallel primitives, we have designed and implemented GPU-based algorithms for NINLJ, INLJ, SMJ, and HJ. Specifically, our NINLJ is block-nested loops, with a data block mapped to a group of threads within a processor; our INLJ constructs a GPU-based variant of the CSS-Tree (Cache-Sensitive Search Trees) <ref type="bibr" target="#b30">[31]</ref> and performs a massive number of concurrent index searches in the join; our SMJ utilizes quantiles for balanced range-partitioning and merges sorted partitions in parallel; and our HJ recursively splits the relation into multiple partitions and performs joins on the matching partitions in parallel. We have implemented all of our GPU-based primitives and join algorithms using CUDA <ref type="bibr" target="#b26">[27]</ref>, NVIDIA's GPGPU language, and DirectX 10 <ref type="bibr" target="#b5">[6]</ref>, a graphics API for programmable GPUs. We evaluated our GPU-based algorithms in comparison with their optimized parallel counterparts on an Intel quad-core CPU. All join algorithms operate on memory-resident data organized in the column-based model <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b34">[35]</ref>.</p><p>In summary, this paper makes the following three contributions. First, we identify the technical challenges in performing parallel query processing on GPUs and provide general solutions to address these challenges. Our GPU-based data-parallel primitives are applicable to not only joins but also other query operators. Second, we design and implement several representative join algorithms on the new-generation GPUs and empirically evaluate these algorithms in comparison with the optimized CPU-based join algorithms. To the best of our knowledge, this is the first attempt to develop relational joins on graphics processors. Third, we discuss the lessons we have learned from experience and provide insights and suggestions on GPU programming for the GPGPU and database communities.</p><p>The remainder of this paper is organized as follows. In Section 2, we briefly introduce the GPU architecture and review GPU-and CPU-based query processing techniques and parallel join algorithms. In Section 3, we describe the technical challenges of performing parallel query processing on GPUs, and present our solutions. These solutions are then used as building blocks for our join algorithms, which are described in Section 4. We experimentally evaluate our algorithms in Section 5. We discuss the lessons learned from our experience in Section 6, and conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARY AND RELATED WORK</head><p>In this section, we introduce the GPU architecture and discuss related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graphics Processors (GPUs)</head><p>GPUs are widely available as commodity components in modern machines. They are used as co-processors for the CPU <ref type="bibr" target="#b0">[1]</ref>. GPU programming languages include graphics APIs such as OpenGL <ref type="bibr" target="#b27">[28]</ref> and DirectX <ref type="bibr" target="#b5">[6]</ref>, and GPGPU languages such as NVIDIA CUDA <ref type="bibr" target="#b26">[27]</ref>, AMD CTM <ref type="bibr" target="#b1">[2]</ref>, Brook <ref type="bibr" target="#b7">[8]</ref> and Accelerator <ref type="bibr" target="#b36">[37]</ref>.</p><p>With these APIs, programmers write two kinds of code, the kernel code and the host code. The host code runs on the CPU to control the data transfer between the GPU and the main memory, and to start kernels on the GPU. The kernel code is executed in parallel on the GPU. A general flow for a computation task on the GPU consists of three steps. First, the host code allocates GPU memory for input and output data, and copies input data from the main memory to the GPU memory. Second, the host code starts the kernel on the GPU. The kernel performs the task on the GPU. Third, when the kernel execution is done, the host code copies results from the GPU memory to the main memory.</p><p>The GPU architecture model is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Such architecture is a common design for both AMD <ref type="bibr" target="#b1">[2]</ref>[7] and NVIDIA GPUs <ref type="bibr" target="#b26">[27]</ref>. At a high level, the GPU consists of many SIMD multi-processors. At any given clock cycle, each processor of a multiprocessor executes the same instruction, but operates on different data. The GPU has a large amount of device memory, which has high bandwidth and high access latency. For example, the G80 GPU has an access latency of 200 cycles and the memory bandwidth of 86 GB/second. Additionally, each multiprocessor usually has a fast on-chip local memory, which is shared by all the processors in a multi-processor. The size of this local memory is small and the access latency is low. This model is applicable to both AMD's CTM <ref type="bibr" target="#b1">[2]</ref>[7] and NVIDIA's CUDA <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Device memory P1 P2 Pn</head><p>GPU threads are different from CPU threads in that they have low context-switch and low creation time as compared to their CPU counterparts. On the GPU, threads on each multiprocessor are organized into thread groups. These thread groups are dynamically scheduled on the multiprocessors. Threads within a thread group share computation resources such as registers on a multiprocessor. Moreover, when multiple threads in a thread group access consecutive memory addresses, these memory accesses are grouped into one access. This hardware feature is called coalesced access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Query Processing on GPUs</head><p>Recently, GPUs have been used to accelerate scientific, geometric, database and imaging applications. For an overview on the state-of-the-art GPGPU techniques, we refer the reader to the recent survey by Owens et al. <ref type="bibr" target="#b29">[30]</ref>. We now briefly survey the techniques that use GPUs to improve the performance of database operations.</p><p>Sun et al. <ref type="bibr" target="#b35">[36]</ref> used the rendering and search capabilities of GPUs for spatial selection and join operations. Bandi et al. <ref type="bibr" target="#b4">[5]</ref> implemented GPU-based spatial operations as external procedures to a commercial DBMS. Govindaraju et al. presented novel GPUbased algorithms for relational operators including selections, aggregations <ref type="bibr" target="#b14">[15]</ref> as well as sorting <ref type="bibr" target="#b13">[14]</ref>, and for data mining operations such as computing frequencies and quantiles for data streams <ref type="bibr" target="#b15">[16]</ref>. The existing work mainly develops OpenGL/DirectX programs to exploit the specialized hardware features of GPUs. In contrast, we focus on GPU-based algorithms for the join operation, which is a core operator in relational databases. Moreover, our algorithms are based on a many-core SIMD architecture model of the GPU, and thus can be applied to CPUs of a similar architecture. Based on a similar model, Sengupta et al. <ref type="bibr" target="#b32">[33]</ref> implemented the segmented scan using the scatter. He et al. <ref type="bibr" target="#b18">[19]</ref> proposed a multi-pass scheme to improve the scatter and the gather operations on the GPU. Our algorithms utilize these operations as primitives to compose join algorithms. Most recently, Lieberman et al. <ref type="bibr" target="#b22">[23]</ref> implemented a similarity join using CUDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">In-Memory Query Processing on CPUs</head><p>Memory stalls are an important factor for the overall performance of relational query processing <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b33">[34]</ref>. Cache-conscious techniques have been the leading approach to improve the memory performance of the CPU joins. Shatdal et al. <ref type="bibr" target="#b33">[34]</ref> proposed the blocked NINLJ algorithm by applying cache blocking on the nested-loop join. In comparison, we determine the block size in NINLJ by the size of the local memory. Rao et al. <ref type="bibr" target="#b30">[31]</ref> proposed a cache-optimized B+-tree index, namely the CSS-tree. A CSS-tree has a node size equal to the cache block size. Each node is fully packed with keys. Pointers are eliminated by laying out nodes contiguously, level by level. Index search is done through address arithmetic. We adopt this tree index to the GPU and optimize its performance by fitting the top levels of the tree index into the local memory. Lamarca et al. <ref type="bibr" target="#b21">[22]</ref> studied the cache performance for the quick sort and showed that cache optimizations can significantly improve the overall performance. In comparison, we implement the quick sort on the GPU and use bitonic sort to sort partitions that fit into the local memory. Boncz et al. <ref type="bibr" target="#b9">[10]</ref> proposed the radix hash join with a multi-pass partitioning method in order to optimize the cache performance. Our GPU-based hash join is a parallel version of the radix hash join with optimizations for the local memory.</p><p>With the same goal of reducing memory stalls, our local memory optimization aims at improving the spatial locality and temporal locality of the data accesses. In contrast with the hardwaremanaged cache on the CPU, our techniques are specifically designed for the local memory on GPUs, which is manipulated by the programmer and is shared by multiple threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Parallel Joins</head><p>Parallel algorithms greatly improve the performance of the relational join in shared-nothing systems <ref type="bibr" target="#b23">[24]</ref>[32] or sharedmemory systems <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b24">[25]</ref>.</p><p>Liu et al. <ref type="bibr" target="#b23">[24]</ref> investigated the pipelined parallelism for multi-join queries. In comparison, we focus on exploiting the parallelism within a single join operation. For a single join, Lu et al. <ref type="bibr" target="#b24">[25]</ref> studied four hash-based join algorithms on a shared-memory multiprocessor system. Schneider et al. <ref type="bibr" target="#b31">[32]</ref> evaluated one sortmerge and three hash-based join algorithms in a shared-nothing system. In the presence of data skews, techniques such as bucket tuning <ref type="bibr" target="#b31">[32]</ref> and partition tuning <ref type="bibr" target="#b20">[21]</ref> are used to balance loads among processor nodes. Azadegan et al. <ref type="bibr" target="#b2">[3]</ref>[4] used machinespecific communication primitives to develop parallel join algorithms on the SIMD Connection Machine (CM-2). Recently, Cieslewicz et al. <ref type="bibr" target="#b10">[11]</ref> implemented a multi-threaded hash join using the atomic operations supported in the Cray MTA-2 architecture.</p><p>In comparison with previous parallel join algorithms, our GPUbased parallel join algorithms take into account the GPU architectural characteristics and provide general, yet efficient solutions. Specifically, in contrast with using machine-specific primitives <ref type="bibr" target="#b2">[3]</ref>[4], we develop software primitives that are general and highly scalable for GPUs. Additionally, our thread parallelism does not require hardware-supported atomic operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRIMITIVES</head><p>Based on the GPU architectural model, we have identified three technical challenges in join processing on GPUs: • How to efficiently utilize both the computation resource and the memory bandwidth of the GPU, and to use parallel computation to hide memory latency. This challenge is critical in that joins are both computation and data intensive.</p><p>Even though the GPU has massive thread parallelism and high memory bandwidth, its memory latency is also high. Therefore, we need to examine individual join algorithms and develop common building blocks that improve data parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>How to handle read/write conflicts efficiently. Since we do not have hardware-supported atomic operations for conflict handling, we need to develop an efficient conflict handling mechanism that is suitable for GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>How to handle data skews on GPUs. As on any parallel architecture, data skews must be handled effectively to balance the workload among processors so as to improve the overall performance. We address these challenges in primitives, a small set of common operations that we design for join processing on the GPU. These primitives exploit the hardware features of the GPU and can be used for database query processing, including joins. Notation. In this paper, we consider a join on two relations R and S with a single join attribute. We assume the join attribute to be an integer for simplicity. R[i] represents the ith tuple of R. The notations used throughout this paper are summarized in Table <ref type="table">1</ref>.</p><p>Table <ref type="table">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline Design</head><p>We aim at designing and implementing a complete set of parallel primitives for relational query processing. In this section, we describe our primitives, namely map, scatter, gather, prefix scan, split and sort. These primitives are used as constructs for our join algorithms and have the following features: 1) They have low synchronization overhead, thus achieving close to peak performances on GPUs. 2) They are scalable to hundreds of processors. 3) They are applicable not only to joins but also to other relational query operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Map</head><p>A map is similar to a database scan. We define the map primitive as follows:</p><p>We use multiple thread groups to implement the map. Each thread group is responsible for a segment of the relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Scatter and Gather</head><p>We adopt the definitions of scatter and gather used by He et al. <ref type="bibr" target="#b18">[19]</ref>. A scatter performs indexed writes to a relation, for example, hashing. Its definition is as follows, where the array L defines the distinct write location for each R in tuple.</p><p>The gather primitive performs indexed reads from a relation. It can be used, for instance, when fetching a tuple given a record id, and probing hash tables. Its definition is as follows, where the array L defines the read location for each R in tuple.</p><p>In general, tuples in the output relation can be a superset or a subset of the input relation in gather and scatter. For simplicity, our definitions assume the tuples in the output relation are the same set as those in the input relation. We implemented the scatter and the gather using the multi-pass optimization scheme proposed by He et al. <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Prefix scan</head><p>A prefix scan applies a binary operator to the input relation of size n and generates an output relation of size n <ref type="bibr" target="#b29">[30]</ref>. We present the definition of a prefix scan that applies the binary operator ⊕ to the input relation as follows:</p><p>An example of prefix scan is the prefix sum, which is an important operation in parallel databases <ref type="bibr" target="#b8">[9]</ref>: Given an input relation (or array) R in , the value of each output array element</p><formula xml:id="formula_0">R out [i] ( | | 2 in R i ≤ ≤ ) is obtained from the sum of R in [1],..., and R in [i-1] (R out [1]=0).</formula><p>We use the prefix sum implementation from the CUDA library <ref type="bibr" target="#b26">[27]</ref>. The prefix sum has two stages, reduce and down-sweep. The reduce stage has</p><formula xml:id="formula_1">| in R | 2 log steps. In step i ( | in |R i 2 log 0 &lt; ≤ ), thread j computes the partial sum of R in [j*2 i ] and R in [(j+1)*2 i ]. The down- sweep stage also takes | in R | 2 log steps. In step i ( | in |R i 2 log 0 &lt; ≤ ), the partial sum is applied to R in [j*2 i ] and R in [(j+1)*2 i ].</formula><p>Both stages are highly parallel on the GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Split</head><p>A split primitive divides a relation into a number of disjoint partitions according to a given partitioning function. The result partitions are stored in the output relation. Splits are used in hash partitioning or range partitioning. Given the partitioning fanout F, the definition of the split is as follows:</p><p>A basic implementation is that each thread processes a portion of the input relation and inserts tuples to their target partitions. A major issue is the write conflicts among threads. They occur when multiple threads try to insert tuples into a partition concurrently. Unfortunately, there are no atomic operations such as locks for handling such conflicts on most GPUs. Thus, we propose a software approach to implement a lock-free split algorithm. The basic idea is that, prior to writing the output, we use histograms to compute the write locations of each thread. Since each thread knows its target position to write, the write conflicts among threads are avoided.</p><p>Our histogram-based algorithm is partially inspired by the parallel radix sort proposed by Zagha <ref type="bibr" target="#b38">[39]</ref>, which uses histograms to perform the radix sort. The major difference is that our histogram scheme is embedded in our primitives on the GPU. In particular, our split algorithm uses the histogram to compute the write location for each tuple (stored in the array L) and scatters R in to R out according to the array L. (1)</p><p>Each thread constructs its tHist histogram from Rin.</p><p>Each thread writes its histogram to</p><formula xml:id="formula_3">L so that L[(p-1)*#thread+t]= tHist[t][p].<label>(3)</label></formula><p>Perform a prefix sum on L. The result is stored in L. (4)</p><p>Each thread updates its offset so that tOffset</p><formula xml:id="formula_4">[t][p]=L[(p- 1)*#thread+t].<label>(5)</label></formula><p>Each thread scatters its tuples to Rout based on its offset.</p><p>Each thread group is responsible for a similar-sized portion of R in .</p><p>Each thread maintains a thread-level histogram (tHist[1…F]). It records the number of tuples of each partition for the thread. We use the thread-level histogram to compute the thread-level offset array (tOffset[1…F]), which contains the start position for outputting the tuples belonging to the partition for the thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Primitive: Gather</head><formula xml:id="formula_5">Input: R in [1, …, n], L [1, …, n]. Output: R out [1, …, n]. Function: R out [i]=R in [L[i]], i=1, …, n. Primitive: Scatter Input: R in [1, …, n], L [1, …, n]. Output: R out [1, …, n]. Function: R out [L[i]]=R in [i], i=1, …, n. Primitive: Map Input: R in [1, …, n], a map function fcn. Output: R out [1,…,n]. Function: R out [i]=fcn(R in [i]). Primitive: Split Input: R in [1, …, n], F] [1,..., [i]) in func(R ∈ , i=1, …, n. Output: R out [1, …, n]. Function: {R out [i], i=1,…, n}={R in [i], i=1, …, n}<label>and</label></formula><formula xml:id="formula_6">j i n], [1,.., j i, [j]), out func(R [i]) out func(R ≤ ∈ ∀ ≤ . Primitive: Prefix Scan Input: R in [1, …, n], binary operator ⊕ . Output: R out [1,…,n]. Function: R out [i]= ⊕ j&lt;i R in [j].</formula><p>Our split works in five steps, as illustrated in Algorithm 1. The five steps are implemented using our other primitives. The first step is implemented using a map primitive; the third one uses prefix scan; the forth one uses a gather; and the other two use scatter. Figure <ref type="figure" target="#fig_2">2</ref> shows an example of our split operation, where we divide R in into two partitions. The arrows represent how the data is loaded and stored. In this example, there are two thread groups, one containing T1 and T2 and the other containing T3 and T4. The portions of R in processed by different threads are in different shades. In the first step, there are four thread-level histograms.</p><p>Step (2) creates the histograms and outputs them to L. Step (3) uses prefix sum to compute the offset array of each thread. For example, the start positions for writing the tuples belonging to the first and the second partitions are 0 and 4 respectively for thread T1. With these offsets, the write locations of the four threads are deterministic, and tuples can be output in parallel.</p><formula xml:id="formula_7">2 1 2 2 1 1 1 2 T1 T2 T3 T4</formula><p>Step (1), count</p><formula xml:id="formula_8">1 1 0 2 2 0 1 1 tHist</formula><p>Step (3), prefix sum</p><p>Step (4), load counts</p><formula xml:id="formula_9">Rin Rout 1 1 1 1 2 2 2 2</formula><p>Thread group 1 Thread group 2</p><formula xml:id="formula_10">L 1 0 2 1 1 2 0 1 0 1 1 3 4 5 7 7 L 0 4 1 5 1 7 3 7 2 1 2 2 1 1 1 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rin tOffset</head><p>Step (2), output counts</p><p>Step (5), scatter</p><p>For partition 1</p><p>For partition 2</p><p>For partition 1</p><p>For partition 2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.5">Sort</head><p>The sort primitive is used in a number of operators such as aggregation and join operators.</p><p>We have implemented two comparison-based sorting algorithms including the bitonic sort and the quick sort. The bitonic sort uses the GPU-based bitonic sorting network <ref type="bibr" target="#b13">[14]</ref>, because independent swaps between the elements in this sorting algorithm map well to the massively threaded architecture of GPU. However, the complexity of the bitonic sort is</p><formula xml:id="formula_11">N) log 2 O(N</formula><p>, where N is the number of tuples to be sorted. In contrast, the complexity of the quick sort is O(NlogN) , which is lower than the bitonic sort. With the split primitive, the quick sort can be implemented on the GPU. Bitonic sort. The bitonic sort merges bitonic sequences in multiple stages. A bitonic sequence is of a monotonic ascending or descending order. Given a relation R in , the bitonic sorting algorithm has</p><formula xml:id="formula_12">| in R | 2 log stages. Stage x has x steps ( | in R | 2 log x 1 ≤ ≤</formula><p>). In Step i, it constructs bitonic sequences each of size i 2 . Thus, Stage x generates the bitonic sequences each of size x 2 . After</p><formula xml:id="formula_13">| R | 2 log</formula><p>stages, R is sorted. Each step of the bitonic sort performs a map on the input relation and a scatter to output the results.</p><p>Quick sort. The quick sort has a lower complexity than the bitonic sort. Moreover, it uses the efficient split primitive. The quick sort has two steps. First, given a set of pivots, we use the split primitive to divide the relation into multiple chunks. The pivots are chosen randomly <ref type="bibr" target="#b11">[12]</ref>. The split process goes recursively until each chunk is smaller than a preset threshold for the chunk size. (We discuss this preset threshold in Section 3.2.2.) After the split process, we use the bitonic sort on each chunk. We choose the bitonic sort other than the insertion sort, because the bitonic sort can work entirely in the local memory and its computation maps well to the parallel execution of the GPU. We present the local memory optimization in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Memory Optimizations</head><p>Our primitives are developed based on the many-core architecture model. The thread-level parallelism reduces the memory stalls in these primitives. However, thread-level parallelism may not completely hide the memory stalls for database workloads <ref type="bibr" target="#b16">[17]</ref>. Thus, we utilize two memory optimization techniques on the GPU to further reduce the memory stalls: coalesced access to improve spatial locality and local memory optimization to improve temporal locality. Frequently accessed data are stored in the local memory to reduce the accesses to the device memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Coalesced Access</head><p>We use the map primitive to illustrate how we take advantage of the coalesced access.</p><p>The coalesced access improves the memory bandwidth utilization. Figure <ref type="figure" target="#fig_3">3</ref> illustrates two map schemes with and without coalesced accesses. Suppose a thread group consists of three threads. In Figure <ref type="figure" target="#fig_3">3</ref> (a), due to the SIMD nature of GPUs, the accesses to the device memory among the threads are consecutive during the execution. Every three concurrent accesses are coalesced into a single read. In Figure <ref type="figure" target="#fig_3">3</ref> (b), the accesses among threads are not consecutive. Each thread issues a distinct memory request. This results in low utilization of the memory bandwidth. Suppose every k memory requests are merged into a single request, the number of memory requests with the coalesced access is (k-1) times less than that without the coalesced access. With this optimized map primitive, the memory performance of the bitonic sort is greatly improved. Similarly, the memory accesses in steps ( <ref type="formula" target="#formula_2">2</ref>) and ( <ref type="formula">4</ref>) of the split primitive are also designed as coalesced ones.</p><formula xml:id="formula_14">Primitive: Sort Input: R in [1, …, n]. Output: R out [1, …, n]. Function: {R out [i], i=1,…, n}={R in [i], i=1, …, n} and j i and n] [1,.., j i, [j], out R [i] out R ≤ ∈ ∀ ≤ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Local Memory Optimization</head><p>We use the split and the sort algorithms including the bitonic sort and the quick sort as examples to illustrate the local memory optimization. In the split, each tuple accesses the histogram and the offset array. We store these arrays in the local memory. Due to the limited size of the local memory, we determine the maximum partitioning fanout for the split. Suppose the number of partitions is f and each element in the array is encoded in z bytes. We determine F to be the maximum f such that</p><formula xml:id="formula_15">M z f T ≤ ⋅ ⋅</formula><p>. To divide a relation into an arbitrary number of partitions, x, we apply the split operation recursively. The number of levels in the recursion is  </p><p>x F log , and we uniformly set the number of partitions generated in each level of recursion.</p><p>The bitonic sort has repetitive fetches on the device memory. We propose two optimization techniques on the local memory to improve its temporal locality. These two optimizations are illustrated in Figure <ref type="figure" target="#fig_5">4</ref> ).</p><p>These steps sort a bitonic sequence of size M. We store this bitonic sequence into the local memory at the (i-c)th step so that Steps c, c-1, …, and 1 process the data in the local memory. This saves (c-1) fetches in Stage i ( In the quick sort, since we use the bitonic sort to sort each chunk after the partitioning step, the preset threshold for the chunk size is the local memory size. Since each chunk is smaller than the local memory, the bitonic sort performs completely within the local memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">JOIN ALGORITHMS</head><p>We now briefly describe our join algorithms, including the nonindexed and indexed nested-loop join (NINLJ and INLJ respectively), the sort-merge join (SMJ) and the hash join (HJ).</p><p>Since the GPU-based algorithms are similar to their CPU-based counterparts, we focus on the differences in our GPU-based implementations, especially their usage of our primitives. Specifically, NINLJ uses the map primitive on both relations; INLJ uses the map primitive on the outer relation; SMJ uses the sort on both relations and then maps the sorted relation for merging; HJ uses the split primitive on both relations. The result output of each join algorithm uses the prefix scan and the scatter primitives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Join Processing</head><p>We describe the join processing of each join algorithm. Since the scheme for outputting the join result is the same for the four join algorithms, we present the result output in Section 4.2.</p><p>Non-indexed NLJs (NINLJ). Our algorithm is blocked nestedloops. The nested-loop join can be naturally mapped to our GPU model, as shown in Figure <ref type="figure" target="#fig_6">5</ref>. The circles represent tuples generated by the join, some of which may be eliminated by the join predicate.</p><p>Each thread group computes the join on a portion of R and S, denoted as R' and S', respectively. Within a thread group, each thread processes the join on one tuple from R' and all tuples from S'. The joins of the tuple from R' and other tuples from S are computed in other thread groups. Thus, the number of threads in each thread group is equal to the number of tuples in R' (</p><formula xml:id="formula_16">T | R' | = ).</formula><p>Within the join of R' and S', we store S' into the local memory to avoid reading S' repeatedly from the device memory. Due to the limited size of the local memory, the size of S' is set to the local memory size. Since each thread group requires to access R' and S' only once from the device memory, the total volume of data transfer between the GPU and the device memory is Indexed NLJs (INLJ). We implement the indexed join algorithm through adapting the cache-optimized search tree, CSS-Tree <ref type="bibr" target="#b30">[31]</ref>, to the GPU. Different from traditional B+-Trees that use discrete memory pointers for tree traversal, CSS-trees store the entire tree in an array and tree traversal is performed via address arithmetic. This effectively trades off more computation for less memory access, which makes it a suitable index structure to utilize the GPU's computational power.</p><formula xml:id="formula_17">s) M s r (T M T | S | | R | ||) S' || || R' (|| | S' | | R' | | S | | R | ⋅ + ⋅ ⋅ ⋅ ⋅ = + ⋅ ⋅ .</formula><p>A CSS-Tree can be efficiently constructed on the GPU taking a sorted relation as input. In the presence of the tree index, the indexed join consists of two major steps, searching for the first occurrence of matching tuples in the indexed relation, and then accessing the indexed relation for join results. While searching for a single key in such a tree offers little opportunity for parallel processing, multiple searches, however, fit extremely well into the parallel programming model. Therefore, multiple keys are searched in parallel on the tree. Given a relation R, the search starts at the root node and steps one level down the tree in each iteration until reaching the data nodes on the bottom. A binary search or a sequential search is used to locate the index of the node to go. The binary search has fewer comparisons but has more branch divergence among the threads than the sequential search. We empirically evaluated these two search methods in Section 5.4.</p><p>Since the upper levels of the tree index are frequently accessed, we replicate the upper levels of the tree index to the local memory. Given the tree fanout f, and tree node size z, the total size of tree nodes in the upper l levels is</p><formula xml:id="formula_18">1 f 1 l f z - - ⋅</formula><p>. We compute the number of levels that can be replicated into the local memory as the maximum l such that</p><formula xml:id="formula_19">M 1 f 1 l f z ≤ - - ⋅ .</formula><p>Sort-Merge Joins (SMJ). Similar to the traditional sort-merge joins, we first sort the two relations and then perform a merge step on these two sorted relations.</p><p>The merge step is done in parallel to fully utilize the computation resources. The basic idea is to perform the merge on a chunk of S and its matching chunk of R independently. The merge is performed in three steps. First, we divide the smaller relation S to be Q chunks (</p><formula xml:id="formula_20">M ||S|| Q =</formula><p>). The size of each chunk (except the last chunk) is M so that each chunk fits into the local memory. Second, we use the key values of the first and the last tuples of each chunk in S to identify the start and the end positions of its matching chunks in R. Third, we merge each pair of the chunk in S and its matching chunk in R in parallel. Each thread group is responsible for a pair. In the merge process, the S chunk is first loaded into the local memory. Next, the R tuples are used to find the matching results. Each thread reads a tuple from R and performs a search on the S chunk for matching. Either a sequential search or a binary search can be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hash joins (HJ).</head><p>We develop a parallel version of the radix hash join <ref type="bibr" target="#b9">[10]</ref>. The radix partitioning is implemented using our split primitive. Our algorithm has two phases.</p><p>Phase 1) Partitioning. We split R and S into the same number of partitions using</p><formula xml:id="formula_21">) / || (|| 2 log M S</formula><p>radix bits so that most S partitions fit into the local memory. The join on R and S is decomposed into multiple small joins on an R partition and its corresponding S partition.</p><p>Phase 2) Matching. We choose the smaller one of the R and S partitions as the inner partition to be loaded into the local memory, and the other one as the outer partition. Each tuple from the outer partition uses a sequential search or a binary search on the inner partition for matching. If the binary search is used, we use the bitonic sort to sort the inner partition prior to probing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A Lock-Free Scheme for Result Output</head><p>Since the GPU lacks incremental memory allocation on the device memory during the kernel execution, the result output has two major problems. The first one is the unknown join result size. One may consider estimating the (maximum) number of results for each thread. However, since the maximum number of results for the join of m by n tuples is n m× , this upper bound usually exceeds the size of the device memory. The second one is that write conflicts occur when multiple threads write results to the shared output region in the device memory. We propose a three-phase scheme to solve these two problems.</p><p>First, each thread counts the number of join results for the partitioned join it is responsible for. A counter is maintained locally. There is no conflict in this step, because no threads write the actual join result.</p><p>Second, we compute a prefix sum on the counters to get an array of write locations, each of which is the start location in the device memory for the corresponding thread to write. Through the prefix sum, we also know the total number of results generated by the join.</p><p>Third, the host code allocates a memory area of the exact size of the join result and each thread outputs the join results to the device memory according to its start write location. Since each thread has its deterministic positions to write to, any write conflicts are avoided. If the size of the join result is larger than the device memory, we output the join results in multiple passes. In each pass, we output the join results from a portion of the threads. This three-phase scheme does not require the hardware support of atomic functions. However, it requires evaluating the join predicates twice. Fortunately, with the GPU's high computation power, the extra join predicate evaluation poses little overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Skew Handling</head><p>In the partitioning-based algorithms such as SMJ and HJ, the skew in the data results in an imbalanced partition size. The processing of an inner partition that is larger than the local memory requires accesses to the device memory. Consequently, it may suffer from the memory stall and hurt the overall performance.</p><p>The first problem is to identify the partitions that do not fit into the local memory. Taking the input array of partition sizes (i.e., the element i in the array is the size of the ith partition), we use the split primitive to divide the partitions into two groups, one for the partitions larger than the local memory and the other for those not larger than the local memory.</p><p>Once we identify the partitions that are larger than M, we further decompose each of these partitions into multiple chunks each of size M, and process these generated small chunks in the local memory. For the SMJ, we perform a merge step on all possible matching pairs of chunks. For the HJ, we use the NINLJ on each matching pair of chunks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section, we evaluate the performance of our proposed GPU primitives and join algorithms in comparison with the algorithms on the CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We have implemented and tested our algorithms on a PC with a NVIDIA 8800 GTX GPU and a recently-released Intel Core2 Duo Quad-Core processor. The hardware configuration of the PC is shown in Table <ref type="table" target="#tab_1">2</ref>. The GPU uses a PCI-EXPRESS bus to transfer data between the main memory and the device memory with a theoretical bandwidth of 4 GB/s. We compute the theoretical bandwidth to be the bus width multiplied by the memory clock rate. Thus, GPU and CPU have a theoretical bandwidth of 86.4 GB/s and 10.4 GB/s, respectively. Based on our measurements, the G80 achieves a memory bandwidth of around 69.2 GB/s whereas the quad-core CPU has 5.6 GB/s. Considering different parameters in our workload, we performed three sets of experiments on the equijoin query. First, we fixed the size of R and varied the size of S. The key values of R and S are uniformly distributed. For NINLJ, we fixed |R| to be 1 million; while for the other three joins, we fixed |R| to be 16 million. Second, we examined the performance impact of varying the join selectivity. Third, we investigated our algorithms on the nonuniform data sets. In the later two sets of experiments, we fixed both |R| and |S| to be one million for NINLJ. For the other three joins, we fixed both |R| and |S| to be 16 million. This is our default experimental setting for data sizes unless specified otherwise. These settings were chosen to be similar to the previous studies on in-memory join algorithms <ref type="bibr" target="#b9">[10]</ref>. Finally, we varied the δ value in the non-equijoin predicate and examined the performance of nonequijoins.</p><p>In addition to supporting the regular data types such as integers, our primitives and join algorithms support more complex data types such as strings. We support more complex data types through indirection by storing offsets and lengths in our record. Specifically, the values of the field of all tuples are consecutively stored into an array named data array. We represent the field of each tuple using a pair (offset, length), where offset is the start position of the value in the data array and length is the length of the value (in bytes). The value is fetched according to the offset and the length. If two tuples need to be swapped, we swap them without modifying the data array.</p><p>We run each experiment ten times and report the average value.</p><p>Implementation details on CPU. For comparison, we have implemented highly optimized CPU-based primitives and join algorithms. The primitives are designed to be parallel and run on the quad-core machine. We use cache optimization techniques <ref type="bibr" target="#b33">[34]</ref> to fine tune the performance of the parallel implementation. With these optimized primitives, we implement four join algorithms including the blocked NINLJ <ref type="bibr" target="#b33">[34]</ref>, the INLJ with the CSS-tree index <ref type="bibr" target="#b30">[31]</ref>, the SMJ with the optimized quick sort <ref type="bibr" target="#b21">[22]</ref> and the radix HJ <ref type="bibr" target="#b9">[10]</ref>. We compiled our algorithms using MSVC 8.0 with full optimizations. Moreover, we used OpenMP <ref type="bibr" target="#b28">[29]</ref> to implement the threading mechanism on the CPU. In general, the parallel CPU-based primitives and join algorithms are 2-6X faster than the sequential ones on the quad-core CPU. To check whether our CPU-based implementation has a comparable performance with state-of-the-art main memory databases, we also performed a performance comparison between our algorithms and MonetDB <ref type="bibr" target="#b25">[26]</ref>. The comparison was done on the core query processing algorithms, excluding the other components such as query parsing and plan generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation details on GPU.</head><p>We implemented our primitives and join algorithms using CUDA <ref type="bibr" target="#b26">[27]</ref> and DirectX10 <ref type="bibr" target="#b5">[6]</ref>. DirectX is a common graphics API runnable on most GPUs including AMD's and NVIDIA's. In contrast, CUDA is a GPGPU programming framework for recent NVIDIA GPUs. In the CUDA programming API, the developer can program the GPUs without any knowledge of graphics rendering APIs. Similar abstractions are also available on AMD GPUs using their compute abstraction layer (CAL) API. Both of these APIs expose a general-purpose, massively multi-threaded parallel computing architecture and provide a programming environment similar to multi-threaded C/C++. Since the performance results of the DirectX implementation are similar to those of CUDA, we discuss our implementation and results of CUDA in detail and briefly present the results for DirectX.</p><p>Our GPU-based joins can be easily mapped to the CUDA framework. To implement an efficient algorithm using CUDA, we need to determine the following parameters with respect to the target operation: the number of threads for each thread group (T) and the number of thread groups (Bp). Issuing more threads to the GPU can potentially improve the overall performance by hiding memory latency at the cost increasing register pressure. Moreover, due to the limited local memory space on the multiprocessors, Bp and T cannot be arbitrarily large. Through experiments, we find that Bp=128 and T=64 are a good tradeoff value, where the memory latency is sufficiently masked, and each block receives adequate computation resources. Taking the search performance and data transfer into account, we set the number of keys in a node of the GPU CSS-tree to be 32.</p><p>The DirectX10 programmable pipeline contains multiple stages including the Vertex Shader (VS), Geometry Shader (GS) and Pixel Shader (PS). For each algorithm, we draw a set of points corresponding to a tuple processing. The map, the gather and the scatter are implemented as vertex texture fetches and positioning in the VS. These APIs have inherent thread parallelism and achieve a similar performance to the CUDA implementation. The prefix scan is adopted from Horn's algorithm <ref type="bibr" target="#b19">[20]</ref> and our sorting algorithm is the bitonic sort <ref type="bibr" target="#b13">[14]</ref>. We implement the split primitive as two steps, first sorting the tuples according to their partition identifiers, and next scattering these tuples with Min and Max blending to obtain the start and the end positions of each partition. The matching process is performed by drawing points from the outer relation, each point probing the inner relation. Since the local memory is not exposed to DirectX, NINLJ stores the inner block in the constant buffer, a fast on-chip cache exposed to DirectX. INLJ probes the texture storing the CSS-tree using the search keys of the outer relation. The fanout of the CSStree is set to four, which is the number of color channels on the GPU for parallel comparison. SMJ sorts the texture of the inner relation and performs binary search for matching results. HJ builds the hash table for the inner relation in the texture using the split and renders the outer relation to probe the textures of the hash table. Unlike the three-phase result output scheme in the CUDA implementation, the DirectX implementation utilizes the stream-out feature of the GS to output the join results in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Transfer between Device Memory and Main Memory</head><p>Figure <ref type="figure" target="#fig_7">6</ref> shows the memory copy time from the main memory to the device memory. Similar results are obtained for data transfer from the device memory to the main memory. Given a certain block size, we transfer the data block by block. Due to the overhead associated with each transfer, the copy time increases as the block size decreases. When the block size is larger than 4MB, the copy time remains almost constant. That means when the relation size is larger than 4MB, the bandwidth is fully utilized. These results suggest that the programmer could batch small data transfers to reduce the time of data transfer between the GPU and the CPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on Primitives</head><p>Since GPU-based primitives are usually used as intermediate components in the GPU-based join algorithms, their input data are already in the device memory and their output data are stored in the device memory as input to other primitives. Thus, we exclude the time of data transfer between the GPU and the CPU in the results for the primitives on the GPU. Table <ref type="table" target="#tab_2">3</ref> shows the elapsed time of optimized primitives when |R| is fixed to be 16 million. The locations in the scatter and the gather are random. For the prefix scan, we compute the prefix sum on 16 million integers. The split function for the split is fcn(x) = x mod 64. That means the split divides the relation into 64 partitions.</p><p>We define the speedup to be the ratio of the execution time on the CPU to that on the GPU. Overall, the GPU-based primitives achieve a performance speedup of 2-27X over the CPU-based primitives. We obtained similar performance speedup with the data size varied. This speedup is due to the high parallelism and the two memory optimizations. We have the following four observations. First, the average bandwidth of the optimized map primitive is 2.4GB/sec and 64GB/sec on the CPU and the GPU, respectively. The speedup of the optimized GPU map is 27X over the CPU-based map.</p><p>Additionally, it has a high bus utilization of 75%, given the theoretical bandwidth of 86GB/sec. Second, the scatter and the gather have a much lower bandwidth than the map due to their random access nature. Third, in the split on both the GPU and the CPU, the scatter takes over 70% of the total execution time. Forth, the speedup of the GPU-based quick sort algorithm is 2X over the optimized quick-sort on the quad-core CPU. Comparing the two GPU-based sorting algorithms, we find that the quick sort is around 30% faster than the bitonic sort (the results is not shown in the table). This result is consistent with the fact that the quick sort has lower complexity than the bitonic sort. We used the quick sort as our sorting primitive in the CUDA implementation.</p><p>The speedups on the scatter, gather and prefix scan primitives are similar to those in the previous work <ref type="bibr">[19][33]</ref>. Thus, we discuss the map, split and sort primitives in more detail. Specifically, we studied the performance impact of the three optimizations on the GPU. Coalesced access. Figure <ref type="figure" target="#fig_8">7</ref> shows the performance of the map primitive with the relation size varied. To isolate the performance impact of the coalesced access from the thread parallelism, we set Bp=16 and T=32, which is equal to the number of multiprocessors and the number of threads in a schedule unit of G80, respectively. The coalesced access improves the map bandwidth on the GPU by a factor of about two. Note, the bandwidth of the coalesced map is 4.5GB/sec, which is far lower than the theoretical bandwidth due to the absence of high parallelism and the local memory optimization.</p><p>Thread parallelism. Figure <ref type="figure" target="#fig_9">8</ref> shows the elapsed time with a varying number of thread groups for the map and the split primitives. The number of threads in each thread group is fixed to be 32. Since the results for the bitonic sort are similar to those of the map, and the results for the quick sort are similar to the split, the results for the sort are omitted. The map primitive is implemented with coalesced accesses. As Bp is smaller than a threshold value, the elapsed time of both algorithms greatly decreases as the Bp value increases. This is because the memory stalls of accessing the device memory are better hidden by computation and the increase in bandwidth utilization. Since the map is cheaper than the split, the performance impact of increasing the number of thread groups is more significant on the map than on the split. When Bp is larger than the threshold value, the elapsed time slightly increases as Bp increases. The suitable numbers of thread groups are 128 and 64 for the map and the split, respectively.</p><p>After obtaining the suitable number of thread groups, we further varied the number of threads per thread group. The results are shown in Figure <ref type="figure">9</ref>. When T is smaller than a threshold value, the elapsed time decreases as T increases. This indicates memory stalls can be further hidden by increasing the number of threads per thread groups. When T is larger than the threshold value, the performance degrades due to the computation resource contention on the multiprocessor. The suitable numbers of threads per thread group for the map and the split are 64 and 32, respectively. Note, with the optimization of the coalesced access and the thread parallelism, the map primitive achieves a bandwidth of 64 GB/sec.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results on Joins</head><p>Since searching the data in the local memory is a core operation in the join step, we first studied the binary search and the sequential search. Figure <ref type="figure" target="#fig_11">11</ref> compares the performance of our joins with binary search and with sequential search. The result for NINLJ is not shown, because binary search is not used in NINLJ. The result for INLJ does not include the time for constructing the tree index.</p><p>The tree construction time is so small that it can be ignorable on the GPU. Additionally, we observed that the local memory optimization achieved a performance improvement of around 10% on INLJ.</p><p>Although the sequential search takes fewer branches than the binary search, the binary search has fewer data accesses than the sequential search. The binary search improves the performance by 2.5X and 1.5X on INLJ and SMJ, respectively. The binary search has a relatively high speedup in the INLJ, because the search on the tree node is the major operation of INLJ. In contrast, the binary search degrades the performance of HJ due to the overhead of the extra sorting on the local memory.  Table <ref type="table" target="#tab_3">4</ref> shows an end-to-end comparison on the elapsed time of the four relational joins on the GPU and the CPU. The elapsed time on the GPU includes the data transfer time between the device memory and the main memory. Overall, the GPU-based joins have a 2-7X speedup over the CPU-based joins. The high performance speedup is due to the efficient primitives as well as the efficient matching on the data in the local memory.</p><p>Figure <ref type="figure" target="#fig_12">12</ref> shows the time breakdown of the four join algorithms on the GPU. We divide the total execution time of a GPU-based join into three components including the time for copying input data into the device memory, join processing and result output to the main memory. For all join algorithms, the join processing time is dominant. The total time of copying data between the main memory and the GPU memory (one time cost for each join) was around 0.1%, 13%, 4% and 6% of the total execution time of NINLJ, INLJ, SMJ and HJ, respectively. Copying the input relations and outputting the join results are bulk transfers with a block size larger than 4MB. Thus, the bandwidth between the main memory and the device memory is fully utilized.  We studied the join performance with varying workload characteristics. Figure <ref type="figure" target="#fig_13">13</ref> shows the speedup of the GPU-based joins over the CPU-based joins with varying join selectivity and percentage of duplicates in R. The larger the join selectivity is, the larger the join output. The speedup is stable when the join selectivity varies. This result indicates that the data transfer time between the device memory and the main memory has little performance impact on GPU-based joins. As the percentage of duplicates increases, the relation becomes more skewed. The speedup of the SMJ and HJ is stable. This indicates the effectiveness of our skew handling.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison with MonetDB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Results on DirectX Implementation</head><p>Table <ref type="table" target="#tab_5">5</ref> shows the elapsed time of the four relational joins implemented with CUDA and DirectX. For each implementation, we show its total execution time and the time for the join processing only, i.e., the texture copy in/out and encoding/decoding time not included for the DirectX measurement and the data copy in/out is not included for the CUDA measurement. The DirectX-based NINLJ and INLJ achieve a similar performance to their CUDA-based counterparts.</p><p>The GPU pipelines in these DirectX implementations are short and simple. In contrast, the DirectX-based SMJ and HJ are about twice as slow as their CUDA-based counterparts. These DirectX implementations contain more graphics related overhead such as texture coding/decoding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Handling Other Data Types</head><p>Figure <ref type="figure" target="#fig_16">15</ref> shows the performance of our sort primitive on the strings. Each tuple contains two fields, the record ID and the string field. The number of tuples is fixed to be four million, and we varied the average string length. As the string length increases, the variance of the string lengths increases. The performance speedup of our GPU-based sort over the CPU-based sort slightly decreases. One possible reason is the increasing branches in the string matching. Nevertheless, the speedup of the GPU-based quick sort is around 2X over its CPU-based counterpart. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Summary</head><p>In summary, our GPU-based primitives and join algorithms achieve a speedup of 2-27X over their optimized CPU-based counterparts. We evaluated our join algorithms for both equijoins and non-equijoins, different data sizes, join selectivities and data distributions. Generally, INLJ is the suitable join algorithm in the presence of the index, and NINLJ for non-equijoins and HJ for equi-joins otherwise. The performance speedup for the nonindexed NLJ, the indexed NLJ, the sort-merge join and the hash join is over 7.0X, 6.1X, 2.4X and 1.9X, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION</head><p>We first discuss the performance speedups, and next the opportunities and the limitations of query processing on the GPU.</p><p>The performance speedup of our GPU-based join algorithms over quad-core CPU-based join algorithms is resulted from the differences in the architectures as well as the algorithm design. First, the G80 has 18X more total clock cycles and over 12X higher memory bandwidth than the quad-core CPU. The speedups of our join algorithms are smaller than both ratios, mainly due to the inter-thread communication on the GPU. Second, the L2 cache of the quad-core CPU is 32X larger than the local memory on the GPU. Since memory stalls are a significant performance factor, memory optimizations are important in the algorithm design. On the GPU, we utilize the coalesced access to improve the bandwidth utilization, and the local memory optimization for the temporal locality. In comparison, our CPU-based algorithms have only cache optimization for temporal locality. It would be interesting to quantitatively study the performance impact of each individual hardware feature.</p><p>Through designing and implementing relational join algorithms on GPUs, we have identified a number of opportunities and limitations of new-generation GPUs as a database query coprocessor.</p><p>The following are four major opportunities:</p><p>First, GPUs have a highly parallel hardware architecture that fits extremely well with data-parallel query processing. The massive thread parallelism of the GPU hides the memory latency more efficiently than traditional von Neumann architectures. Moreover, the high memory bandwidth and the fast inter-processor communication can significantly accelerate the performance of many database operations.</p><p>Second, the GPU programmability for general-purpose computing has been improving greatly. The AMD CTM and NVIDIA CUDA APIs extend the functionality of GPUs for the high-performance computing market in addition to the traditional gaming market.</p><p>Third, with the new architectural features and the improved general-purpose programmability, new-generation GPUs allow us to utilize traditional wisdom from both the GPU programming model and the CPU-based query processing techniques. Specifically, we adapt CPU-based optimization techniques to the GPU hardware features in order to reduce memory stalls of the primitives and the join algorithms on the GPU.</p><p>Fourth, our primitive-based methodology has a high flexibility for the computation on many-core architectures including GPUs and multi-core CPUs. We proposed to break the four basic join algorithms into a set of simple primitives. The algorithms of these primitives are scalable to hundreds of processors. Moreover, these primitives can be used to develop higher-level primitives and other applications. Additionally, we can easily replace the existing implementation of a certain primitive with a more efficient one whenever applicable. For instance, GPGPU researchers recently released CUDPP <ref type="bibr" target="#b17">[18]</ref>, a CUDA library of data parallel primitives.</p><p>We plan to compare it with our own primitives, and choose the more efficient ones to implement the join algorithms.</p><p>We also identified a few limitations of GPUs for performing relational query processing:</p><p>First, query processing in general and join processing in specific is a complex task in its runtime logic in addition to its dataintensiveness. Mapping such a task onto the SIMD processors in the GPU requires a significant amount of design and implementation effort. In particular, the SIMD architecture by design trades functional simplicity for high efficiency and concurrency. For instance, branches frequently appear in query processing algorithms, e.g., index searches, and need special care on the GPU. Existing techniques <ref type="bibr" target="#b39">[40]</ref> for rewriting the branches on the CPU can also be applied to the GPU. This rewrite is especially useful for common and expensive operations. We acknowledge that this kind of rewriting in general is a difficult task for the run-time environment.</p><p>Another example is that the synchronization mechanism for handling read/write conflicts, which happen constantly in query processing, is limited in the GPU. As a result, our primitives and join algorithms take extra computation such as computing the writing offsets to avoid the conflicts. This extra computation increases the work complexity of our algorithms by a constant factor.</p><p>Second, with the exposure of the massively multi-threaded hardware architecture on the GPU, it also makes GPGPU programming trickier to ensure correctness and to fully utilize the essential GPU features such as data parallelism than the previous GPUs. In our work, we have developed a small set of primitives that are carefully designed and highly tuned for GPU join processing. Similarly, GPGPU programmers could produce better and faster programs using a set of well-defined primitives as building blocks to address this issue.</p><p>Third, even though the latest GPU frameworks, such as CTM and CUDA, are a significant leap from the traditional GPUs in providing great details about the hardware architecture, they are still far behind the CPU vendors' tradition of giving sufficient details about the hardware specification, e.g., the memory hierarchy. Currently, we mainly rely on empirical experiments to</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. The GPU architecture model. The GPU is a coprocessor to the CPU. It consists of multiple SIMD multiprocessors, and has a large amount of device memory. This model is applicable to both AMD's CTM<ref type="bibr" target="#b1">[2]</ref>[7] and NVIDIA's CUDA<ref type="bibr" target="#b26">[27]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>split (Rin, fcn, Rout) Parameters: #thread, the total number of threads (#thread=Bp*T). F, the partitioning fanout. tHist, the thread-level histogram. tHist[t][p] is the number of tuples processed by thread t and belonging to partition p F) p (1 ≤ ≤ . tOffset, the thread-level offset array. tOffset[t][p] contains the start position to output the tuples of thread t that belong to partition p. L, the array storing start positions to output the tuples of each partition for each thread. The start position of partition p for thread t is L[(p-1)*#thread+t].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An example of the split primitive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Maps with and without coalesced accesses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 =</head><label>2</label><figDesc>. The first optimization is applied to the first c stages ( ) of the bitonic sort, which are independently performed on individual chunks of size M. We use local memory to store this chunk of data and process Stages 1 to c in the local memory. This saves the device memory. The second optimization is applied to Steps c, c-1, …, and 1 of Stage i (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Data accesses in the bitonic sort on the GPU with local memory optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The non-indexed NLJ algorithm on the GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Data transfer time from the main memory to the device memory (||R||=256MB). When the block size is larger than 4MB, the peak bandwidth is 3.1 GB/sec.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The map performance with and without the coalesced access. The coalesced access improves the map bandwidth on the GPU by around twice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The elapsed time with the number of thread groups varied. The number of threads in each thread group is fixed to 32. The best numbers of thread groups in the map and the split are 128 and 64, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Figure 9. The elapsed time with the number of threads per thread group varied. The number of thread groups is set to be the best one shown in Figure 8. The suitable numbers of threads per thread group for the map and the split are 64 and 32, respectively. Local memory optimization.Figure 10 compares the GPU-based primitives with and without the local memory optimization. The local memory optimization improves the overall performance of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Elapsed time of join algorithms with binary search and sequential search. Table4. Elapsed time of the four relational joins on the GPU and the CPU for the uniform data sets. The speedup of the GPU-based primitives is 2-7X over the CPU-based primitives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Join processing time and data transfer time between the main memory and the device memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. The speedup of the GPU-based joins over the CPUbased joins: (left) the join selectivity is varied; (right) the percentage of duplicates in R is varied and S is uniform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Performance comparison between our algorithms and MonetDB: (left) sort, (right) hash joins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 compares</head><label>14</label><figDesc>Figure 14  compares the performance of the sort and the hash join. We varied |R| from 4M to 16M for the sort. For the hash join, we kept |R| = |S|, and varied both |R| and |S| from 4M to 16M. As the data size increases, both our CPU-and GPU-based implementations outperform MonetDB. This figure indicates that the efficiency of our implementation is comparable to MonetDB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Sorting strings on the CPU and the GPU. The GPU-based quick sort achieves a speedup of around 2X over its CPU-based counterpart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>. Notations used in this paper</head><label></label><figDesc></figDesc><table><row><cell>Parameter</cell><cell>Description</cell></row><row><cell>Bp</cell><cell>Total number of thread groups on the GPU</cell></row><row><cell>T</cell><cell>Number of threads per thread group</cell></row><row><cell>M</cell><cell>The size of local memory per thread group</cell></row><row><cell>R, S</cell><cell>Outer and inner relations of the join</cell></row><row><cell>r, s</cell><cell>Tuple sizes of R and S (bytes)</cell></row><row><cell>|R|, |S|</cell><cell>Cardinalities of R and S</cell></row><row><cell>||R||, ||S||</cell><cell>Sizes of R and S (bytes)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Hardware configurationWe used synthetic data sets and workloads for detailed studies on our join algorithms. Our homegrown workload contains two join queries on relations R and S. Relations R and S are binary tables each consisting of two four-byte integer attributes, the record ID (rid) and the key value (key). We used both uniform and nonuniform key values. We generated our non-uniform key values by setting a certain percentage of tuples to be a constant key value</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>GPU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CPU(Quad-core)</cell></row><row><cell cols="2">Processors</cell><cell></cell><cell cols="5">1350MHz × 8 × 16</cell><cell cols="2">2.4 GHz × 4</cell></row><row><cell>Data</cell><cell>cache</cell><cell>(local</cell><cell cols="3">16KB × 16</cell><cell></cell><cell></cell><cell cols="2">L1: 32KB × 4, L2:</cell></row><row><cell cols="2">memory)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">4096KB × 2</cell></row><row><cell cols="3">Cache latency (cycle)</cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">L1: 2 , L2: 8</cell></row><row><cell cols="2">DRAM (MB)</cell><cell></cell><cell>768</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">2048</cell></row><row><cell cols="3">DRAM latency (cycle)</cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">300</cell></row><row><cell cols="2">Bus width (bit)</cell><cell></cell><cell>384</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">64</cell></row><row><cell cols="3">Memory clock (GHz)</cell><cell>1.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">1.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">R.key =</cell><cell>S.key</cell><cell>as the</cell></row><row><cell cols="4">predicate and the non-equijoin</cell><cell>R.key</cell><cell>≤</cell><cell>S.key</cell><cell>≤</cell><cell>R.key</cell><cell>+</cell><cell>δ</cell><cell>( δ is a constant</cell></row><row><cell>integer).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>(e.g., one in our experiments). Other tuples are randomly distributed. When this percentage is zero, key values in the relation are uniformly distributed; when it is 100%, all tuples have the same key value. We varied this percentage to simulate different degrees of skewness.</p>The join queries in our own workloads are "SELECT R.rid, S.rid FROM R, S WHERE &lt;predicate&gt;". We used an equijoin and a non-equijoin query. The equi-join takes</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Elapsed time for primitives (|R| is 16 million). The speedup of the GPU-based primitives is 2-27X over the CPUbased primitives.</figDesc><table><row><cell>Primitive</cell><cell>CPU (ms)</cell><cell>GPU (ms)</cell><cell>Speedup</cell></row><row><cell>Map</cell><cell>109</cell><cell>4</cell><cell>27.3</cell></row><row><cell>Scatter</cell><cell>1312</cell><cell>104</cell><cell>12.6</cell></row><row><cell>Gather</cell><cell>1000</cell><cell>103</cell><cell>9.7</cell></row><row><cell>Prefix scan</cell><cell>141</cell><cell>14</cell><cell>10.1</cell></row><row><cell>Split</cell><cell>813</cell><cell>125</cell><cell>6.5</cell></row><row><cell>Sort(qsort)</cell><cell>2313</cell><cell>945</cell><cell>2.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Elapsed time of the four relational joins on the GPU and the CPU for the uniform data sets. The speedup of the GPU-based primitives is 2-7X over the CPU-based primitives.</figDesc><table><row><cell>Joins</cell><cell>CPU (sec)</cell><cell>GPU (sec)</cell><cell>Speedup</cell></row><row><cell>NINLJ</cell><cell>528.0</cell><cell>75.0</cell><cell>7.0</cell></row><row><cell>INLJ</cell><cell>4.2</cell><cell>0.7</cell><cell>6.1</cell></row><row><cell>SMJ</cell><cell>5.0</cell><cell>2.0</cell><cell>2.4</cell></row><row><cell>HJ</cell><cell>2.5</cell><cell>1.3</cell><cell>1.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Elapsed time in seconds of the four relational joins implemented with CUDA and DirectX (DX).</figDesc><table><row><cell></cell><cell>DX (join)</cell><cell cols="2">DX (total) CUDA (join)</cell><cell>CUDA (total)</cell></row><row><cell>NINLJ</cell><cell>72.3</cell><cell>74.1</cell><cell>75.0</cell><cell>75.0</cell></row><row><cell>INLJ</cell><cell>0.7</cell><cell>0.9</cell><cell>0.6</cell><cell>0.7</cell></row><row><cell>SMJ</cell><cell>3.8</cell><cell>4.7</cell><cell>1.9</cell><cell>2.0</cell></row><row><cell>HJ</cell><cell>2.3</cell><cell>2.7</cell><cell>1.2</cell><cell>1.3</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Acknowledgement</head><p>The work of Ke Yang was done while he was visiting HKUST, and the work of Rui Fang was done when he was an undergraduate student at HKUST. This work was supported by grant 617206 from the Hong Kong Research Grants Council.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Code and data used in the paper are available at http://www.sigmod.org/codearchive/sigmod2008/.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>estimate the hardware parameters and to identify the suitable settings for our algorithms.</p><p>Fourth, the power consumption of the GPU is higher than that of the CPU. In our experiments, the GPU requires a power supply of 450 Watts, whereas the CPU requires 95 Watts only. It is desirable to develop software or hardware techniques to reduce the power consumption of the GPU.</p><p>Finally, as a co-processor, the GPU requires advanced software techniques to support complex workloads. For example, lacking hardware support for complex data types is an inherent weakness of the GPU. Currently, we can use software solutions for supporting more complex data types such as high precision numbers on the GPU <ref type="bibr" target="#b37">[38]</ref>. Fortunately, GPU vendors plan to support high precision numbers such as double in the near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>Graphics processors have become an attractive alternative for general-purpose high performance computing on commodity hardware. The continuing advances in hardware and the recent improvements on programmability make GPUs even more suitable for database query processing than before. In this study, we have designed a small set of data-parallel primitives for relational join processing on GPUs. These primitives provide high-level abstractions for data-centric operations and are highly tuned to fully utilize the architectural features of graphics processors. We have implemented four representative relational join algorithms using these primitives and have compared the join performance with optimized CPU-based in-memory join algorithms. We find that our GPU joins achieve a speedup of 2-7X over their optimized CPU-based counterparts. This paper focuses on GPU join processing in the video memory. We believe this is an important but initial step towards building a high-performance, general-purpose database query processor using the GPU. One interesting future direction is to evaluate our join algorithms with more complex workloads. Additionally, we are interested in how to schedule the execution of relational query processing between the GPU and the CPU so that their computation power is fully exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experiment Repeatability</head><p>The repeatability committee has not been able to repeat the experiments of this paper due to the lack of appropriate hardware.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Query co-processing on commodity processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harizopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Amd Ctm</surname></persName>
		</author>
		<ptr target="http://ati.amd.com/products/streamprocessor/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parallel join algorithms for SIMD models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Azadegan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Tripathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICPP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="125" to="133" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A parallel join algorithm for SIMD architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Azadegan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Tripathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Hardware acceleration in commercial databases: A case study of spatial operations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Abbadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>VLDB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The Direct3D 10 system</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mass market applications of massively parallel computing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Brook for GPUs: stream computing on graphics hardware</title>
		<author>
			<persName><forename type="first">I</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sugerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Prefix sums and their applications</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
		<idno>CMU-CS-90-190</idno>
		<imprint>
			<date type="published" when="1990-11">Nov 1990</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Database architecture optimized for the new bottleneck: memory access</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boncz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manegold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Realizing parallelism in database operations: insights from a massively multithreaded architecture</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cieslewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hendrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>DaMoN</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>Introduction to Algorithms, Second Edition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parallel database systems: The future of high performance database systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1992-06">June 1992</date>
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GPUTeraSort: high performance graphics coprocessor sorting for large database management</title>
		<author>
			<persName><forename type="first">N</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fast computation of database operations using graphics processors</title>
		<author>
			<persName><forename type="first">N</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>SIGMOD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast and approximate stream mining of quantiles and frequencies using graphics processors</title>
		<author>
			<persName><forename type="first">N</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Raghuvanshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Database servers on chip multiprocessors: limitations and opportunities</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pandis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mancheril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>CIDR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">CUDPP: CUDA Data Parallel Primitives Library</title>
		<author>
			<persName><forename type="first">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davidson</surname></persName>
		</author>
		<ptr target="http://www.gpgpu.org/developer/cudpp/" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient gather and scatter operations on graphics processors</title>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>ACM/IEEE Supercomputing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stream reduction operations for GPGPU applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPU Gems</title>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Handling data skew in multiprocessor database computers using partition tuning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The influence of caches on the performance of sorting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lamarca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SODA</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A fast similarity join algorithm using graphics processing units</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Lieberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICDE</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting pipelined parallelism in multi-join query processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rundensteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hash-based join algorithms for multiprocessor computers with shared memory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<ptr target="http://monetdb.cwi.nl/" />
		<title level="m">MonetDB</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><surname>Nvidia Cuda</surname></persName>
		</author>
		<ptr target="http://developer.nvidia.com/object/cuda.html" />
	</analytic>
	<monogr>
		<title level="j">Compute Unified Device Architecture)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<ptr target="http://www.opengl.org/" />
		<title level="m">OpenGL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<ptr target="http://www.openmp.org/" />
		<title level="m">OpenMP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey of generalpurpose computation on graphics hardware</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luebke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Lefohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Purcell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="issue">26</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cache conscious indexing for decision-support in main memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A performance evaluation of four parallel join algorithms in a shared-nothing multiprocessor environment</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scan primitives for GPU computing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH/EUROGRAPHICS conference on Graphics hardware</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cache conscious algorithms for relational query processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shatdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cherniack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
		<author>
			<persName><surname>C-Store</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>A column-oriented DBMS. VLDB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hardware acceleration for spatial selections and joins</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Abbadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>SIGMOD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Accelerator: using data parallelism to program GPUs for general-purpose uses</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tarditi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oglesby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Extended-precision floating-point numbers for GPU computation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Thall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>SIGGRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Radix sort for vector multiprocessors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zagha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>ACM/IEEE Supercomputing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Zukowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boncz</surname></persName>
		</author>
		<title level="m">Super-Scalar RAM-CPU Cache Compression. ICDE</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
