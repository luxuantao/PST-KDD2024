<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Representation Learning for Exemplar-Guided Paraphrase Generation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haoran</forename><surname>Yang</surname></persName>
							<email>hryang@se.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
							<email>wlam@se.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Representation Learning for Exemplar-Guided Paraphrase Generation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Exemplar-Guided Paraphrase Generation (EGPG) aims to generate a target sentence which conforms to the style of the given exemplar while encapsulating the content information of the source sentence. In this paper, we propose a new method with the goal of learning a better representation of the style and the content. This method is mainly motivated by the recent success of contrastive learning which has demonstrated its power in unsupervised feature extraction tasks. The idea is to design two contrastive losses with respect to the content and the style by considering two problem characteristics during training. One characteristic is that the target sentence shares the same content with the source sentence, and the second characteristic is that the target sentence shares the same style with the exemplar. These two contrastive losses are incorporated into the general encoder-decoder paradigm.</p><p>Experiments on two datasets, namely QQP-Pos and ParaNMT, demonstrate the effectiveness of our proposed constrastive losses. The code is available at https: //github.com/LHRYANG/CRL_EGPG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Paraphrase generation <ref type="bibr" target="#b13">(Gupta et al., 2017;</ref><ref type="bibr" target="#b22">Li et al., 2019)</ref>, aiming to generate a sentence with the same semantic meaning of the source sentence, has achieved a great success in recent years. To obtain a paraphrase sentence with a particular style, Exemplar-Guided Paraphrase Generation (EGPG) <ref type="bibr" target="#b2">(Chen et al., 2019)</ref> has attracted considerable attention. Different from other controllable text generation tasks whose constraints are taken from a finite set, e.g., binary sentiment or political slant <ref type="bibr" target="#b34">(Yang et al., 2018;</ref><ref type="bibr" target="#b26">Prabhumoye et al., 2018)</ref>, multiple personas <ref type="bibr" target="#b16">(Kang et al., 2019)</ref>, over source <ref type="bibr">(X)</ref> what is the easiest way to get followers on quora ? exemplar (Z) how do i avoid plagiarism in my article ?</p><p>target (Y ) how do i get more followers for my quora ? retrieved (Y ) what are the better ways to ask questions on quora ? Table <ref type="table">1</ref>: An example of EGPG which a classifier can be trained to guide the disentanglement process, the constraints of EGPG are exemplar sentences that can be arbitrarily provided, making it more challenging to learn a good representation for the style and the content. For example, as shown in Table <ref type="table">1</ref>, when using the content embedding of X to retrieve a sentence with the most similar content in the target sentences list, we observe that the ordinary model, which is described in Section 4.5, can often match the sentence Y whose content differs from X instead of the correct target sentence Y . This reveals that the content encoder cannot encode the content information of a sentence appropriately which can result in inconsistent content of the generated sentence. The same problem also exists in the style encoder.</p><p>To learn a better content and style representations, we explore the incorporation of contrastive learning in EGPG to design an end-to-end encoderdecoder paradigm with multiple losses. Contrastive learning originates from computer vision area <ref type="bibr" target="#b3">(Chen et al., 2020a;</ref><ref type="bibr" target="#b17">Khosla et al., 2020</ref>) and now, it also shows its powerfulness in natural language processing area. For instance, <ref type="bibr" target="#b14">Iter et al. (2020)</ref> employ contrastive learning to improve the quality of discourse-level sentence representations. In our proposed model, besides the basic encoderdecoder generation task, a content contrastive loss is designed to force the content encoder to distinguish features of the same content from features of different content. Similarly, a style contrastive loss is also employed to obtain a similar distinguishing effect for the style features. Experimental results on two benchmark datasets, namely QQP-Pos and ParaNMT, show that superior performance can be achieved with the help of the contrastive losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Paraphrase Generation Researches on paraphrase generation has been for a long time. Traditional methods solve this problem mainly through statistical machine translation <ref type="bibr" target="#b29">(Quirk et al., 2004)</ref> or rule-based word substitution <ref type="bibr" target="#b31">(Wubben et al., 2010)</ref>. In the era of deep learning, approaches based on the encoder-decoder framework have emerged in large numbers <ref type="bibr" target="#b27">(Prakash et al., 2016;</ref><ref type="bibr" target="#b4">Chen et al., 2020b)</ref>. In addition to basic seq2seq model, <ref type="bibr" target="#b21">Li et al. (2018)</ref> add a pair-wise discriminator to judge whether the input sentence and generated sentence are paraphrases of each other, with the help of reinforcement learning. To generate diverse paraphrases, i.e., one to many mapping, <ref type="bibr" target="#b13">Gupta et al. (2017)</ref> combine the power of RNN-based sequence-to-sequence model and the variational autoencoder. At decoding time, a noise sampled from the Gaussian distribution are appended to input to generate a diverse output. <ref type="bibr" target="#b28">Qian et al. (2019)</ref> propose a approach which use multiple generators to generate diverse paraphrases without sacrificing quality. Exemplar-Guided Paraphrase Generation Making the generated paraphrases satisfy the style of an exemplar sentence is recently a hot research topic. EGPH is similar to other controlled text generation tasks whose constraints are sentiment <ref type="bibr" target="#b34">(Yang et al., 2018;</ref><ref type="bibr" target="#b32">Xu et al., 2021)</ref>, gender <ref type="bibr" target="#b26">(Prabhumoye et al., 2018)</ref>, topics <ref type="bibr" target="#b30">(Wang et al., 2021)</ref>. These tasks are highy related to Disentangled Representation Learning (DRL) which maps different aspects of the input data to independent low-dimensional spaces <ref type="bibr" target="#b5">(Cheng et al., 2020)</ref>. <ref type="bibr" target="#b15">Iyyer et al. (2018)</ref> and <ref type="bibr" target="#b19">Kumar et al. (2020)</ref> directly utilise the parse tree information of the exemplar as the style information without separating style from sentences. <ref type="bibr" target="#b2">Chen et al. (2019)</ref> propose a model which can directly extract style features from a modified target sentence. <ref type="bibr" target="#b12">Goyal and Durrett (2020)</ref> provide a way to generate paraphrase which is a component rearrangement of the original input through manipulating the parse tree. Contrastive Learning In the past few years, many unsupervised feature extraction algorithms have emerged, for instance, variational autoencoder <ref type="bibr" target="#b18">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b33">Xu et al., 2020;</ref><ref type="bibr">Gao et al., 2019b,a)</ref>, generalised language models <ref type="bibr">(Brown et al., 2020;</ref><ref type="bibr" target="#b8">Devlin et al., 2019)</ref>. All the above methods obtain the feature of input by reconstructing the original input or predicting masked words and so on which do not take the relationships between the inputs into consideration. Therefore, contrastive learning, whose loss is designed to narrow down the distance between features of similar inputs and to enlarge the distance of dissimilar inputs, has been proposed and achieved a great success in both unsupervised <ref type="bibr" target="#b3">(Chen et al., 2020a)</ref> and supervised <ref type="bibr" target="#b17">(Khosla et al., 2020</ref>) image feature extraction. There are also some works trying to apply contrastive learning into natural language processing domain. For instance, <ref type="bibr" target="#b14">Iter et al. (2020)</ref> propose a pretraining method for sentence representation which employs contrastive learning to improve the quality of discourse-level representations. <ref type="bibr" target="#b11">Giorgi et al. (2020)</ref> utilise it to pretrain the transformer and btains state-of-the-art results on SentEval <ref type="bibr" target="#b7">(Conneau and Kiela, 2018)</ref>. All of the above successes spur us to test whether contrastive learning is helpful on EGPG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>Given a source sentence X i and an exemplar sentence Z i , our goal is to generate a sentence Y i that has the same style (syntax) with Z i and retains the content (semantics) of X i . As shown in Figure <ref type="figure" target="#fig_0">1</ref>, we design the encoders E s and E c for style and content respectively. The decoder D generates the output. Our model is trained by optimizing three losses simultaneously: (1) generation loss; (2) content contrastive loss; (3) style contrastive loss.</p><p>Generation Task For X i and Z i , we firstly obtain their corresponding content features c X i and style features s Z i :</p><formula xml:id="formula_0">c X i = E c (X i ) (1) s Z i = E s (Z i )</formula><p>(2) Then c X i and s Z i are concatenated and inputted into the decoder as the initial hidden state to generate a sequence of probabilities over vocabulary. At the step t, the predicted probability p t of the t-th target word is obtained as follows</p><formula xml:id="formula_1">p t = sof tmax(W h t ) (3) h t = GRU (h t−1 , e(y t−1 )) (4) where h 0 is initialized as [c X i , s Z i ]</formula><p>and W is a parameter matrix. y t−1 is the word in the previous step t−1 and y 0 is the special symbol [SOS] which represents the start of the sentence. e(y t−1 ) is the embedding of the word y t−1 .</p><p>Negative log-likelihood loss (NLL) is employed as the basic optimization objective</p><formula xml:id="formula_2">L nll i = − 1 |Y i | |Y i | t=1 I(y t ) T log p t (5)</formula><p>where I(y t ) represents the one-hot encoding of the word y t in the vocabulary.</p><p>Content Contrastive Learning (CCL) Considering that X i and Y i share the same content, their content features should be close with each other in the content feature space. Contrastive Learning which is designed to minimize the distance between positive pairs and maximize the distance between negative pairs can help model this relationship. Formally, during training, given a batch</p><formula xml:id="formula_3">{(X i , Y i , Z i )} n i=1</formula><p>where n is the batch size, we firstly obtain the corresponding content features of</p><formula xml:id="formula_4">X i and Y i , denoted by {(c X i , c Y i )} n i=1 . For c X i , the positive pair is (c X i , c Y i</formula><p>) and c X i with the other remaining features in this batch form 2n − 2 negative pairs. For c Y i , the definition of positive/negative pairs is the same as c X i . Then the contrastive loss is employed, giving</p><formula xml:id="formula_5">L ccl X i = −log exp(c X i • c Y i /τ ) exp( c X i •c Y i τ ) + j =i T ∈{X,Y } exp( c X i •c T j τ )<label>(6)</label></formula><formula xml:id="formula_6">L ccl Y i = −log exp(c Y i • c X i /τ ) exp( c Y i •c X i τ ) + j =i T ∈{X,Y } exp( c Y i •c T j τ )<label>(7)</label></formula><formula xml:id="formula_7">L ccl = n i=1 (L ccl X i + L ccl Y i )<label>(8)</label></formula><p>where • represents the dot product between two vectors and τ denotes a temperature parameter.</p><p>Style Contrastive Learning (SCL) aims to help E s learn a better style representation by considering that Z i and Y i share the same style. Similar to CCL, we firstly obtain the style features</p><formula xml:id="formula_8">{(s Z i , s Y i )} n</formula><p>i=1 and then apply the contrastive loss to these features </p><formula xml:id="formula_9">L scl Y i = −log exp(s Y i • s Z i /τ ) exp( s Y i •s Z i τ ) + j =i T ∈{Z,Y } exp( s Y i •s T j τ ) (9) L scl Z i = −log exp(s Z i • s Y i /τ ) exp( s Z i •s Y i τ ) + j =i T ∈{Z,Y } exp( s Z i •s T j τ )<label>(10)</label></formula><formula xml:id="formula_10">L scl = n i=1 (L scl Y i + L scl Z i )<label>(11)</label></formula><p>As a result, the total loss for a batch is as follows</p><formula xml:id="formula_11">L = n i=1 L nll i + λ 1 L ccl + λ 2 L scl (12)</formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct experiments on two benchmark datasets, namely ParaNMT <ref type="bibr" target="#b2">(Chen et al., 2019)</ref> and QQP-Pos <ref type="bibr" target="#b19">(Kumar et al., 2020)</ref>. ParaNMT consists of about 500k training, 800 testing and 500 validation sentence pairs which are automatically generated through backtranslation of the original English sentences. QQP-Pos consists of 130k training, 3k testing and 3k validation quora question pairs which are more formal than ParaNMT. The split size is the same as previous works to have a fair comparison. Since the exemplar sentences are not provided in both datasets, we adopt a method similar with <ref type="bibr" target="#b19">Kumar et al. (2020)</ref> to search an exemplar Z i for each source-target pair (X i , Y i ) based on the POS tag sequence<ref type="foot" target="#foot_0">1</ref> similarity (refer to Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines &amp; Metrics</head><p>We compare our model with (1) SCPN <ref type="bibr" target="#b15">(Iyyer et al., 2018)</ref> which employs a parse generator to output the full linearized parse tree as the style by inputting a parse template;</p><p>(2) SGCP <ref type="bibr" target="#b19">(Kumar et al., 2020)</ref> which extracts the style information directly from the parse tree of the exemplar sentence;</p><p>(3) CGEN <ref type="bibr" target="#b2">(Chen et al., 2019)</ref>, an approach based on variational inference (Kingma and Welling, 2014). The evaluation metrics are BLEU <ref type="bibr" target="#b24">(Papineni et al., 2002)</ref>, METEOR <ref type="bibr" target="#b20">(Lavie and Agarwal, 2007)</ref> and ROUGE (R) <ref type="bibr" target="#b23">(Lin, 2004)</ref>. We also conduct human evaluation to investigate the quality of the generated sentences. Moreover, we propose Content Matching Accuracy (CMA) to gauge the quality of the generated embeddings for the content. CMA will be introduced in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Each sentence is trimmed with a maximum length 15. The word embedding is initialized with 300-d pretrained GloVe <ref type="bibr" target="#b25">(Pennington et al., 2014)</ref>. We use a BERT-based <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> architecture for the style encoder E s and the dimension of style features is 768. For content encoder E c , we use GRU <ref type="bibr" target="#b6">(Chung et al., 2014)</ref> with hidden state size 512. During training, the teacher forcing technique is applied with the rate 1.0. The balancing parameters λ 1 and λ 2 are both set to 0.1. The temperature parameter τ is set to 0.5. We train our model using Adam optimizer with the learning rate 1e-4 and the training epochs are set to 30 and 45 for ParaNMT and QQP-Pos respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>As summarized in Table <ref type="table" target="#tab_0">2</ref>, our model outperforms SCPN, SGCP and CGEN by a large margin on automatic evaluation metrics. We also conduct human evaluation to investigate the holistic quality of the generated sentences. For each dataset, we firstly choose two source sentences and then randomly select 25 exemplars for each source sentence to generate a total of 50 sentences. Table <ref type="table">5</ref> shows the results of human assessment. It can be seen that our model obtains a higher score than SGCP and CGEN, which is consistent with the automatic evaluation results. These results are expected because SCPN and SGCP use a parse tree as the style which is lack of the lexical information and very unstable. Moreover, CEGN is VAE-based which is intrinsically harder to train <ref type="bibr" target="#b0">(Bowman et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We conduct ablation study with three variants, namely ours without SCL (Ours-w/o-SCL), ours without CCL (Ours-w/o-CCL), ours without both SCL and CCL (Ours-w/o-both). We show that our model is better than the three variants to demonstrate the effectiveness of the contrastive losses.</p><p>As presented in  We also provide the style evaluation of ED-E (edit distance between the POS tag sequence of the generated paraphrase and the exemplar) and ED-R (edit distance between the POS tag sequence of the generated paraphrase and the ground truth reference) in Table <ref type="table" target="#tab_2">3</ref>. We can see that models with style contrastive losses have smaller edit distance.</p><p>To directly assess the quality of the generated embeddings for the content we propose Content Matching Accuracy. To calculate CMA, firstly we input all the source and target sentences into E c to get the content representations A, B ∈ R m×k , where m is the size of the test dataset and k is the dimension of content feature vectors. Then, we calculate the similarity matrix S = AB T . In ideal situation, each diagonal element S [i,i] should be the largest value in row i since the content embedding A i of the i th source sentence should have the greatest similarity with the content embedding B i of its corresponding target sentence. Therefore, the SOURCE: how do i develop good project management skills ? EXEMPLAR GENERATION which is the best laptop model to buy within 30k ? which is the best way to develop project management skills ? how many cups of coffee should i consume in a day ? from what skills can i start in a project management ? which subjects are important to become a chartered accountant ? what skills are necessary to develop a project management ? Table <ref type="table">5</ref>: Human evaluation results. Each sentence is given a score ranging from one to five to assess the holistic quality. We report the average value of two annotators. Higher score is better. The Spearman's correlation coefficients of these two annotators are 0.707 for QQP-Pos and 0.37 for ParaNMT.  </p><formula xml:id="formula_12">CM A = m i=1 1(argmax(S i ) = i) m<label>(13)</label></formula><p>where 1(s) equals 1 if s is true, otherwise 0. The results are illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. We notice that models with CCL can achieve higher accuracy than Ours-w/o-both. It signifies that the content encoder E c is improved with the help of CCL. We provide a failed example in Table <ref type="table" target="#tab_5">6</ref>. Y is the sentence retrieved given X under the model Ours-w/o-both. Y is the target sentence and it is also the sentence retrieved given X under the model Ours or Oursw/o-SCL. We can see that the content generated by Ours-w/o-both is incorrect (changing facebook to instagram) and instagram exists in Y . This illustrates that the poor-quality content embedding of X can cause the incorrect content of the generated sentence. In general, matching accuray can also be calculated for the style. However, the exemplar selection process has a high probability of dropping the sentence with the most similar style of Y . Therefore, style matching accuracy is not provided here. Instead, we list some retrieved sentences based on the style embedding in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study</head><p>Some examples generated by our model are shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Explanations</head><p>We attempt to provide some possible explanations about why the model with these two contrastive losses can achieve better performance. The first reason is that adding additional losses on the output of encoders can alleviate gradient vanishing which is a serious issue when training the encoderdecoder model. The second reason is that the overfitting issue may be prevented, since the contrastive losses restrict the free adjustment of parameters in the model by forcing the encoder and decoder to focus on their own tasks, i.e., feature extraction and sentence generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce the content contrastive loss and the style contrastive loss into EGPG to design a multilosses scheme without requiring additional labeled data. This scheme can obtain better results compared with the baseline and ablative models, which demonstrates the effectiveness of contrastive learning for learning better representations. Moreover, the proposed framework is general and may benefit other similar NLP tasks.</p><p>A Exemplar Searching Algorithm The detailed steps for exemplar sentence searching are described in Algorithm 1.</p><p>Step 2 is done to accelerate the searching procedure since sentences with similar style tend to have similar token lengths.</p><p>Step 3 guarantees that the content information of Y and the selected Z does not overlap much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Style Embedding Quality</head><p>We list some sentences retrieved by ablative models given the style embedding of a sentence S in Table <ref type="table" target="#tab_9">7</ref>. For each model, we obtain the top-5 sentences which are most similar to S. We can see that the sentences retrieved by Ours and Ours-w/o-CCL are more similar to S in style dimension than Oursw/o-both on the whole. For example, in the second case, the fifth sentence of Ours-   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An Overview of Our Model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Content Matching Accuracy over, our model does not directly copy the style words from the exemplar, but instead adopts the overall structure of the exemplar to generate sentences, for example, the second one. More examples are provided in Appendix C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>, we can see that Ours</cell></row><row><cell>can achieve better results than the three variants</cell></row><row><cell>on all automatic metrics for QQP-Pos. Particularly,</cell></row><row><cell>Ours, Ours-w/o-CCL and Ours-w/o-SCL outper-</cell></row><row><cell>form Ours-w/o-both a lot, demonstrating the useful-</cell></row><row><cell>ness of the contrastive losses. For ParaNMT, Ours</cell></row><row><cell>obtains the highest score on BLEU, ROUGE-2,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Automatic Evaluation Results.</figDesc><table><row><cell cols="5">Model Ours Ours-w/o-CCL Ours-w/o-SCL Ours-w/o-both</cell></row><row><cell></cell><cell></cell><cell>QQP-Pos</cell><cell></cell><cell></cell></row><row><cell>ED-E</cell><cell>2.49</cell><cell>2.42</cell><cell>2.56</cell><cell>2.57</cell></row><row><cell>ED-R</cell><cell>2.64</cell><cell>2.65</cell><cell>2.78</cell><cell>2.82</cell></row><row><cell></cell><cell></cell><cell>ParaNMT</cell><cell></cell><cell></cell></row><row><cell>ED-E</cell><cell>4.36</cell><cell>4.47</cell><cell>4.49</cell><cell>4.49</cell></row><row><cell>ED-R</cell><cell>4.22</cell><cell>4.24</cell><cell>4.28</cell><cell>4.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Style Evaluation</figDesc><table /><note>MENTOR while it lags behind Ours-w/o-CCL on ROUGE-1 and ROUGE-L. This phenomena may be caused by the poor quality of the dataset. The human evaluation results are also listed in Table5. Our model can generally generate fluent sentences on QQP-Pos. But the overall quality of sentences generated by all models on ParaNMT is unsatisfactory which shows that a high-quality dataset is necessary for training a good model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>An Example Generated by Our Model</figDesc><table><row><cell>Model</cell><cell cols="6">Ours SGCP CGEN Ours-w/o-CCL Ours-w/o-SCL Ours-w/o-both</cell></row><row><cell>QQP-Pos</cell><cell>3.71</cell><cell>3.12</cell><cell>2.97</cell><cell>3.59</cell><cell>3.45</cell><cell>3.33</cell></row><row><cell cols="2">ParaNMT 2.53</cell><cell>1.9</cell><cell>2.05</cell><cell>2.51</cell><cell>2.53</cell><cell>2.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>A failed example without CCL content matching accuracy (CMA) is defined as:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>It can be observed that our model can generally generate high-quality sentences which have similar style with the exemplar and retain the semantic meaning of the source sentence. More-</figDesc><table><row><cell>0.8</cell><cell>Ours-w/o-both Ours-w/o-SCL Ours</cell><cell></cell></row><row><cell>0.6</cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell>Quora</cell><cell>Para</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>w/o-both lacks the adverbial modifier compared with S 2 .</figDesc><table><row><cell>S1</cell><cell>what are newton 's laws of motion?</cell></row><row><cell></cell><cell>what are the after effects of masturbation ?</cell></row><row><cell></cell><cell>what are the health benefits of coffee ?</cell></row><row><cell>Ours</cell><cell>what are the safety precautions on handling shotguns ?</cell></row><row><cell></cell><cell>what are some interesting facts about bengaluru ?</cell></row><row><cell></cell><cell>what are some unknown facts about football ?</cell></row><row><cell></cell><cell>what are the health benefits of coffee ?</cell></row><row><cell></cell><cell>what are the good things about pakistan ?</cell></row><row><cell>Ours-w/o-CCL</cell><cell>what are some interesting facts about bengaluru ?</cell></row><row><cell></cell><cell>what are some unknown facts about football ?</cell></row><row><cell></cell><cell>what are considered abiotic factors of grasslands ?</cell></row><row><cell></cell><cell>what were nelson mandela 's greatest accomplishments ?</cell></row><row><cell></cell><cell>what are craig good 's qualifications to talk about nutrition ?</cell></row><row><cell>Ours-w/o-both</cell><cell>what are reasons of china 's success ?</cell></row><row><cell></cell><cell>what are president obama 's greatest accomplishments and failures ?</cell></row><row><cell></cell><cell>what is newton 's third low of motion with examples ?</cell></row><row><cell>S2</cell><cell>how do i impress a girl on chat ?</cell></row><row><cell></cell><cell>how do i become an engineer in robotics ?</cell></row><row><cell></cell><cell>how do i get a job in europe countries ?</cell></row><row><cell>Ours</cell><cell>how do i get the crown on musical ly ?</cell></row><row><cell></cell><cell>how do i make a website responsive without bootstrap ?</cell></row><row><cell></cell><cell>how do i find the best seo company in dellhi ncr ?</cell></row><row><cell></cell><cell>how do i become an engineer in robotics ?</cell></row><row><cell></cell><cell>how do i get a job in europe countries ?</cell></row><row><cell>Ours-w/o-CCL</cell><cell>how do i leave a girl without hurting her feelings ?</cell></row><row><cell></cell><cell>how do i get the crown on musical ly ?</cell></row><row><cell></cell><cell>how do i root a galaxy s550 at t ?</cell></row></table><note>Ours-w/o-both how do i become an engineer in robotics ? how do i get a job in europe countries ? how do i buy a suit online ? how do i get the crown on musical ly ? how can i help a friend get off drugs ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Sentences Retrieved by Style Embedding Z22 what tv series have you watched and why did you like them ? explain Y22 what type of music do you like ? and how do you recommend ? Z23 what is the best way to reduce weight ? Y23 what is the best music to listen to ?</figDesc><table><row><cell cols="2">C Multiple Paraphrase Sentences</cell></row><row><cell></cell><cell>Generation</cell></row><row><cell>X1</cell><cell>which is the best anime to watch ?</cell></row><row><cell>Z11</cell><cell>can you jailbreak an ios 8 3 ?</cell></row><row><cell>Y11</cell><cell>can you recommend the best anime ?</cell></row><row><cell>Z12</cell><cell>which are the best mba colleges in gwalior ?</cell></row><row><cell>Y12</cell><cell>what are the best anime films of all time ?</cell></row><row><cell>Z13</cell><cell>how can i earn from online ?</cell></row><row><cell>Y13</cell><cell>what anime should i watch now ?</cell></row><row><cell>X2</cell><cell>what type of music do you listen ?</cell></row><row><cell>Z21</cell><cell>is equatorial guinea really rich ?</cell></row><row><cell>Y21</cell><cell>which music is really good ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Generate different sentences given different exemplars.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We use NLTK for POS tagging.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14200620).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<imprint>
			<publisher>ArXiv</publisher>
			<pubPlace>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever</pubPlace>
		</imprint>
	</monogr>
	<note>and Dario Amodei. 2020. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Controllable paraphrase generation with a syntactic exemplar</title>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5972" to="5984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distilling knowledge learned in bert for text generation</title>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving disentangled text representation learning with information-theoretic guidance</title>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Malon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7530" to="7541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Workshop on Deep Learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SentEval: An evaluation toolkit for universal sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating multiple diverse responses for short-text conversation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6383" to="6390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A discrete cvae for response generation on short-text conversation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="1898" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Declutr: Deep contrastive learning for unsupervised textual representations</title>
		<author>
			<persName><forename type="first">Osvald</forename><surname>John M Giorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">D</forename><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv, abs/2006.03659</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural syntactic preordering for controlled paraphrase generation</title>
		<author>
			<persName><forename type="first">Tanya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="238" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A deep generative framework for paraphrase generation</title>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prawaan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32th AAAI Conference on Artificial Intelligence</title>
				<meeting>the 32th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pretraining with contrastive sentence objectives improves discourse performance of language models</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Iter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial example generation with syntactically controlled paraphrase networks</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1875" to="1885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">(male, bachelor) and (female, ph.d) have different connotations: Parallelly annotated stylistic language dataset with multiple personas</title>
		<author>
			<persName><forename type="first">Dongyeop</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<title level="m">Supervised contrastive learning</title>
				<imprint>
			<publisher>ArXiv</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Autoencoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Syntax-guided controlled generation of paraphrases</title>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kabir</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuram</forename><surname>Vadapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhaya</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
				<meeting>the Second Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Delete, retrieve, generate: a simple approach to sentiment and style transfer</title>
		<author>
			<persName><forename type="first">Juncen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<meeting>the 2018 Annual Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1865" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decomposable neural paraphrase generation</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3403" to="3414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop: Text Summarization</title>
				<meeting>the ACL Workshop: Text Summarization</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</title>
				<meeting>the 40th Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Style transfer through back-translation</title>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="866" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural paraphrase generation with stacked residual LSTM networks</title>
		<author>
			<persName><forename type="first">Aaditya</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadid</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashequl</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oladimeji</forename><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COL-ING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
				<meeting>COL-ING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2923" to="2934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring diverse expressions for paraphrase generation</title>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3173" to="3182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Monolingual machine translation for paraphrase generation</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="142" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generating diversified comments via reader-aware topic modeling and saliency detection</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<idno>CoRR, abs/2102.06856</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Paraphrase generation as monolingual translation: Data and evaluation</title>
		<author>
			<persName><forename type="first">Sander</forename><surname>Wubben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Natural Language Generation Conference</title>
				<meeting>the 6th International Natural Language Generation Conference</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Antal van den Bosch, and Emiel Krahmer</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Change or not: A simple approach for plug and play language models on sentiment control</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuangbai</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="15935" to="15936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A neural topical expansion framework for unstructured persona-oriented dialogue generation</title>
		<author>
			<persName><forename type="first">Minghong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02153</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised text style transfer using language models as discriminators</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32th Annual Conference on Advances in Neural Information Processing Systems</title>
				<meeting>the 32th Annual Conference on Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7287" to="7298" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
