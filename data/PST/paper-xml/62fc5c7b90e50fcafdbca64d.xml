<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries</title>
				<funder ref="#_J5MKE6U">
					<orgName type="full">National Science Foundation for Distinguished Young Scholars</orgName>
				</funder>
				<funder ref="#_g2rgJFz #_WPYe5rS">
					<orgName type="full">Technology and Innovation Major Project of the Ministry of Science and Technology of China</orgName>
				</funder>
				<funder ref="#_XUq26bB">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-16">16 Aug 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>liuxiao21@mails.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Shiyu</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
							<email>jiezhongqiu@outlook.com</email>
						</author>
						<author>
							<persName><forename type="first">Mengdi</forename><surname>Zhang</surname></persName>
							<email>zhangmengdi02@meituan.com</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
							<email>yuxiaod@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Meituan-Dianping Group</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Meituan-Dianping Group</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mask and Reason: Pre-Training Knowledge Graph Transformers for Complex Logical Queries</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-16">16 Aug 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539472</idno>
					<idno type="arXiv">arXiv:2208.07638v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts</term>
					<term>Computing methodologies ? Knowledge representation and reasoning</term>
					<term>Unsupervised learning</term>
					<term>Learning latent representations</term>
					<term>? Information systems ? Data mining Knowledge Graph</term>
					<term>Pre-Training</term>
					<term>Graph Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph (KG) embeddings have been a mainstream approach for reasoning over incomplete KGs. However, limited by their inherently shallow and static architectures, they can hardly deal with the rising focus on complex logical queries, which comprise logical operators, imputed edges, multiple source entities, and unknown intermediate entities. In this work, we present the Knowledge Graph Transformer (kgTransformer) 1 with masked pre-training and fine-tuning strategies. We design a KG triple transformation method to enable Transformer to handle KGs, which is further strengthened by the Mixture-of-Experts (MoE) sparse activation. We then formulate the complex logical queries as masked prediction and introduce a two-stage masked pre-training strategy to improve transferability and generalizability. Extensive experiments on two benchmarks demonstrate that kgTransformer can consistently outperform both KG embedding-based baselines and advanced encoders on nine in-domain and out-of-domain reasoning tasks. Additionally, kgTransformer can reason with explainability via providing the full reasoning paths to interpret given answers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge graphs (KGs) store and organize human knowledge about the factual world, such as the human-curated Freebase <ref type="bibr" target="#b1">[2]</ref> and Wikidata <ref type="bibr" target="#b39">[40]</ref> as well as the semi-automatic constructed ones-NELL <ref type="bibr" target="#b3">[4]</ref> and Knowledge Vault <ref type="bibr" target="#b7">[8]</ref>. Over the course of KGs' development, representation learning for querying KGs is one of the fundamental problems. Its main challenge lies in the incomplete knowledge and inefficient queries. Web-scale KGs are known to suffer from missing links <ref type="bibr" target="#b40">[41]</ref>, and the specialized querying tools such as SPARQL cannot deal well with it.</p><p>Knowledge graph embeddings (KGEs), which aim at embedding entities and relations into low-dimensional continuous vectors, have thrived in the last decade <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref>. Specifically, KGEs have found wide adoptions in the simple KG completion problem (?, ?, ?), which features a single head entity ?, a relation ? and the missing tail entity. However, real-world queries can be more complicated with imputed edges, multiple source entities, Existential Positive First-Order (EPFO) logic, and unknown intermediates, namely, the complex logical queries.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates a decomposed query graph for the complex logical query "What musical instrument did Minnesota-born Nobel Prize winner play?". The query composes two source entities ("Minnesota" &amp; "Nobel Prize", denoted as " "), First-Order logic operator conjunction (?), unknown intermediate entities ("people that were born in Minnesota and won Nobel Prize", denoted as " "), unknown target entities ("musical instruments they may play", denoted as " "), and potential missing edges. A major challenge of answering such query is the exponential complexity along its growing combinations of hops and logical operators. Additionally, the rich context information surrounding multiple entities and relations in a single query should also be taken into account during reasoning. Consequently, such query goes beyond the capability of existing KGE-based approaches. First, most KGEs' architectures are shallow and use static vectors, limiting their expressiveness and capacity to capture massive patterns. Second, the training objective of recovering first-order missing links does not comply with the high-order graph nature of complex logical queries, and hence KGEs cannot handle complex queries without training auxiliary logical functions on sampled supervised datasets <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>. In addition, existing KG benchmark datasets <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43]</ref> usually consist of limited taskspecific types of queries, practically prohibiting KGE methods from generalizing well to queries with out-of-domain types.</p><p>Contributions. In this work, we propose to learn deep knowledge representations for answering complex queries from these two perspectives: architecture and training objective. At the architecture level, the goal is to design a deep model specified for KGs such that the complex logical queries can be handled. At the training level, the model is expected to learn general instead of task-specific knowledge from KGs and is thus enabled with strong generalizability for out-of-domain queries. To this end, we present kgTransformer-a Transformer-based GNN architecture-with self-supervised pretraining strategies for handling complex logical queries.</p><p>kgTransformer. We develop kgTransformer to encode KGs. Specifically, to represent relations in KGs, we propose the Triple Transformation strategy that turns relations to relation-nodes and thus transforms a KG into a directed graph without edge attributes.</p><p>To further enlarge the model capacity with low computation cost, we adopt the Mixture-of-Experts strategy to leverage the sparse activation nature of Transformer's feed-forward layers. Altogether, these strategies enable the kgTransformer architecture with highcapacity and computational efficiency, making it capable of answering the EPFO queries on KGs.</p><p>Masked Pre-Training. To further improve kgTransformer's generalizability, we introduce a masked pre-training framework to train it. We formulate complex logical query answering as a masked prediction problem. During pre-training, we randomly sample subgraphs from KGs and mask random entities for prediction. It includes two sequential stages of dense initialization, which targets at enriching the model by training on dense and arbitrary-shaped contexts, and sparse refinement, which is trained on sparse and clean meta-graphs to mitigate the gap between pre-training and downstream queries.</p><p>Extensive experiments on two widely-used KG benchmarks, i.e., FB15k-237 and NELL995, demonstrate kgTransformer's performance advantages over state-of-the-art-particularly KGE-basedapproaches on nine in-domain and out-of-domain downstream reasoning challenges. Additionally, the case studies suggest that masked pre-training can endow kgTransformer's reasoning with explainability and interpretability via providing predictions over unknown intermediates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The EPFO Logical Queries</head><p>We introduce the Existential Positive First-Order (EPFO) logical queries on KGs <ref type="bibr" target="#b27">[28]</ref> and identify the unique challenges.</p><p>EPFO. Let G = (E, R) denote a KG, where ? ? E denotes an entity and ? ? R is a binary predicate (or relation) ? : E ? E ? {True, False} that indicates whether a relation holds for a pair of entities. Given the First-Order logical existential (?) and conjunctive (?) operations, the conjunctive queries are defined as:</p><formula xml:id="formula_0">Q [?] ??? : ?? 1 , . . . , ? ? .? 1 ? . . . ? ? ? where ? ? = ? (?, ?), with ? ? {?, ? 1 , . . . , ? ? }, ? ? E, ? ? R or ? ? = ? (?, ? ? ), with ?, ? ? ? {?, ? 1 , . . . , ? ? }, ? ? ? ? , ? ? R.<label>(1)</label></formula><p>where ? refers to the (unknown) target entity of the query, ? 1 , . . . , ? ? refer to existentially quantified bound variables (i.e., unknown intermediate entity sets), and ? refers to the source entity. Given the query Q, the goal is to find its target entity set A ? E that satisfies ? ? A iff Q [?] is true.</p><p>Besides the conjunctive queries, EPFO also covers the disjunctive (?) queries. A rule-of-thumb practice is to transform an EPFO query into the Disjunctive Normal Form <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28]</ref>. In other words, a disjunctive query can be decomposed into several conjunctive queries, and a rule can be applied to synthesize conjunctive results for disjunctive predictions.</p><p>Challenges. Compared to the KG completion task in which the KGE-based methods are prevalent, the EPFO queries can be multihop; their numerous combinations the test reasoner's out-ofdomain generalizability. All these characteristics together pose the following unique challenges to reasoners:</p><p>? Exponential complexity: The complexity of EPFO queries grows exponentially as the hop increases <ref type="bibr" target="#b26">[27]</ref>, requiring highcapacity and advanced models to handle them. KGE-based reasoners rely on embeddings and simple operators <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref> to reason in a "left-to-right" autoregressive fashion. However, there are evidences <ref type="bibr" target="#b0">[1]</ref> showing that such models' performance gradually saturates as the embedding dimension grows to 1000. Additionally, during reasoning, the first-encoded entities are unaware of the later-encoded, ignoring the useful bidirectional interactions. In summary, these challenges make EPFO queries different from conventional KG completion, which only involves single-hop and single-type queries. In this work, we explore how to effectively handle EPFO queries with the pre-training and fine-tuning paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The KG Pre-Training Framework</head><p>In this section, we introduce kgTransformer-a Transformer-based graph neural network (GNN)-for handling EPFO queries on KGs. To endow kgTransformer with strong generalization, we design a masked pre-training and fine-tuning framework. The overall KG pre-training and reasoning framework is illustrated in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The kgTransformer Architecture</head><p>As discussed above, the relatively simple architecture of KGE-based reasoners limits their expressiveness. To overcome this issue, we propose a Transformer-based GNN architecture, kgTransformer, with the Mixture-of-Expert strategy to scale up model parameters while keeping its computational efficiency.</p><p>Transformer for KGs. Transformer <ref type="bibr" target="#b37">[38]</ref>, a neural architecture originally proposed to handle sequences, has achieved early success in the graph domain <ref type="bibr" target="#b17">[18]</ref>. To apply transformers to KGs, there are two questions to answer: 1) How to encode the node adjacency in graph structures, and 2) how to model both entities and relations.</p><p>First, there have been well-trodden practices for encoding node adjacency in graph transformers. For node and graph classification, it is common to view graphs as sequences of tokens with positional encoding, ignoring the adjacency matrices <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b46">47]</ref>. For link prediction, however, adjacency matrices can be crucial and should be masked to self-attention for better performance <ref type="bibr" target="#b17">[18]</ref>. As EPFO reasoning is intrinsically a link prediction problem, we follow HGT <ref type="bibr" target="#b17">[18]</ref> to mask adjacency matrices to self-attention.</p><p>Second, how to incorporate both entities and relations into Transformer's computation has thus far seldom studied. Here, we present the Triple Transformation operation, denoted as function ?(?), to turn relations into relation-nodes and consequently transform KGs into directed graphs without edge attributes.</p><p>For each directed triple (?, ?, ?) in G with ? (?, ?) = True, we create a node ? ?? in ?(G) (i.e., a relation-node) that connects to both ? and ? . By mapping each relation edge in G to a relationnode, the resultant graph becomes ?(G) = (E ? , R ? ) where E ? = E ? {? ?? |? (?, ?) = True} and the unattributed edge set R ? = {? : ? (?, ? ?? ) = ? (? ?? , ?) = True|? (?, ?) = True}. In practice, the computational cost of the triple transformation is low as the reasoning graphs for EPFO queries are usually very small and sparse (~10 1 entities and relations).</p><p>Given the input embeddings x (0) ? ? R ? for ? ? E ? with dimension ?, we add a special node-type embedding ? I(? ? E) ? R ? to distinguish whether it is an entity-node or a relation-node. In the ?-th layer of kgTransformer, ? ? = ?/? and ? denotes the number of attention heads, and the multi-head attention is computed as</p><formula xml:id="formula_1">Attn (?) ? = softmax(? ? ?/ ?? ? ? )? ,<label>(2)</label></formula><p>where ? = x</p><formula xml:id="formula_2">(?-1) ? W ? and {?, ? } = ? ?N (?) x (?-1) ? {W ? , W ? }.</formula><p>Here W {?,?,? } ? R ??? ? , denotes concatenation, and N (?) refers to node ?'s neighbor set.</p><p>Next, the feed-forward network FFN(?) is applied to attentionweighted outputs projected by W ? ? R ??? as</p><formula xml:id="formula_3">x (?) ? = FFN( ? ?=1 Attn (?) ? ? W ? ), FFN(x) = ? (xW 1 + b 1 )W 2 + b 2 (3) where W 1 ? R ??4? , W 2 ? R 4??? ,</formula><p>and ? is the activation function (e.g., GeLU <ref type="bibr" target="#b13">[14]</ref>). Note that FFN is critical for Transformers to capture massive patterns and proved equivalent to the key-value networks <ref type="bibr" target="#b10">[11]</ref>.</p><p>Different from 1) existing KGE-based reasoners with the "leftto-right" reasoning order and 2) the sequence encoder-based model <ref type="bibr" target="#b19">[20]</ref> that can only reason on acyclic query graphs, kgTransformer designs an architecture to aggregate information from all directions in each layer's computation, making it more flexible and capable of answering queries in arbitrary shapes. Figure <ref type="figure">2 (a)</ref> illustrates the kgTransformer architecture.</p><p>Mixture-of-Experts (MoE). Though kgTransformer's architecture allows it to capture complicated reasoning patterns, the number of its parameters soars up quadratically with the embedding dimension ?, which is a common challenge faced by Transformers.</p><p>As mentioned earlier, Transformer's FFN is known to be equivalent to the key-value networks <ref type="bibr" target="#b10">[11]</ref> where a key activates a few values as responses. Given ? ? R ? , FFN's intermediate activation</p><formula xml:id="formula_4">? (xW 1 + b 1 ) = [x 0 , x 1 , .</formula><p>.., x ? , ..., x ? , ..., x 4? ] = [0, 0, ..., x ? , ..., x ? , ..., 0] Most of elements are 0 (4) can be extremely sparse, where W 1 ? R ??4? . The level of sparsity varies with tasks. For instance, a recent study <ref type="bibr" target="#b48">[49]</ref> shows that usually less than 5% of neurons are activated for each input in NLP. In our preliminary experiments on EPFO queries (Cf. Figure <ref type="figure">3</ref>), only 10%-20% neurons are activated for certain inputs (except the last decoder layer).</p><p>Thus, we propose to leverage the sparsity of kgTransformer via the Mixture-of-Experts (MoE) strategy <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>. MoE first decomposes a large FFN into blockwise experts, and then utilizes a light gating network to select experts to be involved in the computation. For example, an FFN with W 1 ? R ??16? and W 2 ? R 16??? (4 times larger than that in Equation <ref type="formula">4</ref>) can be transformed into</p><formula xml:id="formula_5">FFN(x) = {FFN (?) Exp (x) = ? (xW (?) 1 + b 1 )W (?) 2 + b 2 | ? ? [1, 8], ? ? N} (5) where W (?) 1 ? R ??2? , W (?) 2 ? R 2??? . Each FFN (?)</formula><p>Exp is referred to as an expert. Given a gating network, which is usually a trainable matrix W gate ? R ??? with ? as the number of experts, we can select the top-2 experts for computation <ref type="bibr" target="#b20">[21]</ref> as</p><formula xml:id="formula_6">x (?) ? = ?? (?,?) ?? ? ? FFN (?) Exp (x (?-1) ? )<label>(6)</label></formula><p>where </p><formula xml:id="formula_7">? = {(?, ?)|? ? softmax(Top-2(x (?-1) ? W gate )),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Triple Transform kgTransformer Layer ( )</head><formula xml:id="formula_8">Residual Conn. Mixture-of-Experts (MoE) Multi- Head Attention ?L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D w E + z W M M S S d h c / 6 7 2 E / / h r V Z T V A = " &gt; A A A C 3 3 i c j V H L S s N A F D 2 N r 1 p f U X e 6 C R b B V U l E 0 G X R j Q s X F e w D 2 l K S d F q H 5 k U y E U s p u H M n b v 0 B t / o 3 4 h / o X 3 h n T E E t o h M y c + b c e 8 7 M n e t E H k + E a b 7 m t J n Z u f m F / G J h a X l l d U 1 f 3 6 g l Y R q 7 r O q G X h g 3 H D t h H g 9 Y V X D h s U Y U M 9 t 3 P F Z 3 B i c y X r 9 i c c L D 4 E I M I 9 b 2 7 X 7 A e 9 y 1 B V E d f a s l u M 8 S o y X Y t X B 6 I 7 V y M T o b j z t 6 0 S y Z a h j T w M p A E d m o h P o L W u g i h I s U P h g C C M I e b C T 0 N W H B R E R c G y P i Y k J c x R n G K J A 2 p S x G G T a x A 5 r 7 t G t m b E B 7 6 Z k o t U u n e P T H p D S w S 5 q Q 8 m L C 8 j R D x V P l L N n f v E f K U 9 5 t S K u T e f n E C l w S + 5 d u k v l f n a x F o I c j V Q O n m i L F y O r c z C V V r y J v b n y p S p B D R J z E X Y r H h F 2 l n L y z o T S J q l 2 + r a 3 i b y p T s n L v Z r k p 3 u U t q c H W z 3 Z O g 9 p + y T J L 1 v l B s X y c t T q P b e x g j / p 5 i D J O U U G V v G / w i C c 8 a 7 Z 2 q 9 1 p 9 5 + p W i 7 T b O L b 0 B 4 + A K J h m v E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D w E + z W M M S S d h c / 6 7 2 E / / h r V Z T V A = " &gt; A A A C 3 3 i c j V H L S s N A F D 2 N r 1 p f U X e 6 C R b B V U l E 0 G X R j Q s X F e w D 2 l K S d F q H 5 k U y E U s p u H M n b v 0 B t / o 3 4 h / o X 3 h n T E E t o h M y c + b c e 8 7 M n e t E H k + E a b 7 m t J n Z u f m F / G J h a X l l d U 1 f 3 6 g l Y R q 7 r O q G X h g 3 H D t h H g 9 Y V X D h s U Y U M 9 t 3 P F Z 3 B i c y X r 9 i c c L D 4 E I M I 9 b 2 7 X 7 A e 9 y 1 B V E d f a s l u M 8 S o y X Y t X B 6 I 7 V y M T o b j z t 6 0 S y Z a h j T w M p A E d m o h P o L W u g i h I s U P h g C C M I e b C T 0 N W H B R E R c G y P i Y k J c x R n G K J A 2 p S x G G T a x A 5 r 7 t G t m b E B 7 6 Z k o t U u n e P T H p D S w S 5 q Q 8 m L C 8 j R D x V P l L N n f v E f K U 9 5 t S K u T e f n E C l w S + 5 d u k v l f n a x F o I c j V Q O n m i L F y O r c z C V V r y J v b n y p S p B D R J z E X Y r H h F 2 l n L y z o T S J q l 2 + r a 3 i b y p T s n L v Z r k p 3 u U t q c H W z 3 Z O g 9 p + y T J L 1 v l B s X y c t T q P b e x g j / p 5 i D J O U U G V v G / w i C c 8 a 7 Z 2 q 9 1 p 9 5 + p W i 7 T b O L b 0 B 4 + A K J h m v E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D w E + z W M M S S d h c / 6 7 2 E / / h r V Z T V A = " &gt; A A A C 3 3 i c j V H L S s N A F D 2 N r 1 p f U X e 6 C R b B V U l E 0 G X R j Q s X F e w D 2 l K S d F q H 5 k U y E U s p u H M n b v 0 B t / o 3 4 h / o X 3 h n T E E t o h M y c + b c e 8 7 M n e t E H k + E a b 7 m t J n Z u f m F / G J h a X l l d U 1 f 3 6 g l Y R q 7 r O q G X h g 3 H D t h H g 9 Y V X D h s U Y U M 9 t 3 P F Z 3 B i c y X r 9 i c c L D 4 E I M I 9 b 2 7 X 7 A e 9 y 1 B V E d f a s l u M 8 S o y X Y t X B 6 I 7 V y M T o b j z t 6 0 S y Z a h j T w M p A E d m o h P o L W u g i h I s U P h g C C M I e b C T 0 N W H B R E R c G y P i Y k J c x R n G K J A 2 p S x G G T a x A 5 r 7 t G t m b E B 7 6 Z k o t U u n e P T H p D S w S 5 q Q 8 m L C 8 j R D x V P l L N n f v E f K U 9 5 t S K u T e f n E C l w S + 5 d u k v l f n a x F o I c j V Q O n m i L F y O r c z C V V r y J v b n y p S p B D R J z E X Y r H h F 2 l n L y z o T S J q l 2 + r a 3 i b y p T s n L v Z r k p 3 u U t q c H W z 3 Z O g 9 p + y T J L 1 v l B s X y c t T q P b e x g j / p 5 i D J O U U G V v G / w i C c 8 a 7 Z 2 q 9 1 p 9 5 + p W i 7 T b O L b 0 B 4 + A K J h m v E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 3 0 n v n J k y K B l K c C y m N n r q T x J g V 4 = " &gt; A A A C t X i c j V L L S g M x F D 0 d X 7 V W r W s 3 g 0 V w V T J u d C n o w m U F + 4 B a Z C Z N a + y 8 T D J C K f 6 A W z 9 O / A P 9 C 2 / i C G o R z T A z J + f e c 5 K b m y i P p T a M v V S 8 p e W V 1 b X q e m 2 j X t v c 2 m 7 U u z o r F B c d n s W Z 6 k e h F r F M R c d I E 4 t + r k S Y R L H o R d N T G + / d C 6 V l l l 6 a W S 6 G S T h J 5 V j y 0 B D V v m 4 0 W Y u 5 4 S + C o A R N l C N r P O M K I 2 T g K J B A I I U h H C O E p m e A A A w 5 c U P M i V O E p I s L P K B G 2 o K y B G W E x E 7 p O 6 H Z o G R T m l t P 7 d S c V o n p V a T 0 s U + a j P I U Y b u a 7 + K F c 7 b s b 9 5 z 5 2 n 3 N q N / V H o l x B r c E P u X 7 j P z v z p b i 8 E Y x 6 4 G S T X l j r H V 8 d K l c K d i d + 5 / q c q Q Q 0 6 c x S O K K 8 L c K T / P 2 X c a 7 W q 3 Z x u 6 + K v L t K y d 8 z K 3 w J v d J f U 3 + N n N R d A 9 b A W s F V w w V L G L P R x Q G 4 9 w g n O 0 0 S H L E R 7 x 5 J 1 5 t 9 7 d x z 3 w K u W F 2 M G 3 4 e l 3 4 Y W M 3 A = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c n Y S 2 y p 2 s h T r m x 0 k c C S S S P F V d 0 g = " &gt; A A A C 1 H i c j V L L S g M x F D 0 d X 7 V W r S 5 1 M 1 g E V 2 X G j S 4 F N y 5 c V L A P a E v J T N M a O i 9 m M m I Z C u 7 c i V / h V v 9 G / A P 9 C 2 / i F N Q i m m G S k 3 P v O c l N 4 k S e S K R l v R a M h c W l 5 Z X i a m m t v L 6 x W d k q N 5 M w j V 3 e c E M v j N s O S 7 g n A t 6 Q Q n q 8 H c W c + Y 7 H W 8 7 4 V M V b 1 z x O R B h c y k n E e z 4 b B W I o X C a J 6 l d 2 u l L 4 P D G 7 k t 9 I Z 5 j p U c j s f D r t V 6 p W z d L N n A d 2 D q r I W z 2 s v K C L A U K 4 S O G D I 4 A k 7 I E h o a 8 D G x Y i 4 n r I i I s J C R 3 n m K J E 2 p S y O G U w Y s f U j 2 j W y d m A 5 s o z 0 W q X V v H o j 0 l p Y p 8 0 I e X F h N V q p o 6 n 2 l m x v 3 l n 2 l P t b U K j k 3 v 5 x E p c E f u X b p b 5 X 5 2 q R W K I Y 1 2 D o J o i z a j q 3 N w l 1 a e i d m 5 + q U q S Q 0 S c w g O K x 4 R d r Z y d s 6 k 1 i a 5 d n S 3 T 8 T e d q V g 1 d / P c F O 9 q l 3 T B 9 s / r n A f N w 5 p t 1 e w L C 0 X s Y g 8 H d I 1 H O M E Z 6 m i Q 5 S 0 e 8 Y R n g x l 3 x v 3 n U z A K + Z v Y x r d m P H w A 8 a a Z p g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c n Y S 2 y p 2 s h T r m x 0 k c C S S S P F V d 0 g = " &gt; A A A C 1 H i c j V L L S g M x F D 0 d X 7 V W r S 5 1 M 1 g E V 2 X G j S 4 F N y 5 c V L A P a E v J T N M a O i 9 m M m I Z C u 7 c i V / h V v 9 G / A P 9 C 2 / i F N Q i m m G S k 3 P v O c l N 4 k S e S K R l v R a M h c W l 5 Z X i a m m t v L 6 x W d k q N 5 M w j V 3 e c E M v j N s O S 7 g n A t 6 Q Q n q 8 H c W c + Y 7 H W 8 7 4 V M V b 1 z x O R B h c y k n E e z 4 b B W I o X C a J 6 l d 2 u l L 4 P D G 7 k t 9 I Z 5 j p U c j s f D r t V 6 p W z d L N n A d 2 D q r I W z 2 s v K C L A U K 4 S O G D I 4 A k 7 I E h o a 8 D G x Y i 4 n r I i I s J C R 3 n m K J E 2 p S y O G U w Y s f U j 2 j W y d m A 5 s o z 0 W q X V v H o j 0 l p Y p 8 0 I e X F h N V q p o 6 n 2 l m x v 3 l n 2 l P t b U K j k 3 v 5 x E p c E f u X b p b 5 X 5 2 q R W K I Y 1 2 D o J o i z a j q 3 N w l 1 a e i d m 5 + q U q S Q 0 S c w g O K x 4 R d r Z y d s 6 k 1 i a 5 d n S 3 T 8 T e d q V g 1 d / P c F O 9 q l 3 T B 9 s / r n A f N w 5 p t 1 e w L C 0 X s Y g 8 H d I 1 H O M E Z 6 m i Q 5 S 0 e 8 Y R n g x l 3 x v 3 n U z A K + Z v Y x r d m P H w A 8 a a Z p g = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G w / z J 8 h 6 1 D 6 7 d j M 5 r v 4 c M I D g b L k = " &gt; A A A C 3 3 i c j V H L S s N A F D 2 N r 1 p f U X e 6 C R b B V U n c 6 L L o x o W L C v Y B t p Q k n d a h e Z F M x B I K 7 t y J W 3 / A r f 6 N + A f 6 F 9 4 Z U 1 C L 6 I T M n D n 3 n j N z 5 z q R x x N h m q 8 F b W Z 2 b n 6 h u F h a W l 5 Z X d P X N x p J m M Y u q 7 u h F 8 Y t x 0 6 Y x w N W F 1 x 4 r B X F z P Y d j z W d 4 b G M N 6 9 Y n P A w O B e j i H V 8 e x D w P n d t Q V R X 3 2 o L 7 r P E a A t 2 L Z x + p l Y u s t P x u K u X z Y q p h j E N r B y U k Y 9 a q L + g j R 5 C u E j h g y G A I O z B R k L f B S y Y i I j r I C M u J s R V n G G M E m l T y m K U Y R M 7 p H l A u 4 u c D W g v P R O l d u k U j / 6 Y l A Z 2 S R N S X k x Y n m a o e K q c J f u b d 6 Y 8 5 d 1 G t D q 5 l 0 + s w C W x f + k m m f / V y V o E + j h U N X C q K V K M r M 7 N X V L 1 K v L m x p e q B D l E x E n c o 3 h M 2 F X K y T s b S p O o 2 u X b 2 i r + p j I l K / d u n p v i X d 6 S G m z 9 b O c 0 a O x X L L N i n Z n l 6 l H e 6 i K 2 s Y M 9 6 u c B q j h B D X X y v s E j n v C s 2 d q t d q f d f 6 Z q h V y z i W 9 D e / g A o S G a 7 Q = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D w E + z W M M S S d h c / 6 7 2 E / / h r V Z T V A = " &gt; A A A C 3 3 i c j V H L S s N A F D 2 N r 1 p f U X e 6 C R b B V U l E 0 G X R j Q s X F e w D 2 l K S d F q H 5 k U y E U s p u H M n b v 0 B t / o 3 4 h / o X 3 h n T E E t o h M y c + b c e 8 7 M n e t E H k + E a b 7 m t J n Z u f m F / G J h a X l l d U 1 f 3 6 g l Y R q 7 r O q G X h g 3 H D t h H g 9 Y V X D h s U Y U M 9 t 3 P F Z 3 B i c y X r 9 i c c L D 4 E I M I 9 b 2 7 X 7 A e 9 y 1 B V E d f a s l u M 8 S o y X Y t X B 6 I 7 V y M T o b j z t 6 0 S y Z a h j T w M p A E d m o h P o L W u g i h I s U P h g C C M I e b C T 0 N W H B R E R c G y P i Y k J c x R n G K J A 2 p S x G G T a x A 5 r 7 t G t m b E B 7 6 Z k o t U u n e P T H p D S w S 5 q Q 8 m L C 8 j R D x V P l L N n f v E f K U 9 5 t S K u T e f n E C l w S + 5 d u k v l f n a x F o I c j V Q O n m i L F y O r c z C V V r y J v b n y p S p B D R J z E X Y r H h F 2 l n L y z o T S J q l 2 + r a 3 i b y p T s n L v Z r k p 3 u U t q c H W z 3 Z O g 9 p + y T J L 1 v l B s X y c t T q P b e x g j / p 5 i D J O U U G V v G / w i C c 8 a 7 Z 2 q 9 1 p 9 5 + p W i 7 T b O L b 0 B 4 + A K J h m v E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D w E + z W M M S S d h c / 6 7 2 E / / h r V Z T V A = " &gt; A A A C 3 3 i c j V H L S s N A F D 2 N r 1 p f U X e 6 C R b B V U l E 0 G X R j Q s X F e w D 2 l K S d F q H 5 k U y E U s p u H M n b v 0 B t / o 3 4 h / o X 3 h n T E E t o h M y c + b c e 8 7 M n e t E H k + E a b 7 m t J n Z u f m F / G J h a X l l d U 1 f 3 6 g l Y R q 7 r O q G X h g 3 H D t h H g 9 Y V X D h s U Y U M 9 t 3 P F Z 3 B i c y X r 9 i c c L D 4 E I M I 9 b 2 7 X 7 A e 9 y 1 B V E d f a s l u M 8 S o y X Y t X B 6 I 7 V y M T o b j z t 6 0 S y Z a h j T w M p A E d m o h P o L W u g i h I s U P h g C C M I e b C T 0 N W H B R E R c G y P i Y k J c x R n G K J A 2 p S x G G T a x A 5 r 7 t G t m b E B 7 6 Z k o t U u n e P T H p D S w S 5 q Q 8 m L C 8 j R D x V P l L N n f v E f K U 9 5 t S K u T e f n E C l w S + 5 d u k v l f n a x F o I c j V Q O n m i L F y O r c z C V V r y J v b n y p S p B D R J z E X Y r H h F 2 l n L y z o T S J q l 2 + r a 3 i b y p T s n L v Z r k p 3 u U t q c H W z 3 Z O g 9 p + y T J L 1 v l B s X y c t T q P b e x g j / p 5 i D J O U U G V v G / w i C c 8 a 7 Z 2 q 9 1 p 9 5 + p W i 7 T b O L b 0 B 4 + A K J h m v E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D w E + z W M M S S d h c / 6 7 2 E / / h r V Z T V A = " &gt; A A A C 3 3 i c j V H L S s N A F D 2 N r 1 p f U X e 6 C R b B V U l E 0 G X R j Q s X F e w D 2 l K S d F q H 5 k U y E U s p u H M n b v 0 B t / o 3 4 h / o X 3 h n T E E t o h M y c + b c e 8 7 M n e t E H k + E a b 7 m t J n Z u f m F / G J h a X l l d U 1 f 3 6 g l Y R q 7 r O q G X h g 3 H D t h H g 9 Y V X D h s U Y U M 9 t 3 P F Z 3 B i c y X r 9 i c c L D 4 E I M I 9 b 2 7 X 7 A e 9 y 1 B V E d f a s l u M 8 S o y X Y t X B 6 I 7 V y M T o b j z t 6 0 S y Z a h j T w M p A E d m o h P o L W u g i h I s U P h g C C M I e b C T 0 N W H B R E R c G y P i Y k J c x R n G K J A 2 p S x G G T a x A 5 r 7 t G t m b E B 7 6 Z k o t U u n e P T H p D S w S 5 q Q 8 m L C 8 j R D x V P l L N n f v E f K U 9 5 t S K u T e f n E C l w S + 5 d u k v l f n a x F o I c j V Q O n m i L F y O r c z C V V r y J v b n y p S p B D R J z E X Y r H h F 2 l n L y z o T S J q l 2 + r a 3 i b y p T s n L v Z r k p 3 u U t q c H W z 3 Z O g 9 p + y T J L 1 v l B s X y c t T q P b e x g j / p 5 i D J O U U G V v G / w i C c 8 a 7 Z 2 q 9 1 p 9 5 + p W i 7 T b O L b 0 B 4 + A K J h m v E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D w E + z W M M S S d h c / 6 7 2 E / / h r V Z T V A = " &gt; A A A C 3 3 i c j V H L S s N A F D 2 N r 1 p f U X e 6 C R b B V U l E 0 G X R j Q s X F e w D 2 l K S d F q H 5 k U y E U s p u H M n b v 0 B t / o 3 4 h / o X 3 h n T E E t o h M y c + b c e 8 7 M n e t E H k + E a b 7 m t J n Z u f m F / G J h a X l l d U 1 f 3 6 g l Y R q 7 r O q G X h g 3 H D t h H g 9 Y V X D h s U Y U M 9 t 3 P F Z 3 B i c y X r 9 i c c L D 4 E I M I 9 b 2 7 X 7 A e 9 y 1 B V E d f a s l u M 8 S o y X Y t X B 6 I 7 V y M T o b j z t 6 0 S y Z a h j T w M p A E d m o h P o L W u g i h I s U P h g C C M I e b C T 0 N W H B R E R c G y P i Y k J c x R n G K J A 2 p S x G G T a x A 5 r 7 t G t m b E B 7 6 Z k o t U u n e P T H p D S w S 5 q Q 8 m L C 8 j R D x V P l L N n f v E f K U 9 5 t S K u T e f n E C l w S + 5 d u k v l f n a x F o I c j V Q O n m i L F y O r c z C V V r y J v b n y p S p B D R J z E X Y r H h F 2 l n L y z o T S J q l 2 + r a 3 i b y p T s n L v Z r k p 3 u U t q c H W z 3 Z O g 9 p + y T J L 1 v l B s X y c t T q P b e x g j / p 5 i D J O U U G V v G / w i C c 8 a 7 Z 2 q 9 1 p 9 5 + p W i 7 T b O L b 0 B 4 + A K J h m v E = &lt; / l a t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D w E + z W M M S S d h c / 6 7 2 E / / h r V Z T V A = " &gt;</head><formula xml:id="formula_9">A A A C 3 3 i c j V H L S s N A F D 2 N r 1 p f U X e 6 C R b B V U l E 0 G X R j Q s X F e w D 2 l K S d F q H 5 k U y E U s p u H M n b v 0 B t / o 3 4 h / o X 3 h n T E E t o h M y c + b c e 8 7 M n e t E H k + E a b 7 m t J n Z u f m F / G J h a X l l d U 1 f 3 6 g l Y R q 7 r O q G X h g 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H D t h H g 9 Y V X D h s U Y U M 9 t 3 P F Z 3 B i c y X r 9 i c c L D 4 E I M I 9 b 2 7 X 7 A e 9 y 1 B V E d f a s l u M 8 S o y X Y t X B 6 I 7 V y M T o b j z t 6 0 S y Z a h j T w M p A E d m o h P o L W u g i h I s U P h g C C M I e b C T 0 N W H B R E R c G y P i Y k J c x R n G K J A 2 p S x G G T a x A 5 r 7 t G t m b E B 7 6 Z k o t U u n e P T H p D S w S 5 q</head><formula xml:id="formula_10">Q 8 m L C 8 j R D x V P l L N n f v E f K U 9 5 t S K u T e f n E C l w S + 5 d u k v l f n a x F o I c j V Q O n m i L F y O r c z C V V r y J v b n y p S p B D R J z E X Y r H h F 2 l</formula><p>n L y z o T S J q l 2 + r a 3 i b y p T s n L v Z r k p 3 u U t q c H W z 3 Z O g 9 p + y T J L 1 v l B s X y c t T q P b e x g j / p 5 i D J O U U G V v G / w i C c 8 a 7 Z 2 q 9 1 p 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">+ p W i 7 T b O L b 0 B 4 + A K J h m v E = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D w E + z W M M S S d h c / 6 7 2 E / / h r V Z T V A = " &gt;</head><formula xml:id="formula_11">A A A C 3 3 i c j V H L S s N A F D 2 N r 1 p f U X e 6 C R b B V U l E 0 G X R j Q s X F e w D 2 l K S d F q H 5 k U y E U s p u H M n b v 0 B t / o 3 4 h / o X 3 h n T E E t o h M y c + b c e 8 7 M n e t E H k + E a b 7 m t J n Z u f m F / G J h a X l l d U 1 f 3 6 g l Y R q 7 r O q G X h g 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H D t h H g 9 Y V X D h s U Y U M 9 t 3 P F Z 3 B i c y X r 9 i c c L D 4 E I M I 9 b 2 7 X 7 A e 9 y 1 B V E d f a s l u M 8 S o y X Y t X B 6 I 7 V y M T o b j z t 6 0 S y Z a h j T w M p A E d m o h P o L W u g i h I s U P h g C C M I e b C T 0 N W H B R E R c G y P i Y k J c x R n G K J A 2 p S x G G T a x A 5 r 7 t G t m b E B 7 6 Z k o t U u n e P T H p D S w S 5 q</head><formula xml:id="formula_12">Q 8 m L C 8 j R D x V P l L N n f v E f K U 9 5 t S K u T e f n E C l w S + 5 d u k v l f n a x F o I c j V Q O n m i L F y O r c z C V V r y J v b n y p S p B D R J z E X Y r H h F 2 l</formula><p>n L y z o T S J q l 2 + r a 3 i b y p T s n L v Z r k p 3 u U t q c H W z 3 Z O g 9 p + y T J L 1 v l B s X y c t T q P b e x g j / p 5 i D J O U U G V v G / w i C c 8 a 7 Z 2 q 9 1 p 9 5 + p W i 7 T b O L b 0 B 4 + A K J h m v E = &lt; / l a t e x i t &gt; in Equation <ref type="formula">4</ref>. During inference, all experts are included in the computation and the FFN output becomes the weighted average of every expert's output. The implementation details can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Masked Pre-Training and Fine-Tuning</head><p>To further improve the generalizability of kgTransformer, we present a masked pre-training and fine-tuning framework for KG reasoning, which is illustrated in Figure <ref type="figure">2</ref> (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Two-stage Pre-training: Initialization and Refinement</head><p>Most existing EPFO reasoners train the embeddings over a limited number of sampled queries with few specific query types in a supervised manner <ref type="bibr" target="#b27">[28]</ref>. The low coverage of entities and relations in original KGs in training usually leads to poor transferability to queries with unseen entities in testing. In addition, the reasoners are expected to answer out-of-domain types of queries after training. For example, the standard benchmark <ref type="bibr" target="#b27">[28]</ref> trains the model on 5 types of queries (1p, 2p, 3p, 2i, and 3i), and asks the model to test on additional types of queries (ip, pi, 2u, and up).</p><p>To overcome this challenge, we propose to do masked pretraining for kgTransformer. The main idea is to randomly sample arbitrary-shaped masked subgraphs from the original KGs' training set to pre-train the model. Its advantages lie in the large receptive fields of sampled subgraphs, diverse shapes of queries, and the high coverage over original KGs. To fully explore general knowledge from pre-training, we should sample masked query graphs as dense and large as possible; however, the query graphs during downstream reasoning are usually small and sparse. Thus, such strategy would cause a mismatch between pre-training and downstream distributions, resulting in performance degradation.</p><p>To mitigate the issue, we introduce a two-stage masked pretraining strategy for kgTransformer. The first stage aims to initialize kgTransformer with KGs' general knowledge, and the second one further refines its ability for small and sparse queries during inference.</p><p>Stage 1: Dense Initialization. The goal of this stage is to enable kgTransformer with the general knowledge in KGs via masked pre-training over dense and large sampled subgraphs.</p><p>We use two random-walk-based strategies to sample on original KGs' training set. One is the random walk with restart (RWR), for which we make no assumption on the shapes of subgraphs it may sample. Another is a tree-based RWR strategy, for which we constrain the shape of sampled graphs as tree structures to cater conjunctive queries. We include a majority of induced relations between sampled entities for denser contexts.</p><p>Both methods are used to produce sampled query subgraphs. Given a kgTransformer parameterized by ? , a set of random sampled entities E mask that are masked by a special mask embedding turn the trainable input embeddings X = [x (0) ? 0 , ..., x (0)</p><p>? ? ] into mask corrupted X. The dense initialization stage seeks to optimize</p><formula xml:id="formula_13">L init = - ?? ? m ? E mask log ? ? (x ? m | X, A)<label>(7)</label></formula><p>where A is the adjacency matrix. The prediction is done simultaneously for all masked nodes, rather in an autoregressive manner.</p><p>Stage 2: Sparse Refinement. The goal of this stage is to further enhance the model's capacity by using meta-graph sampling for the EPFO queries that are commonly sparse and small. We adopt the basic EPFO patterns (1p, 2p, 3p, 2i, and 3i) as metagraphs, which involve no more than four entities and relations. No induced relations between entities are reserved. Following real query practices, except for source entities (" " in Figure <ref type="figure">2(b)</ref>), other entities are all masked but only the target entity (" ") is predicted in each query type.</p><p>Given each meta-graph type, the set of masked entities E mask = E inter ? E target where E inter and E target refer to the sets of intermediate and target entities, respectively. The sparse refinement stage only optimizes the error L refine on target entity ? t ? E target as the same form of L init as</p><formula xml:id="formula_14">L refine = - ?? ? t ? E target log ? ? (x ? t | X, A)<label>(8)</label></formula><p>During kgTransformer pre-training, these two stages are conducted sequentially rather than in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Fine-tuning</head><p>We introduce the fine-tuning strategy of kgTransformer for downstream reasoning tasks. Though with the sparse refinement stage during pre-training, our preliminary experiments (Cf. Table <ref type="table" target="#tab_3">2</ref>) show that fine-tuning is still necessary for kgTransformer as the labeled supervision information is used to tune the pre-trained model for downstream tasks. Specifically, we fine-tune the pre-trained kgTransformer using the downstream EPFO reasoning datasets in the form of masked prediction. Given a set of feasible answers E ? for a masked query graph, the fine-tuning loss function per query is formulated as</p><formula xml:id="formula_15">L ft = - 1 |E ? | ?? ? ? ? E ? log exp(x (?)? ? ? u ? ? ) exp(x (?)? ? ? u ? ? ) + ??E ? exp(x (?)? ? u ? )<label>(9</label></formula><p>) where other answers' predicted logits are ignored when computing the loss for one answer, and per query loss is averaged among all answers' losses. u ? refers to entity ?'s embedding in the decoder.</p><p>In practice, an intuitive strategy is to fine-tune the pre-trained kgTransformer on the corresponding downstream training set for each reasoning task. However, the recent NLP studies <ref type="bibr" target="#b25">[26]</ref> have demonstrated that the multi-task fine-tuning can avoid over-fitting and let tasks benefit from each other. Inspired from this observation, we first jointly fine-tune kgTransformer on all possible downstream training sets, and then fine-tune it for each single task. In addition, we observe that a combination of several downstream training sets can sometimes be much better than one single task's training set (Cf. Table <ref type="table">6</ref>). Finally, the best checkpoint for testing is selected on the basis of per task validation set performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Out-of-domain Generalization</head><p>To better test the model's generalizability to unseen queries, four specific types of out-of-domain queries, ip, pi, 2u, up, are provided only in the validation and test sets as illustrated in Figure <ref type="figure">2 (b)</ref>.</p><p>Among them, the conjunctive queries ip and pi can be represented as query graphs. For the disjunctive/union queries 2u and up, we adopt the idea of Disjunctive Normal Form <ref type="bibr" target="#b5">[6]</ref>. Specifically, we first predict on the decomposed conjunctive queries respectively, then normalize each probability distribution into its rank, and finally combine the ranks by taking the highest rank. Instead of taking the mean or max of probabilities, we find that re-scoring the entity prediction by its rank from the probability distribution in each decomposed conjunctive query is more effective, as the probability distributions for different decomposed queries can be of different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate kgTransformer's capacity to reason for EPFO queries with at least one imputed edge, i.e., for answers that cannot be obtained by direct KG traverses <ref type="bibr" target="#b28">[29]</ref>. We employ two benchmarks FB15k-237 and NELL995 with nine different reasoning challenges including both in-domain and out-of-domain queries following prior settings in Query2Box <ref type="bibr" target="#b27">[28]</ref>.</p><p>Datasets. The statistics of FB15k-237 <ref type="bibr" target="#b34">[35]</ref> and NELL995 <ref type="bibr" target="#b43">[44]</ref> can be found in Table <ref type="table">7</ref>. We use the standard training/validation/test edge splits <ref type="bibr" target="#b27">[28]</ref> to pre-train the kgTransformer model. G ????? contains the training edges, and the subgraphs are sampled from G ????? for pre-training. We search for hyper-parameters over G ????? . Here we do not include FB15k as it is known to suffer from major test leakage through inverse relations as illustrated in <ref type="bibr" target="#b34">[35]</ref>, which consequently proposes the updated dataset FB15k-237. See Table <ref type="table">8</ref> in Appendix for more detailed information about the datasets.</p><p>In addition, the query-answer datasets ? train , ? valid , ? test are constructed for logic query reasoning by Query2Box <ref type="bibr" target="#b27">[28]</ref>, including chain-shaped queries (1p, 2p, 3p), conjunctive queries (2i, 3i, ip, pi), and disjunctive queries (2u, up). For a query ?, the three sets of answers are obtained by performing subgraph matching over three graphs: ? ????? from G ????? , ? ????? from G ????? \G ????? and ? ???? from G ???? \G ????? . Therefore, it does not spoil the train/dev/test split in the original dataset. We use ? ????? for fine-tuning and training, ? ????? for validating, and ? ???? for testing. Such design allows us to test if the model can predict non-trivial answers that are unable to obtain by traversing the given KG.</p><p>The Evaluation Protocol. We follow the evaluation protocol in Query2Box <ref type="bibr" target="#b27">[28]</ref> including the filtering setting <ref type="bibr" target="#b2">[3]</ref> and metrics calculation. In filtering setting, since the answers to most queries are not unique, we rule out other correct answers when calculating the rank of one answer ?. Hits at K(Hits@Km) is used as the metric, which is different from Hits@K as it requires to first average on Hits@K per query and then average over all queries. See more details in Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Main Results</head><p>We compare kgTransformer with various methods based on both KGEs and sequence encoders, including GQE <ref type="bibr" target="#b11">[12]</ref>, Q2B <ref type="bibr" target="#b27">[28]</ref>,  BiQE <ref type="bibr" target="#b19">[20]</ref>, EmQL <ref type="bibr" target="#b33">[34]</ref>, and the state-of-the-art baseline CQD <ref type="bibr" target="#b0">[1]</ref>. Detail descriptions can be found in Appendix E. Table <ref type="table" target="#tab_2">1</ref> reports the Hits@3m results for all query types on FB15k-237 and NELL995. For FB15K-237, Note that the two-stage pretraining is sequentially adopted for FB15K-237 and only the second stage of pre-training is used for NELL995 (Cf. Section 4.2 for detailed analysis). Among most cases, kgTransformer obtains the best performance on both datasets. In contrast with BiQE, which is also based on Transformer, the kgTransformer model achieves average improvements of 6.4% (18.6% relative) and 5.7% (19.5% relative) without union operation on NELL995 and FB15k-237, respectively. It demonstrates the proposed method's architecture superiority in terms of the model capacity and generalizability. In addition, kgTransformer can perform union operations, supporting the complete set of EPFO queries.</p><p>Compared to the previous state-of-the-art CQD, kgTransformer obtains 2.4% (6.4% relative) and 3.5% (12.1% relative) improvements on average over NELL995 and FB15k-237, respectively. For out-ofdomain reasoning queries on FB15k-237, kgTransformer outperforms CQD by significant margins for most types, demonstrating pre-training's capability in helping the model gain out-of-domain generalizability.</p><p>The major deficiency of kgTransformer happens for the 1p query, which is identical to the conventional knowledge graph completion problem. First, kgTransformer's multi-layer Transformer architecture may be unfriendly to queries with limited contexts. In addition, the training objective of kgTransformer focuses on complex queries with multiple entities and relations instead of those with two entities and one relation in a triplet. We will the improvement of kgTransformer for the 1p query for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>Pre-Training and Fine-Tuning Strategies. We analyze the necessity of two-stage pre-training and the function of fine-tuning through ablation study on both datasets. The contributions of finetuning and the pre-training stages are summarized in Table <ref type="table" target="#tab_3">2</ref>, where the average Hits@3m results for all query types are reported.</p><p>We observe that without fine-tuning, the performance of kg-Transformer drops about 3% in absolute numbers for both datasets, and further without pre-training, the performance drops are enlarged to 7% on FB15k-237 and 11% on NELL995. This demonstrates that both pre-training and fine-tuning can help improve the model capacity of kgTransformer for complex query reasoning. In addition, the pre-trained kgTransformer without fine-tuning can achieve comparable (0.368 vs. 0.375 on NELL99) or even better (0.301 vs. 0.290 on FB15k-237) results than CQD <ref type="bibr" target="#b0">[1]</ref>-the previously state-of-the-art method.</p><p>Furthermore, two pre-training stages introduce different inductive biases towards the underlying logic. kgTransformer with only stage 1 pre-training assumes the dense correlation between pairs of entities, while its stage 2 pre-training assumes that the relations Figure <ref type="figure">4</ref>: Hyperparameter analysis on NELL995 (Hits@3m). The default setting is: 8 layers, 1024 for hidden size, ? = 0.7 for label smoothing, and 32 experts. among entities follow the chain-rule. We observe that kgTransformer with both stages obtains the good results, while kgTransformer with only stage 2 pre-training generates similar performance on NELL995, due to the fact that it is sparser than FB15k-237 with weaker relations within clusters and stronger connections in chains.</p><p>MoE Efficiency. We present an efficiency analysis in terms of the numbers of experts in pre-training, fine-tuning, and inference stages with a batch size of 64 in both training and inference. Table <ref type="table" target="#tab_4">3</ref> reports the running time in millisecond. We observe that MoE can significantly increase the model capacity with limited increase of computational cost. Take the case of 32 experts for example, MoE helps to enlarge kgTransformer 16? in size by only taking 11.6%-38.7% of additional computation.</p><p>Hyperparameters. We conduct the ablation experiments on hyperparameters, including the number of layers, hidden size, the value of label smoothing, and the number of experts. Figure <ref type="figure">4</ref> reports the results on NELL995 in terms of Hits@3m.</p><p>? Number of layers: In Figure <ref type="figure">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case Study</head><p>Interpretability. In masked querying, the model is enforced to produce plausible predictions for every node in the graph because it does not know the queried nodes in advance. Such property implies the interpretability of the results. The embeddings of intermediate variables can be fed into the prediction layer to generate a result, providing paths to understand how the model produces the answers. Take the query "Which school is in the same country as Elmira?"-formally, ??, ??, location in(??????, ?) ? in country(?, ?)-in Table <ref type="table" target="#tab_6">4</ref> for example, the predicted answer is "Davidson College". To verify this answer, we check what the result of ? is. Inputting the embedding of ? into the decoder, it actually gives "U.S." as the top prediction, which is indeed a valid intermediate entity. More cases concerning interpretability for 3p queries are provided in Table <ref type="table">5</ref> in Appendix.</p><p>All-Direction Reasoning. Unlike almost all reasoners' step-bystep searching, kgTransformer is a Transformer-based GNN that captures information from all directions simultaneously. As the second row in Table <ref type="table" target="#tab_6">4</ref> shows that when the intermediate entity </p><p>is required to fit the employs relation, "CIA" is more suitable than "Espionage" and "PTSD" as it is an agency. Without the ability to read from tail to head, the model would suggest "Serial killer" instead, which is inadequate for future deductions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Existing works on complex logical queries are based on KGEs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref>, sequence encoders <ref type="bibr" target="#b19">[20]</ref>, or rules <ref type="bibr" target="#b24">[25]</ref>. Most of them are based on traditional KGEs to adapt for complex logical reasoning with certain geological functions to manipulate the embedding spaces <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref> or logical norms over link predictors <ref type="bibr" target="#b0">[1]</ref>. Graph Query Embedding (GQE) <ref type="bibr" target="#b11">[12]</ref> is proposed to use deep sets as a way for query intersection logic. Logical operators are reformulated as trainable geometric functions in the entity embedding space. Q2B <ref type="bibr" target="#b27">[28]</ref> embeds queries as boxes (i.e., hyper-rectangles), and the points inside which are considered as a potential answer entities of the query. Box Lattice <ref type="bibr" target="#b38">[39]</ref> is proposed to learn representations in the space of all boxes (axis-aligned hyper-rectangles). EmQL <ref type="bibr" target="#b33">[34]</ref> proposes an embedding method with count-min sketch that memories the relations in training set well. CQD <ref type="bibr" target="#b0">[1]</ref> uses the previous neural link predictor ComplEx <ref type="bibr" target="#b35">[36]</ref> as the one-hop reasoner and T-norms <ref type="bibr" target="#b18">[19]</ref> as logic operators. By assembling neural link predictors with T-norms, it translates each query into an end-to-end differentiable objectives. BetaE <ref type="bibr" target="#b28">[29]</ref> utilizes Beta distribution for the embeddings. It takes the advantage of well-defined logic operation over distributions and turns the original logic operation over the embedding space into the one over the distribution space. These approaches can easily incorporate logic operations, but are hard to generalize well to unseen and more complicated queries, as their architectures of pure embeddings limit their expressiveness.</p><p>There have been efforts to employing advanced architectures for complex logical queries. BiQE <ref type="bibr" target="#b19">[20]</ref> introduces the Transformer and mask training, decomposing EPFO queries into sequences to fit the vanilla Transformer's input constraint. However, its nature to process sequences only allows it to answer queries in the shape of directed acyclic graphs (DAGs), which only cover a small set of all possible EPFO queries. In addition, it still follows the supervised training fashion and does not make the full use of the KGs.</p><p>Pre-training graph neural networks for better transferability and generalizability has recently aroused wide interest in graph community, inspired by the progress in language <ref type="bibr" target="#b6">[7]</ref> and vision.</p><p>Generally, these pre-training strategies can be categorized into two different self-supervised objectives <ref type="bibr" target="#b21">[22]</ref>, that is, generative pretraining <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and contrastive pre-training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref>. Nevertheless, they generally focus on pre-training graph neural networks on academic networks <ref type="bibr" target="#b47">[48]</ref>, biochemical substances <ref type="bibr" target="#b32">[33]</ref>, or e-commerce product graphs <ref type="bibr" target="#b14">[15]</ref>. Few efforts have been paid to the challenges of pre-training graph neural networks on knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present the Knowledge Graph Transformer (kgTransformer), a Transformer-based GNN, to pre-train for complex logical reasoning by leveraging the masked pre-training and fine-tuning approach. To adapt to graph structural data, we introduce the Triple Transformation and Mixture-of-Experts strategies for a high-capacity Transformer architecture, and propose a two-stage pre-training framework to gradually endow kgTransformer with the ability to transfer and generalize. Extensive experiments and ablation study demonstrate the effectiveness of kgTransformer's architecture and pre-training methods.</p><p>Notwithstanding the promising results, there exist limitations of kgTransformer that require future studies. First, kgTransformer is not competitive on 1p and 2u reasoning queries which are equivalent to the traditional KG completion problem), as we have not injected inductive biases known to be critical for GNNs on KGs (e.g., CompGCN <ref type="bibr" target="#b36">[37]</ref>). Second, the KG pre-training is limited to a single KG and does not benefit from cross-KG knowledge transfer. It would be an interesting direction to fuse knowledge from multiple KGs via pre-training to gain further improvements on reasoning. Third, as the recent work <ref type="bibr" target="#b30">[31]</ref> indicates, a sequence-to-sequence Transformer that leverages text labels of entities as inputs and outputs can serve as a promising architecture for KG completion in certain scenarios. Finally, to jointly model the KG structure and text information in KG reasoning also remains an unsolved challenge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EPFO query reasoning: KGE-based reasoners vs. Pre-trained kgTransformer. The masked prediction training can endow GNNs with natural capability to answer EPFO queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The KG Pre-Training and Reasoning Framework. (a) kgTransformer with Mixture-of-Experts is a high-capacity architecture that can capture EPFO queries with exponential complexity. (b) Two-stage pre-training trades off general knowledge and task-specific sparse property. Together with fine-tuning, kgTransformer can achieve better in-domain performance and out-of-domain generalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>relations in original KGs untouched. Thus the reasoners are prohibited from grasping knowledge of diverse forms and larger contexts beyond those existing types can express, consequently harming the generalizability.</figDesc><table /><note><p><p><p><p><p>? Transfer and generalization: After training, an ideal reasoner is expected to transfer and generalize to out-of-domain queries. But existing EPFO reasoners</p><ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref> </p>are directly trained on a limited number of samples within a few query types (i.e., 1p, 2p, 3p, 2i, and 3i in Figure</p>2 (b)</p>) in a supervised manner, leaving many entities and</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>? is the index of ?}. By using MoE, we can significantly enlarge the actual model size with approximately the same training computation as the FFN</figDesc><table><row><cell>?</cell><cell>[MASK]</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Gating Network</cell></row><row><cell>Born</cell><cell>Born</cell><cell>Multi-Head</cell><cell>FFN Exp. 1 FFN Exp. 2</cell></row><row><cell></cell><cell></cell><cell>Attention</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>FFN Exp. 3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>??</cell></row><row><cell></cell><cell></cell><cell></cell><cell>FFN Exp. N</cell></row><row><cell>Minn.</cell><cell>Minn.</cell><cell>Residual Conn.</cell><cell>Gating Network</cell></row><row><cell>Win</cell><cell>Win</cell><cell></cell><cell>FFN Exp. 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>FFN Exp. 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>FFN Exp. 3</cell></row><row><cell>Nobel Prize</cell><cell>Nobel Prize</cell><cell></cell><cell>?? FFN Exp. N</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Hits@3m for complex query reasoning. (bold denotes the best results; underline denotes the second best results). EmQL's reported results are not under the standard metric. We have verified the mismatch with its authors and re-evaluated the performance.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>Avg</cell><cell>Avg w/o u</cell><cell>1p</cell><cell>2p</cell><cell>In-domain 3p</cell><cell>2i</cell><cell>3i</cell><cell>ip</cell><cell cols="2">Out-of-domain pi 2u</cell><cell>up</cell></row><row><cell></cell><cell>GQE [12]</cell><cell>0.248</cell><cell>0.270</cell><cell>0.417</cell><cell>0.231</cell><cell>0.203</cell><cell>0.318</cell><cell>0.454</cell><cell>0.081</cell><cell>0.188</cell><cell>0.200</cell><cell>0.139</cell></row><row><cell></cell><cell>Q2B [28]</cell><cell>0.306</cell><cell>0.317</cell><cell>0.555</cell><cell>0.266</cell><cell>0.233</cell><cell>0.343</cell><cell>0.480</cell><cell>0.132</cell><cell>0.212</cell><cell>0.369</cell><cell>0.163</cell></row><row><cell></cell><cell>EmQL [34] 1</cell><cell>0.277</cell><cell>0.294</cell><cell>0.456</cell><cell>0.231</cell><cell>0.172</cell><cell>0.331</cell><cell>0.483</cell><cell>0.143</cell><cell>0.244</cell><cell>0.226</cell><cell>0.207</cell></row><row><cell>NELL995</cell><cell>BiQE [20] CQD(CO) [1]</cell><cell>-0.368</cell><cell cols="3">0.344 0.370 0.667 0.265 0.587 0.305</cell><cell cols="3">0.326 0.220 0.410 0.529 0.371 0.531</cell><cell>0.103 0.196</cell><cell cols="3">0.187 0.302 0.531 0.194 --</cell></row><row><cell></cell><cell>CQD(Beam) [1]</cell><cell>0.375</cell><cell cols="3">0.385 0.667 0.350</cell><cell cols="3">0.288 0.410 0.529</cell><cell>0.171</cell><cell cols="3">0.277 0.531 0.156</cell></row><row><cell></cell><cell cols="12">kgTransformer 0.399 0.408 0.625 0.401 0.367 0.405 0.546 0.203 0.306 0.469 0.270</cell></row><row><cell></cell><cell>GQE [12]</cell><cell>0.230</cell><cell>0.250</cell><cell>0.405</cell><cell>0.213</cell><cell>0.153</cell><cell>0.298</cell><cell>0.411</cell><cell>0.085</cell><cell>0.182</cell><cell>0.167</cell><cell>0.160</cell></row><row><cell></cell><cell>Q2B [28]</cell><cell>0.268</cell><cell>0.283</cell><cell>0.467</cell><cell>0.240</cell><cell>0.186</cell><cell>0.324</cell><cell>0.453</cell><cell>0.108</cell><cell>0.205</cell><cell>0.239</cell><cell>0.193</cell></row><row><cell></cell><cell>EmQL [34] 1</cell><cell>0.219</cell><cell>0.241</cell><cell>0.389</cell><cell>0.201</cell><cell>0.154</cell><cell>0.275</cell><cell>0.386</cell><cell>0.101</cell><cell>0.184</cell><cell>0.115</cell><cell>0.165</cell></row><row><cell>FB15k-237</cell><cell>BiQE [20] CQD(CO) [1]</cell><cell>-0.272</cell><cell>0.293 0.290</cell><cell>0.439 0.512</cell><cell>0.281 0.213</cell><cell>0.239 0.131</cell><cell>0.333 0.352</cell><cell>0.474 0.457</cell><cell>0.110 0.146</cell><cell>0.177 0.222</cell><cell>-0.281</cell><cell>-0.132</cell></row><row><cell></cell><cell>CQD(Beam) [1]</cell><cell>0.290</cell><cell cols="3">0.315 0.512 0.288</cell><cell>0.221</cell><cell>0.352</cell><cell>0.457</cell><cell>0.129</cell><cell cols="3">0.249 0.284 0.121</cell></row><row><cell></cell><cell cols="12">kgTransformer 0.325 0.350 0.459 0.312 0.276 0.398 0.528 0.189 0.286 0.263 0.214</cell></row></table><note><p>1 </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation on pre-training &amp; fine-tuning (Hits@3m).</figDesc><table><row><cell></cell><cell cols="2">FB15k-237 NELL995</cell></row><row><cell>kgTransformer (Stage 1 + Stage 2)</cell><cell>0.336</cell><cell>0.395</cell></row><row><cell>-only Stage 1 in pre-training</cell><cell>0.308</cell><cell>0.307</cell></row><row><cell>-only Stage 2 in pre-training</cell><cell>0.307</cell><cell>0.399</cell></row><row><cell>-w/o fine-tuning</cell><cell>0.301</cell><cell>0.368</cell></row><row><cell>-w/o pre-training</cell><cell>0.262</cell><cell>0.288</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Training and inference time (ms) per step along with different numbers of experts using per step batch size 64.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Number of experts</cell></row><row><cell></cell><cell>2</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell>Pre-training stage 1</cell><cell>41.46</cell><cell>42.02</cell><cell>45.38</cell><cell>49.30 (+18.9%)</cell></row><row><cell>Pre-training stage 2</cell><cell>17.37</cell><cell>18.83</cell><cell>19.78</cell><cell>24.09 (+38.7%)</cell></row><row><cell>Fine-tuning</cell><cell>32.41</cell><cell>32.91</cell><cell>36.46</cell><cell>37.47 (+15.6%)</cell></row><row><cell>Inference</cell><cell>12.86</cell><cell>13.36</cell><cell>13.85</cell><cell>14.35 (+11.6%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Figure 4 (b)  studies the influence of hidden size. We observe that the increase of hidden size helps to capture massive EPFO reasoning patterns, but a too-large one (e.g., 2048) can harm the performance, as it may suffer from the similar training difficulty witnessed in KGE-based reasoners<ref type="bibr" target="#b0">[1]</ref>.? Label smoothing: Label smoothing is crucial to kgTransformer's pre-training, as it mitigates the bias in the pre-training data due to the on-the-fly sampling. Through experiments in Figure4(c), the optimal value ? = 0.7 for NELL995 is exceptionally high in comparison with that in natural language processing (? = 0.1). It indicates that by training for long epochs (around 1000 epochs) with a rather large embedding size (1024), the kg-Transformer model is prone to overfitting in pre-training. We also observe that exclusion of label smoothing will yield a considerable performance decrease by 2.6%. ? Number of experts: We evaluate the performance of kgTrans-</figDesc><table><row><cell>(a), we test the performance of different number of layers. It suggests that a sufficient and proper model depth (8 in this case) is essential for kgTransformer, which can be attributed to the relation between the depth and the size of the receptive field. Previously studies have shown that very deep GNNs may suffer from over-smoothing and training instability, thus causing performance drops [9]. ? Hidden size: former with different numbers of experts in Figure 4 (d). The correspondence of sparse nature in KG reasoning and mixture-of-experts is one key point in kgTransformer. Compared with the vanilla Transformer (2 experts), kgTransformer achieves perfor-mance improvements with more and more experts until 32 (16? large in model size), demonstrating the power of high-capacity models in EPFO challenges.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Case study on 2p examples. (a) Interpretability. Filling a certain entity as the tail prediction, we use kgTransformer to predict the unknown intermediates to test its interpretability. (b) All-direction Reasoning. In 2p, intermediate and tail entities are masked. kgTransformer considers both relations when predicting intermediate node (marked orange). If the second relation is masked (i.e., conventional autoregressive reasoning), intermediate's predictions are valid for the first relation, but no longer for the second (marked red).</figDesc><table><row><cell>Case</cell><cell>Head</cell><cell></cell><cell>Intermediate entity</cell><cell></cell><cell>Tail</cell></row><row><cell></cell><cell>(Groundtruth)</cell><cell></cell><cell>(Top 3 prediction)</cell><cell></cell><cell>(Prediction)</cell></row><row><cell>Interpretability</cell><cell>Elmira(U.S. City)</cell><cell>location in</cell><cell>U.S., Canada, UK</cell><cell>in country</cell><cell>Davidson College</cell></row><row><cell>All-direction Reasoning</cell><cell>Salt(2010 film)</cell><cell>has subject</cell><cell>CIA, Espionage, PTSD Serial killer, Espionage, PTSD</cell><cell>employs</cell><cell>George Bush</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/laekov/fastmoe</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>We thank the reviewers for their valuable feedback to improve this work. This work is supported by <rs type="funder">Technology and Innovation Major Project of the Ministry of Science and Technology of China</rs> under Grant <rs type="grantNumber">2020AAA0108400</rs> and <rs type="grantNumber">2020AAA0108402</rs>, <rs type="funder">Natural Science Foundation of China</rs> (<rs type="programName">Key Program</rs>, No. <rs type="grantNumber">61836013</rs>), and <rs type="funder">National Science Foundation for Distinguished Young Scholars</rs> (No. <rs type="grantNumber">61825602</rs>).</p></div>
			</div>
			<div type="funding">
<div><p>q f p 7 X p D X c e E 8 h K b R g H d I 8 7 C I 7 5 Q = " &gt; A A A D R X i c j V F d a x Q x F L 0 z f r T W a l d F X 3 w J L s I W y j A j g u K L 3 f r i i 7 I F d 7 a w s 5 R M N t 2 G Z p J p k p G u y 4 L / z p 8 g / g H R B 9 / E V 7 2 J U 1 A X 0 Q w z c 3 L O P T f 3 5 p a 1 F N a l 6 Y c o v n D x 0 u W 1 9 S s b V z e v X d / q 3 L i Z W 9 0 Y x o d M S 2 0 O S m q 5 F I o P n X C S H 9 S G 0 6 q U f F S e P P P 6 6 D U 3 V m j 1 y s 1 r P q n o T I k j w a h D 6 r D z t q i o O 2 Z U L v a X 4 / 6 E F M 4 I q m a S n 5 K n f f K E F P w M i 7 A k J z Z x C c r 8 z C 3 2 t F H L 3 g u h F L f a 0 R 2 S b 5 N C U j V t 9 Z F A + a U u u S Q D I 9 7 w l Y C B p P N l L 9 8 h / e 3 D T j d N 0 r D I K s h a 0 I V 2 D X T n P R Q w B Q 0 M G q i A g w K H W A I F i 8 8 Y M k i h R m 4 C C + Q M I h F 0 D k v <rs type="person">Y Q G + D U R</rs> w j K L I n + J 3 h b t y y C v c + p w 1 u h q d I f A 0 6 C d x H j 8 Y 4 g 9 i f R o L e h M y e / V v u R c j p a 5 v j v 2 x z V c g 6 O E b 2 X 7 7 z y P / 1 + V 4 c H M H j 0 I P A n u r A + O 5 Y m 6 U J t + I r J 7 9 0 5 T B D j Z z H U 9 Q N Y h a c 5 / d M g s e G 3 v 3 d 0 q B / C p G e 9 X v W x j b w 2 V e J A 8 7 + H O c q y B 8 k W Z p k + w + 7 u 3 v t q N f h L t y D H s 7 z E e z C c x j A E H N / j D a j 2 9 G d + F 3 8 J f 4 a f / s Z G k e t 5 x b 8 t u L v P w A H H b 3 x &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q f p 7 X p D X c e E 8 h K b R g H d I 8 7 C I 7 5</p><p>m q 5 F I o P n X C S H 9 S G 0 6 q U f F S e P P P 6 6 D U 3 V m j 1 y s 1 r P q n o T I k j w a h D 6 r D z t q</p><p>y e / V v u R c j p a 5 v j v 2 x z V c g 6 O E b 2 X 7 7 z y P / 1 + V 4 c H M H j 0 I P A n u r A + O 5 Y m 6 U J t + I r J 7 9 0 5 T B D j Z z H U 9 Q N Y h a c 5 / d M g s e G 3 v 3 d 0 q B / C p G e 9 X v W x j b w 2 V e J A 8 7 + H O c q y B 8 k W Z p k + w + 7 u 3 v t q N f h L t y D H s 7 z E e z C c x j A E H N / j D a j 2 9 G d + F 3 8 J f 4 a f / s Z G k e t 5 x b 8 t u L v P w A H H b 3 x &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G 3 0 n v n J k y K B l K c C y m N n r q T x J g</p><p>9 p k E j w 2 9 + 7 2 l Q f 8 e I j 3 r 5 6 y N b e C H r x I P O P v 7 O F d B / i T J 0 i T b S 2 E d t u A R 9 P A Y n 8 J L e A M D G G L K b 1 E n e h A 9 j D / H P + N f 5 1 c h j t o 7 c R / + G P H v M 8 y z v E A = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t s D 0 c Y 9 z b n H j / 0 2 s t S P M 5 5 2 w j a</p><p>b w 1 V W J A 8 7 + H O c q y J 8 k W Z p k e 2 l 3 Z z e M e g 0 e w i P o 4 T y f w g 6 8 h A E M M f f n a C O 6 H z 2 I P 8 T f 4 u / x 2 c / Q O A q e e / D b i s 9 / A A X d v e 0 = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q f p 7 X p D X c e E 8 h K b R g H d I 8 7 C I 7 5 Q = " &gt; A A A D R X i c j V F d a x Q x F L 0 z f r T W a l d F X 3 w J L s I W y j A j g u K L 3 f r i i 7 I F d 7 a w s 5 R M N t 2 G Z p J p k p G u y 4 L / z p 8 g / g H R B 9 / E V 7 2 J U 1 A X 0 Q w z c 3 L O P T f 3 5 p a 1 F N a l 6 Y c o v n D x 0 u W 1 9 S s b V z e v X d / q 3 L i Z W 9 0 Y x o d M S 2 0 O S m q 5 F I o P n X C S H 9 S G 0 6 q U f F S e P P P 6 6 D U 3 V m j 1 y s 1 r P q n o T I k j w a h D 6 r D z t q i o O 2 Z U L v a X 4 / 6 E F M 4 I q m a S n 5</p><p>y e / V v u R c j p a 5 v j v 2 x z V c g 6 O E b 2 X 7 7 z y P / 1 + V 4 c H M H j 0 I P A n u r A + O 5 Y m 6 U J t + I r J 7 9 0 5 T B D j Z z H U 9 Q N Y h a c 5 / d M g s e G 3 v 3 d 0 q B / C p G e 9 X v W x j b w 2 V e J A 8 7 + H O c q y B 8 k W Z p k + w + 7 u 3 v t q N f h L t y D H s 7 z E e z C c x j A E H N / j D a j 2 9 G d + F 3 8 J f 4 a f / s Z G k e t 5 x b 8 t u L v P w A H H b 3 x &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q f p 7</p><p>m q 5 F I o P n X C S H 9 S G 0 6 q U f F S e P P P 6 6 D U 3 V m j 1 y s 1 r P q n o T I k j w a h D 6 r D z t q</p><p>m q 5 F I o P n X C S H 9 S G 0 6 q U f F S e P P P 6 6 D U 3 V m j 1 y s 1 r P q n o T I k j w a h D 6 r D z t q</p><p>y e / V v u R c j p a 5 v j v 2 x z V c g 6 O E b 2 X 7 7 z y P / 1 + V 4 c H M H j 0 I P A n u r A + O 5 Y m 6 U J t + I r J 7 9 0 5 T B D j Z z H U 9 Q N Y h a c 5 / d M g s e G 3 v 3 d 0 q B / C p G e 9 X v W x j b w 2 V e J A 8 7 + H O c q y B 8 k W Z p k + w + 7 u 3 v t q N f h L t y D H s 7 z E e z C c x j A E H N / j D a j 2 9 G d + F 3 8 J f 4 a f / s Z G k e t 5 x b 8 t u L v P w A H H b 3 x &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q f p 7 X p D X c e E 8 h K b R g H d I 8 7 C I 7 5 Q = " &gt; A A A D R X i c j V F d a x Q x F L 0 z f r T W a l d F X 3 w J L s I W y j A j g u K L 3 f r i i 7 I F d 7 a w s 5 R M N t 2 G Z p J p k p G u y 4 L / z p 8 g / g H R B 9 / E V 7 2 J U 1 A X 0 Q w z c 3 L O P T f 3 5 p a 1 F N a l 6 Y c o v n D x 0 u W 1 9 S s b V z e v X d / q 3 L i Z W 9 0 Y x o d M S 2 0 O S m q 5 F I o P n X C S H 9 S G 0 6 q U f F S e P P P 6 6 D U 3 V m j 1 y s 1 r P q n o T I k j w a h D 6 r D z t q i o O 2 Z U L v a X 4 / 6 E F M 4 I q m a S n 5 K n f f K E F P w M i 7 A k J z Z x C c r 8 z C 3 2 t F H L 3 g u h F L f a 0 R 2 S b 5 N C U j V t 9 Z F A + a U u u S Q D I 9 7 w l Y C B p P N l L 9 8 h / e 3 D T j d N 0 r D I K s h a 0</p><p>y e / V v u R c j p a 5 v j v 2 x z V c g 6 O E b 2 X 7 7 z y P / 1 + V 4 c H M H j 0 I P A n u r A + O 5 Y m 6 U J t + I r J 7 9 0 5 T B D j Z z H U 9 Q N Y h a c 5 / d M g s e G 3 v 3 d 0 q B / C p G e 9 X v W x j b w 2 V e J A 8 7 + H O c q y B 8 k W Z p k + w + 7 u 3 v t q N f h L t y D H s 7 z E e z C c x j A E H N / j D a j 2 9 G d + F 3 8 J f 4 a f / s Z G k e t 5 x b 8 t u L v P w A H H b 3 x &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q f p 7 X p D X c e E 8 h K b R g H d I 8 7 C I 7 5 Q = " &gt; A A A D R X i c j V F d a x Q x F L 0 z f r T W a l d F X 3 w J L s I W y j A j g u K L 3 f r i i 7 I F d 7 a w s 5 R M N t 2 G Z p J p k p G u y 4 L / z p 8 g / g H R B 9 / E V 7 2 J U 1 A X 0 Q w z c 3 L O P T f 3 5 p a 1 F N a l 6 Y c o v n D x 0 u W 1 9 S s b V z e v X d / q 3 L i Z W 9 0 Y x o d M S 2 0 O S m q 5 F I o P n X C S H 9 S G 0 6 q U f F S e P P P 6 6 D U 3 V m j 1 y s 1 r P q n o T I k j w a h D 6 r D z t q</p><p>y e / V v u R c j p a 5 v j v 2 x z V c g 6 O E b 2 X 7 7 z y P / 1 + V 4 c H M H j 0 I P A n u r A + O 5 Y m 6 U J t + I r J 7 9 0 5 T B D j Z z H U 9 Q N Y h a c 5 / d M g s e G 3 v 3 d 0 q B / C p G e 9 X v W x j b w 2 V e J A 8 7 + H O c q y B 8 k W Z p k + w + 7 u 3 v t q N f h L t y D H s 7 z E e z C c x j A E H N / j D a j 2 9 G d + F 3 8 J f 4 a f / s Z G k e t 5 x b 8 t u L v P w A H H b 3 x &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " q f p 7 X p D X c e E 8 h K b R g H d I 8 7 C I 7 5 Q = " &gt; A A A D R X i c j V F d a x Q x F L 0 z f r T W a l d F X 3 w J L s I W y j A j g u K L 3 f r i i 7 I F d 7 a w s 5 R M N t 2 G Z p J p k p G u y 4 L / z p 8 g / g H R B 9 / E V 7 2 J U 1 A X 0 Q w z c 3 L O P T f 3 5 p a 1 F N a l 6 Y c o v n D x 0 u W 1 9 S s b V z e v X d / q 3 L i Z W 9 0 Y x o d M S 2 0 O S m q 5 F I o P n X C S H 9 S G 0 6 q U f F S e P P P 6 6 D U 3 V m j 1 y s 1 r P q n o T I k j w a h D 6 r D z t q i o O 2 Z U L v a X 4 / 6 E F M 4 I q m a S n 5 K n f f K E F P w M i 7 A k J z Z x C c r 8 z C 3 2 t F H L 3 g u h F L f a 0 R 2 S b 5 N C U j V t 9 Z F A + a U u u S Q D I 9 7 w l Y C B p P N l L 9 8 h / e 3 D T j d N 0 r D I K s h a 0 I V 2 D X T n P R Q w B Q 0 M G q i A g w K H W A I F i 8 8 Y M k i h R m 4 C C + Q M I h F 0 D k v <rs type="person">Y Q G + D U R</rs> w j K L I n + J 3 h b t y y C v c + p w 1 u h q d I f A 0 6 C d x H j 8 Y 4 g 9 i f R o L e h M y e / V v u R c j p a 5 v j v 2 x z V c g 6 O E b 2 X 7 7 z y P / 1 + V 4 c H M H j 0 I P A n u r A + O 5 Y m 6 U J t + I r J 7 9 0 5 T B D j Z z H U 9 Q N Y h a c 5 / d M g s e G 3 v 3 d 0 q B / C p G e 9 X v W x j b w 2 V e J A 8 7 + H O c q y B 8 k W Z p k + w + 7 u 3 v t q N f h L t y D H s 7 z E e z C c x j A E H N / j D a j 2 9 G d + F 3</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_g2rgJFz">
					<idno type="grant-number">2020AAA0108400</idno>
				</org>
				<org type="funding" xml:id="_WPYe5rS">
					<idno type="grant-number">2020AAA0108402</idno>
				</org>
				<org type="funding" xml:id="_XUq26bB">
					<idno type="grant-number">61836013</idno>
					<orgName type="program" subtype="full">Key Program</orgName>
				</org>
				<org type="funding" xml:id="_J5MKE6U">
					<idno type="grant-number">61825602</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Sampling Method</head><p>In the stage 1 of pre-training, we used random-walk based graph sampling strategies to sample dense and large subgraphs on the training set. We adopt LADIES <ref type="bibr" target="#b49">[50]</ref> sampling method and a variant of random walk called meta-tree sampling.</p><p>Meta-tree Sampling. As a variant of random walk with restart, meta-tree sampling strategy does not necessarily restart from the target node every time. Instead, it restarts from any sampled node with equal probability. The change is inspired by the intention to sample more neighbors for each node rather than for the target node in a subgraph. In comparison with tuning the parameters of RW, meta-tree sampling provides a direct way to spread the density across all the sampled nodes. Such a density spread is essential for preventing overfitting to highly dense area. Besides, to sample a dense graph, we specify the restart probability to be 1.0, which means it jumps to any sampled node after one step and continue. In this way, meta-tress sampling balances between the width and the depth and prevents overfitting problem in our setting. Note that we do not sample a node more than once, so the sampled subgraph contains no cycle and forms a tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Evaluation Protocol</head><p>To be more precise, we denote the whole entity set as V, and the filtered entity set w.r.t qeury ? as S = V\ ? test .</p><p>The metric Mean Reciprocal Rank (MRR) is caculated as 1 rank and Hits at K(Hits@Km) is calculated as ?[???? &lt; ?]. In this paper, we report the Hits@3m metric, the frequency of the correct answer to appear in top 3 ranking. The metric of a query is an average of the metrics over all its predicted answers, and the metric of a type of query is an average of the metrics over all queries of this type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Reproducibility</head><p>In this section, we describe the experiment setting in more details. Label Smoothing. Label smoothing is a technique of regularization. It utilizes soft one-hop labels instead of hard ones to add noise in the training, reduces the weights of true labels when calculating the training loss and thus prevents overfitting when training. Suppose ? ? is the original true label, ? ?? is the smoothed label, ? is the parameter of label smoothing, ? is the number of classes.</p><p>Since the data used in pre-training is the full graph, much larger than the query data in fine-tuning, we add label smoothing only in pre-training. We used ? = 0.1 for FB15k-237 and ? = 0.7 for NELL. Note that because the data used in fine-tuning is limited, we do not add label smoothing in fine-tuning.</p><p>Sampling Parameters. We leverage several sampling methods in the two pre-training stages where sampling ratio makes a difference to the final performance. In pre-training stage 1, for FB15k-237, the ratio between meta-tree sampling and LADIES sampling is 1 : 1; for NELL, we only use meta-tree sampling. In pre-training stage 2, the ratio between chain-like meta-graphs and branch-like metagraphs can be various. We applied grid search within the range [1 : 20, . . . 1 : 2, 1 : 1, 2 : 1, . . . 20 : 1] and set the ratio to be 4 : 1 for FB15k-237 and 10 : 1 for NELL. Besides, in the stage 1 of pre-training, for each sampled subgraphs, we keep 80% of the induced edges between nodes and limit the number of nodes in each subgraph within <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>. In the stage 2 of pre-training, we do not add induced edges and limit the size of meta-graphs to be less than 4. In stage 2 pre-training, we sample two patterns of small graphs for refinement, chain-like subgraphs and branch-like subgraphs. We first select a target node from all the nodes uniformly at random. For the chain-like subgraph sampling, we sample a chain by the Markov process. In each step, we sample a neighbor of the current node independently. It is allowed to sample a node more than once in a chain. For the branch-like subgraph sampling, we sample multiple neighbors of the target node. We drop those sampled graphs whose target node has no or only one neighbor and prevent sampling the same neighbors multiple times to avoid meaningless cases.</p><p>Mixture-of-Experts (MoE). We have already discussed the importance of MoE in the setting of multiple patterns reasoning especially when the FFN is sparsely activated. Note that we use Pre-LN (Pre-Layer Normalization, which refers to placing the layer normalization in the residual connections and appending an auxiliary layer normalization before the final linear decoder <ref type="bibr" target="#b41">[42]</ref>) instead of Post-LN (Post-Layer Normalization, which refers to vanilla Transformer's design of placing the layer normalization between the residual blocks) in MoE, as evidences show that Pre-LN converges faster and more robustly compared to Post-LN <ref type="bibr" target="#b41">[42]</ref>. For the number of experts, using 2 experts of MoE means the original setting of FFN. We applied grid search over the number of experts within [2, 4, 8 . . . 32, 64] and select 32 as the number of experts. We implement the MoE strategy using the FastMoE 2 <ref type="bibr" target="#b12">[13]</ref>, an open-source pytorch-based package for MoE with transformers.</p><p>Training Parameters. To enhance the generalization ability of our model, we add some noise to the mask in pre-training stage 1, following the strategies in BERT. For the nodes to be masked, ? 80% of the time, we replace the node with [mask] token; ? 10% of the time, we keep the node unchanged; ? 10% of the time, we replace the node with a random node.</p><p>Such masking ratio forces our model to keep a contextual representation for every node and thus learn the neighborhood information. We train our model with batch size 258 in pre-training and 12288 in fine-tuning. We use AdamW <ref type="bibr" target="#b22">[23]</ref> with ?? = 1? -4, ? 1 = 0.9, ? 2 = 0.999, exponential decay rate of 0.997. We also use a dropout probability of 0.1 every layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Combinatorial Fine-Tuning</head><p>In the fine-tuning stage, we conduct multi-task fine-tuning followed by single-task fine-tuning. Multi-task fine-tuning refers to fine-tuning the pretrained model with all of the query-answer training sets, including 1?, 2?, 3?, 2?, 3?. Single-task fine-tuning follows multi-task fine-tuning by tuning with only part of the query-answer training sets, either a single query's set or a combination of several   query's sets. To compare different combinations in single-task finetuning, we just pick an arbitrary checkpoint in pretraining stage with preliminary result, do multi-task fine-tuning and then show different combinations of single-task fine-tune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Comparison Methods</head><p>In this section, we briefly go through the compared baselines for reference.</p><p>? GQE <ref type="bibr" target="#b11">[12]</ref> utilizes deep set for intersection (conjunctive) logical operation as learned geometric functions in this space. ? Q2B <ref type="bibr" target="#b27">[28]</ref> embeds queries as hyper-rectangles and answers as points inside the rectangles. ? EmQL <ref type="bibr" target="#b33">[34]</ref> utilizes count-min sketch for query embedding in order to be more faithful to deductive reasoning. It focuses on deductive reasoning which does not require generalization. ? BiQE <ref type="bibr" target="#b19">[20]</ref> utilizes bi-directional attention mechanism and positional embedding to capture interactions within a graph. ? CQD <ref type="bibr" target="#b0">[1]</ref> uses ComplEx as one-hop reasoner and various T-norms as logic operators. It provides end-to-end differentiable objective by uniting one-hop reasoners and logic operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Statistics of Datasets</head><p>We provide the information of the splitting of original datasets in Table <ref type="table">7</ref> and the splitting of the query-answer datasets in Table <ref type="table">8</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Complex Query Answering with Neural Link Predictors</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Arakelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Daza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cochez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
		<idno>SIGMOD. 1247-1250</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Estevam R Hruschka</surname></persName>
		</author>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Siegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06209</idno>
		<title level="m">Neural Query Language: A Knowledge Base Query Language for Tensorflow</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hilary</forename><forename type="middle">A</forename><surname>Davey</surname></persName>
		</author>
		<author>
			<persName><surname>Priestley</surname></persName>
		</author>
		<title level="m">Introduction to lattices and order</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>Second Edition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledge vault: A web-scale approach to probabilistic knowledge fusion</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geremy</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilko</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Strohmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="601" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Graph Random Neural Network for Semi-Supervised Learning on Graphs</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Hyperbolic neural networks. In NeurIPS</title>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>B?cigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5345" to="5355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transformer Feed-Forward Layers Are Key-Value Memories</title>
		<author>
			<persName><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roei</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5484" to="5495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Embedding logical queries on knowledge graphs</title>
		<author>
			<persName><forename type="first">Payal</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2030" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fastmoe: A fast mixture-of-expert training system</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13262</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Strategies For Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gpt-gnn: Generative pre-training of graph neural networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In WWW. 2704-2710</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Erich</forename><surname>Peter Klement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radko</forename><surname>Mesiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Endre</forename><surname>Pap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Triangular norms</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Answering Complex Queries in Knowledge Graphs with Bidirectional Sequence Encoders</title>
		<author>
			<persName><forename type="first">Bhushan</forename><surname>Kotnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolin</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4968" to="4977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101[cs.LG]</idno>
		<title level="m">Decoupled Weight Decay Regularization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">RNNLogic: Learning Logic Rules for Reasoning on Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Pascal</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Hongyu Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Schuurmans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14890</idno>
		<title level="m">SMORE: Knowledge Graph Completion and Multi-hop Reasoning in Massive Knowledge Graphs</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Query2box: Reasoning over Knowledge Graphs in Vector Space Using Box Embeddings</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hongyu Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Lior</forename><surname>Rokach</surname></persName>
		</author>
		<title level="m">Pattern classification using ensemble methods</title>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">75</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence-to-Sequence Knowledge Graph Completion and Question Answering</title>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Kochsiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2814" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<title level="m">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ZINC 15-ligand discovery for everyone</title>
		<author>
			<persName><forename type="first">Teague</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="2324" to="2337" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faithful Embeddings for Knowledge Base Queries</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tania</forename><forename type="middle">Bedrax</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-4007</idno>
		<ptr target="https://doi.org/10.18653/v1/W15-4007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Composition-based Multi-Relational Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06627</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Knowledge base completion via search-based question answering</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohua</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<idno>WWW. 515-526</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="564" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1060</idno>
		<ptr target="https://doi.org/10.18653/v1/D17-1060" />
	</analytic>
	<monogr>
		<title level="m">ACL Workshop</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="564" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Do Transformers Really Perform Badly for Graph Representation?</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Oag: Toward linking large-scale heterogeneous entity graphs</title>
		<author>
			<persName><forename type="first">Fanjin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2585" to="2595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01786</idno>
		<title level="m">MoEfication: Conditional Computation of Transformer Models for Efficient Inference</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Layer-Dependent Importance Sampling for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07323</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
