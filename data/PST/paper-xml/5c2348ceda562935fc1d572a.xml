<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multivariate Time Series Imputation with Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yonghong</forename><surname>Luo</surname></persName>
							<email>luoyonghong@dbis.nankai.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xiangrui</forename><surname>Cai</surname></persName>
							<email>caixiangrui@dbis.nankai.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
							<email>yingzhang@nankai.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
							<email>junxu@ruc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xiaojie</forename><surname>Yuan</surname></persName>
							<email>yuanxj@nankai.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Information Renmin</orgName>
								<orgName type="institution">University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">College of Computer Science</orgName>
								<orgName type="institution">Nankai University</orgName>
								<address>
									<settlement>Tianjin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multivariate Time Series Imputation with Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4F165D0156EA69DB06FF2566FE5F6A7D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multivariate time series usually contain a large number of missing values, which hinders the application of advanced analysis methods on multivariate time series data. Conventional approaches to addressing the challenge of missing values, including mean/zero imputation, case deletion, and matrix factorization-based imputation, are all incapable of modeling the temporal dependencies and the nature of complex distribution in multivariate time series. In this paper, we treat the problem of missing value imputation as data generation. Inspired by the success of Generative Adversarial Networks (GAN) in image generation, we propose to learn the overall distribution of a multivariate time series dataset with GAN, which is further used to generate the missing values for each sample. Different from the image data, the time series data are usually incomplete due to the nature of data recording process. A modified Gate Recurrent Unit is employed in GAN to model the temporal irregularity of the incomplete time series. Experiments on two multivariate time series datasets show that the proposed model outperformed the baselines in terms of accuracy of imputation. Experimental results also showed that a simple model on the imputed data can achieve state-of-the-art results on the prediction tasks, demonstrating the benefits of our model in downstream applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The real world is filled with multivariate time series data such as network records, medical logs and meteorologic observations. Time series analysis is useful in many situations such as forecasting the stock price <ref type="bibr" target="#b21">[22]</ref> and indicating fitness and diagnosis category of patients <ref type="bibr" target="#b6">[7]</ref>. However, some of these time series are incomplete due to the broken of collective devices, the collecting errors and willful damages <ref type="bibr" target="#b14">[15]</ref>. Besides, the time intervals of the observations in time series are not always fixed. Figure <ref type="figure" target="#fig_0">1</ref> and Figure <ref type="figure" target="#fig_2">2</ref> demonstrate the high missing rate of the Physionet <ref type="bibr" target="#b41">[42]</ref> dataset. As time goes by, the maximum missing rate at each timestamp is always higher than 95%. We can also observe that most variables' missing rate are above 80% and the mean of the missing rate is 80.67%. The missing values in time series data make it hard to analyze and mine <ref type="bibr" target="#b13">[14]</ref>. Therefore, the processing of missing values in time series has become a very important problem.   Usually there are two ways to handle the missing values of the dataset. Some researches try to directly model the dataset with missing values <ref type="bibr" target="#b47">[48]</ref>. However, for every dataset, we need to model them separately. The second way is to impute the missing values to get the complete dataset and then use conventional methods to analyze the dataset. Existing missing values processing methods can be categorized into 3 classes. The very first one is case deletion methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43]</ref>. Its main idea is to discard the incomplete observations. However, these case deletion methods will ignore some important information. Additionally, the higher the missing rate, the worse the result <ref type="bibr" target="#b17">[18]</ref>. The second kind of algorithms is simple imputation methods such as mean imputation, median imputation, and the most common value imputation. The main drawback of these statistical imputation methods is the lack of the utilization of the temporal information. The last kind of methods is machine learning based imputation algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34]</ref>. These methods contain maximum likelihood Expectation-Maximization (EM) based imputation, KNN based imputation and Matrix Factorization based imputation. However, all of these methods rarely take into account the temporal relations between two observations. In recent years, Goodfellow et al. <ref type="bibr" target="#b16">[17]</ref> have introduced the generative adversarial networks (GAN) which learns the latent distribution of a dataset and is able to generate "real" samples from a random "noise". GAN has been successfully applied to face completion and sentence generation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b46">47]</ref>. However, before completion of the faces or generation of the sentences, these methods require the complete training dataset which is impossible in our scenario. There also exists a few works that use GAN to impute the missing values <ref type="bibr" target="#b45">[46]</ref>. However, what these works focused is non-sequential dataset and they have not adopted pertinent measures to process the temporal relations. Hence, these algorithms can not be applied to the imputation of time series data well.</p><p>Inspired by the success of GAN in image imputation, we take the advantage of the adversarial model to generate and impute the original incomplete time series data. In order to learn the latent relationships between observations with non-fixed time lags, we propose a novel RNN cell called GRUI which can take into account the non-fixed time lags and fade the influence of the past observations determined by the time lags. In the first phase, by adopting the GRUI in the discriminator and generator in GAN, the well trained adversarial model can learn the distribution of the whole dataset, the implicit relationships between observations and the temporal information of the dataset. In the second phase, we train the input "noise" of the generator of the GAN so that the generated time series is as close as possible to the original incomplete time series and the generated data's probability of being real is the biggest. To the best of our knowledge, this is the first work that uses adversarial networks to impute time series dataset. We evaluate our method on a real-world medical dataset and a real-world meteorologic dataset. The results show the superiority of our approach compared to the baselines in terms of imputation accuracy. Our model is also superior to the baselines in prediction and regression tasks using the imputed datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Given a collection of multivariate time series with d dimensions, one time series X observed in T =(t 0 ,. . .,t n-1 ), is denoted by X=(x t0 , . . . , x ti , . . . , x tn-1 ) ∈ R n×d , where x ti is the t i th observation of X, and x j ti is the jth variable of x ti . In the following example, d=4, n=3 and "none" is missing value,</p><formula xml:id="formula_0">X = 1 6 none 9 7 none 7 none 9 none none 79 , T = 0 5 13 .</formula><p>The time series X is incomplete, we introduce the mask matrix M ∈ R n×d to present whether the values of X exist or not, i.e., M j ti =1, if x j ti exists, otherwise M j ti =0. In order to replace missing values in time series data with reasonable values, we first train a GAN based model to learn the distribution of the original time series dataset. In this custom GAN model, the generator which generates fake time series from a random input vector and the discriminator which distinguishes between fake data and real data, will achieve an equilibrium that not only increases the representative ability of the generator but also upgrades the discernment ability of the discriminator (see Figure <ref type="figure" target="#fig_3">3</ref>). Next, we fix the network structure and optimize the input random vector of the generator so that the generated fake time series can best replace the missing values. In subsection 2.1, we show the details of the GAN Architecture. Subsection 2.2 demonstrates the method to impute the missing values. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GAN Architecture</head><p>A GAN is made up of a generator (G) and a discriminator (D). The G learns a mapping G(z) that tries to map the random noise vector z to a realistic time series. The D tries to find a mapping D(.) that tell us the input data's probability of being real. It is noteworthy that the input of the D contains the real but incomplete samples and the fake but complete samples generated by G. Because of the mode collapse problem <ref type="bibr" target="#b2">[3]</ref>, the traditional GAN is hard to train <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>. WGAN <ref type="bibr" target="#b2">[3]</ref> is another training way of GAN which uses the Wasserstein distance that is easier to train than the original. WGAN can improve the stability of the learning stage, get out of the problem of mode collapse and make it easy for the optimization of the GAN model. In our method, we prefer WGAN <ref type="bibr" target="#b2">[3]</ref> to traditional GAN. The following is the loss function of WGAN.</p><formula xml:id="formula_1">L G = E z∼Pg [-D(G(z))] ,<label>(1)</label></formula><formula xml:id="formula_2">L D = E z∼Pg [D(G(z))] -E x∼Pr [D(x)] .<label>(2)</label></formula><p>When we design the detail structure of the GAN, we adopt Gated Recurrent Unit (GRU) <ref type="bibr" target="#b9">[10]</ref>, a state-of-the-art RNN cell, as the basic network of G and D. It is worth noting that, others RNN variants can also be used in this work, such as the Long Short-Term Memory (LSTM) <ref type="bibr" target="#b20">[21]</ref> cell. However, the time lags between two consecutive valid observations vary a lot due to data incompleteness, which makes traditional GRU cell or LSTM cell not applicable to our senario. In order to effectively handle the irregular time lags and to learn the implicit information from the time intervals, we propose the GRUI cell based on GRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GRUI.</head><p>To appropriate learn the distribution and characteristic of the original incomplete time series dataset, we find that, the time lag between two consecutive valid observations is always in changing because of the "none" values. The time lags between observations are very important since they follow an unknown nonuniform distribution. These changeable time lages remind us that the influence of the past observations should decay with time if the variable has been missing for a while. In order to fit this decayed influence of the past observations, we propose the Gated Recurrent Unit for data Imputation (GRUI) cell to model the temporal irregularity of the incomplete time series.</p><p>In order to record the time lag of two adjacent existent values of X, we introduce the time lag matrix δ ∈ R n×d to record the time lag between current value and last valid value. The followings is the calculation way and calculated results of δ of the sample dataset.</p><formula xml:id="formula_3">δ j ti =    t i -t i-1 , M j ti-1 == 1 δ j ti-1 + t i -t i-1 , M j ti-1 == 0 &amp; i &gt; 0 0, i<label>== 0</label></formula><p>; δ = 0 0 0 0 5 5 5 5 8 13 8 13</p><p>.</p><p>We introduce a time decay vector β to control the influence of the past observations. Each value of the β should be bigger than zero and smaller than one, and the larger the δ, the smaller the decay vector. So we model the time decay vector β as a combination of δ:</p><formula xml:id="formula_4">β ti = 1/e max(0,W β δt i +b β ) ,<label>(3)</label></formula><p>where W β and b β are parameters that need to learn. We use the negative exponential formulation to make sure that β ti ∈ (0, 1]. Besides, in order to capture the interactions of the δ's variables, we prefer full weight matrix to diagonal matrix for W β . After we have got the decay vector, we update the GRU hidden state h ti-1 by element-wise multiplying the decay factor β. Since we have used the batch normalization <ref type="bibr" target="#b23">[24]</ref> technology, the hidden state h is smaller than 1 with a high probability.</p><p>We choose multiplicative decay way rather than some other decay ways such as h β . The update functions of GRUI are:</p><formula xml:id="formula_5">h ti-1 = β ti h ti-1 ,<label>(4)</label></formula><formula xml:id="formula_6">µ ti = σ(W µ h ti-1 , x ti + b µ ) , r ti = σ(W r h ti-1 , x ti + b r ) ,<label>(5)</label></formula><formula xml:id="formula_7">hti = tanh(W h r ti h ti-1 , x ti + b h) , h ti = (1 -µ ti ) h t i-1 + µ ti hti , (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where µ is update gate, r is reset gate, h is candidate hidden state, σ is the sigmoid activation function, D and G structure. The D is first composed by a GRUI layer to learn the incomplete or complete time series. Then a full-connection layer is stacked on the top of the last hidden state of GRUI. To prevent overfit, we adopt dropout <ref type="bibr" target="#b43">[44]</ref> to the full-connection layer. When we feed original incomplete real time series into D, values at one row of δ are not the same. When we feed fake time series generated by G, values in each row of δ are the same (because there is no missing value). We want to make sure that the time lags of the generated samples are the same as those of the original samples, so the G is also made up of a GRUI layer and a full-connection layer. The G is a self-feed network, it means that the current output of the G will be fed into the next iteration of the same cell. The very first input of G is the random noise vector z and every row of the δ of fake sample is a constant value. That batch normalization <ref type="bibr" target="#b23">[24]</ref> is applied both to G and D.</p><formula xml:id="formula_9">W h, W r , W µ ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Missing Values Imputation by GAN</head><p>From the GAN architecture, we can know that, the generator G can learn a mapping G(z) = z → x that maps the random noise vector z to a complete time series which contains no missing value. However, the problem is the random noise vector z is randomly sampled from a latent space, e.g., Gaussian distribution. It means that, the generated samples may change a lot with the changing of the input random noise z. Although the generated samples obey the distribution of the original samples, the distance between the generated samples and the original samples may also be large. In other words, the degree of similarity between x and G(z) is not large enough. For example, the original incomplete time series contains two classes, and the G learns a distribution that can fit these two classes very well. Given a incomplete sample x and a random input vector z, the G(z) may belong to the opposite class of x, this is not what we want. Although the G(z) may belong to the true class, the similarity of samples within a class could also be large.</p><p>For any incomplete time series x, we try to find a best vector z from the latent input space so that the generated sample G(z) is most similar to x. How to replace the missing values with the most reasonable values? Inspired by <ref type="bibr" target="#b40">[41]</ref>, we introduce a way to measure the degree of imputation fitness. We define a two-part loss function to evaluate the fitness of imputation. The first part of this loss function is the masked reconstruction loss. It means that the generated sample G(z) should be close enough to the original incomplete time series x. The another part of this loss function is the discriminative loss. This part forces the generated sample G(z) as real as possible. The following paragraphs will describe the masked reconstruction loss and the discriminative loss in details. Masked Reconstruction Loss. The masked reconstruction loss is defined by the masked squared errors between the original sample x and the generated sample G(z). It is noteworthy that we only calculate the non-missing part of the data.</p><formula xml:id="formula_10">L r (z) = ||X M -G(z) M || 2 . (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>Discriminative Loss. The discriminative loss stands for the generated sample G(z)'s degree of authenticity. It is based on the output of the discriminator D which represents the confidence level of the input sample G(z)'s being real. We feed the noise vector z into G, then we get the generated sample G(z), next, we feed G(z) into D, finally we get the discriminative loss.</p><formula xml:id="formula_12">L d (z) = -D(G(z)) . (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>Imputation Loss. We define the imputation loss to optimize the random noise vector z. The imputation loss is a combination of the masked reconstruction loss and the discriminative loss.</p><formula xml:id="formula_14">L imputation (z) = L r (z) + λL d (z) ,<label>(9)</label></formula><p>where λ is a hyper-parameter that controls the proportion between the masked reconstruction loss and the discriminative loss.</p><p>For each original time series x, we randomly sample a z from the Gaussian distribution with zero mean and unit variance and feed it into the well trained generator G to get G(z). Then we begin to train the noise z with the loss function L imputation (z) by back propagation method. After the imputation loss converging to the optimal solution, we replace the missing values of x with the generated G(z) just as the following equation shows,</p><formula xml:id="formula_15">x imputed = x M + (1 -M ) G(z) .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate the proposed method in two real-world datasets which include a medical dataset and a air quality dataset. In order to demonstrate the imputation results of the proposed method, we compare our algorithm with simple imputation methods, matrix factorization based imputation method and KNN based imputation method. We also compare our GAN based imputation method against some other baselines in the prediction and regression tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Tasks</head><p>Physionet Challenge 2012 dataset (PhysioNet). The Physionet dataset is a public electronic medical record dataset that comes from the PhysioNet Challenge 2012 <ref type="bibr" target="#b41">[42]</ref>. This dataset consists of records from 4,000 intensive care unit (ICU) stays. Every ICU stay is a roughly 48 hours time series with 41 variables such as age, weight, albumin, heart-rate, glucose, etc. One task of the PhysioNet Challenge 2012 is the mortality prediction task that predicts whether the patient dies in the hospital. There are 554 (13.85%) patients with positive mortality label. This task is a binary classification problem with non-balance dataset, so the AUC score is used to judge the performance of the classifier. Because of the lack of complete dataset, the direct evaluation of missing values filling accuracy is impossible. Therefore, we use the mortality prediction results calculated by the same classifier but trained on different imputed datasets to determine the performance of imputation methods. Machine learning methods must have enough training dataset to learn the potential relation between samples. We do not use the dataset processed by case deletion methods to train the classifier when we use the PhysioNet dataset because of its high missing rate (80.67%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KDD CUP 2018 Dataset (KDD).</head><p>The KDD CUP 2018 dataset is a public air quality dataset that comes from the KDD CUP Challenge 2018 <ref type="bibr" target="#b10">[11]</ref>. KDD dataset contains the historical air quality data of Beijing. We select 11 common air quality and weather data observatories for our experiments. Each observatory owns records observed every one hour from January 1, 2017 to December 30, 2017. The records have total 12 variables which include PM2.5 (ug/m3), PM10 (ug/m3), CO (mg/m3), weather, temperature and so on. We split this dataset for every 48 hours just like the PhysioNet dataset, then we get about 182 time series. For the split dataset, we conduct two tasks as the following described. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training Settings</head><p>Network details and training strategies. The discriminator consists of a GRUI layer and a fullconnection layer. We feed the real incomplete time series x, the fake but complete time series G(z) and their corresponding δ into GRUI layer. Then the the last hidden state of GRUI layer will be fed into full-connection layer with a dropout to get the discriminator's output. The generator is a self-feed network that consists of a GRUI layer and a full-connection layer too. The current hidden state of GRUI layer is fed into full-connection layer with a dropout, then the output of full-connection layer will be treated as the input of the next iteration. All the outputs of full-connection layer are concatenated and batch normalized into the G(z). The very first input of the generator is the random noise z. Before the training of the GAN, the generator is pretrained for some epochs with a squared error loss for predicting the next value in the training time series. For the PhysioNet dataset, the input dimension is 41 (we use all the variables of the PhysioNet dataset), batch size is 128, the hidden units number in GRUI of G and D is 64 and the dimension of random noise is also 64. For the KDD dataset, the input dimension is 132 (11 observatories × 12 variables), the batch size is 16, the number of hidden units in GRUI of G and D is 64 and the dimension of z is 256.</p><p>Comparative Methods. When it is feasible to directly evaluate the imputation accuracy (task 1 of the KDD dataset), we compare the proposed method with simple imputation methods, the matrix factorization imputation method and the KNN imputation method. If it is impracticable to get the complete dataset, we use two tasks to indirectly measure the imputation accuracy. 1) Classification task (mortality prediction task): we use different datasets imputed by proposed method and some other methods to train logistic regression classifier, SVM classifier, random forest classifier and RNN classifier. Then we indirectly compare the filling accuracy of these methods.</p><p>2) Regression task (air quality prediction task): we use datasets imputed by different imputation methods to train linear regression model, decision tree regression model, random forest regression model and RNN based regression model. Then we indirectly compare the filling accuracy of these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Experimental results on Physionet dataset. For the PhysioNet dataset, we can not access the complete samples. Therefore, we measure the filling accuracies of our proposed method and some other imputation methods indirectly. The hyper-parameters of our method are: the train epochs is 30, pretrain epochs is 5, learning rate is 0.001, λ is 0.15 and the number of optimization iterations of the imputation loss is 400. Figure <ref type="figure">6</ref> is the comparison results of the classification task (mortality prediction task). We first complete the dataset by filling last value, zero value, mean value and GAN generated value. The standardization of input dataset is conducted when we impute the missing values with mean value, last value and GAN generated value. If we also conduct standardization on zero value imputation, the zero value imputation will be same as the mean imputation. So we do not standardize the input dataset when we impute with zero value. We train the logistic regression classifier, SVM (with RBF kernel, Linear kernel, Poly kernel and Sigmoid kernel) classifiers, random forest classifier and RNN classifier on these above imputed complete datasets to indirectly compare the filling accuracy of these filling methods. The RNN classifier is composed by a GRUI layer that processes complete time series and a full-connection layer that outputs classification results. We can see that, except for the SVM classifier with RBF kernel, the classifiers trained on dataset imputed by proposed method always gain the best AUC score. These results can prove the success of GAN based imputation method indirectly because of the lack of complete dataset. It is worth noting that, we achieve the new state-of-the-art mortality prediction result with AUC score of 0.8603 by using the dataset imputed by the GAN based imputation method, while the previous state-of-the-art AUC score is 0.848 <ref type="bibr" target="#b24">[25]</ref>. Table <ref type="table" target="#tab_1">2</ref> is the detail description of mortality prediction task results produced by different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Result</head><p>Neural Network model called GRUD <ref type="bibr" target="#b6">[7]</ref> 0.8424 Hazard Markov Chain model <ref type="bibr" target="#b28">[29]</ref> 0.8381 Regularized Logistic Regression model <ref type="bibr" target="#b24">[25]</ref>  Experimental results on KDD dataset. Table <ref type="table" target="#tab_2">3</ref> shows the comparison results between the proposed GAN based method and some other imputation methods which include imputation method that uses the last observed value to impute missing values (last imputation), method that uses mean value to fill missing values (mean imputation), KNN based method and matrix factorization based method. Before the starting of the experiments, we have standardized the input dataset. Therefore, filling zero value is the same as filling mean value. The hyper-parameters of our method are: the train epochs is 25, pretrain epochs is 20, learning rate is 0.002, λ is 0.0 and the number of optimization iterations of the imputation loss is 800. The first column of the table <ref type="table" target="#tab_2">3</ref> is the missing-rate which indicates there are how many percent missing values in the dataset. The remaining columns are the mean squared errors (MSE) of the corresponding imputation methods. We can see that, with most missing-rates of the dataset, the proposed method owns the best filling accuracy. This is because the proposed GAN based method can automatically learn the temporal relationship of the same sample, the similarities between the similar samples, the association rules of two variables and the distribution of the dataset. By this way, the proposed GAN based method can fill the missing holes with the most reasonable values.</p><p>Figure <ref type="figure">7</ref> is the experimental results of the regression task. We use the KDD dataset with 50% percent missing values. Just like the settings of the classification task, we first fill the missing values. Then we train some regression models that include decision tree model, linear regression model, random forest model and RNN regression model. The RNN regression model is also made up of a GRUI layer and a full-connection layer. The hyper-parameters are the same as direct comparison. Because we have standardized the input dataset, zero filling is the same as mean filling. Figure <ref type="figure">7</ref> shows that the regression model trained with dataset which is imputed by the proposed method always gains the minimum MSE value. These results prove the success of GAN based imputation method. In most cases, the proposed method owns the best imputation accuracy.</p><p>Comparison with GAN using a non-modified GRU. We have also compared the proposed method with a GAN that use a non-modified GRU. In this situation, we do not take the advantage of the time interval information and then treat the time series as fixed interval data. So we do not model the time decay vector β to control the influence of the past observations. We find that, with a non-modified GRU, the final AUC score of the Physionet dataset is 0.8029 while the GRUI is 0.8603. At the mean time, Table <ref type="table" target="#tab_3">4</ref> shows the advantages of the GRUI cell tested on the KDD dataset. We can see that with the damping of the hidden state, the final performance of the imputation will increase a lot in all situations. The reason is that our model can learn and make use of the flexible time lags of the dataset and then produces better results than a non-modified GRU cell.</p><p>Missing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussions</head><p>The proportion between discriminative loss and masked reconstruction loss. In this part, we investigate the most influential hyper-parameter λ. Figure <ref type="figure">8</ref> and<ref type="figure">9</ref> show the impact of the λ, that is, the impact of the proportion between discriminative loss and masked reconstruction loss. We sample 13 values from 0.0 to 16.0 for λ and compare the experimental results of these varied λ. When we perform the regression task on KDD dataset, we can conclude that with the growth of λ, the MSE of the KDD dataset grows near-linearly. It can be interpreted that the masked reconstruction loss dominates the imputation loss and the discriminative loss helps a little for the regression task on KDD dataset. The classification task results on PhysioNet dataset show that, the AUC score is small when the λ is 0.0, and the AUC score reaches the maximum at the point of 0.15, then it decreases over the growth of λ. This phenomenon shows that the discriminative loss helps a lot for the classification task on PhysioNet dataset, especially with the λ value of 0.15. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>This part will introduce the related works about missing value processing methods and generative adversarial networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Missing Value Processing Methods</head><p>The presence of missing values in datasets will significantly degrade the data analyses results <ref type="bibr" target="#b7">[8]</ref>.</p><p>In order to deal with the missing values in datasets, researchers have proposed many missing data handling methods in recent years. These methods can be classified into case deletion based methods, simple imputation methods and machine learning based imputation methods.</p><p>Deletion based methods erase all observations/records with missed values, including Listwise deletion <ref type="bibr" target="#b44">[45]</ref> and Pairwise deletion <ref type="bibr" target="#b34">[35]</ref>. The common drawback of the deletion methods is the loss of power when the missing rate is large enough (i.e. bigger than 5%) <ref type="bibr" target="#b17">[18]</ref>.</p><p>Simple imputation algorithms impute the missing values with some statistical attributes, such as replace missing value with mean value <ref type="bibr" target="#b26">[27]</ref>, impute with median value <ref type="bibr" target="#b0">[1]</ref>, impute with most common value <ref type="bibr" target="#b11">[12]</ref> and complete the dataset with last observed valid value <ref type="bibr" target="#b1">[2]</ref>.</p><p>Machine learning based imputation methods include maximum likelihood Expectation-Maximization (EM) based imputation <ref type="bibr" target="#b37">[38]</ref>, K-Nearest Neighbor (KNN) based imputation <ref type="bibr" target="#b39">[40]</ref>, Matrix Factorization (MF) based imputation and Neural Network (NN) based imputation. The EM imputation algorithm is made up of the "expectation" step and the "maximization" step which iteratively updates model parameters and imputed values so that the model best fits the dataset. The KNN based imputation method uses the mean value of the k nearest samples to impute missing values. The MF based imputation factorizes the incomplete matrix into low-rank matrices 'U' and 'V' solved by gradient descent algorithm, with a L1 sparsity penalty on the elements of 'U' and a L2 penalty on the elements of 'V'. Neural Network based imputation <ref type="bibr" target="#b15">[16]</ref> uses the numerous parameters of the neural network to learn the distribution of train dataset and then fills the missing values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Generative Adversarial Networks</head><p>At the year of 2014, Goodfellow et al <ref type="bibr" target="#b16">[17]</ref> introduced the generative adversarial networks (GAN), which is a framework for estimating generative model via an adversarial process. The generative adversarial networks is made up of two components: a generator and a discriminator. The generator tries to fool the discriminator by generating fake samples from a random "noise" vector. The discriminator tries to distinguish between fake and real samples, i.e., to produce the probability that a sample comes from real datasets rather than the generator. However, the traditional GAN is hard to train, WGAN <ref type="bibr" target="#b2">[3]</ref> is another training way of GAN, WGAN can improve the stability of learning and get out of the problem of mode collapse.</p><p>Many works have shown that the well trained GAN can produce realistic images in computer vision field <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref>. GAN is also successfully used to complete faces <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31]</ref>. Only A few works has introduced GAN into sequences generating field <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47]</ref>, such as SeqGAN <ref type="bibr" target="#b46">[47]</ref> and MaskGAN <ref type="bibr" target="#b12">[13]</ref>. However, these works are not suitable for missing values imputation field. That is, before the generating of the sequences, these methods require the complete train dataset which is impossible in our scenario, yet our model needn't complete train datasets. Besides, most of GAN based sequence generation methods produce new samples from a random "noise" vector. With the changing of the random "noise" vector, the generated samples will change a lot. However, the data imputation task requires the imputed value as close as possible to the original incomplete data. There also exists a few work that uses GAN to impute the missing values such as GAIN <ref type="bibr" target="#b45">[46]</ref>. The drawback of GAIN is the lack of consideration for the imputation of time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel generative adversarial networks for data imputation. In order to learn the unfixed time lags of two observed values, a modified GRU cell (called GRUI) is proposed for processing the incomplete time series. After the training of the GAN model with GRUI cell, the "noise" input vector of the generator is trained and generating reasonable values for imputation. In this way, the temporal relationships, the inner-class similarities, and the distribution of the dataset can be automatically learned under the adversarial architecture. Experimental results show that our method can outperform the baselines in terms of accuracy of missing value imputation, and has benefits for downstream applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Missing rates in the Physionet dataset. The X-axis is the time. The Y-axis is the selected 7 variables. Redder the color, higher the missing rate.</figDesc><graphic coords="2,129.95,71.54,154.45,114.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The lines stand for maximum, minimum and average missing rates at each hour. The global missing rate is 80.67%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The structure of the proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 4: GRU cell.</figDesc><graphic coords="4,134.04,318.46,106.71,57.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: The AUC score of mortality prediction by different classification models trained on different imputed datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: The influence of λ in classification task. AUC score reaches the maximum at λ = 0.15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>1) Time series imputation task: For every 48 hours length time series, we randomly discard p percent of the dataset. Then we fill the missing values and calculate the imputation accuracy, where p ∈ {20, 30, . . . , 90}. The imputation accuracy is defined as the mean squared error (MSE) between original values and imputed values. 2) Air quality prediction task: For every 48 hours length time series, we randomly discard 50 percent of the dataset. Then we predict the mean air quality of the next 6 hours. Just like what we did previously, we use the air quality prediction results calculated by the same regression model but trained on different imputed datasets to determine the performance of imputation methods. Dataset Statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># of Features # of Samples Missing Rate</cell></row><row><cell>Physionet</cell><cell>41</cell><cell>4000</cell><cell>80.67%</cell></row><row><cell>KDD</cell><cell>132</cell><cell>182</cell><cell>1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The AUC score of the mortality prediction task on the Physionet dataset. The RNN model that uses the dataset imputed by our method achieves the highest AUC score.</figDesc><table><row><cell>0.848</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The MSE results of the proposed method and other imputation methods on the KDD dataset.</figDesc><table><row><cell cols="6">Missing-rate Last filling Mean filling KNN filling MF filling GAN filling</cell></row><row><cell>90%</cell><cell>2.870</cell><cell>1.002</cell><cell>1.243</cell><cell>1.196</cell><cell>1.018</cell></row><row><cell>80%</cell><cell>1.689</cell><cell>0.937</cell><cell>0.873</cell><cell>0.860</cell><cell>0.837</cell></row><row><cell>70%</cell><cell>1.236</cell><cell>0.935</cell><cell>0.852</cell><cell>0.805</cell><cell>0.780</cell></row><row><cell>60%</cell><cell>1.040</cell><cell>0.973</cell><cell>0.856</cell><cell>0.834</cell><cell>0.803</cell></row><row><cell>50%</cell><cell>0.990</cell><cell>0.923</cell><cell>0.798</cell><cell>0.772</cell><cell>0.743</cell></row><row><cell>40%</cell><cell>0.901</cell><cell>0.914</cell><cell>0.776</cell><cell>0.787</cell><cell>0.753</cell></row><row><cell>30%</cell><cell>0.894</cell><cell>0.907</cell><cell>0.803</cell><cell>0.785</cell><cell>0.780</cell></row><row><cell>20%</cell><cell>1.073</cell><cell>0.916</cell><cell>0.892</cell><cell>0.850</cell><cell>0.844</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The MSE comparison of a GAN with GRU and a GAN with GRUI on KDD dataset.</figDesc><table><row><cell cols="2">-rate 90%</cell><cell>80%</cell><cell>70%</cell><cell>60%</cell><cell>50%</cell><cell>40%</cell><cell>30%</cell><cell>20%</cell></row><row><cell>GRU</cell><cell cols="8">1.049 0.893 0.841 0.823 0.794 0.767 0.820 0.849</cell></row><row><cell>GRUI</cell><cell cols="8">1.018 0.837 0.780 0.803 0.743 0.753 0.780 0.844</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>We thank the reviewers for their constructive comments. We also thank Zhicheng Dou for his helpful suggestions. This research is supported by National Natural Science Foundation of China (No. 61772289 and No.61872338).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The treatment of missing values and its effect on classifier accuracy</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Classification, clustering, and data mining applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="639" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Missing data imputation using fuzzy-rough methods</title>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page" from="152" to="164" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of four missing data treatment methods for supervised learning</title>
		<author>
			<persName><forename type="first">Eapa</forename><surname>Gustavo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monard</forename><surname>Carolina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="519" to="533" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ambientgan: Generative models from lossy measurements</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Maximum-likelihood augmented discrete generative adversarial networks</title>
		<author>
			<persName><forename type="first">Yanran</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07983</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6085</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A review of missing data handling methods in education research</title>
		<author>
			<persName><surname>Jehanzeb R Cheema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Educational Research</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="487" to="508" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<ptr target="http://www.kdd.org/kdd" />
		<title level="m">KDD Cup</title>
		<imprint>
			<date type="published" when="2018">2018/, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A gentle introduction to imputation of missing values</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rogier</surname></persName>
		</author>
		<author>
			<persName><surname>Donders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jmg</forename><surname>Geert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theo</forename><surname>Van Der Heijden</surname></persName>
		</author>
		<author>
			<persName><surname>Stijnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Karel</surname></persName>
		</author>
		<author>
			<persName><surname>Moons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of clinical epidemiology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1087" to="1091" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Maskgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07736</idno>
		<title level="m">Better text generation via filling in the</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Missing data imputation on the 5-year survival prediction of breast cancer patients with unknown discrete values</title>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">J</forename><surname>García-Laencina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Henriques</forename><surname>Abreu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><forename type="middle">Henriques</forename><surname>Abreu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noémia</forename><surname>Afonoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in biology and medicine</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="125" to="133" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pattern classification with missing data: a review</title>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">J</forename><surname>García-Laencina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José-Luis</forename><surname>Sancho-Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aníbal R Figueiras-</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="282" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A neural network-based framework for the reconstruction of incomplete data sets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Iffat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">S</forename><surname>Gheyas</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">16-18</biblScope>
			<biblScope unit="page" from="3039" to="3065" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Missing data analysis: Making it work in the real world</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of psychology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="549" to="576" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Matrix completion and low-rank svd via fast alternating least squares</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3367" to="3402" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Günter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Forecasting stock markets using wavelet transforms and recurrent neural networks: An integrated system based on artificial bee colony algorithm</title>
		<author>
			<persName><forename type="first">Jung</forename><surname>Tsung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><forename type="middle">Chang</forename><surname>Fen Hsiao</surname></persName>
		</author>
		<author>
			<persName><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing Journal</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2510" to="2525" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stacked generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data preprocessing and mortality prediction: The physionet/cinc 2012 challenge revisited</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Alistair Ew Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gari D</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing in Cardiology Conference (CinC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="157" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dealing with missing values in data</title>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of systems integration</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Data mining: concepts, models, methods, and algorithms</title>
		<author>
			<persName><forename type="first">Mehmed</forename><surname>Kantardzic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predicting mortality of intensive care patients via learning about hazard</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Dae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4953" to="4954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative face completion</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Semantically consistent image completion with fine-grained details</title>
		<author>
			<persName><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinjia</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09345</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Approximation and convergence properties of generative adversarial learning</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5551" to="5559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recent progress of face image synthesis</title>
		<author>
			<persName><forename type="first">Zhihe</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04717</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spectral regularization algorithms for learning large incomplete matrices</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2287" to="2322" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Missing data: A gentle introduction</title>
		<author>
			<persName><forename type="first">Katherine</forename><forename type="middle">M</forename><surname>Patrick E Mcknight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Souraya</forename><surname>Mcknight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><forename type="middle">Jose</forename><surname>Sidani</surname></persName>
		</author>
		<author>
			<persName><surname>Figueredo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Guilford Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gradient descent gan optimization is locally stable</title>
		<author>
			<persName><forename type="first">Vaishnavh</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kolter</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5591" to="5600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Missing data: A comparison of neural network and expectation maximization techniques</title>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Fulufhelo V Nelwamondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tshilidzi</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><surname>Marwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Science</title>
		<imprint>
			<biblScope unit="page" from="1514" to="1521" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Dutil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10929</idno>
		<title level="m">Adversarial generation of natural language</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">MATLAB Release. The mathworks. Inc</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Natick, Massachusetts, United States, 488</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Seeböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Predicting inhospital mortality of icu patients: The physionet/computing in cardiology challenge 2012</title>
		<author>
			<persName><forename type="first">Ikaro</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><forename type="middle">A</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing in Cardiology (CinC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="245" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A brief review of the main approaches for treatment of missing data</title>
		<author>
			<persName><forename type="first">O</forename><surname>Luciana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">E</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><surname>Zárate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1177" to="1198" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Longitudinal and multigroup modeling with missing data</title>
		<author>
			<persName><forename type="first">Werner</forename><surname>Wothke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Gain: Missing data imputation using generative adversarial nets</title>
		<author>
			<persName><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02920</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Resolving the bias in electronic medical records</title>
		<author>
			<persName><forename type="first">Kaiping</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kee</forename><surname>Yuan Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beng</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chin</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Luen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Yip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2171" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
