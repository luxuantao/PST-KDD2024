<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tracing Fake-News Footprints: Characterizing Social Media Messages by How They Propagate</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
							<email>wuliang@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<settlement>Tempe</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
							<email>huanliu@asu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<settlement>Tempe</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tracing Fake-News Footprints: Characterizing Social Media Messages by How They Propagate</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">31CBE22D3B0CE3119B0BF4252D6A0116</idno>
					<idno type="DOI">10.1145/3159652.3159677</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Misinformation</term>
					<term>Fake News Detection</term>
					<term>Graph Mining</term>
					<term>Social Network Analysis</term>
					<term>Social Media Mining</term>
					<term>Classication</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When a message, such as a piece of news, spreads in social networks, how can we classify it into categories of interests, such as genuine or fake news? Classication of social media content is a fundamental task for social media mining, and most existing methods regard it as a text categorization problem and mainly focus on using content features, such as words and hashtags. However, for many emerging applications like fake news and rumor detection, it is very challenging, if not impossible, to identify useful features from content. For example, intentional spreaders of fake news may manipulate the content to make it look like real news. To address this problem, this paper concentrates on modeling the propagation of messages in a social network. Specically, we propose a novel approach, TraceMiner, to (1) infer embeddings of social media users with social network structures; and (2) utilize an LSTM-RNN to represent and classify propagation pathways of a message. Since content information is sparse and noisy on social media, adopting TraceMiner allows to provide a high degree of classication accuracy even in the absence of content information. Experimental results on real-world datasets show the superiority over state-of-the-art approaches on the task of fake news detection and news categorization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>that 62% of adults get their news from social media in United States, with 29% among them doing so very often <ref type="foot" target="#foot_0">1</ref> . Concomitant with the expansive and varied sources of data are the challenges for personalizing the massive amount of information andltering out unwanted messages such as fake news and spam. However, the sparse and noisy social media content makes it dicult for traditional approaches, which heavily rely on content features, to tackle these challenges.</p><p>By contrast, our study aims to nd additional data sources to solve the problem. In this work, we focus on the diusion of information. A key driving force behind the diusion of information is its spreaders. People tend to spread information that caters to their interests and/or ts their system of belief <ref type="bibr" target="#b7">[8]</ref>. Hence, similar messages usually leads to similar traces of information diusion: they are more likely to be spread from similar sources, by similar people and in similar sequences. Since the diusion information is pervasively available on social networks, in this work, we aim to investigate how the traces of information diusion in terms of spreaders can be exploited to categorize a message. The message can be a piece of news, a story or a meme that has been posted and forwarded in social networks, and those users who post or forward it are the spreaders. Traces of a message refer to by whom and when the message is spread, i.e., posted or forwarded.</p><p>We propose TraceMiner, a novel approach for classifying social media messages with diusion network information. TraceMiner takes traces of a message as input and outputs its category. Consider the huge number of social media users and all the possible combinations of spreaders, traces will be of high dimensionality and thus may result in sparsity in the feature space. To cope with the problem, TraceMiner utilizes the proximity of nodes <ref type="bibr" target="#b33">[34]</ref> and social dimensions <ref type="bibr" target="#b34">[35]</ref> manifested in the social network, which have been successfully applied to capture the intrinsic characteristics of social media users in a myriad of applications.</p><p>To demonstrate TraceMiner's potential on real-world applications, we evaluate it with traditional approaches on Twitter data. TraceMiner outperforms competitors on multi-label information classication problems in large graphs. Therefore, TraceMiner provides an alternative way for modeling social media messages through learning abundant diusion data that has not be fully utilized. Existing graph mining research mainly focuses on learning representation of graphs and nodes, while little attention has been paid to classifying information circulating between nodes. TraceMiner distances from existing graph representation methods by directly modeling information and making predictions in an end-to-end manner other than providing only an attribute vector or embedding vector.</p><p>TraceMiner is scalable and the optimization can be easily parallelized through open-source software libraries. Hence, our method can be useful for a variety of social media mining problems where information from content is insucient. Our contributions can be summarized as follows:</p><p>• We propose a novel approach for classifying social media messages with diusion network information. • We derive ecient optimization methods for TraceMiner, and provide analysis to guarantee the correctness. • We extensively evaluate the performance on real social network data, and the experimental results demonstrate the eectiveness on dierent tasks.</p><p>The rest of this paper is organized as follows. In Section 2, we provide the denition for the problem. In Section 3, we introduce the proposed approaches and the optimization methods. How TraceMiner can be utilized to classify information diusion sequences is presented in Section 4. In Section 5, we show empirical evaluation with discussions. Related work is discussed in Section 6. Conclusion and future work are presented in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION</head><p>We consider the problem of classifying social media messages into one or more categories. We dene a graph G 2 hV , Ei, where</p><formula xml:id="formula_0">i 2 V with i 2 [1, |V |] is a node (user) and E ✓ V ⇥ V is the set of edges. If e i j 2 E,</formula><p>there is an edge between i and j , otherwise there is not. Let M be the set of messages where</p><formula xml:id="formula_1">m i 2 M with i 2 [1, |M |]. Each message m i has a corresponding set of spreaders {( m i 1 , t m i 1 ), {( m i 2 , t m i 2 ), • • • , {( m i n , t m i n )},</formula><p>where n is the number of spreaders for m i and m i j is a user who spreads m i at the time of t m i j . Messages are partially labeled and thus only some of them have an associated class label. We denote the set of labels as Y , where i 2 Y indicates that m i is labeled. Our goal is to learn a model with the social network graph G and partially labeled message M with the corresponding diusion traces and label information Y , to predict ˆ for the unlabeled messages.</p><p>Problem denition for traditional approaches: In order to make predictions for messages, most existing methods take the problem as a text categorization task, hence, each message m i has a set of spreaders {(</p><formula xml:id="formula_2">m i 1 , t m i 1 , c m i 1 ), • • • , {( m i n , t m i n , c m i n )}</formula><p>, where c m i j is the content information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED METHOD</head><p>In this section, we introduce how a diusion trace can be used to facilitate classication. We rst utilize sequential modeling methods to enable sequences to be used as attribute vectors. To alleviate the sparsity of sequences, we present a novel embedding method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sequence Modeling</head><p>Given the spreader information {(</p><formula xml:id="formula_3">m i 1 , t m i 1 ), • • • , {( m i n , t m i n )</formula><p>} and the graph G, the topology of information diusion can be inferred by graph mining techniques <ref type="bibr" target="#b8">[9]</ref>. The topology, which is usually a tree or forest (multiple trees) rooted with the initial spreader, contains informative patterns for characterizing a message. However, it is extremely dicult to directly deal with the tree structure. Consider two messages with similar diusion networks, adding or removing one spreader, or changing any direction of the information ow would lead to a dierent tree. Theoretically, there can be n n 2 dierent trees with n number of dierent nodes according to the Cayley's formula <ref type="bibr" target="#b6">[7]</ref>.</p><p>In order to solve this problem, we convert the tree structure into a temporal sequence. For example, given the spreaders of</p><formula xml:id="formula_4">m i {( m i 1 , t m i 1 ), • • • , {( m i n , t m i n )}, we generate a sequence x i = [( m i q(1) , t m i q(1) ), • • • , ( m i q(n) , t m i q(n) )]</formula><p>where for any two elements k and j in the sequence, if k &lt; j, then t m i q(k ) &lt; t m i q(j) , meaning that m i q(k)</p><p>spread the information earlier than m i q(j) did. Therefore, given n nodes, the number of all possible diusion networks are reduced to n!. In order to further alleviate the sparsity, we incorporate social proximity and social dimensions in Section 3.2.</p><p>However, a possible problem of temporally sequencing spreaders is the loss of dependencies between users. Given m i and m j where e i j 2 E. If t m i &lt; t m j , it is likely that user i spreads it to j or j is inuenced by i <ref type="bibr" target="#b8">[9]</ref>. Such direct dependency will be of vital importance in characterizing the information. For example, the information ow from the controller account to the botnet followers is a key signal in detecting crowdturng <ref type="bibr" target="#b10">[11]</ref>. But if there is a spreader (u m k , t m k ) where &lt; t m i &lt; t m j , in the sequence, i and j will be separated. Therefore, it would be appealing if the model can take advantage of dependencies between separated and distant items in a sequence. To this end, we propose to apply Recurrent Neural Networks (RNNs).</p><p>RNNs have been successfully applied in a myriad of domains for modeling sequential data <ref type="bibr" target="#b9">[10]</ref>, such as information retrieval <ref type="bibr" target="#b27">[28]</ref>, sentiment analysis <ref type="bibr" target="#b32">[33]</ref> and machine translation <ref type="bibr" target="#b5">[6]</ref>. We propose to use an RNN to sequentially accept each spreader of a message and recurrently project it into a latent space with the contextual information from previous spreaders in the sequence. As the RNN reaches the end of the sequence, a prediction can be made based on the embedding vector produced by the hidden activations. In order to better encode the distant and separated dependencies, we further incorporate the Long Short-Term Memory cells into the RNN model, i.e., the LSTM-RNN.</p><p>In information diusion, the rst spreader who initiates the diusion process is more likely to be useful for classifying the message <ref type="bibr" target="#b0">[1]</ref>. Hence, we feed the spread sequence in the reverse order, where the rst spreader in the sequence directly interacts with the prediction result, and thus it has more impact. Each spreader is represented by a local RNN. Parameters W of RNNs are shared across each replication in the sequence and h 0 is the previous recurrent output sent between RNNs to exploit the contextual information. In order to make the prediction, the last local RNNs are taking the rst spreader's attribute vector, prior recurrent output (and the label of the message) as input to predict the category of the message (or to train the RNNs model). In this work, we set the hidden node size (k) as 10. The way we obtain the attribute vector of nodes is introduced in Section 3.2.</p><p>Having chosen LSTM-RNNs as our method to classify messages, we now need a suitable way of learning attribute vectors f, for social media users. An intuitive way is to utilize the social network   graph G to generate embedding vectors <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34]</ref>, and feed sequences of embedding vectors to the LSTM-RNNs <ref type="bibr" target="#b27">[28]</ref>. Such embeddingbased preprocessing for sequential data has been widely used for natural language processing. We follow the practice since 1) several social graph embedding approaches have been proven useful for classication tasks, such as LINE <ref type="bibr" target="#b33">[34]</ref> and DeepWalk <ref type="bibr" target="#b28">[29]</ref>, and 2) users appear in spread traces follow similar distribution of how words appear in the social media posts.</p><p>Figure <ref type="figure" target="#fig_2">1</ref> illustrates the distribution of users and words. The distribution in Figure <ref type="figure" target="#fig_2">1</ref>(a) comes from a real-world Twitter message trace dataset showing how users appear in message traces. The distribution in Figure <ref type="figure" target="#fig_2">1</ref>(b) comes from the same dataset showing how words appear in message content. They both follow a powerlaw distribution, which motivates us to embed users into low dimensional vectors, as how embedding vectors of words are used in natural language processing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>. Several graph embedding algorithms are available, we will compare their performance and provide our solution and reasons behind our choice in the next subsection. For the rest of the subsection, we will introduce the optimization for the proposed LSTM-RNNs.</p><p>We show the training of the proposed LSTM-RNNs in Algorithm 1. We input the labeled spreader sequences X and the corresponding labels Y , which are randomly split into a training and a validation set in line 2. In addition to the maximum number of iterations Max iter , we also have a function Earl Stop() for controlling early termination of the training, which takes the loss on the validation set as the input. In line 1, we initialize the model parameters randomly with Gaussian distribution. From line 3 to 7, we update W with training data until the maximum epoch is reached or the early termination condition is met. The loss function used in line 4 is shown below:</p><formula xml:id="formula_5">|X t r | ' i=1 |Y tr = 0| i log( ˆ i ) + |Y tr = 1|(1 i )(log(1 ˆ i )), (<label>1</label></formula><formula xml:id="formula_6">)</formula><p>where i is the true label of i and ˆ i is the corresponding prediction. So Eq.( <ref type="formula" target="#formula_5">1</ref>) calculates the cross entropy between the true labels and the prediction. </p><formula xml:id="formula_7">6: i = i + 1 7: while EarlyStop(V Loss, i) = FALSE AND (i &lt; Max iter )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Embedding of Users</head><p>Given the framework of sequence modeling, the next problem is to nd the proper embedding method that captures the intrinsic features of social media users. As discussed previously, using embedding vectors can help alleviate the data sparsity through leveraging social proximity and social dimensions. In this work, among the existing embedding methods, we will mainly focus on two state-of-the-art approaches that have been proven eective on social graphs, LINE <ref type="bibr" target="#b33">[34]</ref> and DeepWalk <ref type="bibr" target="#b28">[29]</ref>. Both LINE and DeepWalk aim to provide a representation for data instances that captures the inherent properties, such as social proximity.</p><p>These methods mainly focus on the microscopic structure of networks. For example, rst-order proximity constrains users that are connected to be similar and second-order proximity constrains users that have common friends to be similar. LINE achieves this by sampling such nodes from the network and updating their representations jointly, while DeepWalk samples a sequence of data with a random walk algorithm. Nevertheless, for a large social graph, some mesoscopic structure such as social dimensions <ref type="bibr" target="#b34">[35]</ref> and community structures <ref type="bibr" target="#b43">[44]</ref> are more useful in characterizing information <ref type="bibr" target="#b16">[17]</ref>. Therefore, the ideal embedding method should be able to capture both local proximity and community structures.</p><p>Table <ref type="table" target="#tab_0">1</ref> illustrates our results of using dierent embedding methods. We test LINE, DeepWalk and SocDim <ref type="bibr" target="#b34">[35]</ref> on Twitter data and show the distance between neighbors with the new representation. We also detect community structures in the network and calculate the average of distances between nodes that are in the same community. The community detection algorithm is an accelerated version of Louvain method <ref type="bibr" target="#b1">[2]</ref>. As shown in the table, LINE captures the rst and second-degree proximity, while SocDim best captures the community-wise proximity. Based on the random walk, DeepWalk achieves better community-wise proximity, however, it is still outperformed by SocDim, which directly models the community structure.</p><p>In order to capture both the social proximity and communitywise similarity among users, we propose a principled framework that directly models both kinds of information. Given the social graph G, we can derive an adjacency matrix S 2 R n⇥n , where n is the number of users. Our goal is to learn a transformation matrix M 2 R n⇥k which converts users to a latent space with the dimensionality of k. Note that we reuse k for brevity of presentation, and the number of features and hidden nodes in the LSTM-RNNs are not necessarily the same. In order to capture the community-wise similarity, we introduce two auxiliary matrices, a community indicator matrix H 2 R n⇥ , where is the number of communities and tr (HH T ) = n (only one element is 1 in each row and all the others are 0), and a community representation matrix C 2 R ⇥k , where each row c i is an embedding vector describing the community. In order to capture the community structure, we embed the problem into an attributed community detection model <ref type="bibr" target="#b43">[44]</ref>:</p><formula xml:id="formula_8">min M,H,C n ' i=1 ||s i M h i C|| 2 2 + ||H MC T || 2 F , s.t. tr (HH T ) = n,<label>(2)</label></formula><p>where s i M is the embedding vector and we regularize it to be similar to the representation of its corresponding community h i C.</p><p>The second term aims to achieve the intra-group coherence by predicting the community assignment by group the embedding vectors of users and communities <ref type="bibr" target="#b43">[44]</ref>. The objective function in Eq.(2) aims to cluster nodes with embedding vectors. In order to further regularize the clusters to be social communities, we adopt a modularity maximization-based method, which has been widely used to detect communities with network information <ref type="bibr" target="#b36">[37]</ref>. Specically, given the adjacency matrix S and the community membership indicator, the modularity is dened as follows <ref type="bibr" target="#b34">[35]</ref>:</p><formula xml:id="formula_9">Q = 1 2|E| ' i, j (S i j d i d j 2|E| )(h i h T j ),<label>(3)</label></formula><p>where |E| is the number of edges and d i is the degree of i. h i is the community assignment vector for i, and h i h T j = 1 if i and j belong to the same community, otherwise h i h T j = 0.</p><formula xml:id="formula_10">d i d j 2 |E |</formula><p>is the expected number of edges between i and j if edges are placed at random. Modularity Q measures the dierence between the number of actual edges within a community and the expected number of edges placed at random. An optimal community structure H should maximize the modularity Q. By dening the modularity matrix B 2 R n⇥n where B i j = S i j d i d j 2|E | and suppressing the constant which has no eect on the modularity, we rewrite Eq.(3) as follows:</p><formula xml:id="formula_11">Q = tr (H T BH).</formula><p>In order to guarantee that the embedding vectors preserve the community structure in the latent space, we propose to integrate modularity maximization into the embedding method.</p><p>The objective function can be rewritten with the modularity maximization regularizer as follows:</p><formula xml:id="formula_12">min M,H,C n ' i=1 ||s i M h i C|| 2 2 + ||H MC T || 2 F tr (H T BH) s.t. tr (HH T ) = n,<label>(4)</label></formula><p>where controls the inuence of community structures. As discussed previously, the microscopic structure is also of vital importance for generating embedding vectors. In order to jointly consider both mesoscopic and microscopic structures, we decompose M into a conjunction of a global model parameter M and a localized variable M i for each user i (M = M +M i for each user i). Therefore, M captures the community structure and M i can be used to directly apprehend the microscopic structure between nodes. Motivated by recent research on network regularization, we fortify the representation of nodes with proximity by the network lasso regularization term <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_13">' i, j A i j ||M i M j || 2 F ,</formula><p>where A 2 R n⇥n is the microscopic structure matrix, A i j = 1 if we aim to preserve the proximity between i and j in the latent space. Following conventional graph embedding practices <ref type="bibr" target="#b33">[34]</ref>, we consider rst-and second-degree proximity, meaning that A i j = 1 if i and j are connected or share a common friend. Note that A can be specied with particular applications. Imposing the Frobenius norm of the dierence between M i and M j incentivizes them to be the same when A i j = 1. By incorporating the network lasso regularizer, the objective function can be reformulated as follows:</p><p>min</p><formula xml:id="formula_14">M,H,C n ' i=1 ||s i ( M + M i ) h i C|| 2 2 + ||H MC T || 2 F tr (H T BH) + ' i, j A i j ||M i M j || 2 F , s.t. tr (HH T ) = n,<label>(5)</label></formula><p>where controls the inuence of the network lasso. As we can see, we establish the consensus relationship between mesoscopic and microscopic network structures by jointly considering the social communities and proximity. By introducing the global parameter M and the personal variable M i , we force both kinds of information to be preserved in the newly-learnt embedding vectors. However, Eq.( <ref type="formula" target="#formula_14">5</ref>) is not jointly convex to all the parameters M,H and C. In order to solve the problem, we separate the optimization into four subproblems and iteratively optimize them. We will introduce details of the optimization for the rest of the section.</p><p>Update M whilexing M i , H and C: By removing terms that are irrelevant to M, we obtain the following optimization problem:</p><formula xml:id="formula_15">min M n ' i=1 ||s i M + s i M i h i C|| 2 2 + ||H MC T || 2 F ,<label>(6)</label></formula><p>which is convex w.r.t. M. In real applications, the number of users n may be huge. Hence, we adopt a gradient-based update rule as follows:</p><formula xml:id="formula_16">M = M @ M @ M ,<label>(7)</label></formula><p>Technical Presentation WSDM'18, February 5-9, 2018, Marina Del Rey, CA, USA</p><p>where is the step size that can be obtained through backtracking line search <ref type="bibr" target="#b26">[27]</ref>. The derivative of M is shown as follows:</p><formula xml:id="formula_17">@ M @ M = s T i n ' i=1 (s i M + s i M i h i C) + (H MC T )C.<label>(8)</label></formula><p>Update M i whilexing M, H and C: By removing terms that are irrelevant to M i , we obtain the following optimization problem:</p><formula xml:id="formula_18">min M i n ' i=1 ||s i M + s i M i h i C|| 2 2 + ' i, j A i j ||M i M j || 2 F ,<label>(9)</label></formula><p>which is convex w.r.t. M i . Similarly, we derive the gradient:</p><formula xml:id="formula_19">@ M i @M i = s T i n ' i=1 (s i M + s i M i h i C) + ' i, j A i j (M i M j ). (<label>10</label></formula><formula xml:id="formula_20">)</formula><p>Update C whilexing M, M i , and H: By removing terms that are irrelevant to C, we obtain the following optimization problem:</p><formula xml:id="formula_21">min C n ' i=1 ||s i ( M + M i ) h i C|| 2 2 + ||H MC T || 2 F ,<label>(11)</label></formula><p>which is convex w.r.t. C. Similarly, the gradient can be obtained as:</p><formula xml:id="formula_22">@ C @C = n ' i=1 h T i (h i C s i M s i M i ) + ( MC T H) T M.<label>(12)</label></formula><p>Update H whilexing M, M i , and C: By removing terms that are irrelevant to H, we obtain the following optimization problem:</p><formula xml:id="formula_23">min H ||SM HC|| 2 F + ||H MC T || 2 F tr (H T (S B)H), s.t. tr (HH T ) = n,<label>(13)</label></formula><p>where Bij =</p><formula xml:id="formula_24">d i d j 2 |E | .</formula><p>Consider that H is an indicator matrix, the constraint makes the problem in Eq.( <ref type="formula" target="#formula_23">13</ref>) NP-complete, which is extremely dicult to solve. In order to cope with the problem, we relax the constraint to orthogonality H T H = I and nonnegativity H 0 and reformulate the objective function as follows:</p><formula xml:id="formula_25">H = tr (H T SH) + tr (H T BH)<label>(14)</label></formula><formula xml:id="formula_26">+ ||SM HC|| 2 F + ||H MC T || 2 F + ||H T H I|| 2 F</formula><p>, where &gt; 0 should be a large number to guarantee the orthogonal constraint to be satised, and we set it as 10 8 in this work. We then utilize the property that ||X||<ref type="foot" target="#foot_1">2</ref> F = tr (X T X) to reformulate the loss function as follows:</p><formula xml:id="formula_27">H = tr (H T SH) + tr (H T BH)<label>(15)</label></formula><p>+ tr (SMM T S T + HCC T H T 2SMC T H T )</p><formula xml:id="formula_28">+ tr (HH T + MC T C MT 2HC MT ) + tr (H T HH T H 2H T H + I) + tr (ΘH T ),</formula><p>where Θ = [ i j ] is a Lagrange multiplier matrix to impose the nonnegative constraint. Set the derivative of @ H @H to 0, we have:</p><formula xml:id="formula_29">Θ = 2SH 2 BH 2CC T H T + 2SMC T (16) 2 H T + 2 C MT 4 HH T H + 4 H.</formula><p>Following the Karush-Kuhn-Tucker (KKT) condition for the nonnegativity, we have the equation as follows:</p><formula xml:id="formula_30">(2SH 2 BH 2CC T H T + 2SMC T 2 H T (17) +2 C MT 4 HH T H + 4 H) i j H i j = i j H i j = 0,</formula><p>which is the xed point equation that the solution must satisfy at convergence. The update rule for H can be written as follows:</p><formula xml:id="formula_31">H = H s 2 BH + p 8 HH T H ,<label>(18)</label></formula><p>where is dened as:</p><formula xml:id="formula_32">= 2 ( BH) ( BH) + 16 (HH T H)<label>(19)</label></formula><formula xml:id="formula_33">(2SH 2CC T H T + 2SMC T 2 H T + 2 C MT + 4 H).</formula><p>The convergence of Eq.( <ref type="formula" target="#formula_32">19</ref>) can be proven as an instance of nonnegative matrix factorization (NMF) problem <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Time complexity</head><p>TraceMiner consists of two components, LSTM-RNNs and the embedding method. Though LSTM-RNNs take O(|E| + |V |)-time for backpropagations, the scalability can be easily increased with deep learning software library like Theano 2 , especially when GPU is available.</p><p>Since the number of users is usually far larger than the number of features and number of communities, the embedding method takes O(n 2 )-time. Only matrix multiplication is used in all update rules, so the optimization can be accelerated by utilizing matrix optimization library like OpenBLAS<ref type="foot" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ALGORITHM-TRACEMINER</head><p>In this section, we introduce the detailed procedure of TraceMiner method for network diusion classication. The overall process consists of two steps:</p><p>• Learning embeddings based on network connectivity. In this work, we aim to utilize the social identity of a user to infer the information she spreads. Hence, we learn embeddings from friendships and social community memberships. • Construct a sequence classier with LSTM-RNNs. After we obtain the embeddings of social media users, we consider a social media message as a sequence of its spreaders. We employ LSTM-RNNs to model the sequence, and thenal hidden output are aggregated using softmax to produce a predicted class label.</p><p>The rst step utilizes network structures to embed social media users into space of low dimensionality, which alleviates the data sparsity of utilizing social media users as features. The second step represents user sequences of information diusion, which allows for the classication of propagation pathways. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>In this section, we introduce experiment details to validate the eectiveness of the proposed framework. Through the experiments, we aim to answer two questions:</p><p>• How well can network information be used to classify social messages compared with content information? • How eective are the LSTM-RNNs by integrating with the proposed embedding method?</p><p>Therefore, we test the methods on two dierent classication tasks with real-world datasets and include both content-based and network-based baselines for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Over 200 million posts are posted per day on Twitter <ref type="foot" target="#foot_3">4</ref> and the popularity has made Twitter a testbed for informationltering research. In this work, we aim to collect a large dataset that includes tweets about specic messages. Following <ref type="bibr" target="#b29">[30]</ref>, we leverage Twitter Search API <ref type="foot" target="#foot_4">5</ref> to retrieve tweets of interests by compiling queries with certain topics. We deal with two tasks in this work, standard news classication and fake news detection. News classication is a classical multilabel text categorization problem and existing eorts have mainly focused on the content. We obtain a news dataset which was originally used for content-based classication <ref type="foot" target="#foot_5">6</ref> by selecting news that has at least two posts on Twitter. Queries for Twitter Search API are compiled by words in the title of the corresponding news. Based on the spreaders of news, we try to use TraceMiner to classify the news into four categories: business (b), science and technology (t), entertainment (e), medical (m). Statistics about the dataset are shown in Table <ref type="table" target="#tab_1">2</ref>. We sample 68, 892 pieces of news, which relate to 288, 591 posts with 121, 211 unique users. The ratio of dierent categories is also presented.</p><p>The other task is fake news detection. The openness of social media platforms enables timely information to be spread at a high rate. Meanwhile, it also allows for the rapid creation and dissemination of fake news. Following <ref type="bibr" target="#b29">[30]</ref>, we retrieve tweets related to fake news by compiling queries with a fact-checking website. In this work, we choose Snopes<ref type="foot" target="#foot_6">7</ref> to obtain ground truth, where we collect articles tagged with fake news <ref type="foot" target="#foot_7">8</ref> . In order to obtain non-fake news posts pertaining to the same topic, we extract keywords in regular expressions as queries to retrieve posts. Statistics of the dataset is shown in Table <ref type="table" target="#tab_1">2</ref>. We collect 3, 600 messages with 50% are fake news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head><p>A core contribution of our work is the idea that spreaders of information can be used to predict message categories. Therefore, we try to test the eectiveness of the proposed method comparing with the state-of-the-art content-based approaches. We experiment a variety of approaches, and report the following two which achieve better results.</p><p>• SVM <ref type="bibr" target="#b12">[13]</ref> trains on content information, which isrst preprocessed with Stanford CoreNLP toolkit <ref type="bibr" target="#b23">[24]</ref>. We adopt bigram and trigram features based on results on the validation set. • XGBoost <ref type="bibr" target="#b4">[5]</ref> is an optimized distributed gradient boosting library that implements machine learning algorithms under the Gradient Boosting framework. It has been successfully applied to various problems and competitions. We feed it with the preprocessed content produced by Stanford CoreNLP. XGBoost presents the best results among all content-based algorithms we tested.</p><p>We propose a novel embedding method to cater to TraceMiner. In order to evaluate its eectiveness, we introduce two variants of TraceMiner and present their results for comparison:</p><p>• TM(DeepWalk) is a variant of TraceMiner by adopting the embedding vectors from DeepWalk as input. As discussed earlier, DeepWalk captures proximity between nodes with random walk: nodes that are sampled together with one random walk are forced to preserve the similarity in the latent space. Therefore, DeepWalk does not directly model the rst and second-degree proximity or the community structure. • TM(LINE) is a variant of TraceMiner by adopting the embedding vectors from LINE. LINE models rst and seconddegree proximity while does not consider the community structure between users.</p><p>To test the prediction accuracy in terms of both precision and recall, we adopted the F 1 -measure to evaluate the performance. Since there are multiple labels to be predicted, for each task t, F t 1 can be computed. In order to get the overall performance, werst adopt the Macro-averaged F 1 -measure as:</p><formula xml:id="formula_34">Macro F 1 = 1 |T | ' t 2T F t 1 ,<label>(20)</label></formula><p>where T is the set of all identity labels and F t 1 is the F 1 -measure of task t.</p><p>A possible problem of Macro-F 1 is, since the sizes of dierent categories are dierent, the task with fewer instances may be overemphasized. In order to cope with this problem, we adopted Micro-averaged F 1 -measure. First, we calculate the micro averaged Technical Presentation WSDM'18, February 5-9, 2018, Marina Del Rey, CA, USA </p><formula xml:id="formula_35">Micro precision = #T P #T P + #FP (<label>21</label></formula><formula xml:id="formula_36">)</formula><formula xml:id="formula_37">Micro recall = #T P #T P + #F N ,</formula><p>where #TP is the number of true positives, #FP is the number of false positives and #FN is the number of false negatives. Micro-F 1 is the harmonic average of Micro-precision and Micro-recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head><p>Social Media News Categorization: The performance of dierent methods on Twitter News data with varying training ratio, from 10% to 90%, is illustrated in Table <ref type="table" target="#tab_2">3</ref>. For each experiment, samples are randomly split into training and testing set. We repeat this process 10 times and report the average results. The highest performance under each setting is highlighted in bold face.</p><p>In terms of Micro-F 1 , our proposed model TraceMiner outperforms all the baselines and its variations, TM(DeepWalk), TM(LINE). Diusion-based methods perform better than contentbased methods. XGBoost performs slightly better than SVM. TM(DeepWalk) is the runner-up method for 10%, 20% and 50%, and TM(LINE) is the runner-up for the rest cases. The result shows that when less network data is available, the random walk-based approach produces better embeddings of users; And a more deterministic method constraining on social proximity better apprehends user behaviors when the network information is more complete. TraceMiner achieves the best result for all tasks. By jointly modeling the microscopic and mesoscopic structures, TraceMiner is more robust to data sparsity.</p><p>In terms of Macro-F 1 , XGBoost outperforms SVM for all cases. Similar pattern has again been observed: TM(DeepWalk) outperforms TM(LINE) with less training information, while TM(LINE) outperforms TM(DeepWalk) when the information is more complete. TraceMiner still performs the best among most cases until we increase the training ratio up to 80%. XGBoost and TM(LINE) achieves the best result for 80% and 90%, respectively. Two observations can be made here: with more training information becoming available, 1) the margin between proposed methods and the contentbased methods becomes smaller; and 2) the margin between TraceMiner and its variants TM(LINE) and TM(DeepWalk) becomes smaller. Based on the observations we can draw conclusions that TraceMiner is more useful when less training information is available, and the proposed TraceMiner can well handle scarce data in the early phase of learning when less training information is known. XGBoost gets the best when 80% of information is available. Since text-based categorization is a well-studied problem, and it is easy to solve when rich information is available, TraceMiner will be able to complement those cases that are dicult for content-based approaches to deal with, and such cases are pervasively present in social media mining tasks where content information is insucient and noisy.</p><p>Another observation that again validates our ndings is that TraceMiner performs better in terms of Micro-F 1 . As shown in Eq.( <ref type="formula" target="#formula_34">20</ref>) and ( <ref type="formula" target="#formula_35">21</ref>), in a multi-label classication task, the category with fewer instances is more advantageous for Macro-F 1 . The results show that TraceMiner actually ends up with correctly classifying more instances.</p><p>Fake News Detection: The performance of dierent methods on Twitter fake news data with varying training ratio, from 10% to 90%, is illustrated in Table <ref type="table" target="#tab_3">4</ref>. Since the dataset is balanced, Micro-and Macro-F 1 are the same, so only one set of results are presented. For the content-based approaches, XGBoost consistently outperforms SVM for all cases. For the two variants of TraceMiner, similar patterns are observed: TM(DeepWalk) outperforms TM(LINE) when less training information is available. TM(LINE) outperforms TM(DeepWalk) when more information is available for training. It again proves that random walk-based sampling is more eective for scarce data, and proximity-based regularization better captures data structures with more training information.</p><p>An interesting dierence between the results for fake news and the previous experiment is the larger margin between proposed methods and content-based methods. Unlike posts related to news where the content information is more self-explanatory, content of posts about fake news is less descriptive. Intentional spreaders of fake news may manipulate the content to make it look more similar to non-rumor information. Hence, TraceMiner can be useful for many emerging tasks in social media where adversarial attacks are present, such as detecting rumors and crowdturng. The margin between content-based approaches and TraceMiner becomes smaller when more information is available for training, however, in these emerging tasks, training information is usually time-consuming and labor-intensive to obtain. Another point we would like to discuss is the performance when the training information is very insucient. When 10% of information is available, SVM has an F 1 score of 58% which is slightly better than a random guess, while TraceMiner has an F 1 score of 78%. Although such margin is reduced when more information is available, the optimal performance with very few training information is of crucial signicance for tasks which emphasize on the earliness. For example, detecting fake news at an early stage is way more meaningful than detecting it when 90% percent of its information is known <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40]</ref>. In conclusion, TraceMiner provides an eective method for modeling messages diused in social media with only network information, which provides a complementary tool for emerging tasks that require earliness and/or suers from the scarcity of content information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>This work mainly focuses on classifying social media messages, which is a fundamental problem in social media mining. It can be useful for many classical tasks including social recommendation, personalization and targeted advertising. Accurate categorization of social media content allows for precise ltering of information, which helps alleviate the information overloading. A recent surge for social media platforms is the attacks of disinformation launched by malicious users. Both content and network information has been studied to detect malicious users, such as spammers <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> and crowdturfers <ref type="bibr" target="#b40">[41]</ref>. In terms of network information, traditional approaches usually derive features from the social networks and spreaders of a message. For example, Hu et al. assume that the information spread by similar users tend to share similar properties <ref type="bibr" target="#b37">[38]</ref>, and the network information mainly centers around the user instead of information itself. Our work distances from the existing work by directly studying the network information.</p><p>Our work is also related to network structure mining methods. Neural network models have been applied on network data for tasks such as classication <ref type="bibr" target="#b42">[43]</ref> and clustering <ref type="bibr" target="#b35">[36]</ref>. These existing methods focus on the nodes in the graph, while our work focuses on the network structure itself, which is manifested by the diusion of messages. In addition, unlike existing graph representation methods, our goal is to provide an end-to-end system with prediction results, instead of oering only the embedding vectors. Recent research has been proposed to utilize RNNs for classication in an semi-supervised manner <ref type="bibr" target="#b24">[25]</ref>, which is also related to our work.</p><p>We present a novel graph embedding model, which is related to existing embedding methods and feature selection on networked data <ref type="bibr" target="#b19">[20]</ref>. For example, DeepWalk <ref type="bibr" target="#b28">[29]</ref> links a network embedding problem into a word embedding problem by showing the similar distribution of nodes appearing in random walks and words appearing in sentences. They employ a Skip-Gram model, which was originally proposed for modeling natural languages, to learn embedding of graphs. LINE <ref type="bibr" target="#b33">[34]</ref> aims to preserve the rst-and second-order proximity between nodes, and provides an embedding vector by concatenating results on both levels. Our work focuses on encoding both social proximity and social community information to alleviate the data sparsity, instead of investigating only one of them <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Recent studies also study and utilize network dynamics by observing the change of social networks over time <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>. We focus on a snapshot since the newly established/withdrawn links during the study are very few.</p><p>Our work is related to information diusion. There are various models which are designed to abstract the pattern of information diusion, such as SIR Model <ref type="bibr" target="#b14">[15]</ref>, Tipping Model <ref type="bibr" target="#b3">[4]</ref>, Independent Cascade Model <ref type="bibr" target="#b13">[14]</ref> and Linear Threshold Model <ref type="bibr" target="#b13">[14]</ref>. However, traditional information diusion models abstract the diusion process to estimate the virality of information and ignore the interaction between multiple campaigns, which cannot be directly applied here.</p><p>Our work can be particularly helpful for identifying messages that cannot be easily detected with the content. An emerging problem that has the feature is rumor and fake news detection. For example, supervised learning approaches have been used to detect rumors <ref type="bibr" target="#b39">[40]</ref> and the spreaders <ref type="bibr" target="#b25">[26]</ref>. The proposed process usually consists of two stages: employing a feature engineering approach to distinguish misinformation <ref type="bibr" target="#b41">[42]</ref> from Twitter's normal content stream and utilize a supervised learning approach to train a detector. However, supervised approaches depend on content information, which can be easily manipulated by malicious spreaders. Previous studies have explored how malicious information can be detected from node to node <ref type="bibr" target="#b2">[3]</ref>, however, the proposed systems can only help visualize and track known events and require experts to observe it and make decisions. The process requires certain domain knowledge and expertise, while TraceMiner is an end-to-end method taht directly studies the information diusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we aim to classify messages spread in social networks, which is a fundamental problem for social media mining. We observe that for many emerging tasks, content information is usually insucient or less descriptive, while pervasively available network information is left unused. Therefore, we propose a novel method TraceMiner that classies social media messages with diusion traces in social networks. To address the problem, we propose an end-to-end classication model based on LSTM-RNNs. In order to alleviate the data sparsity, we propose an embedding method that captures both social proximity and community structures.</p><p>Experimental results with real-world datasets show that TraceMiner eectively classies social media messages and is especially useful when content information is insucient.</p><p>Future work can be focused on two aspects. First, we would like to investigate if TraceMiner can be used to facilitate other network mining tasks, like recommendation and link prediction. Second, since content information is also readily available, we will study how content and network information can be jointly utilized.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Frequency of users in social media message traces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Frequency of words in social media message.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The frequency of users appearing in traces of social media messages follows a power-law distribution, which is similar to the distribution of word frequencies in messages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>|Y tr = 0| (|Y tr = 1|) is the number of negative (positive) instances in the training set. Since we aim to work on multi-label classication, the data is naturally imbalanced when we model one of them, introducing the weight helps the model balance the gradient of skewed data. In next subsection, we will introduce how we generate embeddings and the reason behind our choice. Training Algorithm of LSTM-RNNs Input: Labeled sequences and labels X , Y Maximum number of iterations: Max iter Early termination function : EarlyStop() Output: Weights of LSTM-RNNs: W 1: Initialize W randomly with Gaussian distribution, V Loss[Max iter ], i = 0 2: Split X and Y into training and validation set, (X tr , Y tr ) and (X al , Y al ) 3: do 4: Train RNNs with (X tr , Y tr ) for 1 epoch with Eq.(1) 5: Test RNNs with (X al , Y al ) to obtain loss V Loss[i]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average Euclidean distance between nodes with low dimensional representation.</figDesc><table><row><cell>Method</cell><cell cols="3">1 st -degree 2 nd -degree Intra-group</cell></row><row><cell>LINE</cell><cell>5.16</cell><cell>5.00</cell><cell>10.76</cell></row><row><cell>DeepWalk</cell><cell>7.74</cell><cell>7.69</cell><cell>6.04</cell></row><row><cell>SocDim</cell><cell>6.87</cell><cell>6.12</cell><cell>4.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the datasest used in this study.</figDesc><table><row><cell></cell><cell>Messages</cell><cell>Posts</cell><cell>Unique Users</cell><cell>Class Ratio</cell></row><row><cell>Real News</cell><cell>68,892</cell><cell>288,591</cell><cell>121,211</cell><cell>0.27(b):0.25(t):0.37(e):0.11(m)</cell></row><row><cell>Fake News</cell><cell>3,600</cell><cell>17,613</cell><cell>9,153</cell><cell>0.5:0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The F 1 -measure of dierent methods on the task of social media news categorization.</figDesc><table><row><cell></cell><cell>Training Ratio</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row><row><cell></cell><cell>SVM</cell><cell cols="9">0.6967 0.7138 0.7447 0.7577 0.7988 0.8096 0.8499 0.8787 0.8996</cell></row><row><cell></cell><cell>XGBoost</cell><cell cols="9">0.7121 0.7349 0.7512 0.7794 0.8248 0.8250 0.8638 0.8951 0.9047</cell></row><row><cell>Micro-F 1 (%)</cell><cell cols="10">TM(DeepWalk) 0.7895 0.8081 0.8149 0.8374 0.8569 0.8627 0.8852 0.8917 0.9184</cell></row><row><cell></cell><cell>TM(LINE)</cell><cell cols="9">0.7691 0.7926 0.8163 0.8379 0.8467 0.8744 0.8980 0.9106 0.9253</cell></row><row><cell></cell><cell>TraceMiner</cell><cell cols="9">0.8275 0.8460 0.8658 0.8835 0.8885 0.9141 0.9218 0.9357 0.9380</cell></row><row><cell></cell><cell>SVM</cell><cell cols="9">0.6988 0.7260 0.7425 0.7754 0.7665 0.7872 0.8118 0.8314 0.8722</cell></row><row><cell></cell><cell>XGBoost</cell><cell cols="9">0.7305 0.7438 0.7857 0.7887 0.8144 0.8344 0.8726 0.8941 0.9044</cell></row><row><cell>Macro-F 1 (%)</cell><cell cols="10">TM(DeepWalk) 0.7746 0.8010 0.8156 0.8313 0.8377 0.8611 0.8646 0.8734 0.8839</cell></row><row><cell></cell><cell>TM(LINE)</cell><cell cols="9">0.7561 0.7895 0.8019 0.8138 0.8235 0.8568 0.8775 0.8896 0.9153</cell></row><row><cell></cell><cell>TraceMiner</cell><cell cols="9">0.8181 0.8347 0.8359 0.8549 0.8635 0.8788 0.8779 0.8882 0.9064</cell></row><row><cell>precision and recall:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The F 1 -measure of dierent methods on the task of fake news detection.</figDesc><table><row><cell>Training Ratio</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row><row><cell>SVM</cell><cell cols="9">0.5825 0.5779 0.6122 0.6194 0.6658 0.7114 0.7224 0.7252 0.7581</cell></row><row><cell>XGBoost</cell><cell cols="9">0.6558 0.7004 0.7002 0.7153 0.7288 0.7703 0.7984 0.8115 0.8226</cell></row><row><cell cols="10">TM(DeepWalk) 0.7804 0.7810 0.8078 0.8264 0.8194 0.8491 0.8542 0.8738 0.8894</cell></row><row><cell>TM(LINE)</cell><cell cols="9">0.7542 0.7547 0.7913 0.8015 0.8083 0.8485 0.8733 0.8936 0.8971</cell></row><row><cell>TraceMiner</cell><cell cols="9">0.7867 0.7935 0.8344 0.8459 0.8547 0.8751 0.8988 0.9089 0.9124</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.journalism.org/2016/05/26/news-use-across-social-media-platforms-2016/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://deeplearning.net/software/theano/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.openblas.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://blog.twitter.com/2011/200-million-tweets-per-day</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://dev.twitter.com/rest/public/search</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://archive.ics.uci.edu/ml/datasets/News+Aggregator</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>http://www.snopes.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://www.snopes.com/tag/fake-news/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported, in part, by the Oce of Naval Research grants N000141410095 and N000141310835.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Provenance data in social media</title>
		<author>
			<persName><forename type="first">Georey</forename><surname>Barbier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pritam</forename><surname>Gundecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="84" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName><forename type="first">Jean-Loup</forename><surname>Vincent D Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renaud</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">10008</biblScope>
			<date type="published" when="2008">2008. 2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TargetVue: Visual analysis of anomalous user behaviors in online communication systems</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conglei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Ru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Yung</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="280" to="289" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The spread of behavior in an online social network experiment</title>
		<author>
			<persName><forename type="first">Damon</forename><surname>Centola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">329</biblScope>
			<biblScope unit="page" from="1194" to="1197" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On Cayley&apos;s formula for counting trees</title>
		<author>
			<persName><surname>Le Clarke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the London Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="471" to="474" />
			<date type="published" when="1958">1958. 1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The spreading of misinformation online</title>
		<author>
			<persName><forename type="first">Michela</forename><surname>Del Vicario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bessi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabiana</forename><surname>Zollo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Scala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Caldarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Quattrociocchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="554" to="559" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inferring networks of diusion and inuence</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Gomez Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">BotSnier: Detecting botnet command and control channels in network trac</title>
		<author>
			<persName><forename type="first">Guofei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenke</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Network lasso: Clustering and optimization in large graphs</title>
		<author>
			<persName><forename type="first">David</forename><surname>Hallac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text categorization with support vector machines: Learning with many relevant features</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="137" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Maximizing the spread of inuence through a social network</title>
		<author>
			<persName><forename type="first">David</forename><surname>Kempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éva</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the ninth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A contribution to the mathematical theory of epidemics</title>
		<author>
			<persName><forename type="first">O</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anderson</forename><forename type="middle">G</forename><surname>Kermack</surname></persName>
		</author>
		<author>
			<persName><surname>Mckendrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="700" to="721" />
			<date type="published" when="1927">1927</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional neural networks for sentence classication</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><forename type="middle">U</forename><surname>Laumann</surname></persName>
		</author>
		<author>
			<persName><surname>Pappi</surname></persName>
		</author>
		<title level="m">Networks of collective action: A perspective on community inuence systems</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung</forename><surname>Sebastian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="556" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Dani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01860</idno>
		<title level="m">Attributed Network Embedding for Learning in a Dynamic Environment</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust unsupervised feature selection on networked data</title>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 SIAM International Conference on Data Mining</title>
		<meeting>the 2016 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Streaming Link Prediction on Dynamic Attributed Networks</title>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Kewei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 11th ACM International Conference on Web Search and Data Mining</title>
		<meeting>11th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised Personalized Feature Selection</title>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Harsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd AAAI Conference on Articial Intelligence. AAAI</title>
		<meeting>The 32nd AAAI Conference on Articial Intelligence. AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Toward personalized relational learning</title>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Osmar R Zaïane</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 SIAM International Conference on Data Mining</title>
		<meeting>the 2017 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="444" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><surname>Mcclosky</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P/P14/P14-5010" />
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations. 55-60</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>The Stanford CoreNLP Natural Language Processing Toolkit</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep collective inference</title>
		<author>
			<persName><forename type="first">John</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Articial Intelligence</title>
		<meeting>the 31st AAAI Conference on Articial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A new approach to bot detection: Striking the balance between precision and recall</title>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tahora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">M</forename><surname>Nazer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Carley</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Social Networks Analysis and Mining (ASONAM)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="533" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Numerical optimization</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval</title>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinying</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rabab</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="694" to="707" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rumor has it: Identifying misinformation in microblogs</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vahed Qazvinian</surname></persName>
		</author>
		<author>
			<persName><surname>Rosengren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Dragomir R Radev</surname></persName>
		</author>
		<author>
			<persName><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1589" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Leveraging the implicit structure within social media for emergent rumor detection</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2377" to="2382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fake News Detection on Social Media: A Data Mining Perspective</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Sliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="22" to="36" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page">1642</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Deep Representations for Graph Clustering</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Fei Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Relational learning with social status analysis</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Ninth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="513" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive Spammer Detection with Sparse Group Modeling</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detecting Camouaged Content Polluters</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="696" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gleaning wisdom from the past: Early detection of emerging rumors in social media</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 SIAM International Conference on Data Mining</title>
		<meeting>the 2017 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="99" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Detecting Crowdturng in Social Media</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mining misinformation in social media</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data in Complex and Social Networks</title>
		<imprint>
			<biblScope unit="page" from="123" to="152" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Community detection in networks with node attributes</title>
		<author>
			<persName><forename type="first">Jaewon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1151" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
