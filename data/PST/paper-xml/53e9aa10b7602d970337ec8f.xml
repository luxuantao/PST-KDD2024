<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">User preferences based software defect detection algorithms selection using MCDM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-04-24">24 April 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yi</forename><surname>Peng</surname></persName>
							<email>pengyi@uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Management and Economics</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>610054</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoxun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Management and Economics</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>610054</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Honggang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Dartmouth</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">User preferences based software defect detection algorithms selection using MCDM</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-04-24">24 April 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">19DFB9D8DFCCA567E220A88B1EE10B7E</idno>
					<idno type="DOI">10.1016/j.ins.2010.04.019</idno>
					<note type="submission">Received 10 January 2010 Received in revised form 12 April 2010 Accepted 17 April 2010</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Algorithm selection Classification algorithm Knowledge-driven data mining Multi-criteria decision making (MCDM) Software defect detection</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A variety of classification algorithms for software defect detection have been developed over the years. How to select an appropriate classifier for a given task is an important issue in Data mining and knowledge discovery (DMKD). Many studies have compared different types of classification algorithms and the performances of these algorithms may vary using different performance measures and under different circumstances. Since the algorithm selection task needs to examine several criteria, such as accuracy, computational time, and misclassification rate, it can be modeled as a multiple criteria decision making (MCDM) problem. The goal of this paper is to use a set of MCDM methods to rank classification algorithms, with empirical results based on the software defect detection datasets. Since the preferences of the decision maker (DM) play an important role in algorithm evaluation and selection, this paper involved the DM during the ranking procedure by assigning user weights to the performance measures. Four MCDM methods are examined using 38 classification algorithms and 13 evaluation criteria over 10 public-domain software defect datasets. The results indicate that the boosting of CART and the boosting of C4.5 decision tree are ranked as the most appropriate algorithms for software defect datasets. Though the MCDM methods provide some conflicting results for the selected software defect datasets, they agree on most top-ranked classification algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Data mining and knowledge discovery (DMKD) has made remarkable progress during the past three decades <ref type="bibr" target="#b54">[54]</ref>. It utilizes methods, algorithms, and techniques from many disciplines, including statistics, databases, machine learning, pattern recognition, artificial intelligence, data visualization, and optimization <ref type="bibr" target="#b31">[31]</ref>. One of the major tasks in DMKD is classification. Researchers in a variety of fields have created a large number of classification algorithms, such as decision tree, neural networks, Bayesian network, linear logistic regression, Naı ¨ve Bayes, and K-nearest-neighbor. How to select the most appropriate algorithms for a given task is an important issue in DMKD.</p><p>The algorithm selection problem is actually a central issue in many fields, including Artificial Intelligence, Operations Research, and Machine Learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b64">64]</ref>. As early as 1976, <ref type="bibr" target="#b58">Rice (1976)</ref> formalized the problem of algorithm selection as abstract models with five ingredients: the problem space, the feature space, the criteria space, the algorithm space, and the performance measures <ref type="bibr" target="#b58">[58]</ref>. The machine learning community presents the algorithm selection as a learning problem and focuses on the classification algorithm selection problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b61">61]</ref>. Berrer included user preferences in learning algorithm ranking schemes <ref type="bibr" target="#b5">[6]</ref>. <ref type="bibr" target="#b50">Nakhaeizadeh and Schnabl (1997)</ref> proposed data envelopment analysis (DEA) approach to take both positive and negative properties of data mining algorithms into consideration when ranking classification algorithms <ref type="bibr" target="#b50">[50]</ref>. <ref type="bibr" target="#b59">Rokach (2009)</ref> suggests that the algorithm selection can be considered as a multiple criteria decision making (MCDM) problem and encourages researchers to utilize MCDM methods to systematically choose the appropriate algorithm <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b59">59]</ref>.</p><p>As Wolpert and Macready pointed out in their No Free Lunch (NFL) theorem that there exists no single algorithm that could achieve the best performance for all measures for a given problem domain <ref type="bibr" target="#b70">[70]</ref>. Thus, a list of classification algorithm ranking is more useful than providing the best performed algorithm for a particular task <ref type="bibr" target="#b5">[6]</ref>. In addition, the preferences of users play an important role in algorithms evaluation and selection. One way to get users involved in the algorithm selection procedure is to allow them to assign priorities to performance measures. Since MCDM methods can satisfy both requirements, they have great potential in the area of ranking classification algorithms.</p><p>Inspired by these previous works, this paper introduces a set of MCDM methods to rank classification algorithms for software defect prediction. As an useful software testing tool, software defect prediction can help detect software faults in an early stage, which facilitates efficient test resource allocation, improves software architecture design, and reduces the number of defective modules <ref type="bibr" target="#b44">[44]</ref>. Software defect prediction can be modeled as a two-group classification problem by categorizing software units as either fault-prone (fp) or non-fault-prone (nfp) using historical data. Researchers have developed many classification models for software defect prediction (for example, see <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b56">56]</ref>).</p><p>This paper conducts an empirical study to evaluate a selection of classification algorithms using 13 performance measures over 10 public-domain datasets from the NASA Metrics Data Program (MDP) repository <ref type="bibr" target="#b19">[19]</ref>. The classification results are then analyzed using four MCDM methods in order to rank the classifiers for software defect prediction task.</p><p>The rest of this paper is organized as follows: Sections 2 and 3 briefly describe the selected classification algorithms and MCDM methods, respectively. Section 4 presents details of the experimental study and analyzes the results; Section 5 summarizes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Classification algorithms</head><p>This section gives a short description of classification algorithms and ensemble learning algorithms used in the experimental study. Twelve classifiers which represent five categories of classifiers (i.e., trees, functions, Bayesian classifiers, lazy classifiers, and rules) and four ensemble learning algorithms were implemented in WEKA <ref type="bibr" target="#b68">[68]</ref>.</p><p>For trees category, we chose classification and regression tree (CART) <ref type="bibr" target="#b11">[12]</ref>, Naı ¨ve Bayes tree <ref type="bibr" target="#b41">[41]</ref>, and C4.5 <ref type="bibr" target="#b57">[57]</ref>. Functions category includes linear logistic regression <ref type="bibr" target="#b14">[15]</ref>, radial basis function (RBF) network <ref type="bibr" target="#b6">[7]</ref>, sequential minimal optimization (SMO) <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b65">65]</ref>, and Neural Networks <ref type="bibr" target="#b68">[68]</ref>. Bayesian classifiers category includes Bayesian network <ref type="bibr" target="#b66">[66]</ref> and Naı ¨ve Bayes <ref type="bibr" target="#b28">[28]</ref>. K-nearest-neighbor (KNN) was chosen to represent lazy classifiers <ref type="bibr" target="#b24">[24]</ref>. For rules category, decision table and Repeated Incremental Pruning to Produce Error Reduction (RIPPER) rule induction were selected <ref type="bibr" target="#b22">[22]</ref>.</p><p>Ensemble learning algorithms construct a set of classifiers and then combine the results of these classifiers using some mechanisms to classify new data records <ref type="bibr" target="#b26">[26]</ref>. Experimental results have shown that ensembles are often more accurate, more robust to the effects of noisy data, and achieve lower average error rate than any of the constituent classifiers <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b67">67]</ref>. Thus this paper includes four ensemble methods (bagging, boosting, stacking, and voting) in the experimental study.</p><p>Bagging combines multiple outputs of a learning algorithm by taking a plurality vote to get an aggregated single prediction <ref type="bibr" target="#b10">[11]</ref>. In this study, bagging is generated by averaging probability estimates <ref type="bibr" target="#b68">[68]</ref>. Similar to bagging, boosting method also combines the different decisions of a learning algorithm to produce an aggregated prediction <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b62">62]</ref>. In boosting, however, weights of training instances change in each iteration to force learning algorithms to put more emphasis on instances that were predicted incorrectly previously and less emphasis on instances that were predicted correctly previously <ref type="bibr" target="#b27">[27]</ref>. This study chooses the adaptive boosting (AdaBoost) algorithm in the experiment <ref type="bibr" target="#b34">[34]</ref>.</p><p>In addition to bagging and boosting, two other meta-learning methods: stacking and voting are included in the experiment. Stacking generalization is a scheme for minimizing the generalization error rate of one or more learning algorithms and can combine different types of learning algorithms <ref type="bibr" target="#b68">[68,</ref><ref type="bibr" target="#b69">69]</ref>. Voting is a simple average of multiple classifiers probability estimates provided by WEKA <ref type="bibr" target="#b68">[68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MCDM methods</head><p>Ranking of classification algorithms normally need to examine several criteria, such as accuracy, computational time, and misclassification rate. Therefore algorithm selection can be modeled as multiple criteria decision making (MCDM) problems <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b59">59]</ref>. As mentioned heretofore, algorithm ranking is a useful strategy for choosing the appropriate classifier and the preferences of users are important in algorithm ranking <ref type="bibr" target="#b5">[6]</ref>. Some existing MCDM methods are able to rank classifiers based on multiple performance measures and take the preferences of users into the ranking process. This section introduces four MCDM methods, i.e., DEA, TOPSIS, ELECTRE, and PROMETHEE, and explains how they can be used to rank classification algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data Envelopment Analysis (DEA)</head><p>Charnes, Cooper, and Rhodes developed data envelopment analysis (DEA) to evaluate the efficiency of decision making units (DMUs) through identifying the efficiency frontier and comparing each DMU with the frontier <ref type="bibr" target="#b20">[20]</ref>. Since DEA is able to estimate efficiency with minimal prior assumptions <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b45">45]</ref>, it has a comparative advantage to approaches that require a priori assumptions, such as standard forms of statistical regression analysis <ref type="bibr" target="#b21">[21]</ref>. During the past thirty years, various DEA extensions and models have been developed and established themselves as powerful analytical tools <ref type="bibr" target="#b23">[23]</ref>.</p><p>The original DEA model presented by Charnes, Cooper, and Rhodes <ref type="bibr" target="#b20">[20]</ref> is called ''CCR ratio model", which uses the ratio of outputs to inputs to measure the efficiency of DMUs. Assume that there are n DMUs with m inputs to produce s outputs. x ij and y rj represent the amount of input i and output r for DMU j (j = 1, 2, . . ., n), respectively. Then the ratio-form of DEA can be represented as:</p><formula xml:id="formula_0">max h 0 ðu; vÞ ¼ X r u r y ro = X i v i x io ;</formula><p>subject to X r u r y rj = X i v i x ij 6 1 for j ¼ 1; . . . ; n; u r ; v i P 0 for all i and r:</p><p>where the u r 's and the v i 's are the variables and the y ro 's and x io 's are the observed output and input values of the DMU to be evaluated (i.e., DMU o ), respectively <ref type="bibr" target="#b23">[23]</ref>. The equivalent linear programming problem using the Charnes-Cooper transformation is <ref type="bibr" target="#b23">[23]</ref>:</p><formula xml:id="formula_1">max z ¼ X s r¼1 l r y ro ; subject to X s r¼1 l r y rj À X m i¼1 m i x ij 6 0; X m i¼1 m i x io ¼ 1;</formula><p>l r ; m i P 0:</p><p>Banker, Charnes, and Cooper introduced the BCC model by adding a constraint P n j¼1 k j ¼ 1 to the CCR model <ref type="bibr" target="#b4">[5]</ref>. These models can be solved using the simplex method for each DMUs. DMUs with value of 1 are efficient and others are inefficient.</p><p>Nakhaeizadeh and Schnabl proposed to use DEA approach in data mining algorithms selection <ref type="bibr" target="#b50">[50]</ref>. They argued that in order to make an objective evaluation of data mining algorithms that all the available positive and negative properties of algorithms are important and DEA models are able to take both aspects into consideration. Positive and negative properties of data mining algorithms can be considered as output and input components in DEA, respectively. For example, the overall accuracy rate of a classification algorithm is an output component and the computation time of an algorithm is an input component. Using existing DEA models, it is possible to give a comprehensive evaluation of data mining algorithms.</p><p>In this paper, the CCR and BCC models are utilized to rank a wide selection of classification algorithms. In the experiment, input components include mean absolute error, false positive rate, false negative rate, training time, and test time. Output components include the area under receiver operating characteristic (AUC), overall accuracy, F-measure, Kappa statistic, true positive rate, true negative rate, precision, and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Technique for Order Preference by Similarity to Ideal Solution (TOPSIS)</head><p>Technique for order preference by similarity to ideal solution (TOPSIS) was initially developed by <ref type="bibr" target="#b38">Hwang and Yoon (1981)</ref> to rank alternatives over multiple criteria <ref type="bibr" target="#b38">[38]</ref>. TOPSIS finds the best alternatives by minimizing the distance to the ideal solution and maximizing the distance to the nadir or negative-ideal solution <ref type="bibr" target="#b51">[51]</ref>.</p><p>Since its first introduction, a number of extensions and variations of TOPSIS have been developed over the years <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">40]</ref>. In the experimental study of this paper, the following TOPSIS procedure adopted from Opricovic and Tzeng and Olsonwas used <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b52">52]</ref>:</p><p>Step 1: calculate the normalized decision matrix. The normalized value r ij is calculated as:</p><formula xml:id="formula_2">r ij ¼ x ij = ffiffiffiffiffiffiffiffiffiffiffiffi ffi X J j¼1 x 2 ij v u u t ; j ¼ 1; . . . ; J; i ¼ 1; . . . ; n:</formula><p>where J and n denote the number of alternatives and the number of criteria, respectively. For alternative A j , the performance measure of the ith criterion C i is represented by x ij .</p><p>Step 2: develop a set of weights w i for each criterion and calculate the weighted normalized decision matrix. The weighted normalized value v ij is calculated as:</p><formula xml:id="formula_3">v ij ¼ w i x ij ; j ¼ 1; . . . ; J; i ¼ 1; . . . ; n:</formula><p>where w i is the weight of the ith criterion, and</p><formula xml:id="formula_4">P n i¼1 w i ¼ 1.</formula><p>Step 3: find the ideal alternative solution S + , which is calculated as:</p><formula xml:id="formula_5">S þ ¼ v þ 1 ; . . . ; v þ n È É ¼ max j v ij ji 2 I 0 À Á ; min j v ij ji 2 I 00 À Á È É ;</formula><p>where I 0 is associated with benefit criteria and I 00 is associated with cost criteria.</p><p>Step 4: find the negative-ideal alternative solution S À , which is calculated as:</p><formula xml:id="formula_6">S À ¼ v À 1 ; . . . ; v À n È É ¼ ðmin j v ij ji 2 I 0 Þ; ðmax j v ij ji 2 I 00 Þ È É ;</formula><p>Step 5: calculate the separation measures, using the n-dimensional Euclidean distance. The separation of each alternative from the ideal solution is calculated as:</p><formula xml:id="formula_7">D þ j ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi X n i¼1 v ij À v þ i À Á 2 ; v u u t j ¼ 1; . . . ; J:</formula><p>The separation of each alternative from the negative-ideal solution is calculated as:</p><formula xml:id="formula_8">D À j ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi X n i¼1 v ij À v À i À Á 2 ; v u u t j ¼ 1;</formula><p>. . . ; J:</p><p>Step 6: calculate a ratio R þ j that measures the relative closeness to the ideal solution and is calculated as:</p><formula xml:id="formula_9">R þ j ¼ D À j = D þ j þ D À j ; j ¼ 1;</formula><p>. . . ; J:</p><p>Step 7: rank alternatives by maximizing the ratio in Step 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Elimination and choice expressing reality (ELECTRE)</head><p>ELECTRE stands for ELimination et Choix TRaduisant la REalité (ELimination and Choice Expressing the REality) and was first proposed by Roy <ref type="bibr" target="#b60">[60]</ref> to choose the best alternative from a collection of alternatives. Over the last four decades, a family of ELECTRE methods has been developed, including ELECTRE I, ELECTRE II, ELECTRE III, ELECTRE IV, ELECTRE IS, and ELECTRE TRI.</p><p>There are two main steps of ELECTRE methods: the first step is the construction of one or several outranking relations; the second step is an exploitation procedure that identifies the best compromise alternative based on the outranking relation obtained in the first step <ref type="bibr" target="#b33">[33]</ref>. This paper uses ELECTRE I in the experimental study following the solution procedure, which was summarized by Milani <ref type="bibr" target="#b48">[48]</ref>:</p><p>Step 1: define a concordance and discordance index set for each pair of alternatives A j and A k , j, k = 1,. . ., m;ik Step 5: complete the two tests for all pairs of alternatives. The preferred alternatives are those outrank more being outranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Preference Ranking Organisation Method for Enrichment of Evaluations (PROMETHEE)</head><p>Brans (1982) proposed the PROMETHEE I and PROMETHEE II in 1982 <ref type="bibr" target="#b7">[8]</ref>. The PROMETHEE methods use pairwise comparisons and outranking relationships to choose the best alternatives. Specifically, they compute positive and negative preference flows for each alternative and help the DM to make final selection. The positive preference flow indicates how an alternative is outranking all the other alternatives and the negative preference flow indicates how an alternative is outranked by all the other alternatives <ref type="bibr" target="#b9">[10]</ref>. While PROMETHEE I obtains partial ranking, PROMETHEE II provides a complete ranking. The ranking generated by PROMETHEE I is partial because it does not compare conflicting actions <ref type="bibr" target="#b8">[9]</ref>. On the other hand, PROMETHEE II ranks alternatives according to the net flow which equals to the balance of the positive and the negative preference flows. The higher the net flow, the better the alternative <ref type="bibr" target="#b9">[10]</ref>. Since the purpose of this paper is to build a ranking of classification algorithms, PROMETHEE II is selected. The PROMETHEE II procedure described by Brans and Mareschal was used in the experimental study <ref type="bibr" target="#b9">[10]</ref>:</p><p>Step 1: define aggregated preference indices. where A is a finite set of possible alternatives {a 1 , a 2 , . . ., a n }, k represents the number of evaluation criteria and w j is the weight of each criterion. Arbitrary numbers for the weights can be assigned by the DM. The weights are then normalized to ensure that P k j¼1 w j ¼ 1:pða; bÞ indicates how a is preferred to b and p(b, a) indicates how b is preferred to a. P j (a, b)</p><p>and P j (b, a) are the preference functions for alternatives a and b.</p><p>Step 2: calculate p(a, b) and p(b, a) for each pair of alternatives of A.</p><p>Step 3: define the positive and the negative outranking flow as follows:</p><p>The positive outranking flow:</p><formula xml:id="formula_10">/ þ ðaÞ ¼ 1 n À 1 X x2A pða; xÞ:</formula><p>The negative outranking flow:</p><formula xml:id="formula_11">/ À ðaÞ ¼ 1 n À 1 X x2A pðx; aÞ;</formula><p>Step 4: compute the net outranking flow for each alternative as follows:</p><p>/ðaÞ ¼ / þ ðaÞ À / À ðaÞ:</p><p>When /(a) &gt; 0, a is more outranking all the alternatives on all the evaluation criteria. When /(a) &lt; 0, a is more outranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental study</head><p>The experiment is designed to rank classification algorithms using the four MCDM methods described in the previous section in the application domain of software defect prediction. The following subsections describe the performance measures, data sources, experimental design, and the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance measures</head><p>There are an extensive number of performance measures for classification. Commonly used performance measures in software defect classification are accuracy, precision, recall, F-measure, the area under receiver operating characteristic (AUC), and mean absolute error <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b46">46]</ref>. Besides these popular measures, this work includes seven other classification measures. The following paragraphs briefly describe these measures.</p><p>Overall accuracy: Accuracy is the percentage of correctly classified modules <ref type="bibr" target="#b36">[36]</ref>. It is one the most widely used classification performance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overall accuracy</head><formula xml:id="formula_12">¼ TN þ TP TP þ FP þ FN þ TN :</formula><p>True positive (TP): TP is the number of correctly classified fault-prone modules. TP rate measures how well a classifier can recognize fault-prone modules. It is also called sensitivity measure.</p><p>True positive rate=sensitivity ¼ TP TP þ FN :</p><p>False positive (FP): FP is the number of non-fault-prone modules that is misclassified as fault-prone class. FP rate measures the percentage of non-fault-prone modules that were incorrectly classified.</p><p>False positive rate ¼ FP FP þ TN :</p><p>True negative (TN): TN is the number of correctly classified non-fault-prone modules. TN rate measures how well a classifier can recognize non-fault-prone modules. It is also called specificity measure.</p><p>True negative rate=specificity ¼ TN TN þ FP :</p><p>False negative (FN): FN is the number of fault-prone modules that is misclassified as non-fault-prone class. FN rate measures the percentage of fault-prone modules that were incorrectly classified.</p><p>False negative rate ¼ FN FN þ TP :</p><p>Precision: This is the number of classified fault-prone modules that actually are fault-prone modules.</p><p>Precision ¼ TP TP þ FP :</p><p>Recall: This is the percentage of fault-prone modules that are correctly classified.</p><p>Recall ¼ TP TP þ FN :</p><p>F-measure: It is the harmonic mean of precision and recall. F-measure has been widely used in information retrieval <ref type="bibr" target="#b3">[4]</ref>.</p><formula xml:id="formula_13">F-measure ¼ 2 Â Precision Â Recall Precision þ Recall :</formula><p>AUC: ROC stands for Receiver Operating Characteristic, which shows the tradeoff between TP rate and FP rate <ref type="bibr" target="#b36">[36]</ref>. AUC represents the accuracy of a classifier. The larger the area, the better the classifier. Kappa statistic (KapS): This is a classifier performance measure that estimates the similarity between the members of an ensemble in multi-classifiers systems <ref type="bibr" target="#b43">[43]</ref>.</p><p>KapS ¼ PðAÞ À PðEÞ 1 À PðEÞ :</p><formula xml:id="formula_14">P(A)</formula><p>is the accuracy of the classifier and P(E) is the probability that agreement among classifiers is due to chance.</p><formula xml:id="formula_15">PðEÞ ¼ P c k¼1 P c j¼1 P m i¼1 f ði; kÞCði; jÞ h i Á P c j¼1 P m i¼1 f ði; jÞCði; kÞ h i m 2 ;</formula><p>m is the number of modules andc is the number of classes. f(i, j) is the actual probability of i module to be of class j. P m i¼1 f ði; jÞ is the number of modules of class j. Given threshold h, C h (i, j) is 1 if j is the predicted class for i obtained from P(i, j); otherwise it is 0 <ref type="bibr" target="#b32">[32]</ref>. Mean absolute error (MAE): This measures how much the predictions deviate from the true probability. P(i, j) is the estimated probability of i module to be of class j taking values in In two-class software defect prediction, modules with non-empty defect_id are labeled as fp and modules with empty defect_id are labeled as nfp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental design</head><p>The experiment was carried out according to the following process: Input: a software defect dataset Output: Ranking of classifiers Step 1: Prepare target datasets: select and transform relevant features; data cleaning; data integration.</p><p>Step 2: Train and test the selected classification models on a randomly sampled partitions (i.e., 10-fold cross-validation) using WEKA 3.7 <ref type="bibr" target="#b68">[68]</ref>.</p><p>Step 3: Evaluate classification models using DEA, TOPSIS, ELECTRE I, and PROMETHEE II. All the MCDM methods are implemented using MATLAB.</p><p>Step 4: Generate four separate tables of the final ranking of classifiers provided by each MCDM method. END Note that each of bagging and boosting is applied to the twelve individual classifiers to generate twelve ensembled outputs, while each of stacking and voting is applied to the twelve classifiers to produce one prediction. Therefore there are total of thirty-eight classifiers.</p><p>Among the four MCDM methods used in the experiment, TOPSIS, ELECTRE I, and PROMETHEE II can take user preferences into the ranking procedure by assigning weights to criteria or performance measures. For the DEA method, the weights are determined for each classification algorithm during the computation of LP problem <ref type="bibr" target="#b50">[50]</ref>.</p><p>For TOPISIS, ELECTRE I, and PROMETHEE II, weights for each criterion or performance measure are defined according to the results from previous research and use the scale ranges from 1 to 9 with increasing importance. Number 1, 3, 5, 7 and 9 represent equal, moderate, strong, very strong, and extreme importance, respectively; while 2, 4, 6 and 8 indicate intermediate values <ref type="bibr" target="#b71">[71]</ref>. AUC is assigned a weight of 9 to indicate that it is extremely important since previous studies have proved that it is the most informative and objective measurement of predictive accuracy in software defect prediction <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b44">44]</ref>. Mean absolute error, overall accuracy, and F-measure are assigned a weight of 7. False positive rate, false negative rate, Kappa statistic, true positive rate, true negative rate, precision, and recall are assigned a weight of 5. Training time and test time are assigned a weight of 1. The weights are then normalized and the sum of all weights equal to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion of results</head><p>The ranking of classifiers generated by DEA, TOPSIS, ELECTRE I, and PROMETHEE II are summarized in Table <ref type="table" target="#tab_1">1</ref>-4, respectively. For each classifier listed on the leftmost column, the corresponding output value is presented. The classifiers are named following the formats of WEKA.</p><p>Table <ref type="table" target="#tab_1">1</ref> summarizes the results of the CCR and BCC models for the thirty-eight classifiers. The efficient algorithms identified by CCR and BCC models only differ at two instances: linear logistic regression (functions.Logistic) and bagging of decision table (rules.DecisionTable.Bagging). For inefficient algorithms, both models produce similar results.</p><p>The ranking results produced by the TOPSIS for the software defect datasets are summarized in Table <ref type="table" target="#tab_0">2</ref>. Boosting of CART (trees.SimpleCart.Adaboost), boosting of C4.5 decision tree (trees.J48.Adaboost), and K-nearest neighbours classifier (lazy.IBK) are ranked as the top-three classifiers according to the computed ratio that measures the relative closeness of each classifier to the ideal solution.</p><p>Table <ref type="table">3</ref> summarizes the evaluation results of ELECTRE I. Boosting of CART (trees.SimpleCart.Adaboost), bagging of C4.5 decision tree (trees.J48.Bagging), and boosting of C4.5 decision tree (trees.J48.Adaboost) are preferred classifiers. In experiments, we arbitrary set the threshold of the concordance test to 0.95. However, as all data has been normalized to value between 0 to 1, we arbitrary set the threshold of the discordance test to 0.1.</p><p>Table <ref type="table" target="#tab_2">4</ref> represents the ranking results generated by PROMETHEE II. Boosting of CART (trees.SimpleCart.Adaboost), boosting of C4.5 decision tree (trees.J48.Adaboost), and bagging of C4.5 decision tree (trees.J48.Bagging) have the highest net flows, which are computed as the balance of the positive and the negative preference flows.</p><p>Table <ref type="table" target="#tab_1">1</ref>-4 indicate that DEA, TOPSIS, ELECTRE I, and PROMETHEE II provide similar top-ranked classification algorithms and differ about some classifiers for software defect datasets. For example, they all consider boosting of CART and boosting of C4.5 decision tree as the top two classifiers. However, they have different opinions on Naı ¨ve Bayes (bayes. NaiveBayes) and Bayesian network (bayes.BayesNet). Both classifiers are efficient according to the results of DEA, but they are not efficient in TOPSIS, ELECTRE I, and PROMETHEE II. In fact, both classifiers are ranked quite low in TOPSIS and PROMETHEE II. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion remarks</head><p>Inconsistencies exist in different studies and the performances of learning algorithms may vary using different performance measures and under different circumstances. The selection of an appropriate classification algorithm is an important and difficult task. This paper proposed to use MCDM methods (i.e., DEA, ELECTRE I, TOPSIS, and PROMETHEE II) to evaluate and rank a selection of classification algorithms using a set of performance measures for software defect prediction. Since the preferences of the decision maker (DM) play an important role in algorithm evaluation and selection, this paper involved user's preferences during the ranking procedure by assigning weights to evaluation criteria. It started by introducing the classification algorithms and the MCDM methods. Then the experiment, which used 38 classification algorithms and 13 evaluation criteria over 10 software defect datasets, was described.</p><p>The experimental results indicate that the boosting of CART and the boosting of C4.5 decision tree are ranked as the most appropriate algorithms for software defect datasets. The second observation is that the four MCDM methods generate similar top-ranked classification algorithms while produce different ranking for some classifiers for the selected software defect datasets. The third observation is that TOPSIS and PROMETHEE II may be more appropriate than DEA and ELECTRE I for the given task since they provide a complete ranking of algorithms. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Step 2: calculate a global concordance index C ki and a global discordance index D ki for each pair of alternatives by summing the decision maker's (DM) weights. Step 3: choose a global concordance threshold c and a global discordance threshold d to perform the global concordance and discordance tests. Step 4: test both global concordance and global discordance thresholds: if C ik P c and D ik 6 d; an outranking relation is judged as true;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Let a, b 2 A</head><label>2</label><figDesc>, and let:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>jf ði; jÞ À Pði; jÞj m Á c :Training time: the time needed to train a classification algorithm or ensemble method.Test time: the time needed to test a classification algorithm or ensemble method.4.2. Data sourcesThe data used in this study are 10 public-domain software defect datasets provided by the NASA IV&amp; V Facility Metrics Data Program (MDP) repository. The NASA website gives brief descriptions of each MDP dataset<ref type="bibr" target="#b19">[19]</ref>: CM1: This dataset is from a science instrument written in a C code with approximately 20 kilo-source lines of code (KLOC). It contains 505 modules. JM1: This dataset is a real-time C project containing about 315 KLOC. There are eight years of error data associated with the metrics and has 2012 modules. KC3: This dataset is about the collection, processing and delivery of satellite metadata. It is written in Java with 18 KLOC and has 458 modules. KC4: This dataset is a ground-based subscription server written in Perl code containing of 25 KLOC with 125 modules. MC1: This dataset is about a combustion experiment that is designed to fly on the space shuttle written in C &amp; C++ code containing 63 KLOC. There are 9466 modules. MW1: This dataset is about a zero gravity experiment related to combustion written in C code containing 8 KLOC, with 403 modules. PC1: This dataset is flight software from an earth orbiting satellite that is no longer operational. It contains 40 KLOC of C code with 1107 modules. PC2: This dataset is dynamic simulator for attitude control systems. It contains 26 KLOC of C code with 5589 modules. PC3: This dataset is flight software from an earth orbiting satellite that is currently operational. It has 40 KLOC of C code with 1563 modules. PC4: This dataset is flight software from an earth orbiting satellite that is currently operational. It has 36 KLOC of C code with 1458 modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Results of the TOPSIS approach.</figDesc><table><row><cell>DMU/Classifier</cell><cell>TOPSIS</cell><cell>DMU/Classifier</cell><cell>TOPSIS</cell></row><row><cell>trees.SimpleCart.Adaboost</cell><cell>0.9791</cell><cell>functions.MultilayerPerceptron.Adaboost</cell><cell>0.668286</cell></row><row><cell>trees.J48.Adaboost</cell><cell>0.969652</cell><cell>rules.JRip</cell><cell>0.658249</cell></row><row><cell>lazy.IBk</cell><cell>0.8816</cell><cell>functions.Logistic</cell><cell>0.569089</cell></row><row><cell>rules.JRip.Adaboost</cell><cell>0.871979</cell><cell>functions.Logistic.Bagging</cell><cell>0.567159</cell></row><row><cell>trees.NBTree.Adaboost</cell><cell>0.867601</cell><cell>rules.DecisionTable.Bagging</cell><cell>0.561985</cell></row><row><cell>lazy.IBk.Bagging</cell><cell>0.84845</cell><cell>functions.Logistic.Adaboost</cell><cell>0.558039</cell></row><row><cell>lazy.IBk.Adaboost</cell><cell>0.846795</cell><cell>rules.DecisionTable</cell><cell>0.546139</cell></row><row><cell>trees.J48.Bagging</cell><cell>0.819102</cell><cell>bayes.BayesNet.Adaboost</cell><cell>0.524538</cell></row><row><cell>rules.DecisionTable.Adaboost</cell><cell>0.810651</cell><cell>functions.SMO.Adaboost</cell><cell>0.46479</cell></row><row><cell>meta.Stacking</cell><cell>0.805535</cell><cell>functions.SMO.Bagging</cell><cell>0.453615</cell></row><row><cell>meta.Vote</cell><cell>0.800557</cell><cell>functions.SMO</cell><cell>0.448293</cell></row><row><cell>trees.J48</cell><cell>0.789733</cell><cell>bayes.BayesNet.Bagging</cell><cell>0.447375</cell></row><row><cell>trees.NBTree.Bagging</cell><cell>0.774144</cell><cell>bayes.BayesNet</cell><cell>0.440049</cell></row><row><cell>trees.SimpleCart.Bagging</cell><cell>0.772255</cell><cell>functions.RBFNetwork.Adaboost</cell><cell>0.41195</cell></row><row><cell>trees.NBTree</cell><cell>0.728909</cell><cell>bayes.NaiveBayes</cell><cell>0.356969</cell></row><row><cell>functions.MultilayerPerceptron.Bagging</cell><cell>0.705117</cell><cell>bayes.NaiveBayes.Bagging</cell><cell>0.356266</cell></row><row><cell>rules.JRip.Bagging</cell><cell>0.701797</cell><cell>bayes.NaiveBayes.Adaboost</cell><cell>0.332357</cell></row><row><cell>trees.SimpleCart</cell><cell>0.695209</cell><cell>functions.RBFNetwork.Bagging</cell><cell>0.3255</cell></row><row><cell>functions.MultilayerPerceptron</cell><cell>0.686196</cell><cell>functions.RBFNetwork</cell><cell>0.32445</cell></row><row><cell>Table 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Results of the ELECTRE I approach.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DMU/Classifier</cell><cell>ELECTRE</cell><cell>DMU/Classifier</cell><cell>ELECTRE</cell></row><row><cell>bayes.BayesNet.Adaboost</cell><cell>0</cell><cell>lazy.IBk.Bagging</cell><cell>0</cell></row><row><cell>bayes.NaiveBayes.Adaboost</cell><cell>0</cell><cell>rules.DecisionTable.Bagging</cell><cell>0</cell></row><row><cell>functions.Logistic.Adaboost</cell><cell>0</cell><cell>rules.JRip.Bagging</cell><cell>0</cell></row><row><cell>functions.MultilayerPerceptron.Adaboost</cell><cell>0</cell><cell>trees.J48.Bagging</cell><cell>1</cell></row><row><cell>functions.RBFNetwork.Adaboost</cell><cell>0</cell><cell>trees.NBTree.Bagging</cell><cell>0</cell></row><row><cell>functions.SMO.Adaboost</cell><cell>0</cell><cell>trees.SimpleCart.Bagging</cell><cell>0</cell></row><row><cell>Lazy.IBk.Adaboost</cell><cell>0</cell><cell>bayes.BayesNet</cell><cell>0</cell></row><row><cell>rules.DecisionTable.Adaboost</cell><cell>0</cell><cell>bayes.NaiveBayes</cell><cell>0</cell></row><row><cell>rules.JRip.Adaboost</cell><cell>0</cell><cell>functions.Logistic</cell><cell>0</cell></row><row><cell>Trees.J48.Adaboost</cell><cell>1</cell><cell>functions.MultilayerPerceptron</cell><cell>0</cell></row><row><cell>Trees.NBTree.Adaboost</cell><cell>0</cell><cell>functions.RBFNetwork</cell><cell>0</cell></row><row><cell>Trees.SimpleCart.Adaboost</cell><cell>1</cell><cell>functions.SMO</cell><cell>0</cell></row><row><cell>bayes.BayesNet.Bagging</cell><cell>0</cell><cell>lazy.IBk</cell><cell>0</cell></row><row><cell>bayes.NaiveBayes.Bagging</cell><cell>0</cell><cell>rules.DecisionTable</cell><cell>0</cell></row><row><cell>functions.Logistic.Bagging</cell><cell>0</cell><cell>rules.JRip</cell><cell>0</cell></row><row><cell>functions.MultilayerPerceptron.Bagging</cell><cell>0</cell><cell>trees.J48</cell><cell>0</cell></row><row><cell>functions.RBFNetwork.Bagging</cell><cell>0</cell><cell>trees.NBTree</cell><cell>0</cell></row><row><cell>functions.SMO.Bagging</cell><cell>0</cell><cell>trees.SimpleCart</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Results of the DEA approach.</figDesc><table><row><cell>DMU/Classifier</cell><cell>CCR</cell><cell>BCC</cell><cell>DMU/Classifier</cell><cell>CCR</cell><cell>BCC</cell></row><row><cell>bayes.BayesNet.Adaboost</cell><cell>0.67</cell><cell>0.68</cell><cell>rules.DecisionTable.Bagging</cell><cell>0.74</cell><cell>1.00</cell></row><row><cell>bayes.NaiveBayes.Adaboost</cell><cell>0.70</cell><cell>0.74</cell><cell>rules.JRip.Bagging</cell><cell>0.83</cell><cell>0.84</cell></row><row><cell>functions.Logistic.Adaboost</cell><cell>0.66</cell><cell>0.67</cell><cell>trees.J48.Bagging</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>functions.MultilayerPerceptron.Adaboost</cell><cell>0.69</cell><cell>0.71</cell><cell>trees.NBTree.Bagging</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>functions.RBFNetwork.Adaboost</cell><cell>0.55</cell><cell>0.58</cell><cell>trees.SimpleCart.Bagging</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>functions.SMO.Adaboost</cell><cell>0.55</cell><cell>0.56</cell><cell>bayes.BayesNet</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>lazy.IBk.Adaboost</cell><cell>0.91</cell><cell>0.93</cell><cell>bayes.NaiveBayes</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>rules.DecisionTable.Adaboost</cell><cell>0.78</cell><cell>0.79</cell><cell>functions.Logistic</cell><cell>0.84</cell><cell>1.00</cell></row><row><cell>rules.JRip.Adaboost</cell><cell>1.00</cell><cell>1.00</cell><cell>functions.MultilayerPerceptron</cell><cell>0.68</cell><cell>0.71</cell></row><row><cell>trees.J48.Adaboost</cell><cell>1.00</cell><cell>1.00</cell><cell>functions.RBFNetwork</cell><cell>0.74</cell><cell>0.75</cell></row><row><cell>trees.NBTree.Adaboost</cell><cell>0.93</cell><cell>0.93</cell><cell>functions.SMO</cell><cell>0.67</cell><cell>0.71</cell></row><row><cell>trees.SimpleCart.Adaboost</cell><cell>1.00</cell><cell>1.00</cell><cell>lazy.IBk</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>bayes.BayesNet.Bagging</cell><cell>1.00</cell><cell>1.00</cell><cell>rules.DecisionTable</cell><cell>0.86</cell><cell>0.99</cell></row><row><cell>bayes.NaiveBayes.Bagging</cell><cell>0.75</cell><cell>0.76</cell><cell>rules.JRip</cell><cell>0.90</cell><cell>0.91</cell></row><row><cell>functions.Logistic.Bagging</cell><cell>0.65</cell><cell>0.67</cell><cell>trees.J48</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>functions.MultilayerPerceptron.Bagging</cell><cell>0.72</cell><cell>0.72</cell><cell>trees.NBTree</cell><cell>0.86</cell><cell>0.87</cell></row><row><cell>functions.RBFNetwork.Bagging</cell><cell>0.58</cell><cell>0.62</cell><cell>trees.SimpleCart</cell><cell>0.92</cell><cell>0.93</cell></row><row><cell>functions.SMO.Bagging</cell><cell>0.58</cell><cell>0.62</cell><cell>meta.Stacking</cell><cell>0.95</cell><cell>0.96</cell></row><row><cell>lazy.IBk.Bagging</cell><cell>1.00</cell><cell>1.00</cell><cell>meta.Vote</cell><cell>0.88</cell><cell>0.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Results of the PROMETHEE II approach.</figDesc><table><row><cell>DMU/Classifier</cell><cell>PROMETHEE</cell><cell>DMU/Classifier</cell><cell>PROMETHEE</cell></row><row><cell>Trees.SimpleCart.Adaboost</cell><cell>0.935458</cell><cell>rules.JRip</cell><cell>À0.09802</cell></row><row><cell>Trees.J48.Adaboost</cell><cell>0.822509</cell><cell>functions.MultilayerPerceptron</cell><cell>À0.10044</cell></row><row><cell>Trees.J48.Bagging</cell><cell>0.732957</cell><cell>rules.DecisionTable</cell><cell>À0.1545</cell></row><row><cell>meta.Stacking</cell><cell>0.599032</cell><cell>functions.MultilayerPerceptron.Adaboost</cell><cell>À0.16015</cell></row><row><cell>Trees.SimpleCart.Bagging</cell><cell>0.575635</cell><cell>functions.Logistic</cell><cell>À0.19161</cell></row><row><cell>rules.JRip.Adaboost</cell><cell>0.571601</cell><cell>functions.Logistic.Bagging</cell><cell>À0.25938</cell></row><row><cell>Trees.NBTree.Bagging</cell><cell>0.570795</cell><cell>functions.Logistic.Adaboost</cell><cell>À0.33683</cell></row><row><cell>Trees.NBTree.Adaboost</cell><cell>0.561113</cell><cell>bayes.BayesNet.Adaboost</cell><cell>À0.39653</cell></row><row><cell>Lazy.IBk</cell><cell>0.50948</cell><cell>bayes.BayesNet.Bagging</cell><cell>À0.42719</cell></row><row><cell>Lazy.IBk.Bagging</cell><cell>0.501412</cell><cell>bayes.BayesNet</cell><cell>À0.45623</cell></row><row><cell>rules.DecisionTable.Adaboost</cell><cell>0.393304</cell><cell>functions.SMO.Adaboost</cell><cell>À0.48689</cell></row><row><cell>Trees.J48</cell><cell>0.351351</cell><cell>functions.SMO.Bagging</cell><cell>À0.57886</cell></row><row><cell>Lazy.IBk.Adaboost</cell><cell>0.335216</cell><cell>functions.RBFNetwork.Adaboost</cell><cell>À0.5837</cell></row><row><cell>rules.JRip.Bagging</cell><cell>0.201694</cell><cell>functions.SMO</cell><cell>À0.61799</cell></row><row><cell>meta.Vote</cell><cell>0.198064</cell><cell>functions.RBFNetwork.Bagging</cell><cell>À0.62888</cell></row><row><cell>rules.DecisionTable.Bagging</cell><cell>0.153691</cell><cell>functions.RBFNetwork</cell><cell>À0.65551</cell></row><row><cell>functions.MultilayerPerceptron.Bagging</cell><cell>0.143203</cell><cell>bayes.NaiveBayes</cell><cell>À0.69342</cell></row><row><cell>Trees.NBTree</cell><cell>0.119</cell><cell>bayes.NaiveBayes.Bagging</cell><cell>À0.70069</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Y. Peng et al. / Information Sciences 191 (2012) 3-13</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements A short 5 pages version of this paper has been appeared at the proceeding of the 2nd International Conference on Software Engineering and Data Mining. This research has been supported by Grants from the National Natural Science Foundation of China under the Grant No. 70901011, No. 70901015, No. 70921061, No. 70531040; the Scientific Research Foundation for the Returned Overseas Chinese Scholars, State Education Ministry.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Extensions of TOPSIS for multi-objective large-scale nonlinear programming problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abo-Sinna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics and Computation</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="243" to="256" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalizing from case studies: a case study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Aha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineth International Conference on Machine Learning</title>
		<meeting>the Nineth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Using the analytical hierarchy process in selecting commercial real-time operating systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laplante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Technology and Decision Making</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="168" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Modern Information Retrieval</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Some models for estimating technical and scale inefficiencies in data envelopment analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Banker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Charnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1078" to="1092" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decision Support, Meta-Learning and ILP: Forum for Practical Problem Presentation and Prospective Solutions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Berrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Paterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the PKDD2000 Workshop on Data Mining</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Jorge</surname></persName>
		</editor>
		<meeting>the PKDD2000 Workshop on Data Mining</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note>Evaluation of machine-learning algorithm ranking advisors</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">L&apos;ingénièrie de la décision; elaboration d&apos;instruments d&apos;aide à la décision</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Brans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">La méthode PROMETHEE</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Nadeau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Landry</surname></persName>
		</editor>
		<meeting><address><addrLine>Québec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="183" to="213" />
		</imprint>
	</monogr>
	<note>L&apos;aide à la décision</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Brans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<ptr target="&lt;http://www.visualdecision.com/Pdf/How%20to%20use%20PROMETHEE.pdf&gt;" />
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>How to decide with PROMETHEE, available at</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PROMETHEE methods</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Brans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiple Criteria Decision Analysis: State of the Art Surveys</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Figueira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Mousseau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Roy</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="163" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>Belmont, California</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Wadsworth International Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MINDFUL: a framework for meta-inductive neuro-fuzzy learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Castiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fanelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="3253" to="3274" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Investigating the effect of dataset size, metrics sets, and feature selection techniques on software fault prediction problem</title>
		<author>
			<persName><forename type="first">C</forename><surname>Catal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Diri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1040" to="1058" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ridge estimators in logistic regression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Le Cessie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Houwelingen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="201" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Satisfaction assessment of multi-objective schedules using neural fuzzy methodology</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Production Research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1831" to="1849" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Empirical assessment of machine learning based software defect prediction techniques</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">U B</forename><surname>Challagulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Artificial Intelligence Tools</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="400" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T.-Ch</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Facility location selection using fuzzy TOPSIS under group decisions</title>
		<author>
			<persName><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty, Fuzziness &amp; Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="687" to="701" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Callis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jackson</surname></persName>
		</author>
		<ptr target="&lt;http://mdp.ivv.nasa.gov/&gt;" />
		<title level="m">Metrics Data Program, NASA IV and V Facility</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Measuring the efficiency of decision making units</title>
		<author>
			<persName><forename type="first">A</forename><surname>Charnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rhodes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="429" to="444" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Methodological advances in DEA: a survey and an application for the Dutch electricity sector</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cherchye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Neerlandica</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="410" to="438" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast effective rule induction</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<meeting>the Twelfth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufman</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data envelopment analysis: history, models and interpretations</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Seiford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook on Data Envelopment Analysis</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cooper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Seiford</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publisher</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Dasarathy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>IEEE Computer Society Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Machine learning research: four current directions</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An experimental comparison of three methods for constructing ensembles of decision trees: bagging, boosting, and randomization</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="157" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International Workshop on Multiple Classifier Systems</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1857</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the optimality of the simple Bayesian classifier under zero-one loss</title>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">203</biblScope>
			<biblScope unit="page" from="103" to="130" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comparing case-based reasoning classifiers for predicting high risk software components</title>
		<author>
			<persName><forename type="first">K</forename><surname>El-Emam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benlarbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="310" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Predicting defect-prone software modules using support vector machines</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Elish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Elish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="649" to="660" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From data mining to knowledge discovery: an overview</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Piatetsky-Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Piatetsky-Shapiro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Uthurusamy</surname></persName>
		</editor>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An experimental comparison of performance measures for classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ferri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hernandezorallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Modroiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="2009-01">2009. January 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Eletre methods, ELECTRE methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiple Criteria Decision Analysis: State of the Art Surveys</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Figueira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Mousseau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Roy</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="133" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Experiments with a new boosting algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th International Conference on Machine Learning</title>
		<meeting>13th International Conference on Machine Learning<address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufman</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Case-based software quality prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Software Engineering and Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="152" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<title level="m">Data Mining: Concepts and Techniques</title>
		<imprint>
			<publisher>Morgan Kaufman</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Combining predictors: comparison of five meta machine learning methods</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="91" to="105" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multiple Attribute Decision Making Methods and Applications</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Using regression trees to classify fault-prone software modules</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Reliability</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="455" to="462" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Identifying investment opportunities for advanced manufacturing system with comparative-integrated performance measurement</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Production Economics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="23" to="33" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scaling up the accuracy of Naı ¨ve Bayes classifiers: a decision tree hybrid</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Simoudis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Han</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</editor>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining<address><addrLine>Portland, OR. Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A DEA-based approach to ranking multi-criteria alternatives</title>
		<author>
			<persName><forename type="first">M</forename><surname>Koksalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tuncer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Technology and Decision Making</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="54" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<title level="m">Combining Pattern Classifiers: Methods and Algorithms</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Benchmarking classification models for software defect prediction: a proposed framework and novel findings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lessmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pietsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="496" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ranking decision alternatives by integrated DEA, AHP and gower plot techniques</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal Of Information Technology &amp; Decision Making</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="258" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An investigation of machine learning based prediction systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kadoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leflel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Phapl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schofield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shepperd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Webster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems Software</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="29" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Assessing predictors of software defects</title>
		<author>
			<persName><forename type="first">T</forename><surname>Menzies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Distefano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Orrego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop Predictive Software Models</title>
		<meeting>Workshop Predictive Software Models</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Using different ELECTRE methods in strategic planning in the presence of human behavioral resistance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Milani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>El-Lahham</surname></persName>
		</author>
		<idno type="DOI">10.1155/JAMDS/2006/10936</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Mathematics and Decision Sciences</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
	<note>Article ID 10936, 19 pages</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The detection of fault-prone programs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Munson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="423" to="433" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Development of multi-criteria metrics for evaluation of data mining algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nakhaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schnabl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Third International Conference on Knowledge Discovery and Data Mining (KDD&apos;97)</title>
		<meeting>eeding of the Third International Conference on Knowledge Discovery and Data Mining (KDD&apos;97)<address><addrLine>Newport Beach, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">August 14-17, 1997</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Comparison of weights in TOPSIS models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical and Computer Modelling</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="721" to="727" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Compromise solution by MCDM methods: a comparative analysis of VIKOR and TOPSIS</title>
		<author>
			<persName><forename type="first">S</forename><surname>Opricovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Tzeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="445" to="455" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Empirical evaluation of classifiers for software risk management</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Technology and Decision Making</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="749" to="768" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A descriptive framework for the field of data mining and knowledge discovery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Information Technology and Decision Making</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="639" to="682" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fast training of support vector machines using sequential minimal optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods-Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schotolkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="185" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Estimating software readiness using predictive models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Quah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="430" to="445" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<imprint>
			<publisher>Morgan Kaufman</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The algorithm selection problem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Computers</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="65" to="118" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Ensemble-based classifiers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10462-009-9124-7</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<date type="published" when="2009-11-19">2009. 19 November 2009</date>
		</imprint>
	</monogr>
	<note>published online</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Classement et choix en presence de points de vue multiples (la methode ELECTRE</title>
		<author>
			<persName><forename type="first">B</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">R.I.R.O</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="57" to="75" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cross-Disciplinary perspectives on meta-learning for algorithm selection</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Smith-Miles</surname></persName>
		</author>
		<idno type="DOI">10.1145/1456650.1456656</idno>
		<ptr target="&lt;http://doi.acm.org/10.1145/1456650.1456656&gt;" />
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008-12">2008. December 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The strength of weak learnability</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="227" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A study of AdaBoost with Naı ¨ve Bayesian classifiers: weakness and improvement</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="186" to="200" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Incremental construction of classifier and discriminant ensembles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Semerci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yıldız</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydın</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1298" to="1318" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Kulikowski</surname></persName>
		</author>
		<title level="m">Computer Systems that Learn: Classification and Predication Methods from Statistics, Neural Nets, Machine Learning and Expert Systems</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Recognizing strong and weak opinion clauses</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="99" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufman</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>second ed.</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">No Free Lunch Theorems for Search</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Macready</surname></persName>
		</author>
		<idno>SFI-TR-95-02-010</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Santa Fe Institute</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The analytic hierarchy process-a survey of the method and its applications</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zahedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interfaces</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="96" to="108" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
