<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Policy Gradient in Robust MDPs with Global Convergence Guarantee</title>
				<funder ref="#_gs8BQ29">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_mMHFqt7">
					<orgName type="full">CityU Start-Up</orgName>
				</funder>
				<funder ref="#_eWkfJB9 #_5d39u5h">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_JjHMWee">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-07">7 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qiuhao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chin</forename><forename type="middle">Pang</forename><surname>Ho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data Science</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Marek</forename><surname>Petrik</surname></persName>
							<email>&lt;mpetrik@cs.unh.edu&gt;.</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of New Hampshire</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Marek Petrik</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Policy Gradient in Robust MDPs with Global Convergence Guarantee</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-07">7 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2212.10439v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robust Markov decision processes (RMDPs) provide a promising framework for computing reliable policies in the face of model errors. Many successful reinforcement learning algorithms build on variations of policy-gradient methods, but adapting these methods to RMDPs has been challenging. As a result, the applicability of RMDPs to large, practical domains remains limited. This paper proposes a new Double-Loop Robust Policy Gradient (DRPG), the first generic policy gradient method for RMDPs. In contrast with prior robust policy gradient algorithms, DRPG monotonically reduces approximation errors to guarantee convergence to a globally optimal policy in tabular RMDPs. We introduce a novel parametric transition kernel and solve the inner loop robust policy via a gradient-based method. Finally, our numerical results demonstrate the utility of our new algorithm and confirm its global convergence properties.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Markov decision process (MDP) is a standard model in dynamic decision-making and reinforcement learning <ref type="bibr" target="#b50">(Puterman, 2014;</ref><ref type="bibr" target="#b64">Sutton &amp; Barto, 2018)</ref>. However, a fundamental challenge with using MDPs in many applications is that model parameters, such as the transition function, are rarely known precisely. Robust Markov decision processes (RMDPs) have emerged as an effective and promising approach for mitigating the impact of model ambiguity. RMDPs assume that the transition function resides in a predefined ambiguity set and seek a policy that performs best for the worst-case transition function in the ambiguity set. Compared to MDPs, the performance of RMDPs is less sensitive to the parameter errors that arise when one estimates the transition function from empirical data, as is common in reinforcement learning <ref type="bibr" target="#b72">(Xu &amp; Mannor, 2009;</ref><ref type="bibr" target="#b45">Petrik, 2012;</ref><ref type="bibr" target="#b47">Petrik et al., 2016)</ref>.</p><p>As is common in recent literature on RMDPs, we assume that the RMDP's ambiguity set satisfies certain rectangularity assumptions <ref type="bibr" target="#b71">(Wiesemann et al., 2013;</ref><ref type="bibr" target="#b22">Ho et al., 2021;</ref><ref type="bibr">Panaganti &amp; Kalathil, 2021)</ref>. Albeit general RMDPs are NP-hard to solve <ref type="bibr" target="#b71">(Wiesemann et al., 2013)</ref>, they become tractable under rectangularity assumptions and can be solved using dynamic programming <ref type="bibr" target="#b23">(Iyengar, 2005;</ref><ref type="bibr" target="#b40">Nilim &amp; El Ghaoui, 2005;</ref><ref type="bibr" target="#b26">Kaufman &amp; Schaefer, 2013;</ref><ref type="bibr" target="#b22">Ho et al., 2021)</ref>. The simplest rectangularity assumption is known as (s, a)-rectangularity and allows the adversarial nature to choose the worst transition probability for each state and action independently. Because the (s, a)-rectangularity assumption can be too restrictive, we assume the moregeneral s-rectangular ambiguity set (Le <ref type="bibr" target="#b29">Tallec, 2007;</ref><ref type="bibr" target="#b71">Wiesemann et al., 2013;</ref><ref type="bibr" target="#b15">Derman et al., 2021;</ref><ref type="bibr">Wang et al., 2022)</ref>, which restricts the adversarial nature to choose a transition probability without observing the action. Our results also readily extend to other notions of rectangularity, including k-rectangular <ref type="bibr" target="#b37">(Mannor et al., 2016)</ref>, and r-rectangular RMDPs <ref type="bibr" target="#b17">(Goyal &amp; Grand-Cl?ment, 2022)</ref>.</p><p>Policy gradient techniques have gained considerable popularity in reinforcement learning due to their remarkable empirical performance and flexibility in large and complex domains <ref type="bibr" target="#b63">(Silver et al., 2014;</ref><ref type="bibr" target="#b74">Xu et al., 2014)</ref>. By parameterizing policies, policy gradient methods easily scale to large state and action spaces, and they also easily leverage generic optimization techniques <ref type="bibr" target="#b27">(Konda &amp; Tsitsiklis, 1999;</ref><ref type="bibr" target="#b8">Bhatnagar et al., 2009;</ref><ref type="bibr" target="#b46">Petrik &amp; Subramanian, 2014;</ref><ref type="bibr" target="#b48">Pirotta et al., 2015;</ref><ref type="bibr" target="#b59">Schulman et al., 2015;</ref><ref type="bibr" target="#b3">2017;</ref><ref type="bibr">Behzadian et al., 2021a)</ref>. In addition, recent work shows that many policy gradient algorithms are guaranteed to find a globally-optimal policy in tabular MDPs even though they optimize a non-convex objective function <ref type="bibr" target="#b0">(Agarwal et al., 2021;</ref><ref type="bibr" target="#b7">Bhandari &amp; Russo, 2021)</ref>.</p><p>As our first contribution, we propose a new policy gradient method for solving s-rectangular RMDPs. We call this method the Double-Loop Robust Policy Gradient (DRPG), because it is inspired by double-loop algorithms designed for solving saddle point problems <ref type="bibr" target="#b24">(Jin et al., 2020;</ref><ref type="bibr" target="#b35">Luo et al., 2020;</ref><ref type="bibr" target="#b51">Razaviyayn et al., 2020;</ref><ref type="bibr" target="#b35">Zhang et al., 2020)</ref>. In particular, DRPG solves RMDPs using two nested loops: an outer loop updates policies, and an inner loop approximately computes the worst-case transition probabilities. While the outer loop resembles policy gradient updates in regular MDPs, the inner loop must optimize over an infinite number of transition probabilities in the ambiguity set. To effectively optimize the continuous transition probabilities, we use a projected gradient method with a finite but complete parametrization in tabular MDPs. To scale the algorithm to large problems, we propose to use a parametrization based on KL-divergence ambiguity sets.</p><p>As our second contribution, we show that DRPG is guaranteed to converge to a globally optimal policy in s-rectangular RMDPs. While this result mirrors similar known results for ordinary MDPs, the robust setting involves several additional non-trivial challenges. Unlike in ordinary MDPs, the RMDP return is not differentiable in terms of the policy <ref type="bibr" target="#b51">(Razaviyayn et al., 2020)</ref>, which precludes us from leveraging MDP results. Since the RMDP return is not convex, it also does not admit subgradients. Instead, we show that it is sufficient to approximate it by its Moreau envelope, which is differentiable. An additional challenge is that solving the inner loop optimally in every policy carries an unacceptable computational policy, but solving it approximately may cause oscillations. We address this problem by proposing a schedule of decreasing approximation errors that are sufficient to converge to the optimal solution. In fact, the policy updates are guaranteed to converge to the optimal policy as long as the inner loop can be solved with sufficient precision, even when the RMDP is non-rectangular.</p><p>Despite the recent advances in robust reinforcement learning <ref type="bibr" target="#b54">(Roy et al., 2017;</ref><ref type="bibr" target="#b2">Badrinath &amp; Kalathil, 2021;</ref><ref type="bibr" target="#b69">Wang &amp; Zou, 2021;</ref><ref type="bibr" target="#b43">Panaganti &amp; Kalathil, 2022)</ref>, policy gradient methods for solving RMDPs have received only limited attention. A concurrent work proposes a policy gradient method for solving RMDPs with a particular R-contamination ambiguity sets <ref type="bibr" target="#b70">(Wang &amp; Zou, 2022)</ref>. While this algorithm is compellingly simple, the R-contamination set is very limited in comparison with the general sets that we consider. In fact, we show in Proposition F.1 that RMDPs with R-contamination ambiguity sets simply equal to ordinary MDPs with a reduced discount factor; please see Appendix F for more details. Another recent work develops an extended mirror descent method for solving RMDPs <ref type="bibr" target="#b31">(Li et al., 2022)</ref>; however, their results are limited to (s, a)rectangular MDPs only, and their algorithm requires the exact robust Q function to update the policy at every iteration. On the other hand, our proposed algorithm is compatible with any compact ambiguity set, and we do not require an exact optimal solution when solving the inner maximization problem. Moreover, by parameterizing the inner problem, the proposed algorithm is scalable to large problems.</p><p>While this paper exclusively focuses on RMDPs, it is worth mentioning that there is an active line of research studying a related model, called distributionally robust MDPs, which assumes the transition kernel is random and governed by an unknown probability distribution that lies in an ambiguity set <ref type="bibr" target="#b57">(Ruszczy?ski, 2010;</ref><ref type="bibr" target="#b73">Xu &amp; Mannor, 2010;</ref><ref type="bibr" target="#b61">Shapiro, 2016;</ref><ref type="bibr">Chen et al., 2019;</ref><ref type="bibr">Grand-Cl?ment &amp; Kroer, 2021a;</ref><ref type="bibr" target="#b62">Shapiro, 2021;</ref><ref type="bibr" target="#b34">Liu et al., 2022)</ref>.</p><p>The remainder of the paper is organized as follows. Section 2 outlines RMDP and optimization properties that are needed for our results. Then, Section 3 describes the outer loop of DRPG, our proposed algorithm, and shows its global convergence guarantee. The algorithms for solving the inner loop are then described in Section 4. Finally, in Section 5, we present experimental results that illustrate the effective empirical performance of DRPG.</p><p>Notation: We reserve lowercase letters for scalars, lowercase bold characters for vectors, and uppercase bold characters for matrices. We denote ? S as the probability simplex in R S + . For vectors, we use ? ? ? to denote the l 2 -norm. For a differentiable function f (x, y), we use ? x f (x, y) to denote the partial gradient of f with respect to x. The symbol e denotes a vector of all ones of the size appropriate to the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Notations and Settings</head><p>An ordinary MDP is specified by a tuple ?S, A, p, c, ?, ??, where S = {1, 2, ? ? ? , S} and A = {1, 2, ? ? ? , A} are the finite state and action sets, respectively. The discount factor is ? ? (0, 1) and the distribution of the initial state is ? ? ? S . The probability distribution of transiting from a current state s to a next state s ? after taking an action a is denoted as a vector p sa ? ? S and in a part of the transition kernel p := (p sa ) s?S,a?A ? (? S ) S?A . The cost of the aforementioned transition is denoted as c sas ? for each (s, a, s ? ) ? S ?A?S. It is well-known that translating the costs by a constant or multiplying them by a positive scalar does not change the set of optimal policies. Therefore, we can assume without loss of generality that the cost function is bounded in</p><formula xml:id="formula_0">[0, 1]. Assumption 2.1 (Bounded cost). For any (s, a, s ? ) ? S ? A ? S, the cost c sas ? ? [0, 1].</formula><p>Given a stationary randomized policy ? := (? s ) s?S that lies in the policy space ? = (? A ) S , ? maps from state s ? S to a distribution over action a ? A, and the quality of a policy ? is evaluated by the value function v ?,p ? R S , defined as</p><formula xml:id="formula_1">v ?,p s = E ?,p ? t=0 ? t ? c statst+1 | s 0 = s ,</formula><p>where a t follows the distribution ? st , and E ?,p denotes expectation with respect to the distribution induced by ? and transition function p conditioned on the initial state event {s 0 = s}. Similarly, the value of taking action a at state s is referred as the action value function as below</p><formula xml:id="formula_2">q ?,p sa = E ?,p ? t=0 ? t c statst+1 | s 0 = s, a 0 = a ,</formula><p>where it is known that v ?,p s = a?A ? sa q ?,p sa <ref type="bibr" target="#b50">(Puterman, 2014;</ref><ref type="bibr" target="#b64">Sutton &amp; Barto, 2018)</ref>. The objective of an MDP is to compute the optimal policy ? ? that yields the minimum expected cost, i.e.,</p><formula xml:id="formula_3">? ? = arg min ??? E ?,p ? t=0 ? t c statst+1 |s 0 ? ? . (1)</formula><p>In most domains, the exact transition kernel and cost function are not known precisely and must be estimated from data. These estimation errors often result in policies that perform poorly when deployed. To compute reliable policies with model errors, RMDPs, defined as ?S, A, P, c, ?, ??, aim to optimize the worst-case performance with respect to plausible errors <ref type="bibr" target="#b23">(Iyengar, 2005;</ref><ref type="bibr" target="#b40">Nilim &amp; El Ghaoui, 2005;</ref><ref type="bibr" target="#b71">Wiesemann et al., 2013)</ref>, i.e.</p><formula xml:id="formula_4">min ??? max p?P J ? (?, p) := ? ? v ?,p = s?S ? s v ?,p s ,<label>(2)</label></formula><p>where P is known as the ambiguity set. By carefully calibrating P so that it contains the unknown true transition kernel, the optimal policy in (2) can achieve reliable performance in practice <ref type="bibr" target="#b56">(Russell &amp; Petrik, 2019;</ref><ref type="bibr">Behzadian et al., 2021b;</ref><ref type="bibr" target="#b43">Panaganti et al., 2022)</ref>.</p><p>Note that, at this point, there is no need to assume that the RMDP in (2) is rectangular, such as (s, a)-rectangular or s-rectangular <ref type="bibr" target="#b23">(Iyengar, 2005;</ref><ref type="bibr" target="#b40">Nilim &amp; El Ghaoui, 2005;</ref><ref type="bibr" target="#b71">Wiesemann et al., 2013;</ref><ref type="bibr" target="#b22">Ho et al., 2021)</ref>. We do not need these assumptions to describe or analyze DRPG and only require P to be compact. Rectangularity assumptions will be helpful, however, when developing algorithms for solving the inner maximization problem.</p><p>Given a specific policy and transition kernel, the occupancy measure represents the frequencies of visits to states (Puterman, 2014), which is defined as follow.</p><p>Definition 2.2 (Occupancy measure). The discounted state occupancy measure d ?,p ? : S ? [0, 1] for an initial distribution ?, a policy ? ? ?, and a transition kernel p is defined as</p><formula xml:id="formula_5">d ?,p ? (s ? ) = (1 -?) s?S ? t=0 ? t ?(s)p ? ss ? (t).<label>(3)</label></formula><p>Here, p ? ss ? (t) is the probability of arriving in a state s ? after transiting t time steps from state s over the policy ? and the transition kernel p.</p><p>The non-convex minimax problem in (2) can be reformulated as an equivalent problem of minimizing the worst-case return:</p><formula xml:id="formula_6">min ??? ?(?) := max p?P J ? (?, p) .<label>(4)</label></formula><p>Then, it may seem natural to solve (4) by a gradient descent on the function ?. This is, in general, not possible because the function ? is not differentiable. In addition, since ? is neither convex nor concave, its subgradient does not exist either <ref type="bibr" target="#b41">(Nouiehed et al., 2019;</ref><ref type="bibr" target="#b33">Lin et al., 2020)</ref>. These complications motivate the need for the double-loop iterative scheme to solve RMDPs in Section 3.</p><p>Next, we introduce two crucial definitions on smoothness and Lipschitz continuity, which we need to analyze DRPG.</p><formula xml:id="formula_7">Definition 2.3. A function h : X ? R is L-Lipschitz if for any x 1 , x 2 ? X , we have that ?h(x 1 ) -h(x 2 )? ? L?x 1 -x 2 ?, and ?-smooth if for any x 1 , x 2 ? X , we have ??h(x 1 ) -?h(x 2 )? ? ??x 1 -x 2 ?.</formula><p>To discuss the global optimality of RMDPs, we introduce the following definition of weak convexity officially.</p><p>Definition 2.4 (Weak Convexity). The function h :</p><formula xml:id="formula_8">X ? R is ?-weakly convex if for any g ? ?h(x) and x, x ? ? X , h(x ? ) -h(x) ? ?g, x ? -x? - ? 2 ?x ? -x? 2 .</formula><p>Here, ?h(x) represents the Fr?chet sub-differential (See Definition D.1 in the appendix) of h(?) at x ? X , which generalizes the notion of gradient for the non-smooth function <ref type="bibr" target="#b67">(Vial, 1983;</ref><ref type="bibr" target="#b13">Davis &amp; Drusvyatskiy, 2019;</ref><ref type="bibr" target="#b66">Thekumparampil et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Solving the Outer Loop</head><p>In this section, we describe a policy gradient approach that solves the minimization problem in (4). Surprisingly, we show that a form of gradient descent applied to (4) converges to a globally-optimal solution, even though the objective function is neither convex nor concave. This result is inspired by the recent analysis of policy gradient methods for ordinary MDPs <ref type="bibr" target="#b0">(Agarwal et al., 2021;</ref><ref type="bibr" target="#b7">Bhandari &amp; Russo, 2021)</ref>. For now, we assume that there exists an oracle that solves the inner maximization problem. We provide the discussion and algorithms for solving the inner problem in Section 4.</p><p>The remainder of the section is organized as follows. In Section 3.1, we describe our new policy gradient scheme and then, in Section 3.2, we show that our scheme is guaranteed to converge to the global solution. To the best of our knowledge, this is the first generic robust policy gradient algorithm with global convergence guarantees.</p><p>Algorithm 1 Double-Loop Robust Policy Gradient (DRPG) Input: initial policy ? 0 , iteration time T , tolerance sequence {? t } t?0 such that ? t+1 ? ?? t , step size sequence {? t } t?0 for t = 0, 1, . . . , T -1 do Find p t so that J ? (? t , p t ) ? max p?P J ? (? t , p) -? t . Set ? t+1 ? Proj ? (? t -? t ? ? J ? (? t , p t )). (Eq. ( <ref type="formula" target="#formula_11">5</ref>)) end for Output:</p><formula xml:id="formula_9">? t ? ? {? 0 , . . . , ? T -1 } s.t. J ? (? t ? , p t ) = min t ? ?{0,...,T -1} J ? (? t ? , p t ) 3.1. Double-Loop Robust Policy Gradient Method (DRPG)</formula><p>We now describe the proposed policy gradient scheme summarized in Algorithm 1, named Double-Loop Robust Policy Gradient (DRPG). We refer to DRPG as a "double loop" method in order to be consistent with the terminology in game theory literature <ref type="bibr" target="#b41">(Nouiehed et al., 2019;</ref><ref type="bibr" target="#b66">Thekumparampil et al., 2019;</ref><ref type="bibr" target="#b24">Jin et al., 2020;</ref><ref type="bibr" target="#b35">Zhang et al., 2020)</ref>.</p><p>The inner loop of DRPG updates the worst-case transition probabilities p t while the outer loop updates the policies ? t . Specifically, DRPG iteratively takes steps along the policy gradient to search for an optimal policy in (2). At each iteration t, we first solve the inner maximization problem to some specific precision ? t ; that is, for a policy ? t at iteration t, we seek for any transition kernel p t such that</p><formula xml:id="formula_10">J ? (? t , p t ) ? max p?P J ? (? t , p) -? t .</formula><p>Once p t is computed, DRPG then takes a projected gradient step to minimize J ? (?, p t ) subject to a constraint ? ? ?.</p><p>When chosen appropriately, the sequence ? t allows for quick policy updates in the initial stages of the algorithm without putting the global convergence in jeopardy. Similar algorithms studied in the context of zero-sum games do not include this tolerance ? t <ref type="bibr" target="#b41">(Nouiehed et al., 2019;</ref><ref type="bibr" target="#b66">Thekumparampil et al., 2019)</ref>. The adaptive tolerance sequence {? t } t?0 is inspired by prior work on algorithms for RMDPs <ref type="bibr" target="#b22">(Ho et al., 2021)</ref>. The convergence analysis below provides further guidance on appropriate choices of ? t .</p><p>DRPG updates policies using projected gradient descent. The well-known proximal representation of projected gradient is <ref type="bibr" target="#b6">(Bertsekas, 2016)</ref>:</p><formula xml:id="formula_11">? t+1 ? arg min ??? ?? ? J ? (? t , p t ), ? -? t ? + 1 2? t ?? -? t ? 2 = Proj ? (? t -? t ? ? J ? (? t , p t )) ,<label>(5)</label></formula><p>where Proj ? is the projection operator onto ? and ? t &gt; 0 is the step size. This projected gradient update on ? t := (? t,s ) s?S ? (? A ) S can be further decoupled to multiple projection updates that across states and take the form as</p><formula xml:id="formula_12">? t+1,s = Proj ? A (? t,s -? t ? ?s J ? (? t , p t )) , ?s ? S,</formula><p>which can also be seen as a gradient step followed by a projection onto ? A for each state s ? S. Note that the gradient ? ? J ? (? t , p t ) used in DRPG is identical to the the gradient in ordinary MDPs, e.g., <ref type="bibr" target="#b0">(Agarwal et al., 2021;</ref><ref type="bibr" target="#b7">Bhandari &amp; Russo, 2021)</ref>,</p><formula xml:id="formula_13">?J ? (?, p) ?? sa = 1 1 -? ? d ?,p ? (s) ? q ?,p sa .<label>(6)</label></formula><p>Actor-critic RL algorithms are typically based on this form of the policy gradient.</p><p>An alternative to double-loop algorithms is to use singleloop algorithms. Single-loop algorithms interleave gradient updates to the inner and outer optimization problems <ref type="bibr" target="#b39">(Mokhtari et al., 2020;</ref><ref type="bibr" target="#b35">Zhang et al., 2020)</ref>. Interleaving gradient updates is fast but prone to instabilities and oscillations. The most-common approach to preventing such instabilities is to resort to two-scale step size updates <ref type="bibr" target="#b20">(Heusel et al., 2017;</ref><ref type="bibr" target="#b12">Daskalakis et al., 2020;</ref><ref type="bibr" target="#b55">Russel et al., 2020)</ref>.</p><p>We focus in this work on double-loop algorithms because of their conceptual simplicity and good empirical behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Convergence Analysis</head><p>We now turn to analyzing the convergence behavior of DRPG. First, recall that we assume that P is compact. Virtually all ambiguity sets considered in prior work, such as L 1 -ambiguity sets, L ? -ambiguity sets, L 2 -ambiguity sets, and KL-ambiguity sets, are compact.</p><p>Then, the following lemma helps us to derive the weak convexity of this non-convex, non-differentiable (i.e., nonsmooth) objective function ?(?).</p><p>Lemma 3.1. The objective function J ? (?, p) in (2) is L ? -Lipschitz and ? ? -smooth in ? with</p><formula xml:id="formula_14">L ? := ? A (1 -?) 2 , ? ? := 2?A (1 -?) 3 .</formula><p>Furthermore, the objective ?(?) is ? ? -weakly convex and L ? -Lipschitz.</p><p>The proof of this lemma, as well as of all the remaining auxiliary results, are provided in the appendix. Lemma 3.1 establishes some general continuity properties of ?(?) and serves as an important stepping stone for deriving the global convergence of Algorithm 1; however, weak convexity alone is insufficient to guarantee that gradient-based updates converge to a global optimum. on a "gradient dominance condition". Informally speaking, a function h(x) is said to satisfy the gradient dominance condition if h(x) -h(x ? ) = O(G(x)), where G(?) is a suitable notion that measures the gradient of h. By having a gradient dominance condition, one can prevent the gradient from vanishing before reaching a globally optimal point. Despite the non-smoothness of ?(?), weakly convex problems naturally admit an implicit smooth approximation through the Moreau envelope <ref type="bibr" target="#b13">(Davis &amp; Drusvyatskiy, 2019;</ref><ref type="bibr" target="#b36">Mai &amp; Johansson, 2020)</ref>. Inspired by the idea of gradient dominance, we introduce the gradient of the Moreau envelope and show that ?(?) satisfies a particular variant of the gradient dominance condition in the next theorem.</p><p>Theorem 3.2. Denote ? ? as the global optimal policy for RMDPs. Then, for any policy ?, we have</p><formula xml:id="formula_15">?(?) -?(? ? ) ? D ? SA 1 -? + L ? 2? ? ??? 1 2?? (?)?, (<label>7</label></formula><formula xml:id="formula_16">)</formula><p>where ? ? (?) is the Moreau envelope function of ?(?) (see Definition D.3) and D := sup ???,p?P ? d ?,p ? /?? ? &lt; ? for every ? with min s?S ? s &gt; 0.</p><p>Here, ? d ?,p ? /?? ? is formally named as distribution mismatch coefficient which is often assumed to be bounded <ref type="bibr" target="#b58">(Scherrer, 2014;</ref><ref type="bibr" target="#b9">Chen &amp; Jiang, 2019;</ref><ref type="bibr" target="#b38">Mei et al., 2020;</ref><ref type="bibr" target="#b0">Agarwal et al., 2021;</ref><ref type="bibr" target="#b30">Leonardos et al., 2021)</ref>. This gradient-dominance type property implies that any firstorder stationary point of the Moreau envelope results in an approximately global optimal policy. We are now ready to state our main result.</p><p>Theorem 3.3 (Global convergence for DRPG). Denote ? t ? as the policy that Algorithm 1 outputs. Then, for a constant step size ? := ? ? T with any ? &gt; 0 and the initial tolerance ? 0 ? ? T , we have</p><formula xml:id="formula_17">?(? t ? ) -min ??? ?(?) ? ?,<label>(8)</label></formula><p>and T is chosen to be a large enough such that</p><formula xml:id="formula_18">T ? D ? SA 1-? + L? 2?? 4 4??S ? + 2?? ? L 2 ? + 4?? 1-? 2 ? 4 = O(? -4 ).<label>(9)</label></formula><p>Compared to the ordinary MDPs, the convergence analysis for solving RMDPs poses additional difficulties as objective function ?(?) is not only non-convex but also nondifferentiable <ref type="bibr" target="#b41">(Nouiehed et al., 2019;</ref><ref type="bibr" target="#b33">Lin et al., 2020)</ref>. Theorem 3.3 shows that the proposed Algorithm 1 converges to the global optimal for RMDPs by the following strategy. We first show the existence of an ?-first order stationary point (see Definition D.4) of ?(?). More concretely, we prove the gradient of the Moreau envelope is smaller than ? on the output policy. Then, by applying the derived gradient dominance condition (Theorem 3.2), we finally complete the proof as this stationary point is arbitrarily close to the global optimal solution.</p><p>Theorem 3.3 shows that DRPG converges to an ? global optimum within O(? -4 ) steps, which has a slower rate compared to standard policy gradient methods <ref type="bibr" target="#b0">(Agarwal et al., 2021)</ref>. The additional complexity arises from this need to control the approximation error in order to avoid looping.</p><p>In particular, computational errors at the inner loops could break the convergence of the outer loop. Similar behaviors are also observed in policy iteration for robust MDPs <ref type="bibr" target="#b11">(Condon, 1990;</ref><ref type="bibr" target="#b22">Ho et al., 2021)</ref>. Nevertheless, our analysis matches and is consistent with the other minimax convergence results obtained in non-convex non-concave minimax optimization <ref type="bibr" target="#b13">(Davis &amp; Drusvyatskiy, 2019;</ref><ref type="bibr" target="#b24">Jin et al., 2020)</ref>, and provides a conservative convergence guarantee.</p><p>DRPG relies on an oracle that outputs at least one worst-case transition kernel for any given ?. In fact, solving the inner loop problem could still be NP-hard for non-rectangular cases <ref type="bibr" target="#b71">(Wiesemann et al., 2013)</ref>. The following section proposes an algorithm for solving the inner loop problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Solving the Inner Loop</head><p>So far, we have described the outline of DRPG and proved its global convergence. In Algorithm 1, the transition kernel p t is obtained by approximately solving the inner maximization problem with a fixed outer policy ? k ? ?:</p><formula xml:id="formula_19">max p?P J ? (? k , p) = max p?P ? ? v ? k ,p .<label>(10)</label></formula><p>Whereas assumptions of boundness and compactness are used to ensure the inner maximum existing for the maximization problem, solving this maximization problem is still computationally challenging due to its nonconvexity <ref type="bibr" target="#b71">(Wiesemann et al., 2013)</ref>. This section discusses two solution methods for solving the inner maximization problem, which we refer to as the robust policy evaluation problem. Note that the convergence results in Section 3 are independent of the method used to solve this robust policy evaluation problem.</p><p>We now introduce two broad classes of ambiguity sets that are considered in the rest of this section. An ambiguity set P is (s, a)-rectangular <ref type="bibr" target="#b23">(Iyengar, 2005;</ref><ref type="bibr" target="#b40">Nilim &amp; El Ghaoui, 2005;</ref><ref type="bibr" target="#b29">Le Tallec, 2007)</ref> if it is a Cartesian product of sets P s,a ? ? S for each state s ? S and action a ? A, i.e.,</p><formula xml:id="formula_20">P = {p ? (? S ) S?A | p s,a ? P s,a , ?s ? S, a ? A},</formula><p>whereas an ambiguity set P is s-rectangular <ref type="bibr" target="#b71">(Wiesemann et al., 2013)</ref> if it is defined as a Cartesian product of sets</p><formula xml:id="formula_21">P s ? (? S ) A , i.e., P = {p ? (? S ) S?A | (p s,a</formula><p>) a?A ? P s , ?s ? S}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Value-iteration Approach</head><p>The optimum of the inner problem ( <ref type="formula" target="#formula_19">10</ref>) is attained by solving v ? k := min p?P v ? k ,p , which is commonly defined as the robust value function <ref type="bibr" target="#b23">(Iyengar, 2005;</ref><ref type="bibr" target="#b40">Nilim &amp; El Ghaoui, 2005;</ref><ref type="bibr" target="#b71">Wiesemann et al., 2013)</ref>. The robust value function v ? of a rectangular RMDP for a policy ? ? ? can be computed using the robust Bellman policy update T ? : R S ? R S <ref type="bibr" target="#b22">(Ho et al., 2021)</ref>. Specifically, for (s, a)rectangular RMDPs, the operator T ? is defined for each state s ? S</p><formula xml:id="formula_22">(T ? v) s := a?A ? sa ? max psa?Psa p ? sa (c sa + ?v) ,</formula><p>while for s-rectangular RMDPs, the the operator T ? is defined as</p><formula xml:id="formula_23">(T ? v) s := max ps?Ps a?A ? sa ? p ? sa (c sa + ?v) .</formula><p>For rectangular RMDPs, T ? is a contraction and the robust value function is the unique solution to v ? = T ? v ? . To solve the robust value function, the state-of-the-art method is to compute the sequence v ? t+1 = T ? v ? t with any initial values v ? 0 , which is similar to the policy evaluation for ordinary MDPs.</p><p>Note that computing the value function update v ? t to v ? t+1 requires solving an optimization problem. For the common ambiguity sets which are constrained by the support information and one additional convex constraint (e.g. L 1 -norm ball), one has to solve A convex optimization problems with O(S) variables and O(S) constraints for all s ? S at each iteration <ref type="bibr">(Grand-Cl?ment &amp; Kroer, 2021b)</ref>. Examples of common ambiguity sets are provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Gradient-based Approach</head><p>Unlike the extensive study of efficient value-based methods <ref type="bibr" target="#b23">(Iyengar, 2005;</ref><ref type="bibr" target="#b40">Nilim &amp; El Ghaoui, 2005;</ref><ref type="bibr" target="#b71">Wiesemann et al., 2013;</ref><ref type="bibr" target="#b46">Petrik &amp; Subramanian, 2014;</ref><ref type="bibr" target="#b21">Ho et al., 2018;</ref><ref type="bibr">Behzadian et al., 2021a)</ref>, there has been little work on designing gradient-based algorithms to compute the robust value function. In this subsection, a first gradient-based algorithm is proposed in Algorithm 2 to solve the inner-loop robust policy evaluation problem with a global convergence guarantee, under the assumptions of having rectangular and convex ambiguity set.</p><p>Note that the inner problem (10) could be regarded as a constrained non-concave maximization problem when the outer Algorithm 2 Projected gradient descent for the inner problem Input: Target fixed policy ? k , initial transition kernel p 0 , iteration time T k , step size sequence</p><formula xml:id="formula_24">{? t } t?0 for t = 0, 1, . . . , T k -1 do Set p t+1 ? Proj P (p t + ? t ? p J ? (? k , p t )). end for Output: p t ? ? {p 0 , . . . , p T k -1 } s.t. J ? (? k , p t ? ) = min t?{0,...,T k -1} J ? (? k , p t )</formula><p>policy ? k is fixed. Therefore, the most intuitive approach to solve (10) is to iteratively update the variable by following its ascent direction within the feasible set.</p><p>To maximize J ? (? k , p), Algorithm 2 iteratively computes the projected gradient step on p; that is, at iteration t, we compute</p><formula xml:id="formula_25">p t+1 = Proj P (p t + ? t ? p J ? (? k , p t )),<label>(11)</label></formula><p>which depends on the explicit form of P. Although (s, a)rectangular ambiguity sets can be viewed as a special case of s-rectangular ambiguity sets in general <ref type="bibr" target="#b71">(Wiesemann et al., 2013;</ref><ref type="bibr" target="#b22">Ho et al., 2021)</ref>, the implementations of the projected gradient step for two rectangular ambiguity sets are different.</p><p>For (s, a)-rectangular RMDPs, this projected gradient update can be decoupled to multiple projection updates that across state-action pairs such as</p><formula xml:id="formula_26">p t+1,sa = Proj Ps,a (p t,sa + ? t ? psa J ? (? k , p t )).</formula><p>Similarly, for s-rectangular RMDPs, the projected gradient update can be computed across states as</p><formula xml:id="formula_27">p t+1,s = Proj Ps (p t,s + ? t ? ps J ? (? k , p t )).</formula><p>If the ambiguity set is convex, the projected update can be implemented by solving a convex optimization problem with a quadratic objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inner Loop Global Optimality</head><p>To establish some general convergence properties of Algorithm 2, we first derive some continuity properties for the inner objective (10). Then, we prove the global optimality of Algorithm 2 by introducing a particular gradient dominance condition for the inner problem.</p><p>The next lemma derives the gradient for the inner loop. Lemma 4.1 (Differentiability). The partial derivative of J ? (?, p) has the explicit form for any (s, a, s ? ) ? S ?A?S,</p><formula xml:id="formula_28">?J ? (?, p) ?p sas ? = 1 1 -? d ?,p ? (s)? sa (c sas ? + ?v ?,p s ? ) . Moreover, J ? (?, p) is L p -Lipschitz in p with L p := ? SA (1-?) 2 .</formula><p>If a function is smooth, then a gradient update with a sufficiently small step size is guaranteed to improve the objective value. As it turns out, inner problem is ? p -smooth.</p><p>Lemma 4.2 (Smoothness). The function</p><formula xml:id="formula_29">J ? (?, p) is ? p - smooth in p with ? p := 2?S 2 (1-?) 3 .</formula><p>Due to the non-convexity of J ? , smoothness is not sufficient to establish the global convergence guarantee. We notice that the inner problem can be interpreted as having an adversarial nature to maximize the total reward (decision maker's cost) by selecting a proper transition kernel from the ambiguity set P <ref type="bibr" target="#b32">(Lim et al., 2013;</ref><ref type="bibr" target="#b17">Goyal &amp; Grand-Cl?ment, 2022</ref>).</p><p>Hence we leverage the idea from the convergence analysis of the classical policy gradient <ref type="bibr" target="#b0">(Agarwal et al., 2021)</ref> and derive our global convergence guarantee by first deriving the following inner problem's gradient dominance condition.</p><p>Lemma 4.3 (Gradient dominance). For any fixed ? ? ?, J ? (?, p) satisfies the following condition for any p ? P such that</p><formula xml:id="formula_30">J ? (?, p ? )-J ? (?, p) ? D 1 -? max p?P ? p -p, ? p J ? (?, p)? , where J ? (?, p ? ) := max p?P J ? (?, p).</formula><p>Using this notion of gradient dominance, we now give an iteration complexity bound for Algorithm 2.</p><p>Theorem 4.4. Let p t ? be the point obtained by Algorithm 2 and ? k &gt; 0 be the desired precision. Algorithm 2 with constant step size ?</p><formula xml:id="formula_31">= (1-?) 3 2?S 2 satisfies max p?P J ? (? k , p) -J ? (? k , p t ? ) ? ? k ,<label>(12)</label></formula><p>whenever</p><formula xml:id="formula_32">T ? 32?S 3 AD 2 (1 -?) 6 ? 2 k = O(? -2 k ).<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Scalability of Parametric Transition</head><p>In standard policy-gradient methods, one considers a family of policies parametrized by lower-dimensional parameter vectors to limit the number of variables when scaling to large problems. The projected gradient step in Algorithm 2 needs to update each p sas ? , which is difficult with large state and action spaces. To overcome this problem, we provide a new approach to transition probability parameterization. To the best of our knowledge, comparable parameterizations for the inner problem have not been studied previously.</p><p>We parameterize transition kernel with the following form for any (s, a, s ? ) ? S ? A ? S,</p><formula xml:id="formula_33">p ? sas ? := psas ? ? exp( ? ? ?(s ? ) ?sa ) k psak ? exp( ? ? ?(k) ?sa ) ,<label>(14)</label></formula><p>where ?(s</p><formula xml:id="formula_34">) := [? 1 (s), ? ? ? , ? m (s)</formula><p>] is a m-dimensional feature vector corresponding to the state s ? S, ? := (?, ?) is the collection of parameters, consisting of the strictly positive parameter ? := {? sa &gt; 0 | ?(s, a) ? S ? A} and the unconstrained parameter</p><formula xml:id="formula_35">? := [? 1 , ? ? ? , ? m ].</formula><p>The symbol p represents the nominal transition kernel, which is typically estimated from the empirical sample of state transitions.</p><p>The parameterization in ( <ref type="formula" target="#formula_33">14</ref>) is motivated by the form of the worst-case transition probabilities in RMDPs with KL-divergence constrained (s, a)-rectangular ambiguity sets <ref type="bibr" target="#b40">(Nilim &amp; El Ghaoui, 2005)</ref>. In fact, the worst-case transitions has an identical form to ( <ref type="formula" target="#formula_33">14</ref>) when linear approximation ? ? ?(s) is applied.</p><p>Then, the RMDPs problem then becomes,</p><formula xml:id="formula_36">min ??? max ??? J ? (?, ?),</formula><p>where ? is the ambiguity set for the parameter ?. In practice, ? could be constructed via distance-type constraint; that is, we consider</p><formula xml:id="formula_37">? := {? | D (??? c ) ? ?},</formula><p>where D(???) represents a distance function, such as L 1norm and L ? -norm, ? c is the user-specified empirical estimation of ?, and ? ? R ++ is a given radius.</p><p>To apply the gradient-based update on parameterized transition, we introduce the following lemma to derive the gradient of the inner problem, which is similar to the classical policy gradient theorem <ref type="bibr" target="#b65">(Sutton et al., 1999)</ref> Lemma 4.5. Consider a map ? ? p ? sas ? that is differentiable for any (s, a, s ? ) . Then, the partial gradient of</p><formula xml:id="formula_38">J ? (?, ?) on ? is ?J ? (?, ?) ?? = 1 1 -? E s?d ?,? ? a??s? s ? ?psa? ? log p ? sas ? ?? c sas ? + ?v ?,? s ? . (<label>15</label></formula><formula xml:id="formula_39">)</formula><p>Moreover, when parameterization (14) is applied, the score function</p><formula xml:id="formula_40">? log p ? sas ? ??</formula><p>has the analytical form:</p><formula xml:id="formula_41">? log p ? sas ? ?? i = ? i (s ? ) ? sa - j p ? saj ? ? i (j) ? sa ,<label>(16)</label></formula><formula xml:id="formula_42">? log p ? sas ? ?? sa = j p ? saj ? ? ? ?(j) ? 2 sa - ? ? ?(s ? ) ? 2 sa . (<label>17</label></formula><formula xml:id="formula_43">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we demonstrate the global convergence of DRPG and verify the robustness of the policies computed by DRPG. All algorithms are implemented in Python 3.8.8, and performed on a computer with an i7-11700 CPU with 16GB RAM. We use Gurobi 9.5.2 to solve any linear or quadratic optimization problems involved. To facilitate the reproducibility of the domains, the full source code, which was used to generate them, is available at https: //github.com/JerrisonWang/ICML-DRPG. The repository also contains CSV files with the precise specification of the RMDPs being solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>To demonstrate the convergence behavior, we test our algorithm on random GARNET MDPs, one of the widely-used benchmarks for RL algorithms, with three different problem sizes and two settings on the ambiguity sets: (s, a)and s-rectangular ambiguity sets. We then apply DRPG with inner parameterization on the practical inventory management problem to demonstrate its convergence and robustness.</p><p>Garnet MDPs are a class of abstract, but representative, finite MDPs that can be generated randomly <ref type="bibr" target="#b1">(Archibald et al., 1995)</ref>. A general GARNET G(|S|, |A|, b) is characterized by three parameters, where |S| is the number of states, |A| is the number of actions, and b is a branching factor which determines the number of possible next states for each state-action pair and controls the level of connectivity of underlying Markov chains.</p><p>In our inventory management problem <ref type="bibr" target="#b49">(Porteus, 2002;</ref><ref type="bibr" target="#b21">Ho et al., 2018)</ref>, a retailer orders, stores, and sells a single product over an infinite time horizon. The states and actions of the MDP represent the inventory levels and the order quantities in any given time period, respectively. The stochastic demands drive the stochastic state transitions. Any items held in inventory incur deterministic per-period holding costs. The retailer's goal is to find a policy that minimizes the total cost without knowing the exact transition kernel. More details on the problem settings, parameter choice, and feature selection are available in the Appendix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results and Discussion</head><p>In each of our GARNET problems, we compare the objective values of DRPG at different iterations with the optimal objective value J ? , which is computed by robust value iteration. Robust value iteration solves the robust Bellman equation by iteratively applying robust Bellman updates. For each setup of our GARNET problems, we solve 50 sample instances using both DRPG and robust value iteration. Figure <ref type="figure" target="#fig_0">1</ref> shows how the error (i.e., |J(? t , p t ) -J ? |) decreases when DRPG is performed. The upper and lower envelopes of the curves correspond to the 95 and 5 percentiles of the 50 samples, respectively. As expected, the error decreases to zero as the iteration step increases, which confirms the convergence behavior of DRPG. Similar results are observed for the s-rectangular case.</p><p>The results of our numerical study on the inventory management problem are provided in Figure <ref type="figure" target="#fig_1">2</ref>. We run DRPG with inner parametrization and compare the performance with the non-robust policy gradient. At each iteration t, we consider the policy ? t obtained by DRPG, and then we compute its worst-case expected return ?(? t ) = max p?P J(? t , p).</p><p>We do the same for the non-robust policy gradient method.</p><p>As we can see, DRPG obtains a policy that performs much better than the non-robust policy gradient, which demonstrates the robustness of our method. Different step sizes are chosen for DRPG, and they lead to different convergence behaviors; yet, in both cases, DRPGs outperform the non-robust policy gradient method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed a new policy optimization algorithm DRPG to solve RMDPs over general compact ambiguity sets. By selecting a suitable step size and an adaptive decreasing tolerance sequence, our algorithm converges to the global optimal policy under mild conditions. Moreover, we provide the first gradient-based solution method with a novel parameterization for solving the inner maximization. In our experiments, our results demonstrate the global convergence of DRPG and its reliable performance against the non-robust approach. Future work should address extensions to related models (e.g., distributionally RMDP) and scalable model-free algorithms.</p><p>Zhang, J., Xiao, P., Sun, R., and Luo, Z. A singleloop smoothed gradient descent-ascent algorithm for nonconvex-concave min-max problems. Advances in Neural Information Processing Systems, 33:7377-7389, 2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Examples of Common Ambiguity Sets</head><p>We discuss a particularly popular class of rectangular ambiguity sets which are defined by norm constraints bounding the distance of any feasible transition probabilities from a nominal (average) state distribution. It is usually referred to as an L 1 -constrained ambiguity set <ref type="bibr" target="#b46">(Petrik &amp; Subramanian, 2014;</ref><ref type="bibr" target="#b47">Petrik et al., 2016;</ref><ref type="bibr" target="#b22">Ho et al., 2021)</ref> or L ? -constrained ambiguity set <ref type="bibr" target="#b14">(Delgado et al., 2016;</ref><ref type="bibr">Behzadian et al., 2021a)</ref>. For such rectangular ambiguity sets, problem (10) can be solved efficiently by updating the value function with the robust Bellman operator T ? : R S ? R S . Below, we show forms of Bellman operator within different rectangular conditions. Example B. L 1 -constrained (s, a)-rectangular ambiguity sets generally assume the uncertain in transition probabilities is independent for each state-action pair and are defined as</p><formula xml:id="formula_44">P = ? s?S,a?A P s,a where P s,a := p ? ? S | ?p -psa ? 1 ? ? sa .</formula><p>For (s, a)-rectangular RMDPs constrained by the L 1 -norm, T ? is defined for each s ? S as</p><formula xml:id="formula_45">(T ? v ?,p ) s := a?A ? sa ? max psa?Psa p ? sa (c sa + ?v ?,p ) | ?p sa -psa ? 1 ? ? sa .</formula><p>Example C. L ? -constrained s-rectangular ambiguity sets generally assume the uncertain in transition probabilities is independent for each state-action pair and are defined as</p><formula xml:id="formula_46">P = ? s?S P s where P s := (p s1 , . . . , p sA ) ? (? S ) A | a?A ?p sa -psa ? ? ? ? s .</formula><p>For s-rectangular RMDPs constrained by the L ? -norm, T ? is defined for each s ? S as</p><formula xml:id="formula_47">(T ? v ?,p ) s := max ps?Ps a?A ? sa ? p ? sa (c sa + ?v ?,p ) | a?A ?p sa -psa ? ? ? ? s .</formula><p>There exists an unique solution to the Bellman equation v ?,p = T ? v ?,p , which is called the robust value function <ref type="bibr" target="#b23">(Iyengar, 2005;</ref><ref type="bibr" target="#b71">Wiesemann et al., 2013)</ref>. Specially, both L 1 -constrained ambiguity sets and L ? -constrained ambiguity sets are in fact polyhedral, which implies the worst-case transition probabilities in bellman updates can be computed as the solution of linear programs (LPs). Instead, RMDPs with other distance-type ambiguity sets, such as L 2 -constrained ambiguity sets can compute an Bellman update T ? by solving convex optimization problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Technical Lemmas and Definitions</head><p>As promised, we first introduce the definition of the Fr?chet sub-differential for general functions.</p><p>Definition D.1. The Fr?chet sub-differential of a function h :</p><formula xml:id="formula_48">X ? R at point x ? X is defined as the set ?h(x) = {u| lim inf x ? ?x h(x ? ) -h(x) -?u, x ? -x?/?x ? -x? ? 0}.</formula><p>Then, a common lemma is provided to illustrate a basic property that a smooth function satisfies.</p><p>Lemma D.2. Let h : X ? R be ?-smooth, then it is a ?-weakly convex function.</p><p>Proof of Lemma D.2. Let r(t) := h(x + t(x ? -x)), for any x, x ? ? X . The following holds true h(x) = r(0) and h(x ? ) = r(1).</p><p>Then, we observe that</p><formula xml:id="formula_49">h(x ? ) -h(x) = r(1) -r(0) = 1 0 ?r(t)dt,</formula><p>where</p><formula xml:id="formula_50">?r(t) = ?h(x + t(x ? -x)) ? (x ? -x).</formula><p>We complete the proof as</p><formula xml:id="formula_51">?h(x ? ) -h(x)-?h(x) ? (x ? -x)? ? 1 0 ?r(t)dt -?h(x) ? (x ? -x) ? 1 0 ?r(t) -?h(x) ? (x ? -x) dt = 1 0 ?h(x + t(x ? -x)) ? (x ? -x) -?h(x) ? (x ? -x) dt ? 1 0 ??h(x + t(x ? -x)) -?h(x)? ? ?(x ? -x)?dt ? 1 0 t??x ? -x? 2 dt = ? 2 ?x ? -x? 2 .</formula><p>For smooth function h(x), a point x ? X is defined as the first-order stationary point (FOSP) when 0 ? ?h(x). However, this notion of stationarity can be very restrictive when optimizing nonsmooth functions <ref type="bibr" target="#b33">(Lin et al., 2020)</ref>. In respond to this issue, an alternative measure of the first-order stationarity is proposed based on the construction of the Moreau envelope <ref type="bibr" target="#b66">(Thekumparampil et al., 2019)</ref>. Definition D.3. For function h : X ? R and ? &gt; 0, the Moreau envelope function of h is given by</p><formula xml:id="formula_52">h ? (x) := min x ? ?X h(x ? ) + 1 2? ?x -x ? ? 2 . (<label>18</label></formula><formula xml:id="formula_53">)</formula><p>Definition D.4. Given an ?-weakly convex function h, we say that x ? is an ?-first order stationary point (?-FOSP) if, ??h 1 2? (x ? )? ? ?, where h 1 2? (x) is the Moreau envelope function of h with parameter ? = 1 2? .</p><p>The following lemma connects ?-weakly convex function and its Moreau envelope function and will be useful in our proofs. Lemma D.5. <ref type="bibr">(Rockafellar &amp; Wets, 2009, Proposition 13.37</ref>) Assume h : X ? R is a ?-weakly convex function. Then, for ? &lt; ?, the Moreau envelope function h ? is C 1 -smooth with the gradient given by,</p><formula xml:id="formula_54">?h ? (x) = ? -1 x -arg min x ? h(x ? ) + 1 2? ?x -x ? ? 2 .</formula><p>Lemma D.6. Assume the function h : X ? R n ? R is ?-weakly convex and not differentiable at any point. Let ? &lt; 1 ? and x? = arg min x ? ?X h(x ? ) + 1 2? ?x -x ? ? 2 . Then we have</p><formula xml:id="formula_55">1 ? ? x? -x? = ??h ? (x)?. As a result, ??h ? (x)? ? ? implies?x ? -x? ? ?? and ?? ? ?h( x? ) such that -? ? N X ( x? ) + 1 ? ( x? -x) ? N X ( x? ) + 1 ? ? x? -x? B(1),</formula><p>where N X ( x? ) denotes the normal cone of X at x? and B(r) := {x ? R n : ?x? ? r}.</p><p>Proof. Here, we consider the function f (x) = h(x) + I X (x) where I is the indicate function and here f (x) := R n ? R.</p><p>The Moreau envelope function of f (x) is defined as</p><formula xml:id="formula_56">f ? (x) = min x ? ?R n h(x ? ) + I X (x ? ) + 1 2? ?x -x ? ? 2 , ?x ? R n = min x ? ?X h(x ? ) + 1 2? ?x -x ? ? 2 , ?x ? R n .</formula><p>The gradient of the moreau envelope f ? (x) is well defined (Lemma D.5) as</p><formula xml:id="formula_57">?f ? (x) = ? -1 (x -x) ,</formula><p>where</p><p>x : = arg min</p><formula xml:id="formula_58">x?R n ? ? ? ? h( x) + I X ( x) + 1 2? ?x -x? 2 :=?x( x) ? ? ? ? = arg min x?X h( x) + 1 2? ?x -x?<label>2</label></formula><p>Then, we consider the optimality of the function ? x (y) = h(y) + I X (y) + 1 2? ?x -y? 2 . Notice that, for any x ? R n , x is the optimal solution of ? x (y), then for some ? ? ?h( x), we have</p><formula xml:id="formula_59">? x ( x(x)) = min y?R n ? x (y) ?? ? x ( x(x)) = min y?R n h(y) + I X (y) + 1 2? ?x -y? 2 ?? 0 ? ? h(y) + I X (y) + 1 2? ?x -y? 2 y= x, ?? 0 ? ? + N X ( x) + 1 ? ( x -x) ?? -? ? N X ( x) + 1 ? ( x -x) . (<label>19</label></formula><formula xml:id="formula_60">)</formula><p>The above equation ( <ref type="formula" target="#formula_59">19</ref>) implies that, for any z ? R n ,</p><formula xml:id="formula_61">?? + 1 ? ( x -x) , z -x? ? 0 ?? ?-?, z -x? ? ? 1 ? ( x -x) , z -x?, ?z ? R n ?? ?-?, z -x? ? 1 ? ? x -x? ? ?z -x?, ?z ? R n ?? ?-?, z -x? ? 1 ? ? x -x?, ?z ? R n , ?z -x? = 1. (<label>20</label></formula><formula xml:id="formula_62">)</formula><p>The above Lemma D.6 implies that if ??h ? (x)? is small enough, then x is an approximate stationary point of the original constrained optimization min X h(x), by the definition of ?-FOSP. This motivates us to consider the optimality of the Moreau envelope function of ?(?) instead of the optimality of ?(?) directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proofs of Section 3</head><p>Proof of Lemma 3.1. First, we first derive the form of partial derivative for ? sa to obtain (6). While this form was known <ref type="bibr">(Agarwal et al., 2019)</ref>, we included a proof for the sake of completeness. Notice that,</p><formula xml:id="formula_63">?J ? (?, p) ?? sa = ??S ?v ?,p ? ?? sa ? ?.</formula><p>Then, we discuss ?v ?,p ? ??sa over two cases: ? ? = s and ? = s</p><formula xml:id="formula_64">?v ?,p ? ?? sa ?? =s = ? ?? sa ? ? ?? s ? ?S p ??s ? (c ??s ? + ?v ?,p s ? ) = ? ? ? ?? s ? ?S p ??s ? ?v ?,p s ? ?? sa ; ?v ?,p ? ?? sa ?=s = ? ?? sa ? ? ? ? ? ? ? ? ? s? s ? ?S p s?s ? (c s?s ? + ?v ?,p s ? ) q ?,p s? ? ? ? ? ? ? ? = q ?,p sa + ? ? ? s? s ? ?S p s?s ? ?v ?,p s ? ?? sa ;</formula><p>Condense the notation</p><formula xml:id="formula_65">? ? s? p s?s ? = p ? ss ? (1) (21) p ? ss ? (t -1) ? a ? s ? a p s ? as ?? = p ? ss ?? (t)<label>(22)</label></formula><p>Then, combining these two equations, we can obtain,</p><formula xml:id="formula_66">?v ?,p ? ?? sa ?? =s = ? s ? ? =s p ? ?s ? (1) ?v ?,p s ? ?? sa + ? s ? =s p ? ?s ? (1) ?v ?,p s ? ?? sa = ? 2 s ? ? =s p ? ?s ? (1) ? ? s ? ? s ?? ?S p s ? ?s ?? ?v ?,p s ?? ?? sa + ?p ? ?s (1) q ?,p sa + ? ? ? s? s ? ?S p s?s ? ?v ?,p s ? ?? sa = ?p ? ?s (1)q ?,p sa + ? 2 s ? p ? ?s ? (2) ?v ?,p s ? ?? sa = ?p ? ?s (1)q ?,p sa + ? 2 p ? ?s (2)q ?,p sa + ? 3 s ? p ? ?s ? (3) ?v ?,p s ? ?? sa = ? ? ? = ? t=1 ? t p ? ?s (t)q ?,p sa = ? t=0 ? t p ? ?s (t)q ?,p sa .</formula><p>The last equality is from the initial assumption ? ? = s, i.e., p ? ?s (0) = 0, and similarly for the case ? = s we have, ?v ?,p ? ?? sa ?=s = ? t=0 ? t p ? ss (t)q ?,p sa .</p><p>Hence, the partial derivative is obtained</p><formula xml:id="formula_67">?J ? (?, p) ?? sa = ? ? ?v ?,p s ?? sa ? s + ?? =s ?v ?,p ? ?? sa ? ?? ? = 1 1 -? ? ? ? ? ? ? ? (1 -?) ??S ? t=0 ? t ? ?p ? ?s (t) d ?,p ? (s) ? ? ? ? ? ? ? q ?,p sa .</formula><p>After deriving the form of partial derivative, we next prove that J ? (?, p) is L ? -Lipschitz in ? by showing the boundedness of ? ? J ? (?, p). The uniformly bounded cost c sas ? implies that, the absolute value of the action value function is bounded for any policy ? and transition kernel p,</p><formula xml:id="formula_68">|q ?,p sa | = E ?,p ? t=0 ? t c statst+1 | s 0 = s, a 0 = a ? ? t=0 ? t = 1 1 -? .</formula><p>Then, by vectorizing the ? as a SA-dimensional vector, we have</p><formula xml:id="formula_69">?? ? J ? (?, p)? = s,a ?J ? (?, p) ?? sa 2 = 1 1 -? a s (d ?,p ? (s)q ?,p sa ) 2 ? 1 (1 -?) 2 a s (d ?,? ? (s)) 2 ? ? A (1 -?) 2 ,</formula><p>where the last inequality holds since the discounted state occupancy measure satisfies</p><formula xml:id="formula_70">s (d ?,? ? (s)) 2 ? s (d ?,? ? (s)) 2 = 1.</formula><p>About the smoothness of J ? (?, p), it can be immediately proved by <ref type="bibr">(Agarwal et al., 2021, Lemma 54)</ref>. Finally, we turn to derive the continuity of ?(?). 1. We first show</p><formula xml:id="formula_71">?(?) is L ? -Lipschitz if J ? (?, p) is L ? -Lipschitz in ?.</formula><p>For any ? 1 , ? 2 ? ?, we let p 1 := arg max p?P J ? (? 1 , p) and p 2 := arg max p?P J ? (? 2 , p), then</p><formula xml:id="formula_72">?(? 1 ) -?(? 2 ) = max p?P J ? (? 1 , p) -max p?P J ? (? 2 , p) = J ? (? 1 , p 1 ) -J ? (? 2 , p 2 ) ? J ? (? 1 , p 1 ) -J ? (? 2 , p 1 ) ? L ? ?? 1 -? 2 ?.</formula><p>2. Then, <ref type="bibr">(Thekumparampil et al., 2019, Lemma 3)</ref> shows that,</p><formula xml:id="formula_73">?(?) = max p?P J ? (?, p) is ? ? -weakly convex if J ? (?, p) is ? ? -smooth.</formula><p>Combining the results of these two parts, this lemma is proved.</p><p>The following lemma is helpful throughout in the convergence analysis of policy optimization.</p><p>Lemma E.1. (The performance difference lemma) For any ?, ? ? ? ?, p ? P and ? ? ? S , we have</p><formula xml:id="formula_74">J ? (?, p) -J ? (? ? , p) = 1 1 -? s,a d ?,p ? (s)? sa q ? ? ,p sa -v ? ? ,p s . (<label>23</label></formula><formula xml:id="formula_75">)</formula><p>Generally, the term q ?,p sa -v ?,p s is defined as the advantage function.</p><p>Proof of Lemma E.1. By the definition of J ? (?, p) in (2), we have</p><formula xml:id="formula_76">J ? (?, p) -J ? (? ? , p) = s ? s v ?,p s -v ? ? ,p s .</formula><p>We introduce the advantage functionA ?,p sa := q ?,p sa -v ?,p s for convenience, and observe that, for any s ? S,</p><formula xml:id="formula_77">v ?,p s -v ? ? ,p s = v ?,p s - a ? sa s ? p sas ? c sas ? + ?v ? ? ,p s ? + a ? sa s ? p sas ? c sas ? + ?v ? ? ,p s ? -v ? ? ,p s = a ? sa s ? p sas ? (c sas ? + ?v ?,p s ? ) - a ? sa s ? p sas ? c sas ? + ?v ? ? ,p s ? + a ? sa s ? p sas ? c sas ? + ?v ? ? ,p s ? q ? ? ,p sa -v ? ? ,p s = ? a ? sa s ? p sas ? v ?,p s ? -v ? ? ,p s ? + a ? sa q ? ? ,p sa -v ? ? ,p s = ? a ? sa s ? p sas ? v ?,p s ? -v ? ? ,p s ? + a ? sa A ? ? ,p sa (a) = ? s ? p ? ss ? (1) ? s ?? p ? s ? s ?? (1) v ?,p s ?? -v ? ? ,p s ?? + a ? ? s ? a ? A ? ? ,p s ? a ? + a ? sa A ? ? ,p sa = a ? sa A ? ? ,p sa + ? s ? p ? ss ? (1) a ? ? s ? a ? A ? ? ,p s ? a ? + ? 2 s ? p ? ss ? (2) v ?,p s ? -v ? ? ,p s ? = ? ? ? = ? t=0 ? t s ? p ? ss ? (t) a ? ? s ? a ? A ? ? ,p s ? a ?</formula><p>, where p ? ss ? (t) is defined in (21), and (a) uses the recursion. We then obtain</p><formula xml:id="formula_78">J ? (?, p) -J ? (? ? , p) = s ? s v ?,p s -v ? ? ,p s = s ? s ? t=0 ? t s ? p ? ss ? (t) a ? ? s ? a ? A ? ? ,p s ? a ? = s ? s ? t=0 ? t ? s p ? ss ? (t) a ? ? s ? a ? A ? ? ,p s ? a ? = 1 1 -? s,a d ?,p ? (s)? sa A ? ? ,p sa .</formula><p>The last equality is obtained by the definition of state occupancy measure (See Definition 2.2).</p><p>Then, we introduce the gradient dominance condition for non-RMDPs proposed in <ref type="bibr" target="#b0">(Agarwal et al., 2021)</ref>, which will be used in the proof of Theorem 3.2.</p><p>Lemma E.2 (Gradient dominance). For any p ? P and ? ? ? S , we have</p><formula xml:id="formula_79">J ? (?, p) -J ? (? ? , p) ? D 1 -? max ??? (? -?) ? ? ? J ? (?, p), (<label>24</label></formula><formula xml:id="formula_80">)</formula><p>where ? ? is one of optimal policies over p, i.e., ? ? ? arg min ??? J ? (?, p).</p><p>Proof of Lemma E.2. From the Lemma E.1, we have</p><formula xml:id="formula_81">J ? (? ? , p) -J ? (?, p) = 1 1 -? s,a d ? ? ,p ? (s)? ? sa (q ?,p sa -v ?,p s ) = 1 1 -? s,a d ? ? ,p ? (s)? ? sa A ?,p sa ? 1 1 -? s,a d ? ? ,p ? (s)? ? sa min ? A ?,p s? = 1 1 -? s d ? ? ,p ? (s) min ? A ?,p s? .</formula><p>Then, we multiply -1 on both sides</p><formula xml:id="formula_82">0 ? J ? (?, p) -J ? (? ? , p) ? 1 1 -? s d ? ? ,p ? (s) -(min ? A ?,p s? ) = 1 1 -? s d ? ? ,p ? (s) max ? (-A ?,p s? ) = 1 1 -? s d ? ? ,p ? (s) d ?,p ? (s) d ?,p ? (s) max ? (-A ?,p s? ) ? 1 1 -? max s d ? ? ,p ? (s) d ?,p ? (s) s d ?,p ? (s) max ? (-A ?,p s? ) = d ? ? ,p ? d ?,p ? ? 1 1 -? s d ?,p ? (s) max ? (-A ?,p s? ) ? D 1 -? 1 1 -? s d ?,p ? (s) max ? (-A ?,p s? )<label>(25)</label></formula><p>The last inequality ( <ref type="formula" target="#formula_82">25</ref>) is due to the fact <ref type="bibr" target="#b25">(Kakade &amp; Langford, 2002</ref>)</p><formula xml:id="formula_83">d ? ? ,p ? d ?,p ? ? ? 1 1 -? d ? ? ,p ? ? ? ? D 1 -? .</formula><p>Notice that, the term in ( <ref type="formula" target="#formula_82">25</ref>) is equivalent to</p><formula xml:id="formula_84">1 1 -? s d ?,p ? (s) max ? (-A ?,p s? ) = max ??? 1 1 -? s,a d ?,p ? (s)? sa (-A ?,p sa ) = max ??? 1 1 -? s,a d ?,p ? (s)(? sa -? sa )(-A ?,p sa ) = max ??? 1 1 -? s,a d ?,p ? (s)(? sa -?sa )q ?,p sa = max ??? (? -?) ? ? ? J ? (?, p).</formula><p>The first equality holds since the optimal ? is a deterministic policy, i.e., for some ? ? A, ?s? = 1. The second step is supported by the property a ? sa A ?,p sa = 0. The third step follows as a (? sa -?sa )v ?,p s = 0 and the last equation is obtained from Lemma 3.1. Thus, we obtain that</p><formula xml:id="formula_85">J ? (?, p) -J ? (? ? , p) ? D 1 -? max ??? (? -?) ? ? ? J ? (?, p).</formula><p>Before providing the proof of Theorem 3.3, we introduce the below intermediate results which are helpful to our proof. We first introduce a common property for strongly convex functions.</p><p>Lemma E.3. Let h : X ? R be a ?-strongly convex function. Then for any x, y ? X , h(x), we have</p><formula xml:id="formula_86">h(y) -h(x) ? ?h(y) ? (y -x) - ? 2 ?x -y? 2 . (<label>26</label></formula><formula xml:id="formula_87">)</formula><p>Moreover, by taking y = x ? := arg min x?X h(x) as the minimum point of h(x), we get</p><formula xml:id="formula_88">h(x) -min x?X h(x) ? ? 2 ?x -x ? ? 2 . (<label>27</label></formula><formula xml:id="formula_89">)</formula><p>Proof of Lemma E.3. The inequality ( <ref type="formula" target="#formula_86">26</ref>) is a basic property that the strongly convex function hold, whereas the second inequality is obtained by the first-order optimality condition for the convex optimization problem, i.e., ?h(x ? ) ? (x -x ? ) ? 0.</p><p>We also need to introduce the following Danskin's Theorem, which helps prove our global convergence theorem.</p><p>Proposition E.4. <ref type="bibr">(Bertsekas, 2016, Proposition B.25</ref>) Let Z ? R m be a compact set, and let ? : R n ? Z ? R be continuous function and such that ?(?, z) : R n ? R is convex for each z ? Z. If ?(?, z) is differentiable for all z ? Z and ??(x, ?) is continuous on Z for each x, then for f (x) := max z?Z ?(x, z) and any</p><formula xml:id="formula_90">x ? R n , ?f (x) = conv ? x ?(x, z) | z ? arg max z?Z ?(x, z) .</formula><p>Notice that, Lemma 3.1 successfully proves that J ? (?, p) is ? ? -smooth and L ? -Lipschitz in ?. We want to emphasize that, these results also leads to the fact that J ? (?, p) is ? ? -weakly convex in ? by applying the Lemma D.2. Now, we are ready to prove Theorem 3.2 and Theorem 3.3.</p><p>Proof of Theorem 3.2. Since J ? (?, p) is non-concave in p and the ambiguity set P is only assumed as a compact set, there may exists multiple inner maxima. In particular, we denote p (k) as the k-th element of the set arg max p?P J ? (?, p) for fixed policy ? ? ?. Then, we apply Lemma E.2 to obtain</p><formula xml:id="formula_91">?(?) -?(? ? ) = J(?, p (k) ) -J(? ? , p ? ) = J(?, p (k) ) -min ??? max p?P J(?, p) ? J(?, p (k) ) -min ??? J(?, p (k) ) ? D 1 -? max ? (? -?) ? ? ? J(?, p (k) ).<label>(28)</label></formula><p>As we mentioned before this proof that, <ref type="bibr" target="#b28">(Kruger, 2003</ref>, Corollary 1.12.2). Let ?(?) := max p?P J? (?, p). Due to the convexity of J? (?, p) and the compactness of P, we can apply Proposition E.4 to attain</p><formula xml:id="formula_92">J ? (?, p) is ? ? -weakly convex in ?, it implies that J? (?, p) := J ? (?, p) + ?? 2 ??? 2 is convex in ? and ? ? J? (?, p) = ? ? J ? (?, p) + ? ? ?, referring to</formula><formula xml:id="formula_93">? ?(?) = conv ? ? J? (?, p) | p ? arg max p?P J? (?, p) =? ??(?) + ? ? ? = conv ? ? J ? (?, p) + ? ? ? | p ? arg max p?P J ? (?, p) =? ??(?) = conv ? ? J ? (?, p) | p ? arg max p?P J ? (?, p) .<label>(29)</label></formula><p>Assume the set arg max p?P J ? (?, p) contains N finite components, then, Proposition E.4 implies that, for any ? ? ?, there exists a sequence {? k } N k=1 with k ? k = 1 such that for any sub-gradient ? ? ??(?), it can be represented by a convex combination, i.e.,</p><formula xml:id="formula_94">? = N k=1 ? k ? ? J ? (?, p (k) ) p (k) ? arg max p?P J ? (?, p), k = 1, 2, ? ? ? , N</formula><p>Let us define ? = arg min ??? ?( ?) + ? ? ?? -?? 2 and Lemma D.6 implies that there exists ? ? ??( ?) such that it satisfies -? ? N X ( ?) + 2? ? ? ? -?? ? B(1). Then by assuming arg max p?P J ? ( ?, p) contains N finite components, there exists a specific sequence { ?k } N k=1 with k ?k = 1 such that</p><formula xml:id="formula_95">? = N k ?k ? ? J ? ( ?, p(k) ), p(k) ? arg max p?P J ? ( ?, p), k = 1, 2, ? ? ? , N<label>(30)</label></formula><p>Then, we have</p><formula xml:id="formula_96">?( ?) -?(? ? ) = N k=1 ?k (?( ?) -?(? ? )) ? D 1 -? N k=1 ?k max ??? ( ? -?) ? ? ? J( ?, p(k) ) ? D 1 -? N k=1 ?k ?max ??? ( ? -?), -? ? J( ?, p(k) )? ? D 1 -? N k=1 ?k ?( ?k -?), -? ? J( ?, p(k) )?,<label>(31)</label></formula><p>where ?k := arg max ??? ?( ??), -? ? J( ?, p(k) )?, and the second step is obtained by using (28). Since the cost function is bounded, i.e., 0 ? c sas ? ? 1 for any (s, a, s ? ) ? S ? A ? S, it implies the action value function q ?,p s,a and the partial gradient ? ? J(?, p) are non-negative. Since ?( ?) -?(? ? ) and the partial gradient ? ? J(?, p) are both non-negative, we can denote the maximum element of the vector sequence { ?k -?} N k=1 as ?sa which satisfies 0 &lt; ?sa ? 1. Then we get</p><formula xml:id="formula_97">(31) ? D 1 -? N k=1 ?k ?? sa e, -? ? J( ?, p(k) )? (32) = D 1 -? ?? sa e, N k=1 ?k -? ? J( ?, p(k) ) ? ? D 1 -? ?e, -?? ? D ? SA 1 -? ??? 1 2?? (?)?.<label>(33)</label></formula><p>Here, the last inequality follows from the definition of d( ?t ) which is mentioned in ( <ref type="formula" target="#formula_95">30</ref>) and e is all-one vector defined in Section 1. Remind that, Lemma 3.1 implies J ? (?, p) is L ? -Lipschitz in ?, and Lemma 3.1 also shows that ?(?) is L ? -Lipschitz. Thus, combine this Lipschitz property and the above equation ( <ref type="formula">32</ref>), we get</p><formula xml:id="formula_98">?(?) -?(? ? ) = ?(?) -?( ?) + ?( ?) -?(? ? ) ? D ? SA 1 -? ??? 1 2?? (?)? + ?(?) -?( ?) ? D ? SA 1 -? ??? 1 2?? (?)? + L ? ?? -?? = D ? SA 1 -? ??? 1 2?? (?)? + L ? ? ??? 1 2?? (?)? 2? ? ,<label>(34)</label></formula><p>where (34) holds by using arguments of Lemma D.5 and Lemma D.6.</p><p>Proof of Theorem 3.3. The proof is split into two parts. We first show our algorithm can reach a ?-first stationary point of ?(?) := max p?P J ? (?, p). Then, we next prove that this ?-first stationary point is close enough to the global minimum of ?(?).</p><p>We begin by defining a policy ?t = arg min ??? ?( ?) + ? ? ?? t -?? 2 where ?(?) has been well defined as the objective function J ? (?, p) taking the worst-case transition probability, then, we have</p><formula xml:id="formula_99">? 1 2?? (? t+1 ) = min ? ?(?) + ? ? ?? t+1 -?? 2 ? ?( ?t ) + ? ? ?? t+1 -?t ? 2 = ?( ?t ) + ? ? ?P ? (? t -?? ? J ? (? t , p t )) -P ? ( ?t )? 2 (a) ? ?( ?t ) + ? ? ?? t -?? ? J ? (? t , p t ) -?t ? 2 = ?( ?t ) + ? ? ?? t -?t ? 2 -2? ? ??? ? J ? (? t , p t ), ? t -?t ? + ? 2 ? ? ?? ? J ? (? t , p t )? 2 ? ? 1 2?? (? t ) + 2? ? ? ?( ?t ) -?(? t ) + ? t + ? ? 2 ?? t -?t ? 2 + ? 2 ? ? L 2 ? ,<label>(35)</label></formula><p>where (? t , p t ) is produced from the DRPG scheme at iteration step t and ? 1 2?? is the Moreau envelope fucntion of ? with parameter ? = 1 2?? . The inequality (a) follows the basic projection property <ref type="bibr" target="#b52">(Rockafellar, 1976)</ref>, i.e., for any</p><formula xml:id="formula_100">x 1 , x 2 ? R n , ?P X (x 1 ) -P X (x 2 )? ? ?x 1 -x 2 ?,</formula><p>and the last inequality holds due to the fact that J ? (?, p) is ? ? -weakly convex in ?, in the sense that, for the ?t ,</p><formula xml:id="formula_101">?( ?t ) ? J ? ( ?t , p t ) ? J ? (? t , p t ) + ?? ? J ? (? t , p t ), ?t -? t ? - ? ? 2 ?? t -?t ? 2 ? max p?P J ? (? t , p) ?(?t) -? t + ?? ? J ? (? t , p t ), ?t -? t ? - ? ? 2 ?? t -?t ? 2 .</formula><p>Next, by summing (35) up over t, we obtain,</p><formula xml:id="formula_102">? 1 2?? (? T -1 ) ? ? 1 2?? (? 0 ) + 2? ? ? T -1 t=0 ?( ?t ) -?(? t ) + ? t + ? ? 2 ?? t -?t ? 2 + T ? 2 ? ? L 2 ? .</formula><p>Rearranging this inequality yields</p><formula xml:id="formula_103">T -1 t=0 ?(? t ) -?( ?t ) - ? ? 2 ?? t -?t ? 2 ? ? 1 2?? (? 0 ) -? 1 2?? (? T -1 ) 2? ? ? + T ?L 2 ? 2 + T -1 t=0 ? t .<label>(36)</label></formula><p>Then, we have</p><formula xml:id="formula_104">?(? t ) -?( ?t ) - ? ? 2 ?? t -?t ? 2 = ?(? t ) + ? ? ?? t -? t ? 2 -?( ?t ) -? ? ?? t -?t ? 2 + ? ? 2 ?? t -?t ? 2 = ?(? t ) + ? ? ?? t -? t ? 2 -min ??? ?(?) + ? ? ?? t -?? 2 + ? ? 2 ?? t -?t ? 2 (a) ? ? ? ?? t -?t ? 2 = 1 4? ? ??? 1 2?? (? t )? 2 . (<label>37</label></formula><formula xml:id="formula_105">)</formula><p>The inequality (a) is obtained by the Lemma E.3 and the last equality in (37) is obtained by using the gradient of Moreau envelope function proposed in Lemma D.5, i.e.,</p><formula xml:id="formula_106">?? 1 2?? (? t ) = 2? ? ? t -arg max ??? ?(?) + ? ? ?? t -?? 2 = 2? ? (? t -?t ) .</formula><p>Let ?1 := arg min ??? ?( ?) + ? ? ?? 1 -?? 2 and ?2 := arg min ??? ?( ?) + ? ? ?? 2 -?? 2 for any ? 1 , ? 2 ? ?, and then we have</p><formula xml:id="formula_107">? 1 2?? (? 1 ) -? 1 2?? (? 2 ) = min ??? ?( ?) + ? ? ?? 1 -?? 2 -min ??? ?( ?) + ? ? ?? 2 -?? 2 (38) = ?( ?1 ) + ? ? ?? 1 -?1 ? 2 -?( ?2 ) -? ? ?? 2 -?2 ? 2 (39) ? ?( ?2 ) + ? ? ?? 1 -?2 ? 2 -?( ?2 ) -? ? ?? 2 -?2 ? 2 (40) = ? ? ?? 1 -?2 ? 2 -?? 2 -?2 ? 2 (41) ? 2? ? S.<label>(42)</label></formula><p>Plug ( <ref type="formula">38</ref>) and ( <ref type="formula" target="#formula_104">37</ref>) into ( <ref type="formula" target="#formula_103">36</ref>) and reach the first result that</p><formula xml:id="formula_108">T -1 t=0 ??? 1 2?? (? t )? 2 ? 4? ? S ? + 2T ?? ? L 2 ? + 4? ? T -1 t=0 ? t .<label>(43)</label></formula><p>Notice that, when the LHS is smaller than T ? 2 , i.e.,</p><formula xml:id="formula_109">T ? min t ??? 1 2?? (? t )? 2 ? T -1 t=0 ??? 1 2?? (? t )? 2 ? T ? 2 ,</formula><p>there exists one t such that ??? 1 2?? (? t)? ? ? and ? t is a ?-first order stationary point for ?(?). We finished the first part of the proof, and the next step is to show this approximate stationary point is close to the global minimum of ?(?). Formally, we next to show there exists some t such that</p><formula xml:id="formula_110">J ? (? ? , p ? ) -max p?P J ? (? t , p) = ?(? ? ) -?(? t ) ? ?.<label>(44)</label></formula><p>Applying the result in Theorem 3.2 for the iterative policy ? t , we have</p><formula xml:id="formula_111">J(? t , p t ) -min ??? max p?P J ? (?, p) ? ?(? t ) -?(? ? ) ? D ? SA 1 -? ??? 1 2?? (? t )? + L ? ? ??? 1 2?? (? t )? 2? ? . (<label>45</label></formula><formula xml:id="formula_112">)</formula><p>Combined this two parts, we finally state the global convergence guarantee. Equation ( <ref type="formula" target="#formula_111">45</ref>) implies that</p><formula xml:id="formula_113">min t?{0,??? ,T -1} J(? t , p t ) -min ??? max p?P J ? (?, p) ? 1 T T -1 t=0 J(? t , p t ) -min ??? max p?P J ? (?, p) ? 1 T T -1 t=0 (?(? t ) -?(? ? )) ? 1 T D ? SA 1 -? + L ? 2? ? T -1 t=0 ?? 1 2?? (? t )<label>(46)</label></formula><p>By Cauchy-Schwarz inequality, we can obtain</p><formula xml:id="formula_114">1 ? T T -1 t=0 ?? 1 2?? (? t ) ? T -1 t=0 ??? 1 2?? (? t )? 2 .</formula><p>We then multiply the constant D ? SA 1-? + L? 2?? on both sides and combine the inequality (43) to obtain the result that, if the iteration time T satisfies</p><formula xml:id="formula_115">(46) ? 1 ? T D ? SA 1 -? + L ? 2? ? T -1 t=0 ??? 1 2?? (? t )? 2 = 1 ? T D ? SA 1 -? + L ? 2? ? 4? ? S ? + 2T ?? ? L 2 ? + 4? ? T -1 t=0 ? t (a) ? 1 ? T D ? SA 1 -? + L ? 2? ? 4? ? S ? T ? + 2 ? T ?? ? L 2 ? + 4? ? ? 0 1 -? ? 1 ? T D ? SA 1 -? + L ? 2? ? 4? ? S ? T ? + 2 ? T ?? ? L 2 ? + 4? ? ? T 1 -? = ?</formula><p>where the inequality (a) holds due to the adaptive tolerance sequence, in the sense that,</p><formula xml:id="formula_116">T -1 t=0 ? t ? ? t=0 ? t ? ? 0 ? 1 + ? + ? 2 + ? ? ? ? ? 0 1 -? , which implies that T ? D ? SA 1-? + L? 2?? 4 4??S ? + 2?? ? L 2 ? + 4?? 1-? 2 ? 4 = O(? -4 ),</formula><p>then, we have</p><formula xml:id="formula_117">min t?{0,??? ,T -1} J(? t , p t ) -min ??? max p?P J ? (?, p) ? ?.</formula><p>Intuitively, we have</p><formula xml:id="formula_118">min t?{0,??? ,T -1} ?(? t ) -min ??? ?(?) ? ?.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussion on R-contamination ambiguity set</head><p>Recall that the R-contamination ambiguity set is a kind of (s, a)-rectangular set P = ? s?S,a?A P s,a where P s,a is defined as</p><formula xml:id="formula_119">P s,a := {(1 -R) psa + Rq | q ? ?(S)}, s ? S, a ? A. (<label>47</label></formula><formula xml:id="formula_120">)</formula><p>We have the following property of the R-contamination sets which illustrates their limited applicability.</p><p>Proposition F.1. Any RMDP with an R-contamination ambiguity set has the same optimal robust policy as a corresponding ordinary MDP with a reduced discount factor.</p><p>Proof of Proposition F.1. The robust optimal bellman operator of a RMDP with R-contamination ambiguity can be written as</p><formula xml:id="formula_121">(T r v) s : = min a?A max psa?Ps,a (c sa + ?p ? sa v) = min a?A c sa + ? (1 -R) p? sa v + R max s ? v s ? = min a?A c sa + ?(1 -R) p? sa v + R? max s ? v s ?</formula><p>Consider an ordinary MDP with the same reward function, transition kernel p := ( psa ) s?S,a?A ? (? S ) S?A and discount factor ?(1 -R). The optimal bellman operator is defined as</p><formula xml:id="formula_122">(T v) s := min a?A c sa + ?(1 -R) p? sa v.</formula><p>Then, we have that</p><formula xml:id="formula_123">(T r v) s = (T v) s + R??v? ?<label>(48)</label></formula><p>We define optimal value functions for T r and T as follow</p><formula xml:id="formula_124">T r v r = v r , T v nr = v nr ,</formula><p>and consider the value iteration with the given initial value functions v r first. Then we have that</p><formula xml:id="formula_125">T r v r = T v r + R??v r ? ? e ?? (T r ) 2 v r = (T ) 2 v r + R??T r v r ? ? e + R? 2 (1 -R)?v r ? ? e = (T ) 2 v r + R? + R? 2 (1 -R) ? ?v r ? ? ? e ?? (T r ) k v r = (T ) k v r + R? + R? 2 (1 -R) + R? 3 (1 -R) 2 + ? ? ? ? ?v r ? ? ? e = (T ) k v r + k n=1 R? k (1 -R) k-1 ? ?v r ? ? ? e.</formula><p>By taking the limitation for both side, we obtain</p><formula xml:id="formula_126">lim k?? (T r ) k v r = v r = lim k?? (T ) k v r + k n=1 R? n (1 -R) n-1 ? ?v r ? ? ? e = v nr + lim k?? k n=1 R? n (1 -R) n-1 ? ?v r ? ? ? e = v nr + lim k?? 1 -(?(1 -R)) k 1 -?(1 -R) ? ?v r ? ? ? e = v nr + 1 1 -?(1 -R) ? ?v r ? ? ? e.</formula><p>Each operation T r on v r will take the same optimal action due to the definition of v r , which implies operation T r on v r works with the same action is taken. This intuitive result shows that the RMDP with R-contamination ambiguity and its corresponding ordinary MDP with discount factor ?(1 -R) has the same optimal policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Proofs of Section 4</head><p>Proof of Lemma 4.1. Notice that ?J ? (?, p)</p><formula xml:id="formula_127">?p sas ? = ??S ?v ?,p ? ?p sas ? ? ?.</formula><p>Then, we discuss </p><formula xml:id="formula_128">?p sas ? = ? ? ? = ? t=1 ? t p ? ?s (t)? sa (c sas ? + ?v ?,p s ? ) = ? t=0 ? t p ? ?s (t)? sa (c sas ? + ?v ?,p s ? ) .</formula><p>The last equality is from the initial assumption ? ? = s, i.e., p ? ?s (0) = 0, and similarly for the case ? = s we have,</p><formula xml:id="formula_129">?v ?,p ? ?p sas ? ?=s = ? t=0 ? t p ? ss (t)? sa (c sas ? + ?v ?,p s ? ) .</formula><p>Hence, the partial derivative for transition probability is obtained</p><formula xml:id="formula_130">?J ? (?, p) ?p sas ? = 1 1 -? ? ? ? ? ? ? ? (1 -?) ??S ? t=0 ? t ? ?p ? ?s (t) d ?,p ? (s) ? ? ? ? ? ? ? ? sa (c sas ? + ?v ?,p s ? ) = 1 1 -? d ?,p ? (s)? sa (c sas ? + ?v ?,p s ? ) .</formula><p>The uniformly bounded cost c sas ? implies that, the absolute value of the value function is bounded for any policy ? and transition kernel p,</p><formula xml:id="formula_131">|v ?,p s | = E ?,p ? t=0 ? t c statst+1 | s 0 = s ? ? t=0 ? t = 1 1 -? , then we obtain that |? sa (c sas ? + ?v ?,p s ? )| ? |? sa | ? |c sas ? + ?v ?,p s ? | ? 1 + ?1 1 -? ? 1 1 -? .</formula><p>Therefore, by vectorizing the p as a S 2 A-dimensional vector, we have</p><formula xml:id="formula_132">?? p J ? (?, p)? = s,a,s ? ?J ? (?, p) ?p sas ? 2 = 1 1 -? s,a,s ? [d ?,p ? (s)? sa (c sas ? + ?v ?,p s ? )] 2 ? 1 (1 -?) 2 a,s ? s (d ?,? ? (s)) 2 ? ? SA (1 -?) 2 ,</formula><p>where the last inequality holds since the discounted state occupancy measure satisfies</p><formula xml:id="formula_133">s (d ?,? ? (s)) 2 ? s (d ?,? ? (s)) 2 = 1.</formula><p>Notice that, the objective function J ? (?, p) is twice differentiable on p. Hence, to prove the smoothness condition in Lemma 4.2 is equal to show that there exists a constant L ? ? such that</p><formula xml:id="formula_134">? 2 p J ? (?, p) ? LI ?? ?x ? R AS 2 , x ? ? 2 p J ? (?, p)x ? Lx ? x.</formula><p>Proof of Lemma 4.2. Denote p(?) := p + ?z ? P where ? ? R is a small scalar, whereas z ? (R S ) S?A . Since</p><formula xml:id="formula_135">J ? (?, p) = s ? s v ?,p(?) s</formula><p>with a known initial distribution ?, we turn to consider the derivative of value function v ?,p(?) s of the transition kernel p(?) over ?,</p><formula xml:id="formula_136">v ?,p(?) s = a ? sa s ? [p(?)] sas ? c sas ? + ? ? a ? sa s ? [p(?)] sas ? v ?,p(?) s ? ,<label>(49)</label></formula><p>First, let us simplify the form of v ?,p(?) s</p><p>. We define P (?) ? (? S ) S as the state transition kernel and for any s, s ? ? S,</p><formula xml:id="formula_137">[P (?)] ss ? = a ? sa [p(?)] sas ? ,<label>(50)</label></formula><p>and c(?) ? R S where for any s ? S,</p><formula xml:id="formula_138">|[c(?)] s | = a ? sa s ? [p(?)] sas ? c sas ? ? 1.<label>(51)</label></formula><p>Then, the value function (49) can be written as,</p><formula xml:id="formula_139">v ?,p(?) s = e ? s (I -?P (?)) -1 M (?) c(?),<label>(52)</label></formula><p>where</p><formula xml:id="formula_140">e s := [0, ? ? ? , 1, ? ? ? , 0]</formula><p>? ? R S is a vector whose s-th element is 1 and others are 0. By using power series expansion technique <ref type="bibr" target="#b0">(Agarwal et al., 2021;</ref><ref type="bibr" target="#b38">Mei et al., 2020)</ref>, we can obtain that,</p><formula xml:id="formula_141">M (?) = (I -?P (?)) -1 = ? t ? t P (?) t ,<label>(53)</label></formula><p>which implies that, for any s, s ? ? S, [M (?)] ss ? ? 0, and we have</p><formula xml:id="formula_142">e = 1 1 -? ? (I -?P (?)) e ?? M (?)e = 1 1 -? ? e,<label>(54)</label></formula><p>which implies each row of M (?) sums to 1/(1 -?). Therefore, for any vector x ? R S , we have</p><formula xml:id="formula_143">?M (?)x? ? = max i |[M (?)x] i | ? 1 1 -? ? ?x? ? .<label>(55)</label></formula><p>Taking derivative with respect to ? on v ?,p(?) s defined in (52),</p><formula xml:id="formula_144">?v ?,p(?) s ?? = e ? s M (?) ?c(?) ?? + ?e ? s M (?) ?P (?) ?? M (?)c(?).<label>(56)</label></formula><p>Then taking the twice derivative with respect to ?,</p><formula xml:id="formula_145">? 2 v ?,p(?) s (??) 2 = e ? s M (?) ? 2 c(?) (??) 2 + 2?e ? s M (?) ?P (?) ?? M (?) ?c(?) ?? + 2? 2 e ? s M (?) ?P (?) ?? M ?P (?) ?? M (?)c(?) + ?e ? s M (?) ? 2 P (?) (??) 2 M (?)c(?).<label>(57)</label></formula><p>Notice that, above two form of derivatives are obtained by using matrix calculus techniques, i.e., for any matrix A, B, U (x) and scalar x,</p><formula xml:id="formula_146">?AU (x)B ?x = A ?U (x) ?x B and ?U (x) -1 ?x = -U (x) -1 ?U (x) ?x U (x) -1 .</formula><p>So far, we get the derivative form of the value function. Then we'd like to bound</p><formula xml:id="formula_147">? 2 v ?,p(?) s (??) 2 ?=0</formula><p>.</p><p>For the first term in (57), we have,</p><formula xml:id="formula_148">e ? s M (?) ? 2 c(?) (??) 2 ?=0 ? e ? s 1 ? M (?) ? 2 c(?) (??) 2 ?=0 ? ? 1 1 -? ? ? 2 c(?) (??) 2 ?=0 ? = 0,<label>(58)</label></formula><p>where the last but one inequality is obtained from (55) and the last equality holds since for any ? ? R,</p><formula xml:id="formula_149">? 2 c(?) (??) 2 ? = max s ? ?? ?[c(?)] s ?? = max s ? ?? ? ( a ? sa s ? [p(?)] sas ? c sas ? ) ?? = max s ? ?? a ? sa s ? z sas ? c sas ? = 0.<label>(59)</label></formula><p>For the second term in (57), we have</p><formula xml:id="formula_150">e ? s M (?) ?P (?) ?? M (?) ?c(?) ?? ?=0 ? e ? s 1 ? M (?) ?P (?) ?? M (?) ?c(?) ?? ?=0 ? ? 1 1 -? ? ?P (?) ?? M (?) ?c(?) ?? ?=0 ? . (<label>60</label></formula><formula xml:id="formula_151">)</formula><p>According to (50), for any x ? R S and s ? S, we have,</p><formula xml:id="formula_152">?P (?) ?? x s = s ? a ? sa ?[p(?)] sas ? ?? x s ? ,</formula><p>and its ? ? norm can be upper bounded as</p><formula xml:id="formula_153">?P (?) ?? ?=0 x ? = max s s ? a ? sa ?[p(?)] sas ? ?? ?=0 x s ? ? max s s ? a ? sa |z sas ? | |x s ? | ? max s s ? a ? sa |z sas ? | ? ?x? ? = s ? a ? sa |z sas ? | ? ?x? ? ? s ? a ? sa max s,a,s ? |z sas ? | ? ?x? ? = max s,a,s ? |z sas ? | ? s ? ?x? ? ? S ? ?z? ? ? ?x? ? ? S ? ?z? 2 ? ?x? ?<label>(61)</label></formula><p>Similarly, for any ? ? R, we have</p><formula xml:id="formula_154">?c(?) ?? ? = max s ? ( a ? sa s ? [p(?)] sas ? c sas ? ) ?? = max s a ? sa s ? z sas ? c sas ? ? S ? ?z? 2 . (<label>62</label></formula><formula xml:id="formula_155">)</formula><p>Then, we obtain an upper bound of the second term,</p><formula xml:id="formula_156">e ? s M (?) ?P (?) ?? M (?) ?c(?) ?? ?=0 ? S 1 -? ? M (?) ?c(?) ?? ?=0 ? ? ?z? 2 ? S (1 -?) 2 ? ?c(?) ?? ?=0 ? ? ?z? 2 ? S 2 (1 -?) 2 ? ?z? 2 2 .<label>(63)</label></formula><p>For the third term of in (57), we can similarly bound it as</p><formula xml:id="formula_157">e ? s M (?) ?P (?) ?? M (?) ?P (?) ?? M (?)c(?) ?=0 ? M (?) ?P (?) ?? M (?) ?P (?) ?? M (?)c(?) ?=0 ? ? 1 1 -? ? S ? ?z? 2 ? 1 1 -? ? S ? ?z? 2 ? 1 1 -? = S 2 (1 -?) 3 ? ?z? 2 2 . (<label>64</label></formula><formula xml:id="formula_158">)</formula><p>Denote that, for any x ? R S ,</p><formula xml:id="formula_159">? 2 P (?) (??) 2 ?=0 x ? = max s s ? a ? sa ? 2 [p(?)] sas ? ?(?) 2 ?=0 x s ? = 0.<label>(65)</label></formula><p>Therefore, we combine (58), ( <ref type="formula" target="#formula_156">63</ref>), ( <ref type="formula" target="#formula_157">64</ref>) and ( <ref type="formula" target="#formula_159">65</ref>),</p><formula xml:id="formula_160">? 2 v ?,p(?) s (??) 2 ?=0 = e ? s M (?) ? 2 c(?) (??) 2 ?=0 + 2? 2 ? e ? s M (?) ?P (?) ?? M (?) ?P (?) ?? M (?)c(?) ?=0 + 2? ? e ? s M (?) ?P (?) ?? M (?) ?c(?) ?? ?=0 + ? ? e ? s M (?) ? 2 P (?) (??) 2 M (?)c(?) ?=0 ? 2? ? S 2 (1 -?) 2 ? ?z? 2 2 + 2? 2 ? S 2 (1 -?) 3 ?z? 2 2 = 2?S 2 (1 -?) 3 ? ?z? 2 2 .<label>(66)</label></formula><p>Then, for any y ? R AS 2 , we have</p><formula xml:id="formula_161">y ? ? 2 p J ? (?, p)y ? s ? s ? y ? ? 2 v ?,p s (?p) 2 y = s ? s ? ( y ?y? 2 ) ? ? 2 v ?,p s (?p) 2 ( y ?y? 2 ) ? ?y? 2 2 ? s ? s ? max ?z?2=1 ? 2 v ?,p s (?p) 2 z, z ? ?y? 2 2 = s ? s ? max ?z?2=1 ? 2 v ?,p(?) s (?p(?)) 2 ?=0 ?p(?) ?? , ?p(?) ?? ? ?y? 2 2 = s ? s ? max ?z?2=1 ? 2 v ?,p(?) s (??) 2 ?=0 ? ?y? 2 2 ? 2?S 2 (1 -?) 3 ? ?y? 2 2 . (<label>67</label></formula><formula xml:id="formula_162">)</formula><p>Proof of Lemma 4.3. By the definition of J ? (?, p), we have</p><formula xml:id="formula_163">J ? (?, p) -J ? (?, p ? ) = s ? s v ?,p s -v ?,p ? s .</formula><p>For any s ? S and p, p ? ? P, we have  </p><formula xml:id="formula_164">v ?,p s -v ?,p ? s = v ?,</formula><p>where t := 1 + arg min t?T -1 ?G ? (p t )?. Recall Lemma G.1, we showed that</p><formula xml:id="formula_166">?G ? (p t-1 )? ? 2? p (f ? ? -f ? (p 0 )) T ? 2? p (1 -?)T ,</formula><p>where the last inequality holds due to</p><formula xml:id="formula_167">v ? s = E ?,p ? t=0 ? t c statst+1 | s 0 = s ? ? t=0 ? t = 1 1 -? . (<label>72</label></formula><formula xml:id="formula_168">)</formula><p>If we set that</p><formula xml:id="formula_169">2? p (1 -?)T ? (1 -?)? 4D ? SA ?? T ? 32? p D 2 SA (1 -?) 3 ? 2 = O(? -2 ), then ?G ? (p t-1 )? ? (1 -?)? 4D ? SA .</formula><p>Hence, by applying the equation ( <ref type="formula">70</ref> We found that for all (s, a, s ? ) ? S ? A ? S, ? i will appear in the parametrization form of p ? sas ? . Hence we consider partial derivative of log p ? sas ? then.</p><formula xml:id="formula_170">? log p ? sas ? ?? i = ? ?? i log psas ? + ? ? ?(s ? ) ? sa - ? ?? i log k psak ? exp( ? ? ?(k) ? sa ) = ? i (s ? ) ? sa - k psak ? exp( ? ? ?(k) ?sa ) ? ?i(k) ?sa k psak ? exp( ? ? ?(k) ?sa ) = ? i (s ? ) ? sa - j psaj ? exp( ? ? ?(j) ?sa ) k psak ? exp( ? ? ?(k) ?sa ) ? ? i (j) ? sa = ? i (s ? ) ? sa - j p ? saj ? ? i (j) ? sa .</formula><p>Now we can obtain that ?J ? (?, ?)</p><formula xml:id="formula_171">?? i = 1 1 -? s d ?,? s a ? sa s ? p ? sas ? ? ? ? ? ? i (s ? ) ? sa - j p ? saj ? ? i (j) ? sa ? ? ? c sas ? + ?v ?,? s ? ? ? .</formula><p>Similarly we can derive the partial derivative on ? sa for any state-action pair (s, a). Interestingly, we notice that for </p><formula xml:id="formula_172">? ? ?(k) ? sa ) = k psak ? exp ? ? ?(k) ?sa ? ? ? ?(k) ? 2 sa k psak ? exp ? ? ?(k) ?sa - ? ? ?(s ? ) ? 2 sa = j psaj ? exp ? ? ?(j) ?sa k psak ? exp ? ? ?(k) ?sa ? ? ? ?(j) ? 2 sa - ? ? ?(s ? ) ? 2 sa = j p ? saj ? ? ? ?(j) ? 2 sa - ? ? ?(s ? ) ? 2 sa .</formula><p>Then we can obtain that ?J ? (?, ?)</p><formula xml:id="formula_173">?? sa = 1 1 -? d ?,? s ? ? sa ? s ? p ? sas ? ? ? ? ? j p ? saj ? ? ? ?(j) ? 2 sa - ? ? ?(s ? ) ? 2 sa ? ? ? c sas ? + ?v ?,? s ? ? ? .</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The error of value functions computed by DRPG for three Garnet problems with different sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. DRPG with parameterization v.s. Non-robust Policy Gradient on the Inventory Management Problem</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(p ? sas ? -p sas ? ) (c sas ? + ?v ?,p s ? sa (c sas ? + ?v ?,p s ? ) ? (p ? sas ? -p sas ? ) ? sa (c sas ? + ?v ?,p s ? ) ? (p sas ? -p sas ? ) ?J ? (?, p) ?p .which completes the proof. The first inequality (a) is obtained due to the fact that for any s ? S, sas ? -p sas ? ) (c sas ? + ?v ?,p s ? ) ? 0 holds under the s-rectangularity assumption. Now, we proceed to prove main theorem in section 4. Here we can define f ? (p) := J ? (?, p) for a fixed policy ? ? ? and define the gradient mappingG ? (p) := 1 ? (Proj P (p + ??f ? (p)) -p) . (68)Notice that P is convex and f ? (p) is ? p -smooth, then the following lemma can be derived directly using existing classic results:Lemma G.1.(Beck, 2017, Theorem 10.15) Let {p t } t?0 be the sequence generated by Algorithm 2 for solving the inner problem with the constant step size ? := 1 ?p , thenmin t?{0,??? ,T -1} ?G ? (p t )? ? 2? p (f ? ? -f ? (p 0 )) T (69)Proof of Theorem 4.4. It has been shown in Lemma 3 in<ref type="bibr" target="#b16">(Ghadimi &amp; Lan, 2016)</ref> that if ?G ? (p)? ? ?, then ?f ? (p + ) ? N P (p + ) + 2?B(1), (70) where p + := p + ?G ? (p), N P is the norm cone of the set P and B(r) := {x ? R n : ?x? ? r}. By the gradient dominance condition established in Lemma 4.3, min t?{0,??? ,T -1} {f ? (p ? ) -f ? (p t )} ? D 1 -? min t?{0,??? ,T -1} max p?P ? pp t , ?f ? (p t )? ? D 1 -? max p?P ? pp t, ?f ? (p t)? ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>p 1 , p 2 ? P, ?p 1 -p 2 ? ? ?p 1 ? + ?p 2 ? ? 2 provide the standard proof of Lemma 4.5.Proof of Lemma 4.5. We first show that the inner problem gradient form. Notice that,?J ? (?, ?)By condensing a ? sa p ? sas ? = p ?,? ss ? (1), we can obtain, consider the partial derivative on ? and ? separately. Notice thatE s?d ?,? ? E a??s? E s ? ?psa? ? log p ? sas ? ??sa c sas ? + ?v ?,? s ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(s, ?) ? = (s, a), ? log(p ? s?s ? ) ??sa = 0. Therefore, we can consider the case (s, ?) = (s,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>By condensing ? ? ?? p ??? ? = p ? ?? ? (1), we can obtain,</figDesc><table><row><cell>?v ?,p ? ?p sas ? ?? =s</cell><cell>= ?</cell><cell cols="2">?? ? =s</cell><cell>p ? ?? ? (1)</cell><cell cols="3">?v ?,p ?? ?p sas ?</cell><cell cols="2">+ ?</cell><cell cols="2">?? =s</cell><cell>p ? ?? ? (1)</cell><cell>?v ?,p ?? ?p sas ?</cell></row><row><cell></cell><cell>= ?</cell><cell cols="2">?? ? =s</cell><cell cols="2">p ? ?? ? (1) ? ?</cell><cell></cell><cell cols="3">? ? ?? ?</cell><cell cols="2">??? ?S</cell><cell>p ?? ?? ??</cell><cell>?v ?,p ??? ?p sas ?</cell></row><row><cell></cell><cell cols="5">+ ?p ? ?s (1) ? ?</cell><cell cols="3">? ? s?</cell><cell cols="2">?? ?S</cell><cell>p s?? ?</cell><cell>?v ?,p ?? ?p sas ?</cell><cell>+ ? sa (c sas ? + ?v ?,p s ? )</cell></row><row><cell></cell><cell cols="11">= ?p ? ?s (1)? sa (c sas ? + ?v ?,p s ? ) + ? 2</cell><cell>??</cell><cell>p ? ?? ? (2)</cell><cell>?v ?,p ?? ?p sas ?</cell></row><row><cell></cell><cell cols="11">= ?p ? ?s (1)? sa (c sas ? + ?v ?,p s ? ) + ? 2 p ? ?s (2)? sa (c sas ? + ?v ?,p s ? ) + ? 3</cell><cell>p ? ?? ? (3)</cell><cell>?v ?,p ??</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>??</cell></row><row><cell cols="12">?v ?,p ? ?p sas ? over two cases: ? ? = s and ? = s</cell></row><row><cell cols="2">?v ?,p ? ?p sas ? ?? =s</cell><cell>=</cell><cell cols="2">? ?p sas ?</cell><cell cols="3">? ? ??</cell><cell cols="2">?? ?S</cell><cell cols="2">p ??? ? (c ??? ? + ?v ?,p ?? ) = ?</cell><cell>? ? ??</cell><cell>?? ?S</cell><cell>p ??? ?</cell><cell>?v ?,p ?? ?p sas ?</cell><cell>;</cell></row><row><cell cols="2">?v ?,p ? ?p sas ? ?=s</cell><cell cols="2">= ?</cell><cell>? ? s?</cell><cell cols="2">?? ?S</cell><cell cols="2">p s?? ?</cell><cell cols="3">?v ?,p ?? ?p sas ?</cell><cell>+ ? sa (c sas ? + ?v ?,p s ? ) ;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(c sas ? + ?v ?,p s ? ) -v ?,p ? sas ? (c sas ? + ?v ?,p s ? ) -(p sas ? -p ? sas ? ) (c sas ? + ?v ?,p s ? ) + ?</figDesc><table><row><cell>s</cell><cell>p</cell><cell>-</cell><cell>? sa</cell><cell cols="2">p ? sas ? (c sas ? + ?v ?,p s ? ) +</cell><cell>? sa</cell><cell>p ? sas ? s</cell></row><row><cell></cell><cell></cell><cell>a</cell><cell>s ?</cell><cell></cell><cell>a</cell><cell>s ?</cell></row><row><cell>=</cell><cell cols="2">? sa</cell><cell></cell><cell cols="2">? sa</cell><cell cols="2">p ? sas ? (c sas ? + ?v ?,p s ? )</cell></row><row><cell>a</cell><cell></cell><cell>s ?</cell><cell></cell><cell>a</cell><cell>s ?</cell><cell></cell></row><row><cell>+</cell><cell></cell><cell>? sa</cell><cell cols="2">p ? sas ? (c sas ? + ?v ?,p s ? ) -</cell><cell>? sa</cell><cell cols="2">p ? sas ? c sas ? + ?v ?,p ? s ?</cell></row><row><cell></cell><cell>a</cell><cell></cell><cell>s ?</cell><cell>a</cell><cell></cell><cell>s ?</cell></row><row><cell>=</cell><cell cols="2">? sa</cell><cell></cell><cell></cell><cell></cell><cell>? sa</cell><cell>p ? sas ? v ?,p s ? -v ?,p ? s ?</cell></row><row><cell>a</cell><cell></cell><cell>s ?</cell><cell></cell><cell></cell><cell></cell><cell>a</cell><cell>s ?</cell></row><row><cell>= ? ? ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>=</cell><cell cols="2">? t</cell><cell>p ?? ss ? (t)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>t=0</cell><cell></cell><cell>s ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>p a ? ? s ? a ? s ?? (p s ? a ? s ?? -p ? s ? a ? s ?? ) (c s ? a ? s ?? + ?v ?,p s ?? ) .</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the anonymous reviewers for their comments. This work was supported, in part, by <rs type="funder">NSF</rs> grants <rs type="grantNumber">2144601</rs> and <rs type="grantNumber">1815275</rs>, the <rs type="funder">CityU Start-Up</rs> Grant (Project No. <rs type="grantNumber">9610481</rs>), the <rs type="funder">National Natural Science Foundation of China</rs> (Project No. <rs type="grantNumber">72032005</rs>), and <rs type="grantName">Chow Sang Sang Group Research Fund sponsored by Chow Sang Sang Holdings International Limited</rs> (Project No. <rs type="grantNumber">9229076</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eWkfJB9">
					<idno type="grant-number">2144601</idno>
				</org>
				<org type="funding" xml:id="_5d39u5h">
					<idno type="grant-number">1815275</idno>
				</org>
				<org type="funding" xml:id="_mMHFqt7">
					<idno type="grant-number">9610481</idno>
				</org>
				<org type="funding" xml:id="_gs8BQ29">
					<idno type="grant-number">72032005</idno>
					<orgName type="grant-name">Chow Sang Sang Group Research Fund sponsored by Chow Sang Sang Holdings International Limited</orgName>
				</org>
				<org type="funding" xml:id="_JjHMWee">
					<idno type="grant-number">9229076</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Experiment Details</head><p>Note that, in our simulations, we test our algorithm for both high connectivity (i.e., b = |S|) in s-rectangular case, and low connectivity (i.e., b = |S|/5) in (s, a)-rectangular case. We also apply DRPG on random RMDPs with L 1 -constrained s-rectangular ambiguity, which generally assumes the uncertain in transition probabilities is independent for each state-action pair and are defined as</p><p>We run DRPG with a sample size of 50 for 200 iteration times on Garnet problems with three sizes for the (s, a)-rectangular case and two medium sizes for the s-rectangular case. We record the absolute value of gaps between objective values of DRPG and robust value iteration at each iteration time step, and then plot the relative difference under the s-rectangular assumption in Figure <ref type="figure">3</ref>. The upper and lower envelopes of the curves correspond to the 95 and 5 percentiles of the 50 samples, respectively. From Figure <ref type="figure">3</ref>, we can obtain similar results with the (s, a)-rectangular case that DRPG converges to a nearly identical value computed by the value iteration computed by the robust value iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2. Details on the inventory management example</head><p>In our inventory management example, we present a specific example of this problem with eight states and three actions.</p><p>We draw the cost for each (s, a, s) ? S ? A ? S at random uniformly in [0, 1], and we fix a discount factor ? = 0.95. Below we give details about the nominal transitions and the parameter ?.</p><p>The feature function we use is the radial-type features which is introduced in <ref type="bibr" target="#b64">(Sutton &amp; Barto, 2018)</ref>, i.e., ? i (s) = exp -?s-ci? 2 2? 2 i . We define a two-dimension state feature with deterministic c i and ? i . Our parameters also share the same dimensions as these two features from our parameterization form.</p><p>The ambiguity set ? in our problem is simply chosen as a L 1 -norm constrained set, that is,</p><p>The updating step size for ? = (?, ?) on the inner problem are taken 0.01. For simplicity, we choose all elements of ? c as one and ? c := [0.4, 0.9] ? , and set ? ? = 1, ? ? = 1 in this problem. Other parameters are included in the published codes. Note that the instances for a larger number of states are constructed in the same fashion by adding some condition states.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the theory of policy gradient methods: Optimality, approximation, and distribution shift</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">98</biblScope>
			<biblScope unit="page" from="1" to="76" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the generation of Markov decision processes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Archibald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckinnon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="354" to="361" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust reinforcement learning using least squares policy iteration with provable performance guarantees</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Badrinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalathil</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="511" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">First-order methods in optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast algorithms for l ? -constrained s-rectangular robust MDPs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Behzadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimizing percentile criterion using robust MDPs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Behzadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Russel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AIStats)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Programming. Athena scientific</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the linear convergence of policy gradient methods for finite MDPs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Russo</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2386" to="2394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural actor-critic algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2471" to="2482" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Information-theoretic considerations in batch reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1042" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributionally robust optimization for sequential decision-making</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Haskell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2397" to="2426" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On algorithms for simple stochastic games</title>
		<author>
			<persName><forename type="first">A</forename><surname>Condon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in computational complexity theory</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="51" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Independent policy gradient methods for competitive reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Golowich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5527" to="5540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic model-based minimization of weakly convex functions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drusvyatskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="207" to="239" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time dynamic programming for Markov decision processes with imprecise probabilities</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>De Barros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">230</biblScope>
			<biblScope unit="page" from="192" to="223" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Twice regularized MDPs and the equivalence between robustness and regularization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Derman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Accelerated gradient methods for nonconvex nonlinear and stochastic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghadimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="99" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Robust Markov decision processes: Beyond rectangularity</title>
		<author>
			<persName><forename type="first">V</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grand-Cl?ment</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Mathematics of Operations Research</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">First-order methods for Wasserstein distributionally robust MDPs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grand-Cl?ment</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kroer</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scalable first-order methods for robust MDPs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grand-Cl?ment</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kroer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12086" to="12094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast bellman updates for robust MDPs</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wiesemann</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1979" to="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Partial policy iteration for l1-robust Markov decision processes</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wiesemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">275</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust dynamic programming</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="280" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What is local optimality in nonconvex-nonconcave minimax optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4880" to="4889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Approximately optimal approximate reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust modified policy iteration</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Schaefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INFORMS Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="396" to="410" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Actor-critic algorithms. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">V</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On fr?chet subdifferentials</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Kruger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="3325" to="3358" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Robust, risk-sensitive, and data-driven control of Markov decision processes</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Global convergence of multi-agent policy gradient in Markov potential games</title>
		<author>
			<persName><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Overman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Panageas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Piliouras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01969</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">First-order policy optimization for robust Markov decision process</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.10579</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reinforcement learning in robust markov decision processes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On gradient descent ascent for nonconvex-concave minimax problems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6083" to="6093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distributionally robust q-learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blanchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="13623" to="13643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stochastic recursive gradient descent ascent for stochastic nonconvexstrongly-concave minimax problems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="20566" to="20577" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Convergence of a stochastic gradient method with momentum for non-smooth nonconvex optimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johansson</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6630" to="6639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust MDPs with k-rectangular uncertainty</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1484" to="1509" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the global convergence rates of softmax policy gradient methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6820" to="6829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pattathil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1497" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust control of Markov decision processes with uncertain transition matrices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nilim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>El Ghaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="780" to="798" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Solving a class of non-convex min-max games using iterative first order methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nouiehed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Sample complexity of robust reinforcement learning with a generative model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Panaganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalathil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01506</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sample complexity of robust reinforcement learning with a generative model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Panaganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalathil</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="9582" to="9602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust reinforcement learning using offline data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Panaganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalathil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Approximate dynamic programming by minimizing distributionally robust bounds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Petrik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Raam: The benefits of robustness in approximating aggregated MDPs in reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Petrik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Safe policy improvement by minimizing robust baseline regret. Advances in Neural Information Processing Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Petrik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Policy gradient in Lipschitz Markov decision processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pirotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Restelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bascetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="283" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Porteus</surname></persName>
		</author>
		<title level="m">Foundations of stochastic inventory theory</title>
		<imprint>
			<publisher>Stanford University Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Markov decision processes: discrete stochastic dynamic programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Nonconvex min-max optimization: Applications, challenges, and recent theoretical advances</title>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nouiehed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="55" to="66" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Monotone operators and the proximal point algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="877" to="898" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Variational analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Wets</surname></persName>
		</author>
		<author>
			<persName><surname>.-B</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">317</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Reinforcement learning under model mismatch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pokutta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Russel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Baar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04870</idno>
		<title level="m">Robust constrained-MDPs: Soft-constrained robust policy optimization under model uncertainty</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Beyond confidence regions: Tight Bayesian ambiguity sets for robust MDPs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Risk-averse dynamic programming for Markov decision processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ruszczy?ski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical programming</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="235" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Approximate policy iteration schemes: a comparison</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scherrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1314" to="1322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Rectangular sets of probability measures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="528" to="541" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Distributionally robust optimal control and MDP modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="809" to="814" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Efficient algorithms for smooth minimax optimization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Strong and weak convexity of sets and functions</title>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Vial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="259" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12929</idno>
		<title level="m">The geometry of robust value functions</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Online robust reinforcement learning with model uncertainty</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7193" to="7206" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Policy gradient method for robust reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Robust Markov decision processes</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wiesemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rustem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="183" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Parametric regret in uncertain Markov decision processes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Decision and Control (CDC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Distributionally robust Markov decision processes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Reinforcement learning algorithms with function approximation: Recent advances and applications</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
