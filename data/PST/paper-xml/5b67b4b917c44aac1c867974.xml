<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RenderNet: A deep convolutional network for differentiable rendering from 3D shapes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thu</forename><surname>Nguyen-Phuoc</surname></persName>
							<email>t.nguyen.phuoc@bath.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Balaban</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yong-Liang</forename><surname>Yang</surname></persName>
							<email>y.yang@cs.bath.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Bath</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Lambda Labs</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Bath</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">RenderNet: A deep convolutional network for differentiable rendering from 3D shapes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">16285ABED618F39F82C72240780235B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional computer graphics rendering pipelines are designed for procedurally generating 2D images from 3D shapes with high performance. The nondifferentiability due to discrete operations (such as visibility computation) makes it hard to explicitly correlate rendering parameters and the resulting image, posing a significant challenge for inverse rendering tasks. Recent work on differentiable rendering achieves differentiability either by designing surrogate gradients for non-differentiable operations or via an approximate but differentiable renderer. These methods, however, are still limited when it comes to handling occlusion, and restricted to particular rendering effects. We present RenderNet, a differentiable rendering convolutional network with a novel projection unit that can render 2D images from 3D shapes. Spatial occlusion and shading calculation are automatically encoded in the network. Our experiments show that RenderNet can successfully learn to implement different shaders, and can be used in inverse rendering tasks to estimate shape, pose, lighting and texture from a single image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Rendering refers to the process of forming a realistic or stylized image from a description of the 3D virtual object (e.g., shape, pose, material, texture), and the illumination condition of the surrounding scene (e.g., light position, distribution, intensity). On the other hand, inverse rendering (graphics) aims at estimating these properties from a single image. The two most popular rendering methods, rasterization-based rendering and ray tracing, are designed to achieve fast performance and realism respectively, but not for inverse graphics. These two methods rely on discrete operations, such as z-buffering and ray-object intersection, to identify point visibility in a rendering scene, which makes these techniques non-differentiable. Although it is possible to treat them as non-differentiable renderers in computer vision tasks <ref type="bibr" target="#b0">[1]</ref>, inferring parameters, such as shapes or poses, from the rendered images using traditional graphics pipelines is still a challenging task. A differentiable renderer that can correlate the change in a rendered image with the change in rendering parameters therefore will facilitate a range of applications, such as vision-as-inverse-graphics tasks or imagebased 3D modelling and editing.</p><p>Recent work in differentiable rendering achieves differentiability in various ways. Loper and Black  [2]  propose an approximate renderer which is differentiable. Kato et al. [3]  achieve differentiability by proposing an approximate gradient for the rasterization operation. Recent work on image-based reconstruction uses differentiable projections of 3D objects onto silhouette masks as a surrogate for a rendered image of the objects <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Wu et al. [6] and Tulsiani et al. [7]  derive differentiable projective 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montr√©al, Canada.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>functions from normal, depth, and silhouette maps, but respectively can only handle orthographic projection, or needs multiple input images. These projections can then be used to construct an error signal for the reconstruction process. All of these approaches, however, are restricted to specific rendering styles (rasterization) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8]</ref>, input geometry types <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, or limiting output formats such as depth or silhouette maps <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Moreover, none of these approaches try to solve the problem from the network architecture design point of view. Recent progress in machine learning shows that network architecture plays an important role for improving the performance of many tasks. For example, in classification, ResNet <ref type="bibr" target="#b12">[13]</ref> and DenseNet <ref type="bibr" target="#b13">[14]</ref> have contributed significant performance gains. In segmentation tasks, U-Net <ref type="bibr" target="#b14">[15]</ref> proves that having short-cut connections can greatly improve the detail level of the segmentation masks. In this paper, we therefore focus on designing a neural network architecture suitable for the task of rendering and inverse rendering.</p><p>We propose RenderNet, a convolutional neural network (CNN) architecture that can be trained endto-end for rendering 3D objects, including object visibility computation and pixel color calculation (shading). Our method explores the novel idea of combining the ability of CNNs with inductive biases about the 3D world for geometry-based image synthesis. This is different from recent imagegenerating CNNs driven by object attributes <ref type="bibr" target="#b15">[16]</ref>, noise <ref type="bibr" target="#b16">[17]</ref>, semantic maps <ref type="bibr" target="#b17">[18]</ref>, or pixel attributes <ref type="bibr" target="#b18">[19]</ref>, which make very few assumption about the 3D world and the image formation process. Inspired by the literature from computer graphics, we propose the projection unit that incorporates prior knowledge about the 3D world, and how it is rendered, into RenderNet. The projection unit, through learning, is a differentiable approximation of the non-differentiable visibility computation step, making RenderNet an end-to-end system. Unlike non-learnt approaches in previous work, a learnt projection unit uses deep features instead of low-level primitives, making RenderNet generalize well to a variety of input geometries, robust to erroneous or low-resolution input, as well as enabling learning multi-style rendering with the same network architecture. RenderNet is differentiable and can be easily integrated to other neural networks, benefiting various inverse rendering tasks, such as novel-view synthesis, pose prediction, or image-based 3D shape reconstruction, unlike previous image-based inverse rendering work that can recover only part of the full 3D shapes <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>We choose the voxel presentation of 3D shapes for its regularity and flexibility, and its application in visualizing volumetric data such as medical images. Although voxel grids are traditionally memory inefficient, computers are becoming more powerful, and recent work also addresses this inefficiency using octrees <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, enabling high-resolution voxel grids. In this paper, we focus on voxel data, and leave other data formats such as polygon meshes and unstructured point clouds as possible future extensions. We demonstrate that RenderNet can generate renderings of high quality, even from low-resolution and noisy voxel grids. This is a significant advantage compared to mesh renderers, including more recent work in differentiable rendering, which do not handle erroneous inputs well.</p><p>By framing the rendering process as a feed-forward CNN, RenderNet has the ability to learn to express different shaders with the same network architecture. We demonstrate a number of rendering styles ranging from simple shaders such as Phong shading <ref type="bibr" target="#b23">[24]</ref>, suggestive contour shading <ref type="bibr" target="#b24">[25]</ref>, to more complex shaders such as a composite of contour shading and cartoon shading <ref type="bibr" target="#b25">[26]</ref> or ambient occlusion <ref type="bibr" target="#b26">[27]</ref>, some of which are time-consuming and computationally expensive. RenderNet also has the potential to be combined with neural style transfer to improve the synthesized results, or other complex shaders that are hard to define explicitly.</p><p>In summary, the proposed RenderNet can benefit both rendering and inverse rendering: RenderNet can learn to generate images with different appearance, and can also be used for vision-as-inverse-graphics tasks. Our main contributions are threefold.</p><p>‚Ä¢ A novel convolutional neural network architecture that learns to render in different styles from a 3D voxel grid input. To our knowledge, we are the first to propose a neural renderer for 3D shapes with the projection unit that enables both rendering and inverse rendering. ‚Ä¢ We show that RenderNet generalizes well to objects of unseen category and more complex scene geometry. RenderNet can also produce textured images from textured voxel grids, where the input textures can be RGB colors or deep features computed from semantic inputs. ‚Ä¢ We show that our model can be integrated into other modules for applications, such as texturing or image-based reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Our work is related to three categories of learning-based works: image-based rendering, geometrybased rendering and image-based shape reconstruction. In this section, we review some landmark methods that are closely related to our work. In particular, we focus on neural-network-based methods.</p><p>Image-based rendering There is a rich literature of CNN-based rendering by learning from images. Dosovitskiy et al. <ref type="bibr" target="#b15">[16]</ref> create 2D images from low-dimensional vectors and attributes of 3D objects. Cascaded refinement networks <ref type="bibr" target="#b17">[18]</ref>, and Pix2Pix <ref type="bibr" target="#b27">[28]</ref> additionally condition on semantic maps or sketches as inputs. Using a model that is more deeply grounded in computer graphics, DeepShading <ref type="bibr" target="#b18">[19]</ref> learns to create images with high fidelity and complex visual effects from per-pixel attributes. DC-IGN <ref type="bibr" target="#b28">[29]</ref> learns disentangled representation of images with respect to transformations, such as out-of-plane rotations and lighting variations, and thus is able to edit images with respect to these factors. Relevant works on novel 3D view synthesis <ref type="bibr" target="#b29">[30]</ref> leverage category-specific shape priors and optical flow to deal with occlusion/disocclusion. While these methods yield impressive results, we argue that geometry-based methods, which make stronger assumptions about the 3D world and how it produces 2D images, will be able to perform better in certain tasks, such as out-of-plane rotation, image relighting, and shape texturing. This also coincides with Rematas et al. <ref type="bibr" target="#b30">[31]</ref>, Yang et al. <ref type="bibr" target="#b31">[32]</ref> and Su et al. <ref type="bibr" target="#b32">[33]</ref> who use strong 3D priors to assist the novel-view synthesis task.</p><p>Geometry-based rendering Despite the rich literature in rendering in computer graphics, there is a lot less work using differentiable rendering techniques. OpenDR <ref type="bibr" target="#b1">[2]</ref> has been a popular framework for differentiable rendering. However, being a more general method, it is more strenuous to be integrated into other neural networks and machine learning frameworks. Kato et al. <ref type="bibr" target="#b2">[3]</ref> approximate the gradient of the rasterization operation to make the rendering differentiable. However, this method is limited to rasterization-based rendering, making it difficult to represent more complex effects that are usually achieved by ray tracing such as global illumination, reflection, or refraction.</p><p>Image-based 3D shape reconstruction Reconstructing 3D shape from 2D image can be treated as estimating the posterior of the 3D shape conditioned on the 2D information. The prior of the shape could be a simple smoothness prior or a prior learned from 3D shape datasets. The likelihood term, on the other hand, requires estimating the distribution of 2D images given the 3D shape. Recent work has been using 2D silhouette maps of the images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. While this proves effective, silhouette images contain little information about the shape. Hence a large number of images or views of the object is required for the reconstruction task. For normal maps and depth maps of the shape, Wu et al. <ref type="bibr" target="#b5">[6]</ref> derive differentiable projective functions assuming orthographic projection. Similarly, Tulsiani et al. <ref type="bibr" target="#b6">[7]</ref> propose a differentiable formulation that enables computing gradients of the 3D shape given multiple observations of depth, normal or pixel color maps from arbitrary views. In our work, we propose RenderNet as a powerful model for the likelihood term. To reconstruct 3D shapes from 2D images, we do MAP estimation using our trained rendering network as the likelihood function, in addition to a shape prior that is learned from a 3D shape dataset. We show that we can recover not only the pose and shape, but also lighting and texture from a single image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>The traditional computer graphics pipeline renders images from the viewpoint of a virtual pin-hole camera using a common perspective projection. The viewing direction is assumed to be along the negative z-axis in the camera coordinate system. Therefore, the 3D content defined in the world coordinate system needs to be transformed into the camera coordinate system before being rendered. The two currently popular rendering methods, rasterization-based rendering and ray tracing, procedurally compute the color of each pixel in the image with two major steps: testing visibility in the scene, and computing shaded color value under an illumination model.</p><p>RenderNet jointly learns both steps of the rendering process from training data, which can be generated using either rasterization or ray tracing. Inspired by the traditional rendering pipeline, we also adopt the world-space-to-camera-space coordinate transformation strategy, and assume that the camera is axis-aligned and looks along the negative z-axis of the volumetric grid that discretizes the input shape. Instead of having the network learn operations which are differentiable and easy to implement, such as rigid-body coordinate transformation or the interaction of light with surface normals (e.g. assuming a Phong illumination model <ref type="bibr" target="#b23">[24]</ref>), we provide most of them explicitly to the network. This allows RenderNet to focus its capacity on more complex aspects of the rendering task, such as recognizing visibility and producing shaded color.</p><p>RenderNet receives a voxel grid as input, and applies a rigid-body transformation to convert from the world coordinate system to the camera coordinate system. The tranformed input, after being trilinearly sampled, is then fed to a CNN with a projection unit to produce a rendered 2D image. RenderNet consists of 3D convolutions, a projection unit that computes visibility of objects in the scene and projects them onto 2D feature maps, followed by 2D convolutions to compute shading.</p><p>We train RenderNet using a pixel-space loss between the target image and the output. Optionally, the network can produce normal maps of the 3D input which can be combined with light sources to illuminate the scene. While the projection unit can easily incorporate orthographic projections, the 3D convolutions can morph the scene and allows for perspective camera views. In future versions of RenderNet, perspective transformation may also be explicitly incorporated into the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rotation and resampling</head><p>The transformed input via rigid body motion ensures that the camera is always in the same canonical pose relative to the voxel grid being rendered. The transformation is parameterized by the rotation around the y-axis and z-axis, which corresponds to the azimuth and elevation, and a distance R that determines the scaling factor, i.e., how close the object is to the camera. We embedded the input voxel grid into a larger grid to make sure the object is not cut off after rotation. The total transformation therefore includes scaling, rotation, translation, and trilinear resampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Projection unit</head><p>The input of RenderNet is a voxel grid V of dimension H V √óW V √óD V √óC V (corresponding to height, width, depth, and channel), and the output is an image I of dimension H I √óW I √óC I (corresponding to height, width and channel). To bridge the disparity between the 3D input and 2D output, we devise a novel projection unit. The design of this unit is straightforward: it consists of a reshaping layer, and a multilayer perceptron (MLP). Max pooling is often used to flatten the 3D input across the depth dimension <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, but this can only create the silhouette map of the 3D shape. The projection unit, on the other hand, learns not only to perform projection, but also to determine visibility of different parts of the 3D input along the depth dimension after projection.</p><p>For the reshaping step of the unit, we collapse the depth dimension with the feature maps to map the incoming 4D tensor to a 3D squeezed tensor V with dimension W √óH√ó(D ‚Ä¢ C). This is immediately followed by an MLP, which is capable of learning more complex structure within the local receptive field than a conventional linear filter <ref type="bibr" target="#b12">[13]</ref>. We apply the MLP on each (D ‚Ä¢ C) vector, which we implement using a 1√ó1 convolution in this project. The reshaping step allows each unit of the MLP to access the features across different channels and the depth dimension of the input, enabling the network to learn the projection operation and visibility computation along the depth axis. Given the squeezed 3D tensor V with (D ‚Ä¢ C) channels, the projection unit produces a 3D tensor with K channels as follows:</p><formula xml:id="formula_0">I i,j,k = f dc w k,dc ‚Ä¢ V i,j,dc + b k<label>(1)</label></formula><p>where i, j are pixel coordinates, k is the image channel, dc is the squeezed depth channel, where d and c are the depth and channel dimension of the original 4D tensor respectively, and f is some non-linear function (parametric ReLU in our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Extending RenderNet</head><p>We can combine RenderNet with other networks to handle more rendering parameters and perform more complex tasks such as shadow rendering or texture mapping. We model a conditional renderer p(I | V, h) where h can be extra rendering parameter such as lights, or spatially-varying parameters such as texture.</p><p>Here we demonstrate the extensibility of RenderNet using the example of the Phong illumination model <ref type="bibr" target="#b23">[24]</ref>. The per-pixel shaded color for the images is calculated by S = max(0, l ‚Ä¢ n + a), where l is the unit light direction vector, n is the normal vector, whose components are encoded by the RGB channels of the normal map, and a is an ambient constant. Shading S and albedo map A are further combined to create the final image I based on I = A S <ref type="bibr" target="#b33">[34]</ref>. This is illustrated in Section 4.1, where we combine the albedo map and normal map rendered by the combination of a texture-mapping network and RenderNet to render shaded images of faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To explore the generality of RenderNet, we test our method on both computer graphics and vision tasks. First, we experiment with different rendering tasks with varying degree of complexity, including challenging cases such as texture mapping and surface relighting. Second, we experiment with vision applications such as image-based pose and shape reconstruction.</p><p>Datasets We use the chair dataset from ShapeNet Core <ref type="bibr" target="#b34">[35]</ref>. Apart from being one of the categories with the largest number of data points (6778 objects), the chair category also has large intra-class variation. We convert the ShapeNet Dataset to 64√ó64√ó64 voxel grids using volumetric convolution <ref type="bibr" target="#b35">[36]</ref>. We randomly sampled 120 views of each object to render training images at 512√ó512 resolution.</p><p>The elevation and azimuth are uniformly sampled between <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">170]</ref> degrees and [0, 359] degrees, respectively. Camera radius are set at 3 to 6.3 units from the origin, with the object's axis-aligned bounding box normalized to 1 unit length. For the texture mapping tasks, we generate 100,000 faces from the Basel Face Dataset <ref type="bibr" target="#b36">[37]</ref>, and render them with different azimuths between [220, 320] degrees and elevations between [70, 110] degrees. We use Blender3D to generate the Ambient Occlusion (AO) dataset, and VTK for the other datasets. For the contour dataset, we implemented the pixel-based suggestive contour <ref type="bibr" target="#b24">[25]</ref> algorithm in VTK.</p><p>Training We adopt the patch training strategy to speed up the training process in our model. We train the network using random spatially cropped samples (along the width and height dimensions) from the training voxel grids, while keeping the depth and channel dimensions intact. We only use the full-sized voxel grid input during inference. The patch size starts as small as 1/8 of the full-sized grid, and progressively increases towards 1/2 of the full-sized grid at the end of the training.</p><p>We train RenderNet using a pixel-space regression loss. We use mean squared error loss for colored images, and binary cross entropy for grayscale images. We use the Adam optimizer <ref type="bibr" target="#b37">[38]</ref>, with a learning rate of 0.00001.</p><p>Code, data and trained models will be available at: https://github.com/thunguyenphuoc/ RenderNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning to render and apply texture</head><p>Figure <ref type="figure" target="#fig_1">2</ref> shows that RenderNet is able to learn different types of shaders, including Phong shading, contour line shading, complex multi-pass shading (cartoon shading), and a ray-tracing effect (Ambient Occlusion) with the same network architecture. RenderNet was trained on datasets for each of these shaders, and the figure shows outputs generated for unseen test 3D shapes. We report the PSNR score for each shader in Figure <ref type="figure" target="#fig_5">5</ref>.</p><p>RenderNet generalizes well to shapes of unseen categories. While it was trained on chairs, it can also render non-man-made objects such as the Stanford Bunny and Monkey (Figure <ref type="figure" target="#fig_3">3</ref>). The method also works very well when there are multiple objects in the scene, suggesting the network recognizes the visibility of the objects in the scene.</p><p>RenderNet can also handle corrupted or low-resolution volumetric data. For example, Figure <ref type="figure" target="#fig_3">3</ref> shows that the network is able to produce plausible renderings for the Bunny when the input model was artificially corrupted by adding 50% random noise. When the input model is downsampled (here we linearly downsampled the input by 50%), RenderNet can still render a high-resolution image with smooth details. This is advantageous compared to the traditional computer graphics mesh rendering, which requires a clean and high-quality mesh in order to achieve good rendered results.</p><p>It is also straightforward to combine RenderNet with other modules for tasks such as mapping and rendering texture (Figure <ref type="figure" target="#fig_4">4</ref>). We create a texture-mapping network to map a 1D texture vector representation (these are the PCA coefficients for generating albedo texture using the BaselFace dataset) to a 3D representation of the texture that has the same width, height and depth as the shape input. This output is concatenated along the channel dimension with the input 3D shape before given RenderNet to render the albedo map. This is equivalent to assigning a texture value to the corresponding voxel in the binary shape voxel grid. We also add another output branch of 2D convolutions to RenderNet to render the normal map. The albedo map and the normal map produced by RenderNet are then combined to create shaded renderings of faces as described in Section 3.3. See Section 2.3 in the supplementary document for network architecture details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Architecture comparison</head><p>In this section, we compare RenderNet with two baseline encoder-decoder architectures to render Phong-shaded images. Similar to RenderNet, the networks receive the 3D shape, pose, light position and light intensity as input. In contrast to RenderNet, the 3D shape given to the alternative network is in the canonical pose, and the networks have to learn to transform the 3D input to the given pose. The first network follows the network architecture by Dosovitskiy et al. <ref type="bibr" target="#b15">[16]</ref>, which consists of a   series of fully-connected layers and up-convolution layers. The second network is similar but has a deeper decoder than the first one by adding residual blocks. For the 3D shape, we use an encoding network to map the input to a latent shape vector (refer to Section 2.2 in the supplementary document for details). We call these two networks EC and EC-Deep, respectively. These networks are trained directly on shaded images with a binary cross-entropy loss, using the chair category from ShapeNet. RenderNet, on the other hand, first renders the normal map, and combines this with the lighting input to create the shaded image using the shading equation in Section 3.3.</p><p>As shown in Figure <ref type="figure" target="#fig_5">5</ref>, the alternative model (here we show the EC model) fails to produce important details of the objects and achieves lower PSNR score on the Phong-shaded chair dataset. More importantly, this architecture "remembers" the global structure of the objects and fails to generalize to objects of unseen category due to the use of the fully connected layers. In contrast, our model is better for rendering tasks as it generalizes well to different categories of shapes and scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Shape reconstruction from images</head><p>Here we demonstrate that RenderNet can be used for single-image reconstruction. It achieves this goal via an iterative optimization that minimizes the following reconstruction loss: minimize z,Œ∏,œÜ,Œ∑</p><formula xml:id="formula_1">I -f (z, Œ∏, œÜ, Œ∑) 2<label>(2)</label></formula><p>where I is the observed image and f is our pre-trained RenderNet. z is the shape to reconstruct, Œ∏ and Œ∑ are the pose and lighting parameters, and œÜ is the texture variable. In essence, this process maximizes the likelihood of observing the image I given the shape z. However, directly minimizing this loss often leads to noisy, unstable results (shown in Figure <ref type="figure" target="#fig_1">2</ref> in the supplementary document). In order to improve the reconstruction, we use a shape prior for regularizing the process -a pre-trained 3D auto-encoder similar to the TL-embedding network <ref type="bibr" target="#b38">[39]</ref> with 80000 shapes. Instead of optimizing z, we optimize its latent representation z : minimize z ,Œ∏,œÜ ,Œ∑</p><formula xml:id="formula_2">I -f (g(z ), Œ∏, h(œÜ ), Œ∑) 2<label>(3)</label></formula><p>where g is the decoder of the 3D auto-encoder. It regularizes the reconstructed shape g(z ) by using the prior shape knowledge (weights in the decoder) for shape generation. Similarly, we use the decoder h that was trained with RenderNet for the texture rendering task in Section 4.1 to regularize the texture variable œÜ . This corresponds to MAP estimation, where the prior term is the shape decoder and the likelihood term is given by RenderNet. Note that it is straightforward to extend this method to the multi-view reconstruction task by summing over multiple per-image losses with shared shape and appearance.</p><p>We compare RenderNet with DC-IGN by Kulkarni et al. <ref type="bibr" target="#b28">[29]</ref> in Figure <ref type="figure" target="#fig_7">6</ref>. DC-IGN learns to decompose images into a graphics code Z, which is a disentangled representation containing a set of latent variables for shape, pose and lighting, allowing them to manipulate these properties to generate novel views or perform image relighting. In contrast to their work, we explicitly reconstruct the 3D geometry, pose, lighting and texture, which greatly improves tasks such as out-of-plane rotation, and allows us to do re-texturing. We also generate results with much higher resolution (512√ó512) compared to DC-IGN (150√ó150). Our results show that having an explicit reconstruction not only creates sharper images with higher level of details in the task of novel-view prediction, but also gives us more control in the relighting task such as light color, brightness, or light position (here we manipulate the elevation and azimuth of the light position), and especially, the re-texturing task.</p><p>For the face dataset, we report the Intersection-over-Union (IOU) between the ground truth and reconstructed voxel grid of 42.99 ¬± 0.64 for 95% confidence interval. We also perform the same experiment for the chair dataset -refer to Section 1 in the supplementary material for implementation details and additional results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and conclusion</head><p>In this paper, we presented RenderNet, a convolutional differentiable rendering network that can be trained end-to-end with a pixel-space regression loss. Despite the simplicity in the design of the network architecture and the projection unit, our experiments demonstrate that RenderNet successfully performs rendering and inverse rendering. Moreover, as shown in Section 4.1, there is the potential to combine different shaders in one network that shares the same 3D convolutions and projection unit, instead of training different networks for different shaders. This opens up room for improvement and exploration, such as extending RenderNet to work with unlabelled data, using other losses like adversarial losses or perceptual losses, or combining RenderNet with other architectures, such as U-Net or a multi-scale architecture where the projection unit is used at different resolutions. Another interesting possibility is to combine RenderNet with a style-transfer loss for stylization of 3D renderings.</p><p>The real world is three-dimensional, yet the majority of current image synthesis CNNs, such as GAN <ref type="bibr" target="#b16">[17]</ref> or DC-IGN <ref type="bibr" target="#b28">[29]</ref>, only operates in 2D feature space and makes almost no assumptions about the 3D world. Although these methods yield impressive results, we believe that having a more geometrically grounded approach can greatly improve the performance and the fidelity of the generated images, especially for tasks such as novel-view synthesis, or more fine-grained editing tasks such as texture editing. For example, instead of having a GAN generate images from a noise vector via 2D convolutions, a GAN using RenderNet could first generate a 3D shape, which is then rendered to create the final image. We hope that RenderNet can bring more attention to the computer graphics literature, especially geometry-grounded approaches, to inspire future developments in computer vision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Network architecture. See Section 2 in the supplementary document for details.</figDesc><graphic coords="4,216.66,152.74,61.15,61.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: Different types of shaders generated by RenderNet (intput at the top). Right: Comparing Phong shading between RenderNet, a standard OpenGL mesh renderer, and a standard Marching Cubes algorithm. RenderNet produces competitive results with the OpenGL mesh renderer without suffering from mesh artefacts (notice the seating pad of chair (c) or the leg of chair (d) in Mesh renderer), and does not suffer from low-resolution input like Marching cubes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Generalization. Even with input from unseen categories or of low quality, RenderNet can still produce good results in different styles (left) and from different views (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Rendering texture and manipulating rendering inputs. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Left: Architecture comparison in different tasks: a) Novel-view synthesis, b) Relighting and c) Generalization. Right: PSNR score of different shaders, including the two alternative architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Image-based reconstruction. We show both the reconstructed images and normal maps from a single image. The cross indicates a factor not learnt by the network. Note: for the re-texturing task, we only show the albedo to visualize the change in texture more clearly. Best viewed in color.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Christian Richardt for helpful discussions. We thank Lucas Theis for helpful discussions and feedback on the manuscript. This work was supported in part by the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 665992, the UK's EPSRC Centre for Doctoral Training in Digital Entertainment (CDE), EP/L016540/1, and CAMERA, the RCUK Centre for the Analysis of Motion, Entertainment Research and Applications, EP/M023281/1. We also received GPU support from Lambda Labs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3d structure from images</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4996" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">OpenDR: An approximate differentiable renderer</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="154" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural 3d mesh renderer</title>
		<author>
			<persName><forename type="first">Hiroharu</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision</title>
		<author>
			<persName><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1696" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking reprojection: Closing the loop for pose-aware shape reconstruction from a single image</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiani</forename><surname>Hamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="57" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MarrNet: 3D Shape Reconstruction via 2.5D Sketches</title>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-view supervision for single-view reconstruction via differentiable ray consistency</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="209" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to generate and reconstruct 3d meshes with only 2d supervision</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised training for 3d morphable model regression</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via renderand-compare</title>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.589</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="5553" to="5562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly supervised 3d reconstruction with adversarial constraint</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Fifth International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In 3D Vision (3DV</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning to generate chairs, tables and cars with convolutional networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="692" to="705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1520" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep shading: Convolutional neural networks for screen-space shading</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Nalbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Arabadzhiyska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Ritschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum (Proc. EGSR)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="65" to="78" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shape, illumination, and reflectance from shading</title>
		<author>
			<persName><forename type="first">Jonathan T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural inverse rendering for general reflectance photometric stereo</title>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Taniai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4864" to="4873" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2107" to="2115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG (Siggraph)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Illumination for computer generated pictures</title>
		<author>
			<persName><forename type="first">Phong</forename><surname>Bui Tuong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="311" to="317" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Suggestive contours for conveying shape</title>
		<author>
			<persName><forename type="first">Doug</forename><surname>Decarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Szymon</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Santella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG (Siggraph)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="848" to="855" />
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time video abstraction</title>
		<author>
			<persName><forename type="first">Holger</forename><surname>Winnem√∂ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><forename type="middle">C</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Gooch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG (Siggraph)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1221" to="1226" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient algorithms for local and global accessibility shading</title>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep convolutional inverse graphics network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tejas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">F</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2539" to="2547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transformationgrounded image generation network for novel 3d view synthesis</title>
		<author>
			<persName><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="702" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Novel views of objects from a single image</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1576" to="1590" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly-supervised disentangling with recurrent transformations for 3d view synthesis</title>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1099" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">3d-assisted image feature synthesis for novel views of an object</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>CoRR, abs/1412.0003</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Determining lightness from an image</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Image Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="277" to="299" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi-Xing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<idno>CoRR, abs/1512.03012</idno>
		<title level="m">Shapenet: An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simplification and repair of polygonal models using volumetric techniques</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Nooruddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Vis. and Comp. Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="205" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A 3d face model for pose and illumination invariant face recognition</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<meeting>the 2009 Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimm</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="484" to="499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
