<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A hybrid particle swarm optimization with a feasibility-based rule for constrained optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qie</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">Institute of Process Control</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">Institute of Process Control</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A hybrid particle swarm optimization with a feasibility-based rule for constrained optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7B3B9C6A5215B603EF06666B24E85143</idno>
					<idno type="DOI">10.1016/j.amc.2006.07.134</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Particle swarm optimization</term>
					<term>Feasibility-based rule</term>
					<term>Constrained optimization</term>
					<term>Simulated annealing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>During the past decade, hybrid algorithms combining evolutionary computation and constraint-handling techniques have shown to be effective to solve constrained optimization problems. For constrained optimization, the penalty function method has been regarded as one of the most popular constraint-handling technique so far, whereas its drawback lies in the determination of suitable penalty factors, which greatly weakens the efficiency of the method. As a novel population-based algorithm, particle swarm optimization (PSO) has gained wide applications in a variety of fields, especially for unconstrained optimization problems. In this paper, a hybrid PSO (HPSO) with a feasibility-based rule is proposed to solve constrained optimization problems. In contrast to the penalty function method, the rule requires no additional parameters and can guide the swarm to the feasible region quickly. In addition, to avoid the premature convergence, simulated annealing (SA) is applied to the best solution of the swarm to help the algorithm escape from local optima. Simulation and comparisons based on several well-studied benchmarks demonstrate the effectiveness, efficiency and robustness of the proposed HPSO. Moreover, the effects of several crucial parameters on the performance of the HPSO are studied as well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many optimization problems involve a number of constraints that the decision solutions need to satisfy, and hence the aim of constrained optimization is to search for feasible solutions with better objective values. Generally, a constrained optimization problem can be described as follows: Find x to minimize f ðxÞ ð1Þ</p><p>Subject to g i ðxÞ 6 0; i ¼ 1; 2; . . . ; n; ð2Þ h j ðxÞ ¼ 0; j ¼ 1; 2; . . . ; p; where x = [x 1 , x 2 , . . . , x d ] T denotes the decision solution vector, n is the number of inequality constraints and p is the number of equality constraints. In a common practice, an equality constraint h j (x) = 0 can be replaced by a couple of inequality constraints h j (x) 6 d and h j (x) P À d (d is a small tolerant amount). Therefore all constraints can be transformed to N = n + 2p inequality constraints. We call x a feasible solution if it satisfies all the constraints. Traditional mathematical programming methods such as the Lagrange multiplier methods <ref type="bibr" target="#b0">[1]</ref> usually require the derivative information of the objective function and constraints. Besides, the obtained solution often tends to be a local optimum unless the search space is convex. In recent years, evolutionary algorithms (EAs) have attracted much attention for a variety of optimization problems due to their superior advantages. EAs do not require the objective function to be derivable or even continuous, and EAs perform as global optimization techniques due to the well balance between the exploration and exploitation of the whole search space. So far, a number of approaches have been proposed by incorporating constraint-handling techniques into EAs to solve constrained optimization problems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>To our knowledge, the penalty function method has been the most popular constraint-handling technique due to its simple principle and ease of implementation. The violations of constraints of the solutions are incorporated into the objective function so that the original constrained problems are transformed into unconstrained ones. Homaifar et al. <ref type="bibr" target="#b3">[4]</ref> proposed an approach, in which penalty factors were set to different values for each level corresponding to the violations of constraints. In <ref type="bibr" target="#b4">[5]</ref>, a dynamic scheme adjusting the values of penalty factors was designed, which aimed at performing wide exploration of the search space at the early stage and gradually guided the search to focus on the feasible region. As we know, the main difficulty of the penalty function method lies in that the appropriate values of penalty factors are problem-dependent and a large amount of efforts are needed for fine-tuning of penalty factors. Therefore, some self-adaptive mechanisms have been introduced for the penalty function method to avoid the trial and error process of tuning factors. Coello <ref type="bibr" target="#b5">[6]</ref> presented the notion of co-evolution to adapt the penalty factors of a fitness function in a genetic algorithm (GA). He and Wang <ref type="bibr" target="#b6">[7]</ref> proposed a co-evolutionary particle swarm optimization (CPSO), where PSO was applied to evolve both decision solutions and penalty factors. In <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, the penalty factors were treated as searching variables and evolved by GA or PSO to the optimal values.</p><p>Apart from the penalty function method, several novel techniques have been incorporated into EAs to handle constraints. Koziel and Michalewicz <ref type="bibr" target="#b7">[8]</ref> proposed a homomorphous mapping (HM) between a high-dimensional cube and a feasible search space to transform the original problem to be unconstrained. Runarsson and Yao <ref type="bibr" target="#b8">[9]</ref> proposed a constraint-handling technique (Stochastic Ranking, SR) from the viewpoint of balancing the dominance between the objective and penalty functions, and the approach focused on the rank of the individuals directly using a bubble-sort like algorithm. Coello and Montes <ref type="bibr" target="#b9">[10]</ref> presented a dominance-based selection scheme to handle constraints in a GA, motivated by the earlier constraint-handling technique known as the niched pareto genetic algorithm (NPGA) <ref type="bibr" target="#b10">[11]</ref>. Coello and Becerra <ref type="bibr" target="#b11">[12]</ref> incorporated a cultural algorithm that used domain knowledge to improve the performance of an evolutionary programming technique.</p><p>During the past decade, a novel evolutionary computation technique, particle swarm optimization, has been proposed <ref type="bibr" target="#b12">[13]</ref>. The development of PSO is based on observations of the social behavior of animals such as bird flocking and fish schooling. Each individual of the swarm is assigned a random velocity, which is updated according to the flying experiences of its own and companions, and then the individuals called particles fly towards the promising areas of the searching space. There is a mechanism of constructive cooperation and information sharing between particles in PSO. Due to the simple concept, easy implementation and quick convergence, PSO has gained much attention and been successfully applied in a variety of fields mainly for unconstrained continuous optimization problems <ref type="bibr" target="#b13">[14]</ref>. As for constrained optimization problems, relatively less work based on PSO can be found than those based on other kinds of EAs. Parsopoulos and Vrahatis <ref type="bibr" target="#b14">[15]</ref> proposed a non-stationary multi-stage assignment penalty function method to transform a constrained problem to an unconstrained one. In the work of Hu and Eberhart <ref type="bibr" target="#b15">[16]</ref>, the initial swarm contained only feasible solutions and a strategy to preserve feasibility was employed. Motivated by multi-objective optimization techniques, Ray and Liew <ref type="bibr" target="#b16">[17]</ref> proposed a swarm algorithm with a multilevel information sharing strategy to deal with constraints. Pulido and Coello <ref type="bibr" target="#b17">[18]</ref> presented a simple mechanism to handle constraints with PSO, which was based on closeness of a particle to the feasible region.</p><p>In this paper, a hybrid PSO with a feasibility-based rule (HPSO) is proposed to solve constrained optimization problems. Motivated by <ref type="bibr" target="#b18">[19]</ref>, the feasibility-based rule is applied to PSO, which gives an instruction on the determination of the best solution of the population (gbest) and the best historical solution of every particle (pbest). Since no additional parameter is needed, the burden of fine-tuning of penalty factors can be avoided so as to highly improve the optimization efficiency. In addition, it was pointed out in <ref type="bibr" target="#b19">[20]</ref> that hybridization was an effective way to improve the performances of a single algorithm. Considering the feasibilitybased rule tends to cause high pressure of feasibility on the particles, hybrid algorithm combining PSO and SA is developed to overcome the premature convergence. In particular, the jumping property of SA is employed to the gbest of the swarm to help PSO escape from local optima. Simulation results based on some famous testing functions and engineering design problems as well as comparisons with some previous approaches show that, HPSO is of great effectiveness, efficiency and robustness. Some solutions better than those previously reported in the literature are obtained through HPSO. Meanwhile, the effects of several crucial parameters on the performance of HPSO are studied as well.</p><p>The rest of the paper is organized as follows. In Section 2, PSO, the feasibility-based rule and SA are simply introduced. In Section 3, the HPSO is proposed and explained in detail. Simulation and comparisons are presented in Section 4, and the discussion is provided in Section 5. Finally, we end the paper with some conclusions and future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Introduction to PSO, the feasibility-based rule and SA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">PSO</head><p>Based on the simulation of simplified social models such as bird flocking and fish schooling, PSO is first introduced in <ref type="bibr" target="#b12">[13]</ref> as a novel evolutionary computation technique with the mechanism of individual improvement, population cooperation and competition. In PSO, multiple candidate individuals called particles coexist and collaborate simultaneously, where the position of each particle denotes a decision vector for the original problem. The trajectory of each particle in the search space is dynamically adjusted by updating the velocity of each particle, according to its own flying experience as well as the experience of neighboring particles (built through tracking and memorizing the best position encountered). Therefore, PSO combines the local search technique (by the particle's own experience) and the global search method (by the neighboring experience) to well balance the exploration and exploitation and finally achieves the global optimum.</p><p>The core operations of PSO are two updating equations of the velocity and position for each particle. In detail, suppose that the position and the velocity of the ith particle in the d-dimensional search space are represented as</p><formula xml:id="formula_1">X i = [x i,1 , x i,2 , . . . , x i,d ] T and V i = [v i,1 , v i,2 , . . . , v i,d</formula><p>] T respectively. Each particle's own best historical position (pbest) is denoted by P i = [p i,1 , p i,2 , . . . , p i,d ] T , and the best historical position that the entire swarm has passed (gbest) is denoted by P g = [p g,1 , p g,2 , . . . , p g,d ] T . The new velocity of each particle is calculated as follows:</p><formula xml:id="formula_2">v i;j ðk þ 1Þ ¼ wv i;j ðkÞ þ c 1 r 1 ðp i;j À x i;j ðkÞÞ þ c 2 r 2 ðp g;j À x i;j ðkÞÞ; j ¼ 1; 2; . . . ; d;<label>ð4Þ</label></formula><p>where c 1 and c 2 are two positive constants called acceleration coefficients, w is called the inertia factor, r 1 and r 2 are two independent random numbers uniformly distributed in the range of [0, 1]. After the velocity is updated, the new position of each particle for the next generation is determined according to the following equation:</p><formula xml:id="formula_3">x i;j ðk þ 1Þ ¼ x i;j ðkÞ þ v i;j ðk þ 1Þ; j ¼ 1; 2; . . . ; d:<label>ð5Þ</label></formula><p>Then the particle is evaluated according to its new position, and pbest and gbest are updated at each generation. The process is repeated until a user-defined stopping criterion is reached. We refer to <ref type="bibr" target="#b13">[14]</ref> for more detail. The procedure of standard PSO is summarized as follows:</p><p>Step 1: Initialize a population of particles with random positions and velocities, where each particle contains d variables.</p><p>Step 2: Evaluate the objective values of all particles, and set pbest of each particle and its objective value equal to its current position and objective value, and set gbest and its objective value equal to the position and objective value of the best initial particle.</p><p>Step 3: Update the velocity and position of each particle according to Eqs. ( <ref type="formula" target="#formula_2">4</ref>) and <ref type="bibr" target="#b4">(5)</ref>.</p><p>Step 4: Evaluate the objective values of all particles.</p><p>Step 5: For each particle, compare its current objective value with the objective value of its pbest. If the current value is better, then update pbest and its objective value with the current position and objective value.</p><p>Step 6: Determine the best particle of the current swarm with the best objective value. If the objective value is better than the objective value of gbest, then update gbest and its objective value with the position and objective value of the current best particle.</p><p>Step 7: If a stopping criterion is met, then output gbest and its objective value; otherwise go back to Step 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The feasibility-based rule</head><p>Due to the simplicity and ease of implementation, the penalty function method has been considered as the most popular technique to handle constraints. A penalty function can be formulated as follows:</p><formula xml:id="formula_4">F ðxÞ ¼ f ðxÞ þ X N j¼1 w j Â G j ;<label>ð6Þ</label></formula><p>where f(x) is the objective function; G j denotes the constraint violation for the jth constraint, which is employed as the penalty term; and w j denotes the penalty factor. Since the objective function and the constraint violation are simultaneously considered in the penalty function, the performance of this kind of approach is significantly affected by the penalty factor. However, the suitable penalty factors are usually difficult to determine and problem-dependent.</p><p>Motivated by <ref type="bibr" target="#b18">[19]</ref>, a feasibility-based rule is employed in this paper to handle constraints, which is described as follows:</p><p>(1) Any feasible solution is preferred to any infeasible solution.</p><p>(2) Between two feasible solutions, the one having better objective function value is preferred.</p><p>(3) Between two infeasible solutions, the one having smaller constraint violation is preferred.</p><p>Based on the above criteria, objective function and constraint violation information are considered separately. Consequently, penalty factors are not used at all. Moreover, in the first and the third cases the search tends to the feasible region rather than infeasible region, and in the second case the search tends to the feasible region with good solutions. In brief, such a simple rule aims at obtaining good feasible solutions. Different from <ref type="bibr" target="#b18">[19]</ref> where an additional fitness function F(x) was designed to evaluate solutions; there is no need to design the additional fitness function in this paper by incorporating the rule into PSO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Simulated annealing</head><p>Simulated annealing <ref type="bibr" target="#b20">[21]</ref> is an effective optimization algorithm motivated from an analogy between the simulation of the annealing of solid and the strategy of solving combinatorial optimization problems. Starting from an initial state, the system is perturbed at random to a new state in the neighborhood of the original one, for which a change of DE in the objective function value takes place. For minimization problems, the new state is accepted with a probability of min{1, exp(ÀDE/t)}, where t is a control parameter corresponding to the temperature in the analogy. SA provides a mechanism to probabilistically escape from local optima and the search process can be controlled by the cooling schedule. It has been theoretically proved that under certain conditions, SA is globally convergent in probability 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HPSO hybrid strategy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Updating pbest and gbest</head><p>In this paper, the constraint violation value of an infeasible solution is calculated as follows:</p><formula xml:id="formula_5">violðxÞ ¼ X N j¼1 ½maxðg j ðxÞ; 0Þ:<label>ð7Þ</label></formula><p>Suppose that P i (k) represents pbest of the ith particle at generation k and X i (k + 1) represents the newly generated position of the ith particle at generation k + 1. In the standard PSO,</p><formula xml:id="formula_6">P i (k + 1) = X i (k + 1) only if f(X i (k + 1)) &lt; f(P i (k))</formula><p>. Whiles in our HPSO, the feasibility-based rule is employed. That is, P i (k) will be replaced by X i (k + 1) at any of the following scenarios:</p><p>(1</p><formula xml:id="formula_7">) P i (k) is infeasible, but X i (k + 1) is feasible. (2) Both P i (k) and X i (k + 1) are feasible, but f(X i (k + 1)) &lt; f(P i (k)).</formula><p>(3) Both P i (k) and X i (k + 1) are infeasible, but viol(X i (k + 1)) &lt; viol(P i (k)).</p><p>In a similar way, gbest is updated based on the rule at every generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SA-based local search for gbest</head><p>According to the feasibility-based rule, feasible solutions are always considered better than infeasible solutions. That may cause the overpressure of selecting feasible solutions so as to result in premature convergence. Thus, the jumping property of SA is applied to help the search escape from local optima. Considering the searching efficiency, the mechanism of SA and the feasibility-based rule are fused as a local search for gbest to make a well balance between exploration and exploitation. In particular, such a local search is implemented as follows, where P g (k) denotes gbest of the population at generation k and p a denotes the acceptable probability of a new generated solution.</p><p>Step 1: Let m = 1, P 0 g ¼ P g ðkÞ.</p><p>Step 2: Generate a new solution using the following equation:</p><formula xml:id="formula_8">x 0 ¼ P 0 g þ g Â ðX max À X min Þ Â N ð0; 1Þ:<label>ð8Þ</label></formula><p>Step 3: Calculate p a according to the following criteria:</p><p>(1) If x 0 is feasible and P 0 g is infeasible, p a = 1. (2) If P 0 g is feasible and x 0 is infeasible, p a = 0.</p><p>(3) If both x 0 and P 0 g are feasible, p a is calculated using</p><formula xml:id="formula_9">p a ¼ minf1; exp½ðf ðP 0 g Þ À f ðx 0 ÞÞ=tðkÞg:<label>ð9Þ</label></formula><p>(4) If both x 0 and P 0 g are infeasible, p a is calculated using</p><formula xml:id="formula_10">p a ¼ minf1; exp½ðviolðP 0 g Þ À violðx 0 ÞÞ=tðkÞg:<label>ð10Þ</label></formula><p>Step 4: If p a P U[0, 1], P 0 g ¼ x 0 . Step 5: Let m = m + 1. If m &gt; L, stop and output P 0 g as the new gbest; else go to Step 2.</p><p>Remark 1. In Eq. ( <ref type="formula" target="#formula_8">8</ref>), g is used to control the step size, X max and X min denote the upper and lower bounds of the solutions defined by the problem, N(0, 1) denotes a random number normally distributed with mean 0 and variance 1. In Eqs. ( <ref type="formula" target="#formula_9">9</ref>) and <ref type="bibr" target="#b9">(10)</ref>, t(k) denotes the temperature at generation k. In Step 4, U[0, 1] represents a random number uniformly distributed in the range of [0, 1]. In Step 6, L is a user-defined number of iterations used to stop the local search.</p><p>Remark 2. In this paper, the initial temperature is determined by the following empirical formula:</p><formula xml:id="formula_11">t :0 ¼ À f max À f min lnð0:1Þ ;<label>ð11Þ</label></formula><p>where f max and f min are the maximum and minimum objective values of the solutions in the initial swarm, respectively. Besides, the exponential annealing, i.e. t(k + 1) = k AE t(k), is employed, where the annealing rate satisfies 0 &lt; k &lt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The framework of HPSO</head><p>After the main elements of our hybrid PSO for constrained optimization problems are explained, the framework of HPSO is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>The features of HPSO can be summarized as follows. (a) The simple feasibility-based rule is employed to update pbest and gbest, and penalty factors are not required. (b) The jumping property of SA is incorporated into PSO to adjust gbest at each generation, which is helpful for the search to escape from local optima. (c) The HPSO performs a population-based evolution in light of the mechanism of PSO. In a word, with the help of the feasibility-based rule and the hybridization of SA and PSO, HPSO will be a powerful approach to solve constrained optimization problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Simulation and analysis</head><p>In this section, numerical simulations are carried out to investigate the performances of the proposed HPSO, where several constrained benchmark functions <ref type="bibr" target="#b7">[8]</ref> and three well-studied engineering design problems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24]</ref> are used for testing. All the testing problems are described in the Appendix in detail. The parameters of HPSO are listed in Table <ref type="table" target="#tab_1">1</ref>, and 30 independent runs are carried out for each example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yes No</head><p>Initialize M particles with random positions and velocities; Evaluate the population; k = 0, t(k) = t 0 Initialize pbest with a copy of the position for each particle, determine gbest according to the feasibility-based rule k&lt;Gmax?</p><p>Output gbest</p><p>Update the velocities and positions using Eq. ( <ref type="formula" target="#formula_2">4</ref>) and Eq. ( <ref type="formula" target="#formula_3">5</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Population evaluation</head><p>Update pbest and gbest Applied SA-based local search to gbest </p><formula xml:id="formula_12">) ( ) 1 ( k t k t ⋅ = + λ , k = k+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Simulation results for testing functions (Examples 1-3)</head><p>Based on the three testing functions, the simulations results of HSPO and the comparisons with the homomorphous mapping (HM) <ref type="bibr" target="#b7">[8]</ref>, the stochastic ranking (SR) <ref type="bibr" target="#b8">[9]</ref> and Coello and Becerra's evolutionary programming with a cultural algorithm <ref type="bibr" target="#b11">[12]</ref> are shown in Table <ref type="table">2</ref>.</p><p>From Table <ref type="table">2</ref>, it can be seen that results of our approach are competitive to those of SR and better than those of HM and Coello and Becerra's method. All the solutions obtained by HPSO at 30 runs are very close to the global optimum, and the standard deviations for the three examples are very small, which shows the robustness of our approach. Besides, it needs to be mentioned that are only 81,000 fitness function evaluations (FFEs) used in HPSO, whereas HM performed 1,400,000 FFEs and SR performed 350,000 FFEs. Although Coello and Becerra's method performed only 50,020 FFEs, it needs to store a large amount of spatial data structures that explode exponentially with the increase of the number of decision variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Example 4: a tension/compression string design problem</head><p>This problem has been well studied and solved before by the following approaches: eight different numerical optimization techniques (only the best results are shown) <ref type="bibr" target="#b21">[22]</ref>, a numerical optimization technique called constraint correction at constant cost <ref type="bibr" target="#b22">[23]</ref>, a GA-based co-evolution model <ref type="bibr" target="#b5">[6]</ref>, a GA through the use of dominance-based tournament selection <ref type="bibr" target="#b9">[10]</ref>, Coello and Becerra's evolutionary programming with a cultural algorithm <ref type="bibr" target="#b11">[12]</ref> and CPSO <ref type="bibr" target="#b6">[7]</ref>. The best solutions obtained by the above mentioned approaches as well as HPSO are listed in Table <ref type="table" target="#tab_2">3</ref>, and their statistical simulation results are shown in Table <ref type="table">4</ref>.</p><p>From Table <ref type="table" target="#tab_2">3</ref>, it can be seen that the best feasible solution obtained by HPSO is better than those previously reported solutions. In addition, as shown in Table <ref type="table">4</ref>, the average searching quality of HPSO is superior to those of other methods, and even the worst solution found by HPSO is better than the best solutions reported in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12]</ref>. Moreover, the standard deviation of the results by HPSO in 30 independent runs for this problem is the smallest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Example 5: a welded beam design problem</head><p>This problem is taken from <ref type="bibr" target="#b5">[6]</ref> and has been solved before by the following approaches: a GA-based coevolution model <ref type="bibr" target="#b5">[6]</ref>, a GA through the use of dominance-based tournament selection <ref type="bibr" target="#b9">[10]</ref>, Coello and Becerra's evolutionary programming with a cultural algorithm <ref type="bibr" target="#b11">[12]</ref> and CPSO <ref type="bibr" target="#b6">[7]</ref>. The best solutions obtained by the above mentioned approaches as well as HPSO are listed in Table <ref type="table">5</ref>, and their statistical simulation results are shown in Table <ref type="table">6</ref>. From Table <ref type="table">5</ref>, it can be seen that the best feasible solution obtained by HPSO is competitive to the result obtained in <ref type="bibr" target="#b11">[12]</ref> and better than the results obtained by other techniques. From Table <ref type="table">6</ref>, it can be found that the average searching quality of HPSO is competitive to that of CPSO and greatly superior to those of other methods. In addition, the standard deviation of the results by HPSO in 30 independent runs is also very small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Example 6: a pressure vessel design problem</head><p>This problem is taken from <ref type="bibr" target="#b23">[24]</ref> and has been solved before by the following approaches: genetic adaptive search <ref type="bibr" target="#b24">[25]</ref>, an augmented Lagrange multiplier approach <ref type="bibr" target="#b23">[24]</ref>, a branch and bound technique <ref type="bibr" target="#b25">[26]</ref>, a GA-based co-evolution model <ref type="bibr" target="#b5">[6]</ref>, a GA through the use of dominance-based tournament selection <ref type="bibr" target="#b9">[10]</ref> and CPSO <ref type="bibr" target="#b6">[7]</ref>. The best solutions obtained by the above mentioned approaches as well as HPSO are listed in Table <ref type="table" target="#tab_3">7</ref>, and their statistical simulation results are shown in Table <ref type="table">8</ref>.</p><p>It is shown in Table <ref type="table" target="#tab_3">7</ref> that the best feasible solution obtained by HPSO is better than those previously reported solutions. From Table <ref type="table">8</ref>, it can be found that the average searching quality of HPSO is greatly superior to those of other methods. Besides, even the worst solution found by HPSO is better than the best solutions reported in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Based on the above simulation results and comparisons, it can be concluded that HPSO is of superior searching quality and robustness for constrained engineering design problems. Moreover, comparing the two most effective methods, i.e. HPSO and CPSO, HPSO needs only 81,000 FFEs while CPSO needs 200,000 FFEs, which shows that our HPSO is more efficient than CPSO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this section, we will carry out further discussion on the computational efforts and the searching efficiency as well as the parameter settings of HPSO, where we use the tension/compression design problem (Example 4) as an example and the parameters are the same as those mentioned in Table <ref type="table" target="#tab_1">1</ref> except for a special explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Searching efficiency of HPSO</head><p>In HPSO, the total number of fitness evaluations is (M  Sandgren <ref type="bibr" target="#b25">[26]</ref> 8129.8000 N/A N/A N/A Kannan <ref type="bibr" target="#b23">[24]</ref> 7198.0428 N/A N/A N/A Deb <ref type="bibr" target="#b24">[25]</ref> 6410.3811 N/A N/A N/A Coello <ref type="bibr" target="#b5">[6]</ref> 6288 Fig. <ref type="figure">2</ref> illustrates a typical evolving process of the objective value of gbest when solving Example 4. As shown in Fig. <ref type="figure">2</ref>, at the early stage of evolution the solution with inferior objective value could be accepted as gbest due to the jumping property of SA, which helps HPSO escape from local optima; and as the temperature decreases HPSO converges to the global optimum very quickly. So, it is demonstrated that HPSO is of effective and efficient ability of global search.</p><formula xml:id="formula_13">• G max + L • G max ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effects of crucial parameters on HPSO</head><p>During the simulation experiments, it has been found that the population size M and the step size g have significant effects on the performances of HPSO. Firstly, we investigate the performance of HPSO with differ- ent population size M. The effects with different M on the average objective values and the total number of evaluation are illustrated in Figs. <ref type="figure">3</ref> and<ref type="figure" target="#fig_3">4</ref>, respectively.</p><p>As shown in Fig. <ref type="figure">3</ref>, when M is too small, the average searching quality is poor because the solution space may not be covered sufficiently during the evolution process. As M increases, the results become better at a cost of more fitness evaluations, but there is a threshold beyond which the results will not be affected in a significant manner. Therefore, considering both the searching quality and computational efforts, it is recommended to choose M between 250 and 300.</p><p>Next, we investigate the performance of HPSO with different values of g. The effects with different g on the average objective values is illustrated in Fig. <ref type="figure" target="#fig_4">5</ref>.  As shown in Fig. <ref type="figure" target="#fig_4">5</ref>, if g is too small, the rate of convergence of HPSO will be slow and the gbest obtained at the end of evolution may still stay far away from the global optimum; on the contrary, if g is too large, the new generated solution would easily arrive the boundary of the search space and HPSO is unable to well explore the search space. Therefore, the recommended value of g is 0.01 in HPSO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper has introduced a novel hybrid particle swarm optimization with a feasibility-based rule, which provides an effective alternative for solving constrained optimization problems to overcome the weakness of penalty function methods. Simulation results and comparisons showed that our proposed HPSO is of good performances in terms of searching quality, efficiency and robustness. Besides, some better solutions than those previously reported were obtained by HPSO. The future work is to study the adaptive HPSO and to incorporate the feasibility-based rule to other heuristics such as differential evolution <ref type="bibr" target="#b26">[27]</ref> to solve constrained optimization problems. where 0 6 x i 6 10 (i = 1, 2, 3) and p, q, r = 1,2,. . . , 9. The feasible region of the search space consists of 9 3 disjoint spheres. A point (x 1 , x 2 , x 3 ) is feasible if and only if there exist p, q, r such that the above inequality holds. The optimum solution is x * = (5, 5, 5) with f(x * ) = 1.</p><p>Example 4 (A tension/compression string design problem). This problem is described in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, and the aim is to minimize the weight (f(x)) of a tension/compression spring (as shown in Fig. <ref type="figure" target="#fig_6">6</ref>) subject to constraints on minimum deflection, shear stress, surge frequency, limits on outside diameter and on design variables. The design variables are the mean coil diameter D (x 2 ); the wire diameter d (x 1 ) and the number of active coils P (x 3 ).</p><p>The mathematical formulation of this problem can be described as follows:</p><p>Minimize</p><formula xml:id="formula_14">f ðxÞ ¼ ðx 3 þ 2Þx 2 x 2 1</formula><p>Subject to g 1 ðxÞ ¼ 1 À x 3 2 x 3 71785x 4 1 6 0; Example 5 (A welded beam design problem). The following problem is taken from <ref type="bibr" target="#b5">[6]</ref>, in which a welded beam is designed for minimum cost (f(x)) subject to constraints on shear stress (s); bending stress in the beam (h); buckling load on the bar (P c ); end deflection of the beam (d); and side constraints. There are four design variables as shown in Fig. <ref type="figure" target="#fig_7">7</ref>, i.e. h (x 1 ), l (x 2 ), t (x 3 ) and b (x 4 ).</p><formula xml:id="formula_15">g 2 ðxÞ ¼ 4x 2 2 À x 1 x 2 12566ðx 2 x 3 1 À x 4 1 Þ þ 1 5108x 2 1 À 1 6 0;</formula><p>The problem can be mathematically formulated as follows:</p><p>Minimize f ðxÞ ¼ 1:10471x 2 1 x 2 þ 0:04811x 3 x 4 ð14:0 þ x 2 Þ Subject to g 1 ðxÞ ¼ sðxÞ À 13600 6 0; g 2 ðxÞ ¼ rðxÞ À 30000 6 0; g 3 ðxÞ ¼ x 1 À x 4 6 0; g 4 ðxÞ ¼ 0:10471x 2 1 þ 0:04811x 3 x 4 ð14:0 þ x 2 Þ À 5:0 6 0; g 5 ðxÞ ¼ 0:125 À x 1 6 0; g 6 ðxÞ ¼ dðxÞ À 0:25 6 0; g 7 ðxÞ ¼ P À P c ðxÞ 6 0; 0:1 6 x 1 ; x 4 6 2; 0:1 6 x 2 ; x 3 6 10; where</p><formula xml:id="formula_16">sðxÞ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ðs 0 Þ 2 þ 2s 0 s 00 x 2 2R þ ðs 00 Þ 2 r ; s 0 ¼ P ffiffi ffi 2 p</formula><p>x 1 x 2 ;</p><p>s 00 ¼ QR J ; Example 6 (A pressure vessel design problem). In this problem, the objective is to minimize the total cost (f(x)), including the cost of the material, forming and welding. A cylindrical vessel is capped at both ends by hemispherical heads as shown in Fig. <ref type="figure">8</ref>. There are four design variables: T s (x 1 , thickness of the shell), T h (x 2 , thickness of the head), R (x 3 , inner radius) and L (x 4 , length of the cylindrical section of the vessel, not including the head). Among the four variables, T s and T h are integer multiples of 0.0625 in., which are the available thicknesses of rolled steel plates, and R and L are continuous variables. </p><formula xml:id="formula_17">Q ¼ P L þ x 2 2 ; R ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi x 2 2 4 þ x 1 þ x 3 2 2 r ; J ¼ 2 ffiffi ffi 2 p x 1 x 2 x 2 2 12 þ x 1 þ x 3<label>2</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The flow chart of HPSO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where M • G max FFEs are used for PSO-based search and L • G max FFEs are used for SA-based local search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. A typical evolving process of fitness function for Example 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Total number of fitness evaluations in HPSO with different M for Example 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Average objective values resulted by HPSO with different g for Example 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The tension/compression string design problem (Example 4).</figDesc><graphic coords="13,119.38,67.32,301.32,91.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The welded beam design problem (Example 5).</figDesc><graphic coords="14,171.66,67.32,205.56,167.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>The parameters adopted in the implementation of HPSO</figDesc><table><row><cell></cell><cell>Parameters</cell><cell>Notation</cell><cell>Value</cell></row><row><cell>PSO group</cell><cell>Population size</cell><cell>M</cell><cell>250</cell></row><row><cell></cell><cell>Maximum number of generations</cell><cell>G max</cell><cell>300</cell></row><row><cell></cell><cell>Acceleration coefficients</cell><cell>c 1</cell><cell>2.0</cell></row><row><cell></cell><cell></cell><cell>c 2</cell><cell>2.0</cell></row><row><cell></cell><cell>The inertia factor</cell><cell>w</cell><cell>Linearly decreases from 0.9 to 0.4</cell></row><row><cell></cell><cell>Maximum velocity of particles</cell><cell>V max</cell><cell>V max = X max À X min</cell></row><row><cell>SA group</cell><cell>Number of iterations at each generation</cell><cell>L</cell><cell>20</cell></row><row><cell></cell><cell>Annealing rate</cell><cell>k</cell><cell>0.94</cell></row><row><cell></cell><cell>Step size</cell><cell>g</cell><cell>0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Comparison of the best solution for Example 4 by different methods</figDesc><table><row><cell>Methods</cell><cell>x 1 (d)</cell><cell>x 2 (D)</cell><cell>x 3 (P)</cell><cell></cell><cell>f(x)</cell></row><row><cell>Belegundu [22]</cell><cell>0.050000</cell><cell>0.315900</cell><cell cols="2">14.250000</cell><cell>0.0128334</cell></row><row><cell>Arora [23]</cell><cell>0.053396</cell><cell>0.399180</cell><cell cols="2">9.185400</cell><cell>0.0127303</cell></row><row><cell>Coello [6]</cell><cell>0.051480</cell><cell>0.351661</cell><cell cols="2">11.632201</cell><cell>0.0127048</cell></row><row><cell>Coello and Montes [10]</cell><cell>0.051989</cell><cell>0.363965</cell><cell cols="2">10.890522</cell><cell>0.0126810</cell></row><row><cell>Coello and Becerra [12]</cell><cell>0.050000</cell><cell>0.317395</cell><cell cols="2">14.031795</cell><cell>0.0127210</cell></row><row><cell>CPSO [7]</cell><cell>0.051728</cell><cell>0.357644</cell><cell cols="2">11.244543</cell><cell>0.0126747</cell></row><row><cell>HPSO</cell><cell>0.051706</cell><cell>0.357126</cell><cell cols="2">11.265083</cell><cell>0.0126652</cell></row><row><cell>Table 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Statistical results of different methods for Example 4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Best</cell><cell>Mean</cell><cell>Worst</cell><cell></cell><cell>Std.</cell></row><row><cell>Belegundu [22]</cell><cell>0.0128334</cell><cell>N/A</cell><cell>N/A</cell><cell></cell><cell>N/A</cell></row><row><cell>Arora [23]</cell><cell>0.0127303</cell><cell>N/A</cell><cell>N/A</cell><cell></cell><cell>N/A</cell></row><row><cell>Coello [6]</cell><cell>0.0127048</cell><cell>0.0127690</cell><cell cols="2">0.012822</cell><cell>3.9390eÀ005</cell></row><row><cell>Coello and Montes [10]</cell><cell>0.0126810</cell><cell>0.0127420</cell><cell cols="2">0.012973</cell><cell>5.9000eÀ005</cell></row><row><cell>Coello and Becerra [12]</cell><cell>0.0127210</cell><cell>0.0135681</cell><cell cols="2">0.015116</cell><cell>8.4152eÀ004</cell></row><row><cell>CPSO [7]</cell><cell>0.0126747</cell><cell>0.0127300</cell><cell cols="2">0.012924</cell><cell>5.1985eÀ004</cell></row><row><cell>HPSO</cell><cell>0.0126652</cell><cell>0.0127072</cell><cell cols="2">0.0127191</cell><cell>1.5824eÀ005</cell></row><row><cell>Table 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Comparison of the best solution for Example 5 by different methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>x 1 (h)</cell><cell>x 2 (l)</cell><cell>x 3 (t)</cell><cell>x 4 (b)</cell><cell>f(x)</cell></row><row><cell>Coello [6]</cell><cell>0.208800</cell><cell>3.420500</cell><cell>8.997500</cell><cell>0.210000</cell><cell>1.748309</cell></row><row><cell>Coello and Montes [10]</cell><cell>0.205986</cell><cell>3.471328</cell><cell>9.020224</cell><cell>0.206480</cell><cell>1.728226</cell></row><row><cell>Coello and Becerra [12]</cell><cell>0.205700</cell><cell>3.470500</cell><cell>9.036600</cell><cell>0.205700</cell><cell>1.724852</cell></row><row><cell>CPSO [7]</cell><cell>0.202369</cell><cell>3.544214</cell><cell>9.048210</cell><cell>0.205723</cell><cell>1.728024</cell></row><row><cell>HPSO</cell><cell>0.205730</cell><cell>3.470489</cell><cell>9.036624</cell><cell>0.205730</cell><cell>1.724852</cell></row><row><cell>Table 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Statistical results of different methods for Example 5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Best</cell><cell>Mean</cell><cell>Worst</cell><cell></cell><cell>Std.</cell></row><row><cell>Coello [6]</cell><cell>1.748309</cell><cell>1.771973</cell><cell cols="2">1.785835</cell><cell>0.011220</cell></row><row><cell>Coello and Montes [10]</cell><cell>1.728226</cell><cell>1.792654</cell><cell cols="2">1.993408</cell><cell>0.074713</cell></row><row><cell>Coello and Becerra [12]</cell><cell>1.724852</cell><cell>1.971809</cell><cell cols="2">3.179709</cell><cell>0.443131</cell></row><row><cell>CPSO [7]</cell><cell>1.728024</cell><cell>1.748831</cell><cell cols="2">1.782143</cell><cell>0.012926</cell></row><row><cell>HPSO</cell><cell>1.724852</cell><cell>1.749040</cell><cell cols="2">1.814295</cell><cell>0.040049</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7</head><label>7</label><figDesc>Comparison of the best solution for Example 6 by different methods</figDesc><table><row><cell>Methods</cell><cell>x 1 (T s )</cell><cell>x 2 (T h )</cell><cell>x 3 (R)</cell><cell>x 4 (L)</cell><cell>f(x)</cell></row><row><cell>Sandgren [26]</cell><cell>1.1250</cell><cell>0.6250</cell><cell>47.7000</cell><cell>117.7010</cell><cell>8129.8000</cell></row><row><cell>Kannan [24]</cell><cell>1.1250</cell><cell>0.6250</cell><cell>58.2910</cell><cell>43.6900</cell><cell>7198.0428</cell></row><row><cell>Deb [25]</cell><cell>0.9375</cell><cell>0.5000</cell><cell>48.3290</cell><cell>112.6790</cell><cell>6410.3811</cell></row><row><cell>Coello [6]</cell><cell>0.8125</cell><cell>0.4375</cell><cell>40.3239</cell><cell>200.0000</cell><cell>6288.7445</cell></row><row><cell>Coello and Montes [10]</cell><cell>0.8125</cell><cell>0.4375</cell><cell>42.0974</cell><cell>176.6540</cell><cell>6059.9463</cell></row><row><cell>CPSO [7]</cell><cell>0.8125</cell><cell>0.4375</cell><cell>42.0913</cell><cell>176.7465</cell><cell>6061.0777</cell></row><row><cell>HPSO</cell><cell>0.8125</cell><cell>0.4375</cell><cell>42.0984</cell><cell>176.6366</cell><cell>6059.7143</cell></row><row><cell>Table 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Statistical results of different methods for Example 6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Best</cell><cell>Mean</cell><cell></cell><cell>Worst</cell><cell>Std.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>L ¼ 14; E ¼ 30 Â 10 6 ; G ¼ 12 Â 10 6 :</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell><cell>;</cell></row><row><cell>rðxÞ ¼</cell><cell cols="2">6PL 3 x 4 x 2</cell><cell>;</cell><cell></cell><cell></cell><cell></cell></row><row><cell>dðxÞ ¼</cell><cell cols="3">4PL 3 Ex 3 3 x 4</cell><cell>;</cell><cell></cell><cell></cell></row><row><cell cols="2">P c ðxÞ ¼</cell><cell cols="3">4:013E L 2 q</cell><cell>ffiffiffiffiffiffi x 2 3 x 6 4 36</cell><cell>1 À</cell><cell>x 3 2L</cell><cell>ffiffiffiffiffiffi r ! E 4G</cell><cell>;</cell></row><row><cell cols="3">P ¼ 6000;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Q. He, L. Wang / Applied Mathematics and Computation 186 (2007) 1407-1422</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is partially supported by National Science Foundation of China (60374060 and 60574072) and 973 Program (2002CB312200).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Example 1 (g04).</p><p>Minimize f ðxÞ ¼ 5:3578547x 2  3 þ 0:8356891x 1 x 5 þ 37:293239x 1 À 40792:141 Subject to g 1 ðxÞ ¼ 85:334407 þ 0:0056858x 2 x 5 þ 0:0006262x 1 x 4 À 0:0022053x 3 x 5 À 92 6 0; g 2 ðxÞ ¼ À85:334407 À 0:0056858x 2 x 5 À 0:0006262x 1 x 4 þ 0:0022053x 3 x 5 6 0; g 3 ðxÞ ¼ 80:51249 þ 0:0071317x 2 x 5 þ 0:0029955x 1 x 2 þ 0:0021813x 2  3 À 110 6 0; g 4 ðxÞ ¼ À80:51249 À 0:0071317x 2 x 5 À 0:0029955x 1 x 2 À 0:0021813x 2  3 þ 90 6 0; g 5 ðxÞ ¼ 9:300961 þ 0:0047026x 3 x 5 þ 0:0012547x 1 x 3 þ 0:0019085x 3 x 4 À 25 6 0; g 6 ðxÞ ¼ À9:300961 À 0:0047026x 3 x 5 À 0:0012547x 1 x 3 À 0:0019085x 3 x 4 þ 20 6 0; where 78 6 x 1 6 102, 33 6 x 2 6 45, 27 6 x i 6 45 (i = 3, 4, 5). The optimum solution is x * = (78, 33, 29.995256025682, 45, 36.775812905788) with f(x * ) = À30665.539.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2 (g08).</head><p>Maximize f ðxÞ ¼ Maximize</p><p>The problem can be formulated as follows <ref type="bibr" target="#b23">[24]</ref>: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Constrained Optimization and Lagrange Multiplier Methods</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theoretical and numerical constraint-handling techniques used with evolutionary algorithms: a survey of the state of the art</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A C</forename><surname>Coello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Meth. Appl. Mech. Eng</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="page" from="1245" to="1287" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of constraint handling techniques in evolutionary computation methods</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Michalewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Annual Conference on Evolutionary Programming</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Mcdonnell</surname></persName>
		</editor>
		<meeting>the 4th Annual Conference on Evolutionary Programming<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="135" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Constrained optimization via genetic algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Homaifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Simulation</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="242" to="254" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the use of non-stationary penalty functions to solve nonlinear constrained optimization problems with GAs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Joines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Houck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of First IEEE Conference on Evolutionary Computation</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fogel</surname></persName>
		</editor>
		<meeting>First IEEE Conference on Evolutionary Computation<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="579" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Use of a self-adaptive penalty approach for engineering optimization problems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A C</forename><surname>Coello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Ind</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="113" to="127" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An effective co-evolutionary particle swarm optimization for constrained engineering design problems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.engappai.2006.03.003</idno>
	</analytic>
	<monogr>
		<title level="j">Eng. Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evolutionary algorithms, homomorphous mappings, and constrained parameter optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Koziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Michalewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="44" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stochastic ranking for constrained evolutionary optimization</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Runarsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="284" to="294" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Constraint-handling in genetic algorithms through the use of dominance-based tournament selection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A C</forename><surname>Coello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Montes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Eng. Inform</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="193" to="203" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A niched Pareto genetic algorithm for multiobjective optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nafpliotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of First IEEE Conference on Evolutionary Computation</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Fogel</surname></persName>
		</editor>
		<meeting>First IEEE Conference on Evolutionary Computation<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="82" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient evolutionary optimization through the use of a cultural algorithm</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A C</forename><surname>Coello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Becerra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Optimiz</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="236" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Neural Networks</title>
		<meeting>the IEEE International Conference on Neural Networks<address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<title level="m">Swarm Intelligence</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufman Publishers</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Particle swarm optimization method for constrained optimization problems</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Parsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Vrahatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Euro-International Symposium on Computational Intelligence, Kos ˇice</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Kvasnic ˇka</surname></persName>
		</editor>
		<meeting>the 2nd Euro-International Symposium on Computational Intelligence, Kos ˇice<address><addrLine>Slovakia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="214" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Solving constrained nonlinear optimization problems with particle swarm optimization</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth World Multiconference on Systematics, Cybernetics and Informatics</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Callaos</surname></persName>
		</editor>
		<meeting>the Sixth World Multiconference on Systematics, Cybernetics and Informatics<address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="203" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A swarm with an effective information sharing mechanism for unconstrained and constrained single objective optimization problems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Liew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 Congress on Evolutionary Computation</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</editor>
		<meeting>the 2001 Congress on Evolutionary Computation<address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="75" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A constraint-handling mechanism for particle swarm optimization</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename><surname>Pulido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A C</forename><surname>Coello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Congress on Evolutionary Computation</title>
		<meeting>the 2004 Congress on Evolutionary Computation</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1396" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An efficient constraint handling method for genetic algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Meth. Appl. Mech. Eng</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page" from="311" to="338" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Intelligent Optimization Algorithms with Applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Tsinghua University &amp; Springer Press</publisher>
			<pubPlace>Beijing</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gelatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A study of mathematical programming methods for structural optimization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Belegundu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
		<respStmt>
			<orgName>Department of Civil and Environmental Engineering, University of Iowa, IA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Introduction to Optimum Design</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Arora</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An augmented Lagrange multiplier based method for mixed integer discrete continuous optimization and its applications to mechanical design</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mech. Des</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="318" to="320" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GeneAS: a robust optimal design technique for mechanical component design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary Algorithms in Engineering Applications</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Dasgupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Michalewicz</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="497" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonlinear integer and discrete programming in mechanical engineering systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sandgren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mech. Des</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="223" to="229" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Differential evolution -a simple and efficient heuristic for global optimization over continuous spaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Storn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Global Optim</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="341" to="359" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
