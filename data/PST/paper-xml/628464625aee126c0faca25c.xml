<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature and Instance Joint Selection: A Reinforcement Learning Perspective</title>
				<funder ref="#_s8nCxj9 #_mu9UGux #_PuTkcRs">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-12">12 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
							<email>weifan@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
							<email>kunpengliu@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
							<email>liuh@ust.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
							<email>zhuhengshu@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Baidu Talent Intelligence Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
							<email>hxiong@rutgers.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
							<email>yanjie.fu@ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Feature and Instance Joint Selection: A Reinforcement Learning Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-12">12 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.07867v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature selection and instance selection are two important techniques of data processing. However, such selections have mostly been studied separately, while existing work towards the joint selection conducts feature/instance selection coarsely; thus neglecting the latent fine-grained interaction between feature space and instance space. To address this challenge, we propose a reinforcement learning solution to accomplish the joint selection task and simultaneously capture the interaction between the selection of each feature and each instance. In particular, a sequential-scanning mechanism is designed as action strategy of agents and a collaborative-changing environment is used to enhance agent collaboration. In addition, an interactive paradigm introduces prior selection knowledge to help agents for more efficient exploration. Finally, extensive experiments on real-world datasets have demonstrated the improved performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data preprocessing is to make input data most appropriate for model training. Generally, two well-known preprocessing techniques are feature selection <ref type="bibr" target="#b10">[Liu and Motoda, 2012]</ref> and instance selection <ref type="bibr">[Brighton and Mellish, 2002]</ref>. Feature selection is to select most important features to increase predictive performance (e.g., accuracy) and reduce feature size. Instance selection shares the similar objective to simultaneously improve modeling accuracy and decrease instance size.</p><p>In prior literature, feature selection and instance selection are usually regarded as two separate problems. Limited algorithms have been proposed for their joint selection. The joint feature and instance selection was initially addressed using genetic algorithms <ref type="bibr">[Kuncheva and Jain, 1999]</ref>. Then some studies solve this problem with principal component analysis <ref type="bibr" target="#b13">[Suganthi and Karunakaran, 2019]</ref> or application of pairwise similarity <ref type="bibr">[Benabdeslem et al., 2020]</ref>. Others try to apply heuristic search and adopt simulated annealing algorithms <ref type="bibr" target="#b3">[De Souza et al., 2008]</ref> or sequential forward search <ref type="bibr" target="#b8">[Garc?a-Pedrajas et al., 2021]</ref>. In fact, the selection of each feature * Corresponding Author and each instance are mutually influenced and jointly decide the final selection results. However, most previous work of joint selection conducts feature/instance selection coarsely and neglects the fine-grained interaction of feature space and instance space, which largely hinders the predictive performances on the selected data.</p><p>Reinforcement Learning (RL), as an effective tool, has great potential to learn the optimal results for search problems like such selections <ref type="bibr" target="#b10">[Liu et al., 2019]</ref>. In particular, different agents (i) individually take actions step by step that is appropriate to model each fine-grained selection choice; (ii) mutually interact with each other that can capture the interaction between feature space and instance space; (iii) jointly targets for optimal decisions that can be regarded as the selected data of joint selection task. To this end, we make a first attempt to leverage reinforcement learning for the joint feature instance selection problem. For this goal, several challenges arise.</p><p>First, how to formulate the joint feature instance selection task with reinforcement learning? Feature/Instance selection usually repeats two steps: select a subset and test the performance. If viewed from the RL perspective, this exploratory process is an agent first selects features/instances and then observe the selected data to get reward. Considering the two selections, we naturally reformulate the joint selection with a dual-agent reinforcement learning paradigm. Specifically, we create two RL agents: 1) a feature agent aims to select the optimal feature subset; 2) an instance agent aims to select the optimal instance subset. The two agents perceive selected features and instances as the state of environment, collect data characteristics as reward, and interact with each other to search for optimum selection results.</p><p>Second, how can we enable the two agents to simultaneously and collaboratively conduct joint selection? On one hand, if the feature or instance agent selects a subset each time, the agent needs to make n binary selections on n features/instances. This results into action space is exponentially-increasing (2 n ) with the number of features/instances, leading to practical implementation difficulties. Thus, we propose a sequential-scanning mechanism for action design of agents. Specifically, we organize the selection decisions of features as a sequence and let the feature agent iteratively scan over this sequence to (de)select one feature each time. The instance agent adopts the same scanning strategy. This mechanism significantly reduces action space from exponential (2 n ) to binary choice and transforms selection as a sequential decision-making process, which is appropriate for RL. On the other hand, the performance of a downstream model highly depends on the joint quality of both features and instances. To achieve the global optimal of joint selection, the two agents need to collaborate to learn the mutual influence of features and instances. We thus develop a collaborative-changing environment for agents. We regard the environment as the selected data sub-matrix, where the columns (features) and rows (instances) are simultaneously changed by the actions of dual agents. This shared environment captures actions of the two agents and jointly sense the data quality in two dimensions.</p><p>Third, how can the two agents learn prior knowledge to improve learning efficiency? Interactive RL <ref type="bibr" target="#b0">[Amir et al., 2016]</ref> has shown its superiority on speeding up agent exploration by learning from human experts or prior knowledge. In this regard, we utilize two external trainers to teach the two agents respectively via interactive RL: we introduce 1) a random forest based trainer with knowledge of feature importance to teach feature agent to pick features. 2) an isolation forest based trainer to recognize instance anomaly to teach instance agent how to filter out instances. With the advice of the two trainers, the two agents can learn the patterns of spotting and selecting quality features and instances more efficiently.</p><p>In summary, we propose a dual-agent interactive reinforcement learning framework to model the interaction of the joint feature and instance selection. Our contributions are: (i) we formulate the joint selection task with dual-agent reinforcement learning; (ii) we propose a sequential-scanning mechanism and a collaborative-changing environment to achieve the simultaneous and interactive selection; (iii) we leverage interactive reinforcement learning to improve the learning efficiency; (iv) we conduct extensive experiments to demonstrate our improved performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Backgrounds</head><p>Concept 2.1 Dual-Agent Reinforcement Learning is a variant of multi-agent reinforcement learning. The dual-agent RL has two agents collaboratively accomplish two different tasks. For example, it has been applied to interactively generate bounding boxes and detect facial landmarks in computer vision <ref type="bibr" target="#b9">[Guo et al., 2018]</ref>. Concept 2.2 Interactive Reinforcement Learning is to provide agents with action advice from teacher-like trainers, so that agents learn learn the optimal decisions more efficiently in early exploration <ref type="bibr" target="#b0">[Amir et al., 2016]</ref>. Definition 2.3 Feature Selection. Given an input data matrix X ? R n * m , n and m denote the number of instances and features respectively; x ij is the element in the i-th row and jth column. We denote input features F = {f j } m j=1 , where the j-th feature is denoted by f j = {x 1j , x 2j , ..., x nj }. Feature selection aims to select an optimal subset F * ? F making downstream predictive model perform well. Definition 2.4 Instance Selection. As we mentioned above, for the data matrix X, all the instances are denoted as C = {c i } n i=1 where the i-th instance c i = {x i1 , x i2 , ..., x im }. Traditionally, instance selection studies aim to remove data noise or outliers, and find an instance subset C * ? C that has the same performances as C for the downstream predictive task <ref type="bibr" target="#b14">[Wilson and Martinez, 2000]</ref>. Definition 2.5 The Joint Feature and Instance Selection Task is to simultaneously find the optimal feature subset F * and the optimal instance subset C * , in order to achieve the best performances in a downstream predictive task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We address aforementioned challenges and propose a framework which called Dual-Agent Interactive Reinforced Selection (DAIRS), to model joint feature and instance selection task and introduce prior selection knowledge to agents for interactive reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DAIRS</head><p>As a reinforcement learning based framework, the DAIRS framework consists of dual agents, actions, states, reward and trainers. Specifically, Dual Agents. The two agents are: the Feature Agent which models featurefeature correlation to select an optimized feature subset; the Instance Agent which models instance-instance correlation to select an optimized instance subset. However, the local optimal in feature or instance selection can't guarantee the global optimal of joint feature-instance selection. As a result, the two agents need to strategically collaborate and transfer knowledge between feature and instance selections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Actions.</head><p>The dual agent action design is critical, because we need to consider: (i) the action space that determines the computational complexity and the learning efficiency of agents; (ii) the fine-grained interaction between feature space and instance space. The two considerations make it impossible to directly apply classic multi-agent reinforced selection <ref type="bibr" target="#b10">[Liu et al., 2019]</ref> to joint features and instance selection. To tackle this challenge, we develop a sequential-scanning with restart mechanism (Figure <ref type="figure" target="#fig_0">1</ref>). Specifically, Actions of the feature agent: are to sequentially scan all the m features and then restart scanning of the features over steps, where each step will select or deselect one feature. Let us denote a feature action record by a F = {a i } m i=1 , where the i-th action a i = 1 or a i = 0 means to select or deselect the i-th feature. Since the feature agent will restart the sequential scanning of m features, at Step t, the feature agent decides to select or deselect f t (mod m) . Actions of the instance agent: are to sequentially scan all the n instances, and then restart scanning of the instances over steps, where each step will select or deselect one instance. Let us denote an instance action record as a I = {a i } n i=1 , where the i-th action a i = 1 or a i = 0 means to select or deselect the instance c i . At Step t, the instance agent decides to select or deselect instance c t (mod n) .</p><p>The sequential scanning strategy allows features and instances to be simultaneously selected or deselected. The dual interaction will coordinate the scanning actions of the two agents to generate a globally optimized data subspace of instances and features. State of the Environment. Instead of separately creating two environments for feature agent and instance agent, we develop a sharing environment to support simultaneous interaction between agents. The state is to quantitatively represent the situation of the collaboratively-changing environment. However, this is a non-trivial task: at the t-th step, the action records of the feature and instance agents, denoted by a F t and a I t , can respectively derive a feature subset F and an instance subset C . The two subsets jointly form a sub-matrix of the input data X. The challenge is that the selected sub-matrix X cannot directly be regarded as the state, because its dimensions change dynamically over time, while learning the policy networks of the dual agents require a fixed-length state representation.</p><p>To address this challenge, we develop a dynamic state representation method inspired by the image processing technique <ref type="bibr" target="#b11">[Lu and Weng, 2007]</ref>. Specifically, by regarding the selected data sub-matrix as a 2-dimentional image, we first fix the state dimensions by padding the deselected positions with the padding token (zero). Formally, for the input data X ? R n * m , at the t-th step, h t is measured by:</p><formula xml:id="formula_0">h t = a I t T || ? ? ? ||a I t T m ?X ? a F t T || ? ? ? ||a F t T n T (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where ? is element-wise product, T is the transpose of a given matrix, a I t and a F t are the action records of feature agent and instance agent at the step t; m is the number of features; n is the number of instances. We then utilize a single convolutional layer to output the final representation. Formally, the state representation at Step t is computed by:</p><formula xml:id="formula_2">s t = Conv(w s h t + b s )</formula><p>(2) where w s , b s is tuning weight parameters and bias, and Conv is the convolution operation. Figure <ref type="figure" target="#fig_1">2</ref> shows the state representation process of our proposed collaborative-changing environment for dual agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward.</head><p>The reward r is to inspire the exploration of the feature agent and instance agent. Since the actions of dual agents are sequential scanning, we measure the reward based on the characteristic difference between last state and current state, in order to temporally train reinforcement learning decisionmaking process <ref type="bibr" target="#b13">[Sutton and Barto, 2018]</ref>. Specifically, supposing there are metrics set K we are caring about, we measure the performance difference ? i at Step t: where k t i means the i-th metric of the selected data subset at Step t. Then, the overall reward r at Step t is measured by:</p><formula xml:id="formula_3">? t i = k t i -k t-1 i (3) ? ? ? ? ? ? ?</formula><formula xml:id="formula_4">r t = 1 |K| ki?K ? t i .<label>(4)</label></formula><p>The actions of feature agent and instance agent collaboratively change the state, which directly determine the reward measurement. Then the reward is shared between the two agents to inspire exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trainers.</head><p>We introduce the concept of teacher-like trainers from interactive reinforcement learning into our framework. We develop a random forest trainer for the feature agent and an isolation forest trainer for the instance agent. The two trainers can guide agents to explore better selection policies. More details are in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Training</head><p>Figure <ref type="figure" target="#fig_2">3</ref> shows an overview of our framework. Dual agents have their own Deep Q-Networks (DQNs) <ref type="bibr" target="#b11">[Mnih et al., 2013;</ref><ref type="bibr" target="#b15">Zhang et al., 2021]</ref> as action policies. Two external trainers respectively guide the feature agent and the instance agent for more efficient exploration. The actions of the dual agents are to sequentially scan features and instances, which collaboratively decide the selected data sub-matrix X . Then the state of the environment is derived from the sub-matrix and the reward is collected to inspire both feature agent and instance agent to select data.</p><p>To evaluate the policy, we use feed-forward neural network to approximate the Q value. The deep Q-learning iteratively optimizes the following loss function:</p><formula xml:id="formula_5">E(r + ? max a Q ? (s , a ) -Q ? (s, a)) 2 (5)</formula><p>where r is the reward of action a in state s; a is the next action in next state s ; ? is a discount factor; Q ? is the target network. For the function approximation, the Q function is with parameter ?. The gradient descent is:</p><formula xml:id="formula_6">? t+1 ? ? t + ?(r + ? max a Q ? (s , a ) -Q ? t (s, a)) (6)</formula><p>where ? is the learning rate and t is the training step. By optimizing ?, the action policies of agents are constructed in order to maximize the long-term reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selected Data</head><p>Collaborative-changing Environment </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Guiding Dual Agents via Interactive Reinforcement with External Trainers</head><p>Figure <ref type="figure" target="#fig_2">3</ref> shows how we leverage the prior knowledge of external trainers (i.e., classical feature selection and instance filtering methods) to guide feature agent and instance agent to improve their learning efficiency.</p><p>Guiding Feature Agent with Random Forest Trainer.</p><p>Random forest classifier <ref type="bibr" target="#b13">[Pal, 2005]</ref> can learn a set of decision trees to measure feature importance. We propose a trainer, namely random forest trainer, for the feature agent.</p><p>The intuition is that feature importance can provide decision making support to the feature agent: if the trainer found a feature important, it would be suggested to select the feature;</p><p>if not important, it would be suggested to deselect the feature. These advice can make the feature agent more aware of characteristics of feature space. We develop a three-step algorithm as follows:</p><p>Step 1: We train a random forest classifier on the given m features {f 1 , f 2 , ..., f m }, to obtain the importance of each feature, denoted by {imp 1 , imp 2 , ..., imp m }.</p><p>Step 2: Based on the feature importance, we design a selection probability distribution p RF {p 1 , p 2 , ..., p m } for the m features, where the probability for i-th feature is given by:</p><formula xml:id="formula_7">p i = 1 imp i &gt; ? m m * imp i imp i ? ? m (7)</formula><p>where ? is a parameter to control suggested features.</p><p>Step 3: Based on the probability, we sample an advised action list each step, denoted by a RF . The feature agent follows a RF to take actions at the beginning steps of exploration.</p><p>Guiding Instance Agent with Isolation Forest Trainer. We aim to leverage the ability of external trainers for recognizing noisy or perturbing data samples to provide advice to instance agent for instance selection. Our underlying intuition is that bad instances differ from normal instances in the feature space <ref type="bibr" target="#b0">[Aggarwal and Yu, 2001]</ref>, and an agent can follow how classic instance filtering methods identify them.</p><p>We propose to identify bad-performed instances via an outlier detection algorithm and let instance agent try to rule out these points. Based on isolation forest <ref type="bibr" target="#b10">[Liu et al., 2008]</ref>, an outlier detector, we propose another trainer called isolation forest trainer. We show how this trainer gives advice to the instance agent step by step:</p><p>Step 1: We first group instances {c 1 , c 2 , ..., c n } based on their labels Y{y 1 , y 2 , ..., y n } in classification tasks. We denote these instance groups by G = {G l1 , G l2 , ..., G lp }, where distinct labels {l 1 , l 2 , ..., l p } = set(Y) and the group towards</p><formula xml:id="formula_8">k-th distinct label l k is by G l k = {c i | y i = l k &amp; y i ? Y}.</formula><p>Step 2: For each group in G, we utilize the isolation forest algorithm to detect and filter outlier points. The filtered results are by</p><formula xml:id="formula_9">G IF = {IF (G l1 ), IF (G l2 ), ..., IF (G lp )},</formula><p>where IF is the filtering operation with isolation forest.</p><p>Step 3: We derive the advised action list A IF from filtered results G IF . Specifically, for a IF {a 1 , a 2 , ..., a n } , the i-th advised action</p><formula xml:id="formula_10">a i = 1 if c i ? G IF and a i = 0 if c i / ? G IF .</formula><p>Step 4: At several beginning steps of the exploration in reinforcement learning, the instance agent follows the advice and take the advised actions for better training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets and Metrics: we use four public datasets of different domains on classification task to validate our methods: ForestCover (FC) dataset is a publicly available dataset from Kaggle<ref type="foot" target="#foot_0">1</ref> including characteristics of wilderness areas. Madelon dataset is a Nips 2003 workshop dataset containing data points grouped in 32 clusters and labeled by 1 and -1 <ref type="bibr">[Dua and Graff, 2017]</ref>. Spam dataset is a collection of spam emails <ref type="bibr">[Dua and Graff, 2017]</ref>. USPS dataset is a handwritten digit database including handwritten digit images <ref type="bibr" target="#b1">[Cai et al., 2010]</ref>. We report Accuracy and F1-score of certain downstream models to show the quality of selected data. Baseline Algorithms: We compare predictive performance of our proposed model with different baselines, including instance selection methods, feature selection methods and feature instance joint selection methods: DROPs. It includes different instance reduction algorithms where we take DROP1 and DROP5 as baselines <ref type="bibr" target="#b14">[Wilson and Martinez, 2000]</ref>. GCNN. <ref type="bibr" target="#b2">[Chou et al., 2006]</ref> proposes the weak criterion employed by Condensed Nearest Neighbor. LASSO.  <ref type="bibr">et al., 2020]</ref> uses a similarity preserving for co-selection of features and instances. For evaluation, we use these algorithms to select features/instances to get the data subset, and evaluate the quality of selected data towards prediction.</p><p>Implications: To compare fairly, we set the downstream task as a simple classifier logistic regression. The downstream task takes selected data as input and output the classification results. We randomly split the data into train data (70%) and test data (30%) where categorical features are encoded in onehot. Baseline models are set the same selection ratio as our model. The size of memory unit is set to 300 in experience replay. For the reward measurement, we consider accuracy, relevance score and redundancy score following <ref type="bibr" target="#b10">[Liu et al., 2019]</ref>. The policy networks are set as two fully-connected layers of 512 middle states with ReLU as activation function.</p><p>In RL exploration, the discount factor ? is set to 0.9, and we use -greedy exploration with equal to 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performances</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the overall predictive performance of logistic regression on the data selected by our proposed DAIRS and compared baselines. We report on four different datasets with respect to accuracy and f1-score and can observe our proposed model outperforms other selection methods, which signifies the best quality of our selected data for downstream predictions. Apart from our model, we notice other joint selection methods (e.g., IFS-CoCo) can have better performance than other baselines, for these methods consider both two dimensions (feature selection and instance selection). Accordingly, as it shows in Table <ref type="table">2</ref>, our model can also acquire the best selection ratio to achieve higher predictive performance in exploration, while many other methods (e.g., DROP, RFE) cannot automatically learn the ratio that needs to be predefined. This demonstrates the superiority of our methods for  automating to select the best data without handcrafted tunning. Also, it is easy to find out in most cases, most features and instances are useful for accurate predictions, while a few ones disturb predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Study of the Dual-agent Reinforced Selection</head><p>We aim to study the impacts of different components in our dual-agent reinforced selection framework. We consider four variant methods: (1) FA removes the instance agent, only considers the Feature Agent and creates environment for selected features as <ref type="bibr" target="#b10">[Liu et al., 2019]</ref>.</p><p>(2) IA removes the feature agent and only considers Instance Agent. (3) IA+FA-CE removes Collaborative-changing Environment and creates an independent environment for each agent. (4) IA+FA is our proposed DAIRS model with two agents. Figure <ref type="figure">4</ref> and 5 show performance comparison of these variants on FC and Madelon dataset. Compared to single agent's exploration, we can easily observe a performance gain by dual agents. Moreover, while both features and instances decide quality of the selected data, feature agent is more influential on predictions than instance agent when single agent explores. This suggests feature's quality is more important in data selection. We also find out the sharing environment is actually important for dual-agent coordination, which can make for better performance. From both figures, we observe the trend of blue line is more significant than yellow line; this shows our method achieves better results in long-term learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Study of Interactive Reinforced Trainer</head><p>We study the impacts of proposed trainers for interactive reinforcement learning. We consider four variants: (1) Nontrainer removes both random forest trainer (RFT) and isola- Figure <ref type="figure">6</ref> shows comparisons of variant methods. It can be observed that both two trainers help improve the data quality for better predictive performances, while the highest score is achieved when combining two trainers. Figure <ref type="figure">7</ref> shows the efficiency comparison of each variant in terms of the explored best accuracy until current step. The result signifies that without trainers' help, agents need more steps for exploration to achieve better results, and both trainers can help for more efficient exploration and agent learning, especially when both the feature agent and the instance agent take external advice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Case Study of Exploration Process</head><p>We also try to study and visualize the exploration process of agents. Figure <ref type="figure" target="#fig_4">8</ref> shows the accuracy in different steps with respect to selected feature number and instance number on FC dataset and Spam dataset. We visualize 10,000 points of exploration (colored in blue) and mark the top 10 best performed points in red. We easily observe that most blue points are on the certain part of data space, for example the space where feature number is (30 to 50) and instance number is (5,000 to 10,000) of the left subfigure. This signifies after the initial exploration, the exploration efficiently concentrates on the optimal data subspace. Then, the agents can continue searching until finally finding out the best feature and instance selection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Feature selection includes three kinds of methods: (1) Filtering methods rank features based on relevance scores and select top-ranking ones (e.g., univariate feature selection). ( <ref type="formula">2</ref> tion into the classifier construction to search for an optimal feature subset (e.g., <ref type="bibr">LASSO [Tibshirani, 1996]</ref>).</p><p>Instance selection mainly includes two kinds of methods:</p><p>(  <ref type="bibr">, 1999]</ref>. Then some studies solve this problem with greedy algorithms <ref type="bibr" target="#b14">[Zhang et al., 2012]</ref>, principal component analysis <ref type="bibr" target="#b13">[Suganthi and Karunakaran, 2019]</ref> or application of pairwise similarity <ref type="bibr">[Benabdeslem et al., 2020]</ref>. Most existing work towards joint selection tries to apply heuristic search; they adopt simulated annealing algorithms <ref type="bibr" target="#b3">[De Souza et al., 2008]</ref>, cooperative coevolutionary algorithms <ref type="bibr" target="#b4">[Derrac et al., 2012]</ref>, or sequential forward search <ref type="bibr" target="#b8">[Garc?a-Pedrajas et al., 2021]</ref>. The joint selection is also studied in clinical data setting <ref type="bibr" target="#b12">[Olvera-L?pez et al., 2010]</ref> and in social media data setting <ref type="bibr" target="#b14">[Tang and Liu, 2013]</ref>.</p><p>Reinforced Feature Selection has applied reinforcement learning for the feature selection task. Some studies use single agent for feature selection <ref type="bibr" target="#b7">[Fard et al., 2013;</ref><ref type="bibr" target="#b15">Zhao et al., 2020]</ref>; others Multi-agent reinforcement learning have been used to automate feature selection <ref type="bibr" target="#b5">[Fan et al., 2020;</ref><ref type="bibr">Fan et al., 2021b;</ref><ref type="bibr">Fan et al., 2021a;</ref><ref type="bibr" target="#b10">Liu et al., 2019]</ref>. Inspired by these work, we apply reinforcement learning into the joint selection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">conclusion</head><p>In this paper, we propose a dual-agent reinforcement learning framework towards the feature and instance joint selection task. We formulate the joint selection into a reinforcement learning framework with the tailor-designed sequentialscanning mechanism and collaboratively-changing environment, in order to simulate fine-grained interaction of feature space and instance space. The selection knowledge of the random forest trainer and the isolation forest trainer is applied to improve the efficiency of agent learning. The extensive experiments have demonstrated the superiority of our model on data preprocessing, which also reveals a workable design of reinforcement learning on the joint selection task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Actions of dual agents with sequential scanning. Agents iteratively scan over each feature or each instance to make the select/deselect decisions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: State of collaborative-changing environment, coordinating with actions of the feature agent and the instance agent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Framework Overview. Two agents collaboratively act and learn. Two trainers come to advise to help training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of DAIRS variants on Madelon dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Visualizations of agent exploration process on FC and Spam dataset. Points in red are top 10 best performed steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Predictive performance of different selection methods with logistic regression as the downstream model. RFE (Recursive Feature Elimination) recursively deselects the least important features. LS2AOD selects features via Laplacian Score and then samples via AOD. Ge-neticFSIS.<ref type="bibr" target="#b14">[Tsai et al., 2013]</ref> applies genetic algorithms to alternatively select features and instances. IFS-CoCo.<ref type="bibr" target="#b4">[Derrac et al., 2010]</ref> applies the cooperative co-evolutionary algorithm to select features and instances. sCOs. [Benabdeslem</figDesc><table><row><cell></cell><cell>Dataset</cell><cell></cell><cell>FC</cell><cell></cell><cell cols="2">Madelon</cell><cell>Spam</cell><cell></cell><cell>USPS</cell></row><row><cell>Model</cell><cell></cell><cell cols="8">Accuracy F1-score Accuracy F1-score Accuracy F1-score Accuracy F1-score</cell></row><row><cell cols="2">DROP1</cell><cell>58.928</cell><cell cols="2">59.477</cell><cell>51.923</cell><cell>52.048</cell><cell>89.572</cell><cell>89.531</cell><cell>86.559</cell><cell>86.464</cell></row><row><cell cols="2">DROP5</cell><cell>64.682</cell><cell cols="2">64.975</cell><cell>52.435</cell><cell>52.431</cell><cell>89.717</cell><cell>89.718</cell><cell>91.612</cell><cell>91.607</cell></row><row><cell cols="2">GCNN</cell><cell>61.419</cell><cell cols="2">61.460</cell><cell>53.717</cell><cell>53.732</cell><cell>90.803</cell><cell>90.846</cell><cell>93.570</cell><cell>93.585</cell></row><row><cell cols="2">LASSO</cell><cell>61.684</cell><cell cols="2">62.705</cell><cell>54.230</cell><cell>54.223</cell><cell>90.658</cell><cell>90.699</cell><cell>91.899</cell><cell>91.937</cell></row><row><cell></cell><cell>RFE</cell><cell>65.828</cell><cell cols="2">66.430</cell><cell>55.256</cell><cell>55.261</cell><cell>88.776</cell><cell>88.862</cell><cell>93.763</cell><cell>93.786</cell></row><row><cell cols="2">LS2AOD</cell><cell>65.057</cell><cell cols="2">65.665</cell><cell>53.974</cell><cell>53.983</cell><cell>90.948</cell><cell>90.978</cell><cell>93.293</cell><cell>93.343</cell></row><row><cell cols="2">GeneticFSIS</cell><cell>65.834</cell><cell cols="2">66.033</cell><cell>55.938</cell><cell>55.837</cell><cell>90.742</cell><cell>90.675</cell><cell>93.242</cell><cell>93.321</cell></row><row><cell cols="2">IFS-CoCo</cell><cell>66.102</cell><cell cols="2">65.723</cell><cell>58.384</cell><cell>58.323</cell><cell>91.042</cell><cell>91.021</cell><cell>93.417</cell><cell>93.519</cell></row><row><cell cols="2">sCOs</cell><cell>66.213</cell><cell cols="2">66.104</cell><cell>57.992</cell><cell>57.976</cell><cell>90.810</cell><cell>90.632</cell><cell>93.792</cell><cell>93.738</cell></row><row><cell cols="2">DAIRS (Ours)</cell><cell>67.328</cell><cell cols="2">66.908</cell><cell>61.282</cell><cell>61.281</cell><cell>91.745</cell><cell>91.389</cell><cell>94.122</cell><cell>94.071</cell></row><row><cell cols="5">Table 2: Selection ratio on different datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>FC</cell><cell>Madelon</cell><cell>Spam</cell><cell>USPS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Feature 0.8703</cell><cell>0.5980</cell><cell cols="3">0.8771 0.7187</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Instance 0.7483</cell><cell>0.8829</cell><cell cols="3">0.6770 0.6266</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">[Tibshirani, 1996] conducts feature selection and shrinkage</cell><cell></cell><cell></cell><cell></cell></row><row><cell>via l1 penalty.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1) wrapper methods (e.g., k-NN based selection); (2) filter methods (e.g., kd-trees). The criterion is based on the accuracy obtained by a classifier. Most of wrapper methods are based on k-NN classifier. Other methods select instances by using SVM or Evolutionary algorithms, or are accomplished by finding border instances [Olvera-L?pez et al., 2010]. Joint selection of Feature and Instance has also been made some efforts to study; however, limited algorithms were proposed to tackle feature and instance selection simultaneously. The joint selection is firstly studied by genetic algorithms [Kuncheva and Jain</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.kaggle.com/c/forest-cover-type-prediction/data</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was partially supported by the <rs type="funder">National Science Foundation (NSF)</rs> grant <rs type="grantNumber">2040950</rs>, <rs type="grantNumber">2006889</rs>, <rs type="grantNumber">2045567</rs>, IIS-2040799, IIS-2006387, IIS-1814510.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_s8nCxj9">
					<idno type="grant-number">2040950</idno>
				</org>
				<org type="funding" xml:id="_mu9UGux">
					<idno type="grant-number">2006889</idno>
				</org>
				<org type="funding" xml:id="_PuTkcRs">
					<idno type="grant-number">2045567</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Khalid Benabdeslem, Dou El Kefel Mansouri, and Raywat Makkhongkaew. scos: Semi-supervised co-selection by a similarity preserving approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu ; Charu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip S Yu ;</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2001 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>Henry Brighton and Chris Mellish</publisher>
			<date type="published" when="2001">2001. 2001. 2016. 2016. 2020. 2002. 2002</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="153" to="172" />
		</imprint>
	</monogr>
	<note>Data mining and knowledge discovery</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph regularized nonnegative matrix factorization for data representation</title>
		<author>
			<persName><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The generalized condensed nearest neighbor rule as a data reduction method</title>
		<author>
			<persName><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition (ICPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="556" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Jerffeson Teixeira De Souza, Rafael Augusto Ferreira Do Carmo, and Gustavo Augusto Lima De Campos. A novel approach for integrating feature and instance selection</title>
		<author>
			<persName><forename type="first">De</forename><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 International Conference on Machine Learning and Cybernetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="374" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Integrating instance selection, instance weighting, and feature weighting for nearest neighbor classifiers by coevolutionary algorithms</title>
		<author>
			<persName><surname>Derrac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1383" to="1397" />
			<date type="published" when="2010">2010. 2010. 2012. 2012. 2017</date>
		</imprint>
	</monogr>
	<note>Pattern Recognition. Dua and Graff, 2017] Dheeru Dua and Casey Graff. UCI machine learning repository</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autofs: Automated feature selection via diversity-aware interactive reinforcement learning</title>
		<author>
			<persName><forename type="first">Fan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1008" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interactive reinforcement learning for feature selection with decision tree in the loop</title>
		<author>
			<persName><forename type="first">Fan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 2021 SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2021">2021. 2021. 2021</date>
			<biblScope unit="page" from="342" to="350" />
		</imprint>
	</monogr>
	<note>Autogfs: Automated group-based feature selection via interactive reinforcement learning</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using reinforcement learning to find an optimal set of features</title>
		<author>
			<persName><surname>Fard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Mathematics with Applications</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1892" to="1904" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Si (fs) 2: Fast simultaneous instance and feature selection for datasets with many features</title>
		<author>
			<persName><surname>Garc?a-Pedrajas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">107723</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kuncheva and Jain, 1999] Ludmila I Kuncheva and Lakhmi C Jain. Nearest neighbor classifier: Simultaneous editing and feature selection</title>
		<author>
			<persName><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="1999">2018. 2018. 1999</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1149" to="1156" />
		</imprint>
	</monogr>
	<note>Dual-agent deep reinforcement learning for deformable face tracking</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automating feature subspace exploration via multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Motoda</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Motoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2008">2012. 2012. 2008. 2008. 2008. 2019. 2019</date>
			<biblScope unit="volume">454</biblScope>
			<biblScope unit="page" from="207" to="215" />
		</imprint>
	</monogr>
	<note>Eighth IEEE International Conference on Data Mining</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A survey of image classification methods and techniques for improving classification performance. International journal of Remote sensing</title>
		<author>
			<persName><forename type="first">Weng</forename><forename type="middle">;</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengsheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<editor>Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller</editor>
		<imprint>
			<date type="published" when="2007">2007. 2007. 2013. 2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="823" to="870" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Playing atari with deep reinforcement learning</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A review of instance selection methods</title>
		<author>
			<persName><surname>Olvera-L?pez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="143" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Instance selection and feature extraction using cuttlefish optimization algorithm and principal component analysis using decision tree</title>
		<author>
			<persName><forename type="first">Mahesh</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; M</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Suganthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Karunakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cluster Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="101" />
			<date type="published" when="2005">2005. 2005. 2019. 2018. 2018</date>
			<publisher>MIT press</publisher>
			<pubPlace>Barto</pubPlace>
		</imprint>
	</monogr>
	<note>Random forest classifier for remote sensing classification. Reinforcement learning: An introduction</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tibshirani, 1996] Robert Tibshirani. Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">Liu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 SIAM International conference on data mining</title>
		<meeting>the 2013 SIAM International conference on data mining</meeting>
		<imprint>
			<publisher>Wilson and Martinez</publisher>
			<date type="published" when="1996">2013. 2013. 1996. 2013. 2013. 2000. 2000. 2012. 2012</date>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="2379" to="2388" />
		</imprint>
	</monogr>
	<note>IEEE Transactions on Image Processing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Intelligent electric vehicle charging recommendation based on multi-agent reinforcement learning</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09230</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2020">2021. 2021. 2020</date>
			<biblScope unit="page" from="1856" to="1867" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Simplifying reinforced feature selection via restructured choice strategy of single agent</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
