<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MEDALPACA -AN OPEN-SOURCE COLLECTION OF MEDICAL CONVERSATIONAL AI MODELS AND TRAINING DATA A PREPRINT</title>
				<funder ref="#_NvedeBy">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-18">April 18, 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>Han</surname></persName>
							<email>tianyu.han@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">University Hospital Aachen</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lisa</forename><forename type="middle">C</forename><surname>Adams</surname></persName>
							<email>lisa.adams@tum.de</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Diagnostic and Interventional Radiology</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jens-Michalis</forename><surname>Papaioannou</surname></persName>
							<email>michalis.papaioannou@bht-berlin.de</email>
							<affiliation key="aff2">
								<orgName type="institution">Berliner Hochschule f?r Technik (BHT)</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Grundmann</surname></persName>
							<email>pgrundmann@bht-berlin.de</email>
							<affiliation key="aff2">
								<orgName type="institution">Berliner Hochschule f?r Technik (BHT)</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Oberhauser</surname></persName>
							<email>tom.oberhauser@bht-berlin.de</email>
							<affiliation key="aff2">
								<orgName type="institution">Berliner Hochschule f?r Technik (BHT)</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexei</forename><surname>Figueroa</surname></persName>
							<email>alexei.figueroa@bht-berlin.de</email>
							<affiliation key="aff2">
								<orgName type="institution">Berliner Hochschule f?r Technik (BHT)</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexander</forename><surname>L?ser</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Berliner Hochschule f?r Technik (BHT)</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Truhn</surname></persName>
							<email>dtruhn@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology</orgName>
								<orgName type="institution">University Hospital Aachen</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Keno</forename><forename type="middle">K</forename><surname>Bressem</surname></persName>
							<email>keno-kyrill.bressem@charite.de</email>
							<affiliation key="aff3">
								<orgName type="department">Institute for Radiology</orgName>
								<orgName type="institution" key="instit1">Charit? -Universit?tsmedizin Berlin</orgName>
								<orgName type="institution" key="instit2">Freie Universit?t Berlin and Humboldt-Universit?t zu Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Berlin Institute of Health at Charit? -Universit?tsmedizin Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MEDALPACA -AN OPEN-SOURCE COLLECTION OF MEDICAL CONVERSATIONAL AI MODELS AND TRAINING DATA A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-18">April 18, 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2304.08247v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Natural Language Processing</term>
					<term>Artificial Intelligence</term>
					<term>Medicine</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As large language models (LLMs) like OpenAI's GPT series continue to make strides, we witness the emergence of artificial intelligence applications in an ever-expanding range of fields. In medicine, these LLMs hold considerable promise for improving medical workflows, diagnostics, patient care, and education. Yet, there is an urgent need for open-source models that can be deployed on-premises to safeguard patient privacy. In our work, we present an innovative dataset consisting of over 160,000 entries, specifically crafted to fine-tune LLMs for effective medical applications. We investigate the impact of fine-tuning these datasets on publicly accessible pre-trained LLMs, and subsequently, we juxtapose the performance of pre-trained-only models against the fine-tuned models concerning the examinations that future medical doctors must pass to achieve certification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The advent of large language models (LLMs), trained using reinforcement learning through human feedback (RLHF) and exemplified by OpenAI's GPT series, has profoundly influenced the fields of natural language processing (NLP) and artificial intelligence (AI) research <ref type="bibr" target="#b0">[1]</ref>. Their remarkable capacity to produce coherent, contextually apt, and intricate responses has increased their value across diverse domains. Notably, the medical field is poised to reap substantial benefits from the implementation of these models.</p><p>A salient benefit of these LLMs lies in their ability to perform tasks following instructions in natural language, thereby eliminating the necessity for users to have programming proficiency. This feature empowers medical professionals to seamlessly engage with and steer the models through diverse medical workflows.</p><p>Potential applications include aiding medical professionals in note-taking, composing discharge letters, retrieving information from extensive documents, summarizing content, and converting free-form texts into structured formats <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Provided the model has been trained on a sufficient number of medical documents, it may possess the medical knowledge necessary to assist in consultations by supplying accurate information derived from its base texts <ref type="bibr" target="#b3">[4]</ref>. Furthermore, the training of medical students can also benefit from these models, wherein they assume the role of a study partner, capable of quizzing students or elucidating complex subjects, provided the model demonstrates sufficient coherence and accuracy. However, the most adept LLM models are currently not openly accessible, being available exclusively through APIs that necessitate data transmission to the parent company for processing.</p><p>Considering the sensitive nature of medical data and the imperative for robust privacy safeguards, non-transparent models with unclear data management practices are ill-suited for medical applications. To tackle this challenge and avert unauthorized data transfers, it is essential to employ open-source models that enable on-site implementation, thus mitigating privacy concerns.</p><p>Addressing this demand, we present a compilation of language models specifically fine-tuned for biomedical tasks. Utilizing a blend of new and established open-source biomedical datasets, we adapt them into an instruction-following format. This structure facilitates supervised fine-tuning as the initial phase, as detailed in <ref type="bibr" target="#b0">[1]</ref>.</p><p>To assess the effectiveness of these models, we evaluate their performance on the United States Medical Licensing Examination (USMLE), a standardized assessment undertaken by medical students in the United States as part of their qualification process to become physicians. This evaluation offers valuable insights into the models' competencies and prospective applications within the medical domain.</p><p>We make all models and datasets publicly available, anticipating that they will confer significant advantages to both medical and AI researchers as well as practitioners in their respective fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Datasets</head><p>In this section, we present Medical Meadow a collection of medical tasks that we have compiled for fine-tuning and evaluating the performance of large language models in the context of medicine. Medical Meadow consists of two main categories, a collection of established medical NLP tasks reformatted in instruction tuning formats as well as a crawl of various internet resources. Each dataset focuses on different aspects of medical knowledge and practice, providing a comprehensive training and evaluation framework. See Table <ref type="table" target="#tab_0">1</ref> for a detailed overview of the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Dataset 1: Flash Cards Used by Medical Students</head><p>Medicine as a whole encompasses a wide range of subjects that medical students and graduates must master in order to practice effectively. This includes a profound understanding of basic medical sciences, clinical knowledge, and clinical skills. The Anki Medical Curriculum flashcards are created and updated by medical students and cover the entirety of the medical school curriculum, addressing subjects such as anatomy, physiology, pathology, pharmacology, and more. These flashcards frequently feature succinct summaries and mnemonics to aid in the learning and retention of important medical concepts. In our investigation, we leveraged flashcards as a source to create question-answer pairs for training purposes. Upon excluding cards containing images, we harnessed OpenAI's GPT-3.5-Turbo to restructure the cards into coherent, contextually pertinent question-answer pairs. Generally, the questions and answers are concise and targeted, as the flashcards offer limited space for incorporating extensive information. See Table <ref type="table">3</ref> for representative Q/A pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Dataset 2: Stackexchange Medical Sciences</head><p>The stackexchange dataset consists of 52,475 question-answer pairs obtained from five Stack Exchange forums related to biomedical sciences and related fields:</p><p>1. Academia: This forum offers insights into research methodologies, scientific publication processes, and career paths within the scientific community. While not directly affiliated with medicine, considering the volume of medical research, it is likely that medical professionals will also consult models pertaining to this subject matter. 2. Bioinformatics: As an interdisciplinary field combining biology, computer science, and data analysis, the Bioinformatics forum offers valuable information on the techniques and tools used for analyzing complex biological data, which is increasingly important in modern medical research. 3. Biology: Biology covers topics such as genetics, physiology, and molecular biology, which are all relevant to basic medical research. By including this forum, we aim to add core concepts of life sciences to the training data. 4. Fitness: This forum addresses the practical aspects of maintaining and improving physical health, including exercise routines, nutrition, and injury prevention. By incorporating the Fitness forum, we introduce models to health-related information that might be directly applicable to patient care and lifestyle recommendations. 5. Health: The Health forum covers a broad range of topics related to personal health, disease prevention, and medical treatments which could be directly transferable to medical care.</p><p>To maintain a high level of answer quality, we collected data exclusively from responses that received a minimum of five up-votes within the forum discussions and paired them with their corresponding questions. See Table <ref type="table" target="#tab_3">4</ref> for representative Q/A pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Dataset 3: Wikidoc</head><p>We incorporated medical question-answer pairs extracted from WikiDoc, a collaborative platform for medical professionals to share and contribute up-to-date medical knowledge. The platform has two main sub-sites, the "Living Textbook" and "Patient Information". The "Living Textbook" contains chapters for various medical specialties, which we crawled. We then used GTP-3.5-Turbo to rephrase the paragraph heading to a question and used the paragraph as answers. Patient Information is structured differently, in that each section subheading is already a question, making rephrasing obsolete. See Table <ref type="table" target="#tab_4">5</ref> for representative Q/A pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Dataset 4: medical NLP Benchmarks</head><p>We additionally use data from open NLP datasets and benchmarks, including:</p><p>1. The COVID-19 Open Research Dataset Challenge (CORD-19), consisting of more than one million scholarly articles [5] 2. Benchmark data from Measuring Massive Multitask Language Understanding <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> 3. Training data from the MedQA benchmark, question answering datasets consisting of medical exam questions <ref type="bibr" target="#b7">[8]</ref> 4. Training data from the Pubmed Causal Benchmark <ref type="bibr" target="#b8">[9]</ref> 5. Conversational data from medical forums as presented in <ref type="bibr" target="#b9">[10]</ref> 6. The OpenAssistant dataset. A Crowd sourced conversational dataset, especially targeted towards training models with RLHF</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Training</head><p>Our models are built upon the LLaMA (Large Language Model Meta AI) foundation models. LLaMA represents a cutting-edge large language model released by Meta, demonstrating their commitment to open science. It is available in various sizes, including 7 billion, 13 billion, 33 billion, and 65 billion parameters. In this study, we fine-tuned the 7 and 13 billion parameter LLaMA variants, adhering to the approach delineated by Taori et al <ref type="bibr" target="#b10">[11]</ref>.</p><p>We trained each model for five epochs, employing a learning rate of 2e -5 for the 7b model and 1e -5 for the 13b model, using a cosine learning rate scheduler. Gradient accumulation facilitated training with an effective batch size of 256.</p><p>Given that this training impacts all model parameters, the hardware requirements are substantial. Consequently, we explored alternative training procedures.</p><p>First, we implemented Low-Rank Adaptation (LoRA) for weight updates to adapt the pre-trained language models to our specific tasks. LoRA is a method that involves freezing the pre-trained model weights and incorporating trainable rank decomposition matrices into each layer of the Transformer architecture <ref type="bibr" target="#b11">[12]</ref>. This approach substantially diminishes the number of trainable parameters and GPU memory requirements for downstream tasks, making it more efficient compared to full fine-tuning and significantly reducing training time.</p><p>To further decrease memory and compute demands, we employed 8-bit matrix multiplication for the feed-forward and attention projection layers, along with an 8-bit optimizer. When combined with LoRA, this strategy further reduces the memory needed for training <ref type="bibr">[13] [14]</ref>. All models trained with LoRA underwent three epochs of training at a learning rate of 2e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Procedure</head><p>To evaluate the performance of the fine-tuned language models, we devised an assessment methodology centered on their zero-shot performance across the United States Medical Licensing Examination (USMLE) Step 1, Step 2, and Step 3 self-assessment datasets. We excluded all questions containing images, as our primary interest lies in the models' language capabilities, and they lack visual abilities. We instructed the models to present answers in the format "Option: Answer" (e.g., "A: Penicillin"). If a model's output did not adhere to this format, they were prompted up to five times until the response was generated in the desired format. If the model failed to provide the response in the desired format, the last response was retained.</p><p>Interestingly, most of the fine-tuned models typically produced answers in the correct format after the first prompt, while only the base LLaMA models required multiple prompts. We conducted separate evaluations for each model, measuring their accuracy on the USMLE Step 1, Step 2, and Step 3 datasets individually. This approach allowed us to gain a comprehensive understanding of the models' performance across the various stages of the medical licensing examination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Our findings on the USMLE test set are displayed in Table <ref type="table" target="#tab_1">2</ref>. Fine-tuned LLMs consistently surpassed the performance of their pre-trained-only counterparts. It is worth noting that while LoRa and 8-bit fine-tuning expedited the training process, employing these methods resulted in reduced accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion and conclusion</head><p>In this study, we introduced a novel, high-quality collection of medical text data specifically designed for training instruction-following, medical large language models (LLMs). This dataset serves as a comprehensive resource for enhancing LLM performance in the medical domain, laying the groundwork for potential integration into medical education and practice.  <ref type="bibr" target="#b15">[16]</ref>. This approach is vital, as full fine-tuning of language model parameters is often unfeasible for most academic institutions. Our study demonstrates the viability of parameterefficient fine-tuning.</p><p>We evaluated LLM performance using the United States Medical Licensing Examination (USMLE) for Steps 1, 2, and 3, which assess medical knowledge at various complexity levels. As expected, performance improved with larger pre-trained models. Applying approximation techniques, such as 8-bit precision and LoRa, during fine-tuning yielded less optimal results. However, due to considerable computational costs, we did not conduct extensive hyperparameter optimization and fine-tuning; thus, it may be possible to achieve performance comparable to vanilla-trained models through more thorough hyperparameter optimization, which we leave for future research.</p><p>The availability of additional medical datasets will likely enhance the applicability and performance of these models, creating various potential applications such as extracting structured medical information from unstructured text, supporting medical students' education through question-answering interactions to reinforce their knowledge and clarify lecture uncertainties, or assisting patients in understanding their health and improving communication between doctors and patients who often find medical language challenging. Nevertheless, implementing LLMs for these application scenarios presents challenges and concerns. Ensuring data privacy and compliance with ethical standards is critical when handling sensitive patient data; these concerns can be addressed by deploying models locally within secure hospital networks. Moreover, models must be thoroughly evaluated and safeguarded for potential biases and inaccuracies to prevent unintended consequences in medical decision-making. A significant limitation is LLMs' tendency to confabulate or generate text that appears plausible but is factually incorrect <ref type="bibr" target="#b16">[17]</ref>. This issue is especially concerning in the medical domain, where disseminating incorrect information can have serious implications for patient care and safety. Guaranteeing the accuracy and reliability of generated information is therefore essential, necessitating rigorous evaluation and continuous monitoring to mitigate confabulation risks and the potential harm it may cause in medical settings.</p><p>In conclusion, our work substantially contributes to the field of LLMs in medicine by providing a novel, high-quality medical dataset for research and application purposes. Further, we successfully fine-tuned and evaluated various LLMs, demonstrating that their medical domain performance increases with pre-trained model size and high-quality data availability. This progress paves the way for further exploration and development of LLMs in medicine, with potential implications for medical education, patient care, and healthcare communication.</p><p>Table <ref type="table">3</ref>: Representative question from the medical flashcards dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answer</head><p>What is the relationship between very low Mg2+ levels, PTH levels, and Ca2+ levels?</p><p>Very low Mg2+ levels correspond to low PTH levels which in turn results in low Ca2+ levels.</p><p>What are the cautionary considerations and contraindications for the use of ?-blockers?</p><p>?-blockers must be used cautiously in decompensated heart failure and are contraindicated in cardiogenic shock.</p><p>What is the recommended next step in management for a stable child who is suspected of having intussusception and has a "target sign" on ultrasound?</p><p>The recommended next step in management for a stable child with suspected intussusception and a "target sign" on ultrasound is to undergo an ultrasoundguided air contrast enema. This procedure involves using a small amount of air and contrast material to inflate the colon and push the telescoped intestine back into place. It is a non-invasive and effective method for both diagnosing and treating intussusception in children. In some cases, the procedure may need to be repeated or followed by surgery if the intussusception cannot be reduced. It is important to treat intussusception promptly to avoid complications such as bowel obstruction, bowel perforation, or sepsis.</p><p>What condition should be suspected in a young infant who is healthy and active and has bloodtinged stools? What is the cause of this condition and how is it managed?</p><p>The presence of blood-tinged stools in a young infant who is healthy and active should raise suspicion of protein-induced proctocolitis. This condition is caused by an intolerance to milk or soy protein, which can cause inflammation in the lining of the rectum. The management of protein-induced proctocolitis involves continuing breastfeeding and advising the mother to avoid dairy and soy products in her diet. In most cases, this will lead to resolution of the symptoms within a few days to a few weeks. If the symptoms persist, further evaluation may be necessary to rule out other causes of gastrointestinal bleeding.</p><p>Which muscle abducts the arm from 0 to 15 ? and what is the name of the nerve that innervates it?</p><p>The muscle that abducts the arm from 0 to 15 What is a risk factor for pyelonephritis related to the urinary system?</p><p>One risk factor for pyelonephritis is vesicoureteral reflux.</p><p>Which type of fibrillation is a fatal arrhythmia that requires immediate CPR and defibrillation to prevent death?</p><p>Ventricular fibrillation is the type of fibrillation that is a fatal arrhythmia without immediate CPR and defibrillation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answer</head><p>Given the thalidomide birth defect disaster of the 1960s, why was it later approved by the U.S. FDA for treatment of leprosy (1996) and of multiple myeloma (2006)? Presumably, the teratogenic issues and problems with permanent nerve damage didn't go away, as the liver can convert the "good" enantiomer to the "bad." Thalidomide affects even non-pregnant women, so why approve a drug with known severe complications?</p><p>Because leprosy and multiple myeloma are conditions for which other treatment options are limited. All treatments have side effects, some more debilitating than others. The trick with pharmacology is to balance the benefit of treatment with the hazards of the side effects. Take cancer chemotherapy. The drugs we use for cancer chemotherapy are horrible poisons. We literally use derivatives of chemical weapons to treat cancer. Why do we deliberately poison cancer patients with what amounts to a weapon of mass destruction? Because if we don't, the cancer will kill them. In pharmacology there's a concept of a therapeutic window. That is, there's a particular amount of compound that will cause death or other severe disability. There's another (hopefully lower) level of the compound that will effectively treat the disease. In treating, we hope to hit that middle ground: enough to treat the disease, but not enough to kill the patient. Different drugs have different therapeutic window. The window for over-the-counter drugs like asprin is rather large: the amount needed for severe side effects is many times higher than what people take to treat headaches. That's why we can allow untrained people to self-dose. (Though look at acetaminophen/paracetamol as an over-the-counter example where the window is not as large as it probably should be.)</p><formula xml:id="formula_0">[...]</formula><p>What is the way they determine someone died before declaring them dead? There are stories I've heard of over the years, people who's heart stopped, were brain dead, fell under ice rivers for half an hour and miraculously survived. So there are many instances of unexplained recoveries. How do the medical teams or doctors determine then that this person won't all of sudden get a heart beat back?</p><p>You are correct that this happens. It is infrequent (there are not "many", as you say, compared to true deaths), but it occasionally happens that someone -even in a hospital -is thought to be dead when they are not actually dead. In one of your stories, the girl was presumed to be brain dead, not dead. So strike that one. Hypothermia is a beast all unto itself, and declaring a cold person dead is a bit tricky.* Determining death is not simple. The International Guidelines for the Determination of Death -Phase I (May 30-31, 2012) Montreal Forum Report is 46 pages long and it still doesn't have a definitive conclusion. For the most part (and to simplify a bit), death is determined to have occurred when someone is exceedingly unlikely (determined from experience of millions of deaths) to regain function of their heart. It can be from a very wide variety of causes, but basically it follows cardiac arrest or respiratory arrest leading to cardiac arrest. The procedure is to observe the patient carefully. In hospital, that usually includes electronic monitors of one sort or another. Out of hospital it's by observation. When there is no evidence of cardiac electrical activity capable of generating a pulse, the patient has not been breathing for some time, oxygenation of blood has fallen to beyond critical levels, and there is no neurological activity, they are pronounced dead.[...]</p><p>It seems there are a ton of easy ab exercises, but very few for the lower back to keep the muscles balanced as your abs get stronger. What are simple home exercises for the lower back?</p><p>Try rolling over onto your stomach and doing a "superman". You can hold tension in the position, or do reps similar to doing "reverse situps". ref site Also, you might think about doing slow squats or wall sits where you focus on contracting both your abs and lower back. Tense up so as to make the line between anus and belly-button as short as possible during this exercisemaybe you can replace your ab work at the same time!</p><p>The accepted range for the wavelengths of light that the human eye can detect is roughly between 400nm and 700nm. Is it a coincidence that these wavelengths are identical to those in the Photosynthetically Active Radiation (PAR) range (the wavelength of light used for normal photosynthesis)? Alternatively is there something special about photons with those energy levels that is leading to stabilising selection in multiple species as diverse as humans and plants?</p><p>Good question. If you look at the spectral energy distribution in the accepted answer here, we see that photons with wavelengths less than 300 nm are absorbed by species such as ozone. Much beyond 750 infrared radiation is largely absorbed by species such as water and carbon dioxide. Therefore the vast majority of solar photons reaching the surface have wavelengths that lie between these two extremes. Therefore, I would suggest that surface organisms will have adapted to use these wavelengths of light whether it be used in photoreceptors or in photosynthesis since these are the wavelengths available; i.e., organisms have adapted to use these wavelengths of light, rather than these wavelengths being special per se (although in the specific case of photosynthesis there is a photon energy sweet spot). For example this study suggests that some fungi might actually be able to utilize ionizing radiation in metabolism. This suggests that hypothetical organisms on a world bathed in ionizing radiation may evolve mechanisms to utilize this energy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answer</head><p>What is the explanation for the epidemiology and demographics of anaphylaxis?</p><p>In 2006, the lifetime prevalence of Anaphylaxis was estimated to be 0.05% to 2% in the United States. However it is believed that this is an underestimation because the disease is underdiagnosed. The mortality rate of Anaphylaxis is approximately 186 to 225 deaths/year. Patients of all age groups may develop anaphylaxis, however, children and adolescents account for the majority of cases. There is no racial predilection to anaphylaxis. Anaphylaxis affects men and women equally with more women being admitted to the hospital for the disease. There is no regional predilection to anaphylaxis. An estimated 1.24% to 16.8% of the United States population is considered at risk for developing anaphylaxis if they are exposed to one or more allergens. Anaphylaxis results in fewer than 1,000 deaths per year in the U.S and the most common presentation is cardiovascular collapse What do coronary angiographic projections refer to?</p><p>For the beginner angiographer the anatomic landmarks formed by the spine, catheter and diaphragm provide information to discern which tomographic view from which the image is obtained. In the LAO view the catheter and spine are seen on the right side of the image, while in the RAO they are found on the right. PA imaging places these landmarks in the center of the image.</p><p>Cranial angulation can usually be distinguished from caudal angulation by the presence of the diaphragm. For cranial imaging, the patient should be asked to inspire to remove the diaphragmatic shadow from the image.</p><p>What are the symptoms of Hypothermia?</p><p>As people develop hypothermia, their abilities to think and move are often lost slowly. In fact, they may even be unaware that they need emergency treatment. Someone with hypothermia also is likely to have frostbite. The symptoms include: Drowsiness Weakness and loss of coordination Pale and cold skin Confusion Uncontrollable shivering (although at extremely low body temperatures, shivering may stop) Slowed breathing or heart rate Lethargy, cardiac arrest, shock, and coma can set in without prompt treatment. Hypothermia can be fatal.</p><p>Who is at risk for hereditary pancreatitis?</p><p>Studies demonstrate that cationic trypsinogen gene mutations are associated with hereditary pancreatitis. The major mutations are known as cationic trypsinogen "R122H", "N29I". Further more, hereditary pancreatitis has also been linked to an increased lifetime risk of pancreatic cancer.</p><p>Who is at highest risk for Glomerular disease ?</p><p>The following may increase your risk of this condition: Blood The outcome varies. Patients who are in deep comas after an aneurysm rupture generally do not do as well as those with less severe symptoms. Ruptured cerebral aneurysms are often deadly. About 25% of people die within 1 day, and another 25% die within about 3 months. Of those who survive, about 25% will have some sort of permanent disability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of medical datasets created for this work. For information regarding other, already published data, please refer to the respective original publication.</figDesc><table><row><cell>Dataset</cell><cell>Source</cell><cell>Description</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Zero shot performance on the USMLE self assessment Using our medical text data, we fine-tuned several open-source LLM variants, adopting parameter-efficient tuning methodologies to address limited computing resources</figDesc><table><row><cell>Model</cell><cell>Step1 Step2 Step3</cell></row><row><cell>LLaMA 7b [15]</cell><cell>0.198 0.202 0.203</cell></row><row><cell>Alpaca 7b naive [11]</cell><cell>0.275 0.266 0.293</cell></row><row><cell>Alpaca 7b LoRA</cell><cell>0.220 0.138 0.252</cell></row><row><cell>MedAlpaca 7b</cell><cell>0.297 0.312 0.398</cell></row><row><cell>MedAlpaca 7b LoRA</cell><cell>0.231 0.202 0.179</cell></row><row><cell>MedAlpaca 7b LoRA 8bit</cell><cell>0.231 0.241 0.211</cell></row><row><cell>ChatDoctor (7b) [10]</cell><cell>0.187 0.185 0.148</cell></row><row><cell>LLaMA 13b [15]</cell><cell>0.222 0.248 0.276</cell></row><row><cell>Alpaca 13b naive</cell><cell>0.319 0.312 0.301</cell></row><row><cell>MedAlpaca 13b</cell><cell>0.473 0.477 0.602</cell></row><row><cell>MedAlpaca 13b LoRA</cell><cell>0.250 0.255 0.255</cell></row><row><cell cols="2">MedAlpaca 13b LoRA 8bit 0.189 0.303 0.289</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Representative question from the Stack Exchange dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Representative question from the Wikidoc Living Textbook and Patient Information.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>or lymphatic system disorders Exposure to hydrocarbon solvents History of cancer Infections such as strep infections, viruses, heart infections, or abscesses Many conditions cause or increase the risk for glomerulonephritis, including: Amyloidosis Anti-glomerular basement membrane antibody disease Blood vessel diseases such as vasculitis or polyarteritis Focal segmental glomerulosclerosis Goodpasture syndrome Heavy use of pain relievers, especially NSAIDs Henoch-Schonlein purpura IgA nephropathy Lupus nephritis Membranoproliferative GN</figDesc><table><row><cell cols="3">What to expect if I have In-</cell></row><row><cell>tracranial</cell><cell>aneurysms</cell><cell>(Out-</cell></row><row><cell cols="2">look/Prognosis)?</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0"><p>Appendix</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5">Acknowledgements</head><p>The authors acknowledge the Scientific Computing of the <rs type="institution">IT Division at the Charit? -Universit?tsmedizin Berlin</rs> for providing computational resources that have contributed to the research results reported in this paper. URL: https://www.charite.de/en/research/research_support_services/research_infrastructure/science_it/#<rs type="grantNumber">c306460</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NvedeBy">
					<idno type="grant-number">c306460</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Leveraging gpt-4 for post hoc transformation of free-text radiology reports into structured reporting: A multilingual feasibility study</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Truhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Makowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Bressem</surname></persName>
		</author>
		<idno type="PMID">37014240</idno>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="page">230725</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The utility of chatgpt as an example of large language models in healthcare education, research and practice: Systematic review on the future perspectives and potential limitations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sallam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">medRxiv</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Petro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">388</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1233" to="1239" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Covid-19 open research dataset challenge (cord-19)</title>
		<author>
			<persName><forename type="first">Czi</forename><surname>Ai2</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Msr, Georgetown</surname></persName>
		</author>
		<author>
			<persName><surname>House</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Kaggle Challenge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aligning ai with shared human values</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Critch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">What disease does this patient have? a large-scale open domain question answering dataset from medical exams</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oufattole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13081</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting causal language use in science findings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yunxiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zihan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ruilong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.14070</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Stanford alpaca: An instruction-following llama model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">-bit matrix multiplication for transformers at scale</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.07339</idno>
	</analytic>
	<monogr>
		<title level="j">Llm.int</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">8-bit optimizers via block-wise quantization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Peft: State-of-the-art parameter-efficient finetuning methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mangrulkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Belkada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
