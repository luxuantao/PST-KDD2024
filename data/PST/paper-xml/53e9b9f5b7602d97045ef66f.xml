<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Show me the Money! Deriving the Pricing Power of Product Features by Mining Consumer Reviews</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nikolay</forename><surname>Archak</surname></persName>
							<email>narchak@stern.nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information, Operations, and Management Sciences Leonard N. Stern School of Business</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<settlement>San Jose</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Show me the Money! Deriving the Pricing Power of Product Features by Mining Consumer Reviews</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">577FBB16F0BB9CDBEC07C2FA5B359172</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2.7 [Artificial Intelligence]: Natural Language Processing-text analysis</term>
					<term>H.2.4 [Database Management]: Systems-Textual databases</term>
					<term>H.2.8 [Database Applications]: Data mining Algorithms, Measurement, Economics, Experimentation consumer reviews, e-commerce, econometrics, electronic commerce, electronic markets, hedonic analysis, Internet, opinion mining, product review, sentiment analysis, text mining, user-generated content</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The increasing pervasiveness of the Internet has dramatically changed the way that consumers shop for goods. Consumergenerated product reviews have become a valuable source of information for customers, who read the reviews and decide whether to buy the product based on the information provided. In this paper, we use techniques that decompose the reviews into segments that evaluate the individual characteristics of a product (e.g., image quality and battery life for a digital camera). Then, as a major contribution of this paper, we adapt methods from the econometrics literature, specifically the hedonic regression concept, to estimate: (a) the weight that customers place on each individual product feature, (b) the implicit evaluation score that customers assign to each feature, and (c) how these evaluations affect the revenue for a given product. Towards this goal, we develop a novel hybrid technique combining text mining and econometrics that models consumer product reviews as elements in a tensor product of feature and evaluation spaces. We then impute the quantitative impact of consumer reviews on product demand as a linear functional from this tensor product space. We demonstrate how to use a lowdimension approximation of this functional to significantly reduce the number of model parameters, while still providing good experimental results. We evaluate our technique using a data set from Amazon.com consisting of sales data and the related consumer reviews posted over a 15-month period for 242 products. Our experimental evaluation shows that we can extract actionable business intelligence from the data and better understand the customer preferences and actions. We also show that the textual portion of the reviews can improve product sales prediction compared to a baseline technique that simply relies on numeric data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Consumer product reviews are now widely recognized to have a significant impact on consumer buying decisions <ref type="bibr" target="#b5">[6]</ref>. Moreover, prior research on consumer decision making has established that consumer-generated product information on the Internet attracts more product interest than vendor information <ref type="bibr" target="#b1">[2]</ref>. In contrast to product descriptions provided by vendors, consumer reviews are, by construction, more user-oriented: in a review, customers describe a product in terms of usage scenarios and evaluate the product from a user's perspective <ref type="bibr" target="#b3">[4]</ref>. Despite the subjectivity of consumer evaluations in the reviews, such evaluations are often considered more credible and trustworthy by customers than traditional sources of information <ref type="bibr" target="#b1">[2]</ref>.</p><p>The rapid growth of the number of consumer reviews on the Web gave birth to several interesting opinion mining problems. Early work in this area was targeted primarily at evaluating the polarity of the reviews: review sentiments were classified as positive or negative by looking for occurrences of specific sentiment phrases. Different sources of sentiment phrases were proposed, including manually constructed dictionaries <ref type="bibr" target="#b6">[7]</ref>, WordNet <ref type="bibr" target="#b14">[15]</ref>, and search engine hit counts <ref type="bibr" target="#b27">[28]</ref>. Machine learning methods were also applied to sentiment-based classification of consumer reviews <ref type="bibr" target="#b21">[22]</ref>; all methods performed relatively well but failed to achieve high accuracy that is typical for topic-based document classification. Results suggested that sentiment classification of consumer reviews is complicated since consumers may provide a mixed review, praising some aspects of a product but criticizing others. Such heterogeneity stimulated additional research on identifying product features on which consumers expressed their opinions <ref type="bibr">[9, 15-17, 26, 27]</ref>. After identifying the product features, it is then possible to use identification techniques to extract consumer opinions about each feature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Ultimately, though, we want to identify not only the opinions of the customers, but also want to examine the importance of these opinions. What features do customers value most? What is the relative importance of battery life vs. image quality in a digital camera? Customers may praise or criticize the zoom capabilities of a digital camera, but these discussions may not really affect their decision to buy the product. Current work in opinion mining has not focused on capturing such behavior. Furthermore, current opinion mining systems cannot capture reliably the pragmatic meaning of the customer evaluations. For example, is "good battery life" better than "nice battery life"? How can we define an objective measure for ranking evaluations?</p><p>Towards answering questions of this nature, we propose utilizing the economic context in which the opinions are evaluated to estimate both the intensity and the polarity of the opinion. In particular, we investigate how product feature evaluations contained in consumer reviews affect product demand. Then, by tracing the respective changes in demand, we derive both the weight of the different product features, and the pragmatic "meaning" of these evaluations, providing actionable intelligence to the manufacturers that are trying to understand consumer behavior. We illustrate the intuition behind our approach with this (simplified) example:</p><p>Example 1.1. Consider two reviews r A and rB for two similar digital cameras A and B on Amazon.com: review r A states that the " lenses" of camera A are " excellent", while review r B states that the " lenses" of camera B are " good". To understand both the polarity and the strength of these evaluations, we observe the changes in sales associated with these reviews, all other things being equal. We observe that the evaluation " excellent lenses" increases sales by 5%, while the evaluation " good lenses" causes sales to drop by 1%. Therefore, we assign the score +5% to the evaluation " excellent" and the score -1% to the evaluation " good." In a similar manner, we also observe that the evaluations " excellent battery life" and " good battery life" cause a respective change in sales of +2.5% and -0.5%. By comparing the sales changes associated with the evaluation of " battery life" with the respective changes associated with " lenses," we infer that the weight of the " lenses" feature is twice as high as the weight of " battery life." 2 Our approach is conceptually similar to the hedonic regressions that are commonly used in econometrics to identify the weight of individual features in determining the overall price of a product. For example, hedonic regressions are used to identify the marginal value of an extra megapixel in digital cameras. <ref type="foot" target="#foot_0">1</ref> As an important research contribution, we show how to incorporate in a hedonic-like framework qualitative features, such as "ease of use" or "image quality," that are not directly measurable and are ignored in existing economics and marketing research. Towards adapting a hedonic-like framework, we model qualitative consumer opinions as quantitative evaluation scores and then model consumer reviews as elements in a tensor product of feature space and evaluation space. To study the impact of consumer reviews on the product demand, we represent review impact as a linear functional in this tensor product. Finally, we show how to use a rank constraint for this functional to significantly reduce the number of model parameters, while still providing good experimental results. Our experimental evaluation over a real data set of 242 products monitored over a 15-month period on Amazon.com shows the valid-ity of our approach and provides significant insights on the behavior of consumers who buy products online.</p><p>The rest of the paper is organized as follows. Section 2 gives the background for the paper. Section 3 presents our hybrid model that combines econometrics and text mining for analyzing consumer reviews using product demand. Section 4 discusses our experimental setting and results, obtained over two big product categories. Finally, Section 5 discusses related work and Section 6 provides further discussion and concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>In this section, we give the necessary background for this paper. In Section 2.1, we give a brief description of hedonic regressions, which can be considered as the microeconomic framework in which our model is embedded in. Then, Section 2.2 describes existing techniques for product feature identification from consumer reviews and Section 2.3 briefly discusses existing approaches for identifying consumer opinions from product reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hedonic Regressions</head><p>The hedonic model assumes that differentiated goods can be described by vectors of objectively measured features and the consumer's valuation of a good can be decomposed into implicit values of each product feature <ref type="bibr" target="#b23">[24]</ref>. Hedonic models are designed to estimate the value that different product aspects contribute to a consumer's utility. Implicit in the hedonic price framework is the assumption that a particular product can be viewed as consisting of various bundles of a small number of characteristics or basic attributes <ref type="bibr" target="#b0">[1]</ref>. For instance, a backpacking tent can be decomposed to characteristics such as weight (w), capacity (c), and pole material (p), and the tent utility can be represented as a function u(w, c, p, . . .). Various restrictions can be put on the utility function. For example, it is often assumed that the utility function is monotonic, increasing, and concave to satisfy the "law of diminishing marginal utility" <ref type="bibr" target="#b24">[25]</ref>. In brief, the hedonic hypothesis is based on the notion that heterogeneous goods are an aggregation of individual characteristics. It is important to point out that not all product categories are consistent with the assumptions of the hedonic framework. For example, while consumer appliances fit well into the hedonic model, products such as movies or books do not have clear utilitarian characteristics and are not typically classified as hedonic products. In order to avoid potential complexities arising from applying the model to non-hedonic products, in this paper we consider hedonic goods only.</p><p>The primary weakness of the existing hedonic models is the need to identify manually product features and measurement scales for them. In traditional hedonic regressions (i.e., those used by the BLS<ref type="foot" target="#foot_1">2</ref> to calculate the CPI) the decision of what features to include and how to measure them is made by individuals. This can lead to a bias from subjective judgments of these individuals. Furthermore, features that cannot be easily measured (e.g., image quality, style) are commonly ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Product Feature Identification</head><p>In an orthogonal direction to economic research, the problem of identifying product features has been studied extensively in the last few years in the data mining and natural language processing communities. Many techniques use a part-of-speech tagger to annotate each review word with its part-of-speech (POS), identifying whether the word is a noun, an adjective, a verb and so on. Nouns and noun phrases are popular candidates for product features, though other constructs (like verb phrases) can be used as well. Alternative techniques search for statistical patterns in the text, for example, words and phrases that appear frequently in the reviews <ref type="bibr" target="#b7">[8]</ref>. For example, Hu and Liu <ref type="bibr" target="#b15">[16]</ref> use association rule mining to find frequent n-grams 3 in consumer product reviews and use these n-grams as candidate features. Hybrid methods are also developed by combining both approaches, where a POS-tagger is used as a preprocessing step before applying association mining algorithm to discover frequent nouns and noun phrases <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref>. An alternative technique for discovering product features that are not explicitly mentioned in the review is to use a classifier <ref type="bibr" target="#b8">[9]</ref> that determines whether a particular feature is discussed (implicitly) in the review or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mining Consumer Opinions</head><p>Of course, identifying product features per se is not the end goal. The important goal is to understand what the customer's opinion is about each of the identified product features. An increasingly popular trend in opinion mining is in combining feature mining and sentiment identification techniques to extract consumer opinions in form of featurebased summaries <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref>. Though technical details may differ, the proposed algorithms usually consist of three basic steps. In the first step, a feature mining technique is used to identify product features. In the second step, the algorithms extract sentences that give (positive or negative) opinions for a product feature. Finally, a summary is produced using the discovered information. Unfortunately, the existing approaches could not provide reasonable quantitative evaluations of product features. In most cases, the evaluation of a product feature was done in a binary scale (positive or negative). It is also possible to have a counting scale that computes the number of positive and negative opinion sentences for a particular feature; such counts can be used for feature-based comparison of two products (see, for example <ref type="bibr" target="#b18">[19]</ref>). Such a comparison tool is undoubtedly useful for consumers using an online shopping environment. Unfortunately, such techniques fail to identify the strength of the underlying evaluations, and do not show the importance of the underlying feature in the consumer's purchasing process. In this paper we show how to address these issues by taking into consideration the economic context into which the opinions are being evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ECONOMETRIC OPINION ANALYSIS</head><p>In this section, we describe our econometric approach for modeling and analyzing consumer reviews. In Section 3.1, we present our approach for identifying the opinions of consumers about product features. Then, in Section 3.2, we show how to represent consumer reviews as elements of a tensor product space and in Section 3.3, we describe our econometric approach for measuring the weight of the individual product features, and the implicit evaluation score that a consumer review assigns to each feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Identifying Customer Opinions</head><p>In the first step of our approach, we need to identify the product features mentioned in the reviews and the respective consumer evaluations. Recall that in the hedonic model each product can be characterized by a vector of its features X = (ef 1 , . . . , ef n ), where each vector element ef i represents the quality level of the corresponding feature. In our current work, we rely on existing approaches to identify the product features in the text. Specifically, we assume that each of the n features can be expressed by a noun, chosen from the set of all nouns that appear in the reviews. For example, for a digital camera dimension 1 might be "lens", dimension 2 might be "size", dimension 3 might be "shutter ". 4 Furthermore, to avoid overfitting, we only considered as features the nouns that appeared frequently in our data set. (We provide more details in Section 4.1.) After identifying the product features, we need to identify the consumer evaluation of product feature quality. We rely on the observation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref> that consumers typically use adjectives, such as "bad," "good," and "amazing" to evaluate the quality of a product characteristic. Therefore, we use a syntactic dependency parser to identify the adjectives that modify a noun that we have identified as product feature.</p><p>The result of this technique is a set of noun-adjective pairs that correspond to pairs of product features and their respective evaluations. We refer to these pairs as opinion phrases. It should be noted that such opinion phrases cannot capture all opinion information contained in consumer reviews. We would like to emphasize though that the goal of this paper is not to develop new techniques for identifying product features or phrases that evaluate such features. 5 Rather, we are interested in measuring the weight that customers place in each product feature and the implicit evaluation score (polarity and strength) associated with each feature evaluation. The core contributions of this paper are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Structuring the Opinion Phrase Space</head><p>Existing consumer review mining approaches tend to consider extracted product features and opinions as simple sets 4 Some researchers have noted that many technical terms are compound nouns <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref>. In particular, compound terms like "battery life", "image quality" and "memory card" are rated by end users as more helpful for choosing products than single noun features ("battery", "image(s)", "memory") <ref type="bibr" target="#b19">[20]</ref>. Nevertheless, we observed that many compound terms without loss of information can be represented by a single noun only. For example, if the mining tool finds word "battery" in a digital camera review, we observed sure that "battery life" is the feature being evaluated. For this reason we didn't consider compound nouns as candidate product features and restricted ourselves to simple nouns only. 5 Of course, our approach can clearly benefit from orthogonal advances in the topics of product feature identification, and in the extraction of the respective evaluation phrases.</p><p>and no algebraic structure is imposed on these sets. In order to use the concepts similar to hedonic regressions, we need to represent the product demand as a function from the space of consumer reviews. Such representation assumes a vector space structure on the set of feature opinions. To construct such structure, we build two vector spaces: one for product features and one for feature evaluations.</p><p>We model multiple sets of n product features as elements of a vector space over R with basis f 1, . . . , fn. We denote this feature space as F , (dim F = n). In the same way, we define a space of evaluations as a vector space over R with basis e 1, e2, . . . , em. We denote this evaluation space as E (dim E = m). Intuitively, when we represent a review r in the feature space F , the representation contains the weights that customers assign to each product feature. Similarly, when we represent a review r in the evaluation space E , the representation contains the implicit evaluation scores that the review assigns to a product feature (notice that the evaluations in the space E are not bound, yet, to any specific feature). Now, based on the algebraic structures in the feature and the evaluation space, we can represent the opinion phrases and whole consumer reviews as elements of the review space R that we define as the tensor product of the evaluation and feature spaces:</p><formula xml:id="formula_0">R = F ⊗ E</formula><p>Note that the set of opinion phrases f i ⊗ ej form a basis of the space R and we denote this basis as V . Now, each review can be represented as a vector in the review space R: we only need to determine the weight that we assign to each dimension of R to represent the review. There are several different ways to determine the weight of an opinion phrase in a text corpus (e.g., tf.idf weights). In our work, we use a standard term frequency measure that discounts influence of longer reviews and calculates the weight of the opinion phrase phrase in review rev for product prod as:</p><formula xml:id="formula_1">w(phrase, rev , prod ) = N (phrase, rev , prod ) + s y∈V (N (y, rev , prod ) + s) (1)</formula><p>where N (y, r, p) is the number of occurrences of the opinion phrase y in the consumer review r for product p, and s is a "smoothing" constant. 6 Notice that using our convention, the sum of the weights of the opinion phrases within each review is equal to one, discounting the influence of longer reviews.</p><p>Example 3.1. Consider the following review for a digital camera: " The camera is of high quality and relatively easy to use. The lens are fantastic! I have been able to use the LCD viewfinder for some fantastic shots... To summarize, this is a very high quality product." This review can be represented by the following element of the tensor product, assuming s = 0:</p><formula xml:id="formula_2">0.4 • (quality ⊗ high) + 0.2 • (use ⊗ easy) + 0.2 • (lens ⊗ fantastic) + 0.2 • (shots ⊗ fantastic)</formula><p>Notice that each opinion phrase dimension has a weight coefficient determining its relative importance in the review. Since the opinion phrase quality-high appears twice in the 6 We set s = 0.01 in our paper review, the weight of this dimension is 0.4, in contrast to all the other opinion phrases that appear only once and have weight equal to 0.2. 2 So far, we have discussed how to represent reviews in an algebraic form. While we could achieve a similar result by simply defining directly a space of opinion phrases and then represent each review as a vector in this space, we will see next that our tensor space approach has significant analytic advantages over this simpler approach. Specifically, we will see that the tensor space approach will allow us to estimate naturally the weight of each product feature and the implicit score of each evaluation, using relatively small training sets and avoiding the problems of data sparsity and of overfitting. Next, we describe our approach in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Econometric model of product reviews</head><p>A simple econometric technique for modeling product demand as a function of product characteristics and its price, is the following simple linear model:</p><formula xml:id="formula_3">ln(D kt ) = a k + β ln(p kt ) + ε kt .</formula><p>(</p><formula xml:id="formula_4">)<label>2</label></formula><p>where D kt is the demand for the product k at time t, p kt is its price at time t, β is the price elasticity, and a k is a product specific constant term which captures the unobserved product heterogeneity, such as differences in product characteristics and brand equity. The variable ε kt represents a random disturbance factor which is usually assumed to be normally distributed, i.e., ε kt ∼ N (0, σ 2 ). This is a classical ordinary-least-squares (OLS) regression with product level fixed effects and the parameters can be estimated using standard panel data methods <ref type="bibr" target="#b30">[31]</ref>.</p><p>A potential drawback of such a model is that it can not be used to evaluate separately different product characteristics because it mixes all product features into the single term a k . Another limitation is time invariant nature of the product specific effect. Though the technical characteristics of the product typically stay the same during the product's life cycle, its popularity may change as a result of consumer evaluations of the product.</p><p>We extend the model using our original assumption that qualitative consumer reviews correspond to some quantitative evaluations of product characteristics. Such evaluations cannot be measured directly but can be learned from product demand fluctuations, if a sufficiently large data set is available. We replace the product specific effect a k by a sum of two components:</p><formula xml:id="formula_5">a k = α + Ψ(W kt ). (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>where we have</p><p>• a consumer review component W kt ∈ R that captures opinions contained in consumer product reviews for product k available at time t, including all reviews before time t, and</p><p>• a time and product invariant constant term α.</p><p>The vector W kt (for fixed k and t) contains the weights of all opinion phrases that appeared in the consumer reviews for product k available at time t. (Essentially, we take the vectors that represent the consumer reviews posted by time t and "collapse" them into a single vector.) For simplicity, we compute these weights by averaging<ref type="foot" target="#foot_2">7</ref> the weights for each opinion phrase across all reviews posted until time t.</p><p>In Equation 3, we represented the impact of a set of consumer reviews on the product demand as some generic functional Ψ : R → R. Similar to hedonic regressions, we assume that the functional form is linear and formally this can be written as Ψ ∈ R * where R * is the dual space of R (space of linear functionals). A linear functional from a tensor product of two vector spaces is just a bilinear form of two parameters, so Ψ is just a bilinear form of features and evaluations. Any linear functional can be written in the basis representation:</p><formula xml:id="formula_7">Ψ(W kt ) = phrase∈V ψ(x) • w(phrase, reviewst, product k ) = n i=1 m j=1 ψ(fi ⊗ ej) • w((fi ⊗ ej), reviewst, product k ).</formula><p>where ψ(x) = ψ(fi ⊗ ej) is the value of the functional on the basis vector f i ⊗ ej. Intuitively, the value of ψ(x) is the influence of the opinion phrase f i, ej , which in turn is a function of the weight of feature f i (when evaluated by ej) and of the implicit score that e j assigns to fi. Continuing the example from Section 3.2, we can write the value of the functional on the sample consumer review as:</p><formula xml:id="formula_8">0.4 • ψ(quality ⊗ high) + 0.2 • ψ(use ⊗ easy)+ 0.2 • ψ(lens ⊗ fantastic) + 0.2 • ψ(shots ⊗ fantastic)</formula><p>Using Equations 2 and 3, we have our extended linear model:</p><formula xml:id="formula_9">ln(D kt ) = α + β ln(p kt ) + Ψ(W kt ) + ε kt</formula><p>Unfortunately, this model has a very large number of parameters and would require a very large training set of product reviews to estimate. In the case where we have n product features and m evaluation words for N products with similar features, the number of model parameters will be n • m + N . Since, we rarely have more than 30 reviews for a product, model overfitting is definitely a problem.</p><p>To alleviate this problem, we reduce the model dimension by placing a rank constraint on the matrix Ψ. In other words, we are not going to search for any linear function but only for those with low matrix ranks. The approach is based on the following observation. Corollary 3.1 shows how to model features and evaluations as independent multiplicative components. We can replace a complex functional from R * by a product Δ ⊗ Γ of two simple functionals Δ ∈ F * and Γ ∈ E * . In other words, we assume that each coefficient ψ(x) can be decomposed as a product of the feature component (defining importance of the feature) and the evaluation component (determining relative weight of the evaluation). For example:</p><formula xml:id="formula_10">ψ(shots ⊗ fantastic) = γ(shots) • δ(fantastic)</formula><p>The resulting approach is similar to ANOVA decomposition, which is frequently used to reduce model complexity for multidimensional splines and generalized additive models <ref type="bibr" target="#b13">[14]</ref>. Our model, though, operates in different context because features and evaluations do not represent independent dimensions of consumer reviews: each evaluation word occurrence describes some feature and each feature word occurrence relates to some evaluation.</p><p>One limitation of this rank-1 approximation is the restriction that each adjective has one meaning, independently of the feature that it evaluates. While this restriction might seem too strict, we observed that it worked well for our setting. We should note though that our approach allows flexibility in the approximation approach. If we would like to assign two potential meanings for each adjective, we can use a rank-2 approximation, allowing two meanings per adjective and two potential weights for each feature (e.g., negative evaluations may have high influence when evaluating a feature, but positive evaluations may have low weight). In this case, the functional ψ would simulate a mixture model and have the form ψ = γ 1δ1 +γ2δ2. (Of course, estimating such a model would require a larger number of training examples.) Similarly, we can use even higher rank approximations, allowing more meanings per adjective, but we would need more training data to avoid overfitting.</p><p>Using the rank-1 approximation of the tensor product functional, we rewrite the model of Equation <ref type="formula" target="#formula_5">3</ref>.3 as:</p><formula xml:id="formula_11">ln(D kt ) = α + β • p kt + γ T • W kt • δ + ε kt . (<label>4</label></formula><formula xml:id="formula_12">)</formula><p>where γ is a vector that contains n elements corresponding to the weight of each product feature, and δ is a vector that contains the implicit score that each adjective assigns to a product feature. The new model has only m + n + 1 + N parameters instead of original n • m + 1 + N , but this is achieved by sacrificing the linearity of the original model. Despite the loss of linearity, we can still search for the maximum likelihood estimate (MLE) of the model parameters.</p><p>Assuming that ε kt are independent and identically normally distributed residuals, the ML estimate is the same as the minimum of the least-squares fit function:</p><formula xml:id="formula_13">(α * , β * , γ * , δ * ) = argmin ⎛ ⎝ k,t ln(D kt ) -α -β • p kt -γ T • W kt • δ 2 ⎞ ⎠</formula><p>One of possible approaches to parameter estimation is to use the Newton-Rhapson method, or its variations, to search for the minimum. Unfortunately the Newton-Rhapson algorithm convergence suffers from numerical difficulties, in particular, ill-conditioning of the Hessian in the neighborhood of the solution. To solve the problem, we propose a different iterative algorithm. Our algorithm is based on the observation that if one of the vectors γ or δ is fixed, the Equation <ref type="formula" target="#formula_11">4</ref>represents a linear model. The steps of the algorithm are described below:</p><p>1. Set δ to a vector of initial feature weights.</p><p>2. Minimize the fit function by choosing the optimal evaluation weights (γ) assuming that the feature weights (δ) are fixed. If δ is fixed, the equation is just a usual linear model and γ can be estimated by ordinary-leastsquares (OLS) or generalized-least-squares (GLS) estimators <ref type="bibr" target="#b11">[12]</ref>.</p><p>3. Minimize the fit function by choosing the optimal feature weights (δ) assuming that the evaluation weights (γ) are fixed. Again this can be done by using OLS or GLS estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Repeat</head><p>Step 2 and 3, until the algorithm converges.</p><p>Note that the algorithm described above (and the Newton-Rhapson algorithm as well) may converge to some local minimum of the fit function. To compensate for such potential errors, we repeat the execution of the algorithm with multiple, randomly generated, initial starting points.</p><p>Summary: In this section, we have presented our approach for estimating the weights of the different product features and the implicit scores that the adjectives assign to the product features. We have presented how to use a lowrank approximation of the review space to estimate these values, and presented an efficient algorithm for estimating the parameters of our model. Next, we present the experimental evaluation of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL EVALUATION</head><p>In this section, we validate our techniques on a data set covering 242 products drawn from the product categories "Audio &amp; Video" and "Camera &amp; Photo" sold on Amazon. We first describe our data set (Section 4.1). Then, in Section 4.2, we describe how we run our text mining algorithm to identify the product features and the evaluations. In Section 4.3, we present the exact setup of our econometric-based technique, and, in Section 4.4 we discuss the experimental findings that show that our techniques can provide actionable business intelligence and can give useful insights on how customers behave when shopping online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We gathered data on a set of products using publicly available information at Amazon.com. The data set covered two different product categories: "Camera &amp; Photo" (115 products) and "Audio &amp; Video" (127 products). During a 15month period (from March 2005 to May 2006), we have been collecting daily price and sales rank information for the products in our data set, using the API provided by Amazon Web Services. In the "Camera &amp; Photo" category we had 31,233 observations and in the "Audio &amp; Video" category we had 35,143 observations. Each observation contains the collection date, the product ID, the price on Amazon.com (which includes a possible Amazon discount), the suggested retail price, the sales rank of the product, and the average product rating according to the posted consumer reviews. Additionally, we used Amazon Web Services to collect the full set of reviews for each product. Each product review has a numerical rating on a scale of one to five stars, the date the review was posted and the actual text posted by the reviewer. We collected 1,955 reviews in the "Camera &amp; Photo" category and 2,580 reviews in the "Audio &amp; Video" category, overlapping with our sales rank observations. Thus, each product had about 20 reviews on average. Figures <ref type="figure" target="#fig_1">1</ref> and<ref type="figure" target="#fig_2">2</ref> show distributions of the number of consumer reviews per product in the "Audio &amp; Video" and "Camera &amp; Photo" categories. A  few outlier products, with more than 50 reviews, are not displayed on the figures. For example, in the "Audio &amp; Video" category Apple 1GB iPod Shuffle had 271 consumer reviews.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Selecting Feature and Evaluation Words</head><p>In order to determine the set of product features and evaluation words to use, we applied a simple approach consisting of three steps. First, we used the part-of-speech tagger developed by the Stanford NLP Group, to analyze all reviews and assign a part of speech tag to each word. Second, a set of frequent nouns was extracted from the tagged review corpus. The second step tends to produce a significant number of infrequent, non-relevant items. Fortunately, the set of frequent nouns was relatively small (about 800 items per category). Since, our work is not focused on product feature identification from product review, we decided to quickly process the list of frequent items, and manually select a subset of approximately 30 nouns to use as product features. For example, in the "Camera &amp; Photo" category the set of features included "battery/batteries," "screen/lcd/display," "software," "viewfinder," "lens/lenses," and so on. To identify the set of evaluations, we used the syntactic dependency parser from the Stanford NLP toolkit, and we extracted the adjectives that evaluated the selected product features. The set of evaluations contained popular adjectives describing the level of satisfaction with each particular feature. Words like "amazing," "bad," "great," "exceptional," and "outstanding" appeared frequently in our data set. We kept the list of 30 most frequent adjectives to create our evaluation space. We used the same set of evaluation words for both product categories, but, of course, the set of features was different in each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Setup</head><p>In order to evaluate our techniques using data from Amazon, we had to modify slightly the model of Section 3.3. Specifically, Amazon.com doesn't report the demand for the products available on the web site. Instead, Amazon.com reports a sales rank figure for each product, which ranks the demand for a product relative to other products in its category. Prior research in economics and in marketing (for instance, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>) has associated these sales ranks with demand levels for products such as software and electronics. The association is based on the experimentally observed fact that the distribution of demand in terms of sales rank has a Pareto distribution (i.e., a power law) <ref type="bibr" target="#b4">[5]</ref>. Based on this observation, it is possible to convert sales ranks into demand levels using the following Pareto relationship:</p><formula xml:id="formula_14">ln(D) = a + b • ln(S) (<label>5</label></formula><formula xml:id="formula_15">)</formula><p>where D is the unobserved product demand, S is its observed sales rank, and a &gt; 0, b &lt; 0 are industry-specific parameters. Therefore, we can use the log of product sales rank on Amazon.com as a proxy of the log of product demand. We also modified the model of Section 3.3 to include both the suggested retail price (P 1) and the price on Amazon.com (P2) since its natural to expect the prices will influence product demand, besides word-of-mouth. Furthermore, we included the review rating variable (R), which represented the average numeric rating of the product as given by the reviews.</p><p>Our model predicts the product sales rank based on its price, average review rating, and a set of consumer reviews. Note that as defined before, the review-level variables are the ones published before a given observation date. 8 Using our model, we analyze how different feature-evaluation combinations affect sales, after controlling for product price. The transformed equation is given below: <ref type="bibr" target="#b5">(6)</ref> In the above equation, W kt is the "review matrix" (see Section 3.3) and W ktij was calculated using Equation <ref type="formula">1</ref>. We found that due to sparsity of the review data if we use a large number of features and evaluations, our model tends to overfit the noise in the training data set. To alleviate this concern, we added the regularization constraint 8 We should clarify that ours is not a hedonic demand estimation model. In hedonic demand estimations, the first step involves computing individual equilibrium trait prices based on estimates of a hedonic price function to find the market price of a trait in the bundle. The second step is to estimate an inverse demand or marginal bid function using the trait prices as the dependent variable. Typically two-stage least squares (2SLS) regressions are employed with the supplier traits being appropriate instruments for the endogenous variables in the marginal bid function. Our approach is nested in a framework that is similar in notion to a hedonic demand model but very different in implementation.</p><formula xml:id="formula_16">ln(S kt ) = α + β1 • R kt + β2 • ln(P 1kt ) + β3 • ln(P 2kt )+ m i=1 n j=1 W ktij • γi • δj + ε kt = α + y kt • β + γ T • W kt • δ + ε kt</formula><formula xml:id="formula_17">m i=1 γ 2 i n j=1 δ 2 j</formula><p>≤ λ, which transformed this linear model to a "ridge regression" <ref type="bibr" target="#b13">[14]</ref>. Note that due to negligible correlation<ref type="foot" target="#foot_3">9</ref> between the list and retail prices, multicollinearity is not a concern in our model. Our results are also robust (and similar to the reported ones) if we were to only use the retail price in Equation <ref type="formula">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head><p>Predicting Future Sales: After obtaining the review matrix, we used 10-fold cross-validation to test the model's performance and obtain reliable coefficient estimates. The original sample was partitioned into 10 sub-samples of products. <ref type="foot" target="#foot_4">10</ref> Of the 10 sub-samples, a single sub-sample was retained as validation data for testing the model, and the remaining 9 sub-samples were used as training data. The cross-validation process was repeated 10 times and each of the 10 sub-samples was used exactly once as validation data.</p><p>For our experiments, we run our regression on the training data and derived the coefficients for each of the regressors. After deriving the coefficients from the training set, we tested how well the derived regressions predicts the sales rank for the products in the test set. We compared performance for predicting sales rank, using the Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) metrics, averaged over all the observations in the test set. We compared our model with the same model that has no consumer review text available (but including all other variables). We observed a 5% improvement in RMSE and 3% improvement in MAE. To test for statistical significance we used the Wilcoxon<ref type="foot" target="#foot_5">11</ref> signed rank test (p &lt; 0.001). We should note that the 5% improvement is a significant achievement, given the high volatility of Amazon sales rank information, the fact that the numeric rating information is already taken into account (both models contained average rating of consumer reviews as a variable) and the fact that we are measuring errors in log-scale. This result indicates that the text of the reviews contains information that influences the behavior of the consumers, and that the numeric ratings alone cannot capture the information in the text.</p><p>Estimating Product Feature Weights and Evaluation Scores: The other significant contribution of our work is the ability to identify the product feature weights and the evaluation scores associated with the adjectives, within the context of an electronic market.</p><p>Tables <ref type="table" target="#tab_2">2</ref> and<ref type="table" target="#tab_3">3</ref> present the average feature and evaluation coefficients that were obtained for the 49 digital cameras 12 from the "Camera &amp; Photo" category. The coefficients were averaged across cross-validation runs to produce reliable estimates and empirical standard error values. We should also note that we excluded the rating variable from the regression, in order to obtain feature and evaluation coefficients that are not detrended with respect to numeric review rat-  ings. Note that higher values of evaluation coefficients signify increase of the sales rank and, therefore, have negative impact on product sales. We should note that making conclusions based on the individual model coefficients may be incorrect, due to model complexity and interaction effects between coefficients. So, in addition to the raw coefficients, we also calculated the partial effects for frequent opinion phrases. The partial effect of a opinion phrase x = e⊗f is defined as γ T • Wx kt •δ-γ T • Wkt •δ where Wkt is the "average review" obtained by averaging all consumer reviews in this product category with proper normalization and Wx kt is the "average review" where all evaluations of the feature f were replaced by the evaluation e. For example, to calculate a partial effect of the "solid lens" phrase (f = lens, e = solid ) we take the "average digital camera review" and replace all evaluations of the "lens" feature to "solid". This enables us to simply calculate the difference between the modified review score and the average review score. The calculated partial effects for the "Camera &amp; Photo" product category are presented in Table <ref type="table" target="#tab_4">4</ref>. Remember, that a negative sign on an opinion phrase denotes the fact that there is an increase in sales (since sales rank on Amazon is inversely proportional to demand) when that "opinion feature" is present in the content of the product review.</p><p>We observed that seemingly positive evaluations like "decent quality" or "nice/fine camera" end up hurting sales. Even though this may seem counterintuitive, it actually reflects the nature of an online marketplace: most of the positive evaluations contain superlatives, and a mere "decent" or "fine" is actually interpreted by the buyers as a lukewarm, slightly negative evaluation. Furthermore, we observed only a small number of unambiguously negative evaluations, such as "bad" and "horrible": products that get bad reviews, tend to disappear from the market quickly, and do not get many further negative evaluations. In general, the reviews that appear on Amazon are positive, especially for products with large number of posted reviews. Finally, a strange observations is that the evaluations "best camera," "excellent camera," "amazing camera," "perfect camera," and so on, have a negative effect on demand. This puzzling result may be caused by the fact that most of the reviews praising the camera in general do not usually have many details about the product features of the camera. Customers discount such reviews, and potentially treat them as bogus: a review that simply says that a camera is the "best camera" in the market, without going into other details does not provide useful information to the customer and has negative effect on sales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Conclusions:</head><p>We have presented our experimental evaluation of our technique by presenting results on a real data set from Amazon.com. Our results show that we can identify the features that are important to consumers and that we can derive the implicit evaluation scores for each adjective, in an objective and context-aware manner. Deriving the polarity and strength of an evaluation was generally considered a hard problem, but by evaluating an opinion within an economic framework, we showed that we can solve the problem in a natural manner. Furthermore, our technique takes into consideration the peculiarities of the environment in which the opinion is evaluated. Weak positive opinions like nice and decent are actually evaluated in a negative manner in electronic markets, since customers are used to see only strong positive opinions, and a mere nice or decent is a bad signal to the buyers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>Our paper adds to a growing literature on sentiment analysis. Similar to almost any other sentiment mining technique, the first step involves selecting the set of features to use (see Section 2.2). Our approach to feature selection is very close to the one presented by Hu and Liu <ref type="bibr" target="#b14">[15]</ref>. Hu and Liu used a POS-tagger and association miner to find frequent itemsets, which are then treated as candidate features. For each instance of a candidate feature in a sentence, the nearby adjective (if there is such) was treated as the effective opinion. Note that effective opinions used by Hu and Liu are direct counterparts of evaluation words in our study. A major difference of our approach is that whereas Hu and Liu were interested in the effectiveness of feature extraction and high recall values, we are concerned about gathering a small set of major features and evaluation words. In order to ensure that gathered features reflect hedonic characteristics of the products, we performed manual post-processing of frequent itemsets. Contrary to Hu and Liu, who performed additional research to identify infrequent features, we intentionally discarded such features to obtain robust estimates of model coefficients. It should also be noted that while Hu and Liu used WordNet to identify polarity of opinion words, our model evaluates polarity and strength of each evaluation directly from the regression on product demand.</p><p>Our research was inspired by previous studies about opinion strength analysis. Popescu and Etzioni <ref type="bibr" target="#b22">[23]</ref> presented OPINE, an unsupervised information extraction system capable of identifying product features, identifying user opinions regarding product features, determining polarity of the opinions and ranking the opinions based on their strength. Unfortunately, so far, no paper was published explaining how OPINE solves the last task (opinion ranking). Evaluating strength of a sentiment quantitatively is even more challenging task than simple ranking and there were just a few papers on the topic published thus far. Wilson, Wiebe and Hwa <ref type="bibr" target="#b29">[30]</ref> presented a supervised learning approach able to distinguish between weak, medium and strong subjectivity of the opinion. Unfortunately, this technique requires a manually annotated corpus of opinions. Human annotators might not able to distinguish strength of opinions on a finer-grained scale or estimate economic impact of each particular opinion. In a close stream of research, Pang and Lee <ref type="bibr" target="#b20">[21]</ref> studied the rating-inference problem. They augmented SVM classifier with a metric labeling concept and applied the altered classifier to infer the author's numerical rating for Rotten Tomatoes movie reviews.</p><p>We also add to an emerging stream of literature that combines economic methods with text mining <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref>. For example, Das and Chen <ref type="bibr" target="#b6">[7]</ref> examined bulletin boards on Yahoo! Finance to extract the sentiment of individual investors about technology companies. They have shown that the aggregate tech sector sentiment predicts well the stock index movement, even though the sentiment cannot predict well the individual stock movements. There has also been related work on studying connections between online content such as blogs, bulletin boards and consumer reviews, and consumer behavior, in particular purchase decisions. Gruhl et al. <ref type="bibr" target="#b12">[13]</ref> analyzed the correlation between online mentions of a product and sales of that product. Using sales rank information for more than 2,000 books from Amazon.com, Gruhl et al. demonstrated that, even though sales rank motion might be difficult to predict in general, online chatter can be used to successfully predict spikes in the sales rank.</p><p>To the best of our knowledge, our study is the first to use econometric approaches for analyzing the strength and polarity of consumer review opinions. Contrary to Gruhl et al. who used blogs as the indicator of spikes in the sales rank, our research is based on the premise that changes in sales rank, can be partially explained by the impact of consumer reviews. We draw on the results of Chevalier and Mayzlin <ref type="bibr" target="#b5">[6]</ref> who have shown that online book ratings affect product sales by examining the relationship between relative market shares (sales rank) and consumer reviews across the two leading online booksellers, Amazon.com and Bar-nesandNoble.com. Comparing the sales and reviews of a given book across the two sites, allows to control for external factors that can affect the sales and word-of-mouth of both retailers (for example, release of a new Harry Potter movie may result in a temporary increase of the corresponding book sales). While our research is similar to the study by Chevalier and Mayzlin, it does differ in several significant ways. While the model in <ref type="bibr" target="#b5">[6]</ref> included only numerical rating information (such as average star rating and fraction of 1 star and 5 star reviews), our approach employs text mining to extract consumer opinions about different product features. Contrary to the work in <ref type="bibr" target="#b5">[6]</ref>, we were not able to use multiple retailers in our experimental study. In order to alleviate possible concerns from the influence of external factors or measurement errors on our coefficient estimates, we used 10-fold cross validation of the model to precisely infer product-specific characteristics. We found that our results are robust across products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS AND FUTURE WORK</head><p>We have presented a novel method for mining consumer reviews that combines existing text mining approaches with econometric techniques. The major novelty of our technique is that it allows an economic-aware analysis of the consumer reviews, identifying the weight that customers place on individual product features and the polarity and strength of the underlying evaluations. By using product demand as the objective function, we derive a context-aware interpretation of opinions and we show how customers interpret the posted comments and how they affect their choices. We have presented examples, where opinions that would be interpreted as positive by existing opinion mining systems, are actually negative within the context of electronic markets. The source code with our implementation, together with the data set used in this paper are available from http://economining.stern.nyu.edu.</p><p>Using a unique data set from a leading online retailer, Amazon, we have demonstrated the value of using economic data and econometric modeling for a quantitative interpretation of consumer reviews on the Internet. Our results can be used by manufacturers to determine which features contribute most to the demand for their product. Such information can also help manufacturers facilitate changes in product design over the course of a product's life cycle as well as help retailers decide on which features to promote and highlight in advertisements and in-store displays.</p><p>We believe that there is rich potential for future research. From the econometric point of view, future work can focus on pure price-hedonic regressions that highlight which feature of the product, both implicit and explicit, constitutes what proportion of the product price. Such an analysis can enable consumers to figure out exactly how much they are paying for each product attribute within a class of products.</p><p>It would also be interesting to examine how traditional hedonic studies can be enhanced when they incorporate qualitative features, and not just directly measurable parameters.</p><p>From the text mining point of view, advances in algorithms for product feature identification, and for extraction of the associated evaluation phrases, can definitely improve the results of our technique. We plan to examine the limits of these improvements by analyzing manually product reviews and extracting the features that are discussed together with their evaluations. Another interesting direction that we plan to explore is the results of our technique when using higher rank approximations of the tensor space. Right now, our rank-1 approximation allows a single score for each evaluation and a single weight for a product feature. A natural extension is to examine how many different meanings we can allow for each phrase, without running into overfitting problems.</p><p>Overall, we believe that the interaction of economics research with data mining research can benefit tremendously both fields. Economic approaches can offer natural solutions to problems that seemed too hard to solve in a vacuum (e.g., determining the strength of an opinion). Similarly, data mining approaches can improve the current state of the art in empirical economics, where the focus has traditionally been on relatively smaller data sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 3 . 1 .Corollary 3 . 1 .</head><label>3131</label><figDesc>(Singular Value Decomposition) Any linear functional Ψ ∈ R * such that rank Ψ = rank(ψij) ≤ p can be represented as p i=1 Δi ⊗ Γi where Δi ∈ F * and Γ i ∈ E * and ||Δi|| 2 = 1. Any linear functional Ψ ∈ R * such that rank(Ψ) = rank(ψ ij ) = 1 can be represented as Δ ⊗ Γ where Δ ∈ F * and Γ ∈ E * and ||Δ|| 2 = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of the number of consumer reviews per product (Audio &amp; Video)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of the number of consumer reviews per product (Camera &amp; Photo)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>3</head><label></label><figDesc>An n-gram is a sequence of n consecutive words.</figDesc><table><row><cell cols="3">Var Dim Description</cell></row><row><cell>F</cell><cell></cell><cell>feature space</cell></row><row><cell>E</cell><cell></cell><cell>evaluation space</cell></row><row><cell>R</cell><cell></cell><cell>review space</cell></row><row><cell>V</cell><cell></cell><cell>basis of review space</cell></row><row><cell>D kt</cell><cell>R</cell><cell>demand for the product k at time t</cell></row><row><cell>α</cell><cell>R</cell><cell>constant term</cell></row><row><cell>β</cell><cell>R</cell><cell>price elasticity of demand</cell></row><row><cell>p kt</cell><cell>R</cell><cell>price for the product k at time t</cell></row><row><cell cols="3">W kt R mn consumer review component, see Eq. 1</cell></row><row><cell>γ</cell><cell>R m</cell><cell>vector of evaluation weights</cell></row><row><cell>δ</cell><cell>R n</cell><cell>vector of feature weights, ||δ|| 2 = 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 : Notation and symbol descriptions</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Feature weights for the Camera &amp; Photo category. Higher weight values signify higher im- portance of each attribute. (The * symbol indicates statistical significance at the 5% confidence level.)</head><label>2</label><figDesc></figDesc><table><row><cell>Feature</cell><cell cols="2">Weight Std.Err.</cell></row><row><cell>camera  *</cell><cell>0.810</cell><cell>0.091</cell></row><row><cell>quality  *</cell><cell>0.484</cell><cell>0.106</cell></row><row><cell>battery  *</cell><cell>0.192</cell><cell>0.048</cell></row><row><cell>resolution  *</cell><cell>0.129</cell><cell>0.018</cell></row><row><cell>size</cell><cell>0.096</cell><cell>0.063</cell></row><row><cell>color</cell><cell>0.086</cell><cell>0.052</cell></row><row><cell>photos</cell><cell>0.074</cell><cell>0.040</cell></row><row><cell>lens</cell><cell>0.046</cell><cell>0.033</cell></row><row><cell>screen</cell><cell>0.037</cell><cell>0.037</cell></row><row><cell cols="3">Evaluation Score Std.Err.</cell></row><row><cell>great  *</cell><cell>-2.460</cell><cell>0.353</cell></row><row><cell>good  *</cell><cell>-1.693</cell><cell>0.211</cell></row><row><cell>best  *</cell><cell>-0.914</cell><cell>0.154</cell></row><row><cell>excellent  *</cell><cell>-0.442</cell><cell>0.180</cell></row><row><cell>perfect  *</cell><cell>-0.433</cell><cell>0.146</cell></row><row><cell>nice</cell><cell>-0.006</cell><cell>0.051</cell></row><row><cell>decent</cell><cell>0.001</cell><cell>0.056</cell></row><row><cell>fantastic</cell><cell>0.085</cell><cell>0.050</cell></row><row><cell>bad  *</cell><cell>0.206</cell><cell>0.038</cell></row><row><cell>amazing  *</cell><cell>0.220</cell><cell>0.094</cell></row><row><cell>fine  *</cell><cell>0.258</cell><cell>0.101</cell></row><row><cell>poor  *</cell><cell>0.345</cell><cell>0.066</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 : Some evaluation scores for the Camera &amp; Photo category. Higher scores mean increase in sales rank and are therefore negative. Lower values are better</head><label>3</label><figDesc></figDesc><table /><note><p>. (The * symbol indicates statistical significance at the 5% confidence level.)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 : Partial Effects for the Camera &amp; Photo product category: A negative sign signifies decrease in sales rank and means higher sales.</head><label>4</label><figDesc></figDesc><table><row><cell>Phrase</cell><cell>Effect Phrase</cell><cell>Effect</cell></row><row><cell>great camera</cell><cell>-0.4235 excellent photos</cell><cell>0.0040</cell></row><row><cell>good camera</cell><cell>-0.1128 nice size</cell><cell>0.0045</cell></row><row><cell>great quality</cell><cell>-0.0931 decent photos</cell><cell>0.0062</cell></row><row><cell>good quality</cell><cell>-0.0385 fantastic photos</cell><cell>0.0066</cell></row><row><cell>great battery</cell><cell>-0.0138 amazing resolution</cell><cell>0.0069</cell></row><row><cell>great size</cell><cell>-0.0060 amazing photos</cell><cell>0.0073</cell></row><row><cell>great photos</cell><cell>-0.0060 fine photos</cell><cell>0.0075</cell></row><row><cell cols="2">great resolution -0.0052 excellent battery</cell><cell>0.0089</cell></row><row><cell>good battery</cell><cell>-0.0051 decent battery</cell><cell>0.0139</cell></row><row><cell>great lens</cell><cell>-0.0037 amazing battery</cell><cell>0.0164</cell></row><row><cell>good size</cell><cell>-0.0027 fine battery</cell><cell>0.0168</cell></row><row><cell>great color</cell><cell>-0.0023 best quality</cell><cell>0.0170</cell></row><row><cell>good photos</cell><cell>-0.0022 excellent quality</cell><cell>0.0507</cell></row><row><cell cols="2">good resolution -0.0017 nice quality</cell><cell>0.0817</cell></row><row><cell>good lens</cell><cell>-0.0016 decent quality</cell><cell>0.0822</cell></row><row><cell>great screen</cell><cell>-0.0012 fantastic quality</cell><cell>0.0882</cell></row><row><cell>good color</cell><cell>-0.0004 amazing quality</cell><cell>0.0979</cell></row><row><cell>good screen</cell><cell>-0.0004 poor quality</cell><cell>0.1067</cell></row><row><cell>nice screen</cell><cell>0.0014 best camera</cell><cell>0.2026</cell></row><row><cell>excellent lens</cell><cell>0.0020 excellent camera</cell><cell>0.3936</cell></row><row><cell>excellent color</cell><cell>0.0027 perfect camera</cell><cell>0.3973</cell></row><row><cell>perfect size</cell><cell>0.0027 nice camera</cell><cell>0.5703</cell></row><row><cell>nice lens</cell><cell>0.0032 decent camera</cell><cell>0.5731</cell></row><row><cell>decent lens</cell><cell>0.0032 fantastic camera</cell><cell>0.6071</cell></row><row><cell>fantastic lens</cell><cell>0.0035 bad camera</cell><cell>0.6547</cell></row><row><cell>amazing lens</cell><cell>0.0038 amazing camera</cell><cell>0.6619</cell></row><row><cell>fine lens</cell><cell>0.0039 fine camera</cell><cell>0.6770</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Hedonic regressions are also commonly used by the U.S. Bureau of Labor Statistics (BLS) to take into account product quality adjustments when recalculating the consumer price index (CPI).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>U.S. Bureau of Labor Statistics.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2"><p>Instead of giving equal weight to each review, we also experimented with weighting schemes involving a time-discount factor (older reviews have smaller weights) and a "usefulness" factor. In the latter case, opinion weights were multiplied by a factor (h + 1)/(t + 1) where h and t are the number of helpful and total votes for the parent review. In both cases, the results were qualitatively similar to those of the simpler model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_3"><p>This happens because in the two product categories that we analyze, the retail price on Amazon is not a fixed discount off the list price, unlike in the case of other products such as books where publishers typically give retailers a fixed discount off the list price for a wide variety of books.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_4"><p>We did not split observations for the same product due to time-series effects within a product over time.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_5"><p>We used a non-parametric test to avoid problems with non-normality of the underlying distributions.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b11">12</ref> <p>We separated the results for point-and-shoot from those for SLR cameras and camcorders to avoid possible heterogeneity issues. Due to space constraints we do not list the results for these products and for the "Audio &amp; Video" category. The results were qualitatively similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Rhong Zheng for assistance in data collection. This work was partially supported by a Microsoft Live Labs Search Award, a Microsoft Virtual Earth Award, and by NSF grants IIS-0643847 and IIS-0643846. Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the Microsoft Corporation or of the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Berndt</surname></persName>
		</author>
		<title level="m">The Practice of Econometrics: Classic and Contemporary</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Internet forums as influential sources of consumer information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bickart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Interactive Marketing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="31" to="40" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Extracting knowledge from evaluative text</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carenini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">K-CAP&apos;05: Proceedings of the 3rd International Conference on Knowledge Capture</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Online consumer review: A strategic analysis of an emerging type of word-of-mouth</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>University of Arizona, Working Paper</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Measuring prices and price competition online</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goolsbee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amazon.com and BarnesandNoble.com. Quantitative Marketing and Economics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="203" to="222" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The effect of word of mouth on sales: Online book reviews</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Chevalier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mayzlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marketing Research</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="354" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Yahoo! for Amazon: Sentiment extraction from small talk on the web</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://scumis.scu.edu/~srdas/chat.pdf" />
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Santa Clara University</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Working Paper</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mining the peanut gallery: Opinion extraction and semantic classification of product reviews</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International World Wide Web Conference</title>
		<meeting>the 12th International World Wide Web Conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Text mining for product attribute extraction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="41" to="48" />
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Opinion mining using econometrics: A case study on reputation systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 44th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluating pricing strategy using ecommerce data: Evidence and estimation challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Greene</surname></persName>
		</author>
		<title level="m">Econometric Analysis, 5th</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The predictive power of online chatter</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gruhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2005</title>
		<meeting>the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2001-08">Aug. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004</title>
		<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mining opinion features in customer reviews</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 2004 AAAI Spring Symposium Series: Semantic Web Services</title>
		<meeting>eeding of the 2004 AAAI Spring Symposium Series: Semantic Web Services</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="755" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Use-centric mining of customer reviews</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Information Technology and Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Market distortions when agents are better informed: The value of information in real estate transactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Syverson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Working Paper, University of Chicago</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Opinion observer: Analyzing and comparing opinions on the Web</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International World Wide Web Conference</title>
		<meeting>the 14th International World Wide Web Conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="342" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple but powerful automatic term extraction method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COMPUTERM 2002: Second International Workshop on Computational Terminology</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Thumbs up? Sentiment classification using machine learning techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Extracting product features and opinions from reviews</title>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hedonic prices and implicit markets: Product differentiation in pure competition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Political Economy</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="55" />
			<date type="published" when="1974-02">Jan.-Feb. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Samuelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Nordhaus</surname></persName>
		</author>
		<author>
			<persName><surname>Economics</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>McGraw-Hill/Irwin</publisher>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
	<note>th ed.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Application of a probability-based algorithm to extraction of product features from online reviews</title>
		<author>
			<persName><forename type="first">C</forename><surname>Scaffidi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
		<respStmt>
			<orgName>Institute for Software Research, School of Computer Science, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. CMU-ISRI-06-111</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiple aspect ranking using the good grief algorithm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>HLT-NAACL</publisher>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Measuring praise and criticism: Inference of semantic orientation from association</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="315" to="346" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recognizing strong and weak opinion clauses</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="99" />
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Econometric Analysis of Cross Section and Panel Data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wooldridge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
