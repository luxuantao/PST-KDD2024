<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
							<email>zhaocheng.zhu@mila.quebec</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">HEC Montréal 3</orgName>
								<orgName type="institution" key="instit2">CIFAR AI Chair</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zuobai</forename><surname>Zhang</surname></persName>
							<email>zuobai.zhang@mila.quebec</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">HEC Montréal 3</orgName>
								<orgName type="institution" key="instit2">CIFAR AI Chair</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Louis-Pascal</forename><surname>Xhonneux</surname></persName>
							<email>louis-pascal.xhonneux@mila.quebec</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">HEC Montréal 3</orgName>
								<orgName type="institution" key="instit2">CIFAR AI Chair</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
							<email>jian.tang@hec.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Mila -Québec AI Institute 1</orgName>
								<orgName type="institution" key="instit2">Université de Montréal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Bellman-Ford Networks: A General Graph Neural Network Framework for Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Link prediction is a very fundamental task on graphs. Inspired by traditional path-based methods, in this paper we propose a general and flexible representation learning framework based on paths for link prediction. Specifically, we define the representation of a pair of nodes as the generalized sum of all path representations between the nodes, with each path representation as the generalized product of the edge representations in the path. Motivated by the Bellman-Ford algorithm for solving the shortest path problem, we show that the proposed path formulation can be efficiently solved by the generalized Bellman-Ford algorithm. To further improve the capacity of the path formulation, we propose the Neural Bellman-Ford Network (NBFNet), a general graph neural network framework that solves the path formulation with learned operators in the generalized Bellman-Ford algorithm. The NBFNet parameterizes the generalized Bellman-Ford algorithm with 3 neural components, namely INDICATOR, MESSAGE and AGGREGATE functions, which corresponds to the boundary condition, multiplication operator, and summation operator respectively 1 . The NBFNet covers many traditional path-based methods, and can be applied to both homogeneous graphs and multi-relational graphs (e.g., knowledge graphs) in both transductive and inductive settings. Experiments on both homogeneous graphs and knowledge graphs show that the proposed NBFNet outperforms existing methods by a large margin in both transductive and inductive settings, achieving new state-of-the-art results 2 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Predicting the interactions between nodes (a.k.a. link prediction) is a fundamental task in the field of graph machine learning. Given the ubiquitous existence of graphs, such a task has many applications, such as recommender system <ref type="bibr" target="#b33">[34]</ref>, knowledge graph completion <ref type="bibr" target="#b40">[41]</ref> and drug repurposing <ref type="bibr" target="#b26">[27]</ref>.</p><p>Traditional methods of link prediction usually define different heuristic metrics over the paths between a pair of nodes. For example, Katz index <ref type="bibr" target="#b29">[30]</ref> is defined as a weighted count of paths between two nodes. Personalized PageRank <ref type="bibr" target="#b41">[42]</ref> measures the similarity of two nodes as the random walk probability from one to the other. Graph distance <ref type="bibr" target="#b36">[37]</ref> uses the length of the shortest path between two nodes to predict their association. These methods can be directly applied to new graphs, i.e., inductive setting, enjoy good interpretability and scale up to large graphs. However, they are designed based on handcrafted metrics and may not be optimal for link prediction on real-world graphs.</p><p>To address these limitations, some link prediction methods adopt graph neural networks (GNNs) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b58">59]</ref> to automatically extract important features from local neighborhoods for link prediction. Thanks to the high expressiveness of GNNs, these methods have shown state-of-the-art performance. However, these methods can only be applied to predict new links on the training graph, i.e. transductive setting, and lack interpretability. While some recent methods <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b54">55]</ref> extract features from local subgraphs with GNNs and support inductive setting, the scalability of these methods is compromised. Therefore, we wonder if there exists an approach that enjoys the advantages of both traditional path-based methods and recent approaches based on graph neural networks, i.e., generalization in the inductive setting, interpretability, high model capacity and scalability.</p><p>In this paper, we propose such a solution. Inspired by traditional path-based methods, our goal is to develop a general and flexible representation learning framework for link prediction based on the paths between two nodes. Specifically, we define the representation of a pair of nodes as the generalized sum of all the path representations between them, where each path representation is defined as the generalized product of the edge representations in the path. Many link prediction methods, such as Katz index <ref type="bibr" target="#b29">[30]</ref>, personalized PageRank <ref type="bibr" target="#b41">[42]</ref>, graph distance <ref type="bibr" target="#b36">[37]</ref>, as well as graph theory algorithms like widest path <ref type="bibr" target="#b3">[4]</ref> and most reliable path <ref type="bibr" target="#b3">[4]</ref>, are special instances of this path formulation with different summation and multiplication operators. Motivated by the polynomial-time algorithm for the shortest path problem <ref type="bibr" target="#b4">[5]</ref>, we show that such a formulation can be efficiently solved via the generalized Bellman-Ford algorithm <ref type="bibr" target="#b3">[4]</ref> under mild conditions and scale up to large graphs.</p><p>The operators in the generalized Bellman-Ford algorithm-summation and multiplication-are handcrafted, which have limited flexibility. Therefore, we further propose the Neural Bellman-Ford Networks (NBFNet), a graph neural network framework that solves the above path formulation with learned operators in the generalized Bellman-Ford algorithm. Specifically, NBFNet parameterizes the generalized Bellman-Ford algorithm with three neural components, namely INDICATOR, MESSAGE and AGGREGATE functions. The INDICATOR function initializes a representation on each node, which is taken as the boundary condition of the generalized Bellman-Ford algorithm. The MESSAGE and the AGGREGATE functions learn the multiplication and summation operators respectively.</p><p>We show that the MESSAGE function can be defined according to the relational operators in knowledge graph embeddings <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">52]</ref>, e.g., as a translation in Euclidean space induced by the relational operators of TransE <ref type="bibr" target="#b5">[6]</ref>. The AGGREGATE function can be defined as learnable set aggregation functions <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b8">9]</ref>. With such parameterization, NBFNet can generalize to the inductive setting, meanwhile achieve one of the lowest time complexity among inductive GNN methods. A comparison of NBFNet and other GNN frameworks for link prediction is showed in Table <ref type="table" target="#tab_0">1</ref>. With other instantiations of MESSAGE and AGGREGATE functions, our framework can also recover some existing works on learning logic rules <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b45">46]</ref> for link prediction on knowledge graphs (Table <ref type="table" target="#tab_1">2</ref>).</p><p>Our NBFNet framework can be applied to several link prediction variants, covering not only singlerelational graphs (e.g., homogeneous graphs) but also multi-relational graphs (e.g., knowledge graphs). We empirically evaluate the proposed NBFNet for link prediction on homogeneous graphs and knowledge graphs in both transductive and inductive settings. Experimental results show that the proposed NBFNet outperforms existing state-of-the-art methods by a large margin in all settings, with an average relative performance gain of 18% on knowledge graph completion (HITS@1) and 22% on inductive relation prediction (HITS@10). We also show that the proposed NBFNet is indeed interpretable by visualizing the top-k relevant paths for link prediction on knowledge graphs. Path-based Methods. Early methods on homogeneous graphs compute the similarity between two nodes based on the weighted count of paths (Katz index <ref type="bibr" target="#b29">[30]</ref>), random walk probability (personalized PageRank <ref type="bibr" target="#b41">[42]</ref>) or the length of the shortest path (graph distance <ref type="bibr" target="#b36">[37]</ref>). SimRank <ref type="bibr" target="#b27">[28]</ref> uses advanced metrics such as the expected meeting distance on homogeneous graphs, which is extended by PathSim <ref type="bibr" target="#b50">[51]</ref> to heterogeneous graphs. On knowledge graphs, Path Ranking <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b14">15]</ref> directly uses relational paths as symbolic features for prediction. Rule mining methods, such as NeuralLP <ref type="bibr" target="#b68">[69]</ref> and DRUM <ref type="bibr" target="#b45">[46]</ref>, learn probabilistic logical rules to weight different paths. Path representation methods, such as Path-RNN <ref type="bibr" target="#b39">[40]</ref> and its successors <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b61">62]</ref>, encode each path with recurrent neural networks (RNNs), and aggregate paths for prediction. However, these methods need to traverse an exponential number of paths and are limited to very short paths, e.g., ≤ 3 edges. To scale up path-based methods, All-Paths <ref type="bibr" target="#b56">[57]</ref> proposes to efficiently aggregate all paths with dynamic programming. However, All-Paths is restricted to bilinear models and has limited model capacity.</p><p>Another stream of works <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22]</ref> learns an agent to collect useful paths for link prediction. While these methods can produce interpretable paths, they suffer from extremely sparse rewards and require careful engineering of the reward function <ref type="bibr" target="#b37">[38]</ref> or the search strategy <ref type="bibr" target="#b49">[50]</ref>. Some other works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b43">44]</ref> adopt variational inference to learn a path finder and a path reasoner for link prediction.</p><p>Embedding Methods. Embedding methods learn a distributed representation for each node and edge by preserving the edge structure of the graph. Representative methods include DeepWalk <ref type="bibr" target="#b42">[43]</ref> and LINE <ref type="bibr" target="#b52">[53]</ref> on homogeneous graphs, and TransE <ref type="bibr" target="#b5">[6]</ref>, DistMult <ref type="bibr" target="#b67">[68]</ref> and RotatE <ref type="bibr" target="#b51">[52]</ref> on knowledge graphs. Later works improve embedding methods with new score functions <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b75">76]</ref> that capture common semantic patterns of the relations, or search the score function in a general design space <ref type="bibr" target="#b74">[75]</ref>. Embedding methods achieve promising results on link prediction, and can be scaled to very large graphs using multiple GPUs <ref type="bibr" target="#b77">[78]</ref>. However, embedding methods do not explicitly encode local subgraphs between node pairs and cannot be applied to the inductive setting.</p><p>Graph Neural Networks. Graph neural networks (GNNs) <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b64">65]</ref> are a family of representation learning models that encode topological structures of graphs. For link prediction, the prevalent frameworks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b58">59</ref>] adopt an auto-encoder formulation, which uses GNNs to encode node representations, and decodes edges as a function over node pairs. Such frameworks are potentially inductive if the dataset provides node features, but are transductive only when node features are unavailable. Another stream of frameworks, such as SEAL <ref type="bibr" target="#b72">[73]</ref> and GraIL <ref type="bibr" target="#b54">[55]</ref>, explicitly encodes the subgraph around each node pair for link prediction. While these frameworks are proved to be more powerful than the auto-encoder formulation <ref type="bibr" target="#b73">[74]</ref> and can solve the inductive setting, they require to materialize a subgraph for each link, which is not scalable to large graphs. By contrast, our NBFNet explicitly captures the paths between two nodes for link prediction, meanwhile achieves a relatively low time complexity (Table <ref type="table" target="#tab_0">1</ref>). ID-GNN <ref type="bibr" target="#b69">[70]</ref> formalizes link prediction as a conditional node classification task, and augments GNNs with the identity of the source node. While the architecture of NBFNet shares some spirits with ID-GNN, our model is motivated by the generalized Bellman-Ford algorithm and has theoretical connections with traditional path-based methods. There are also some works trying to scale up GNNs for link prediction by dynamically pruning the set of nodes in message passing <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b19">20]</ref>. These methods are complementary to NBFNet, and may be incorporated into our method to further improve scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first define a path formulation for link prediction. Our path formulation generalizes several traditional methods, and can be efficiently solved by the generalized Bellman-Ford algorithm.</p><p>Then we propose Neural Bellman-Ford Networks to learn the path formulation with neural functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Path Formulation for Link Prediction</head><p>We consider the link prediction problem on both knowledge graphs and homogeneous graphs. A knowledge graph is denoted by G = (V, E, R), where V and E represent the set of entities (nodes) and relations (edges) respectively, and R is the set of relation types. We use N (u) to denote the set of nodes connected to u, and E(u) to denote the set of edges ending with node u. A homogeneous graph G = (V, E) can be viewed as a special case of knowledge graphs, with only one relation type for all edges. Throughout this paper, we use bold terms, w q (e) or h q (u, v), to denote vector representations, and italic terms, w e or w uv , to denote scalars like the weight of edge (u, v) in homogeneous graphs or triplet (u, r, v) in knowledge graphs. Without loss of generality, we derive our method based on knowledge graphs, while our method can also be applied to homogeneous graphs.</p><p>Path Formulation. Link prediction is aimed at predicting the existence of a query relation q between a head entity u and a tail entity v. From a representation learning perspective, this requires to learn a pair representation h q (u, v), which captures the local subgraph structure between u and v w.r.t. the query relation q. In traditional methods, such a local structure is encoded by counting different types of random walks from u to v <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b14">15]</ref>. Inspired by this construction, we formulate the pair representation as a generalized sum of path representations between u and v with a commutative summation operator ⊕. Each path representation h q (P ) is defined as a generalized product of the edge representations in the path with the multiplication operator ⊗.</p><formula xml:id="formula_0">h q (u, v) = h q (P 1 ) ⊕ h q (P 2 ) ⊕ ... ⊕ h q (P |Puv| )| Pi∈Puv P ∈Puv h q (P )<label>(1)</label></formula><formula xml:id="formula_1">h q (P = (e 1 , e 2 , ..., e |P | )) = w q (e 1 ) ⊗ w q (e 2 ) ⊗ ... ⊗ w q (e |P | ) |P | i=1 w q (e i )<label>(2)</label></formula><p>where P uv denotes the set of paths from u to v and w q (e i ) is the representation of edge e i . Note the multiplication operator ⊗ is not required to be commutative (e.g., matrix multiplication), therefore we define to compute the product following the exact order. Intuitively, the path formulation can be interpreted as a depth-first-search (DFS) algorithm, where one searches all possible paths from u to v, computes their representations (Equation <ref type="formula" target="#formula_1">2</ref>) and aggregates the results (Equation <ref type="formula" target="#formula_0">1</ref>). Such a formulation is capable of modeling several traditional link prediction methods, as well as graph theory algorithms. Formally, Theorem 1-5 state the corresponding path formulations for 3 link prediction methods and 2 graph theory algorithms respectively. See Appendix A for proofs.</p><p>Theorem 1 Katz index is a path formulation with ⊕ = +, ⊗ = × and w q (e) = βw e .</p><p>Theorem 2 Personalized PageRank is a path formulation with ⊕ = +, ⊗ = × and w q (e) = αw uv / v ∈N (u) w uv .</p><p>Theorem 3 Graph distance is a path formulation with ⊕ = min, ⊗ = + and w q (e) = w e .</p><p>Theorem 4 Widest path is a path formulation with ⊕ = max, ⊗ = min and w q (e) = w e .</p><p>Theorem 5 Most reliable path is a path formulation with ⊕ = max, ⊗ = × and w q (e) = w e . Generalized Bellman-Ford Algorithm. While the above formulation is able to model important heuristics for link prediction, it is computationally expensive since the number of paths grows exponentially with the path length. Previous works <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b61">62]</ref> that directly computes the exponential number of paths can only afford a maximal path length of 3. A more scalable solution is to use the generalized Bellman-Ford algorithm <ref type="bibr" target="#b3">[4]</ref>. Specifically, assuming the operators ⊕, ⊗ satisfy a semiring system <ref type="bibr" target="#b20">[21]</ref> with summation identity 0 q and multiplication identity 1 q , we have the following algorithm.</p><formula xml:id="formula_2">h (0) q (u, v) ← 1 q (u = v) (3) h (t) q (u, v) ←   (x,r,v)∈E(v) h (t−1) q (u, x) ⊗ w q (x, r, v)   ⊕ h (0) q (u, v)<label>(4)</label></formula><p>where 1 q (u = v) is the indicator function that outputs 1 q if u = v and 0 q otherwise. w q (x, r, v)</p><p>is the representation for edge e = (x, r, v) and r is the relation type of the edge. Equation 3 is known as the boundary condition, while Equation 4 is known as the Bellman-Ford iteration. The high-level idea of the generalized Bellman-Ford algorithm is to compute the pair representation h q (u, v) for a given entity u, a given query relation q and all v ∈ V in parallel, and reduce the total computation by the distributive property of multiplication over summation. Since u and q are fixed in the generalized Bellman-Ford algorithm, we may abbreviate h</p><formula xml:id="formula_3">(t) q (u, v) as h (t)</formula><p>v when the context is clear. When ⊕ = min and ⊗ = +, it recovers the original Bellman-Ford algorithm for the shortest path problem <ref type="bibr" target="#b4">[5]</ref>. See Appendix B for preliminaries and the proof of the above algorithm.</p><p>Theorem 6 Katz index, personalized PageRank, graph distance, widest path and most reliable path can be solved via the generalized Bellman-Ford algorithm. (e i ) ⊗ w q (e j ) h q (P i ) ⊕ h q (P j ) 0 q , 1 q w q (e)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Traditional Link Prediction</head><p>Katz Index <ref type="bibr" target="#b29">[30]</ref> w q (e i ) × w q (e j ) h q (P i ) + h q (P j ) 0, 1 βw e Personalized PageRank <ref type="bibr" target="#b41">[42]</ref> w q (e i ) × w q (e j ) h q (P i ) + h q (P j ) 0, 1 αw uv / v ∈N (u) w uv Graph Distance <ref type="bibr" target="#b36">[37]</ref> w q (e i ) + w q (e j ) min(h q (P i ), h q (P j )) +∞, 0 w e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Theory Algorithms</head><p>Widest Path <ref type="bibr" target="#b3">[4]</ref> min(w q (e i ), w q (e j )) max(h q (P i ), h q (P j )) −∞, +∞ w e Most Reliable Path <ref type="bibr" target="#b3">[4]</ref> w q (e i ) × w q (e j ) max(h q (P i ), h q (P j )) 0, 1 w e</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Logic Rules</head><p>NeuralLP <ref type="bibr" target="#b68">[69]</ref> / w q (e i ) × w q (e j ) h q (P i ) + h q (P j ) 0, 1 Weights learned DRUM <ref type="bibr" target="#b45">[46]</ref> by LSTM <ref type="bibr" target="#b22">[23]</ref> NBFNet Relational operators of Learned set aggregators <ref type="bibr" target="#b8">[9]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learned indicator functions</head><p>Learned relation embeddings knowledge graph embeddings <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b51">52]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Neural Bellman-Ford Networks</head><p>While the generalized Bellman-Ford algorithm can solve many classical methods (Theorem 6), these methods instantiate the path formulation with handcrafted operators (Table <ref type="table" target="#tab_1">2</ref>), and may not be optimal for link prediction. To improve the capacity of path formulation, we propose a general framework, Neural Bellman-Ford Networks (NBFNet), to learn the operators in the pair representations. for v ∈ V do 6:</p><formula xml:id="formula_4">M (t) v ← h (0) v Message augmentation 7: for (x, r, v) ∈ E(v) do 8: m (t) (x,r,v) ← MESSAGE (t) (h (t−1) x , wq(x, r, v)) 9: M (t) v ← M (t) v ∪ m (t) (x,r,v)</formula><p>10: end for 11:</p><formula xml:id="formula_5">h (t) v ← AGGREGATE (t) (M (t) v ) 12:</formula><p>end for 13: end for 14: return h The AGGREGATE function is a permutation invariant function over sets that replaces the n-ary summation operator . Note that one may alternatively define AGGREGATE as the commutative binary operator ⊕ and apply it to a sequence of messages. However, this will make the parameterization more complicated. Now consider the generalized Bellman-Ford algorithm for a given entity u and relation q. In this context, we abbreviate h</p><formula xml:id="formula_6">(t) q (u, v) as h (t)</formula><p>v , i.e., a representation on entity v in the t-th iteration. It should be stressed that h</p><formula xml:id="formula_7">(t)</formula><p>v is still a pair representation, rather than a node representation. By substituting the neural functions into Equation 3 and 4, we get our Neural Bellman-Ford Networks.</p><formula xml:id="formula_8">h (0) v ← INDICATOR(u, v, q)<label>(5)</label></formula><formula xml:id="formula_9">h (t) v ← AGGREGATE MESSAGE h (t−1) x , w q (x, r, v) (x, r, v) ∈ E(v) ∪ h (0) v<label>(6)</label></formula><p>NBFNet can be interpreted as a novel GNN framework for learning pair representations. Compared to common GNN frameworks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b47">48]</ref> that compute the pair representation as two independent node representations h q (u) and h q (v), NBFNet initializes a representation on the source node u, and readouts the pair representation on the target node v. Intuitively, our framework can be viewed as a source-specific message passing process, where every node learns a representation conditioned on the source node. The pseudo code of NBFNet is outlined in Algorithm 1.</p><p>Design Space. Now we discuss some principled designs for MESSAGE, AGGREGATE and INDI-CATOR functions by drawing insights from traditional methods. Note the potential design space for NBFNet is way larger than what is presented here, as one can always borrow MESSAGE and AGGREGATE from the arsenal of message-passing GNNs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b64">65]</ref>.</p><p>For the MESSAGE function, traditional methods instantiate it as natural summation, natural multiplication or min over scalars. Therefore, we may use the vectorized version of summation or multiplication. Intuitively, summation of h (t−1) x and w q (x, r, v) can be interpreted as a translation of h (t−1) x by w q (x, r, v) in the pair representation space, while multiplication corresponds to scaling. Such transformations correspond to the relational operators <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b44">45]</ref> in knowledge graph embeddings <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">52]</ref>. For example, translation and scaling are the relational operators used in TransE <ref type="bibr" target="#b5">[6]</ref> and DistMult <ref type="bibr" target="#b67">[68]</ref> respectively. We also consider the rotation operator in RotatE <ref type="bibr" target="#b51">[52]</ref>.</p><p>The AGGREGATE function is instantiated as natural summation, max or min in traditional methods, which are reminiscent of set aggregation functions <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b8">9]</ref> used in GNNs. Therefore, we specify the AGGREGATE function to be sum, mean, or max, followed by a linear transformation and a non-linear activation. We also consider the principal neighborhood aggregation (PNA) proposed in a recent work <ref type="bibr" target="#b8">[9]</ref>, which jointly learns the types and scales of the aggregation function.</p><p>The INDICATOR function is aimed at providing a non-trivial representation for the source node u as the boundary condition. Therefore, we learn a query embedding q for 1 q and define INDICATOR function as 1(u = v) * q. Note it is also possible to additionally learn an embedding for 0 q . However, we find a single query embedding works better in practice.</p><p>The edge representations are instantiated as transition probabilities or length in traditional methods. We notice that an edge may have different contribution in answering different query relations. Therefore, we parameterize the edge representations as a linear function over the query relation, i.e., w q (x, r, v) = W r q + b r . For homogeneous graphs or knowledge graphs with very few relations, we simplify the parameterization to w q (x, r, v) = b r to prevent overfitting. Note that one may also parameterize w q (x, r, v) with learnable entity embeddings x and v, but such a parameterization cannot solve the inductive setting. Similar to NeuralLP <ref type="bibr" target="#b68">[69]</ref> &amp; DRUM <ref type="bibr" target="#b45">[46]</ref>, we use different edge representations for different iterations, which is able to distinguish noncommutative edges in paths, e.g., father's mother v.s. mother's father.</p><p>Link Prediction. We now show how to apply the learned pair representations h q (u, v) to the link prediction problem. We predict the conditional likelihood of the tail entity v as p(v|u, q) = σ(f (h q (u, v))), where σ(•) is the sigmoid function and f (•) is a feed-forward neural network. The conditional likelihood of the head entity u can be predicted by p(u|v, q −1 ) = σ(f (h q −1 (v, u))) with the same model. Following previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b51">52]</ref>, we minimize the negative log-likelihood of positive and negative triplets (Equation <ref type="formula" target="#formula_10">7</ref>). The negative samples are generated according to Partial Completeness Assumption (PCA) <ref type="bibr" target="#b13">[14]</ref>, which corrupts one of the entities in a positive triplet to create a negative sample. For undirected graphs, we symmetrize the representations and define p q (u, v) = σ(f (h q (u, v) + h q (v, u))). Equation <ref type="formula" target="#formula_11">8</ref>shows the loss for homogeneous graphs.</p><formula xml:id="formula_10">L KG = − log p(u, q, v) − n i=1 1 n log(1 − p(u i , q, v i ))<label>(7)</label></formula><formula xml:id="formula_11">L homo = − log p(u, v) − n i=1 1 n log(1 − p(u i , v i )), (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>where n is the number of negative samples per positive sample and (u i , q, v i ) and (u i , v i ) are the i-th negative samples for knowledge graphs and homogeneous graphs, respectively.</p><p>Time Complexity. One advantage of NBFNet is that it has a relatively low time complexity during inference <ref type="foot" target="#foot_4">4</ref> . Consider a scenario where a model is required to infer the conditional likelihood of all possible triplets p(v|u, q). We group triplets with the same condition u, q together, where each group contains |V| triplets. For each group, we only need to execute Algorithm 1 once to get their predictions. Since a small constant number of iterations T is enough for NBFNet to converge ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>We evaluate NBFNet in three settings, knowledge graph completion, homogeneous graph link prediction and inductive relation prediction. The former two are transductive settings, while the last is an inductive setting. For knowledge graphs, we use FB15k-237 <ref type="bibr" target="#b55">[56]</ref> and WN18RR <ref type="bibr" target="#b12">[13]</ref>. We use the standard transductive splits <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b12">13]</ref> and inductive splits <ref type="bibr" target="#b54">[55]</ref> of these datasets. For homogeneous graphs, we use Cora, Citeseer and PubMed <ref type="bibr" target="#b48">[49]</ref>. Following previous works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b11">12]</ref>, we split the edges into train/valid/test with a ratio of 85:5:10. Statistics of datasets can be found in Appendix E. Additional experiments of NBFNet on OGB <ref type="bibr" target="#b24">[25]</ref> datasets can be found in Appendix G.</p><p>Implementation Details. Our implementation generally follows the open source codebases of knowledge graph completion <ref type="foot" target="#foot_5">5</ref> and homogeneous graph link prediction <ref type="foot" target="#foot_6">6</ref> . For knowledge graphs, we follow <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b45">46]</ref> and augment each triplet u, q, v with a flipped triplet v, q −1 , u . For homogeneous graphs, we follow <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32]</ref> and augment each node u with a self loop u, u . We instantiate NBFNet with 6 layers, each with 32 hidden units. The feed-forward network f (•) is set to a 2-layer MLP with 64 hidden units. ReLU is used as the activation function for all hidden layers. We drop out edges that directly connect query node pairs during training to encourage the model to capture longer paths and prevent overfitting. Our model is trained on 4 Tesla V100 GPUs for 20 epochs. We select the models based on their performance on the validation set. See Appendix F for more details.</p><p>Evaluation. We follow the filtered ranking protocol <ref type="bibr" target="#b5">[6]</ref> for knowledge graph completion. For a test triplet u, q, v , we rank it against all negative triplets u, q, v' or u', q, v that do not appear in the knowledge graph. We report mean rank (MR), mean reciprocal rank (MRR) and HITS at N (H@N) for knowledge graph completion. For inductive relation prediction, we follow <ref type="bibr" target="#b54">[55]</ref> and draw 50 negative triplets for each positive triplet and use the above filtered ranking. We report HITS@10 for inductive relation prediction. For homogeneous graph link prediction, we follow <ref type="bibr" target="#b31">[32]</ref> and compare the positive edges against the same number of negative edges. We report area under the receiver operating characteristic curve (AUROC) and average precision (AP) for homogeneous graphs.</p><p>Baselines. We compare NBFNet against path-based methods, embedding methods, and GNNs. These include 11 baselines for knowledge graph completion, 10 baselines for homogeneous graph link prediction and 4 baselines for inductive relation prediction. Note the inductive setting only includes path-based methods and GNNs, since existing embedding methods cannot handle this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>Table <ref type="table">3</ref> summarizes the results on knowledge graph completion. NBFNet significantly outperforms existing methods on all metrics and both datasets. NBFNet achieves an average relative gain of 21% in HITS@1 compared to the best path-based method, DRUM <ref type="bibr" target="#b45">[46]</ref>, on two datasets. Since DRUM is a special instance of NBFNet with natural summation and multiplication operators, this indicates the importance of learning MESSAGE and AGGREGATE functions in NBFNet. NBFNet also outperforms the best embedding method, LowFER <ref type="bibr" target="#b0">[1]</ref>, with an average relative performance gain of 18% in HITS@1 on two datasets. Meanwhile, NBFNet requires much less parameters than embedding methods. NBFNet only uses 3M parameters on FB15k-237, while TransE needs 30M parameters. See Appendix D for details on the number of parameters.</p><p>Table <ref type="table" target="#tab_3">4</ref> shows the results on homogeneous graph link prediction. NBFNet gets the best results on Cora and PubMed, meanwhile achieves competitive results on CiteSeer. Note CiteSeer is extremely sparse (Appendix E), which makes it hard to learn good representations with NBFNet. One thing to note here is that unlike other GNN methods, NBFNet does not use the node features provided by Table <ref type="table">3</ref>: Knowledge graph completion results. Results of NeuraLP and DRUM are taken from <ref type="bibr" target="#b45">[46]</ref>. Results of RotatE, HAKE and LowFER are taken from their original papers <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b0">1]</ref>. Results of the other embedding methods are taken from <ref type="bibr" target="#b51">[52]</ref>. Since GraIL has scalability issues in this setting, we evaluate it with 50 and 100 negative triplets for FB15k-237 and WN18RR respectively and report MR based on an unbiased estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head><p>Method FB15k-237 WN18RR MR MRR H@1 H@3 H@10 MR MRR H@1 H@3 H@10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path-based</head><p>Path Ranking <ref type="bibr" target="#b34">[35]</ref>  the datasets but is still able to outperform most other methods. We leave how to effectively combine node features and structural representations for link prediction as our future work.</p><p>Table <ref type="table" target="#tab_4">5</ref> summarizes the results on inductive relation prediction. On all inductive splits of two datasets, NBFNet achieves the best result. NBFNet outperforms the previous best method, GraIL <ref type="bibr" target="#b54">[55]</ref>, with an average relative performance gain of 22% in HITS@10. Note that GraIL explicitly encodes the local subgraph surrounding each node pair and has a high time complexity (Appendix C). Usually, GraIL can at most encode a 2-hop subgraph, while our NBFNet can efficiently explore longer paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>MESSAGE &amp; AGGREGATE Functions. Although it has been reported that GNNs with deep layers often result in significant performance drop <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b76">77]</ref>, we observe NBFNet does not have this issue. The performance increases monotonically with more layers, hitting a saturation after 6 layers. We conjecture the reason is that longer paths have negligible contribution, and paths not longer than 6 are enough for link prediction.</p><p>Performance by Relation Category. We break down the performance of NBFNet by the categories of query relations: one-to-one, one-to-many, many-to-one and many-to-many <ref type="foot" target="#foot_8">8</ref> . Table <ref type="table" target="#tab_6">6c</ref> shows the prediction results for each category. It is observed that NBFNet not only improves on easy one-to-one cases, but also on hard cases where there are multiple true answers for the query. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Path Interpretations of Predictions</head><p>One advantage of NBFNet is that we can interpret its predictions through paths, which may be important for users to understand and debug the model. Intuitively, the interpretations should contain paths that contribute most to the prediction p(u, q, v). Following local interpretation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b71">72]</ref>, we approximate the local landscape of NBFNet with a linear model over the set of all paths, i.e., 1st-order Taylor polynomial. We define the importance of a path as its weight in the linear model, which can be computed by the partial derivative of the prediction w.r.t. the path. Formally, the top-k path interpretations for p(u, q, v) are defined as</p><formula xml:id="formula_13">P 1 , P 2 , ..., P k = top-k P ∈Puv ∂p(u, q, v) ∂P<label>(9)</label></formula><p>Note this formulation generalizes the definition of logical rules <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b45">46]</ref> to non-linear models. While directly computing the importance of all paths is intractable, we approximate them with edge importance. Specifically, the importance of each path is approximated by the sum of the importance of edges in that path, where edge importance is obtained via auto differentiation. Then the top-k path interpretations are equivalent to the top-k longest paths on the edge importance graph, which can be solved by a Bellman-Ford-style beam search. Better approximation is left as a future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusion</head><p>Limitations. There are a few limitations for NBFNet. First, the assumption of the generalized Bellman-Ford algorithm requires the operators ⊕, ⊗ to satisfy a semiring. Due to the non-linear activation functions in neural networks, this assumption does not hold for NBFNet, and we do not have a theoretical guarantee on the loss incurred by this relaxation. Second, NBFNet is only verified on simple edge prediction, while there are other link prediction variants, e.g., complex logical queries with conjunctions (∧) and disjunctions (∨) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b44">45]</ref>. In the future, we would like to how NBFNet approximates the path formulation, as well as apply NBFNet to other link prediction settings.</p><p>Social Impacts. Link prediction has a wide range of beneficial applications, including recommender systems, knowledge graph completion and drug repurposing. However, there are also some potentially negative impacts. First, NBFNet may encode the bias present in the training data, which leads to stereotyped predictions when the prediction is applied to a user on a social or e-commerce platform. Second, some harmful network activities could be augmented by powerful link prediction models, e.g., spamming, phishing, and social engineering. We expect future studies will mitigate these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>We present a representation learning framework based on paths for link prediction. Our path formulation generalizes several traditional methods, and can be efficiently solved via the generalized Bellman-Ford algorithm. To improve the capacity of the path formulation, we propose NBFNet, which parameterizes the generalized Bellman-Ford algorithm with learned INDICATOR, MESSAGE, AGGREGATE functions. Experiments on knowledge graphs and homogeneous graphs show that NBFNet outperforms a wide range of methods in both transductive and inductive settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Neural Bellman-Ford Networks Input: source node u, query relation q, #layers T Output: pair representations hq(u, v) for all v ∈ V 1: for v ∈ V do Boundary condition 2:h (0)v ← INDICATOR(u, v, q) 3: end for 4: for t ← 1 to T do Bellman-Ford iteration 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>hq(u, v) for all v ∈ V Neural Parameterization. We relax the semiring assumption and parameterize the generalized Bellman-Ford algorithm (Equation 3 and 4) with 3 neural functions, namely INDICATOR, MESSAGE and AGGREGATE functions. The INDI-CATOR function replaces the indicator function 1 q (u = v). The MESSAGE function replaces the binary multiplication operator ⊗.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of GNN frameworks for link prediction. The time complexity refers to the amortized time for predicting a single edge or triplet. |V| is the number of nodes, |E| is the number of edges, and d is the dimension of representations. The wall time is measured on FB15k-237 test set with 40 CPU cores and 4 GPUs. We estimate the wall time of GraIL based on a downsampled test set.Existing work on link prediction can be generally classified into 3 main paradigms: path-based methods, embedding methods, and graph neural networks.</figDesc><table><row><cell>Method</cell><cell cols="2">Inductive 3 Interpretable Learned Representation Time Complexity Wall Time</cell></row><row><cell>VGAE [32] / RGCN [48]</cell><cell>O(d)</cell><cell>18 secs</cell></row><row><cell>NeuralLP [69] / DRUM [46]</cell><cell>O |E|d |V| + d 2</cell><cell>2.1 mins</cell></row><row><cell>SEAL [73] / GraIL [55]</cell><cell>O(|E|d 2 )</cell><cell>≈1 month</cell></row><row><cell>NBFNet</cell><cell>O |E|d |V| + d 2</cell><cell>4.0 mins</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of operators in NBFNet and other methods from the view of path formulation.</figDesc><table><row><cell>Class</cell><cell>Method</cell><cell>w q</cell><cell>MESSAGE</cell><cell>AGGREGATE</cell><cell>INDICATOR</cell><cell>Edge Representation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 6b), Algorithm 1 has a time complexity of O(|E|d + |V|d 2 ), where d is the dimension of representations. Therefore, the amortized time complexity for a single triplet is O |E|d |V| + d 2 . For a detailed derivation of time complexity of other GNN frameworks, please refer to Appendix C.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Homogeneous graphs link prediction results. Results of VGAE and S-VGAE are taken from their original papers<ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b11">12]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="10">3521 0.174 0.119 0.186 0.285 22438 0.324 0.276 0.360 0.406</cell></row><row><cell></cell><cell>NeuralLP [69]</cell><cell>-</cell><cell>0.240</cell><cell>-</cell><cell>-</cell><cell>0.362</cell><cell>-</cell><cell cols="4">0.435 0.371 0.434 0.566</cell></row><row><cell></cell><cell>DRUM [46]</cell><cell>-</cell><cell cols="4">0.343 0.255 0.378 0.516</cell><cell>-</cell><cell cols="4">0.486 0.425 0.513 0.586</cell></row><row><cell></cell><cell>TransE [6]</cell><cell>357</cell><cell>0.294</cell><cell>-</cell><cell>-</cell><cell>0.465</cell><cell>3384</cell><cell>0.226</cell><cell>-</cell><cell>-</cell><cell>0.501</cell></row><row><cell></cell><cell>DistMult [68]</cell><cell>254</cell><cell cols="4">0.241 0.155 0.263 0.419</cell><cell>5110</cell><cell>0.43</cell><cell>0.39</cell><cell>0.44</cell><cell>0.49</cell></row><row><cell>Embeddings</cell><cell>ComplEx [58] RotatE [52]</cell><cell>339 177</cell><cell cols="4">0.247 0.158 0.275 0.428 0.338 0.241 0.375 0.553</cell><cell>5261 3340</cell><cell cols="4">0.44 0.476 0.428 0.492 0.571 0.41 0.46 0.51</cell></row><row><cell></cell><cell>HAKE [76]</cell><cell>-</cell><cell cols="4">0.346 0.250 0.381 0.542</cell><cell>-</cell><cell cols="4">0.497 0.452 0.516 0.582</cell></row><row><cell></cell><cell>LowFER [1]</cell><cell>-</cell><cell cols="4">0.359 0.266 0.396 0.544</cell><cell>-</cell><cell cols="4">0.465 0.434 0.479 0.526</cell></row><row><cell></cell><cell>RGCN [48]</cell><cell>221</cell><cell cols="4">0.273 0.182 0.303 0.456</cell><cell>2719</cell><cell cols="4">0.402 0.345 0.437 0.494</cell></row><row><cell>GNNs</cell><cell>GraIL [55]</cell><cell>2053</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2539</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>NBFNet</cell><cell>114</cell><cell cols="4">0.415 0.321 0.454 0.599</cell><cell>636</cell><cell cols="4">0.551 0.497 0.573 0.666</cell></row><row><cell>Class</cell><cell>Method</cell><cell></cell><cell></cell><cell cols="2">Cora AUROC</cell><cell>AP</cell><cell cols="2">Citeseer AUROC AP</cell><cell cols="3">PubMed AUROC AP</cell></row><row><cell></cell><cell>Katz Index [30]</cell><cell></cell><cell></cell><cell>0.834</cell><cell></cell><cell>0.889</cell><cell>0.768</cell><cell>0.810</cell><cell></cell><cell>0.757</cell><cell>0.856</cell></row><row><cell>Path-based</cell><cell cols="3">Personalized PageRank [42]</cell><cell>0.845</cell><cell></cell><cell>0.899</cell><cell>0.762</cell><cell>0.814</cell><cell></cell><cell>0.763</cell><cell>0.860</cell></row><row><cell></cell><cell>SimRank [28]</cell><cell></cell><cell></cell><cell>0.838</cell><cell></cell><cell>0.888</cell><cell>0.755</cell><cell>0.805</cell><cell></cell><cell>0.743</cell><cell>0.829</cell></row><row><cell></cell><cell>DeepWalk [43]</cell><cell></cell><cell></cell><cell>0.831</cell><cell></cell><cell>0.850</cell><cell>0.805</cell><cell>0.836</cell><cell></cell><cell>0.844</cell><cell>0.841</cell></row><row><cell>Embeddings</cell><cell>LINE [53]</cell><cell></cell><cell></cell><cell>0.844</cell><cell></cell><cell>0.876</cell><cell>0.791</cell><cell>0.826</cell><cell></cell><cell>0.849</cell><cell>0.888</cell></row><row><cell></cell><cell>node2vec [17]</cell><cell></cell><cell></cell><cell>0.872</cell><cell></cell><cell>0.879</cell><cell>0.838</cell><cell>0.868</cell><cell></cell><cell>0.891</cell><cell>0.914</cell></row><row><cell></cell><cell>VGAE [32]</cell><cell></cell><cell></cell><cell>0.914</cell><cell></cell><cell>0.926</cell><cell>0.908</cell><cell>0.920</cell><cell></cell><cell>0.944</cell><cell>0.947</cell></row><row><cell></cell><cell>S-VGAE [12]</cell><cell></cell><cell></cell><cell>0.941</cell><cell></cell><cell>0.941</cell><cell>0.947</cell><cell>0.952</cell><cell></cell><cell>0.960</cell><cell>0.960</cell></row><row><cell>GNNs</cell><cell>SEAL [73]</cell><cell></cell><cell></cell><cell>0.933</cell><cell></cell><cell>0.942</cell><cell>0.905</cell><cell>0.924</cell><cell></cell><cell>0.978</cell><cell>0.979</cell></row><row><cell></cell><cell>TLC-GNN [67]</cell><cell></cell><cell></cell><cell>0.934</cell><cell></cell><cell>0.931</cell><cell>0.909</cell><cell>0.916</cell><cell></cell><cell>0.970</cell><cell>0.968</cell></row><row><cell></cell><cell>NBFNet</cell><cell></cell><cell></cell><cell>0.956</cell><cell></cell><cell>0.962</cell><cell>0.923</cell><cell>0.936</cell><cell></cell><cell>0.983</cell><cell>0.982</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Inductive relation prediction results (HITS@10). V1-v4 corresponds to the 4 standard versions of inductive splits. Results of compared methods are taken from<ref type="bibr" target="#b54">[55]</ref>.</figDesc><table><row><cell>Class</cell><cell>Method</cell><cell>v1</cell><cell>FB15k-237 v2 v3</cell><cell>v4</cell><cell>v1</cell><cell>WN18RR v2 v3</cell><cell>v4</cell></row><row><cell></cell><cell cols="7">NeuralLP [16] 0.529 0.589 0.529 0.559 0.744 0.689 0.462 0.671</cell></row><row><cell>Path-based</cell><cell>DRUM [46]</cell><cell cols="6">0.529 0.587 0.529 0.559 0.744 0.689 0.462 0.671</cell></row><row><cell></cell><cell>RuleN [39]</cell><cell cols="6">0.498 0.778 0.877 0.856 0.809 0.782 0.534 0.716</cell></row><row><cell>GNNs</cell><cell>GraIL [55] NBFNet</cell><cell cols="6">0.642 0.818 0.828 0.893 0.825 0.787 0.584 0.734 0.834 0.949 0.951 0.960 0.948 0.905 0.893 0.890</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table6ashows the results of different MESSAGE and AGGREGATE functions. Generally, NBFNet benefits from advanced embedding methods (DistMult, RotatE &gt; TransE) and aggregation functions (PNA &gt; sum, mean, max). Among simple AGGREGATE functions (sum, mean, max), combinations of MESSAGE and AGGREGATE functions (TransE &amp; max, DistMult &amp; sum) that satisfy the semiring assumption7 of the generalized Bellman-Ford algorithm, achieve locally optimal performance. PNA significantly improves over simple counterparts, which highlights the importance of learning more powerful AGGREGATE functions.Number of GNN Layers. Table6bcompares the results of NBFNet with different number of layers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies of NBFNet on FB15k-237. Due to space constraints, we only report MRR here. For full results on all metrics, please refer to Appendix H.</figDesc><table><row><cell cols="5">(a) Different MESSAGE and AGGREGATE functions.</cell><cell cols="3">(b) Different number of layers.</cell></row><row><cell>MESSAGE</cell><cell>Sum</cell><cell cols="2">AGGREGATE Mean Max</cell><cell>PNA [9]</cell><cell>Method</cell><cell>2</cell><cell>#Layers (T ) 4 6</cell><cell>8</cell></row><row><cell>TransE [6]</cell><cell cols="3">0.297 0.310 0.377</cell><cell>0.383</cell><cell cols="4">NBFNet 0.345 0.409 0.415 0.416</cell></row><row><cell cols="4">DistMult [69] 0.388 0.384 0.374</cell><cell>0.415</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RotatE [52]</cell><cell cols="3">0.392 0.376 0.385</cell><cell>0.414</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell></cell><cell>1-to-1</cell><cell cols="2">Relation Category 1-to-N N-to-1</cell><cell></cell><cell>N-to-N</cell></row><row><cell></cell><cell cols="2">TransE [6]</cell><cell cols="5">0.498/0.488 0.455/0.071 0.079/0.744 0.224/0.330</cell></row><row><cell></cell><cell cols="7">RotatE [51] 0.487/0.484 0.467/0.070 0.081/0.747 0.234/0.338</cell></row><row><cell></cell><cell>NBFNet</cell><cell></cell><cell cols="5">0.578/0.600 0.499/0.122 0.165/0.790 0.348/0.456</cell></row></table><note>(c) Performance w.r.t. relation category. The two scores are the rankings over heads and tails respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>visualizes path interpretations from FB15k-237 test set. While users may have different insights towards the visualization, here is our understanding. 1) In the first example, NBFNet learns soft logical entailment, such as impersonate −1 ∧ nationality =⇒ nationality and ethnicity −1 ∧ distribution =⇒ nationality. 2) In second example, NBFNet performs analogical reasoning by leveraging the fact that Florence is similar to Rome. 3) In the last example, NBFNet extracts longer paths, since there is no obvious connection between Pearl Harbor (film) and Japanese language.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Path interpretations of predictions on FB15k-237 test set. For each query triplet, we visualize the top-2 path interpretations and their weights. Inverse relations are denoted with a superscript −1 . Hardy, impersonate −1 , R. Little ∧ R. Little, nationality, U.S. 0.224 O. Hardy, ethnicity −1 , Scottish American ∧ Scottish American, distribution, U.S. Query u, q, v : Florence, vacationer, D.C. Henrie 0.251 Florence, contain −1 , Italy ∧ Italy, capital, Rome ∧ Rome, vacationer, D.C. Henrie 0.183 Florence, place live −1 , G.F. Handel ∧ G.F. Handel, place live, Rome ∧ Rome, vacationer, D.C. Henrie Query u, q, v : Pearl Harbor (film), language, Japanese 0.211 Pearl Harbor (film), film actor, C.-H. Tagawa ∧ C.-H. Tagawa, nationality, Japan ∧ Japan, country of origin, Yu-Gi-Oh! ∧ Yu-Gi-Oh!, language, Japanese 0.208 Pearl Harbor (film), film actor, C.-H. Tagawa ∧ C.-H. Tagawa, nationality, Japan ∧ Japan, official language, Japanese</figDesc><table><row><cell>Query</cell><cell>u, q, v : O. Hardy, nationality, U.S.</cell></row><row><cell>0.243</cell><cell>O.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Unless stated otherwise, we use summation and multiplication to refer the generalized operators in the path formulation, rather than the basic operations of arithmetic.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Code is available at https://github.com/DeepGraphLearning/NBFNet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="35" xml:id="foot_2">35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">We consider the inductive setting where a model can generalize to entirely new graphs without node features.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">Although the same analysis can be applied to training on a fixed number of samples, we note it is less instructive since one can trade-off samples for performance, and the trade-off varies from method to method.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding. MIT license.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">https://github.com/tkipf/gae. MIT license.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7">Here semiring is discussed under the assumption of linear activation functions. Rigorously, no combination satisfies a semiring if we consider non-linearity in the model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_8">The categories are defined same as<ref type="bibr" target="#b62">[63]</ref>. We compute the average number of tails per head and the average number of heads per tail. The category is one if the average number is smaller than 1.5 and many otherwise.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_9">https://www.calculquebec.ca/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_10">https://www.computecanada.ca/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Komal Teru for discussion on inductive relation prediction, Guyue Huang for discussion on fused message passing implementation, and Yao Lu for assistance on large-scale GPU training. We thank Meng Qu, Chence Shi and Minghao Xu for providing feedback on our manuscript. This project is supported by the Natural Sciences and Engineering Research Council (NSERC) Discovery Grant, the Canada CIFAR AI Chair Program, collaboration grants between Microsoft Research and Mila, Samsung Electronics Co., Ltd., Amazon Faculty Research Award, Tencent AI Lab Rhino-Bird Gift Fund and a NRC Collaborative R&amp;D Project (AI4D-CORE-06). This project was also partially funded by IVADO Fundamental Research Project grant PRF-2019-3583139727. The computation resource of this project is supported by Calcul Québec <ref type="bibr" target="#b8">9</ref> and Compute Canada 10 .</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lowfer: Lowrank bilinear pooling for link prediction</title>
		<author>
			<persName><forename type="first">Saadullah</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stalin</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Dunfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Günter</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="257" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How to explain individual classification decisions</title>
		<author>
			<persName><forename type="first">David</forename><surname>Baehrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timon</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motoaki</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1803" to="1831" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Path problems in networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Baras</surname></persName>
		</author>
		<author>
			<persName><surname>Theodorakopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Communication Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="77" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On a routing problem</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly of applied mathematics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="90" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PairRE: Knowledge graph embeddings via paired relation vectors</title>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4360" to="4369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variational knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1823" to="1832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chains of reasoning over entities, relations, and text using recurrent neural networks</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04">April 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Hyperspherical variational auto-encoders</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Tim R Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Falorsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Tomczak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Amie: association rule mining under incomplete evidence in ontological knowledge bases</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Antonio Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
				<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient and expressive knowledge base completion using subgraph feature extraction</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1488" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Embedding logical queries on knowledge graphs</title>
		<author>
			<persName><forename type="first">Payal</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2030" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">xerte: Explainable reasoning on temporal knowledge graphs for forecasting future links</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Volker Tresp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semirings: algebraic theory and applications in computer science</title>
		<author>
			<persName><forename type="first">Udo</forename><surname>Hebisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Hanns</surname></persName>
		</author>
		<author>
			<persName><surname>Weinert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Scientific</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reasoning on knowledge graphs with debate dynamics</title>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Andres Quintero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpu</forename><surname>Serna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Ringsquandl</surname></persName>
		</author>
		<author>
			<persName><surname>Joblin</surname></persName>
		</author>
		<author>
			<persName><surname>Volker Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4123" to="4131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ogblsc: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09430</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ge-spmm: General-purpose sparse matrix-matrix multiplication on gpus for graph neural networks</title>
		<author>
			<persName><forename type="first">Guyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huazhong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Few-shot link prediction via graph neural networks for covid-19 drug-repurposing</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Vassilis N Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><surname>Karypis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10261</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simrank: a measure of structural-context similarity</title>
		<author>
			<persName><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="538" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scaling personalized web search</title>
		<author>
			<persName><forename type="first">Glen</forename><surname>Jeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international conference on World Wide Web</title>
				<meeting>the 12th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A new status index derived from sociometric analysis</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="43" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Mehran</forename><surname>Seyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4289" to="4300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-hop knowledge graph reasoning with reward shaping</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31">2018. October 31-November 4, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fine-grained evaluation of rule-and embedding-based systems for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Compositional vector space models for knowledge base completion</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="156" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rnnlogic: Learning logic rules for reasoning on knowledge graphs</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junkun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Pascal</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Query2box: Reasoning over knowledge graphs in vector space using box embeddings</title>
		<author>
			<persName><surname>H Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Drum: End-to-end differentiable rule mining on knowledge graphs</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Armandpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15347" to="15357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">M-walk: learning to walk over graphs using monte carlo tree search</title>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6787" to="6798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Pathsim: Meta path-based top-k similarity search in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>VLDB Endowment</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="992" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Line: Largescale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on World Wide Web</title>
				<meeting>the 24th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Orthogonal relation transforms with graph context modeling for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2713" to="2722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Inductive relation prediction by subgraph reasoning</title>
		<author>
			<persName><forename type="first">Komal</forename><surname>Teru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9448" to="9457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd workshop on continuous vector space models and their compositionality</title>
				<meeting>the 3rd workshop on continuous vector space models and their compositionality</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Compositional learning of embeddings for relation paths in knowledge base and text</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1434" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Composition-based multi-relational graph convolutional networks</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Relational message passing for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1697" to="1707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deeppath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017-09">2017. September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Dynamically pruned message passing networks for large-scale knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Xiaoran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Link prediction with persistent homology: An interactive view</title>
		<author>
			<persName><forename type="first">Zuoyu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangcai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11659" to="11669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Differentiable learning of logical rules for knowledge base reasoning</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Identity-aware graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Gomes-Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="10737" to="10745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks for link prediction</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16103</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Autosf: Searching scoring functions for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Yongqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 36th International Conference on Data Engineering (ICDE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning hierarchy-aware knowledge graph embeddings for link prediction</title>
		<author>
			<persName><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3065" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Pairnorm: Tackling oversmoothing in gnns</title>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Graphvite: A high-performance cpu-gpu hybrid system for node embedding</title>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2494" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
