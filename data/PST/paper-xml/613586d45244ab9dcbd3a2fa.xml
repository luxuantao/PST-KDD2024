<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-09-03">3 Sep 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Adams</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><forename type="middle">Brian</forename><surname>Lester</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai Quoc</surname></persName>
						</author>
						<author>
							<persName><forename type="first">V</forename><surname>Le</surname></persName>
						</author>
						<title level="a" type="main">FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09-03">3 Sep 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2109.01652v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning-finetuning language models on a collection of tasks described via instructions-substantially boosts zero-shot performance on unseen tasks.</p><p>We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instructiontuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 19 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of tasks and model scale are key components to the success of instruction tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Language models (LMs) at scale, such as GPT-3 <ref type="bibr" target="#b11">(Brown et al., 2020)</ref>, have been shown to perform few-shot learning remarkably well. They are less successful at zero-shot learning, however. For example, GPT-3's zero-shot performance is much worse than few-shot performance on tasks such as reading comprehension, question answering, and natural language inference. One potential reason is that, without few-shot exemplars, it is harder for models to perform well on prompts that are not similar to the format of the pretraining data.</p><p>In this paper, we explore a simple method to improve the zero-shot performance of large language models, which would expand their reach to a broader audience. We leverage the intuition that NLP tasks can be described via natural language instructions, such as "Is the sentiment of this movie review positive or negative?" or "Translate 'how are you' into Chinese." We take a pretrained language model of 137B parameters and perform instruction tuning-finetuning the model on a mixture of more than 60 NLP tasks expressed via natural language instructions. We refer to this resulting model as Finetuned LAnguage Net, or FLAN.</p><p>To evaluate the zero-shot performance of FLAN on unseen tasks, we group NLP tasks into clusters based on their task types and hold out each cluster for evaluation while instruction tuning FLAN on all other clusters. For example, as shown in Figure <ref type="figure">1</ref>, to evaluate FLAN's ability to perform natural language inference, we instruction-tune the model on a range of other NLP tasks such as commonsense reasoning, translation, and sentiment analysis. As this setup ensures that FLAN has not seen any natural language inference tasks in instruction tuning, we then evaluate its ability to perform zero-shot natural language inference.</p><p>Our evaluations show that FLAN substantially improves the zero-shot performance of the base 137B-parameter model. FLAN's zero-shot also outperforms 175B-parameter GPT-3's zero-shot on 19 of 25 tasks that we evaluate, and even outperforms GPT-3's few-shot by a large margin on a number of tasks such as ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. In ablation studies, we find that increasing the number of task clusters in instruction tuning improves performance on unseen tasks and that the benefits of instruction tuning emerge only with sufficient model scale.</p><p>Our empirical results underscore the ability of language models to perform tasks described using natural language instructions. More broadly, as shown in Figure <ref type="figure" target="#fig_0">2</ref>, instruction tuning combines appealing characteristics of the pretrain-finetune and prompting paradigms by using supervision via finetuning to improve the ability of language models to respond to inference-time text interactions. Source code for loading the instruction tuning dataset used for FLAN is made publicly available at https://github.com/google-research/flan. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FLAN: INSTRUCTION TUNING IMPROVES ZERO-SHOT LEARNING</head><p>The motivation of instruction tuning is to improve the ability of language models to respond to NLP instructions. The idea is that by using supervision to teach an LM to perform tasks described via instructions, it will learn to follow instructions and do so even for unseen tasks. To evaluate the model's performance on unseen tasks, we group tasks into clusters by task type and hold out each task cluster for evaluation while instruction tuning on all remaining clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TASKS &amp; TEMPLATES</head><p>Creating a viable instruction tuning dataset with a large number of tasks from scratch would be resource-intensive. We therefore choose to transform existing datasets created by the research community into an instructional format. We aggregate 62 text datasets that are publicly available on Tensorflow Datasets,<ref type="foot" target="#foot_0">1</ref> including both language understanding and language generation tasks, into a single mixture. Figure <ref type="figure" target="#fig_1">3</ref> shows all datasets we use; each dataset is categorized into one of twelve task clusters, for which datasets in a given cluster are of the same task type. We refer the involved reader to Appendix B for descriptions, sizes, and examples of individual datasets.</p><p>Natural language inference (7 tasks) We define a task as a particular set of input-output pairs given by a dataset (e.g., we consider RTE and ANLI separate tasks, even though they are both entailment). For each task, we manually compose ten unique templates that describe the task using natural language instructions. Most of the ten templates describe the original task, but to increase diversity, for each task we include up to three templates that "turned the task around," (e.g., for sentiment classification we include templates asking to generate a negative movie review). We then instruction-tune a pretrained language model on the mixture of all tasks, with examples in each task formatted via a randomly selected instruction template for that task. Figure <ref type="figure" target="#fig_2">4</ref> shows multiple instruction templates for a natural language inference task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entailment Not entailment</head><p>Russian cosmonaut Valery Polyakov set the record for the longest continuous amount of time spent in space, a staggering 438 days, between 1994 and 1995.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Premise</head><p>Russians hold the record for the longest stay in space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis Target</head><p>Options:</p><p>yes no &lt;premise&gt; Can we infer the following? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">EVALUATION SPLITS</head><p>We are interested in how FLAN performs on tasks that were not seen in instruction tuning, and so it is crucial to define what counts as an unseen task. Whereas some prior work has categorized unseen tasks by disallowing the same dataset to appear in training, we will use a more conservative definition that leverages the task clusters from Figure <ref type="figure" target="#fig_1">3</ref>. In this work, we only consider task T unseen at evaluation time if no tasks from any clusters that T belongs to were seen during instruction tuning. For instance, if T is an entailment task, then no entailment tasks appeared in the instruction tuning dataset, and we instruction-tuned on tasks from all other clusters.<ref type="foot" target="#foot_1">2</ref> So, using this definition, to evaluate the performance of FLAN on tasks spanning c clusters, we instruction-tune models for c inter-cluster splits, in which each split holds out a different cluster during instruction tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CLASSIFICATION WITH OPTIONS</head><p>The desired output space for a given task is either one of several given classes (e.g., classification) or free text (e.g., generation). As FLAN is an instruction-tuned version of a decoder-only language model, it naturally responds in free text, and so no further modifications are needed for tasks where the desired output is free text.</p><p>For classification tasks, prior work <ref type="bibr" target="#b11">(Brown et al., 2020)</ref> has used a rank classification approach where, for example, only two outputs ("yes" and "no") are considered and the higher probability one is taken as the model's prediction. Though this procedure is logically sound, it is imperfect in that the probability mass for answers may have an undesired distribution among ways of saying each answer (e.g., a large number of alternative ways of saying "yes" may lower the probability mass assigned to "yes"). Therefore, we include an options suffix, in which we append the token OPTIONS to the end of a classification task along with a list of the output classes for that task. This makes the model aware of which choices are desired when responding to classification tasks. Example use of options is shown in the NLI and commonsense examples in Figure <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">TRAINING DETAILS</head><p>Model architecture and pretraining. In our experiments, we use a dense left-to-right, decoder-only transformer language model of 137B parameters. This model is pretrained on a collection of web documents (including those with computer code), dialog data, and Wikipedia tokenized into 2.81T BPE tokens with a vocabulary of 32K tokens using the SentencePiece library <ref type="bibr" target="#b33">(Kudo &amp; Richardson, 2018)</ref>. Approximately 10% of the pretraining data was non-English. This dataset is not as clean as the GPT-3 training set and also has a mixture of dialog and code, and so we expect the zero and few-shot performance of this pretrained LM on NLP tasks to be slightly lower. We henceforth refer to this pretrained model as Base LM. This same model was also previously used for program synthesis <ref type="bibr" target="#b1">(Austin et al., 2021)</ref>.</p><p>Instruction tuning procedure. FLAN is the instruction-tuned version of Base LM. Our instruction tuning pipeline mixes all datasets and randomly samples examples from each dataset. Some datasets have more than ten million training examples (e.g., translation), and so we limit the number of training examples per dataset to 30,000. Other datasets have few training examples (e.g., CommitmentBank only has 250), and so to prevent these datasets from being marginalized, we follow the examples-proportional mixing scheme <ref type="bibr" target="#b57">(Raffel et al., 2020)</ref> with a mixing rate maximum of 3,000. <ref type="foot" target="#foot_2">3</ref> We finetune all models for 30,000 gradient updates at a batch size of 8,192 using the Adafactor Optimizer <ref type="bibr" target="#b65">(Shazeer &amp; Stern, 2018)</ref> with a learning rate of 3e-5. The input and target sequence lengths used in our finetuning procedure are 1024 and 256 respectively. We use packing <ref type="bibr" target="#b57">(Raffel et al., 2020)</ref> to combine multiple training examples into a single sequence, separating inputs from targets using a special end-of-sequence token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>In this section, we evaluate FLAN on tasks spanning natural language inference, reading comprehension, open-domain QA, commonsense reasoning, coreference resolution, and translation. As described in Section 2.2, we evaluate on unseen tasks by grouping tasks into clusters and holding out each cluster for evaluation while instruction tuning on all remaining clusters. For each task we report the mean and standard deviation of performance on all templates, which proxies the expected performance of FLAN given a typical natural language instruction. As a dev set is sometimes available for manual prompt engineering <ref type="bibr" target="#b11">(Brown et al., 2020)</ref>, for each task we also report the test set performance using the template that had the best dev set performance.</p><p>For comparison, we include the zero-shot/few-shot performance of 175B GPT-3. With the best dev template, zero-shot FLAN outperforms zero-shot GPT-3 on 19 out of 25 tasks and even surpasses GPT-3's few-shot performance on 10 tasks. We also report zero and few-shot<ref type="foot" target="#foot_3">4</ref> results from Base LM using the same prompts as GPT-3, as Base LM is not suitable for natural instructions without instruction tuning. This baseline provides the most direct ablation of how much instruction tuning helps. Instruction tuning significantly improves Base LM on most tasks. In terms of supervised models, we report the performance of T5-11B <ref type="bibr" target="#b57">(Raffel et al., 2020)</ref> when available; otherwise, we show the performance of BERT-large <ref type="bibr" target="#b20">(Devlin et al., 2019)</ref>.</p><p>Overall, we observe that instruction tuning is very effective on tasks that can be naturally verbalized as instructions (e.g., natural language inference and question answering) and is less effective on tasks that are directly formulated as language modeling (e.g., commonsense reasoning and coreference resolution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NATURAL LANGUAGE INFERENCE</head><p>Table <ref type="table">1</ref> shows results for natural language inference (NLI), where given a premise and hypothesis, a model must determine whether the hypothesis is true given that the premise is true. FLAN has strong performance in all cases. Despite high variance among results for different templates for CB <ref type="bibr" target="#b19">(de Marneffe et al., 2019)</ref> and RTE <ref type="bibr" target="#b69">(Wang et al., 2018)</ref>, FLAN outperforms zero and few-shot GPT-3 for four datasets by a large margin without any prompt engineering. With the best dev template, FLAN outperforms few-shot GPT-3 on all five datasets. FLAN even surpasses supervised BERT on ANLI-R3 <ref type="bibr" target="#b51">(Nie et al., 2020)</ref>.</p><p>As noted by <ref type="bibr" target="#b11">Brown et al. (2020)</ref>, perhaps one reason why GPT-3 struggles with NLI is that NLI examples are unlikely to have appeared naturally in an unsupervised training set and are thus awkwardly phrased as a continuation of a sentence. For FLAN on the other hand, we phrase NLI as the more natural question "Does &lt;premise&gt; mean that &lt;hypothesis&gt;?" and achieve much higher performance. 83.9 1.8 78.3 7.9 stdev=7.9 84.1 13.9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NATURAL LANGUAGE INFERENCE ANLI-R1</head><p>Table <ref type="table">1</ref>: Results on natural language inference. For FLAN, we report both the average of up to ten templates (proxying the expected performance without prompt engineering), as well as the test set performance of the template that had the highest performance on the dev set. The triangle indicates improvement over few-shot GPT-3. The up-arrow ? indicates improvement only over zero-shot GPT-3. a T5-11B, b BERT-large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">READING COMPREHENSION &amp; OPEN-DOMAIN QA</head><p>Results for reading comprehension, where models are asked to answer a question about a provided passage, are shown in Table <ref type="table">2</ref>. FLAN outperforms GPT-3 by a large margin on BoolQ <ref type="bibr">(Clark et al., 2019a)</ref> and OBQA <ref type="bibr" target="#b48">(Mihaylov et al., 2018)</ref>, although Base LM already achieves high performance on BoolQ. On MultiRC <ref type="bibr" target="#b31">(Khashabi et al., 2018)</ref>, the performance of FLAN is slightly higher than few-shot GPT-3 when using the best dev template. Also shown in Table <ref type="table">2</ref> are results for open-domain question answering, which asks models to answer questions about the world without access to specific information containing the answer. On ARC-easy and ARC-challenge <ref type="bibr" target="#b14">(Clark et al., 2018)</ref>, the performance of FLAN is substantially higher than both zero-shot and few-shot GPT-3. For Natural Questions (NQ; <ref type="bibr" target="#b37">Lee et al., 2019;</ref><ref type="bibr" target="#b34">Kwiatkowski et al., 2019)</ref> FLAN outperforms zero-shot GPT-3 but does not outperform few-shot GPT-3. For TriviaQA <ref type="bibr" target="#b30">(Joshi et al., 2017)</ref>, however, FLAN underperforms zero-shot GPT-3, which achieves remarkable performance and beats supervised T5-11B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>56.7</head><p>Table <ref type="table">2</ref>: Results on reading comprehension and open-domain question answering. For FLAN, we report both the average of up to ten templates (proxying the expected performance without prompt engineering), as well as the test set performance of the template that had the highest performance on the dev set. The triangle indicates improvement over few-shot GPT-3. The up-arrow ? indicates improvement only over zero-shot GPT-3. a T5-11B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">COMMONSENSE REASONING &amp; COREFERENCE RESOLUTION</head><p>Results for five commonsense reasoning datasets are shown in Table <ref type="table">3</ref>. FLAN outperforms GPT-3 on StoryCloze <ref type="bibr" target="#b50">(Mostafazadeh et al., 2016)</ref> and has comparable performance on CoPA <ref type="bibr" target="#b59">(Roemmele et al., 2011)</ref> and PiQA <ref type="bibr" target="#b6">(Bisk et al., 2020)</ref>. For HellaSwag <ref type="bibr" target="#b76">(Zellers et al., 2019)</ref> and ReCoRD <ref type="bibr" target="#b77">(Zhang et al., 2018)</ref>, however, both Base LM and FLAN performs well below GPT-3.</p><p>Table <ref type="table">3</ref> also shows results for two coreference resolution datasets. FLAN with the best dev template outperforms zero-shot GPT-3 on Winogrande <ref type="bibr" target="#b62">(Sakaguchi et al., 2020)</ref>. For WSC273 <ref type="bibr" target="#b39">(Levesque et al., 2012)</ref>, both Base LM and FLAN are well below GPT-3 (we cannot compute the best dev template result for FLAN since WSC273 does not have a validation set).</p><p>One observation is that HellaSwag, PiQA, ReCoRD, WSC273, and Winogrande are directly formulated in the original pretraining objective of language modeling. Hence, the result that GPT-3 outperforms FLAN by a large margin is consistent with the intuition that gains from FLAN will be marginal when instructions are not crucial for describing the given task. Moreover, we note a further limitation with FLAN for these five language modeling tasks, including options actually hurts performance, and so the reported results are for rank classification without options.  Table 3: Results (accuracy in %) for commonsense reasoning and coreference resolution. For FLAN, we report both the average of up to ten templates (proxying the expected performance without prompt engineering), as well as the test set performance of the template that had the highest performance on the dev set. a T5-11B, b BERT-large. The triangle indicates improvement over few-shot GPT-3. The up-arrow ? indicates improvement only over zero-shot GPT-3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COMMONSENSE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">TRANSLATION</head><p>Similar to GPT-3, the training data for Base LM is around 90% English and includes some text in other languages that was not specifically used to train the model to perform machine translation. We also evaluate FLAN's performance on machine translation for the three datasets evaluated in the GPT-3 paper: French-English from WMT'14 <ref type="bibr" target="#b7">(Bojar et al., 2014)</ref>, and German-English and Romanian-English from WMT'16 <ref type="bibr" target="#b8">(Bojar et al., 2016)</ref>.</p><p>These results are shown in Table <ref type="table">4</ref>. For Base LM, zero-shot translation had low performance, but few-shot translation results were comparable with GPT-3; FLAN outperforms few-shot Base LM in five of the six evaluations. Compared with GPT-3, FLAN outperforms zero-shot GPT-3 for all six evaluations, though it underperforms few-shot GPT-3 in most cases. Similar to GPT-3, FLAN shows strong results for translating into English and compares favorably against supervised translation baselines. Translating from English into other languages, however, was relatively weaker, as might be expected given that FLAN uses an English sentencepiece tokenizer and that the majority of pretraining data is English. Table <ref type="table">4</ref>: Translation results (BLEU) for WMT'14 En/Fr and WMT'16 En/De and En/Ro. For FLAN, we report both the average of up to ten templates (proxying the expected performance without prompt engineering), as well as the test set performance of the template that had the highest performance on the dev set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ABLATION STUDIES &amp; FURTHER ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NUMBER OF INSTRUCTION TUNING CLUSTERS</head><p>As the core question of our paper asks how instruction tuning improves a model's zero-shot performance on unseen tasks, in this first ablation we examine how performance is affected by the number of clusters and tasks used in instruction tuning. For this setup, we hold out NLI, open-domain QA, and commonsense reasoning as evaluation clusters, and use the seven remaining clusters for instruction tuning. <ref type="foot" target="#foot_4">5</ref> We show results for 1-7 instruction tuning clusters, where clusters are added in decreasing order of number of tasks per cluster.</p><p>Figure <ref type="figure" target="#fig_5">5</ref> shows these results. As expected, we observe that average performance across the three held-out clusters improves as we add additional clusters and tasks to instruction tuning (with the exception of the sentiment analysis cluster), confirming the benefits of our proposed instruction tuning approach on zero-shot performance on novel tasks. It is further interesting to see that, for the seven clusters we test, the performance does not appear to saturate, implying that performance may further improve with even more clusters added to instruction tuning. Of note, this ablation does not allow us to draw conclusions about which instruction tuning cluster contributes the most to each evaluation cluster, although we see minimal added value from the sentiment analysis cluster. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SCALING LAWS</head><p>As <ref type="bibr" target="#b11">Brown et al. (2020)</ref> shows that zero and few-shot capabilities of language models substantially improve for larger models, we next explore how the benefits of instruction tuning are affected by model scale. Using the same cluster split as in the previous ablation study, we evaluate the effect of instruction tuning on models of size 422M, 2B, 8B, 68B, and 137B parameters.</p><p>Figure <ref type="figure">6</ref> shows these results. First, as a check of correctness, we confirm in Figure <ref type="figure">6A</ref> that, for all model sizes, performance on tasks that were seen during instruction tuning improves by a large margin over the untuned model without instruction tuning, as would be highly expected. As for held-out tasks, in Figure <ref type="figure">6B</ref> we see that for the two models on the order of 100B parameters, instruction tuning substantially improves performance, as is also expected given the prior results in our paper. The behavior on held-out tasks for the 8B and smaller models, however, is thought-provoking-instruction tuning actually hurts performance on held-out tasks. One potential explanation for this result could be that for small-scale models, learning the ?40 tasks used during instruction tuning fills the entire model capacity, causing these models to perform worse on new tasks. For the larger scale models, instruction tuning fills up some model capacity but also teaches these models the ability to follow instructions, allowing them to generalize to new tasks with the remaining capacity. The effect of instruction tuning on performance on unseen tasks depends on the model scale. Whereas instruction tuning helps large models generalize to new tasks, for small models it actually hurts generalization to unseen tasks, potentially because all model capacity is used to learn the mixture of instruction tuning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">INSTRUCTION TUNING FACILITATES PROMPT TUNING</head><p>As we've seen that instruction tuning improves the ability of a model to respond to instructions, it follows that, if FLAN is indeed more amenable to performing NLP tasks, then it should also achieve better performance when performing inference using continuous prompts optimized via prompt tuning <ref type="bibr">(Li &amp; Liang, 2021;</ref><ref type="bibr" target="#b38">Lester et al., 2021)</ref>. As further analysis, we train continuous prompts for each of the SuperGLUE <ref type="bibr">(Wang et al., 2019a)</ref> tasks in accordance with the cluster splits from Section 2.2 such that when prompt-tuning on task T , no tasks in the same cluster as T were seen during instruction tuning. Our prompt tuning setup follows the procedure of <ref type="bibr" target="#b38">Lester et al. (2021)</ref> except that we use a prompt length of 10, weight decay of 1e-4, and did not use dropout on the attention scores; we found in preliminary experiments that these changes improved the performance of Base LM.</p><p>Table <ref type="table">5</ref> shows the results of these prompt tuning experiments for both using a fully-supervised training set and in a low-resource setting with only 32 training examples. We see that in all scenarios, prompt tuning works better with FLAN than Base LM. In many cases, especially for the low-resource setting, prompt tuning on FLAN even achieves more than 10% improvement over prompt tuning on the Base LM. This result exemplifies in another way how instruction tuning can result in a checkpoint that is more desirable for performing NLP tasks. Table <ref type="table">5</ref>: FLAN responds better to continuous inputs attained via prompt tuning than Base LM. When prompt tuning on a given dataset, no tasks from the same cluster as that dataset were seen during instruction tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Language models and multi-task learning. Our work is broadly inspired by a long line of prior work on language models for NLP applications <ref type="bibr" target="#b18">(Dai &amp; Le, 2015;</ref><ref type="bibr" target="#b52">Peters et al., 2018;</ref><ref type="bibr" target="#b28">Howard &amp; Ruder, 2018;</ref><ref type="bibr" target="#b55">Radford et al., 2018;</ref><ref type="bibr">2019, inter alia)</ref>. Instruction tuning can be seen as a formulation of multitask learning (MTL), which is an established area within deep learning <ref type="bibr" target="#b15">(Collobert et al., 2011;</ref><ref type="bibr" target="#b45">Luong et al., 2016;</ref><ref type="bibr" target="#b61">Ruder, 2017;</ref><ref type="bibr" target="#b68">Velay &amp; Daniel, 2018;</ref><ref type="bibr">Clark et al., 2019b;</ref><ref type="bibr">Liu et al., 2019b</ref>, inter alia)-see <ref type="bibr" target="#b73">Worsham &amp; Kalita (2020)</ref> for a recent survey on MTL for NLP. Differing from prior MTL work which focuses on performance improvements across training tasks <ref type="bibr" target="#b57">(Raffel et al., 2020;</ref><ref type="bibr" target="#b0">Aghajanyan et al., 2021)</ref> or to new domains <ref type="bibr" target="#b2">(Axelrod et al., 2011)</ref>, our work focuses on zero-shot generalization to tasks that were not seen in training.</p><p>Zero-shot learning. Our work also falls in the well-established category of zero-shot learning, which has historically been used to refer to classifying instances among a set of unseen categories <ref type="bibr" target="#b35">(Lampert et al., 2009;</ref><ref type="bibr" target="#b60">Romera-Paredes &amp; Torr, 2015;</ref><ref type="bibr">Srivastava et al., 2018;</ref><ref type="bibr">Yin et al., 2019, inter alia)</ref>. In NLP, zero-shot learning work also includes translating between unseen language pairs <ref type="bibr" target="#b29">(Johnson et al., 2017;</ref><ref type="bibr" target="#b53">Pham et al., 2019)</ref>, language modeling on unseen languages <ref type="bibr" target="#b36">(Lauscher et al., 2020)</ref>, as well as various NLP applications <ref type="bibr">(Liu et al., 2019a;</ref><ref type="bibr" target="#b16">Corazza et al., 2020;</ref><ref type="bibr" target="#b72">Wang et al., 2021;</ref><ref type="bibr" target="#b58">Reif et al., 2021)</ref>. Most recently, the emergent ability of language models <ref type="bibr" target="#b11">(Brown et al., 2020)</ref> has led to increased interest in how models generalize to unseen tasks, the definition of zero-shot learning used in our paper. In addition, meta-learning <ref type="bibr" target="#b24">(Finn et al., 2017;</ref><ref type="bibr" target="#b67">Vanschoren, 2018</ref>, inter alia) also broadly tries to train models that adapt quickly to unseen tasks.</p><p>Prompting. Instruction tuning leverages the intuition that language models at scale contain substantial world knowledge and can perform a range of NLP tasks <ref type="bibr" target="#b11">(Brown et al., 2020</ref>, see also <ref type="bibr" target="#b9">Bommasani et al. (2021)</ref>).</p><p>Another line of work that shares this goal prompts models with continuous inputs optimized via backpropagation to substantially improve performance <ref type="bibr">(Li &amp; Liang, 2021;</ref><ref type="bibr" target="#b38">Lester et al., 2021;</ref><ref type="bibr" target="#b54">Qin &amp; Eisner, 2021)</ref>. Although the success of these approaches depends heavily on model scale <ref type="bibr" target="#b38">(Lester et al., 2021)</ref>, for which large models can be costly to serve, the ability of a single large model to perform many tasks slightly eases this burden.</p><p>Multi-task question answering. The instructions we use for instruction tuning are similar to multi-task question answering work such as McCann et al. ( <ref type="formula">2018</ref>), which casts ten NLP tasks as question answering over a context and trains a model on a collection of tasks formulated with natural language prompts. They report transfer learning gains on finetuning tasks as well as zero-shot domain adaptation results on SNLI <ref type="bibr" target="#b10">(Bowman et al., 2015)</ref> and Amazon/Yelp Reviews <ref type="bibr" target="#b32">(Kotzias et al., 2015)</ref>. While <ref type="bibr" target="#b46">McCann et al. (2018)</ref> does not leverage unsupervised pre-training and only reports zero-shot transfer to unseen domains, our work uses a pretrained LM and focuses on zero-shot performance on unseen task clusters. Focusing on binary text classification, <ref type="bibr" target="#b78">Zhong et al. (2021)</ref> finetune T5-770M on 43 tasks phrased as yes/no questions and study the zero-shot performance on unseen tasks. In comparison, our paper is much larger in scope, empirically demonstrating the idea on a wide range of tasks with a much larger model.</p><p>Instructions-based NLP. The recent improvements in the capabilities of language models have led to increased interest in a nascent area of instructions-based NLP <ref type="bibr" target="#b26">(Goldwasser &amp;</ref><ref type="bibr" target="#b26">Roth, 2014, and</ref><ref type="bibr">see McCarthy (1960)</ref>).</p><p>Schick &amp; Sch?tze (2021) use task descriptions in cloze-style phrases to help language models assign soft labels for semi-supervised learning. Efrat &amp; Levy (2020) evaluated GPT-2 <ref type="bibr" target="#b56">(Radford et al., 2019)</ref> on simple tasks ranging from retrieving the nth word of a sentence to generating examples for SQuAD, concluding that GPT-2 performs poorly across all tasks. <ref type="bibr" target="#b49">Mishra et al. (2021)</ref> finetune BART <ref type="bibr" target="#b40">(Lewis et al., 2020)</ref> using instructions for tasks such as question answering, text classification, and text modification. In addition to differing in model scale and number of evaluation tasks, our work crucially differs from theirs in that we study generalization to novel tasks. Whereas <ref type="bibr" target="#b49">Mishra et al. (2021)</ref> finetune and evaluate on task distributions that are similar, the inter-cluster splits we use in this paper evaluate the ability for models to perform new tasks when no similar tasks have been seen during finetuning. <ref type="bibr" target="#b74">Ye et al. (2021)</ref> introduce a setup for cross-task few-shot learning, finding that multi-task learning improves the few-shot capabilities of BART on an unseen downstream task. Finally, OpenAI has a beta product called the Instruct Series. Though they have not released a research paper on it at this time, based on examples posted on their website,<ref type="foot" target="#foot_5">6</ref> their product appears to have goals similar to those in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Our paper has explored a simple question in zero-shot prompting: does instruction tuning a language model improve its ability to perform unseen tasks? Our experiments on FLAN demonstrate that instruction tuning improves performance against an untuned model and surpasses zero-shot GPT-3 on the majority of tasks that we evaluate on. Through ablation studies, we learn that performance on unseen tasks improves with the number of task clusters used in instruction tuning, and, interestingly, that the benefits of instruction tuning emerge only with sufficient model scale. Moreover, our FLAN appears to respond better to prompt tuning than the unmodified base model, demonstrating an additional benefit of instruction tuning.</p><p>One limitation in our study is that there is a certain degree of subjectivity in grouping tasks into clusters (for instance, sentiment analysis can be seen as a small subset of reading comprehension), as there is no accepted method for operationalizing the similarity between two tasks. Hence, we assigned tasks to clusters based on accepted categorizations in the literature, taking the conservative approach when tasks seemed to fall into multiple clusters (such as excluding reading comprehension with commonsense from instruction tuning when evaluating both reading comprehension and commonsense reasoning). As another limitation, we used short instructions (typically a single sentence) to describe well-known NLP tasks. Other tasks may require much longer or more specific instructions to describe adequately, along with explanations involving examples; we leave these scenarios for future work.</p><p>The results shown in this paper suggest several promising directions for future research. Although FLAN is instruction-tuned on over sixty datasets, these datasets only cover ten task clusters (plus some miscellaneous tasks), a relatively small number considering all potential tasks such a model could be used for. It is possible that performance can further improve with many more instruction tuning tasks, which could be generated in a self-supervised fashion, for instance. In addition to gathering more tasks, it will also be valuable to explore the multilingual setting, where one could ask, for instance, whether instruction tuning on supervised data in high-resource languages would improve performance on novel tasks in low-resource languages. Finally, instruction tuning models with supervised data could also potentially be used to improve model behavior with respect to bias and fairness <ref type="bibr" target="#b66">(Solaiman &amp; Dennison, 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>This paper has explored instruction tuning. We presented FLAN, a 137B parameter language model that performs NLP tasks described using instructions. By leveraging supervision from finetuning to improve the ability of language models to respond to instructional prompts, FLAN combines appealing aspects of both the pretrain-finetune and prompting paradigms. The performance of FLAN compares favorably against both zero-shot and few-shot GPT-3, signaling the potential ability for models at scale to follow instructions. We hope that our paper will spur further research on zero-shot learning and using labeled data to improve language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICAL CONSIDERATIONS</head><p>This work uses language models, for which the risks and potential harms are discussed in <ref type="bibr" target="#b3">Bender &amp; Koller (2020)</ref>, <ref type="bibr" target="#b11">Brown et al. (2020)</ref>, <ref type="bibr" target="#b4">Bender et al. (2021</ref><ref type="bibr">), Patterson et al., (2021)</ref>, and others. As our contribution in this paper is not a pretrained language model itself but rather an empirical study of how instruction tuning affects the zero-shot performance of a language model on unseen tasks, we additionally highlight several relevant ethical considerations. First, labeled datasets such as those we use for finetuning can contain undesirable biases, and these biases can be propagated into zero-shot applications of the model on downstream tasks. Next, instruction-tuned models can potentially require less data and expertise to use; such lower barriers to access could increase both the benefits and associated risks of such models. Finally, we want to be clear that, for most datasets, supervised models such as BERT and T5 still outperform the zero-shot instructions-only model in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ENVIRONMENTAL CONSIDERATIONS</head><p>We use the same pretrained language models as <ref type="bibr" target="#b1">Austin et al. (2021)</ref>. The energy cost and carbon footprint for those pretrained models were 451 MWh and 26 tCO2e, respectively. The additional instruction tuning gradient-steps for finetuning flan is less than 2% of the number of pretraining steps, and so the estimated additional energy cost is comparatively smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUTHOR CONTRIBUTIONS</head><p>Maarten Bosma conceived the original idea and implemented the first version of FLAN. Vincent Zhao prototyped the training and evaluation pipelines, as well as rank classification. Kelvin Guu proposed and implemented the idea of task clusters and evaluation using inter-cluster splits. Jason Wei, Maarten Bosma, Vincent Zhao, and Adams Wei Yu implemented the NLP tasks. Jason Wei, Vincent Zhao, and Adams Wei Yu conducted and managed the experiments. Jason Wei designed and ran the ablation studies. Jason Wei, Maarten Bosma, and Quoc V. Le wrote most of the paper. Jason Wei, Maarten Bosma, and Nan Du obtained the zero and few-shot baselines. Brian Lester ran the prompt tuning experiments. Quoc V. Le and Andrew M. Dai advised, provided high-level guidance and helped edit the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A FURTHER ABLATION STUDIES</head><p>A.1 TASKS PER CLUSTER &amp; TEMPLATES PER TASK Our primary hypothesis is that that instruction tuning on a diverse set of tasks improves performance on unseen tasks. Section 4.1 showed that adding more clusters improves performance; here, we further explore whether adding additional tasks improves performance when the number of clusters is held constant. We use the same split as in Section 4.1, where the NLI, commonsense reasoning, and open-domain QA clusters are held-out, and seven other clusters remain for instruction tuning. For these seven clusters, we instruction-tune models using just one task per cluster and using four tasks per cluster (for clusters that did not have four tasks, we just used all available tasks). In addition, we simultaneously explore the role of the number of instruction templates per task; as mentioned in Section 2.1, for each task we manually composed ten instructional templates for instruction tuning. Here, we instruction-tune models using 1, 4, and 10 templates per task.</p><p>Section 7 shows these results. Using more tasks per cluster improved performance by almost 10% on average across the three held-out clusters. Using more templates per task, however, had a comparatively negligible effect on performance when there was one task per cluster, which disappeared when there were four tasks per cluster. The small effect of templates is striking given our original motivation that composing ten templates per task would mitigate overfitting to any particular template. This results serves to underscore, however, the unpredictability of finetuning large language models, as one hypothesis is that models at such scale do not easily overfit to a single task during finetuning. Using more templates per task, however, only had a very small effect on performance, which disappeared when there were sufficient tasks per cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B APPENDIX: TASK DETAILS</head><p>This appendix provides the details of each dataset that we use in this paper. We group datasets into one of the following clusters:</p><p>? Natural language inference (7 datasets) concerns how two sentences relate, typically asking, given a first sentence, whether a second sentence is true, false, or possibly true. ? Reading comprehension (7 datasets) tests the ability to answer a question when given a passage that contains the answer. ? Commonsense reasoning (4 datasets) evaluates the ability to perform physical or scientific reasoning with an element of common sense. ? Sentiment analysis (4 datasets) is a classic NLP task aims to understand whether a piece of text is positive or negative. ? Open-domain QA (3 datasets) asks models to answer questions about the world without specific access to information that contains the answer. ? Paraphrase detection (3 datasets) asks a model to determine whether two sentences are semantically equivalent.<ref type="foot" target="#foot_6">7</ref> ? Coreference resolution (3 datasets) tests the ability to identify expressions of the same entity in some given text. ? Reading comprehension with commonsense (2 datasets) combines elements of both reading comprehension with commonsense. ? Data to text (4 datasets) tests the ability to describe some structured data using natural language.</p><p>? Summarization (11 datasets) asks models to read a piece of text and generate an abbreviated summary of it. ? Translation (6 languages) is the task of translating text from one language into a different language.</p><p>For all tasks, our finetuning and evaluation code uses tensorflow datasets (TFDS) to load and process datasets. Regarding the number of training examples per dataset, we limited the training set size per dataset to 30,000 so that no dataset dominated the finetuning distribution. When a test set with labels was available in TFDS, we used it; otherwise, we used the TFDS validation set as our test set, splitting the training set into a train and dev set. On the following pages, we show example inputs and outputs for all evaluation tasks. See our code for the templates for tasks that were used for instruction-tuning but not for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 NATURAL LANGUAGE INFERENCE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT</head><p>Joey Heindle (born 14 May 1993 in Munich) is a German singer. He is best known for winning the seventh season of the game show Ich bin ein Star -Holt mich hier raus! and finishing in 5th place in season 9 of Deutschland sucht den Superstar, despite universally negative reviews from the jury each week.</p><p>Based on the paragraph above can we conclude that "Joey Heindle was highly disliked by people on television."?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>OPTIONS:</head><p>-Yes -It's impossible to say -No TARGET Yes Table <ref type="table">6</ref>: Example input and target for Adversarial NLI (ANLI). ANLI <ref type="bibr" target="#b51">(Nie et al., 2020</ref>) is a large-scale NLI benchmark with adversarial examples collected iteratively with a human and model in the loop. The task is to determine whether a hypothesis is entailed by a premise (entailment, not entailment, or impossible to say). There are three rounds, R1-R3. Of the three training sets with <ref type="bibr">16,946, 45,460, and 100,459 examples, we use 16,946, 30,000, and 30</ref>,000 for train and 200 from each of the three TFDS validation sets for dev. We use the TFDS "test" sets of 1,000, 1,000, and 1,200 examples as our test set for reporting numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT</head><p>A: so I watch the fish, you know. Whatever I can do to keep myself occupied. I like to have the TV on, because that usually keeps me, um, more occupied. It kind of takes the time away and I don't realize, that's really the only time I ever watch TV, is when I'm on the bike. and then usually after I'm done riding the bike, just to cool myself down, I usually take a walk, you know, and that just kind of uh, gets me, you know, to where I'm not quite as tired I guess. But it's definitely a task. B: You think so? A: I can't say that I really enjoy it.</p><p>Based on the paragraph above can we conclude that "she really enjoys it"? OPTIONS: -Yes -No -It's impossible to say </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT</head><p>After years of study, the Vatican's doctrinal congregation has sent church leaders a confidential document concluding that "sex-change" procedures do not change a person's gender in the eyes of the church.</p><p>Based on the paragraph above can we conclude that "Sex-change operations become more common."? OPTIONS: -yes -no TARGET no Table <ref type="table">8</ref>: Example input and target for Recognizing Textual Entailment (RTE). RTE <ref type="bibr" target="#b17">(Dagan et al., 2005;</ref><ref type="bibr" target="#b27">Haim et al., 2006;</ref><ref type="bibr" target="#b25">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b5">Bentivogli et al., 2009)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT</head><p>Imagine you are standing in a farm field in central Illinois. The land is so flat you can see for miles and miles. On a clear day, you might see a grain silo 20 miles away. You might think to yourself, it sure is flat around here. If you drive one hundred miles to the south, the landscape changes. In southern Illinois, there are rolling hills. Why do you think this is? What could have caused these features? There are no big rivers that may have eroded and deposited this material. The ground is capable of supporting grass and trees, so wind erosion would not explain it. To answer the question, you need to go back 12,000 years. Around 12,000 years ago, a giant ice sheet covered much of the Midwest United States. Springfield, Illinois, was covered by over a mile of ice. Its hard to imagine a mile thick sheet of ice. The massive ice sheet, called a glacier, caused the features on the land you see today. Where did glaciers go? Where can you see them today? Glaciers are masses of flowing ice.</p><p>Question: "How big were the glaciers?" Response: "One mile" Does the response correctly answer the question? OPTIONS: -no -yes TARGET yes Once the rope is inside the hook, he begins moving up the wall but shortly after he stops and begins talking. The male then begins talking about the clip again and goes back up the wall. as he OPTIONS:</p><p>-progresses, there are hooks everywhere on the wall and when he gets near them, he puts his rope inside of it for support and safety.</p><p>-changes time, an instant replay of his initial move is shown a second time.</p><p>-continues to talk, another male speaks about the move and shows another closeup of the plex by the male.</p><p>-continues, other people start to arrive and begin to hang out with him as he makes a few parts of the rope.</p><p>TARGET progresses, there are hooks everywhere on the wall and when he gets near them, he puts his rope inside of it for support and safety.</p><p>Table <ref type="table">13</ref>: Example input and target for Commonsense Sentence Completion (HellaSwag). HellaSwag <ref type="bibr" target="#b76">(Zellers et al., 2019)</ref> tests for sentence completion that requires common sense, asking for the most probable ending given four contexts. Of the training set with 39,905 examples, we use 30,000 for train and 200 for dev. We use the TFDS validation set of 10,042 examples as our test set for reporting numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT</head><p>Here is a goal: Remove smell from garbage disposal.</p><p>How would you accomplish this goal? OPTIONS: -Create soda ice cubes and grind through disposal.</p><p>-Create vinegar ice cubes and grind through disposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TARGET</head><p>Create vinegar ice cubes and grind through disposal.</p><p>Table <ref type="table">14</ref>: Example input and target for Physical Question Answering (PiQA). PiQA <ref type="bibr" target="#b6">(Bisk et al., 2020)</ref>   <ref type="table">16</ref>: Example input and target for The AI2 Reasoning Challenge (ARC). ARC <ref type="bibr" target="#b14">(Clark et al., 2018)</ref> asks grade-school level 4-way multiple choice science questions. There is a challenge set and an easy set, where the challenge set questions were answered incorrectly by both a retrieval-based algorithm and a co-occurrence algorithm. Of the training sets with 1,119 examples (challenge) and 2,251 (easy), we use we use 919 and 2,051 respectively for train and 200 each for dev. We use the TFDS test sets of 1,172 and 2,376 examples respectively as our test set for reporting numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT</head><p>Question: who is the girl in more than you know?? Answer: TARGET Romi Van Renterghem.</p><p>Table <ref type="table">17</ref>: Example input and target for Natural Questions (Open) (NQ). NQ <ref type="bibr" target="#b37">(Lee et al., 2019;</ref><ref type="bibr" target="#b34">Kwiatkowski et al., 2019)</ref> asks for an open-ended answer given a question, where all questions can be answered using the contents of Wikipedia. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing instruction tuning with pretrain-finetune and prompting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Task clusters used in this paper (NLU tasks in blue; NLG tasks in teal).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Multiple instruction templates describing a natural language inference task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>c</head><label></label><figDesc>Edunov et al. (2018), d Durrani et al. (2014), e Wang et al. (2019b), f Sennrich et al. (2016), g Liu et al. (2020). The triangle indicates improvement over few-shot GPT-3. The up-arrow ? indicates improvement only over zero-shot GPT-3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Adding additional task clusters to instruction tuning improves zero-shot performance on held-out task clusters. The evaluation tasks are the following. Commonsense: CoPA, HellaSwag, PiQA, and StoryCloze. NLI: ANLI R1-R3, QNLI, RTE, SNLI, and WNLI. Open-domain QA: ARC easy, ARC challenge, Natural Questions, and TriviaQA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>A</head><label></label><figDesc>Figure 6: (A) Performance on tasks seen during instruction tuning improves for all model sizes. (B)The effect of instruction tuning on performance on unseen tasks depends on the model scale. Whereas instruction tuning helps large models generalize to new tasks, for small models it actually hurts generalization to unseen tasks, potentially because all model capacity is used to learn the mixture of instruction tuning tasks.</figDesc><graphic url="image-3.png" coords="8,132.57,548.68,126.39,109.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Effect of tasks per and templates per task on performance on three held-out clusters: NLI, commonsense reasoning, and open-domain QA. Adding more tasks per cluster substantially improves performance.Using more templates per task, however, only had a very small effect on performance, which disappeared when there were sufficient tasks per cluster.</figDesc><graphic url="image-6.png" coords="19,216.92,320.48,175.68,163.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>is a commonsense QA benchmark for naive physics reasoning, where a solution to a goal must be selected from two choices. Of the training set with 16,113 examples, we use 16,013 for train and 100 for dev. We use the TFDS validation set of 1,838 examples as our test set for reporting numbers. INPUT Caroline never drinks carbonated beverages. Her friends pick on her because of it. One day they challenged her to drink a soda. Caroline wanted to win the challenge. Predict the next sentence. OPTIONS: -Caroline refused to open the soda. -Caroline opened the soda and drank it all in one gulp! TARGET Caroline opened the soda and drank it all in one gulp!Table 15: Example input and target for The Story Cloze Test (StoryCloze). StoryCloze (Mostafazadeh et al., 2016) is a commonsense reasoning framework for story generation, where a system chooses the correct ending to a four-sentence story. We use the 2016 version on TFDS. Of the set with 1,871 examples (no training set is available), we use 1,671 for train and 200 for dev. We use the TFDS test set of 1,871 examples as our test set for reporting numbers. B.4 OPEN-DOMAIN QA INPUT What season is the Northern Hemisphere experiencing when it is tilted directly toward the Sun?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Of the training set of 87,925 examples, we use 30,000 for train and 200 for dev. We use the TFDS validation set of 3,610 examples as our test set for reporting numbers. INPUT Please answer this question: Henry Croft, an orphan street sweeper who collected money for charity, is associated with what organised charitable tradition of working class culture in London, England? TARGET pearly kings and queens Table 18: Example input and target for Trivia Question Answering (TriviaQA). TriviaQA Joshi et al. (2017) includes question-answer pairs authored by trivia enthusiasts. Of the training set of 87,622 examples, we use 30,000 for train and 200 for dev. We use the TFDS validation set of 11,313 examples as our test set for reporting numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TARGET No Table 7 :</head><label>No7</label><figDesc>Example input and target for Commitment Bank (CB). CB<ref type="bibr" target="#b19">(de Marneffe et al., 2019)</ref> is a corpus of texts in which a hypothesis is extracted from a premise, and the task is to determine whether the hypothesis is entailed by the premise (entailment, not entailment, or impossible to say). Of the training set with 250 examples, we use 200 for train and 50 for dev. We use the TFDS validation set of 56 examples as our test set for reporting numbers.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 :</head><label>9</label><figDesc>asks whether a second sentence is entailed by a first (binary, either entailed or not entailed). Of the training set with 2490 examples, we use 2,290 for train and 200 for dev. We use the TFDS validation set of 277 examples as our test set for reporting numbers.B.2 READING COMPREHENSIONINPUTThere are four ways an individual can acquire Canadian citizenship: by birth on Canadian soil; by descent (being born to a Canadian parent); by grant (naturalization); and by adoption. Among them, only citizenship by birth is granted automatically with limited exceptions, while citizenship by descent or adoption is acquired automatically if the specified conditions have been met. Citizenship by grant, on the other hand, must be approved by the Minister of Immigration, Refugees and Citizenship. Example input and target for Boolean Questions (BoolQ). BoolQClark et al. (2019a)  asks a yes/no question based on a passage and a question. Of the training set with 9,427 examples, we use 9,227 for train and 200 for dev. We use the TFDS validation set of 3,270 examples as our test set for reporting numbers.</figDesc><table><row><cell>Can we conclude that can i get canadian citizenship if my grandfather was canadian?</cell></row><row><cell>OPTIONS:</cell></row><row><cell>-no</cell></row><row><cell>-yes</cell></row><row><cell>TARGET</cell></row><row><cell>no</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 :</head><label>10</label><figDesc>Example input and target for Multi-Sentence Reading Comprehension (MultiRC). MultiRC Khashabi et al. (2018) asks an open-ended question given a paragraph that contains the answer. Of the training set with 27,243 examples, we use 27,043 for train and 200 for dev. We use the TFDS validation set of 4,848 examples as our test set for reporting numbers.</figDesc><table><row><cell>INPUT</cell></row><row><cell>soil is a renewable resource for growing plants</cell></row><row><cell>A plant that needs to expand will be able to have an endless resource in</cell></row><row><cell>OPTIONS:</cell></row><row><cell>-dirt</cell></row><row><cell>-pesticides</cell></row><row><cell>-pay</cell></row><row><cell>-beans</cell></row><row><cell>TARGET</cell></row><row><cell>dirt</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11 :</head><label>11</label><figDesc>Example input and target for Openbook Question Answering (OBQA). OBQA<ref type="bibr" target="#b48">(Mihaylov et al., 2018)</ref> asks 4-way multiple choice questions based facts. Of the training set with 4,957 examples, we use all for train and 200 in the TFDS validation set of 500 examples for dev. We use the TFDS test set of 500 examples as our test set for reporting numbers.</figDesc><table><row><cell>B.3 COMMONSENSE REASONING</cell></row><row><cell>INPUT</cell></row><row><cell>I packed up my belongings. What is the cause?</cell></row><row><cell>OPTIONS:</cell></row><row><cell>-I was hunting for a new apartment.</cell></row><row><cell>-I was moving out of my apartment.</cell></row><row><cell>TARGET</cell></row><row><cell>I was moving out of my apartment.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12 :</head><label>12</label><figDesc>Example input and target for Choice of Plausible Alternatives (COPA). COPA<ref type="bibr" target="#b59">(Roemmele et al., 2011)</ref> is a causal reasoning task that asks to infer either a cause of effect of a premise from two choices. Of the training set with 400 examples, we use 350 for train and 50 for dev. We use the TFDS validation set of 100 examples as our test set for reporting numbers.</figDesc><table><row><cell>INPUT</cell></row><row><cell>What happens next in this paragraph?</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.tensorflow.org/datasets</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>When evaluating on the reading comprehension with commonsense cluster, both reading comprehension and commonsense reasoning were dropped from instruction tuning. Conversely, the reading comprehension with commonsense cluster was not used for instruction tuning when evaluating on reading comprehension or commonsense reasoning. We also drop the paraphrase cluster from instruction tuning when evaluating on NLI tasks and vice-versa.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>  3  In this mixing scheme, a mixing rate maximum of 3,000 means that a dataset does not receive additional sampling weight for examples in excess of 3,000. In other words, a dataset i with ni examples would have sampling frequency wi = min(ni, 3000)/ j?D min(nj, 3000), where D is the set of all datasets.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Unlike GPT-3, we do not choose the number of few-shot examples by searching over a dev set. Instead, we either used ten-shot results or the maximum number of exemplars when ten exemplars did not fit into our context length of 1024 tokens.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We do not use the paraphrase or reading comprehension with commonsense clusters for instruction tuning in this ablation because they are too similar to NLI and commmonsense reasoning, respectively.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://beta.openai.com/docs/engines/instruct-series-beta</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Although paraphrasing can be seen as positive entailment in both directions, it has been distinct from NLI in the academic literature.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>We thank <rs type="person">Ed Chi</rs>, <rs type="person">Slav Petrov</rs>, <rs type="person">Dan Garrette</rs>, and <rs type="person">Ruibo Liu</rs> for providing feedback on our manuscript. We thank <rs type="person">Adam Roberts</rs>, <rs type="person">Liam Fedus</rs>, <rs type="person">Hyung Won Chung</rs>, and <rs type="person">Noam Shazeer</rs> for helping debug some of our models. We thank <rs type="person">Ellie Pavlick</rs> for feedback on the study design during the middle stages of the project. We thank <rs type="person">Daniel De Freitas Adiwardana</rs> for helping initiate the project, large language model advising, and giving us access to some computational resources. Finally, we thank the team involved in pretraining of the Base LM model: <rs type="person">De Freitas Adiwardana</rs>, <rs type="person">Noam Shazeer</rs>, <rs type="person">Yanping Huang</rs>, <rs type="person">Dmitry Lepikhin</rs>, <rs type="person">Dehao Chen</rs>, <rs type="person">Yuanzhong Xu</rs> and <rs type="person">Zhifeng Chen</rs>.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 COREFERENCE RESOLUTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT</head><p>How does the sentence end? Elena wanted to move out of her parents fast but Victoria wanted to stay for a while, OPTIONS: -Elena went to school.</p><p>-Victoria went to school.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TARGET</head><p>Victoria went to school.</p><p>Table <ref type="table">19</ref>: Example input and target for Adversarial Winograd Schema Challenge (Winogrande). Winogrande <ref type="bibr" target="#b62">(Sakaguchi et al., 2020)</ref> tests for coreference resolution by asking a model to fill in a masked token in a sentence by choosing an entity from two options. Of the 40.4k examples in the XL training set, we use 30,000 for train and 200 for dev. We use the TFDS validation set of 1,267 as our test set for reporting numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT</head><p>Jane knocked on Susan's door, but there was no answer. OPTIONS: -Jane was out.</p><p>-Susan was out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TARGET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Susan was out.</head><p>Table <ref type="table">20</ref>: Example input and target for Winograd Schema Challenge (WSC273). WSC273 <ref type="bibr" target="#b39">(Levesque et al., 2012)</ref> tests for coreference resolution by asking a model to complete the sentence in a fashion that requires understanding the entities in the sentence. Of the 0 examples in the training set (WSC273 is test-set only), we use none for train and none for dev. We use the TFDS test set as our test set for reporting numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 READING COMPREHENSION WITH COMMONSENSE</head><p>INPUT Complete the passage.</p><p>(CNN) -At first glance, "The Flat" might seem like an episode of "Hoarders," Israeli-style. The documentary film opens after an elderly woman dies in Tel Aviv. Her grandchildren assemble to clean out her apartment, packed with dusty books, vintage clothing (dozens of pairs of fancy gloves, for instance), enough purses to stock a department store, jewelry, mementoes and closets full of knickknacks. But buried among the detritus they chance upon something remarkable -mysterious papers linking the grandparents to an important Nazi figure. How could such ardent Zionists, who left their native Germany in the early 1930s, have been involved with an SS official like Leopold von Mildenstein? What I found out was this journey, the Nazi ( OPTIONS: -Arnon Goldfinger) and his wife were accompanied by my grandparents," Goldfinger told CNN.</p><p>-CNN) and his wife were accompanied by my grandparents," Goldfinger told CNN.</p><p>-Germany) and his wife were accompanied by my grandparents," Goldfinger told CNN.</p><p>-Israeli) and his wife were accompanied by my grandparents," Goldfinger told CNN.</p><p>-Leopold von Mildenstein) and his wife were accompanied by my grandparents," Goldfinger told CNN.</p><p>-Nazi) and his wife were accompanied by my grandparents," Goldfinger told CNN.</p><p>-SS) and his wife were accompanied by my grandparents," Goldfinger told CNN.</p><p>-Tel Aviv) and his wife were accompanied by my grandparents," Goldfinger told CNN.</p><p>-The Flat) and his wife were accompanied by my grandparents," Goldfinger told CNN.</p><p>-Zionists) and his wife were accompanied by my grandparents," Goldfinger told CNN.</p><p>TARGET Leopold von Mildenstein) and his wife were accompanied by my grandparents," Goldfinger told CNN. ReCoRD <ref type="bibr" target="#b77">(Zhang et al., 2018)</ref> asks for the answer to a cloze-style question where an entity is masked out. Of the the training set of 100,730 examples, we use 30,000 for train and 200 for dev. We use the TFDS validation set of 10,000 examples as our test set for reporting numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 TRANSLATION (7 LANGUAGES)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPUT</head><p>Here the largest town of the district is located: Nordenham , lying opposite to Bremerhaven at the Weser mouth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Translate to German</head><p>TARGET An der B 211 befindet sich in Loyermoor der so genannte "Geest-Abbruch", der eine H?hendifferenz von gut 30 Meter ?berbr?ckt.</p><p>Table <ref type="table">22</ref>: Example input and output for translation. This example is from WMT'16 English-German; all languages use the same translation templates.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Muppet: Massive multi-task representations with pre-finetuning</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2101.11038" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Program synthesis with large language models</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carrie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2108.07732" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation via pseudo in-domain data selection</title>
		<author>
			<persName><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D11-1033" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-07">July 2011</date>
			<biblScope unit="page" from="355" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Climbing towards NLU: On meaning, form, and understanding in the age of data</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.463</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.463" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="5185" to="5198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445922</idno>
		<ptr target="https://doi.org/10.1145/3442188.3445922" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The Fifth PASCAL Recognizing Textual Entailment Challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<ptr target="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.232" />
	</analytic>
	<monogr>
		<title level="m">TAC</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1231" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PIQA: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1911.11641" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-33</idno>
		<ptr target="https://aclanthology.org/W14-3300" />
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<editor>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</editor>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Guillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Jimeno Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur?lie</forename><surname>N?v?ol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Pecina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-2200</idno>
		<ptr target="https://aclanthology.org/W16-2200" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<editor>
			<persName><forename type="first">J?rg</forename><surname>Tiedemann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</editor>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niladri</forename><forename type="middle">S</forename><surname>Castellon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annie</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Creel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dora</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Demszky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moussa</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Doumbouya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Etchemendy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ethayarajh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><forename type="middle">E</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Gillespie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saahil</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyusha</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fereshte</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Khani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohith</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Kuditipudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mina</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Levent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suvir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mirchandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zanele</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Munyikwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avanika</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Nilforoshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giray</forename><surname>Nyarko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurel</forename><surname>Ogut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Christopher R'e</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sadigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armin</forename><forename type="middle">W</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><forename type="middle">E</forename><surname>Tram?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitlyn</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<editor>Isabel Papadimitriou, Joon Sung Park, C. Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Robert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz,</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
		<ptr target="https://aclanthology.org/D15-1075" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09">September 2015</date>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1457" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>c0d6bfcb4967418bfb8ac142f64a-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BoolQ: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1300</idno>
		<ptr target="https://aclanthology.org/N19-1300" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2924" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BAM! bornagain multi-task networks for natural language understanding</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1595</idno>
		<ptr target="https://aclanthology.org/P19-1595" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="5931" to="5937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? Try ARC, the AI2 reasoning challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1803.05457" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
		<ptr target="https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hybrid emoji-based masked language models for zero-shot abusive language detection</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Corazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.84</idno>
		<ptr target="https://aclanthology.org/2020.findings-emnlp.84" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="943" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The PASCAL Recognising Textual Entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<idno type="DOI">10.1007/11736790_9</idno>
		<ptr target="https://doi.org/10.1007/11736790_9" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW&apos;05</title>
		<meeting>the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty Visual Object Classification, and Recognizing Textual Entailment, MLCW&apos;05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<ptr target="https://papers.nips.cc/paper/2015/file/7137" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Neural Information Processing Systems</title>
		<meeting>the Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>debd45ae4d0ab9aa953017286b20-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The CommitmentBank: Investigating projection in naturally occurring discourse</title>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Tonhauser</surname></persName>
		</author>
		<idno type="DOI">10.18148/sub/2019.v23i2.601</idno>
		<ptr target="https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Sinn und Bedeutung</title>
		<meeting>Sinn und Bedeutung</meeting>
		<imprint>
			<date type="published" when="2019-07">Jul. 2019</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="107" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://aclanthology.org/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Edinburgh&apos;s phrase-based machine translation systems for WMT-14</title>
		<author>
			<persName><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<idno>doi: 10.3115</idno>
		<ptr target="https://aclanthology.org/W14-3309" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06">June 2014</date>
			<biblScope unit="page" from="14" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding back-translation at scale</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1045</idno>
		<ptr target="https://aclanthology.org/D18-1045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Turking Test: Can language models understand instructions?</title>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.11982" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.03400" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The third PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/W07-1401" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007-06">June 2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning from natural instructions</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-013-5407-y</idno>
		<ptr target="https://link.springer.com/article/10.1007/s10994-013-5407-y" />
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="205" to="232" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Second PASCAL Recognising Textual Entailment Challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Bar Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><surname>Szpektor</surname></persName>
		</author>
		<ptr target="http://www.cs.biu.ac.il/~szpekti/papers/RTE2-organizers.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
		<ptr target="https://aclanthology.org/P18-1031" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00065</idno>
		<ptr target="https://aclanthology.org/Q17-1024" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1147</idno>
		<ptr target="https://aclanthology.org/P17-1147" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1023</idno>
		<ptr target="https://aclanthology.org/N18-1023" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From group to individual labels using deep features</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Kotzias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<idno type="DOI">10.1145/2783258.2783380</idno>
		<ptr target="https://dl.acm.org/doi/10.1145/2783258.2783380" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-2012</idno>
		<ptr target="https://doi.org/10.18653/v1/d18-2012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<editor>
			<persName><forename type="first">Wei</forename><surname>Blanco</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lu</surname></persName>
		</editor>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11-04">October 31 -November 4, 2018. 2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
	<note>EMNLP 2018: System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Natural Questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00276</idno>
		<ptr target="https://aclanthology.org/Q19-1026" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="452" to="466" />
			<date type="published" when="2019-03">March 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName><forename type="first">Hannes</forename><surname>Christoph H Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinit</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glava?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.363</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.363" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="4483" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1612</idno>
		<ptr target="https://aclanthology.org/P19-1612" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.08691" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Winograd Schema Challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
		<idno type="DOI">10.5555/3031843.3031909</idno>
		<ptr target="https://dl.acm.org/doi/10.5555/3031843.3031909" />
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.703" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.353</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.353" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Reconstructing capsule networks for zero-shot intent classification</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuandi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><forename type="middle">Y S</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1486</idno>
		<ptr target="https://aclanthology.org/D19-1486" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="4799" to="4809" />
		</imprint>
	</monogr>
	<note>a. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1441</idno>
		<ptr target="https://aclanthology.org/P19-1441" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00343</idno>
		<ptr target="https://aclanthology.org/2020.tacl-1.47" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-task sequence to sequence learning</title>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Kaiser</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1511.06114" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1806.08730" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Programs with common sense. RLE and MIT computation center</title>
		<author>
			<persName><forename type="first">John</forename><surname>Mccarthy</surname></persName>
		</author>
		<ptr target="http://jmc.stanford.edu/articles/mcc59/mcc59.pdf" />
		<imprint>
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1260</idno>
		<ptr target="https://aclanthology.org/D18-1260" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="2381" to="2391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08773</idno>
		<ptr target="https://arxiv.org/abs/2104.08773" />
		<title level="m">Natural Instructions: Benchmarking generalization to new tasks from natural language instructions</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1098</idno>
		<ptr target="https://aclanthology.org/N16-1098" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter</title>
		<meeting>the 2016 Conference of the North American Chapter<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adversarial NLI: A new benchmark for natural language understanding</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.441</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.441" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="4885" to="4901" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
		<ptr target="https://aclanthology.org/N18-1202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improving zero-shot translation with language-independent constraints</title>
		<author>
			<persName><forename type="first">Ngoc-Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5202</idno>
		<ptr target="https://aclanthology.org/W19-5202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08">August 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
	<note>Research Papers)</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning how to ask: Querying LMs with mixtures of soft prompts</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<ptr target="http://cs.jhu.edu/~jason/papers/#qin-eisner-2021" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</meeting>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://blog.openai.com/language-unsupervised" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A recipe for arbitrary text style transfer with large language models</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach to zero-shot learning</title>
		<author>
			<persName><forename type="first">Bernardino</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2152" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.05098" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">WinoGrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1907.10641" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8732" to="8740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.eacl-main.20" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04">April 2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Edinburgh neural machine translation systems for WMT 16</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-2323</idno>
		<ptr target="https://aclanthology.org/W16-2323" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
	<note>Shared Task Papers</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1804.04235" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4596" to="4604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Zero-shot learning of classifiers from natural language quantification</title>
		<author>
			<persName><forename type="first">Irene</forename><surname>Solaiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christy</forename><surname>Dennison</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1029</idno>
		<idno type="arXiv">arXiv:2106.10328</idno>
		<ptr target="https://aclanthology.org/P18-1029" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</editor>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2021. July 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="306" to="316" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Process for adapting language models to society (palms) with valuestargeted datasets</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Meta-learning: A survey</title>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1810.03548" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Seq2seq and multi-task learning for joint intent and content extraction for domain specific interpreters</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Velay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Daniel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1808.00423" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
		<ptr target="https://aclanthology.org/W18-5446" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">November 2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00537</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multi-agent dual learning</title>
		<author>
			<persName><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HyGhN2A5tm" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Towards zero-label language learning</title>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Multi-task learning for natural language processing in the 2020s: where are we going?</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Worsham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kalita</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.16008" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Crossfit: A few-shot learning challenge for cross-task generalization in nlp</title>
		<author>
			<persName><forename type="first">Qinyuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.08835" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach</title>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamaal</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1404</idno>
		<ptr target="https://aclanthology.org/D19-1404" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="3914" to="3923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">HellaSwag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1472</idno>
		<ptr target="https://aclanthology.org/P19-1472" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Record: Bridging the gap between human and machine commonsense reading comprehension</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno>CoRR, abs/1810.12885</idno>
		<ptr target="http://arxiv.org/abs/1810.12885" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04670</idno>
		<ptr target="https://arxiv.org/abs/2104.04670" />
		<title level="m">Meta-tuning language models to answer prompts better</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
