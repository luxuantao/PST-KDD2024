<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Reconstruction of Face Images from Deep Face Templates</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guangcan</forename><surname>Mai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering at</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>Michigan</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering at</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>Michigan</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering at</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>Michigan</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Anil</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering at</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>Michigan</region>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering at</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<settlement>East Lansing</settlement>
									<region>Michigan</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Reconstruction of Face Images from Deep Face Templates</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4D862F0C5F222DAD28C640A8FABF9A8E</idno>
					<idno type="DOI">10.1109/TPAMI.2018.2827389</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2018.2827389, IEEE Transactions on Pattern Analysis and Machine Intelligence MAI et al.: ON THE RECONSTRUCTION OF FACE IMAGES FROM DEEP FACE TEMPLATES 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face recognition</term>
					<term>template security</term>
					<term>deep networks</term>
					<term>deep templates</term>
					<term>template reconstruction</term>
					<term>neighborly de-convolutional neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art face recognition systems are based on deep (convolutional) neural networks. Therefore, it is imperative to determine to what extent face templates derived from deep networks can be inverted to obtain the original face image. In this paper, we study the vulnerabilities of a state-of-the-art face recognition system based on template reconstruction attack. We propose a neighborly de-convolutional neural network (NbNet) to reconstruct face images from their deep templates. In our experiments, we assumed that no knowledge about the target subject and the deep network are available. To train the NbNet reconstruction models, we augmented two benchmark face datasets (VGG-Face and Multi-PIE) with a large collection of images synthesized using a face generator. The proposed reconstruction was evaluated using type-I (comparing the reconstructed images against the original face images used to generate the deep template) and type-II (comparing the reconstructed images against a different face image of the same subject) attacks. Given the images reconstructed from NbNets, we show that for verification, we achieve TAR of 95.20% (58.05%) on LFW under type-I (type-II) attacks @ FAR of 0.1%. Besides, 96.58% (92.84%) of the images reconstructed from templates of partition fa (fb) can be identified from partition fa in color FERET. Our study demonstrates the need to secure deep templates in face recognition systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>F ACE recognition systems are being increasingly used for secure access in applications ranging from personal devices (e.g., iPhone X 1 and Samsung S8 2 ) to access control (e.g., banking 3 and border control 4 ). In critical applications, face recognition needs to meet stringent performance requirements, including low error rates and strong system security. In particular, the face recognition system must be resistant to spoofing (presentation) attacks and template invertivility. Therefore, it is critical to evaluate the vulnerabilities of a face recognition system to these attacks and devise necessary countermeasures. To this end, several attack mechanisms (such as hill climbing <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, spoofing <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and template reconstruction (template invertibility) <ref type="bibr" target="#b8">[9]</ref>) have been proposed to determine the vulnerabilities of face recognition systems.</p><p>In this paper, we focus on the vulnerability of a face recognition system to template invertibility or reconstruction attacks. In a template reconstruction attack (Fig. <ref type="figure" target="#fig_0">1</ref>), we want to determine if face images can be successfully reconstructed from the face templates of target subjects and then used as input to the system to access privileges. Fig. <ref type="figure" target="#fig_1">2</ref> shows examples of face images reconstructed from their deep templates by the proposed method. Some of these reconstructions are successful in that they match well  with the original images (Fig. <ref type="figure" target="#fig_1">2</ref> (a)), while others are not successful (Fig. <ref type="figure" target="#fig_1">2 (b)</ref>). Template reconstruction attacks generally assume that templates of target subjects and the corresponding black-box template extractor can be accessed. These are reasonable assumptions because: (a) templates of target users can be exposed in hacked databases 5,6 , and (b) the corresponding black-box template extractor can potentially be obtained by purchasing the face recognition SDK. To our knowledge, almost all of the face recognition vendors store templates without template protection, while some of them protect templates with specific hardware (e.g., Secure Enclave on A11 of iPhone X <ref type="bibr" target="#b9">[10]</ref>, TrustZone on ARM 7 ). Note that unlike traditional passwords, biometric templates cannot be directly protected by standard ciphers such as AES and RSA since the matching of templates needs to allow small errors caused by intra-subject variations <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Besides, state-of-the-art template protection schemes </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Template features Evaluation Remarks</head><p>MDS <ref type="bibr" target="#b10">[11]</ref> PCA, BIC, COTS Type-I attack a : TAR of 72% using BIC b and 73% using COTS c at an FAR of 1.0% on FERET Linear model with limited capacity RBF regression <ref type="bibr" target="#b8">[9]</ref> LQP <ref type="bibr" target="#b11">[12]</ref> Type-II attack d : 20% rank-1 identification error rate on FERET; EER = 29% on LFW;</p><p>RBF model may have limited generative capacity CNN <ref type="bibr" target="#b12">[13]</ref> Final feature of FaceNet <ref type="bibr" target="#b13">[14]</ref> Reported results were mainly based on visualizations and no comparable statistical results was reported White-box template extractor was assumed Cole et. al., <ref type="bibr" target="#b14">[15]</ref> Intermediate feature of FaceNet <ref type="bibr" target="#b13">[14]</ref> e High-quality images (e.g., front-facing, neutral-pose) are required for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This paper</head><p>Final feature of FaceNet <ref type="bibr" target="#b13">[14]</ref> Type-I attack: TAR f of 95.20% (LFW) and 73.76% (FRGC v2.0) at an FAR of 0.1%; rank-1 identification rate 95.57% on color FERET Type-II attack: TAR of 58.05% (LFW) and 38.39% (FRGC v2.0) at an FAR of 0.1%; rank-1 identification rate 92.84% on color FERET Requires a large number of images for network training a Type-I attack refers to matching the reconstructed image against the face image from which the template was extracted.</p><p>b BIC refers to Bayesian intra/inter-person classifier <ref type="bibr" target="#b15">[16]</ref>. c COTS refers to commercial off-the-shelf system. A local-feature-based COTS was used in <ref type="bibr" target="#b10">[11]</ref>. d Type-II attack refers to matching the reconstructed image against a face image of the same subject that was not used for template creation. e Output of 1024-D 'avgpool' layer of the "NN2" architecture. f TAR for LFW and FRGC v2.0 cannot be directly compared because their similarity thresholds differ. are still far from practical because of the severe trade-off between matching accuracy and system security <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>.</p><p>Face templates are typically compact binary or realvalued feature representations 8 that are extracted from face images to increase the efficiency and accuracy of similarity computation. Over the past couple of decades, a large number of approaches have been proposed for face representations. These representations can be broadly categorized into (i) shallow <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, and (ii) deep (convolutional neural network or CNN) representations <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, according to the depth of their representational models 9 . Deep representations have shown their superior performances in face evaluation benchmarks (such as LFW <ref type="bibr" target="#b24">[25]</ref>, YouTube Faces <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b25">[26]</ref>, and NIST IJB-A <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>). Therefore, it is imperative to investigate the invertibility of deep templates to determine their vulnerability to template reconstruction attacks. However, to the best of our knowledge, no such work has been reported.</p><p>In our study of template reconstruction attacks, we made no assumptions about subjects used to train the target face recognition system. Therefore, only public domain face images were used to train our template reconstruction model. <ref type="bibr" target="#b7">8</ref>. As face templates refer to face representations stored in a face recognition system, these terms are used interchangeably in this paper. 9. Some researchers <ref type="bibr" target="#b22">[23]</ref> refer to shallow representations as those that are not extracted using deep networks.</p><p>The available algorithms for face image reconstruction from templates <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref> 10 , <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref> are summarized in Table <ref type="table" target="#tab_1">1</ref>. The generalizability of the published template reconstruction attacks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref> is not known, as all of the training and testing images used in their evaluations were subsets of the same face dataset. No statistical study in terms of template reconstruction attack has been reported in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>To determine to what extent face templates derived from deep networks can be inverted to obtain the original face images, a reconstruction model with sufficient capacity is needed to invert the complex mapping used in the deep template extraction model <ref type="bibr" target="#b27">[28]</ref>. De-convolutional neural network (D-CNN) 11  <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> is one of the straightforward deep models for reconstructing face images from deep templates. To design a D-CNN with sufficient model capacity 12 , one could increase the number of output channels (filters) in each de-convolution layer <ref type="bibr" target="#b31">[32]</ref>. However, this often introduces noisy and repeated channels since they are treated equally during the training.</p><p>To address the issues of noisy (repeated) channels and insufficient channel details, inspired by DenseNet <ref type="bibr" target="#b32">[33]</ref> and MemNet <ref type="bibr" target="#b33">[34]</ref>, we propose a neighborly de-convolutional network framework (NbNet) and its building block, neighborly de-convolution blocks (NbBlocks). The NbBlock produces the same number of channels as a de-convolution layer by (a) reducing the number of channels in de-convolution layers to avoid the noisy and repeated channels; and (b) then creating the reduced channels by learning from their neighboring channels which were previously created in the same block to increase the details in reconstructed face images. To train the NbNets, a large number of face images are required. Instead of following the time-consuming and expensive process of collecting a sufficiently large face dataset <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b9">10</ref>. MDS method in the context of template reconstructible was initially proposed for reconstructing templates by matching scores between the target subject and attacking queries. However, it can also be used for template reconstruction attacks <ref type="bibr" target="#b10">[11]</ref>.</p><p>11. Some researchers refer to D-CNNs as CNNs. However, given that its purpose is the inverse of a CNN, we distinguish D-CNN and CNN.</p><p>12. The ability of a model to fit a wide variety of functions <ref type="bibr" target="#b27">[28]</ref>.</p><p>we trained a face image generator, DCGAN <ref type="bibr" target="#b36">[37]</ref>, to augment available public domain face datasets. To further enhance the quality of reconstructed images, we explore both pixel difference and perceptual loss <ref type="bibr" target="#b37">[38]</ref> for training the NbNets. In summary, this paper makes following contributions:</p><p>• An investigation of the invertibility of face templates generated by deep networks. To the best of our knowledge, this is the first such study on security and privacy of face recognition systems.</p><p>• An NbNet with its building block, NbBlock, was developed for reconstructing face images from deep templates. The NbNets were trained by data augmentation and perceptual loss <ref type="bibr" target="#b37">[38]</ref>, resulting in maintaining discriminative information in deep templates.</p><p>• Empirical results show that the proposed face image reconstruction from the corresponding templates is successful. We show that we can achieve the following, verification rates (security), TAR of 95.20% (58.05%) on LFW under type-I (type-II) attack @ FAR of 0.1%. For identification (privacy), we achieve 96.58% and 92.84% rank one accuracy (partition fa) in color FERET <ref type="bibr" target="#b38">[39]</ref> as gallery and the images reconstructed from partition fa (type-I attack) and fb (type-II attack) as probe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we describe the current practice of storing face templates, the limitations of current methods for reconstructing face images from deep templates and introduce GANs for generating (synthesizing) face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Face Template Security</head><p>Unlike traditional passwords that can be matched in their encrypted or hash form with standard ciphers (e.g., AES, RSA, and SHA-3), face templates cannot be simply protected by standard ciphers because of the intra-subject variations in face images <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Due to the avalanche effect 13  <ref type="bibr" target="#b39">[40]</ref> of standard ciphers, the face templates protected by standard ciphers need to be decrypted before matching. This introduces another challenge, (decryption) key management. In addition, decrypted face templates can also be gleaned by launching an authentication attempt.</p><p>Face template protection remains an open challenge. To our knowledge, either the vendors ignore the security and privacy issues of face templates, or secure the encrypted templates and the corresponding keys in specific hardware (e.g., Secure Enclave on A11 of iPhone X <ref type="bibr" target="#b9">[10]</ref>, TrustZone on ARM 14 ). Note that the requirement of specific hardware limits the range of biometric applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reconstructing Face Images from Deep Templates</head><p>Face template reconstruction requires the determination of the inverse of deep models used to extract deep templates from face images. Most deep models are complex and are typically implemented by designing and training a network with sufficiently large capacity <ref type="bibr" target="#b27">[28]</ref>.</p><p>Shallow model based <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>: There are two shallow model based methods for reconstructing face images 13. https://en.wikipedia.org/wiki/Avalanche effect 14. https://www.arm.com/products/security-on-arm/trustzone from templates proposed in the literature: multidimensional scaling (MDS) <ref type="bibr" target="#b10">[11]</ref> and radial basis function (RBF) regression <ref type="bibr" target="#b8">[9]</ref>. However, these methods have only been evaluated using shallow templates. The MDS-based method <ref type="bibr" target="#b10">[11]</ref> uses a set of face images to generate a similarity score matrix using the target face recognition system and then finds an affine space in which face images can approximate the original similarity matrix. Once the affine space has been found, a set of similarities is obtained from the target face recognition system by matching the target template and the test face images. The affine representation of the target template is estimated using these similarities, which is then mapped back to the target face image.</p><p>Deep model based <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>: Zhmoginov and Sandler <ref type="bibr" target="#b12">[13]</ref> learn the reconstruction of face images from templates using a CNN by minimizing the template difference between original and reconstructed images. This requires the gradient information from target template extractor and cannot satisfy our assumption of black-box template extractor. Cole et. al. <ref type="bibr" target="#b14">[15]</ref> first estimate the landmarks and textures of face images from the templates, and then combine the estimated landmarks and textures using the differentiable warping to yield the reconstructed images. High-quality face images (e.g., front-facing, neutral-pose) are required to be selected for generating landmarks and textures in <ref type="bibr" target="#b14">[15]</ref> for training the reconstruction model. Note that both <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b14">[15]</ref> does not aim to study vulnerability on deep templates and hence no comparable statistical results based template reconstruction attack were reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">GAN for Face Image Generation</head><p>With adversarial training, GANs <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref> are able to generate photo-realistic (face) images from randomly sampled vectors. It has become one of the most popular methods for generating face images, compared to other methods such as data augmentation <ref type="bibr" target="#b49">[50]</ref> and SREFI <ref type="bibr" target="#b50">[51]</ref>. GANs typically consist of a generator which produces an image from a randomly sampled vector, and a discriminator which classifies an input image as real or synthesized. The basic idea for training a GAN is to prevent images output by the generator be mistakenly classified as real by co-training a discriminator.</p><p>DCGAN <ref type="bibr" target="#b36">[37]</ref> is believed to be the first method that directly generates high-quality images (64 × 64) from randomly sampled vectors. PPGN <ref type="bibr" target="#b43">[44]</ref> was proposed to conditionally generate high-resolution images with better image quality and sample diversity, but it is rather complicated. Wasserstein GAN <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> was proposed to solve the model collapse problems in GAN <ref type="bibr" target="#b41">[42]</ref>. Note that the images generated by Wasserstein GAN <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref> are comparable with those output by DCGAN. BEGAN <ref type="bibr" target="#b46">[47]</ref> and LSGAN <ref type="bibr" target="#b47">[48]</ref> have been proposed to attempt to address the model collapse, and non-convergence problems with GAN. A progressive strategy for training high-resolution GAN is described in <ref type="bibr" target="#b48">[49]</ref>.</p><p>In this work, we employed an efficient yet effective method, DCGAN to generate face images. The original DCGAN <ref type="bibr" target="#b36">[37]</ref> is easy to collapse and outputs poor quality high-resolution images (e.g., 160 × 160 in this work). We address the above problems with DCGAN (Section 3.6.2).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED TEMPLATE SECURITY STUDY</head><p>An overview of our security study for deep template based face recognition systems under template reconstruction attack is shown in Fig. <ref type="figure" target="#fig_3">3</ref>; the normal processing flow and template reconstruction attack flows are shown as black solid and red dotted lines, respectively. This section first describes the scenario of template reconstruction attack using an adversarial machine learning framework <ref type="bibr" target="#b51">[52]</ref>. This is followed by the proposed NbNet for reconstructing face images from deep templates and the corresponding training strategy and implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Template Reconstruction Attack</head><p>The adversarial machine learning framework <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b53">[54]</ref> categorizes biometric attack scenarios from four perspectives: an adversary's goal and his/her knowledge, capability, and attack strategy. Given a deep template based face recognition system, our template reconstruction attack scenario using the adversarial machine learning framework is as follows.</p><p>• Adversary's goal: The attacker aims to impersonate a subject in the target face recognition system, compromising the system integrity.</p><p>• Adversary's knowledge: The attacker is assumed to have the following information. (a) The templates y t of the target subjects, which can be obtained via template database leakage or an insider attack. (b) The black-box feature extractor y = f (x) of the target face recognition system. This can potentially be obtained by purchasing the target face recognition system's SDK. The attacker has neither information about target subjects nor their enrollment environments. Therefore, no face images enrolled in the target system can be utilized in the attack.</p><p>• Adversary's capability:(a) Ideally, the attacker should only be permitted to present fake faces (2D photographs or 3D face masks) to the face sensor during authentication. In this study, to simplify, the attacker is assumed to be able to inject face images directly into the feature extractor as if the images were captured by the face sensor. Note that the injected images could be used to create fake faces in actual attacks. (b) The identity decision for each query is available to the attacker. However, the similarity score of each query cannot be accessed. (c) Only a small number of trials (e.g., &lt; 5) are permitted for the recognition of a target subject.</p><p>• Attack strategy: Under these assumptions, the attacker can infer a face image x t from the target template y t using a reconstruction model x t = g θ (y t ) and insert reconstructed image as a query to access the target face recognition system. The parameter θ of the reconstruction model g θ (•) can be learned using public domain face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NbNet for Face Image Reconstruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Overview</head><p>An overview of the proposed NbNet is shown in Fig. <ref type="figure">4 (a)</ref>. The NbNet is a cascade of multiple stacked de-convolution blocks and a convolution operator, ConvOp. De-convolution blocks up-sample and expand the abstracted signals in the input channels to produce output channels with a larger size as well as more details about reconstructed images. With multiple (D) stacked de-convolution blocks, the NbNet is able to expand highly abstracted deep templates back to channels with high resolutions and sufficient details for generating the output face images. The ConvOp in Fig. <ref type="figure">4</ref> (a) aims to summarize multiple output channels of D-th deconvolution block to the target number of channels (3 in this work for RGB images). It is a cascade of convolution, batch-normalization <ref type="bibr" target="#b52">[53]</ref>, and tanh activation layers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Neighborly De-convolution Block</head><p>A typical design of the de-convolution block <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b36">[37]</ref>, as shown in Fig. <ref type="figure">4</ref> (b), is to learn output channels with up-sampling from channels in previous blocks only. The number of output channels c is often made large enough to ensure sufficient model capacity for template reconstruction <ref type="bibr" target="#b31">[32]</ref>. However, the up-sampled output channels tend to suffer from the following two issues: (a) noisy and repeated channels; and (b) insufficient details. An example of these two issues is shown in Fig. <ref type="figure">5</ref> (a), which is a visualization of output channels in the fifth de-convolution block of a D-CNN that is built with typical de-convolution blocks. The corresponding input template was extracted from the bottom image of Fig. <ref type="figure">4</ref> (a).</p><p>To address these limitations, we propose NbBlock which produces the same number of output channels as typical de-convolution blocks for the face template reconstruction. One of the reasons for noisy and repeated output channels is that a large number of channels are treated equally in a typical de-convolution block; from the perspective of network architecture, these output channels were learned from the same set of input channels and became the input of the same forthcoming blocks. To mitigate this issue, we first reduce the number of output channels that is simultaneously learned from the previous blocks. We then create the reduced number of output channels with enhanced details by learning from neighbor channels in the same block.</p><p>Let G d (•) denote the d-th NbBlock, which is shown as the dashed line in Fig. <ref type="figure">4 (a</ref> </p><formula xml:id="formula_0">X d = G d (X d-1 ) = [X P ]<label>(1)</label></formula><p>where X d-1 denotes the output of the (d -1)-th NbBlock, [•] denotes a function of channel concatenation, and X P is the set of outputs of DconvOP and all ConvOPs in G d (•),</p><formula xml:id="formula_1">X P = {X d , X d,1 , X d,2 , • • • , X d,P }<label>(2)</label></formula><p>where X d and X d,p denotes the output of DconvOP and the p-th ConvOP in d-th block, resp., and satisfy</p><formula xml:id="formula_2">X d = N d (X d-1 ) , X d,p = N dp [ Xp ] (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where Xp is a non-empty subset of X p .</p><p>Based on this idea, we built two NbBlocks, A and B, as shown in Figs. <ref type="figure">4 (c)</ref> and<ref type="figure">(d)</ref>, where the corresponding reconstructed networks are named NbNet-A and NbNet-B, respectively. In this study, the DconvOp (ConvOp) in Figs. <ref type="figure">4 (b),</ref><ref type="figure">(c</ref>), and (d) denotes a cascade of de-convolution (convolution), batch-normalization <ref type="bibr" target="#b52">[53]</ref>, and ReLU activation layers. The only difference between blocks A and B is the choice of Xp ,</p><formula xml:id="formula_4">Xp =      {X d }, blocks A &amp; B with p = 1; {X d,p-1 }, block A with p &gt; 1; X p , block B with p &gt; 1.<label>(4)</label></formula><p>In our current design of the NbBlocks, half of output channels ( c 2 for block d) are produced by a DconvOP, and the remaining channels are produced by P ConvOPs, each of which gives, in this study, eight output channels (Table . 2). Example of blocks A and B with 32 output channels are shown in Figs. <ref type="figure">5 (b)</ref> and<ref type="figure">(c</ref>). The first two rows of channels are produced by DconvOp and the third and fourth rows of channels are produced by the first and second ConvOps, respectively. Compared with Fig. <ref type="figure">5</ref> (a), the first two rows in Figs. <ref type="figure">5 (b)</ref> and<ref type="figure">(c</ref>) have small amount of noise and fewer repeated channels, where the third and fourth row provide channels with more details about the target face image (the reconstructed image in Fig. <ref type="figure">4 (a)</ref>). The design of our NbBlocks is motivated by DenseNet <ref type="bibr" target="#b32">[33]</ref> and MemNet <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reconstruction Loss</head><p>Let us denote R (x, x ) as the reconstruction loss between an input face image x and its reconstruction x = g θ f (x) , where g θ (•) denotes an NbNet with parameter θ and f (•) denotes a black-box deep template extractor.</p><p>Pixel Difference: A straightforward loss for learning reconstructed image x is pixel-wise loss between x and its original version x. The Minkowski distance could then be used and mathematically expressed as</p><formula xml:id="formula_5">R pixel (x, x ) = ||x -x || k = M m=1 |x m -x m | k 1 k (5)</formula><p>where M denotes number of pixels in x and k denotes the order of the metric.</p><p>Perceptual Loss <ref type="bibr" target="#b37">[38]</ref>: Because of the high discriminability of deep templates, most of the intra-subject variations in a face image might have been eliminated in its corresponding deep template. The pixel difference based reconstruction leads to a difficult task of reconstructing these eliminated intra-subject variations, which, however, are not necessary for reconstruction. Besides, it does not consider holistic contents in an image as interpreted by machines and human visual perception. Therefore, instead of using pixel difference, we employ the perceptual loss <ref type="bibr" target="#b37">[38]</ref> which guides the reconstructed images towards the same representation as the original images. Note that a good representation is robust to intra-subject variations in the input images. The representation used in this study is the feature map in the VGG-19 model <ref type="bibr" target="#b54">[55]</ref>  15 . We empirically determine that using 15. Provided by https://github.com/dmlc/mxnet-model-gallery the output of ReLU3 2 activation layer as the feature map leads the best image reconstruction, in terms of face matching accuracy. Let F (•) denote feature mapping function of the ReLU3 2 activation layer of VGG-19 <ref type="bibr" target="#b54">[55]</ref>, then the perceptual loss can be expressed as</p><formula xml:id="formula_6">R percept (x, x ) = 1 2 ||F (x) -F (x )|| 2 2</formula><p>(6)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Generating Face Images for Training</head><p>To successfully launch an template reconstruction attack on a face recognition system without knowledge of the target subject population, NbNets should be able to accurately reconstruct face images with input templates extracted from face images of different subjects. Let p x (x) denote the probability density function (pdf) of image x, the objective function for training a NbNet can be formulated as</p><formula xml:id="formula_7">arg min θ L (x, θ) = arg min θ R (x, x ) p x (x)dx = arg min θ R x, g θ ( f (x)) p x (x)dx.<label>(7)</label></formula><p>Since there are no explicit methods for estimating p x (x), we cannot sample face images from p x (x). The common approach is to collect a large-scale face dataset and approximate the loss function L(θ) in Eq. ( <ref type="formula" target="#formula_7">7</ref>) as:</p><formula xml:id="formula_8">L(x, θ) = 1 N N i R x i , g θ ( f (x i ))<label>(8)</label></formula><p>where N denotes the number of face images and x i denotes the i-th training image. This approximation is optimal if, and only if, N is sufficiently large. In practice, this is not feasible because of the huge time and cost associated with collecting a large database of face images.</p><p>To train a generalizable NbNet for reconstructing face images from their deep templates, a large number of face images are required. Ideally, these face images should come from a large number of different subjects because deep face templates of the same subject are very similar and can be regarded as either single exemplar or under large intra-user variations, a small set of exemplars in the training of NbNet. However, current large-scale face datasets (such as VGG-Face <ref type="bibr" target="#b22">[23]</ref>, CASIA-Webface <ref type="bibr" target="#b55">[56]</ref>, and Multi-PIE <ref type="bibr" target="#b56">[57]</ref>) were primarily collected for training or evaluating face recognition algorithms. Hence, they either contain an insufficient number of images (for example, 494K images in CASIA-Webface) or an insufficient number of subjects (for instance, 2,622 subjects in VGG-Face and 337 subjects in Multi-PIE) for training a reconstruction NbNet.</p><p>Instead of collecting a large face image dataset for training, we propose to augment current publicly available datasets. A straightforward way to augment a face dataset is to estimate the distribution of face images p x (x) and then sample the estimated distribution. However, as face images generally consist of a very large number of pixels, there is no efficient method to model the joint distribution of these pixels. Therefore, we introduced a generator x = r(z) capable of generating a face image x from a vector z with a given distribution. Assuming that r(z) is one-to-one and smooth, the face images can be sampled by sampling z. The Pixel difference or perceptual loss <ref type="bibr" target="#b37">[38]</ref> loss function L(θ) in Eq. ( <ref type="formula" target="#formula_7">7</ref>) can then be approximated as follows:</p><formula xml:id="formula_9">L (x, θ) = R x, g θ ( f (x)) p x (x)dx = R r(z), g θ f (r(z)) p z (z)dz.<label>(9)</label></formula><p>where p z (z) denotes the pdf of variable z. Using the change of variables method <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, it is easy to show that p z (z) and r(z) have the following connection,</p><formula xml:id="formula_10">p z (z) = p x (r(z)) det dx dz , where dx dz ij = ∂x i ∂z j .<label>(10)</label></formula><p>Suppose a face image x ∈ R h×w×c of height h, width w, and with c channels can be represented by a real vector b</p><formula xml:id="formula_11">= {b 1 , • • • , b k } ∈ R k in a manifold space with h × w × c</formula><p>k. It can then be shown that there exists a generator function b = r(z) that generates b with a distribution identical to that of b, where b can be arbitrarily distributed and z ∈ [0, 1] k is uniformly distributed (see Appendix).</p><p>To train the NbNets in the present study, we used the generative model of a DCGAN <ref type="bibr" target="#b36">[37]</ref> as our face generator r(•). This model can generate face images from vectors z that follow a uniform distribution. Specifically, DCGAN generates face images r(z) with a distribution that is an approximation to that of real face images x. It can be shown empirically that a DCGAN can generate face images of unseen subjects with different intra-subject variations. By using adversarial learning, the DCGAN is able to generate face images that are classified as real face images by a co-trained real/fake face image discriminator. Besides, the intra-subject variations generated using a DCGAN can be controlled by performing arithmetic operations in the random input space <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Differences with DenseNet</head><p>One of the related work to NbNet is DenseNet <ref type="bibr" target="#b32">[33]</ref>, from which the NbNet is inspired. Generally, DenseNet is based on convolution layers and designed for object recognition, while the proposed NbNet is based on de-convolution layers and aimed to reconstruct face images from deep templates. Besides, NbNet is a framework whose NbBlocks produce output channels learned from previous blocks and neighbor channels within the block. The output channels of NbBlocks consist of fewer repeated and noisy channels and contain more details for face image reconstruction than the typical de-convolution blocks. Under the framework of NbNet, one could build a skip-connection-like network <ref type="bibr" target="#b59">[60]</ref>, NbNet-A, and a DenseNet -like network, NbNet-B. Note that NbNet-A sometimes achieves a comparable performance to NbNet-B with roughly 67% of the parameters and 54% running time only (see model VGG-NbA-P and VGG-NbB-P in Section 4). We leave more efficient and accurate NbNets construction as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Network Architecture</head><p>The detailed architecture of the D-CNN and the proposed NbNets is shown in Table . 2. The NbNet-A and NbNet-B show the same structure in Table . 2. However, the input of the ConvOP in the de-convolution blocks (1)-( <ref type="formula">6</ref>) are different (Fig. <ref type="figure">4</ref>), where NbNet-A uses the nearest previous channels in the same block, and NbNet-B uses all the previous channels in the same block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Revisiting DCGAN</head><p>To train our NbNet to reconstruct face images from deep templates, we first train a DCGAN to generate face images. These generated images are then used for training. The face images generated by the original DCGAN could be noisy and sometimes difficult to interpret. Besides, the training as described in <ref type="bibr" target="#b36">[37]</ref> is often collapsed in generating highresolution images. To address these issues, we revisit the DCGAN as below (as partially suggested in <ref type="bibr" target="#b41">[42]</ref>):</p><p>• Network architecture: replace the batch normalization and ReLU activation layer in both generator and discriminator by the SeLU activation layer <ref type="bibr" target="#b60">[61]</ref>, which performs the normalization of each training sample. • Learning rate: in the training of DCGAN, at each iteration, the generator is updated with one batch of samples, while the discriminator is updated with two batches of samples (1 batch of 'real' and 1 batch of 'generated'). This often makes the discriminator always correctly classify the images output by the generator. To balance, we adjust the learning rate of the generator to 2 × 10 -4 , which is greater than the learning rate of the discriminator, 5 × 10 -5 .</p><p>Example generated images were shown in Fig. <ref type="figure" target="#fig_7">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3">Training Details</head><p>With the pre-trained DCGAN, face images were first generated by randomly sampling vectors z from a uniform distribution and the corresponding face templates were extracted. The NbNet was then updated with the generated face images as well as the corresponding templates using batch gradient descent optimization. This training strategy was used to minimize the loss function L(θ) in Eq. ( <ref type="formula" target="#formula_9">9</ref>), which is an approximation of the loss function in Eq. ( <ref type="formula" target="#formula_7">7</ref>).</p><p>The face template extractor we used is based on FaceNet <ref type="bibr" target="#b13">[14]</ref>, one of the most accurate CNN models for face recognition currently available. To ensure that the face reconstruction scenario is realistic, we used an open-source implementation 16 based on TensorFlow 17 without any modifications (model 20170512-110547).</p><p>We implemented the NbNets using MXNet 18 . The networks were trained using a mini-batch based algorithm, Adam <ref type="bibr" target="#b63">[64]</ref> with batch size of 64, β 1 = 0.5 and β 2 = 0.999. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Database and Experimental Setting</head><p>The vulnerabilities of deep templates under template reconstruction attacks were studied with our proposed reconstruction model, using two popular large-scale face datasets for training and three benchmark datasets for testing. The training datasets consisted of one unconstrained datasets, VGG-Face <ref type="bibr" target="#b22">[23]</ref> and one constrained dataset, Multi-PIE <ref type="bibr" target="#b56">[57]</ref>.</p><p>• VGG-Face <ref type="bibr" target="#b22">[23]</ref> comprises of 2.6 million face images from 2,622 subjects. In total, 1,942,242 trainable images were downloaded with the provided links.</p><p>• Multi-PIE <ref type="bibr" target="#b56">[57]</ref>. We used 150,760 frontal images (3 camera views, with labels '14 0 , '05 0 , and '05 1 , respectively), from 337 subjects.</p><p>Three testing datasets were used, including two for verification (LFW <ref type="bibr" target="#b24">[25]</ref> and FRGC v2.0 <ref type="bibr" target="#b62">[63]</ref>) and one for identification (color FERET <ref type="bibr" target="#b38">[39]</ref>) scenarios.</p><p>• LFW <ref type="bibr" target="#b24">[25]</ref> consists of 13,233 images of 5,749 subjects downloaded from the Web.</p><p>• FRGC v2.0 <ref type="bibr" target="#b62">[63]</ref> consists of 50,000 frontal images of 4,003 subjects with two different facial expressions (smiling and neutral), taken under different illumination conditions. A total of 16,028 images of 466 subjects (as specified in the target set of Experiment 1 of FRGC v2.0 <ref type="bibr" target="#b62">[63]</ref>) were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Color FERET <ref type="bibr" target="#b38">[39]</ref> consists of four partitions (i.e., fa, fb, dup1 and dup2), including 2,950 images. Compared to the gallery set fa, the probe sets (fb, dup1 and dup2) contain face images of difference facial expression and aging.  The face images were aligned using the five points detected by MTCNN 20  <ref type="bibr" target="#b64">[65]</ref> and then cropped to 160 × 160 pixels. Instead of aligning images from the LFW dataset, we used the pre-aligned deep funneled version <ref type="bibr" target="#b61">[62]</ref>. Fig. <ref type="figure" target="#fig_6">6</ref> shows example images from these five datasets.</p><p>To determine the effectiveness of the proposed NbNet, we compare three different network architectures, i.e., D-CNN, NbNet-A, and NbNet-B, which are built using the typical de-convolution blocks, NbBlocks A and B. All of these networks are trained using the proposed generatorbased training strategy using a DCGAN <ref type="bibr" target="#b36">[37]</ref> with both pixel difference 21 and perceptual loss 22  <ref type="bibr" target="#b37">[38]</ref>. To demonstrate the effectiveness of the proposed training strategy, we train the NbNet-B directly using images from VGG-Face, Multi-PIE, and a mixture of three datasets (VGG-Face, CASIA-Webface 23 <ref type="bibr" target="#b55">[56]</ref>, and Multi-PIE). Note that both VGG-Face and Multi-PIE are augmented to 19.2M images in our training. Examples of images generated using our trained face image generator are shown in Fig. <ref type="figure" target="#fig_7">7</ref>. In addition, the proposed NbNet based reconstruction method was compared with a state-of-the-art RBF-regression-based method <ref type="bibr" target="#b8">[9]</ref>. In contrast to the neural network based method, the RBF 24 regression model of <ref type="bibr" target="#b8">[9]</ref> used the same dataset for training and testing (either LFW or FRGC v2.0). Therefore, the RBF-regression-based reconstruction method was expected to have better reconstruction accuracy than the proposed method. The MDS-based method <ref type="bibr" target="#b10">[11]</ref> was not compared <ref type="bibr" target="#b18">19</ref>. https://www.anaconda.com 20. https://github.com/pangyupo/mxnet mtcnn face detection.git 21. We simply choose mean absolute error (MAE), where order k = 1. 22. To reduce the training time, we first train the network with pixel difference loss and then fine-tune it using perceptual loss <ref type="bibr" target="#b37">[38]</ref>.</p><p>23. It consists of 494,414 face images from 10,575 subjects. We obtain 455,594 trainable images after preprocessing.</p><p>24. It was not compared in the identification task on color FERET.</p><p>here because it is a linear model and was not as good as the RBF-regression-based method <ref type="bibr" target="#b8">[9]</ref>. We did not compare <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref> because <ref type="bibr" target="#b12">[13]</ref> does not satisfy our assumption of black-box template extractor and <ref type="bibr" target="#b14">[15]</ref> requires to selecting high quality images for training. Table <ref type="table" target="#tab_3">3</ref> summarizes the 16 comparison models used in this study for deep template inversion.</p><p>Examples of the reconstructed images of the first ten subjects in LFW and FRGC v2.0 are shown in Fig. <ref type="figure">8</ref>. The leftmost column shows the original images, and the remaining columns show the images reconstructed using the 16 reconstruction models. For the RBF model, every image in the testing datasets (LFW and FRGC v2.0) has 10 different reconstructed images that can be created using the 10 crossvalidation trials in the BLUFR protocol 25  <ref type="bibr" target="#b65">[66]</ref>. The RBFreconstructed images shown in this paper are those with the highest similarity scores among these 10 different reconstructions. The number below each image is the similarity score between the original and reconstructed images. The similarity scores were calculated using the cosine similarity in the range of [-1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Verification Under Template Reconstruction Attack</head><p>We quantitatively evaluated the template security of the target face recognition system (FaceNet <ref type="bibr" target="#b13">[14]</ref>) under type-I and type-II template reconstruction attacks. The evaluation metric was face verification using the BLUFR protocol <ref type="bibr" target="#b65">[66]</ref>. The impostor scores obtained from the original face images were used in both of the attacks to demonstrate the efficacy of the reconstructed face images. The genuine scores in the type-I attack were obtained by comparing the reconstructed images against the original images. The genuine scores in the type-II attack were obtained by substituting one of the 25. http://www.cbsr.ia.ac.cn/users/scliao/projects/blufr/     For the ease of reading, we only show the curves for D-CNN, NbNet-B trained with perceptual loss, and the RBF based method. Refer to Table <ref type="table" target="#tab_6">4</ref> for the numerical comparison of all models. Note that the curves for VGG-Dn-P and MPIE-Dn-P are overlapping in (a). original images in a genuine comparison (image pair) with the corresponding reconstructed image. For benchmarking, we report the "Original" results based on original face images. Every genuine score of "Original" in type-I attack was obtained by comparing two identical original images and thus the corresponding TAR stays at 100%. The genuine scores of "Original" in type-II attack were obtained by the genuine comparisons specified in BLUFR protocol. The BLUFR protocol uses tenfold cross-validation; the performance reported here is the 'lowest', namely (µ -σ), where µ and σ denote the mean and standard deviation of the attacking accuracy obtained from the 10 trials, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Performance on LFW</head><p>In each trial of the BLUFR protocol <ref type="bibr" target="#b65">[66]</ref> for LFW <ref type="bibr" target="#b24">[25]</ref>, there is an average of 46,960,863 impostor comparisons. The average number of testing images is 9,708. Hence, there are 9,708 genuine comparisons in a type-I attack on LFW. The average number of genuine comparisons in a type-II attack on LFW is 156,915; this is the average number of genuine comparisons specified in the BLUFR protocol.</p><p>The receiver operator characteristic (ROC) curves of type-I and type-II attacks on LFW are shown in Fig. <ref type="figure">9</ref>. Table <ref type="table" target="#tab_6">4</ref> shows the TAR values at FAR=0.1% and FAR=1.0%, respectively. The ROC curve of "Original" in the type-II attack (Fig. <ref type="figure">9b</ref>) is the system performance with BLUFR protocol <ref type="bibr" target="#b65">[66]</ref> based on original images.</p><p>For both type-I and type-II attacks, the proposed NbNets generally outperform the D-CNN, where MPIE-NbA-P is not as effective as MPIE-Dn-P. Moreover, the models trained using the proposed strategy (VGG-NbB-M and MPIE-NbB-M) outperform the corresponding models trained with the non-augmented datasets (VGGr-NbB-M and MPIEr-NbB-M). The models trained using the raw images in VGG (VGGr-NbB-M) outperform the corresponding model trained using the mixed dataset. All NbNets trained with the proposed training strategy outperform the RBF regression based method <ref type="bibr" target="#b8">[9]</ref>. In the type-I attack, the VGG-NbA-P model achieved a TAR of 95.20% (99.14%) at FAR=0.1% (FAR=1.0%). This implies that an attacker has approximately 95% (or 99% at FAR=1.0%) chance of accessing the system using a leaked template.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Performance on FRGC v2.0</head><p>Each trial of the BLUFR protocol <ref type="bibr" target="#b65">[66]</ref> for FRGC v2.0 <ref type="bibr" target="#b62">[63]</ref> consisted of an average of 76,368,176 impostor comparisons and an average of 12,384 and 307,360 genuine comparisons for type-I and type-II attacks, respectively.</p><p>The ROC curves of type-I and type-II attacks on FRGC v2.0 are shown in Fig. <ref type="figure" target="#fig_0">10</ref>. The TAR values at FAR=0.1% and FAR=1.0% are shown in Table <ref type="table">5</ref>. The TAR values (Tables <ref type="table" target="#tab_6">4</ref> and<ref type="table">5</ref>) and ROC plots (Figs. 9 and 10) for LFW and FRGC v2.0 cannot be directly compared, as the thresholds for LFW and FRGC v2.0 differ (e.g., the thresholds at FAR=0.1% are 0.51 and 0.80 for LFW and FRGC v2.0, respectively). The similarity threshold values were calculated based on the impostor distributions of the LFW and FRGC v2.0 databases.</p><p>It was observed that the proposed NbNets generally outperform D-CNN. The only exception is that the MPIE-NbA-P was not as good as MPIE-Dn-P. Significant improvements by using the augmented datasets (VGG-NbB-M and MPIE-NbB-M) were observed, compared with VGGr-NbB-M and MPIEr-NbB-M, for both the type-I and type-II attacks. All NbNets outperform the RBF regression based method <ref type="bibr" target="#b8">[9]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS AND FUTURE WORK</head><p>We investigated the security and privacy of deep face templates by studying the reconstruction of face images via the inversion of their corresponding deep templates. A NbNet was trained for reconstructing face images from their corresponding deep templates and strategies for training generalizable and robust NbNets were developed. Experimental results indicated that the proposed NbNet-based reconstruction method outperformed RBF-regression-based face template reconstruction in terms of attack success rates. We demonstrate that in verification scenario, TAR of 95.20% (58.05%) on LFW under type-I (type-II) attack @ FAR of 0.1% can be achieved with our reconstruction model. Besides, 96.58% (92.84%) of the images reconstruction from templates of partition fa (fb) can be identified from partition fa in color FERET <ref type="bibr" target="#b38">[39]</ref>. This study revealed potential security and privacy issues resulting from template leakage in state-ofthe-art face recognition systems, which are primarily based on deep templates. Our future research goals are two-fold: protecting the system from template reconstruction attacks and improving the proposed reconstruction. For the protection, we aim to design a template protection scheme <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b66">[67]</ref> by introducing user-specific randomness into deep networks for extracting secure and discriminative deep templates. Therefore, the extracted deep templates not only depend on the input images, but also subject-specific keys. To further enhance the system security, stronger anti-spoofing techniques <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> will also be sought. For the improvement of the reconstruction, we plan to (i) devise a more effective reconstruction algorithm by designing a more effective NbNet and considering the holistic contents in face images; and (ii) study cross-system attacks using face images reconstructed from the templates of a given face recognition system to access a different face recognition system (different from the one used to generate he template).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Face recognition system vulnerability to template reconstruction attacks. Face image of a target subject is reconstructed from the corresponding template to gain system access by (a) creating a fake face (for example, a 2D printed image or 3D mask) (blue box) or (b) injecting a reconstructed face image directly into the feature extractor (red box).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example face images reconstructed from their templates using the proposed method (VGG-NbB-P). The top row shows the original images (from LFW) and the bottom row shows the corresponding reconstructions. The numerical value shown between the two images is the cosine similarity between the original and its reconstructed face image. The similarity threshold is 0.51 (0.38) at FAR = 0.1% (1.0%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An overview of the proposed system for reconstructing face images from the corresponding deep templates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. The proposed NbNet for reconstructing face images from the corresponding face templates. (a) Overview of our face reconstruction network, (b) typical deconvolution block for building de-convolutional neural network (D-CNN), (c) and (d) are the neighborly de-convolution blocks (NbBlock) A/B for building NbNet-A and NbNet-B, respectively. Note that ConvOP (DconvOP) denotes a cascade of a convolution (de-convolution), a batch-normalization [53], and a ReLU activation (tanh in ConvOP of (a)) layers, where the width of ConvOp (DconvOP) denotes the number of channels in its convolution (de-convolution) layer. The black circles in (c) and (d) denote the channel concatenation of the output channels of DconvOP and ConvOPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>) and is the building component of our NbNet. Suppose that G d (•) consists of one deconvolution operator (DconvOP) N d and P convolution operators (ConvOPs) {N dp |p = 1, 2, • • • , P }. Let X d and X d,p denote the output of DconvOP N d and p-th ConvOP N dp in d-th NbBlock G d (•), then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Example face images from the training and testing datasets: (a) VGG-Face (1.94M images) [23], (b) Multi-PIE (151K images, only three camera views were used, including '14 0 , '05 0 and '05 1 , respectively) [57], (c) LFW (13,233 images) [25], [62], (d) FRGC v2.0 (16,028 images in the target set of Experiment 1) [63], and (e) Color FERET (2,950 images) [39].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Sample face images generated from face generators trained on (a) VGG-Face, and (b) Multi-PIE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>using the training dataset, and then use face images generated from the pretrained DCGAN for training the target D-CNN. Test the trained D-CNN using testing datasets. train the target D-CNN using face images from the training dataset, and then test the trained D-CNN using testing datasets. using the training dataset, and then use face images generated from the pretrained DCGAN for training the target D-CNN. Test the trained D-CNN using testing datasets. train the target D-CNN using face images from the training dataset, and then test the trained D-CNN using testing datasets. Mixedr-NbB-M VGG-Face CASIA-Webface Multi-PIE RBF [9] LFW N/A LFW Train and test the RBF regression based method using the training and testing images specified in the evaluation protocol. FRGC v2.0 N/A FRGC v2.0 a Dn, NbA, and NbB denote D-CNN, NbNet-A, and NbNet-B, respectively b MAE denotes "mean absolute error"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Original VGG-Dn-P VGG-NbA-P VGG-NbB-P VGG-Dn-M VGG-NbA-M VGG-NbB-M VGGr-NbB-M MPIE-Dn-P MPIE-NbA-P MPIE-NbB-P MPIE-Dn-M MPIE-NbA-M MPIE-NbB-M MPIEr-NbB-M Mixed-NbB-M RBF 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>-P VGG-NbB-P VGG-Dn-M VGG-NbA-M VGG-NbB-M VGGr-NbB-M MPIE-Dn-P MPIE-NbA-P MPIE-NbB-P MPIE-Dn-M MPIE-NbA-M MPIE-NbB-M MPIEr-NbB-M Mixed-NbB-M RBF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>(b) FRGC v2. 0 Fig. 8 .FalseFig. 9 .</head><label>089</label><figDesc>Fig. 8. Reconstructed face images of the first 10 subjects from (a) LFW and (b) FRGC v2.0. The original face images are shown in the first column. Each column denotes the reconstructed face images from different models used for reconstruction. The number below each reconstructed image shows the similarity score between the reconstructed image and the original image. The scores (ranging from -1 to 1) were calculated using the cosine similarity. The mean verification thresholds on LFW and FRGC v2.0 were 0.51 and 0.80, respectively, at FAR=0.1%, and 0.38 and 0.64, respectively, at FAR=1.0%.</figDesc><graphic coords="10,76.58,372.97,456.77,305.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Guangcan Mai and Pong C. Yuen are with the Department of Computer Science, Hong Kong Baptist University, Hong Kong SAR, CHINA. E-mail:{csgcmai,pcyuen}@comp.hkbu.edu.hk;</figDesc><table /><note><p>• Kai Cao and Anil K. Jain are with the Department of Computer Science and Engineering, Michigan State University, MI, 48824, USA. E-mail: {kaicao,jain}@cse.msu.edu</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Comparison of major algorithms for face image reconstruction from their corresponding templates</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Network details for D-CNN and NbNets. "[k 1 × k 2 , c] DconvOP (ConvOP), stride s", denotes cascade of a de-convolution (convolution) layer with c channels, kernel size (k 1 , k 2 ) and stride s, batch normalization, and ReLU (tanh for the bottom ConvOP) activation layer.</figDesc><table><row><cell>Layer name</cell><cell>Output size (c × w × h)</cell><cell>D-CNN</cell><cell>NbNet-A, NbNet-B</cell></row><row><cell>input layer</cell><cell>128 × 1 × 1</cell><cell></cell><cell></cell></row><row><cell>De-convolution Block (1)</cell><cell>512 × 5 × 5</cell><cell>[5 × 5, 512] DconvOP, stride 2</cell><cell>[5 × 5, 256] DconvOP, stride 2 {[3 × 3, 8] ConvOP, stride 1}×32</cell></row><row><cell>De-convolution Block (2)</cell><cell>256 × 10 × 10</cell><cell>[3 × 3, 256] DconvOP, stride 2</cell><cell>[3 × 3, 128] DconvOP, stride 2 {[3 × 3, 8] ConvOP, stride 1}×16</cell></row><row><cell>De-convolution Block (3)</cell><cell>128 × 20 × 20</cell><cell>[3 × 3, 128] DconvOP, stride 2</cell><cell>[3 × 3, 64] DconvOP, stride 2 {[3 × 3, 8] ConvOP, stride 1}×8</cell></row><row><cell>De-convolution Block (4)</cell><cell>64 × 40 × 40</cell><cell>[3 × 3, 64] DconvOP, stride 2</cell><cell>[3 × 3, 32] DconvOP, stride 2 {[3 × 3, 8] ConvOP, stride 1}×4</cell></row><row><cell>De-convolution Block (5)</cell><cell>32 × 80 × 80</cell><cell>[3 × 3, 32] DconvOP, stride 2</cell><cell>[3 × 3, 16] DconvOP, stride 2 {[3 × 3, 8] ConvOP, stride 1}×2</cell></row><row><cell>De-convolution Block (6)</cell><cell>16 × 160 × 160</cell><cell>[3 × 3, 16] DconvOP, stride 2</cell><cell>[3 × 3, 8] DconvOP, stride 2 {[3 × 3, 8] ConvOP, stride 1}×1</cell></row><row><cell>ConvOP</cell><cell>3 × 160 × 160</cell><cell cols="2">[3 × 3, 3] ConvOP, stride 1</cell></row><row><cell>Loss layer</cell><cell>3 × 160 × 160</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Deep template face template reconstruction models for comparison</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>TARs (%) of type-I and type-II attacks on LFW for different template reconstruction methods, where "Original" denotes results based on the original images and other methods are described in Table3. (best, second best)</figDesc><table><row><cell>Attack</cell><cell cols="2">Type-I</cell><cell cols="2">Type-II</cell></row><row><cell>FAR</cell><cell>0.1%</cell><cell>1.0%</cell><cell>0.1%</cell><cell>1.0%</cell></row><row><cell>Original</cell><cell>100.00</cell><cell>100.00</cell><cell>97.33</cell><cell>99.11</cell></row><row><cell>VGG-Dn-P</cell><cell>84.65</cell><cell>96.18</cell><cell>45.63</cell><cell>79.13</cell></row><row><cell>VGG-NbA-P</cell><cell>95.20</cell><cell>99.14</cell><cell>53.91</cell><cell>87.06</cell></row><row><cell>VGG-NbB-P</cell><cell>94.37</cell><cell>98.63</cell><cell>58.05</cell><cell>87.37</cell></row><row><cell>VGG-Dn-M</cell><cell>70.22</cell><cell>88.35</cell><cell>26.22</cell><cell>64.88</cell></row><row><cell>VGG-NbA-M</cell><cell>79.52</cell><cell>94.94</cell><cell>30.97</cell><cell>68.14</cell></row><row><cell>VGG-NbB-M</cell><cell>89.52</cell><cell>97.75</cell><cell>37.09</cell><cell>79.19</cell></row><row><cell>VGGr-NbB-M</cell><cell>72.53</cell><cell>93.21</cell><cell>27.38</cell><cell>70.72</cell></row><row><cell>MPIE-Dn-P</cell><cell>85.34</cell><cell>95.57</cell><cell>41.21</cell><cell>77.51</cell></row><row><cell>MPIE-NbA-P</cell><cell>80.33</cell><cell>95.46</cell><cell>21.75</cell><cell>63.05</cell></row><row><cell>MPIE-NbB-P</cell><cell>89.25</cell><cell>97.69</cell><cell>37.30</cell><cell>80.67</cell></row><row><cell>MPIE-Dn-M</cell><cell>37.11</cell><cell>63.01</cell><cell>3.23</cell><cell>13.26</cell></row><row><cell>MPIE-NbA-M</cell><cell>50.54</cell><cell>78.91</cell><cell>6.11</cell><cell>33.26</cell></row><row><cell>MPIE-NbB-M</cell><cell>67.86</cell><cell>88.56</cell><cell>24.00</cell><cell>57.98</cell></row><row><cell>MPIEr-NbB-M</cell><cell>34.87</cell><cell>65.56</cell><cell>3.67</cell><cell>21.24</cell></row><row><cell>Mixedr-NbB-M</cell><cell>71.62</cell><cell>92.98</cell><cell>19.29</cell><cell>65.63</cell></row><row><cell>RBF [9]</cell><cell>19.76</cell><cell>50.55</cell><cell>4.41</cell><cell>30.70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 7</head><label>7</label><figDesc>Average reconstruction time (ms) for a single template. The total number of network parameters are indicated in the last column.</figDesc><table><row><cell></cell><cell>CPU</cell><cell>GPU</cell><cell>#Params</cell></row><row><cell>D-CNN</cell><cell>84.1</cell><cell>0.268</cell><cell>4,432,304</cell></row><row><cell>NbNet-A</cell><cell>62.6</cell><cell>0.258</cell><cell>2,289,040</cell></row><row><cell>NbNet-B</cell><cell>137.1</cell><cell>0.477</cell><cell>3,411,472</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>MAI et al.: ON THE RECONSTRUCTION OF FACE IMAGES FROM DEEP FACE TEMPLATES</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research was partially supported by a Hong Kong RGC grant (HKBU 12201414) and the Madam Kwok Chung Bo Fun Graduate School Development Fund, HKBU. The authors would like to thank Prof. Wei-Shi Zheng, Dr. Xiangyuan Lan and Miss Huiqi Deng for their helpful suggestions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He served as the editorin-chief of the IEEE Transactions on Pattern Analysis and Machine Intelligence (1991-1994), a member of the United States Defense Science Board and a member of the Forensic Science Standards Board. He has received Fulbright, Guggenheim, Alexander von Humboldt, and IAPR King Sun Fu awards. He is a member of the United States National Academy of Engineering and a Foreign Fellow of the Indian National Academy of Engineering.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>False Accept Rate (%)   In the type-I attack, the best model, MPIE-NbB-P achieved a TAR of 73.76% (98.35%) at FAR=0.1% (FAR=1.0%). This implies that an attacker has a 74% (98%) chance of accessing the system at FAR=0.1% (1.0%) using a leaked template.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Identification with Reconstructed Images</head><p>We quantitatively evaluate the privacy issue of a leaked template extracted by target face recognition system (FaceNet <ref type="bibr" target="#b13">[14]</ref>) under type-I and type-II attacks. The evaluation metric was the standard color FERET protocol <ref type="bibr" target="#b38">[39]</ref>. The partition fa (994 images) was used as the gallery set. For the type-I attack, the images reconstructed from the partition fa was used as the probe set. For the type-II attack, the probe sets (fb with 992 images, dup1 with 736 images, and dup2 with 228 images) specified in the color FERET protocol were replaced by the corresponding reconstructed images.</p><p>The rank-one identification rate of both type-I and type-II attacks on color FERET are shown in Table <ref type="table">6</ref>. The row values under "Original" show the identification rate based on the original images. It stays at 100% for the type-I attack Rank-one recognition rate (%) on color FERET <ref type="bibr" target="#b38">[39]</ref> with partition fa as gallery and reconstructed images from different partition as probe. The partitions (i.e., fa, fb, dup1 and dup2) are described in color FERET protocol <ref type="bibr" target="#b38">[39]</ref>. Various methods are described in Table <ref type="table">3</ref> . This implies a severe privacy issue; more than 90% of the subjects in the database can be identified with a leaked template.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Computation Time</head><p>In the testing stage, with an NVIDIA TITAN X Pascal (GPU) and an Intel(R) i7-6800K @ 3.40 GHz (CPU), the average time (in microseconds) to reconstruct a single face template with D-CNN, NbNet-A, and NbNet-B is shown in Table <ref type="table">7</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sample images can be independently restored from face recognition templates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CCECE</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the vulnerability of face verification systems to hill-climbing attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortega-Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Masquerade attack on transform-based binary-template protection based on perceptron learning</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face spoof detection with image distortion analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Secure face unlock: Spoof detection on smartphones</title>
		<author>
			<persName><forename type="first">K</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d mask face antispoofing with remote photoplethysmography</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep dynamic texture learning with adaptive channel-discriminability for 3d mask face anti-spoofing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<editor>IJCB</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning deep models for face anti-spoofing: Binary or auxiliary supervision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Reconstructing faces from their signatures using rbf regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Face id security</title>
		<ptr target="https://images.apple.com/business/docs/FaceIDSecurityGuide.pdf" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Apple Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From scores to face templates: a model-based approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Face recognition using local quantized patterns</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Napoléon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inverting face embeddings with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04189</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Synthesizing normalized faces from facial identity features</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Beyond eigenfaces: Probabilistic matching for face recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wahid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FG</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Biometric template protection: Bridging the performance gap between theory and practice</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">50 years of biometric research: Accomplishments, challenges, and opportunities</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Binary feature fusion for discriminative and secure multi-biometric cryptosystems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A hybrid approach for generating secure and discriminating face template</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint registration and representation learning for unconstrained face identification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Werghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A survey</title>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Face Detection and Facial Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pixel deconvolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06820</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face search at scale</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Clustering millions of faces by identity</title>
		<author>
			<persName><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Perceptual for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The feret evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cryptography and network security: principles and practice</title>
		<author>
			<persName><forename type="first">W</forename><surname>Stallings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Pearson</publisher>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Nips 2016 tutorial: Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00005</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Wasserstein gan</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Began: Boundary equilibrium generative adversarial networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Paul</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Do we really need to collect millions of faces for effective face recognition?</title>
		<author>
			<persName><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Trn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>in ECCV</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Srefi: Synthesis of realistic example face images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCB</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adversarial biometric recognition: A review on biometric system security from the adversarial machine-learning perspective</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Russu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Didaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">On the guessability of binary biometric templates: A practical guessing entropy based approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<editor>IJCB</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">The cmu multi-pose, illumination, and expression (multi-pie) face database</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<idno>TR-07-08</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>CMU Robotics Institute</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Random: Probability, mathematical statistics, stochastic processes</title>
		<ptr target="http://www.math.uah.edu/stat/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Probability Theory: A Complete One Semester Course</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dokuchaev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Selfnormalizing neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02515</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning to align from scratch</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A benchmark study of largescale unconstrained face recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCB</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning discriminability-preserving histogram representation from unordered features for multibiometric feature-fused-template protection</title>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese Academy of Sciences</title>
		<imprint>
			<date type="published" when="2010">2016. 2013. 2010</date>
			<pubPlace>Guangzhou, China; Hong Kong; Beijing, China; East Lansing</pubPlace>
		</imprint>
		<respStmt>
			<orgName>D degree in Computer Science from Hong Kong Baptist University ; Department of Computer Science and Engineering, Michigan State University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include biometric recognition, image processing and machine learning</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">He also serves as a Hong Kong Research Grant Council Engineering Panel Member. His current research interests include video surveillance, human face recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Editorial Board Member of Pattern Recognition, Associate Editor of IEEE Transactions on Information Forensics and Security and Senior Editor of SPIE Journal of Electronic Imaging</title>
		<imprint>
			<date type="published" when="2009">2009. 2017</date>
		</imprint>
	</monogr>
	<note>Yuen is a Professor of the Department of Computer Science in Hong Kong Baptist University. He was the head of the department from. biometric security and privacy</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
