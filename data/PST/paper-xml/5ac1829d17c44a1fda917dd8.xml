<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Non-Gaussian information from weak lensing data via deep learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-05-18">18 May 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Arushi</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">José</forename><forename type="middle">Manuel</forename><surname>Zorrilla Matilla</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Astronomy</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zoltán</forename><surname>Haiman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Astronomy</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<postCode>10027</postCode>
									<settlement>New York</settlement>
									<region>New York</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Non-Gaussian information from weak lensing data via deep learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-05-18">18 May 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">683F731BCBAFB18986E84532F36023BD</idno>
					<idno type="DOI">10.1103/PhysRevD.97.103515</idno>
					<note type="submission">Received 4 February 2018;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weak lensing maps contain information beyond two-point statistics on small scales. Much recent work has tried to extract this information through a range of different observables or via nonlinear transformations of the lensing field. Here we train and apply a two-dimensional convolutional neural network to simulated noiseless lensing maps covering 96 different cosmological models over a range of fΩ m ; σ 8 g. Using the area of the confidence contour in the fΩ m ; σ 8 g plane as a figure of merit, derived from simulated convergence maps smoothed on a scale of 1.0 arcmin, we show that the neural network yields ≈5× tighter constraints than the power spectrum, and ≈4× tighter than the lensing peaks. Such gains illustrate the extent to which weak lensing data encode cosmological information not accessible to the power spectrum or even other, non-Gaussian statistics such as lensing peaks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The analysis of multiple probes, including the cosmic microwave background (CMB) and large scale structure, has yielded very precise estimates for the parameters that define the standard cosmological model, ΛCDM <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Early fluctuations in the CMB evolved through gravitational instability and formed the structures we observe in the late universe. The evolution of the matter distribution in the universe encodes rich cosmological information that can be mined to test the standard model and constrain the possible values for its defining parameters.</p><p>Over 80% of the matter in the universe is nonbaryonic dark matter (DM), detectable through its gravitational effects. It contributes to gravitational lensing, distorting the shapes of background galaxies to an extent that is usually too small to be directly observed. Weak gravitational lensing (WL) can, nonetheless, be measured statistically through the correlation in the shapes of galaxies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. The lensed galaxies' redshifts allow the reconstruction of the matter density field's evolution <ref type="bibr" target="#b4">[5]</ref>, making WL one of the most promising cosmological probes. Lensing measurements and their analysis in a cosmological context are an essential part of experiments such as CFHTLenS [6], KiDS <ref type="bibr">[7]</ref>, the Dark Energy Survey [8], or HSC [9], and will be included in even wider (≈10× larger) surveys [Large Synoptic Survey Telescope (LSST) <ref type="bibr">[10]</ref>, the Euclid mission <ref type="bibr">[11]</ref>, and the Wide Field Infrared Survey Telescope <ref type="bibr">[12]</ref>].</p><p>The large volume of upcoming data sets raises the question of how to extract all the cosmological information encoded in them. Nonlinear gravitational collapse distorts the Gaussian character of the initial fluctuations. Thus, twopoint statistics are insufficient to characterize weak lensing data and additional descriptors have been considered to extract additional information <ref type="bibr" target="#b5">[13]</ref>. An alternative approach is to transform the data so that nonlinearities become less important and it is easier to recover the information encoded in the transformed field (e.g., with the power spectrum). Logarithmic transformations have been proposed for the three-dimensional (3D) matter density field <ref type="bibr" target="#b6">[14]</ref> and the two-dimensional (2D) convergence <ref type="bibr" target="#b7">[15]</ref>, as well as other local, Gaussianization transformations <ref type="bibr" target="#b8">[16]</ref>.</p><p>Overall, non-Gaussian statistics such as lensing peaks and moments involving gradients of the convergence field are promising, since they can improve parameter errors by a factor of 2-3 compared to using only second-order statistics <ref type="bibr" target="#b9">[17]</ref><ref type="bibr" target="#b10">[18]</ref><ref type="bibr" target="#b11">[19]</ref><ref type="bibr" target="#b12">[20]</ref><ref type="bibr" target="#b13">[21]</ref><ref type="bibr" target="#b14">[22]</ref><ref type="bibr" target="#b15">[23]</ref><ref type="bibr" target="#b16">[24]</ref>. It is not clear where the extra information lies, or if all of it is accessible <ref type="bibr" target="#b17">[25]</ref>. It has been investigated and partially understood only for lensing peaks, which derive some (but not all) information from underlying collapsed DM halos <ref type="bibr" target="#b18">[26]</ref><ref type="bibr" target="#b19">[27]</ref><ref type="bibr" target="#b20">[28]</ref>. This halo-peak connection has inspired the development of approximate analytic models for peak counts <ref type="bibr" target="#b21">[29,</ref><ref type="bibr" target="#b22">30]</ref>.</p><p>All these statistics compress the information in the original data set, typically a map representing a noisy estimate of the projected matter density field, into a lowdimensional descriptor that can be used to infer the parameters that determine how the data were generated. An alternative approach is to use deep learning techniques, which have proven successful in a wide range of areas <ref type="bibr" target="#b23">[31]</ref> to infer cosmological parameters directly from the uncompressed raw data.</p><p>Artificial neural networks (NNs) are pattern recognition algorithms, in which series of processing nodes, capable of performing simple operations, are connected to each other in a network. The nodes of a NN are typically arranged in layers, with nodes in one layer connected to those in the next. Information is fed to the NN through the input layer, its outcome comes from the output layer, and all intermediate steps are called "hidden" layers. The strength of the connections is stored in a series of weights that can be adjusted to match a given output; this process is called "learning." This quality allows the use of NNs for forecasting and inference. While we do not have a full understanding on what drives NNs predictive power <ref type="bibr" target="#b24">[32]</ref>, they have been successfully used in astronomy, from source detection and classification to light curve analyses and even adaptive optics control (see reviews on NNs in astronomy in <ref type="bibr" target="#b25">[33,</ref><ref type="bibr" target="#b26">34]</ref>).</p><p>Convolutional neural networks (CNNs) are particularly well suited to work on data sets with spatial information, such as images, since the connection of their convolution layers' nodes to subsets of the data take advantage of the high correlation of nearby points imprinted by the locality of physical processes. Recently they have been used to infer cosmological parameters from the 3D matter density field <ref type="bibr" target="#b27">[35]</ref> and have been found to outperform constraints estimated from its power spectrum. Weak lensing provides (in principle) an unbiased map of the projected matter distribution. One of the aims of this study is to assess if neural networks overperform relative to the power spectrum when analyzing 2D WL data, as they do for the 3D matter field. Similar techniques have also been used to generate data with the same statistical properties as the output of physically motivated simulations <ref type="bibr" target="#b28">[36,</ref><ref type="bibr" target="#b29">37]</ref>.</p><p>A similar study has recently applied convolutional neural networks to weak lensing data for inference <ref type="bibr" target="#b30">[38]</ref>. Our study shares the same motivation and reaches similar conclusions, but has some differences. While <ref type="bibr" target="#b30">[38]</ref> focused on the ability of deep learning techniques to differentiate between models along a known fΩ m ; σ 8 g degeneracy, Σ 8 ≡ σ 8 ðΩ m =2Þ 0.6 <ref type="bibr" target="#b31">[39]</ref>, we focus on the parameters' constraints that can be inferred by extracting information through neural networks. To do so we trained our networks on a set of 96 cosmological models covering a large region of the parameter space (see Fig. <ref type="figure" target="#fig_0">1</ref> for the distribution of those models). Furthermore, we used different simulation techniques, the architecture of our network is different, and we compared the neural network to a different set of observables (power spectrum and lensing peaks, instead of skewness and kurtosis). Finally, we restricted our analysis to noiseless data, leaving the analysis of the effect of shape noise for a follow-up study.</p><p>The paper is organized as follows, in Sec. II we describe how we generated the data used to train and test the CNN, the architecture and training of the network, and the summary statistics used as benchmarks. In Sec. III we compare the performance of the CNN to that of alternative summary statistics, in terms of its predictive accuracy and the cosmological constraints that can be inferred. In Sec. IV we discuss the implications of our results, and in Sec. V we summarize our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATA</head><p>The goal of this paper is to assess the performance of CNNs predicting cosmological parameters from WL data. We do so comparing the network's predictions with those that can be inferred from statistics measured on the maps, as well as the credible regions that can be inferred around the predicted parameters. In this section, we describe how the WL data used was generated, we describe the design and training of the CNN, and we describe the summary statistics measured on the WL data: the power spectrum and lensing peaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Mock convergence maps</head><p>Our initial data set consists of mock convergence (κ) maps generated assuming 96 different values for the matter density Ω m and the scale of the initial perturbations normalized at the late universe, σ 8 (see Fig. <ref type="figure" target="#fig_0">1</ref>). We adjusted the dark energy density to enforce flatness, Ω DE ¼ 1.0 -Ω m , and kept the rest of the parameters constant: baryon density (Ω b ¼ 0.046), Hubble constant (h ¼ 0.72), scalar spectral index (n s ¼ 0.96), effective number of relativistic degrees of freedom (n eff ¼ 3.04), and neutrino masses (m ν ¼ 0.0). We singled out the cosmology with fΩ m ¼ 0.260; σ 8 ¼ 0.800g as a fiducial to compute the covariance of the observables used to assess the performance of the CNN (see Sec. II C). The density of the model sampling increases toward the fiducial and shows some correlation with the direction of the Σ 8 degeneracy, Σ 8 ¼ σ 8 ð Ω m 0.3 Þ 0.6 , as can be seen in Fig. <ref type="figure" target="#fig_0">1</ref>. We refer the reader to <ref type="bibr" target="#b19">[27]</ref>, where this suite of simulations was also used, and our pipeline LENSTOOLS <ref type="bibr" target="#b32">[40]</ref> for a detailed description of our sampling algorithm and simulation processing. We provide a summary here for convenience.</p><p>We evolved the matter density field using the N-body code GADGET2 <ref type="bibr" target="#b33">[41]</ref>. For each cosmology we simulated a single volume from initial conditions computed with CAMB <ref type="bibr" target="#b34">[42]</ref>. The simulation boxes are cubes with a side length of 240h -1 Mpc, large enough to cover the maps' field of view of 3.5 × 3.5 deg 2 to a redshift of z ≈ 3.0. Each box is populated with 512 3 DM particles, yielding a mass resolution of ≈10 10 M ⊙ .</p><p>We ray-traced the outputs of our simulations following the multiple lens plane algorithm <ref type="bibr" target="#b35">[43]</ref>. It has been shown that while the Born approximation is sufficient for an accurate estimation of the power spectrum even in the largest planned future WL surveys, full ray-tracing is necessary to avoid biased estimations for the counts of lensing peaks and higher order statistics <ref type="bibr" target="#b36">[44]</ref>. The value of κ for each of our maps' pixels is derived from the deflection experienced by a light ray as it crosses a series of lens planes stacked to form its past light cone. For this study, we considered all the lensed galaxies located at a single fixed redshift of z ¼ 1.0. Each resulting map has 1024 × 1024 pixels, and was sliced in 16 smaller patches of 256 × 256 pixels each to speed up the neural network's training (Sec. II B).</p><p>Each lens plane was generated from the snapshot corresponding to its redshift by cutting a 80h -1 Mpc slab along one of its axes, estimating the matter density on a 4096 × 4096 grid, and solving the Poisson equation in 2D for the gravitational potential. By cutting different slabs, combining different planes at each redshift, and randomly translating and rotating them, we ultimately generated 512 independent κ maps from a single simulation box for each cosmology. Through this recycling process, it is possible to generate up to ≈10 4 independent realizations of the convergence field from a single N-body simulation <ref type="bibr" target="#b37">[45]</ref>. The resulting unsmoothed, noiseless convergence maps are analogous to a 2D version of the data set used in <ref type="bibr" target="#b27">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural network training and architecture</head><p>Neural networks consist of interconnected nodes (or neurons) arranged in layers. Each neuron transforms a linear combination of its inputs through an "activation" function, fðWxÞ, where W is a matrix of weights and x a vector of inputs (in our case, the latter contains the values of the convergence in the pixelized 2D lensing map). The inputs can come from other neurons in the network, or from external data. The activation function is usually nonlinear (e.g., a sigmoid function). The weights used to linearly combine the inputs can be adjusted to minimize a loss function, in a process that is called "training" or "learning." Some of the layers in the neural network used for this study convolve their input data with a kernel whose values are fitted during training. The resulting "convolutional neural network" takes advantage of the correlations between neighboring pixels and has been shown to yield good results when analyzing natural images.</p><p>Each "labeled example" the network is exposed to is a 1024 × 1024 map coupled with the fΩ m ; σ 8 g "label" that corresponds to the cosmology used to generate that map. From each such example, we created 16 labeled examples by slicing the map into smaller, 256 × 256 maps. And these are the maps used as input for the neural net. This operation reduced the number of nodes in the CNN and, consequently, its training time. We do not expect the performance of the network to be adversely affected, because of the limited constraining power of the modes that are small enough to be captured by the full maps but not their slices, i.e., spherical harmonic indices in the range l ∈ ½100; 400 (see, e.g., Ref. <ref type="bibr" target="#b38">[46]</ref>, for a demonstration that most of the information is on smaller scales). The prediction for each 1024 × 1024 map is the mean of the predictions for the 16, 256 × 256, maps that were sliced from the original, bigger map. Our whole data set amounts to 96 different cosmological models, each having 512, 1024 × 1024, independent maps. We trained the neural networks using 70% of our data, and set aside the remaining 30% to test their performance.</p><p>The architecture of the CNN was inspired by that used in <ref type="bibr" target="#b27">[35]</ref>. We sketch the architecture in Fig. <ref type="figure">2</ref> and summarize its elements in Table <ref type="table">I</ref>. The network is a combination of convolutions (transformed by a nonlinear "activation" function) and pooling layers that reduce the spatial dimensionality of the output, followed by fully connected layers in charge of the high-level logic. For the convolutional layers, we chose a 3 × 3 kernel for speed. Each convolution layer applies more than one filter to its input in sublayers. The weights (filter values) are the same for all the neurons within a sublayer. This parameter sharing reduces the number of weights to fit during training and is a reasonable choice given the data's translational and rotational symmetries.</p><p>The first layer convolves any input map with four different filters and applies the activation function to the resulting four feature maps. Each filter is defined by 10 parameters (9 determine the convolution kernel plus an overall additive bias). In total, 40 weights need to be adjusted during training for the first layer. The second layer downsamples the feature maps from the first layer substituting 2 × 2 consecutive pixels by their mean ("average pooling"). The third and fourth layers are convolutional layers, and each applies 12 different kernels to all incoming feature maps, including all depth levels from the previous layer. While the convolution is a linear operation, the application of the activation function breaks the linearity. The number of tunable weights grows with each layer as new kernels are added. Another average pooling layer (layer 5) is followed by two sets of convolution plus average pooling (layers 6-9).</p><p>At each layer, we can consider the neurons arranged along three dimensions, two that follow the spatial dimensions of the feature maps fed into the layer (width and height) and another that grows with the number of filters used to process the layer's input (depth). As information flows through the network, the spatial dimensions of the feature maps shrink and the depth of nodes processing those maps grows. The convolutional layers 6 and 8 do not apply an activation function to their output. Another average pooling (layer 10), followed by a flattening layer (layer 11) reduce the spatial dimensionality to unity, with a depth of 2304. TABLE I. Summary of the neural network's architecture. Convolutional layers increase the depth of the network by applying different filters (sublayers) to the same input. The number of neurons in a layer is determined by the dimension of its output. The number of weights for a convolutional layer is given by F out ðF in × 9 þ 1Þ, where F out is the number of feature maps that the layer outputs and F in is the number of feature maps the layer is fed with. A fully connected layer is defined by ðN in þ 1Þ × N out weights, where N in is the number of nodes in the previous layer and N out the number of nodes in the fully connected layer. The network consists of a series of convolutional and (average) pooling layers. Layers increase their "logical" dimension (depth), while reducing their "spatial" dimensions (width and height). Once the spatial dimension has been reduced to unity (flattening), a series of fully connected layers further reduces the number of nodes to two, the required number of outputs. The activation function for the neurons is a leaky rectified linear unit. For clarity, only a few layers are displayed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head><p>A series of fully connected layers (layers 12, 14, and 16) are followed by dropout layers (layers 13, 15, and 17) that shrink the depth of the output. The final fully connected layer (layer 18) outputs the estimated values for Ω m and σ 8 , which are compared with their true value through the loss function to adjust the weights in the network through backpropagation.</p><p>The total number of parameters to be fitted during training is ≈2.6 × 10 6 , a large number but very small compared with the total number of pixels in the training data set (≈3.6 × 10 10 ).</p><p>The adopted activation function is the "leaky rectified linear unit" (LeakyReLU), with a leak parameter of 0.03, within the range suggested in <ref type="bibr" target="#b39">[47]</ref>,</p><formula xml:id="formula_0">fðxÞ ¼ x if x ≥ 0 0.03x if x &lt; 0 :<label>ð1Þ</label></formula><p>This function helps mitigate the "dying" ReLU problem, in which a neuron gets stuck in a region of zero gradient <ref type="bibr" target="#b40">[48]</ref>.</p><p>To prevent overfitting, we enforced regularization applying "dropout" at the fully connected layers: the output of any neuron was ignored with a 50% chance <ref type="bibr" target="#b41">[49]</ref>. This process took part only during training, and the output from the nodes that were not dropped out was doubled to compensate for the ignored neurons.</p><p>We used two loss functions to minimize during the training of our neural networks. The first one is the sum of the absolute error on Ω m and σ 8 , computed over batches of 32 maps each, in which the data are split for each pass of the training examples:</p><formula xml:id="formula_1">X map∈batch jσ pred 8 -σ true 8 j þ jΩ pred m -Ω true m j:<label>ð2Þ</label></formula><p>This is a popular choice and converges faster than the sum of the squares of errors because its gradient does not necessarily cancel near zero. Due to the heterogeneous sampling in parameter space of our simulated models, the network is exposed to fewer examples from cosmologies in sparsely sampled regions. This can induce a bias in the predictions. To assess the impact of the nonuniform sampling on parameter constraints, we also used a weighted loss function:</p><formula xml:id="formula_2">X map∈batch W cosmo ðjσ pred 8 -σ true 8 j þ jΩ pred m -Ω true m jÞ;<label>ð3Þ</label></formula><p>where W cosmo is a weight inversely proportional to the sampling density at the location of a cosmological model in parameter space. Errors in predictions for maps from cosmologies in sparsely sampled regions are more severely penalized than those for maps from densely sampled regions. We show in Sec. IV C that such a weighted loss function reduces the bias in the predictions, at the cost of a longer network training, but has only a limited impact on the parameter constraints inferred from the predictions. The algorithm used to minimize the loss function was an Adam optimizer <ref type="bibr" target="#b42">[50]</ref> with a learning rate of 10 -4 and first and second moment exponential decay rates of 0.9 and 0.999, respectively.</p><p>We trained each network until the loss function converged, which took in most cases 5 epochs (an epoch is a pass of all the training examples in the data set). The training maps were split in batches and randomly reshuffled after each epoch. The networks' weights were recomputed after each batch, minimizing the total loss over the 32 tiles. Each batch took 40-50 s on a NVIDIA K20 GPU with 5 GB of on-board memory, at the NSF XSEDE facility [51]. To further reinforce the rotation invariance of the data set, all maps were rotated 90 deg with a 50% probability before feeding them to the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Alternative descriptors</head><p>In order to assess the performance of the CNN, we compared the accuracy of its predictions with that achieved through analysis of summary statistics. We used two observables, the power spectrum and lensing peak counts. Both compress the information available in a given WL map in a data vector of dimension small compared with the number of pixels in the original map.</p><p>The power spectrum is defined as the Fourier transform of the two-point correlation function of κ <ref type="bibr" target="#b3">[4]</ref>, hκðlÞκ Ã ðl 0 Þi ¼ ð2πÞ 2 δ D ðl -l 0 ÞPðlÞ: ð4Þ</p><p>In the above expression δ D is the Dirac delta function and l is the 2D angular wave vector. We measured the power spectrum on all 512 mock κ maps for each of the 96 cosmological models. We evaluated the power spectra on 20 bins, logarithmically spaced in the interval l ∈ ½1 × 10 2 ; 7.5 × 10 4 . The minimum angular scale (maximum wave number l) is set to prevent any loss of information at the pixel level. The finite resolution of our simulations results in deviations from theory at wave numbers l &gt; 5 × 10 3 with a significant loss of power for l ≈ 10 4 , as Fig. <ref type="figure">3</ref> shows for the fiducial cosmology.</p><p>The power spectrum is a widely used observable in cosmology, mainly because it fully characterizes Gaussian random fields and is a well-developed analytic tool. While the initial conditions for the matter perturbations are Gaussian (or nearly so), nonlinear evolution introduces significant non-Gaussianities in the matter density field at late times.</p><p>Lensing peaks are local maxima in the κ field. In the absence of ellipticity noise, they probe high density regions, where nonlinear effects become relevant. We chose the peaks' count as a function of their κ value as a second observable because they are sensitive to information not captured by the power spectrum. As an illustration, we compare in Fig. <ref type="figure">4</ref> the average peak counts measured on the 512 mock maps generated for the fiducial cosmology to those measured over Gaussian random fields (GRFs) that share their power spectra with the κ maps. That is, for each convergence map, we measured its power spectrum, built a GRF from it, and measured the number of peaks in this new field. The distribution is clearly different, the peak histogram from convergence maps exhibiting a high κ tail resulting from the nonlinear growth of structures.</p><p>Peak counts yield tighter constraints than the power spectrum <ref type="bibr" target="#b9">[17]</ref><ref type="bibr" target="#b10">[18]</ref><ref type="bibr" target="#b11">[19]</ref><ref type="bibr" target="#b12">[20]</ref><ref type="bibr" target="#b13">[21]</ref><ref type="bibr" target="#b14">[22]</ref><ref type="bibr" target="#b15">[23]</ref><ref type="bibr" target="#b16">[24]</ref> and constitute a good benchmark for other methods which aim at extracting additional cosmological information. We counted the peaks in 20 bins, linearly spaced. We set the upper and lower limits of the bins to ½κ min ¼ -2.0; κ max ¼ 12.0, in units of the mean κ root mean square (rms) for the fiducial maps, to fully cover the range of peaks present in the data; this corresponds to κ min ≈ -0.03, κ max ≈ 0.19, and a bin width of Δκ ≈ 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS</head><p>We assessed the CNN's performance in terms of the precision of their predictions for the cosmological parameters, and the constraints for those parameters for a given observation. The left and center panels of Fig. <ref type="figure">5</ref> display the predictions for Ω m and σ 8 as a function of their "ground truth," that is, the values that correspond to the cosmologies used to generate the data. The right panel shows the same comparison for the derived Σ 8 ≡ σ 8 ðΩ m =0.3Þ 0.6 along the degeneracy between both parameters. Each point corresponds to one of the ≈150 test maps available for each of the 96 cosmologies. For the neural network, the predicted fΩ m ; σ 8 g for a given map are the average values for the network's output when fed the 16 tiles in which the map was sliced. For the power spectrum and peak counts, the predictions are the values that minimize χ 2 for that map. We estimated χ 2 for each of the 96 sampled cosmologies as</p><formula xml:id="formula_3">χ 2 ij ¼ ðd i -dj Þ d C -1 fid ðd i -dj Þ;<label>ð5Þ</label></formula><p>where d i is the data vector measured on map i (binned power spectrum or peak counts), dj is the mean of the same descriptor for the model j, and d C -1 fid is the precision matrix for the data vector evaluated at the fiducial model. We used all 512 available maps per model to evaluate both the mean descriptor and the precision matrix, as in any realistic scenario in which a survey provides a mass map where all the simulated data would be used for inference. We corrected for the bias in the precision matrix following <ref type="bibr" target="#b45">[54]</ref> </p><formula xml:id="formula_4">d C -1 fid ¼ N -d -2 N -1 C -1 fid :<label>ð6Þ</label></formula><p>N is the number of realizations used to estimate the covariance (512) and d is the dimension of the data vector (20, the number of bins). The 96 χ 2 ij values were used to interpolate χ 2 ðΩ m ; σ 8 Þ and find its minimum. We used a Clough-Tocher interpolator that builds a continuously differentiable piecewise cubic surface over a nonuniform grid <ref type="bibr" target="#b46">[55,</ref><ref type="bibr" target="#b47">56]</ref>. The minimum was found using the downhill simplex algorithm <ref type="bibr" target="#b48">[57]</ref>. FIG. <ref type="figure">4</ref>. Comparison of peak counts derived from maps generated via our ray-tracing N-body simulations to those derived from Gaussian random fields (GRFs) with the same power spectrum. FIG. <ref type="figure">3</ref>. Comparison of the average convergence power spectrum for the fiducial κ maps with predictions from linear and nonlinear theory. The theoretical curves were computed using NICAEA <ref type="bibr" target="#b43">[52]</ref>, with the revised Halofit parameters from <ref type="bibr" target="#b44">[53]</ref> for the nonlinear power spectrum.</p><p>We verified that the results for the power spectrum and lensing peaks do not change when these observables are measured in a different number of bins (as long as they are more than ≈10) or a different interpolator is used to find the minimum of χ 2 ðΩ m ; σ 8 Þ.</p><p>For all cosmologies, the neural network is significantly more precise than both the power spectrum and lensing peaks: the scatter in its predictions for a given model is smaller. On average, the standard deviation of the CNN's predictions is a factor of 4-7 lower than that of the statistical descriptors, and up to ≈16× smaller for the fiducial (see Table <ref type="table" target="#tab_1">II</ref>). In terms of accuracy (i.e., how close the predictions are to the ground truth), the network shows some bias that may degrade the constraints that can be inferred from the network's predictions.</p><p>We note the presence of a small set of maps from models close to the fiducial for which both the power spectrum and lensing peaks tend to overpredict σ 8 and Σ 8 as a result (the outliers on both panels correspond to the same maps). These maps form a clearly detached clump on the rightmost panel of Fig. <ref type="figure">5</ref>, where a dashed rectangle highlights their location. They represent ≈4% of the maps for ≈28 cosmologies not far from the fiducial model. We found through visual inspection that this overprediction seems to be due to an anomalous number of structures projected in the field of view. Interestingly, the CNN seems to be immune to such chance projections and classifies these maps correctly. This suggests that the neural network extracts different information from the maps than the power spectrum or lensing peaks. Alternatively, these fluctuations may be the result from cosmic variance, and the neural network may be underweighting those effects.</p><p>For a few cosmologies, parameter predictions from the CNN converged at different values from those of neighboring models. This is noticeable in the leftmost panel of Fig. <ref type="figure">5</ref> where a few red points show a relative overprediction in Ω m in the range Ω m ∈ ð0.2; 0.4Þ. These outliers correspond to points in sparsely sampled areas near the boundaries of the explored parameter space. This highlights the importance of a well-sampled parameter space for the neural network to generalize accurately. In Appendix B we analyze the effect of sampling on the predictions and credible contours inferred from the neural network. As these outliers lie far from the fiducial cosmology, they do not alter the parameter constraints presented in this study. Furthermore, they are identifiable in the training data, and as such could be removed if needed. We did not remove any model from our data set even when it was evident from the training data that they could be outliers.</p><p>The relevant metric to compare the performance of the neural network relative to summary statistics is the probability distribution for the cosmological parameters given our data. This posterior distribution is related to the easierto-compute probability of measuring a specific data vector given the cosmological parameters, or likelihood, by Bayes  simulations of ΛCDM cosmologies. For the CNN, we define our data vector as the predicted values for the cosmological parameters, ðΩ m ; σ 8 Þ, and for the alternative statistics, the measured binned power spectra and peak histograms described in Sec. II C. The term that multiplies the likelihood, or prior pðp; MÞ, and that on the denominator, or evidence, are the same when using the neural network or the statistical descriptors. The reason is we are using the same convergence maps from the same sampling of the parameter space. We can drop them as a normalization factor, as well as the explicit dependence on the underlying model used to generate the κ maps, and compare directly the likelihoods derived from the different methods. For the likelihoods, we assumed a Gaussian distribution,</p><formula xml:id="formula_5">pðdjpÞ ∝ exp - 1 2 ðd -dðpÞÞ T d C -1 fid ðd -dðpÞÞ ;<label>ð8Þ</label></formula><p>with a precision matrix d C -1 fid evaluated at the fiducial cosmology and as an expected value for the data, dðpÞ, the mean value measured from the simulations for the cosmology defined by p.</p><p>Since we use the same covariance matrix for all cosmologies, we do not need to include the normalization prefactor. For the power spectrum, we expect the Gaussian likelihood to be accurate. Our simulated maps cover a small field of view of 3.5 × 3.5 deg 2 on which the power spectrum can be measured only for relatively high l &gt; 100. At those scales, many modes contribute to each measurement of the power spectrum, and the central limit theorem shows that its probability distribution function should converge to a Gaussian <ref type="bibr" target="#b49">[58]</ref>. For the lensing peaks and predictions from the neural network, we verified that the approximation remains valid (see Appendix A). The alternative approach of estimating the probability density using a kernel density estimator (KDE) depends on the width of the kernel chosen, and the estimates for a large dimensional data vector such as our power spectra are noisy due to the relative limited number of independent κ map realizations.</p><p>To compute the likelihood, we used as data (observation) the average observable for the fiducial cosmology. For the power spectrum and lensing peaks, all 512 maps were used to estimate the means for each cosmology, and the covariance matrix for the fiducial. For the neural network, only the test maps were used (≈150 per cosmology). We display the 68% and 95% credible contours for the likelihoods computed for the power spectrum, lensing peaks, and neural network in the central panel of Fig. <ref type="figure">6</ref>, and the marginalized distributions for Ω m and σ 8 in the upper and right panels, respectively. At each point in parameter space, the expected data vector is interpolated linearly from the mean data vectors for the simulated cosmologies. Due to the choice of measurement (the predicted mean for the fiducial) all likelihoods peak at the true values for the fiducial cosmology. This is true also for the neural network. The smaller scatter in the CNN predictions translates into tighter parameter constraints, by a factor of ≈2 compared with lensing peaks and ≈6 compared with the power spectrum (see Table <ref type="table" target="#tab_1">III</ref>). The neural network seems capable of extracting more information from noiseless convergence maps than alternative methods such as the power spectrum or lensing peaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Non-Gaussian information extracted by the neural network</head><p>The significantly tighter constraints obtained by the CNN, shown in Fig. <ref type="figure">6</ref>, are encouraging and an indication that weak lensing maps encode more information than what is usually used for inference. Neural networks are capable FIG. <ref type="figure">6</ref>. The 68% and 95% credible contours for unsmoothed (≈0.2 arcmin=pixel) κ maps, derived from the power spectrum (blue curve), lensing peak counts (green curve), and neural network predictions (red curve). The true values for the parameters, fΩ m ¼ 0.260; σ 8 ¼ 0.800g are indicated by black dotted lines. The upper and right panels show the distribution marginalized over the other parameter. TABLE III. Area of the 68% and 95% fΩ m ; σ 8 g credible contours, relative to those obtained from the output of the neural network for unsmoothed, noiseless κ maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN Power spectrum Peak counts</head><p>Area 68 1 5.9 1.9 Area 95 1 6.1 1.9</p><p>of extracting some of it, at least more than the power spectrum and even more than some non-Gaussian statistics such as lensing peaks. Given the large number of parameters that need to be fitted during training, there is the risk that the gain in precision comes from a form of overfitting, in the general sense of making predictions based on irrelevant information <ref type="bibr" target="#b50">[59]</ref>.</p><p>For instance, a GRF is fully determined by its power spectrum. As a result, no other statistic or method used to extract information from it should outperform the power spectrum. To test whether the neural network satisfies this limit, we built a collection of GRFs and used it as a new data set to train and test the CNN's architecture. We generated the GRFs by Fourier transforming random fields with a Gaussian distribution defined by the power spectra measured over the κ maps ray-traced from the outputs of cosmological N-body simulations. The new suite, which has a one-to-one correspondence with the original data, has no information encoded beyond the power spectrum.</p><p>The 68% and 95% credible contours from the power spectrum, the lensing peaks, and the newly trained CNN, as well as the marginalized distributions for Ω m and σ 8 are displayed in Fig. <ref type="figure">7</ref>, which is analogous to Fig. <ref type="figure">6</ref> but for GRFs instead of κ maps from N-body simulations.</p><p>As before, the likelihoods peak on the true parameter values for the fiducial and the contours appear centered around fΩ m ¼ 0.260; σ 8 ¼ 0.800g. The likelihood for the power spectrum is the same as the one computed for the convergence maps. The likelihoods for lensing peaks and the neural network are different, and their contours larger than those derived from the power spectrum. In particular, the contours from lensing peaks are 1.7ð1.4Þ× larger for the 68% (95%) contours, and those from the neural network 2.6 × ð2.0×Þ larger. This result is consistent with the absence of information beyond the power spectrum in the Gaussian random fields, and demonstrates that the small scatter in the parameters' predictions from the neural network trained on convergence maps is not the result of a tendency to overfitting by its architecture or other spurious effects.</p><p>Comparing the fΩ m ; σ 8 g predictions with the ground truth for the test GRFs, as was done in Sec. III for the κ test maps, we see that there is both an increase in the scatter and the bias of the neural network's predictions (see Fig. <ref type="figure">8</ref>). Both effects drive the deterioration in the parameter constraints that can be inferred from those predictions. Furthermore, the neural network seems almost insensitive to Ω m , as the predictions for all the test GRFs scatter around the median Ω m for the 96 cosmologies. The CNN cannot easily distinguish between models with different Ω m and defaults to the value that minimizes the loss function. The FIG. <ref type="figure">7</ref>. Same as Fig. <ref type="figure">6</ref>, except using the Gaussian random fields, rather than the ray-tracing simulations. The network was trained with the unweighted loss function [Eq. ( <ref type="formula" target="#formula_1">2</ref>)].</p><p>FIG. <ref type="figure">8</ref>. Same as Fig. <ref type="figure">5</ref>, except using the Gaussian random fields, rather than the ray-tracing simulations. The network was trained with the unweighted loss function [Eq. ( <ref type="formula" target="#formula_1">2</ref>)].</p><p>use of an unweighted loss function in this analysis may also have some influence, but the same behavior is not seen on σ 8 . The power spectrum and lensing peaks are both sensitive to that parameter, indicating that they extract different information than the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Effect of the smoothing scale on the results</head><p>The angular resolution of the mock convergence maps used for our analysis is ≈0.2 arcmin per pixel. This high resolution is interesting from an academic perspective, but at present it is of little practical interest. Accurate shear estimates require measuring the shape of many galaxies to estimate their correlations. For instance, the upcoming LSST survey will reach an effective number of galaxies of ≈26 arcmin -2 , after considering losses due to blending and masking <ref type="bibr" target="#b51">[60]</ref>. This means that ≈1 arcmin is characteristic of the resolution achievable by future surveys. Furthermore, at small scales (l &gt; 10 4 ), baryonic physics alter the matter distribution and can bias WL observables relative to estimates from DM-only simulations <ref type="bibr" target="#b52">[61,</ref><ref type="bibr" target="#b53">62]</ref>.</p><p>To assess whether the neural network still outperforms alternative observables on ≈1 arcmin resolution data, we trained a new network with the same architecture on the κ maps after smoothing them with a Gaussian kernel. The resulting constraints, for a smoothing scale of 1 arcmin, are displayed in Fig. <ref type="figure">9</ref>.</p><p>The parameters' constraints degrade for all three methods. In principle, we would expect the non-Gaussian statistics' performance to degrade relative to the power spectrum as small scale features are smoothed away from the κ maps. Up to 1 arcmin smoothing, the neural network keeps well its relative advantage to the power spectrum, yielding credible regions 5.6 × ð4.8×Þ smaller at the 68% (95%) level. Lensing peaks are more adversely affected than the CNN by smoothing, yielding contours that are only 1.6 × ð1.5×Þ smaller than the power spectrum. This would indicate that any additional information extracted by the neural network is not confined to very small angular scales.</p><p>The first attempt at training the neural network on smoothed data failed. To guarantee the convergence in the training process, we gradually smoothed the κ maps in a similar way as <ref type="bibr" target="#b30">[38]</ref> added noise to theirs. We fed the network with maps of growing smoothing scale, starting with a kernel of 0.2 arcmin of bandwidth. Once the network reached convergence at a smoothing scale, the kernel's bandwidth was increased by 0.05 arcmin and the network retrained. In all cases the neural network kept its advantage (see Table <ref type="table" target="#tab_2">IV</ref>). The ratio between the areas of the credible regions derived from the power spectrum and the neural network remained roughly constant, while the same ratio for the lensing peaks and neural network increased as the capability of peaks to extract information degraded faster with larger smoothing scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Bias in the CNN predictions</head><p>The parameter predictions from the neural network exhibit some bias (see Fig. <ref type="figure">5</ref>). The bias is more severe when an unweighted loss function is used, as can be seen in Fig. <ref type="figure" target="#fig_2">10</ref>. This can be due to the loss function being dominated by errors in the densely sampled regions of the parameter space.</p><p>Weighting the loss function according to the sampling density helps mitigate the bias. The effect is larger for the high-Ω m region than for the high-σ 8 models. This can be FIG. 9. Same as Fig. <ref type="figure">6</ref>, except smoothing the maps from the ray-tracing simulations with a Gaussian kernel of 1 arcmin of width. The network was trained with the unweighted loss function [Eq. ( <ref type="formula" target="#formula_1">2</ref>)]. due to the difference in sampling between both regions. The high-Ω m region, corresponding to quadrant II in Fig. <ref type="figure" target="#fig_0">1</ref> has more models further from the fiducial and with large spacing between them than the high-σ 8 region (quadrant I).</p><p>The weights in the loss function were computed using a KDE to estimate the sampling density in parameter space. The KDE bandwidth used was 1.0, a value that yielded a smooth estimate.</p><p>Biases in predictions from neural networks have been found in other works (e.g., <ref type="bibr" target="#b27">[35]</ref>), so we cannot guarantee that the heterogeneous sampling of our data is the only source of the bias. Future work using a different data set, uniformly sampled, will address this issue.</p><p>The parameter constraints for an observation near the fiducial model are not affected by the use of an unweighted loss function, as Fig. <ref type="figure" target="#fig_0">11</ref> illustrates. This is because the scatter of the predictions in densely populated areas does not increase significantly when the bias in the sparsely sampled areas is reduced with a modified loss function. We did not retrain our networks with a weighted loss function due to the additional computational cost, since the constraints from the network's predictions are essentially unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We trained a convolutional neural network on simulated, noiseless, weak lensing convergence maps. We demonstrated that neural networks can outperform methods based on traditional observables such as the power spectrum, or even statistics previously shown to extract non-Gaussian information, such as lensing peaks. On data smoothed at 1 arcmin scales, within reach of upcoming surveys, the neural network outperformed the power spectrum by a factor of ≈5 and the lensing peaks by a factor of ≈4 (using the area of the confidence contour in the fΩ m ; σ 8 g plane as a figure of merit).</p><p>We performed null tests to verify that the improvement in the parameter constraints reflects the network's ability to extract additional information present in the WL data and is not a numerical artifact (for instance, some form of overfitting). This sets a lower limit to the cosmological information encoded in noiseless lensing maps, and whether this is also the case in more realistic, noisy data sets remains an open question. The network's constraints are limited by both the precision and bias of its predictions. Whether further improvements are reachable through a different network architecture, or a richer training data set, remains an open question and calls for further investigation.</p><p>Our results are consistent with previous findings in <ref type="bibr" target="#b27">[35]</ref> for the 3D matter power spectrum and in <ref type="bibr" target="#b30">[38]</ref> for the ability of neural networks to distinguish WL data generated from different cosmologies. Some of the questions that future work will address are the following:</p><p>(i) Effect of noise on predictive power. The presence of realistic levels of noise (e.g., shape noise) can pose challenges to neural network training <ref type="bibr" target="#b30">[38]</ref>. It remains to be shown if the ≈5× improvement in parameter constraints compared with the power spectrum is achievable with noisy data. (ii) Propagation of systematics on constraints from neural networks. Before neural networks can be used to infer parameters from weak lensing data, we need to understand the effect of the systematics present in the data on the resulting parameter constraints. (iii) Scaling with survey area. Since neural networks' training time steeply increases with the map size, it is important to assess how the constraining power from their predictions scale with map size, and how the scaling compares with that for alternative methods such as the power spectrum. (iv) Network analysis. While the interpretation of feature maps from deep networks (see <ref type="bibr" target="#b30">[38]</ref>) is not straightforward, it may provide valuable insights to design new summary statistics capable of extracting cosmological information from lensing observations. (v) Improvements in the network's training and architecture. An extended exploration of training parameters (density of models in parameter space, number of independent examples per model, loss function, etc.) and architecture's features (convolutional kernel size, number of layers, etc.) will elucidate the effect of these choices in the resulting constraints.</p><p>likelihood is a reasonable approximation for the predictions from the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B: SENSITIVITY OF RESULTS TO INTERPOLATION</head><p>To assess how sensitive our results were to the models sampled from the parameter space fΩ m ; σ 8 g, we trained an additional network on the same unsmoothed κ maps but removing the model fΩ m ¼ 0.261; σ 8 ¼ 0.802g from the training data set. When fed the test maps for that cosmology, the network that was not exposed to it during training yielded somewhat different predictions than the network which had seen maps from that model during training. The differences in the mean prediction were very small, with a shift of -1.0% in Ω m and -0.1% in σ 8 . The change in scatter is more significant, the standard deviation in the predictions for Ω m increasing by 80.8% and that for the σ 8 predictions by 12.2%. The larger degradation for Ω m may be related with the fact that the network's architecture seems to have greater difficulty in distinguishing between models that differ in that parameter, as was shown in Sec. IV for both GRFs and smoothed convergence maps.</p><p>While this sensitivity to interpolation highlights how relevant a well-sampled training data set is for proper generalization by the network's architecture, we are mostly concerned about how interpolation errors propagate into the inferred parameters' constraints. That effect is small, as Fig. <ref type="figure" target="#fig_0">13</ref> shows. The credible contours inferred from the predictions by both networks barely change, and the same applies to the marginal distributions inferred for both Ω m and σ 8 . We show the contours computed for the worst-case scenario, that is, when the model missing from the training data set is the "true" cosmology.</p><p>The small change in the parameter constraints from both networks indicate that our main conclusions would not change with a different sampling of the parameter space. Besides, as the priors on our cosmological parameters improve with new experiments, the parameter volume to be explored will shrink and the number of models that need to be simulated to sample that space properly will also decrease. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIG. 1 .</head><label>1</label><figDesc>FIG. 1. Location of the 96 cosmological models in our data set on the fΩ m ; σ 8 g plane. The fiducial model, fΩ m ¼ 0.260; σ 8 ¼ 0.800g, is marked by a red star, and grey lines delimitate the quadrants defined by the fiducial parameters. The quadrants labeled I and II are discussed in Sec. IV C. The dashed curves show isolines for Σ 8 ≡ σ 8 ð Ω m 0.3 Þ 0.6 for reference.</figDesc><graphic coords="2,317.59,47.06,240.00,241.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>FIG. 5. Predictions for fΩ m ; σ 8 ; Σ 8 g from unsmoothed (≈0.2 arcmin=pixel) convergence maps, compared to their true values. Each point represents a map in the test data set. Predictions from the CNN are displayed in red, from the power spectrum in blue and from peak counts in green. Vertical dashed lines indicate the true values for the fiducial cosmology, and diagonal dashed lines the unbiased prediction ¼ truth relationship. The dashed rectangles in the middle and right panels mark a small set of realizations of models near the fiducial cosmology; these contain anomalous structures leading to large biases (see text for discussion).</figDesc><graphic coords="7,75.35,47.06,461.50,155.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIG. 10 .</head><label>10</label><figDesc>FIG. 10. Predictions from the CNN for fΩ m ; σ 8 ; Σ 8 g from unsmoothed (≈0.2 arcmin=pixel) convergence maps, compared to their true values. Each point represents a map in the test data set. Predictions using the unweighted loss function [Eq. (2)] are displayed in grey, and those using a weighted loss function [Eq. (3)], to account for the heterogeneous sampling of the parameter space, in red. Vertical dashed lines indicate the true values for the fiducial cosmology, and diagonal dashed lines the unbiased prediction ¼ truth relationship.</figDesc><graphic coords="11,65.65,47.06,480.80,158.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIG. 12 .</head><label>12</label><figDesc>FIG.12. Credible contours for fΩ m ; σ 8 ; Σ 8 g from lensing peak counts on noisy κ maps. Filled contours correspond to a Gaussian likelihood, and solid lines to contours corresponding to KDE estimates.</figDesc><graphic coords="13,52.50,47.06,244.08,272.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,53.07,47.06,505.93,129.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,52.16,383.30,244.80,247.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,52.04,47.06,245.04,247.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,65.99,526.79,480.12,158.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,52.50,416.58,244.08,247.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II .</head><label>II</label><figDesc>Standard deviation ð×10 3 Þ of the predictions for the parameters fΩ m ; σ 8 ; Σ 8 g, averaged for all the cosmological models. In parentheses are values for the fiducial model.</figDesc><table><row><cell></cell><cell></cell><cell>Power spectrum</cell><cell></cell></row><row><cell></cell><cell>CNN</cell><cell>Noiseless, unsmoothed</cell><cell>Peak counts</cell></row><row><cell>Ω m</cell><cell>5.1 (2.4)</cell><cell>21.7 (21.2)</cell><cell>35.6 (13.2)</cell></row><row><cell>σ 8</cell><cell>10.1 (7.9)</cell><cell>52.7 (84.1)</cell><cell>62.5 (63.8)</cell></row><row><cell>Σ 8</cell><cell>7.2 (4.7)</cell><cell>32.3 (73.2)</cell><cell>36.0 (59.2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV .</head><label>IV</label><figDesc>Area of the 68% and 95% fΩ m ; σ 8 g credible contours, relative to those obtained from the output of the neural network, for different smoothing scales of κ maps. The first row corresponds to the unsmoothed data.</figDesc><table><row><cell>Smoothing</cell><cell cols="2">Power spectrum</cell><cell cols="2">Peak counts</cell></row><row><cell>[arcmin]</cell><cell>68%</cell><cell>95%</cell><cell>68%</cell><cell>95%</cell></row><row><cell>Á Á Á</cell><cell>5.9</cell><cell>6.1</cell><cell>1.9</cell><cell>1.9</cell></row><row><cell>0.2</cell><cell>7.0</cell><cell>5.9</cell><cell>1.8</cell><cell>1.6</cell></row><row><cell>0.3</cell><cell>7.7</cell><cell>7.9</cell><cell>2.3</cell><cell>2.7</cell></row><row><cell>0.4</cell><cell>6.5</cell><cell>6.4</cell><cell>1.9</cell><cell>2.2</cell></row><row><cell>0.5</cell><cell>7.1</cell><cell>6.5</cell><cell>2.5</cell><cell>2.5</cell></row><row><cell>0.6</cell><cell>6.5</cell><cell>5.7</cell><cell>2.5</cell><cell>2.4</cell></row><row><cell>0.7</cell><cell>6.4</cell><cell>5.2</cell><cell>2.8</cell><cell>2.4</cell></row><row><cell>0.8</cell><cell>4.7</cell><cell>4.1</cell><cell>2.5</cell><cell>2.3</cell></row><row><cell>0.9</cell><cell>5.2</cell><cell>4.4</cell><cell>3.0</cell><cell>2.8</cell></row><row><cell>1.0</cell><cell>5.6</cell><cell>4.8</cell><cell>3.6</cell><cell>3.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>103515-2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>103515-11</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>103515-15</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Martin Kilbinger and the anonymous referee for valuable comments on the manuscript. The simulations to generate the data and the training of the neural networks were performed at the NSF XSEDE facility, supported by Grant No. ACI-1053575 and the Habanero computing cluster at Columbia University. This work was supported in part by NSF Grant No. AST-1210877 and by the Research Opportunities and Approaches to Data Science (ROADS) program at the Institute for Data Sciences and Engineering (IDSE) at Columbia University. D. H. acknowledges support from a Sloan Research Fellowship. Z. H. gratefully acknowledges sabbatical support by the Simons Fellowship in Theoretical Physics and thanks New York University, where some of this work was performed, for their hospitality.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A: GAUSSIAN LIKELIHOOD APPROXIMATION</head><p>One way to assess how valid the Gaussian approximation is for the likelihood of a given observable is to estimate its probability density function (PDF) from our simulations without assuming any specific functional form. A nonparametric method to do that estimation is the KDE. The main challenge to apply this approach to lensing peaks is how to achieve a density estimator in a high dimensional space with a limited number of independent vectors (512 per model).</p><p>We performed an analysis with noisy data within the framework of a different study that supports that a Gaussian likelihood is not a bad approximation for lensing peaks. The data set corresponds to the same cosmologies used for this study, and the convergence maps have been smoothed with a characteristic scale of 1 arcmin, but they also have an ellipticity noise of σ ϵ ¼ 0.4 present. To reduce the dimensionality of the observable, we performed an independent component analysis (ICA) <ref type="bibr" target="#b54">[63,</ref><ref type="bibr" target="#b55">64]</ref>. This method provides the directions that maximize negative entropy, which can be interpreted as the directions in which the data are less Gaussian. As a preprocessing step, we whitened the data (i.e., we removed its mean and normalized its covariance), and then we projected the whitened data into the nine directions found following ICA. We then used a KDE to estimate the PDF of the resulting data. While we found some non-Gaussianities, especially for peak counts corresponding to high significance, the effect on the likelihood (and corresponding credible contours) is limited.</p><p>As an illustration, in Fig. <ref type="figure">12</ref> we show the difference in credible contours obtained from a Gaussian likelihood from those obtained using a KDE. We display only the contours derived using only peaks with a signal-to-noise ratio greater than 3. These are the peaks for which the non-Gaussianities are the most pronounced, and yet the contours obtained with both methods are comparable. Using a model to predict peak counts that does not rely on N-body simulations, the authors of <ref type="bibr" target="#b56">[65]</ref> also found that a Gaussian likelihood is a good approximation (to ∼10%) for lensing peaks.</p><p>To analyze whether a Gaussian distribution is a good approximation for the fΩ m ; σ 8 g predictions from the neural network we used a modification of the Kolmogorov-Smirnoff test that can be applied to two-dimensional distributions <ref type="bibr" target="#b57">[66]</ref>. For each model, we computed the mean and covariance from the predictions for the test maps. Then, we tested the predictions against a Gaussian distribution defined by the estimated mean and covariance.</p><p>The null hypothesis that there is no statistical difference between the distribution of our empirical samples (neural network predictions) and a Gaussian cannot be rejected with a confidence of 99% except for two models which are far from the fiducial, fΩ m ¼ 0.450; σ 8 ¼ 0.200g and fΩ m ¼ 0.452; σ 8 ¼ 0.454g. We conclude that a Gaussian</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A R</forename><surname>Ade</surname></persName>
			<affiliation>
				<orgName type="collaboration">Planck Collaboration</orgName>
			</affiliation>
		</author>
		<idno type="DOI">10.1051/0004-6361/201525830</idno>
	</analytic>
	<monogr>
		<title level="j">Astron. Astrophys</title>
		<imprint>
			<biblScope unit="volume">594</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>A</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1093/mnras/stu523</idno>
	</analytic>
	<monogr>
		<title level="j">Mon. Not. R. Astron. Soc</title>
		<imprint>
			<biblScope unit="volume">441</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Refregier</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev.astro.41.111302.102207</idno>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Astron. Astrophys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">645</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Kilbinger</surname></persName>
		</author>
		<idno type="DOI">10.1088/0034-4885/78/8/086901</idno>
	</analytic>
	<monogr>
		<title level="j">Rep. Prog. Phys</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">86901</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.66.083515</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">83515</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Weinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mortonson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hirata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rozo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.physrep.2013.05.001</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rep</title>
		<imprint>
			<biblScope unit="volume">530</biblScope>
			<biblScope unit="page">87</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Neyrinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Szapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Szalay</surname></persName>
		</author>
		<idno type="DOI">10.1088/0004-637X/698/2/L90</idno>
	</analytic>
	<monogr>
		<title level="j">Astrophys. J. Lett</title>
		<imprint>
			<biblScope unit="volume">698</biblScope>
			<biblScope unit="page">90</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dodelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takada</surname></persName>
		</author>
		<idno type="DOI">10.1088/2041-8205/729/1/L11</idno>
	</analytic>
	<monogr>
		<title level="j">Astrophys. J. Lett</title>
		<imprint>
			<biblScope unit="volume">729</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>L</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Shirasaki</surname></persName>
		</author>
		<idno type="DOI">10.1093/mnras/stw2950</idno>
	</analytic>
	<monogr>
		<title level="j">Mon. Not. R. Astron. Soc</title>
		<imprint>
			<biblScope unit="volume">465</biblScope>
			<biblScope unit="page">1974</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kratochvil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Haiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>May</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.81.043519</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">43519</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kratochvil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Haiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huffenberger</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.85.103513</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page">103513</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hartlap</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1365-2966.2009.15948.x</idno>
	</analytic>
	<monogr>
		<title level="j">Mon. Not. R. Astron. Soc</title>
		<imprint>
			<biblScope unit="volume">402</biblScope>
			<biblScope unit="page">1049</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Shirasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="DOI">10.1088/0004-637X/786/1/43</idno>
	</analytic>
	<monogr>
		<title level="j">Astrophys. J</title>
		<imprint>
			<biblScope unit="volume">786</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Haiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kratochvil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>May</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.91.063507</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page">63507</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Haiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kratochvil</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.91.103511</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page">103511</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Kacprzak</surname></persName>
		</author>
		<idno type="DOI">10.1093/mnras/stw2070</idno>
	</analytic>
	<monogr>
		<title level="j">Mon. Not. R. Astron. Soc</title>
		<imprint>
			<biblScope unit="volume">463</biblScope>
			<biblScope unit="page">3653</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">N</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harnois-Déraps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heymans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoekstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klaes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuijken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Merten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakajima</surname></persName>
		</author>
		<idno type="DOI">10.1093/mnras/stx2793</idno>
	</analytic>
	<monogr>
		<title level="j">Mon. Not. R. Astron. Soc</title>
		<imprint>
			<biblScope unit="volume">474</biblScope>
			<biblScope unit="page">712</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Takada</surname></persName>
		</author>
		<title level="m">Statistical Challenges in 21st Century Cosmology, IAU Symposium</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Heavens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-L</forename><surname>Starck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krone-Martins</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">306</biblScope>
			<biblScope unit="page" from="78" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kratochvil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Haiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>May</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.84.043529</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page">43529</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M Z</forename><surname>Matilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Haiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Petri</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.94.083506</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">83506</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Haiman</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.94.043533</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">43533</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1088/0004-637X/719/2/1408</idno>
	</analytic>
	<monogr>
		<title level="j">Astrophys. J</title>
		<imprint>
			<biblScope unit="volume">719</biblScope>
			<biblScope unit="page">1408</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C.-A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kilbinger</surname></persName>
		</author>
		<idno type="DOI">10.1051/0004-6361/201425188</idno>
	</analytic>
	<monogr>
		<title level="j">Astron. Astrophys</title>
		<imprint>
			<biblScope unit="volume">576</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
	</analytic>
	<monogr>
		<title level="j">Nature (London)</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tegmark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rolnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08225</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vistas</forename><surname>Astron</surname></persName>
		</author>
		<idno type="DOI">10.1016/0083-6656(93)90118-4</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">141</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Tagliaferri</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0893-6080(03)00028-5</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">297</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fromenteau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the 33rd International Conference on Machine Learning, Machine Learning Research<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2407" to="2416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bhimji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lukić</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02390</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kacprzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sgier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Réfrégier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.09070</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Schmelzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kacprzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sgier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Réfrégier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05167</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Seljak</surname></persName>
		</author>
		<idno type="DOI">10.1086/304372</idno>
	</analytic>
	<monogr>
		<title level="j">Astrophys. J</title>
		<imprint>
			<biblScope unit="volume">484</biblScope>
			<biblScope unit="page">560</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Petri</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ascom.2016.06.001</idno>
	</analytic>
	<monogr>
		<title level="j">Astron. Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">V</forename><surname>Springel</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1365-2966.2005.09655.x</idno>
	</analytic>
	<monogr>
		<title level="j">Mon. Not. R. Astron. Soc</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="page">1105</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Challinor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lasenby</surname></persName>
		</author>
		<idno type="DOI">10.1086/309179</idno>
	</analytic>
	<monogr>
		<title level="j">Astrophys. J</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="page">473</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ehlers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Falco</surname></persName>
		</author>
		<title level="m">Gravitational Lenses, XIV</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page">112</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Haiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>May</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.95.123503</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">123503</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Haiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>May</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.93.063524</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">63524</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Haiman</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevD.75.043010</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page">43010</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<title level="m">Proceedings of a International Conference on Machine Learning</title>
		<meeting>a International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)<address><addrLine>Omnipress, Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Kilbinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Benabed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Astier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tereno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wraith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Coupon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mellier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Balland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bouchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hamana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Mccracken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Regnault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schultheis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yahagi</surname></persName>
		</author>
		<idno type="DOI">10.1051/0004-6361/200811247</idno>
	</analytic>
	<monogr>
		<title level="j">Astron. Astrophys</title>
		<imprint>
			<biblScope unit="volume">497</biblScope>
			<biblScope unit="page">677</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nishimichi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taruya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oguri</surname></persName>
		</author>
		<idno type="DOI">10.1088/0004-637X/761/2/152</idno>
	</analytic>
	<monogr>
		<title level="j">Astrophys. J</title>
		<imprint>
			<biblScope unit="volume">761</biblScope>
			<biblScope unit="page">152</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Hartlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schneider</surname></persName>
		</author>
		<idno type="DOI">10.1051/0004-6361:20066170</idno>
	</analytic>
	<monogr>
		<title level="j">Astron. Astrophys</title>
		<imprint>
			<biblScope unit="volume">464</biblScope>
			<biblScope unit="page">399</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Alfeld</surname></persName>
		</author>
		<idno type="DOI">10.1016/0167-8396(84)90029-3</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Aided Geometric Design</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Renka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cline</surname></persName>
		</author>
		<idno type="DOI">10.1216/RMJ-1984-14-1-223</idno>
	</analytic>
	<monogr>
		<title level="j">Rocky Mountain J. Math</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">223</biblScope>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Nelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mead</surname></persName>
		</author>
		<idno type="DOI">10.1093/comjnl/7.4.308</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">308</biblScope>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ichiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Takeuchi</surname></persName>
		</author>
		<idno type="DOI">10.1103/PhysRevLett.105.251301</idno>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. Lett</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page">251301</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jarvis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Connolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krughoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Peterson</surname></persName>
		</author>
		<idno type="DOI">10.1093/mnras/stt1156</idno>
	</analytic>
	<monogr>
		<title level="j">Mon. Not. R. Astron. Soc</title>
		<imprint>
			<biblScope unit="volume">434</biblScope>
			<biblScope unit="page">2121</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">E</forename><surname>Semboloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoekstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schaye</surname></persName>
		</author>
		<idno type="DOI">10.1093/mnras/stt1013</idno>
	</analytic>
	<monogr>
		<title level="j">Mon. Not. R. Astron. Soc</title>
		<imprint>
			<biblScope unit="volume">434</biblScope>
			<biblScope unit="page">148</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Osato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shirasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="DOI">10.1088/0004-637X/806/2/186</idno>
	</analytic>
	<monogr>
		<title level="j">Astrophys. J</title>
		<imprint>
			<biblScope unit="volume">806</biblScope>
			<biblScope unit="page">186</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dixieme colloque sur le traitement du signal et ses appications (Groupe d</title>
		<author>
			<persName><forename type="first">J</forename><surname>Herault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th GRETSI symposium on signal and image processing</title>
		<meeting>the 10th GRETSI symposium on signal and image processing<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page">1017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
		<idno type="DOI">10.1016/0165-1684(94)90029-9</idno>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">287</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C.-A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kilbinger</surname></persName>
		</author>
		<idno type="DOI">10.1051/0004-6361/201526659</idno>
	</analytic>
	<monogr>
		<title level="j">Astron. Astrophys</title>
		<imprint>
			<biblScope unit="volume">583</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Fasano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Franceschini</surname></persName>
		</author>
		<idno type="DOI">10.1093/mnras/225.1.155</idno>
	</analytic>
	<monogr>
		<title level="j">Mon. Not. R. Astron. Soc</title>
		<imprint>
			<biblScope unit="volume">225</biblScope>
			<biblScope unit="page">155</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
