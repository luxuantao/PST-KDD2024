<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reinforced Imitative Graph Learning for Mobile User Profiling</title>
				<funder ref="#_XEKwgva">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-13">13 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dongjie</forename><surname>Wang</surname></persName>
							<email>wang-dongjie@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengyang</forename><forename type="middle">?</forename><surname>Wang</surname></persName>
							<email>py-wang@um.edu.mo</email>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
							<email>yanjie.fu@ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
							<email>kunpengliu@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
							<email>hxiong@rutgers.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Hughes</surname></persName>
							<email>charles.hughes@ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reinforced Imitative Graph Learning for Mobile User Profiling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-13">13 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.06550v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Mobile User Profiling</term>
					<term>Incremental Learning</term>
					<term>Reinforcement Learning</term>
					<term>Spatial Knowledge Graph</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mobile user profiling refers to the efforts of extracting users' characteristics from mobile activities. In order to capture the dynamic varying of user characteristics for generating effective user profiling, we propose an imitation-based mobile user profiling framework. Considering the objective of teaching an autonomous agent to imitate user mobility based on the user's profile, the user profile is the most accurate when the agent can perfectly mimic the user behavior patterns. The profiling framework is formulated into a reinforcement learning task, where an agent is a next-visit planner, an action is a POI that a user will visit next, and the state of the environment is a fused representation of a user and spatial entities. An event in which a user visits a POI will construct a new state, which helps the agent predict users' mobility more accurately. In the framework, we introduce a spatial Knowledge Graph (KG) to characterize the semantics of user visits over connected spatial entities. Additionally, we develop a mutual-updating strategy to quantify the state that evolves over time. Along these lines, we develop a reinforcement imitative graph learning framework for mobile user profiling. Finally, we conduct extensive experiments to demonstrate the superiority of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>M OBILE user profiling refers to the efforts of extracting user interests and behavioral patterns from mobile activities. Consider the existence of many mobile users in a city, each user is equipped with mobile sensing equipment moving from one location to another location and generates a mobility event stream in real time. Classical mobile user profiling collects large-scale spatio-temporal event data, and then, learns profile representations to characterize user patterns and preferences using the collected data.</p><p>Prior literature in mobile user profiling includes: (1) explicit profile extraction <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b1">(2)</ref> factorization-based approaches <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and (3) deep learning-based approaches <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Other studies exploit adversarial deep learning to emphasize substructure patterns in mobile user profiling <ref type="bibr" target="#b20">[21]</ref>. All of these approaches can be regarded as exploiting user activities' prediction bias or reconstruction loss of the users' profile structure as the learning criteria to model users' profiles. The key limitation of these approaches is the modeling procedure is solely based on the individual user, which lacks a global perception of the dynamic varying environment. However, the behavioral data generated by the mobile users are usually a mixed-user, spatially and temporally discrete, event stream. Users inject their impacts in the environment mixed and chronologically, leading to the dynamic changes in the environment that inversely affect a users' decision.</p><p>After exploring many profiling methods, we found that minimizing user activities' prediction bias or reconstruction loss may not be the best criteria to evaluate profiling accuracy. Unlike traditional loss minimization, we identify a better criteria, which we call the imitation based criteria: considering the objective of teaching an autonomous agent to imitate a mobile user to plan where a user will visit next, based on the profile of the user. The user profile is the most accurate when the agent can perfectly copy the activity patterns of the user.</p><p>The emerging reinforcement learning can train an agent to plan for its next actions in order to function in its environment. Such an ability provides great potential to implement the imitation based criteria in order to achieve more accurate user profiling.</p><p>As a result, we propose to formulate the problem into a reinforcement learning framework. In this framework, an agent is a next-visit planner that tries to perfectly imitate a set of mobile users. The state of environment is a fused representation of a given user and spatial entities (e.g., POIs, activity types, functional zones). An action is a POI that a given mobile user will visit, which is estimated based on the state of the environment by the agent. An event where the user takes the action to visit the POI, will change the environment, resulting into a new state of the user and the spatial entities, which helps the agent to better estimate the next visit. The reward of an action is the reduction of the gap between the agent's activity patterns and the user's activity patterns. After the reformulation, our new objective is to exploit the reinforcement learning framework to extract dynamic profile representations of various users in the state of the environment by incremental learning from an uservisit event stream.</p><p>To further improve the profiling accuracy of the framework, we analyze how a mobility event connect mobile users with spatial entities (e.g., POIs, functional zones), and identify two important structured information.</p><p>Firstly, there is semantic connectivity among spatial entities. Specifically, the profile of a user can be reflected by a sequential composition of mobility events. The semantics of an event are about which building the user visits (POI), what type of activity the user conducts during the visit (POI category), and in which region the POI is located (urban functional zone). Therefore, improving the representations of POIs, activity types, and functional zones of events can, in return, improves user profiling. The semantic connectivity among these spatial entities refers to the observation that every time a mobile user visits a POI, a new connection is established or reinforced among a POI, an activity type, and a functional zone, which indeed is a heterogeneous graph with geographic knowledge. Therefore, we propose to use a spatial knowledge graph (KG) to describe such semantic connectivity, and a translation based embedding method is employed to learn the embedding of the spatial KG.</p><p>Secondly, there is mutual influence between users and spatial entities. An event that a user visits and interacts with the POI, will change and reinforce the edges (semantic connectivity) of the spatial knowledge graph, resulting into new state representations of POIs, activity types, and functional zones. If the states of spatial entities are updated, once a user visits these newly-updated spatial entities, then the state of the user profile will be updated as well. In other words, the state updates between spatial entities and users are sequentially nested together per mobility event. We propose a sequentially nested updating strategy to update user states by jointly considering the spatial KG and temporal contexts, and update spatial KG states by jointly considering user states and temporal contexts.</p><p>In summary, we propose a reinforced imitative graph learning framework for mobile user profiling by integrating a reinforcement learning framework with spatial KG to solve. Our contributions are: <ref type="bibr" target="#b0">(1)</ref> We propose a new imitation based criteria for evaluating the accuracy of user profiling: the better an agent imitates a mobile user, the more accurate the user profile is. (2) Motivated by the imitation based criteria, we reformulate the mobile user profiling problem into a reinforcement learning framework, where an agent is a next-visit planner, the state of environment is the fused representation of users and spatial KG, an action is a POI visited by a user, and the reward of an action is how well the agent can imitate mobile users. (3) We develop a new reward function to evaluate the quality of actions. We extend the reward function in <ref type="bibr" target="#b22">[23]</ref> to be negative when the predicted POI visits are far away from the real, which would positively encourage the agent on good actions and punish the agent on bad actions. (4) We extend the policy network in <ref type="bibr" target="#b22">[23]</ref> based on Double DQN with hierarchical graph pooling, which avoids the overestimation of specific POI visits. <ref type="bibr" target="#b4">(5)</ref> We identify and describe the semantic connectivity of spatial entities by a spatial KG, which is integrated into the reinforcement learning framework. <ref type="bibr" target="#b5">(6)</ref> We develop a new sequentially nested state update strategy by modeling the long-short term effects of interactions between users and spatial KG, by paying more attention to recent ones. <ref type="bibr" target="#b6">(7)</ref> We present extensive experimental results with real-world mobile check-in data to demonstrate improved performances. (8) Our framework can be adapted and generalized to the Tail (categories, locations) representations.</p><formula xml:id="formula_0">g (?) =&lt; h P (?) , r (?) , t (?) &gt; Spatial KG state (representations). s (?) = (u (?) , g (?) ) State. r (?) Reward. Q Policy. x (?)</formula><p>Priority score.</p><formula xml:id="formula_1">T (?) , T(?)</formula><p>Temporal context, transformed.</p><formula xml:id="formula_2">W (?) , b (?)</formula><p>Weights, biases of model.</p><formula xml:id="formula_3">? (?)</formula><p>Weights of rewards.</p><p>tasks of incremental learning with mixture event streams to support mobile user profiling and other applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definitions and Problem Statement</head><p>We first introduce the key definitions and the problem statement. Then, we present an overview of the proposed framework, followed by the discussion of how our approach differs from those presented in the current literature. All the notations are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Spatial Knowledge Graph (KG).</head><p>We construct spatial KG to demonstrate semantic connectivities between spatial entities. Specifically, in the spatial KG, there are three types of spatial entities: POIs, POI categories and locations (i.e. functional zones); and two types of relations: "belong to" which is to demonstrate the affiliation relations between POIs and POI categories, and "locate at" which is to demonstrate the geospatial relation between POIs and the functional zones. The spatial KG is defined as the following groups of triplet facts: (1) &lt;POI, "belong to", POI category&gt;, and (2)&lt;POI, "locate at", functional zones&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Temporal Context.</head><p>In this paper, temporal context quantifies the temporal factors when users make the decision to visit a POI. Following the approach in previous studies <ref type="bibr" target="#b12">[13]</ref>, we utilize the snapshot of transportation traffic in a small time window for quantifying temporal context. Specifically, we first segment the entire area into m grids. For a given grid, we calculate the inner traffic, in-flow traffic and out-flow traffic. Then, we obtain a traffic matrix T ? R m?3 , where each row denotes a grid, each column denotes the inner traffic, in-flow traffic and out-flow traffic of the given grid respectively. We use the traffic matrix T to represent the temporal context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Key Components of Reinforcement Learning.</head><p>In our problem setting, we define the key components of our reinforcement learning framework as follows: 1) Agent. We consider the next-visit planner as the agent.</p><p>The agent provides the personalized POI prediction based on the current environment status.  2) Actions. Actions are defined as the visit event such that at each step, the user will visit which POI. Formally, let a j denote the action that visits the POI P j . The action space is the number of POIs. Suppose the user visits the POI P j at the step l, then the policy would take the action a l = a j . 3) Environment. The environment is defined as the combination of all users and the spatial KG. Within the environment, users would interact with spatial KG by visiting any POIs in the spatial KG. Thus, on one hand, the visit behavior of users would affect the representation of the spatial KG; on the other hand, the spatial KG inversely would affects each user's representation as well. 4) State. The state s is to describe the environment composed by users and the spatial KG. At the step l, the state s l is defined as a pair of (u l , g l ). Specifically, u l = {u l i |u i ? U}, where u l i denotes the representation of user u i at step l; g l =&lt; h l , rel, t l &gt;, where h l denotes the heads (i.e., POIs) and t l denotes the tails (i.e., categories and functional zones). 5) Reward. In the conference version <ref type="bibr" target="#b22">[23]</ref>, we propose the reward function as the weighted summation of reciprocal of the distance r d , the category similarity r c , and binary accuracy between the real and predicted the POI visit r p . The range of such reward function is above zero, which means the agent would be always encouraged positively no matter the generated action is good or bad. However, intuitively, the agent should be encouraged positively for the good actions, but be punished for the bad actions. Based on such intuition, we propose a new formulation for the reward function, which would assign positive values for good actions as the encouragement, and negative values for bad actions as the penalty. Formally, the new reward function can be denoted as the agent behaves better than baselines, the reward is positive, otherwise, it is negative. This reward function urges the agent to find the next POI that is close to the real visited POI from POI category and location. Specifically, we calculate the euclidean distance between the real and predicted POI according to their longitude and latitude coordinates, and regard the reciprocal value of the distance as r d . We exploit GloVe 1 pre-trained word vectors of the category names to calculate the cosine similarity between the predicted and real POI categories as the similarity r c .</p><formula xml:id="formula_4">r = ? d ? (r d -b d ) + ? c ? (r c -b c ) + ? p ? (r p -b p ),<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Problem Statement.</head><p>In this paper, we study the problem of learning to profile users from user mobile activity data. Due to the largescale, nested, sequential and semantic nature of user mobile activities, we reformulate the mobile user profiling problem as an incremental user modeling with the integration of reinforcement learning and spatial KG.</p><p>Formally, given a mobile activity sequence of mixeduser and spatial KG, we aim to find a mapping function f : (u l , g l ) ? (u l+1 , g l+1 ), that takes as input the state of the environment (representations of users and spatial KG) at the step l, and outputs the state at the next step l + 1, while simultaneously following the imitation-based criteria to provide accurate personalized prediction, based on the incrementally updated user representations from the mutual interactions with spatial KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Framework Overview</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows an overview of our framework that includes two key components: (1) state representation, and (2) reinforced imitation. For the state representation, we consider the mutual interactions between users and spatial KG given the temporal context. Specifically, at each step, the user 1. https://nlp.stanford.edu/projects/glove/ representations are updated based on the influence from spatial KG given the temporal context; inversely, the spatial KG representations are updated based on the influence from the user given the temporal context. For the reinforced imitation, we employ a reinforced agent to imitate the user mobility patterns via exploration and exploitation. The prediction of the agent is evaluated by a reward function. Following that, the user visit trajectories are stored in memory to offline train the agent for updating learned policies. In addition, the user and spatial KG are changed to the next state for starting a new visit event. When the reinforced imitation mimics user mobility perfectly, our framework outputs the most effective user profiles. Comparison with literature. Despite the promising results from prior studies on mobile user profiling, one major concern arises that most of these methods are trained on offline data without the ability of self-updating, which is essential for quantifying user' dynamic mobility behaviors changing over time. Therefore, in our work, we embrace reinforcement learning for mobile user modeling. When exploiting reinforcement learning for user modeling, online recommender systems have achieved great performance by regarding the online behavior as a sequential decisionmaking process. Different from the online user modeling, mobile user behavior (e.g., POI visiting) is offline behavior which is constrained by many offline factors (e.g., time, traffic, location). Therefore, we incorporate temporal contexts and simultaneously update user and spatial KG representations based on the mutual interactions. In the meantime, the integrated spatial KG provides rich semantics to better understand user patterns and preferences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">POLICY DESIGN</head><p>In this section, we will introduce how to teach the next-visit planner (agent) to mimic users' patterns and preferences, given state (representations of user and spatial KG) and temporal contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Structure</head><p>While DQN shows exceptional performance on learning strategies <ref type="bibr" target="#b22">[23]</ref>, the maximization step of the action value estimation process leads the DQN to overestimate Q-value, resulting in learning local optimal policies. In order to alleviate this issue, we replace the DQN to the Double Deep Q-Network (DDQN) <ref type="bibr" target="#b17">[18]</ref>.</p><p>Figure <ref type="figure">3</ref> shows the structure of DDQN. Compared to the model structure (Figure <ref type="figure">2</ref>) of the conference version <ref type="bibr" target="#b22">[23]</ref>, we implement two neural networks (Q e and Q t) to decouple the maximization operation in DQN into action selection and action evaluation. Specifically, we first concatenate the user representations and spatial KG representations together as the state vector. Then, we input the state into the Q e network to select the action (POI) with the highest Q-value. Moreover, we input the state and the previously selected action into the Q t network to estimate the real value of the action in order to update learned policies. During this process, Q e and Q t own the same model structure and the same initialization. But we first update the parameters of Q e for several steps and freeze the parameters of Q t. After that, we store the parameters of Q e into Q t and reiterate the previous learning step. Thus, Q e and Q t have different estimation abilities at the same time. The max Q-value estimations in the two networks associated with different actions. So, the Q-value used to update policies is more rational than the Q-value in DQN. Such a learning strategy has been proved effective in avoiding the over-optimistic problem of DQN and learning more effective policies <ref type="bibr" target="#b17">[18]</ref>.</p><p>Since DDQN only takes vectors/matrices as input, graph pooling is desired for graph-structure state G transformed into vectors. But current graph pooling operations do not fit for the heterogeneity of spatial KG. Therefore, we design the graph pooling operation in a hierarchical fashion. We first split the spatial KG into two graphs based on the two types of relations as (1) "location" graph and (2) "belonging" graph. Then, we leverage graph average pooling for both the "location" and "belonging" graph to generate graph-level vectorized representations respectively. Finally, we employ average pooling over the two graph-level vectors to obtain the single unified vectorized representation g l for the spatial KG at step l.</p><p>After the hierarchical pooling module, we concatenate the user state u l i with the spatial KG state g l as the input of the fully connected (FC) layers. Then, the FC layers would map the given state s l into a group of Q(s l , a) for each POI in the spatial KG. The policy chooses the POI with the highest Q(s l , a) as the prediction result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Improved Sampling Strategy in Experience Replay</head><p>Since the space of POIs visiting events is very large, the proposed reinforcement learning framework is computationally intensive. Therefore, we propose a training strategy to accelerate exploration procedure based on experience replay <ref type="bibr" target="#b10">[11]</ref>. The training strategy involves two stages: (1) priority assignment, which assigns the priority score for each data sample (s l , a l , r l , s l+1 ); and (2) sampling strategy, which selects data samples from memory for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Priority</head><p>We design two types of priority scores, including rewardbased and temporal difference-based.</p><p>1) Reward-based. Intuitively, the higher the reward of the POI visit action a l is, the better the next-visit planner (agent) mimic the users, then the more significant the data sample can contribute to the policy training. Therefore, formally, for each data sample (s l , a l , r l , s l+1 ), the reward-based priority score x r is defined as the reward r l : x r (s l , a l , r l , s l+1 ) = r l .</p><p>(2)</p><p>2) Temporal difference (TD)-based. The TD error is originally set for updating the DQN. The larger the TD error is, the more valuable and informative is the data sample for the next-visit planner (agent) to learn. Therefore, we define the TD error as the TD-based priority score that</p><formula xml:id="formula_5">x T D (s l , a l , r l , s l+1 ) = r l + ? max a l+1 Q(s l+1 , a l+1 ) -Q(s l , a l ), (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>where ? is the discount factor. We will evaluate and discuss the performance of these two priority scores in the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Sampling Strategy</head><p>After we obtain the priority score x * , we need to construct a distribution from the priority score x * for sampling data. Therefore, we employ softmax to convert the priority score into a distribution</p><formula xml:id="formula_7">P (k) = e x (k) K k =1 e x (k ) ,<label>(4)</label></formula><p>where P (k) is the sampling probability of the k-th data sample given the corresponding priority score x (k) . Then, we sample a batch size of data from the memory based on the assigned probability.</p><note type="other">User Embedding Knowledge Graph Embedding Visit Temporal Context</note><p>Fig. <ref type="figure">4</ref>: An example of mutual interactions between users and spatial KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STATE REPRESENTATION LEARNING</head><p>In this section, we introduce details about how to initialize and update states based on the mutual interactions between mobile users and spatial KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">State Initialization</head><p>Here, we discuss the initialization of user state (representations), followed by how to initialize spatial KG state (representation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">User</head><p>We leverage StructRL <ref type="bibr" target="#b20">[21]</ref> to initialize the user state (representations). StructRL is a mobile user profiling framework that models the global and substructure patterns of user behaviors by minimizing the structural loss between the input and reconstructed human mobility graph through adversarial learning. For each user, we extract the very first portion (10%) of user mobility trajectories to construct human mobility graph, where the nodes are POI categories and edges are visit transition frequencies or duration among POI categories. Then, we employ StructRL to learn user representations. The learned user representations are regarded as the initialized user state fed to the reinforcement learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Spatial KG</head><p>We exploit TransD <ref type="bibr" target="#b6">[7]</ref> to initialize spatial KG state (representations). TransD is a translation-based model that utilizes two vectors to learn entity/relation representations and construct mapping matrix dynamically with considering both the diversity of entities and relations, which outperforms TransE <ref type="bibr" target="#b0">[1]</ref>, TransH <ref type="bibr" target="#b25">[26]</ref>, and TransR <ref type="bibr" target="#b11">[12]</ref> in learning KG representations. Specifically, to be more compatible with reinforcement learning framework, we adopt the projected entity embedding as the initialization of spatial KG. In this way, the representations of entity and relations will be in the same feature space, which will make the state updating more feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">State Update</head><p>In this section, we will introduce how to update states based on the mutual interactions between user and spatial KG.</p><p>Figure <ref type="figure">4</ref> shows an interaction example in which a user visits a POI in spatial KG During the interaction, the influence from the user visit is directly injected into the visited POI.</p><p>Then, the influence is propagated to other spatial entities through the semantics and topology of spatial KG. The influence from spatial KG then inversely affects the user state (representations). Without the loss of the generality, we consider the scenario of a POI visit event where a user u i visits the POI P j at step l. The states will update for the step l + 1.</p><p>In the conference version <ref type="bibr" target="#b22">[23]</ref>, we propose a state update strategy that updates the states incrementally by equally treating the impacts of each visit. However, intuitively, the impact of each visit should be different, so the most recent visits should be given more attention than previous ones. To solve the problem, we follow the idea from the Long Short Term Memory (LSTM) networks, in which the model should forget partial old information and combine new changes simultaneously. Based on this rule, we propose the new state update strategy as follows.</p><p>(1) User</p><p>We update the user state u l+1 i based on old user state u l i and the interaction between u l i and h l Pj in a temporal context. The update process can be represented as:</p><formula xml:id="formula_8">u l+1 i = ? u ? u l i + (1 -? u ) ? (W u ? (h l Pj ) ? Tl )),<label>(5)</label></formula><p>where ? u is a scalar that indicates the ratio of information of u l i in u l+1 i . At this point, ? u is calculated by u l i and the calculation process can be represented as:</p><formula xml:id="formula_9">? u = ?(W ?u ? u l i + b ?u ),<label>(6)</label></formula><p>where W ?u ? R 1?N is weight, N is the dimension of the weight matrix, and b ?u ? R 1?1 is bias. (2) Spatial KG a) Updating visited POI h l+1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pj</head><p>Similar to the update process of u l+1 i , the new POI state h l+1 Pj is updated by the old POI state h l Pj and the interaction between u l i and h l Pj in a temporal context. The update process can be represented as:</p><formula xml:id="formula_10">h l+1 Pj = ? p ? h l Pj + (1 -? p ) ? (W p ? (u l i ) ? Tl ),<label>(7)</label></formula><p>where ? p is a scalar that represents the proportion of information of h l Pj in h l+1 Pj . Here, ? p is calculated by h l Pj and the process can be represented as:</p><formula xml:id="formula_11">? p = ?(W ?p ? h l Pj + b ?p ),<label>(8)</label></formula><p>where W ?p ? R 1?N is weight , N is the dimension of the weight matrix, b ?p ? R 1?1 is bias. b) Updating category and functional zones (tail) t l+1 * We update tail t l+1 * based on the t l * and the combination between h l+1 Pj and rel (Pj ,?) . The update process can be represented as:</p><formula xml:id="formula_12">t l+1 (Pj ,?) = ? t ?t l (Pj ,?) +(1-? t )?(h l+1 Pj +rel (Pj ,?) ),<label>(9)</label></formula><p>where ? t is a scalar that denotes the ratio of information of t l (Pj ,?) in t l+1 (Pj ,?) . Here, ? t is calculated by t l (Pj ,?) , the process can be represented as:</p><formula xml:id="formula_13">? t = ?(W ?t ? t l (Pj ,?) + b ?t ),<label>(10)</label></formula><p>where W ?t ? R 1?N is weight, N is the dimension of the weight matrix, b ?t ? R 1?1 is bias. c) Updating same category and location POIs h l+1 P j -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We update the h l+1</head><p>Pj-based on h l Pj-and the current state of t l (P j -,?) and rel (P j -,?) . The update process can be represented as:</p><formula xml:id="formula_14">h l n P j -= t l+1 (P j -,?) -rel (P j -,?) , h l+1 P j -= ? p -? h l P j -+ (1 -? p -) ? h l n P j -,<label>(11)</label></formula><p>where h l n P j -represents the new state of P j -that is calculated by t l (P j -,?) and rel (P j -,?) ; ? p -indicates the ratio of information of h l P j -in h l+1 P j -. Here, ? p -is calculated by h l P j -, the process can be represented as:</p><formula xml:id="formula_15">? p -= ?(W ? p -? h l P j -+ b ? p -),<label>(12)</label></formula><p>where</p><formula xml:id="formula_16">W ? p -? R 1?N is weight, N is the dimension of the weight matrix, b ? p -? R 1?1 is bias.</formula><p>The same process is used to update strategy1, here, we only update the local subgraph, which makes the update process efficiently when the spatial KG is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>This section details our empirical evaluation of the proposed method on real-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Description</head><p>Table <ref type="table" target="#tab_2">2</ref> shows the statistics of our two check-in datasets from two cities: New York <ref type="bibr" target="#b27">[28]</ref> and Beijing <ref type="bibr" target="#b23">[24]</ref>. Each dataset includes User ID, Venue ID, Venue Category ID, Venue Category Name, Latitude, Longitude, and Time. Additionally, we also collected taxi data to represent traffic of the New York and Beijing in Table <ref type="table" target="#tab_3">3</ref>. The taxis traffic data includes ID, Pick-up Latitude, Pick-up Longitude, Drop-off Latitude , Drop-off Longitude, Pick-up time and Drop-off time. We split the taxi data into small segments with one-hour time window. For each time window, we calculate the temporal context T, compatible with time users' visit events.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>We evaluate the model performances over the prediction on the activity types (i.e., POI categories) and locations in terms of the following four metrics:</p><p>(1) Precision on Category (Prec Cat): We regard the prediction on activity type as the multi-class classification task. We  the classification prediction via the weighted precision. Let c k denote the k-th category, |c k | denote the number of activity types, I k T P denote the number of true positive predictions, and I k F P denote the number of false positive predictions, then the weighted precision on categories can be represented as</p><formula xml:id="formula_17">Prec Cat = |c k | ? I k T P k |c k |(I k T P + I k F P )<label>(13)</label></formula><p>(2) Recall on Category (Rec Cat): Continuing with the definition of Prec Cat, we use the weighted recall to evaluate the recall on category prediction. Let I k F N denote the number of false negative prediction for the category c k , the weighted recall can be represented as</p><formula xml:id="formula_18">Rec Cat = |c k | ? I k T P k |c k |(I k T P + I k F N )<label>(14)</label></formula><p>(3) Average Similarity (Avg Sim): In addition to evaluating the precision and recall of prediction, we also evaluate the average similarity between the real and predicted POI categories. We adopt the pre-trained GloVe word vectors to calculate the cosine similarity between the category word vector "word l " of the real visited POI and the category word vectors " ? ord l " of the predicted POI. Let L denote the total visit number, then the average similarity on categories is</p><formula xml:id="formula_19">Avg Sim = l cosine(word l , ? ord l ) L .<label>(15)</label></formula><p>The higher the value of Avg Sim, the better the prediction.</p><p>(4) Average Distance (Avg Dist): We evaluate the prediction on location with the average distance. Let Dist(P l , P l ) denote the distance between the real location P l and the predicted location P l for the l-th visit, then the average distance is</p><formula xml:id="formula_20">Avg Dist = l Dist(P l , P l ) L . (<label>16</label></formula><formula xml:id="formula_21">)</formula><p>The lower the value of Avg Dist, the better the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baseline Algorithms</head><p>We compare the performances of our method (namely "IMUP") against the following baseline algorithms.</p><p>(1) PMF is a classic framework for modeling users through the probabilistic matrix factorization over the user-item interaction matrix <ref type="bibr" target="#b13">[14]</ref>.</p><p>(2) PoolNet is proposed to learn user representations by averaging the representations of items with which they have interacted through a deep neural network model <ref type="bibr" target="#b2">[3]</ref>.</p><p>(3) WaveNet is originally designed for generating raw audio waveforms in a generative way. It can also be applied to learning representations for sequential decision-making through stacked causal atrous convolutions <ref type="bibr" target="#b15">[16]</ref>.</p><p>(4) LSTMNet represents users as the hidden state at each timestep with a recurrent neural network by feeding the visiting sequence in to the model <ref type="bibr" target="#b5">[6]</ref>.</p><p>(5) StructRL models substructures of user mobility graph for quantifying specific activity patterns through the adversarial learning paradigm <ref type="bibr" target="#b20">[21]</ref>. By contrast, in the conference version <ref type="bibr" target="#b22">[23]</ref>, there are two variants for incremental user profiling: To validate the effectiveness of each part, we develop Here, we develop two variants for this paper: (1) IMUP *r, (2) IMUP * -TD. Compared with IMUP-r and IMUP-TD, the difference is the two variants utilize all new modifications of this paper during the process of incremental user profiling.</p><p>In the experiment, we split the datasets into two nonoverlapping sets: for each user, the earliest 90% of checkins are the training set and the remaining 10% check-ins are testing set. We set the dimension of user representations as 200 for all the baselines and our proposed methods. We adopt the implementation 2 for evaluating PMF. And we  adopt "spotlight" <ref type="bibr" target="#b8">[9]</ref> for evaluating PoolNet, WaveNet, and LSTMNet, where the learning rate is set as 0.01. We follow the parameter setting of StructRL in <ref type="bibr" target="#b20">[21]</ref>. Our code and data are released by Dropbox 3 .</p><p>All evaluations were conducted using Ubuntu 18.04.3 LTS, on an Intel(R) Core(TM) i9-9920X CPU@ 3.50GHz, with Titan RTX 32 GB and RAM memory size 128G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Overall comparison</head><p>Figure <ref type="figure">5</ref> and Figure <ref type="figure">6</ref> show the comparison results of user profile quality in terms of "Precision on Category", "Recall on Category", "Average Distance" and "Average Similarity". We can find that "IMUP * -r" and "IMUP * -TD" outperform other baseline models on both New York and Beijing datasets, and significantly enhance "Average Distance" and "Average Similarity". A potential reason for the observation is that the representation of the spatial KG involves the geospatial correlations into state vectors. So, the reinforced agent can capture the user mobility semantics and interests based on such states to mimic user mobility patterns more accurately. Moreover, the new state update strategy is able to automatically discard old user visit interests and incorporate new user visit preferences. Thus, the next visit prediction of our method becomes as close as possible to the real one from the semantic and geographical perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Comparison of different sampling strategies</head><p>We propose two priority-based sampling strategies to improve the policy learning procedure. Figure <ref type="figure">5</ref> and Figure <ref type="figure">6</ref> show the comparison results in terms of all evaluation metrics. We can that the T D-based sampling strategy is slightly better than reward-based sampling strategy. A potential reason is that the reinforced agent covers the obvious user visit interests but makes mistakes for several 3. https://www.dropbox.com/sh/l6syw989teu9b96/AADXfq-6Es6LjMEFfrf4t3Kna?dl=0 implicit user preferences. The T D-based strategy can force the reinforced agent to learn such confused knowledge by sampling related data samples in order to improve the user mobility imitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Comparison of different rewards</head><p>We replace the reward function (r1) of the conference version <ref type="bibr" target="#b22">[23]</ref> with the new reward function (r2) in this work. We develop four variants based on different sampling strategies and the two rewards, namely "IMUP-r r1 ","IMUP-r r2 ", "IMUP-T D r1 ","IMUP-T D r2 ". Figure <ref type="figure">7</ref> and Figure <ref type="figure">8</ref> show the comparison results. We can find that r2 outperforms r1 significantly in terms of "Average Distance", and keeps stable in terms of "Precision on Category", "Recall on Category", "Average Similarity". The underlying driver is that r2 encourages the agent to search for possible visited POIs that are semantically and geographically close to the real visited POI. If the predicted POI is further from the real one than our expectation on geographical distance, the agent will be punished via getting negative rewards. Thus, in order to get more positive rewards, the agent makes more accurate next-visit predictions that are more semantically and geographically close to real visited POIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Comparison of different policy networks</head><p>We replace the reinforced module of our framework from DQN with double DQN to mitigate Q-value overestimation to improve model performance. We use "dqn" and "ddqn" to denote the DQN and double DQN respectively, and develop four variants of IMUP based on two sampling strategies: "IMUP-r dqn ","IMUP-r ddqn ", "IMUP-T D dqn " and "IMUP-T D ddqn ".</p><p>Figure <ref type="figure">9</ref> and Figure <ref type="figure" target="#fig_4">10</ref> show the comparison results in terms of all evaluation metrics. We can find that ddqn fluctuates slightly compared with dqn in terms of "Precision on Category", "Recall on Category", "Average Similarity"  and outperforms dqn in terms of "Average Distance" on both New York and Beijing datasets. A potential reason for this observation is that the double DQN alleviates the Q-value overestimation of DQN. So the double DQN can provide an objective and fair Q-value for each action, resulting in learning more effective action selection and action evaluation policies. Thus, the reinforced agent can imitate the user mobility behavior better than before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Comparison of different state update strategies</head><p>We compare our newly proposed state update strategy (i.e., long-short term influence of interactions) with the one from the conference version (i.e., incremental update without differentiating the importance of interactions). Specifically, we use up1 and up2 to denote the state update strategy of the conference version and the one newly proposed in this work respectively. Based on two sampling strategies, we further develop four variants of IMUP, namely "IMUP-r up1 ","IMUP-r up2 ", "IMUP-T D up1 ","IMUP-T D up2 ". Figure <ref type="figure" target="#fig_4">11</ref> and Figure <ref type="figure" target="#fig_4">12</ref> show the comparison results in terms of "Precision on Category", "Recall on Category", "Average Similarity", and "Average Distance". We can observe that up2 is better than up1 in terms of majority evaluation metrics and fluctuates in a few ones on both New York and Beijing datasets. A potential reason is that the long-short term influenced based state update strategy can discard old and ambiguous user visit preferences and capture the newest user interests in time. As a result, the state update strategy of this work can help the reinforced agent imitate user mobility behavior quickly and perfectly.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Comparison of different state initialization</head><p>We adopt StructRL to convert user mobility graphs to representations, which are used to initialize the user state of our framework. To study the influence of state initialization strategies on user profiling, we conduct the comparative experiment based on the journal version framework (IMUP * ) by replacing the StructRL with GAE <ref type="bibr" target="#b7">[8]</ref>, VGAE <ref type="bibr" target="#b7">[8]</ref>, ARGA <ref type="bibr" target="#b16">[17]</ref>, and ARGVA <ref type="bibr" target="#b16">[17]</ref> respectively. Table <ref type="table" target="#tab_7">4</ref> and Table <ref type="table" target="#tab_8">5</ref> show the comparison results in terms of "Precision on Category", "Recall on Category", "Average Distance", and "Average Similarity". We can find that the next-visit prediction performance fluctuates slightly under different state initialization strategies. This demonstrates the robustness of IMUP * against distinct state initialization methods. A potential reason is that the state update strategy is able to discard the old and useless information and capture the new and significant characteristics of user mobility behaviors in time.</p><p>As the progress of the learning process, the information in the initial state will be replaced with new user mobility characteristics, resulting in the robustness of IMUP * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">Analysis of spatial KG</head><p>We evaluate the spatial KG's contribution on modeling user representations. To set the control group, we develop a vari-ant of the proposed "IMUP", namely "IMUP -". "IMUP -" takes only the user and POI representations as the environment state, while other components of remains the same. The POI state (representations) is randomly initialized and state update is still based on the mutual interactions, but Equation 9 and Equation 11 are omitted. Figure <ref type="figure" target="#fig_4">13</ref> and Figure <ref type="figure" target="#fig_4">14</ref> show the comparison results. We can observe that the performance of "IMUP" outperforms "IMUP -" in terms of the four metrics over both two datasets. The results validate that the integration of semantics from spatial KG indeed enhances the modeling of user preferences on visit event. In the meantime, the incrementally updated next-visit planner (agent) is forced to bring in semantics from spatial KG to mimic the personalized user patterns, paced with the incrementally updated user representations (state).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.11">Analysis of rewards</head><p>There are two types of rewards in the experiment: r1 and r2. Each reward function includes three parts: (1) distance r d , (2) category similarity r c , and (3) POI difference between the predicted and real user visit events r p , where contributions are controlled by three weights ? d , ? c , and ? p respectively. To analyze the contribution of these three factors, given the learning rate=1e -5, we project the results (? d , ? c , ? p , metric) into the 3D space, where the x, y, z axis corresponds to each factor respectively. We assign the gradient color to each point such that the better the performance over the metric, the darker the color is assigned.</p><p>Figure <ref type="figure" target="#fig_4">15</ref> and Figure <ref type="figure" target="#fig_4">16</ref> show the reward analysis of r1. An interesting observation is that in the case of "Precision on Category", "Recall on Category" and "Average Similarity", the contribution of category similarity r c is higher than the other two factors, but in the case of "Average Distance", the contributions of distance r d and POI r p surpass the category similarity r c . The reason is quite intuitive that the category similarity r c directly determines the direction of policy training towards exploring more similar POI categories, while  A careful inspection of Figure <ref type="figure" target="#fig_4">15</ref> and Figure <ref type="figure" target="#fig_4">16</ref> suggests that although the performance over certain metric is highly related to specific factors (e.g., the category similarity r c is highly related to category-related performance), the best results are not achieved at the extreme case such that some factors are pushed to zeros. On the contrary, the best results are achieved at the balance of these three factors, which may reveal some dependencies among POI locations and categories introduced by the spatial KG.  <ref type="figure" target="#fig_9">18</ref> show the result of reward analysis for the reward function r2. An interesting observation is that compared with r1, the dark points all shift to the left side for all evaluation metrics. The observation means that in order to obtain better predictions, we need to increase the contribution of r d by increasing the value of ? d . A potential reason is after subtracting baseline expectations, the value of r d is lower than r c and r p . As a result, the agent considers fewer geographical factors of user mobility habits, resulting in poor next-visit prediction performance. To accurately simulate user mobility, we must consider the geographical and semantic characteristics of user mobility completely. Thus, we can improve the contribution of r d by increasing the value of ? d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Mobile User Profiling. Our work is connected to Mobile user profiling. User profiling refers to quantifying users' characteristics <ref type="bibr" target="#b20">[21]</ref>. User profiling methods can be categorized into two groups: (1) explicit extraction, in which user profiles are explicitly predefined on documents, and (2) learning-based approach, which focus on learning user representations from users' historical behavior data <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Our work is especially related to the learning-based approach for mobile users. For example, factorization-based approaches are exploited to model the integration of geographical and temporal influences of human mobility behaviors <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>; deep learning-based approaches are then proposed to learn latent representations of users by leveraging the power of deep neural networks <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>; more advanced techniques (e.g., adversarial learning) are further introduced with emphasis on substructures of user mobility patterns <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Reinforcement Learning for Online User Modeling. Our work is related to reinforcement learning for online user modeling. Reinforcement learning-based algorithms model online user behavior (e.g., clicks, reviews, purchases) by regarding the online behavior as a sequential decisionmaking process <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b29">[30]</ref>. For example, Zhao et al. propose the model "DEERS" to model both the positive and negative feedback through learning the sequential interactions via the reinforcement learning, with recommending trial-and-error items and receiving reinforcements of these items from users' feedback <ref type="bibr" target="#b30">[31]</ref>. Zheng et al. propose a reinforcement learning framework to provide personalized new recommendation by considering user return pattern to capture implicit user feedback and provide new news by effective exploring strategies <ref type="bibr" target="#b31">[32]</ref>. Chen et al. propose to utilize Markov Decision Process to model the sequential interactions between users and online recommender systems, and employing reinforcement learning to force an optimal policy for generating recommendation <ref type="bibr" target="#b1">[2]</ref>. Isshu et al. exploits the multi-armed bandit approach to solve the cold-start problem based on the rewards from testing each options <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION REMARKS</head><p>In this paper, we propose a new learning criteria, namely reinforced imitative graph learning (RIGL), for mobile user profiling. Different from the minimization of traditional reconstruction loss or prediction loss, the proposed RIGL leverages an agent to mimic users' patterns and preferences. The optimal user profiles will be obtained as the agent can perfectly imitate users' decisions. In such setting, we integrated spatial KG to reinforcement learning to incrementally learn user representations and generate the next-visit prediction. Specifically, we formulated the state as the combination of users and spatial KG, where the mutual interactions were modeled to incrementally update the representations of users and spatial KG based on the temporal context, by considering long-short term influences of interactions. The newly developed policy network exploits Double DQN to avoid the problem of overestimating user decisions. The policy aimed to mimic users' patterns to generate accurate next-visit prediction with the newly designed hierarchical DQN and improved sampling strategy. From the experiment, we can observe that the semantics introduced by the spatial KG improve the modeling of user representations for better understanding user patterns and preferences. This demonstrates that the proposed priority-based sampling strategy enhances the learning procedure, where TD-based is better than reward-based priority approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Framework Overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where b d , b c and b p are the reward baselines of r d , r c and r p respectively. We set the reward baselines as follows: (1) we first run the experiment for 100 rounds, and obtain the empirical range of the reward values; (2) we then set the value of b d , b c and b p as the first quartile of the corresponding empirical reward range. So if</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :Fig. 3 :</head><label>23</label><figDesc>Fig. 2: DQN Network structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Fig. 5: Overall comparison w.r.t. New York dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 1 )</head><label>1</label><figDesc>IMUP-r, where the model utilizes the sampling strategy with the priority x r ; (2) IMUP-TD, where the model utilizes the sampling strategy with the priority x T D . In this work, we propose a new reward function, a new policy network, and a new state update strategy enhance the conference version work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Fig. 7: The comparison between the original reward (r1) and the enhanced reward (r2) w.r.t. New York dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :Fig. 10 :Fig. 11 :Fig. 12 :</head><label>9101112</label><figDesc>Fig. 9: The comparison between the original agent (dqn) and the enhanced agent (ddqn) w.r.t. New York dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 :Fig. 14 :</head><label>1314</label><figDesc>Fig. 13: Analysis of spatial KG w.r.t. New York dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 15 :Fig. 17 :</head><label>1517</label><figDesc>Fig. 15: Reward analysis of r1 w.r.t. New York dataset.</figDesc><graphic url="image-8.png" coords="11,181.91,185.57,120.22,96.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 18 :</head><label>18</label><figDesc>Fig. 18: Reward analysis of r2 w.r.t. Beijing dataset. distance r d and POI r p guides the policy to find POIs as close as possible to the users' intention.A careful inspection of Figure15and Figure16suggests that although the performance over certain metric is highly related to specific factors (e.g., the category similarity r c is highly related to category-related performance), the best results are not achieved at the extreme case such that some factors are pushed to zeros. On the contrary, the best results are achieved at the balance of these three factors, which may reveal some dependencies among POI locations and categories introduced by the spatial KG.</figDesc><graphic url="image-15.png" coords="11,56.38,471.36,119.69,111.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 17</head><label>17</label><figDesc>Figure17and Figure18show the result of reward analysis for the reward function r2. An interesting observation is that compared with r1, the dark points all shift to the left side for all evaluation metrics. The observation means that in order to obtain better predictions, we need to increase the contribution of r d by increasing the value of ? d . A potential reason is after subtracting baseline expectations, the value of r d is lower than r c and r p . As a result, the agent considers fewer geographical factors of user mobility habits, resulting in poor next-visit prediction performance. To accurately simulate user mobility, we must consider the</figDesc><graphic url="image-17.png" coords="11,308.31,471.36,119.69,111.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Summary of Notations.</figDesc><table><row><cell>Symbol</cell><cell>Definition</cell></row><row><cell>u, u (?)</cell><cell>User, user state (representations).</cell></row><row><cell>c (?)</cell><cell>POI category.</cell></row><row><cell>a (?)</cell><cell>Action.</cell></row><row><cell>P (?) , h P (?)</cell><cell>POI, POI representations.</cell></row><row><cell>rel (?)</cell><cell>Relation representations.</cell></row></table><note><p>t (?)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>State Representation State Action (POI) Explore Exploit Agent Reward Function Memory Store Reinforced Imitation Update user and spatial KG to next new state</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>User</cell><cell>User Representation</cell></row><row><cell></cell><cell>Visit Event</cell><cell>Concatenate</cell></row><row><cell>Temporal Context</cell><cell>Spatial KG</cell><cell>Spatial KG Representation</cell></row><row><cell></cell><cell></cell><cell>Policy Update</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 :</head><label>2</label><figDesc>Statistics of the checkin data.</figDesc><table><row><cell>City</cell><cell cols="4"># Check-ins # POIs # POI Categories Time Period</cell></row><row><cell>New York</cell><cell>227, 428</cell><cell>38, 334</cell><cell>251</cell><cell>4/2012-2/2013</cell></row><row><cell>Beijing</cell><cell>6, 465</cell><cell>3, 434</cell><cell>9</cell><cell>3/2011-5/2011</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 :</head><label>3</label><figDesc>Statistics of the taxi data.</figDesc><table><row><cell>City</cell><cell># Transactions Time Period</cell></row><row><cell cols="2">New York 161, 211, 550 4/2012-2/2013</cell></row><row><cell>Beijing</cell><cell>12, 000, 000 3/2011-5/2011</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 :</head><label>4</label><figDesc>Comparison of different state initialization methods w.r.t. New York dataset</figDesc><table><row><cell></cell><cell>StructRL</cell><cell>GAE</cell><cell>VGAE</cell><cell>ARGA</cell><cell>ARGVA</cell></row><row><cell>Prec CAT</cell><cell>0.02216</cell><cell>0.02859</cell><cell>0.02955</cell><cell>0.02628</cell><cell>0.02902</cell></row><row><cell>Rec CAT</cell><cell>0.04942</cell><cell>0.04773</cell><cell>0.05465</cell><cell>0.05461</cell><cell>0.05364</cell></row><row><cell>Avg Sim</cell><cell>0.09892</cell><cell>0.08328</cell><cell>0.1039</cell><cell>0.1039</cell><cell>0.09131</cell></row><row><cell>Avg Dist</cell><cell>14.8151</cell><cell>13.2210</cell><cell>15.2624</cell><cell>16.7266</cell><cell>14.3054</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5 :</head><label>5</label><figDesc>Comparison of different state initialization methods w.r.t. Beijing dataset</figDesc><table><row><cell></cell><cell>StructRL</cell><cell>GAE</cell><cell>VGAE</cell><cell>ARGA</cell><cell>ARGVA</cell></row><row><cell>Prec CAT</cell><cell>0.48019</cell><cell>0.47945</cell><cell>0.49185</cell><cell>0.49783</cell><cell>0.47597</cell></row><row><cell>Rec CAT</cell><cell>0.69242</cell><cell>0.69242</cell><cell>0.67542</cell><cell>0.66461</cell><cell>0.66615</cell></row><row><cell>Avg Sim</cell><cell>0.78636</cell><cell>0.78648</cell><cell>0.77479</cell><cell>0.76637</cell><cell>0.76746</cell></row><row><cell>Avg Dist</cell><cell>20.7080</cell><cell>21.2318</cell><cell>29.8552</cell><cell>30.1306</cell><cell>22.9128</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was supported by the <rs type="funder">National Science Foundation(NSF)</rs> via the grant number: <rs type="grantNumber">1755946</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XEKwgva">
					<idno type="grant-number">1755946</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Sungwoon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heonseok</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uiwon</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05532</idno>
		<title level="m">Reinforcement learning based recommender system using biclustering technique</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on recommender systems</title>
		<meeting>the 10th ACM conference on recommender systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">User profiling in personal information agents: a survey</title>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Godoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Analia</forename><surname>Amandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="329" to="361" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Poi recommendation: Towards fused matrix factorization with geographical and temporal influences</title>
		<author>
			<persName><forename type="first">Jean-Benoit</forename><surname>Griesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Talel</forename><surname>Abdessalem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Naacke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM Conference on Recommender Systems</title>
		<meeting>the 9th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="301" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Session-based recommendations with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Bal?zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06939</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Variational graph autoencoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kula</surname></persName>
		</author>
		<author>
			<persName><surname>Spotlight</surname></persName>
		</author>
		<ptr target="https://github.com/maciejkula/spotlight" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geomf: joint geographical modeling and matrix factorization for point-of-interest recommendation</title>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="831" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Reinforcement learning for robots using neural networks</title>
		<author>
			<persName><forename type="first">Long-Ji</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie-Mellon Univ Pittsburgh PA School of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-ninth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling the interaction coupling of multi-view spatiotemporal contexts for destination prediction</title>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 SIAM International Conference on Data Mining</title>
		<meeting>the 2018 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="171" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for recommender systems</title>
		<author>
			<persName><forename type="first">Isshu</forename><surname>Munemasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Tomomatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunioki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Takagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 international conference on information and communications technology (icoiact)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="226" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2609" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Online poi recommendation: Learning dynamic geo-human interactions in streams</title>
		<author>
			<persName><forename type="first">Dongjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10983</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reinforced imitative graph representation learning for mobile user profiling: An adversarial training perspective</title>
		<author>
			<persName><forename type="first">Dongjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanchun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4410" to="4417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial substructured representation learning for mobile user profiling</title>
		<author>
			<persName><forename type="first">Pengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="130" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploiting mutual information for substructureaware graph representation learning</title>
		<author>
			<persName><forename type="first">Pengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanchun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kien</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 29th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Incremental mobile user profiling: Reinforcement learning with spatial knowledge graph for modeling event streams</title>
		<author>
			<persName><forename type="first">Pengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="853" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ensemble-spotting: Ranking urban vibrancy via poi embedding with multi-view spatial graphs</title>
		<author>
			<persName><forename type="first">Pengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guannan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 SIAM International Conference on Data Mining</title>
		<meeting>the 2018 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="351" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-level recommendation reasoning over knowledge graphs with reinforcement learning</title>
		<author>
			<persName><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bridging collaborative filtering and semi-supervised learning: a neural approach for poi recommendation</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanxiao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1245" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling user activity preference by leveraging user spatial temporal characteristics in lbsns</title>
		<author>
			<persName><forename type="first">Dingqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="142" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial-aware hierarchical collaborative deep learning for poi recommendation</title>
		<author>
			<persName><forename type="first">Weiqing</forename><surname>Hongzhi Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2537" to="2551" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning based recommender system: A survey and new perspectives</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recommendations with negative feedback via pairwise deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoye</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1040" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Drn: A deep reinforcement learning framework for news recommendation</title>
		<author>
			<persName><forename type="first">Guanjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
		<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
