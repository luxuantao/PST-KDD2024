<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shifting Weights: Adapting Object Detectors from Image to Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Tang</surname></persName>
							<email>kdtang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
							<email>vigneshr@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
							<email>feifeili@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
							<email>koller@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Shifting Weights: Adapting Object Detectors from Image to Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">808FED1F3A9366EA4339205CB6EE3A77</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Typical object detectors trained on images perform poorly on video, as there is a clear distinction in domain between the two types of data. In this paper, we tackle the problem of adapting object detectors learned from images to work well on videos. We treat the problem as one of unsupervised domain adaptation, in which we are given labeled data from the source domain (image), but only unlabeled data from the target domain (video). Our approach, self-paced domain adaptation, seeks to iteratively adapt the detector by re-training the detector with automatically discovered target domain examples, starting with the easiest first. At each iteration, the algorithm adapts by considering an increased number of target domain examples, and a decreased number of source domain examples. To discover target domain examples from the vast amount of video data, we introduce a simple, robust approach that scores trajectory tracks instead of bounding boxes. We also show how rich and expressive features specific to the target domain can be incorporated under the same framework. We show promising results on the 2011 TRECVID Multimedia Event Detection [1] and LabelMe Video <ref type="bibr" target="#b1">[2]</ref> datasets that illustrate the benefit of our approach to adapt object detectors to video.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Following recent advances in learning algorithms and robust feature representations, tasks in video understanding have shifted from classifying simple motions and actions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> to detecting complex events and activities in Internet videos <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Detecting complex events is a difficult task, requiring probabilistic models that can understand the semantics of what is occuring in the video. Because many events are characterized by key objects and their interactions, it is imperative to have robust object detectors that can provide accurate detections. In this paper, we focus on the problem of detecting objects in complex Internet videos. It is difficult to obtain labeled objects in these types of videos because of the large number of frames, and the fact that objects may not appear in many of them. Thus, a common approach is to train object detectors from labeled images, which are widely available. However, as seen in Figure <ref type="figure">1</ref>, the domain of images and videos is quite different, as it is often the case that images of objects are taken in controlled settings that differ greatly from where they appear in real-world situations, as seen in video. Thus, we cannot typically expect a detector trained on images to work well in videos.</p><p>To adapt object detectors from image to video, we take an incremental, self-paced approach to learn from the large amounts of unlabeled video data available. We make the assumption that within our unlabeled video data, there exist instances of our target object. However, we do not assume that every video has an instance of the object, due to the noise present in Internet videos. We start by introducing a simple, robust method for discovering examples in the video data using Kanade-Lucas-Tomasi (KLT) feature tracks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Building on the discovered examples, we introduce a novel formulation for unsupervised domain adaptation that adapts parameters of the detector from image TRECVID MED (Video domain) ImageNet (Image domain)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skateboard</head><p>Sewing Machine Sandwich</p><p>Figure <ref type="figure">1</ref>: Images of the "Skateboard", "Sewing machine", and "Sandwich" classes taken from (top row) ImageNet <ref type="bibr" target="#b6">[7]</ref> and (bottom row) TRECVID MED <ref type="bibr" target="#b0">[1]</ref> illustrating differences in domain.</p><p>to video. This is done by iteratively including examples from the video data into the training set, while removing examples from the image data based on the difficulty of the examples. We define easy examples as ones with labels that can be predicted confidently (e.g., high likelihood, large distance from margin), and thus are more likely to be correct. In addition, it is common to have discriminative features that are only available in the target domain, which we term target features.</p><p>For example, in the video domain, there are contextual features in the spatial and temporal vicinity of our detected object that we can take advantage of when performing detection. Our approach is able to incorporate the learning of parameters for these target features into a single objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most relevant are works that also deal with adapting detectors to video <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, but these works typically deal with a constrained set of videos and limited object classes. The work of <ref type="bibr" target="#b13">[14]</ref> deals with a similar problem, but they adapt detectors from video to image. Our overall method is also similar to <ref type="bibr" target="#b14">[15]</ref>, in which we adopt an incremental approach to learn object category models.</p><p>Our setting is closely related to the domain adaptation problem, which has been studied extensively in vision settings. Several previous approaches focus on learning feature transformations between domains <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. More similar to our method are approaches based on optimizing Support Vector Machine (SVM) related objectives <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> or joint cost functions <ref type="bibr" target="#b24">[25]</ref>, that treat the features as fixed and seek to adapt parameters of the classifier from source to target domain. However, with the exception of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>, previous works deal with supervised or semi-supervised domain adaptation, which require labeled data in the target domain to generate associations between the source and target domains. In our setting, unsupervised domain adaptation, the target domain examples are unlabeled, and we must simultaneously discover and label examples in addition to learning parameters.</p><p>The objective we optimize to learn our detector draws inspiration from <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>, in which we include and exclude the loss of certain examples using binary-valued indicator variables. Although our formulation is similar to <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, our method is iterative and anneals weights that govern the number of examples to use, which is similar to the idea of self-paced learning <ref type="bibr" target="#b25">[26]</ref>, where a single weight is decreased to eventually include the loss of all examples in the objective. However, our method is different from <ref type="bibr" target="#b25">[26]</ref> in that we have three sets of weights that govern the source examples, target examples, and target features. The weights are annealed in different directions, giving us the flexibility to iteratively include examples from the target domain, exclude examples from the source domain, and include parameters for the target features. In addition, our objective is able to incorporate target features, which is novel and not considered in <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>.</p><p>Previous works have also considered ideas similar to our target features <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. The work of <ref type="bibr" target="#b28">[29]</ref> considers feature augmentation, but only with observed features common to both domains. Unobserved features in the context of clustering are investigated in <ref type="bibr" target="#b30">[31]</ref>, but in their setting all examples are assumed to have the same unobserved features. In <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>, features or modalities unseen in the training data are used to help in testing. However, both works assume there exists relationships between the seen and unseen features, whereas our target features are completely unrestricted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Approach</head><p>We begin by providing an overview of our approach to adapting object detectors, as illustrated in Figure <ref type="figure" target="#fig_0">2</ref>, and then elaborate on each of the steps. We assume that we are given a large amount of unlabeled video data with positive instances of our object class within some of these videos.</p><p>Discover top K video positives + negatives from unlabeled videos using detector Train detector using: image positives + negatives Re-train detector using:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Update annealed weights to include/exclude more examples and features</head><p>Step 1</p><p>Step 2</p><p>Step 3</p><p>Step 4 We start by initializing our detector using image positives and negatives (Step 1). We then proceed to enter a loop in which we discover the top K video positives and negatives (Step 2), re-train our detector using these (Step 3), and then update the annealed parameters of the algorithm (Step 4).</p><formula xml:id="formula_0">IM+</formula><p>We initialize our detector (Step 1 of Figure <ref type="figure" target="#fig_0">2</ref>) by training a classifier on the labeled image positives and negatives, which we denote by our dataset (hx 1 , y 1 i, ..., hx n , y n i) with binary class labels y i 2 { 1, 1}. We consider a common method of learning weights w of a linear classifier:</p><formula xml:id="formula_1">w = arg min w r(w) + C n X i=1 Loss(x i , y i ; w) !<label>(1)</label></formula><p>where r(•) is a regularizer over the weights, Loss(•) is a loss function over the training example, and C controls the tradeoff between the two.</p><p>Our goal then is to discover the top K positive and negative examples from the unlabeled videos, and to use these examples to help re-train our detector. We do not attempt to discover all instances, but simply a sufficient quantity to help adapt our detector to the video domain. To discover the top K video positives and negatives (Step 2 of Figure <ref type="figure" target="#fig_0">2</ref>), we utilize the strong prior of temporal continuity and score trajectory tracks instead of bounding boxes, which we describe in Section 3.1.</p><p>Given the discovered examples, we optimize a novel objective inspired by self-paced learning <ref type="bibr" target="#b25">[26]</ref> that simultaneously selects easy examples and trains a new detector (Step 3 of Figure <ref type="figure" target="#fig_0">2</ref>). Using this new detector, we repeat this process of example discovery and detector training until convergence, as illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discovering Examples in Video</head><p>In this step of the algorithm, we are given weights w of an object detector that can be used to score bounding boxes in video frames. A naive approach would run our detector on frames of video, taking the highest scoring and lowest scoring bounding boxes as the top K video positives and negatives. Although reasonable, this method doesn't take advantage of temporal continuity in videos. An object that appears in one frame of a video is certain to appear close in neighboring frames as well. Previous works have shown this intuition to yield good results <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>Track-based scoring Our key idea is to score trajectory tracks, rather than bounding boxes, as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. We obtain tracks by running a KLT tracker on our videos, which tracks a sparse set of features over large periods of time. Because of the large number of unlabeled videos we have, we elect to extract KLT tracks rather than computing dense tracks using optical flow. Note that these tracks follow features, and so they may not correspond to centered locations of objects. Advantages Compared to the naive approach without tracks, this approach allows us to recover from false detections with high scores, which are common for weak detectors, as it is less likely that there will be multiple false detections with high scores along a KLT track. Similarly, if the detection scores are consistently high along many points of a track, we can be more confident of the object's presence along the track. Hence, we can obtain novel examples of the object from various points of the track that had low scores, since we know the trajectory should correspond to the object. The same intuitions hold for true detections with low scores and obtaining negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-Paced Domain Adaptation</head><p>In this step of the algorithm, we are given the discovered top K video positives and negatives, which we denote by the dataset (hz</p><formula xml:id="formula_2">1 , h 1 i, ..., hz k , h k i).</formula><p>Together with our original dataset (hx 1 , y 1 i, ..., hx n , y n i), we would like to learn a new detector. A simple method would be to re-train our detector with both datasets using Equation <ref type="formula" target="#formula_1">1</ref>. However, we typically aren't certain that the labels h are correct, especially in the first iteration when our detector is trained solely from the image examples. Basic approach We start by introducing our approach without target features. We introduce binary variables v 1 , ..., v n for the source domain (image) examples, and binary variables u 1 , ..., u k for the target domain (video) examples. A value of 0 indicates that an example is difficult, and so we would like to remove its loss from consideration in the objective function. To prevent the algorithm from assigning all examples to be difficult, we introduce parameters K source and K target that control the number of examples considered from the source and target domain, respectively.</p><formula xml:id="formula_3">(w t+1 , v t+1 , u t+1 ) = arg min w,v,u r(w) + C ⇣ n X i=1 v i Loss(x i , y i ; w) + k X j=1 u j Loss(z j , h j ; w) ⌘ 1 K source n X i=1 v i 1 K target k X j=1 u j !<label>(2)</label></formula><p>If K target is large, the algorithm prefers to consider only easy target examples with a small Loss(•), and the same is true for K source . In the annealing of the weights for the algorithm (Step 4 of Figure <ref type="figure" target="#fig_0">2</ref>), we decrease K target and increase K source to iteratively include more examples from the target domain and decrease examples from the source domain.</p><p>Similar to self-paced learning <ref type="bibr" target="#b25">[26]</ref>, we obtain a tight relaxation when allowing the binary variables v and u to take on any value in the interval [0, 1]. With the choice of r(•) and Loss(•) convex in w, the problem becomes a bi-convex problem, and can be solved by alternating between (1) solving for w given v and u, and (2) solving for v and u given w. We refer the reader to <ref type="bibr" target="#b25">[26]</ref> for further intuitions on the binary variables and annealed weights.</p><p>Leveraging target features Often, the target domain we are adapting to has additional features we can take advantage of. At the start, when we've only learned from a few examples in our target domain, we do not wish to rely on these rich and expressive features, as they can easily cause us to overfit. However, as we iteratively adapt to the target domain and build more confidence in our detector, we can start utilizing these target features to help with detection. The inclusion of these features is naturally self-paced as well, and can be easily integrated into our framework.</p><p>We assume there are a set of features that are shared between the source and target domains as target features, we initialize those features to be 0 so that w target doesn't affect the loss on the source data. The new objective function is formulated as:</p><formula xml:id="formula_4">(w t+1 , v t+1 , u t+1 ) = arg min w,v,u r(w) + C ⇣ n X i=1 v i Loss(x i , y i ; w) + k X j=1 u j Loss(z j , h j ; w) ⌘ + 1 K f eat ||w target || 1 1 K source n X i=1 v i 1 K target k X j=1 u j !<label>(3)</label></formula><p>This is similar to Equation <ref type="formula" target="#formula_3">2</ref>, with the addition of the L</p><formula xml:id="formula_5">1 norm term 1 K f eat ||w target || 1 .</formula><p>To anneal the weights for target features, we increase K f eat to iteratively reduce the L 1 norm on the target features so that w target can become non-zero. Intuitively, we are forcing the weights w to only use shared features first, and to consider more target features when we have a better model of the target domain. The optimization can be solved in the same manner as Equation <ref type="formula" target="#formula_3">2</ref>. We can also approximate the L 1 norm term for all target features to be effectively binary, forcing K f eat to be 0 initially and switching to 1 at a particular iteration. This amounts to only considering target features after a certain iteration, and is done in our experiments for more tractable learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present experimental results for adapting object detectors on the 2011 TRECVID Multimedia Event Detection (MED) dataset <ref type="bibr" target="#b0">[1]</ref> and LabelMe Video <ref type="bibr" target="#b1">[2]</ref> dataset. For both, we select a set of objects which are known to appear in the videos. We used images from ImageNet <ref type="bibr" target="#b6">[7]</ref> for the labeled image data, as there are a large number of diverse categories on ImageNet that correspond well with the objects that appear in the videos. We evaluate the detection performance of our models with the measure used in the PASCAL Visual Object Classes challenge <ref type="bibr" target="#b32">[33]</ref>, and report average precision (AP) scores for each class. The detection scores are computed on annotated video frames from the respective video datasets that are disjoint from the unlabeled videos used in the adapting stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>In our experiments, we use object detectors that are rectangular filters over Histogram-of-Gradient (HOG) features <ref type="bibr" target="#b33">[34]</ref>. We use L 2 regularization for r(•) and hinge loss for Loss(•), which corresponds to the standard linear SVM formulation. For target features, we use contextual spatial features. The spatial features are taken to be HOG features bordering the object with dimensions half the size of the object bounding box. As described previously, we approximate the L 1 norm term to be binary to enable fast training using LIBLINEAR <ref type="bibr" target="#b34">[35]</ref> when optimizing for w. This also further decreases the number of model parameters needed to be searched over.</p><p>To isolate the effects of adaptation and better analyze our method, we restrict our experiments to the setting in which we fix the video negatives, and focus our problem on adapting from the labeled image positives to the unlabeled video positives. This scenario is realistic and commonly seen, as we can easily obtain video negatives by sampling from a set of unlabeled or weakly-labeled videos.  Model selection The free model parameters that can be varied are the number of top K examples to discover, the ending K source weight, and whether or not to use target features. In our results, we perform model selection by comparing the distribution of scores on the discovered video positives. The distributions are compared between the initial models from iteration 1 for different model parameters to select K and K source , and between the final iteration 5 models for different model parameters to determine the use of target features. This allows us to evaluate the strength of the initial model trained on the image positives and video negatives, as well as our final adapted model. We select the model with the distributions indicating the highest confidence in its classification boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Comparisons</head><p>InitialBL This baseline is the intial detector trained only on image positives and video negatives.</p><p>VideoPosBL This baseline uses the intial detector to discover the top K video positives from the unlabeled video, then trains with all these examples without iterating. Thus, it incorporates our idea of discovering video positives by scoring tracks and re-training, but does not use self-paced domain adaptation for learning weights. It can also be thought of as our method run for one iteration.</p><p>Our method(nt) This baseline uses our full method with the exception of target features.</p><p>Gopalan et al. This is a state-of-the-art method for unsupervised domain adaptation <ref type="bibr" target="#b17">[18]</ref> that models the domain shift in feature space. Since we are not given labels in the target domain, most previous methods for domain adaptation cannot be applied to our setting. This method samples subspaces along the geodesic between the source and target domains on the Grassman manifold. Using projections of both source and target data onto the common subspaces, they learn a discriminative classifier using partial least squares (PLS) with available labels from either domains. We ran their code using their suggested parameter settings to obtain results for their method on our task. We also show results for their method using a linear SVM as the classifier to allow for fair comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TRECVID MED</head><p>The 2011 TRECVID MED dataset <ref type="bibr" target="#b0">[1]</ref> consists of a collection of Internet videos collected by the Linguistic Data Consortium from various Internet video hosting sites. There are a total of 15 complex events, and videos are labeled with either an event class or no label, where an absence of label indicates the video belongs to no event class. We select 6 object classes to learn object detectors for because they are commonly present in selected events: "Skateboard", "Animal", "Tire", "Vehicle", "Sandwich", and "Sewing machine". These objects appear respectively in the events "Attempting a  board trick", "Feeding an animal", "Changing a vehicle tire", "Getting a vehicle unstuck", "Making a sandwich", and "Working on a sewing project". The video negatives were randomly sampled from the videos that were labeled with no event class.</p><p>To test our algorithm, we manually annotated approximately 200 frames with bounding boxes of positive examples for each object, resulting in 1234 annotated frames total from over 500 videos, giving us a diverse set of situations the objects can appear in. For each object, we use 20 videos from the associated event as unlabeled video training data. Results are given in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">LabelMe Video</head><p>LabelMe Video <ref type="bibr" target="#b1">[2]</ref> is a database of real-world videos that contains a large set of annotations including object category, shape, motion, and activity information. We use the database of videos that was introduced in the original paper <ref type="bibr" target="#b1">[2]</ref>. There are a large number of objects that are annotated in this database, and we select the most frequently occuring objects that are not scene parts, resulting in 5 objects: "Car", "Boat", "Bicycle", "Dog", and "Keyboard". The video negatives were randomly sampled from the videos that were not annotated with any of these objects.</p><p>We extract more than 200 frames with positive examples for each object class, resulting in a test set of 1137 images. For each object class, we use the remaining videos that contain the object as the unlabeled video training data, resulting in around 9 videos per object. Results are given in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>From our results in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref>, we can observe similar patterns for most object classes. First, we note that the "VideoPosBL" baseline typically performs on par with the "InitialBL" baseline, and rarely does it post a slight gain in performance. This shows that if we discover the top K video positives and re-train our detector with all of them, we do not obtain consistent gains in performance. Our method of self-paced domain adaptation is crucial in this case, as we can see that our full method typically outperforms all other methods by significant margins. As illustrated in Figure <ref type="figure">4</ref>, our method is able to add new video positives from iteration to iteration that are good examples, and remove bad examples at the same time. The method of Gopalan et al. <ref type="bibr" target="#b17">[18]</ref> performs very poorly when used in conjunction with the PLS classifier, but becomes more competitive when used with an SVM. However, even then their method performs much worse than our method for nearly all object classes, as it is difficult to model the underlying domain shift in feature space. This also serves to illustrate the difficulty of our problem, as poor adaptation can lead to results worse than the baselines. We show visualizations of our detections compared to baseline methods in Figure <ref type="figure">5</ref>.</p><p>Observing the visualizations of the learned weights for the "Tire", "Car" and "Sandwich" classes in Figure <ref type="figure" target="#fig_7">6</ref>, we see that weights trained with our method exhibit more clearly defined structure  than the "InitialBL" baseline. The target features also help performance significantly. By capturing interesting patterns in the spatial context, difficult objects can become easier to detect in the target domain. For the "Sandwich" class, we can see circular weights in the spatial context surrounding the sandwich, suggesting that sandwiches typically appear on plates, and for "Car", we can clearly distinguish weights for the road beneath the car object. We observe an average AP gain of 3.93% for classes that choose models with target features versus no target features. Note that we chose to use simple spatial context as target features in our models, as they are fast to implement and easily incorporated. However, we hypothesize that the inclusion of more complex target features such as temporal movement could help our method achieve even better results.</p><p>We observe that for the "Vehicle" and "Keyboard" classes, the "VideoPosBL" baseline performs better than our full method. Although this is not a common occurrence, it can happen when our method of self-paced domain adaptation replaces good video positives taken in the first iteration with bad examples in future iterations. This situation arises when there are incorrect examples present in the easiest of the top K video positives, causing our detector to re-train and iteratively become worse. If we had better methods for model selection, we could also search over the number of total iterations as a model parameter, which would include the "VideoPosBL" model in our set of models to select over, as it is essentially our method run for a single iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we have introduced an approach for adapting detectors from image to video. To discover examples in the unlabeled video data, we classify tracks instead of bounding boxes, allowing us to leverage temporal continuity to avoid spurious detections, and to discover examples we would've otherwise missed. Furthermore, we introduce a novel self-paced domain adaptation algorithm that allows our detector to iteratively adapt from source to target domain, while also considering target features unique to the target domain. Our formulation is general, and can be applied to various other problems in domain adaptation. We've shown convincing results that illustrate the benefit of our approach to adapting object detectors to video.</p><p>Possible directions for future work could include better methods for model selection. A measure that would allow us to estimate our performance on the target domain with theoretical guarantees would be an interesting direction. Another possible direction would be to relax the assumption of having no labeled target domain examples, and to formulate similar methods for this scenario.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Overview of our algorithm. We start by initializing our detector using image positives and negatives (Step 1). We then proceed to enter a loop in which we discover the top K video positives and negatives (Step 2), re-train our detector using these (Step 3), and then update the annealed parameters of the algorithm (Step 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>For</head><label></label><figDesc>each track, we consider the set of all bounding box placements B around it that intersect with the track. Each box placement b i 2 B is associated with a relative coordinate (b x i , b y i ) as well as a score b s i . The relative coordinate (b x i , b y i ) is the point within the box (relative to the top-left corner of the box) that intersects the track. Using this coordinate, we can compute the position of b i at every point in time along the track. Note that the number of bounding boxes in B is only dependent on the dimensions of the detector and the scales we search over. The score b s i is computed by pooling scores of the bounding box along multiple points of the track in time. We use average pooling in our experiments to be robust to noisy scores. Finally, we associate the track with the bounding box b max with the highest score, and use the score b s max as the score of the track. After scoring each track in our unlabeled videos, we select the top and bottom few scoring tracks, and extract bounding boxes from each using the associated box coordinates (b x max , b y max ) to get our top K video positives and negatives. The boxes are extracted by sampling frames along the track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: For a given KLT track, we consider all bounding box placements that intersect with it, denoted by the colored rectangular boxes. The purple cross denotes the intersection coordinates (b x i , b y i ) for each box. For each box, we average the scores at each point along the track, and take the box with the maximum score as the score and associated bounding box coordinates for this track.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Ideally, we would like to re-train with a set of easier examples whose labels we are confident of first, and then re-discover video examples with this new detector. We would also like to stop learning from examples we are unsure of in the image domain, as they may be the examples most affected by the differences in domain. By repeating this process, we can avoid bad examples and iteratively refine our set of top K video positives and negatives before having to train with all of them. Formulating this intuition, our algorithm selects easier examples to learn from in the discovered video examples, and simultaneously selects harder examples in the image examples to stop learning from. An example is difficult if it has a large loss, as we are not confident in its correct label. The number of examples selected from the video examples and image examples are governed by weights that will be annealed over iterations (Step 4 of Figure 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>shared , and a set of target domain-only features as target : = [ shared target ]. The weights w we want to learn can now be divided into w shared and w target : w = [w shared w target ]. Since the source data doesn't have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure4: Discovered top K video positives using our method for "Sandwich" and "Car". After sets of iterations, we show samples of newly discovered video positives (red boxes) that were not in the set of top K of previous iterations (left, middle columns). We also show bad examples that were removed from the top K over all iterations (right column). As our model adapts, it is able to iteratively refine its set of top K video positives. Figure best viewed magnified and in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 5: Detections for "Sandwich", "Tire", "Animal", and "Car". Green boxes detections from our method, red boxes detections from "InitialBL", blue boxes detections from "VideoPosBL", and magenta boxes detections from Gopalan et al.(SVM).Figure best viewed magnified and in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualizations of the positive HOG weights learned for three classes for the "InitialBL" baseline and our method. The spatial context weights are 0 for "InitialBL" because it does not consider target features, resulting in a black border. Figure best viewed magnified and in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average Precision (AP) values for detection on the TRECVID MED datasetObjectInitialBL VideoPosBL Our method(nt) Our method(full) Gopalan et al.<ref type="bibr" target="#b17">[18]</ref> (PLS) Gopalan et al.<ref type="bibr" target="#b17">[18]</ref> (SVM)</figDesc><table><row><cell cols="2">Skateboard Animal Tire Vehicle Sandwich Sewing machine 9.76% 4.29% 0.41% 11.22% 4.03% 10.07%</cell><cell>2.89% 0.40% 11.04% 4.08% 9.85% 9.71%</cell><cell>10.44% 0.39% 15.54% 3.57% 9.45% 10.35%</cell><cell>10.44% 3.76% 15.54% 3.57% 12.49% 10.35%</cell><cell>0.04% 0.16% 0.60% 3.33% 0.21% 0.12%</cell><cell>0.94% 0.24% 15.52% 3.16% 6.68% 3.81%</cell></row><row><cell>Mean AP</cell><cell>6.63%</cell><cell>6.33%</cell><cell>8.29%</cell><cell>9.36%</cell><cell>0.74%</cell><cell>5.06%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average Precision (AP) values for detection on the LabelMe Video datasetModel parametersIn our experiments, we fix the total number of iterations to 5 for tractable training time. For the K target and K source weights, we set values for the first and final iterations, and linearly interpolate values for the remaining iterations in between. For the K target weight, we estimate the weights so that we start by considering only the video examples that have no loss, and end with all video examples considered. For the K source weight, we vary the ending weight so that differing numbers of source examples are left for training at the final iteration. For the target features, we set the algorithm to allow target features at the midpoint of total iterations. Based on the number of KLT tracks extracted, we set the top K examples to be between 100 and 500.</figDesc><table><row><cell cols="6">Object InitialBL VideoPosBL Our method(nt) Our method(full) Gopalan et al. [18] (PLS) Gopalan et al. [18] (SVM)</cell></row><row><cell>Car Boat Bicycle Dog Keyboard 0.41% 2.60% 0.22% 19.85% 1.74%</cell><cell>2.13% 0.22% 19.76% 2.42% 0.67%</cell><cell>2.15% 0.22% 20.27% 2.47% 0.59%</cell><cell>9.18% 0.22% 20.27% 4.75% 0.59%</cell><cell>0.34% 0.05% 0.21% 0.18% 0.13%</cell><cell>1.00% 0.32% 16.32% 1.48% 0.09%</cell></row><row><cell>Mean AP 4.96%</cell><cell>5.04%</cell><cell>5.14%</cell><cell>7.00%</cell><cell>0.18%</cell><cell>3.84%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Tianshi Gao for helpful discussions. We also thank Chris Baldassano, Eric Huang, Jia Deng, and Olga Russakovsky for helpful comments on the paper. This work was supported by the Defense Advanced Research Projects Agency under Contract No. HR0011-08-C-0135 and by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center contract number D11PC20069. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, IARPA, DoI/NBC, or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Trecvid 2011 -an overview of the goals, tasks, data, evaluation mechanisms and metrics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Over</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIST</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Labelme video: Building a video database with human annotations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognizing human actions: A local svm approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning latent temporal structure for complex event detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Detection and tracking of point features</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised incremental learning for improved object detection in a video</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detection by detections: Non-parametric detector adaptation for a video</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Correspondence driven adaptation for human profile recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of facial attributes in video</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cherniavsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">OPTIMOL: automatic Online Picture collecTion via Incremental MOdel Learning</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What you saw is not what you get: Domain adaptation using asymmetric kernel transforms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bergamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An empirical analysis of domain adaptation algorithms for genomic sequence analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schweikert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Widmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-domain video concept detection using adaptive svms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual event recognition in videos by learning from web data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transductive inference for text classification using support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Safety in numbers: Learning categories from few examples with multi model knowledge transfer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Orabona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Taylor expansion based classifier adaptation: Application to person detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transfer learning by borrowing examples for multiclass object detection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative learning of relaxed hierarchy for large-scale visual recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning on the test data: Leveraging &apos;unseen&apos; features</title>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generalization in clustering with unobserved features</title>
		<author>
			<persName><forename type="first">E</forename><surname>Krupka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to recognize objects from unseen modalities</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
