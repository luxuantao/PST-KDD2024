<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Graph Learning for Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-21">21 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China Hefei</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Xiang Wang</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore Singapore Fuli Feng</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">National University of Singapore Singapore Xiangnan He</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of Science and Technology of China Hefei</orgName>
								<address>
									<settlement>Anhui</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Liang Chen</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Sun Yat-sen University Guangzhou</orgName>
								<address>
									<settlement>Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="department">Jianxun Lian</orgName>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="institution">Microsoft Research Asia China Xing Xie</orgName>
							</affiliation>
							<affiliation key="aff9">
								<orgName type="institution">Microsoft Research Asia China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-supervised Graph Learning for Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-21">21 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/1122445.1122456</idno>
					<idno type="arXiv">arXiv:2010.10783v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems â†’ Recommender systems Collaborative filtering</term>
					<term>Graph Neural Network</term>
					<term>Self-supervised Learning</term>
					<term>Long-tail</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representation learning on user-item graph for recommendation has evolved from using single ID or interaction history to exploiting higher-order neighbors. This leads to the success of graph convolution networks (GCNs) for recommendation such as PinSage <ref type="bibr" target="#b47">[48]</ref> and LightGCN [17]. Despite effectiveness, we argue that they suffer from two limitations: (1) high-degree nodes exert larger impact on the representation learning, deteriorating the recommendations of low-degree (long-tail) items; and (2) representations are vulnerable to noisy interactions, as the neighborhood aggregation scheme further enlarges the impact of observed edges.</p><p>In this work, we explore self-supervised learning on useritem graph, so as to improve the accuracy and robustness of GCNs for recommendation. The idea is to supplement the classical supervised task of recommendation with an auxiliary selfsupervised task, which reinforces node representation learning via self-discrimination. Specifically, we generate multiple views of a node, maximizing the agreement between different views of the same node compared to that of other nodes. We devise four operators to generate the views -embedding masking, embedding dropout, node dropout, and edge dropout -that augment node representation from two perspectives of ID embedding and graph structure. We term this new learning paradigm as Selfsupervised Graph Learning (SGL), implementing it on the stateof-the-art model LightGCN. Empirical studies on three benchmark datasets demonstrate the effectiveness of SGL, which improves the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Learning high-quality user and item representations from useritem interaction data is the theme of collaborative recommendation. Earlier work like matrix factorization (MF) <ref type="bibr" target="#b31">[32]</ref> projects single ID of each user (or item) into an embedding vector. Some followon studies <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> enrich the single ID with interaction history for learning better representations. More recently, representation learning has evolved from using single ID and interaction history to exploiting higher-order connectivity in user-item graph. The technique is inspired from the graph convolution networks (GCNs), which provide an effective end-to-end way to integrate multi-hop neighbors into node representation learning and achieve state-ofthe-art performance for recommendation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Despite effectiveness, current GCN-based recommendation models suffer from some limitations:</p><p>â€¢ Sparse Supervision Signal. Most models approach the recommendation task under a supervised learning paradigm <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32]</ref>, where the supervision signal comes from the observed user-item interactions. However, the observed interactions are extremely sparse <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref> compared to the whole interaction space, making it insufficient to learn quality representations. â€¢ Skewed Data Distribution. Observed interactions usually follow a power-law distribution <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref>, where the long tail consists of low-degree items that lack supervision signal. In contrast, high-degree items appear more frequently in neighborhood aggregation and supervised loss, thus exert larger impact on the representation learning. Hence, the GCNs are easily biased towards high-degree items <ref type="bibr" target="#b33">[34]</ref>, sacrificing the performance of low-degree (long-tail) items. â€¢ Noises in Interactions. Most feedback that a user provides is implicit (e.g., clicks, views), instead of explicit (e.g., ratings, likes/dislikes). As such, observed interactions usually contain noises, e.g., a user is misled to click an item and finds it unintersting after consuming it. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>. The neighborhood aggregation scheme in GCNs enlarges the impact of the interactions in representation learning, making the learning more vulnerable to interaction noises.</p><p>In this work, we focus on exploring self-supervised learning (SSL) in recommendation, to solve the foregoing limitations. Though being prevalent in computer vision (CV) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38]</ref> and natural language processing (NLP) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref>, SSL is relatively less explored in recommendation. The idea is to set an auxiliary task that distills supervision signal from the input data itself, especially through exploiting the unlabeled data space. For example, BERT <ref type="bibr" target="#b7">[8]</ref> randomly masks some tokens in a sentence, setting the prediction of the masked tokens as the auxiliary task that can capture the dependencies among tokens; RotNet <ref type="bibr" target="#b8">[9]</ref> randomly rotates labeled images, training the model on the rotated images to get improved representations for the mask task of object recognition or image classification. Clearly, compared with supervised learning, SSL allows us to exploit the unlabeled data space via making changes on the input labeled data, achieving remarkable improvements in downstream tasks <ref type="bibr" target="#b4">[5]</ref>.</p><p>Here we wish to bring the SSL's superiority into recommendation representation learning, which differs from CV/NLP tasks since the data are discrete and inter-connected. To address the argued limitations of GCN-based recommendation models, we construct the auxiliary task as discriminating the representation of a node itself. Specifically, it consists of two key components: (1) data augmentation, which generates multiple views for each node, and (2) contrastive learning, which maximizes the agreement between different views of the same node, compared to that of other nodes. By interpreting GCN as propagating node embeddings on the graph, we identify two variables as the model input: embedding matrix, which encodes the intrinsic characteristics of a node, and graph adjacency matrix, which presents the graph structure of user behaviors. From this view, we construct the unlabeled data space by changing the input, developing four operators to this end: embedding masking, embedding dropout, node dropout, and edge dropout. Each of the operators augments the data with a different purpose and rationality. Thereafter, the contrastive loss is employed to distinguish each node based on these augmented representations from the other nodes. As a result, SGL enables the discovery of sound representations by exploring the internal relationship among nodes.</p><p>Conceptually, our SGL supplements existing GCN-based recommendation models in: (1) node self-discrimination offers auxiliary supervision signal, which is complementary to the classical supervisions from observed interactions for representation learning;</p><p>(2) the augmentation operators, especially edge dropout, helps to mitigate the degree biases by changing the graph structure and reduce the influence of high-degree nodes; and (3) node dropout and edge dropout create multiple views for nodes w.r.t. different local structures and neighborhoods, enhancing the model robustness against interaction noises.</p><p>It is worthwhile mentioning that our SGL is model-agnostic and can be applied to any model that consists of user embedding and item embedding. Here we implement it on a state-of-the-art GCN-based model, LightGCN <ref type="bibr" target="#b16">[17]</ref>. Experimental studies on three benchmark datasets demonstrate the effectiveness of SGL, which significantly improves the recommendation accuracy, especially on long-tail items, and enhance the robustness against interaction noises. We summarize the contributions of this work as follows:</p><p>â€¢ To the best of our knowledge, this is the first work that develops self-supervised learning for graph-based recommendation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>We first summarize the common paradigm of GCN-based collaborative filtering models. Let U and I be the set of users and items respectively. Let O + = {ğ‘¦ ğ‘¢ğ‘– |ğ‘¢ âˆˆ U, ğ‘– âˆˆ I} be the observed interactions between users and items, where ğ‘¦ ğ‘¢ğ‘– indicates that user ğ‘¢ has adopted item ğ‘– before. Most existing models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref> construct a bipartite graph G = (V, E), where the node set V = U âˆª I involves all users and items, and the edge set E = O + represents observed interactions.</p><p>Abstract Paradigm of GCN. At the core is to apply the neighborhood aggregation scheme on G, recursively integrate the vectorized information from neighboring nodes, and update the representations of ego nodes. When only ID information is available as the pre-existing feature of each user (or item), a widely-used solution is to parameterize each ID with an embedding vector. As such, the overall scheme is formulated as follows:</p><formula xml:id="formula_0">Z = ğ» (E, G)<label>(1)</label></formula><p>where ğ» (â€¢) is the GCN function to encode connectivity information into representation learning; Z âˆˆ R |V |Ã—ğ‘‘ is the final representations of all nodes, while E âˆˆ R |V |Ã—ğ‘‘ â€² is the ID embedding matrix of all nodes; ğ‘‘ and ğ‘‘ â€² denote the size of final representations and ID embeddings, respectively. Clearly, there are two key factors affecting the representation learning: (1) ID embeddings, which reflect the intrinsic characteristics of users and items; and (2) graph structure, which offers the structural informationi.e., the underlying connectivity (paths) among users and items.</p><p>Neighborhood Aggregation. The aggregation scheme consists of two crucial components:</p><p>(1) Representation aggregation layers. After ğ‘™ layers, a node's representation is able to capture the structural information within its ğ‘™-hop neighbors. The ğ‘™-th layer is formulated as:</p><formula xml:id="formula_1">a (ğ‘™) ğ‘¢ = ğ‘“ aggregate {z (ğ‘™âˆ’1) ğ‘– |ğ‘– âˆˆ N ğ‘¢ } , z (ğ‘™) ğ‘¢ = ğ‘“ combine (z (ğ‘™âˆ’1) ğ‘¢ , a (ğ‘™) ğ‘¢ ),<label>(2)</label></formula><p>where a</p><formula xml:id="formula_2">(ğ‘™)</formula><p>ğ‘¢ denotes the aggregation of vectorized information from node ğ‘¢'s neighborhood N ğ‘¢ ; and z </p><formula xml:id="formula_3">z ğ‘¢ = ğ‘“ readout {z (ğ‘™) ğ‘¢ |ğ‘™ = [0, â€¢ â€¢ â€¢ , ğ¿]} ,<label>(3)</label></formula><p>which can be simply set as the last-layer representation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b47">48]</ref>, concatenation <ref type="bibr" target="#b43">[44]</ref>, or summation <ref type="bibr" target="#b16">[17]</ref> over the representations of all layers.</p><p>Supervised Learning Loss. Thereafter, a prediction layer is built upon the final representations of user ğ‘¢ and item ğ‘–, to predict how likely ğ‘¢ would adopt ğ‘–. A classical solution is the inner product, which supports fast top-ğ‘˜ retrieval:</p><formula xml:id="formula_4">Å·ğ‘¢ğ‘– = z âŠ¤ ğ‘¢ z ğ‘– .<label>(4)</label></formula><p>To optimize model parameters, existing works usually frame the recommendation task as one of supervised learning. Hence, the observed interactions are the source of supervision signal, encouraging the predicted value Å·ğ‘¢ğ‘– to be close to the ground truth value ğ‘¦ ğ‘¢ğ‘– . Whereas, the missing data serve as negative. Besides the point-wise classification loss <ref type="bibr" target="#b18">[19]</ref>, another intensively used loss in recommendation is the pairwise Bayesian Personalized Ranking (BPR) loss <ref type="bibr" target="#b31">[32]</ref>, which enforces the prediction of an observed interaction to be scored higher than its unobserved counterparts: </p><formula xml:id="formula_5">L ğ‘šğ‘ğ‘–ğ‘› = âˆ‘ï¸ (ğ‘¢,ğ‘–,ğ‘—) âˆˆ O âˆ’ log ğœ (Å· ğ‘¢ğ‘– âˆ’ Å·ğ‘¢ ğ‘— ),<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We present the proposed Self-supervised Graph Learning (SGL) paradigm, which supercharges the supervised learning task (cf. Equation ( <ref type="formula" target="#formula_5">5</ref>)) with self-supervised learning. Figure <ref type="figure" target="#fig_2">1</ref> illustrates the working flow of SGL. Specifically, the self-supervised task, also termed as pretext task or auxiliary task, is to construct supervision signal from the correlation within the input data. Such correlation information can supplement the main supervised task effectively.</p><p>Next, we introduce how to perform data augmentation that generates multiple representation views for a node, followed by the contrastive learning based on the generated representations to build the pretext task. SSL is combined with classical GCN in a multi-task learning manner. Lastly, we analyze the complexity of SSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Augmentation</head><p>Directly grafting the data augmentation adopted in CV and NLP tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b44">45]</ref> is infeasible for graph-based recommendation, due to specific characteristics: <ref type="bibr" target="#b0">(1)</ref> The features of users and items are discrete, like one-hot ID and other categorical variables. Hence, the augmentation operators on images, such as random crop, rotation, or blur, are not suitable. (2) More importantly, unlike CV and NLP tasks that treat each data instance as isolated, users and items in the interaction graph are inherently connected and dependent of each others. Thus, we need new augmentation operators tailored for graph-based recommendation. According to our analysis in Section 2, we identify two inputs for GCN learning: ID embeddings and graph structure. Hence, we correspondingly design two types of augmentation, considering different aspects of the data space.</p><p>3.1.1 Augmentation on ID embeddings. Before aggregating the information from neighborhood, the ID embedding can describe the intrinsic characteristics of each node. Hence, we propose two augmentation operators, embedding masking and embedding dropout, to generate the augmented views of ID embeddings. The operators can be formally summarized as:</p><formula xml:id="formula_6">Z â€² = ğ» (ğ‘¡ â€² (E), G), Z â€²â€² = ğ» (ğ‘¡ â€²â€² (E), G), ğ‘¡ â€² , ğ‘¡ â€²â€² âˆ¼ T ,<label>(6)</label></formula><p>where two stochastic data augmentation modules ğ‘¡ â€² and ğ‘¡ â€²â€² are applied on ID embedding matrix E, and result in two correlated views of node representations Z â€² and Z â€²â€² . For each node ğ‘¢, we consider the augmented views z â€² ğ‘¢ and z â€²â€² ğ‘¢ as one positive pair, otherwise the negative pair. In particular, ğ‘¡ â€² and ğ‘¡ â€²â€² can be formulated as:</p><formula xml:id="formula_7">ğ‘¡ â€² (E) = M â€² âŠ™ E, ğ‘¡ â€²â€² (E) = M â€²â€² âŠ™ E,<label>(7)</label></formula><p>where M â€² , M â€²â€² âˆˆ {0, 1} |V |Ã—ğ‘‘ are the masking matrix which are generated by embedding masking or embedding dropout strategy:</p><p>â€¢ ID Embedding Masking (IM). With the mask ratio ğœŒ, a subset of dimensions are masked for all ID embeddings, so as to use partial dimensions to learn representations. By applying two independent masking processes ğ‘¡ â€² and ğ‘¡ â€²â€² , this approach is able to focus on different dimensions and learn the internal relationship between two subsets of embeddings. â€¢ ID Embedding Dropout (ID). With the dropout ratio ğœŒ, a subset of elements in an embedding are set as 0, such that the remaining elements are used to discriminate each node against others. Through two independent dropout processes ğ‘¡ â€² and ğ‘¡ â€²â€² , this approach encourages partial information to recover the instance-wise embeddings, and learn not to heavily rely on certain information, so as to improve model robustness <ref type="bibr" target="#b40">[41]</ref>.</p><p>Note that the stochastic selections are easy to adapt to contentbased recommendation models. Next, we consider the characteristics of graph structure.</p><p>3.1.2 Augmentation on Graph Structure. The bipartite graph is built upon observed user-item interactions, thus containing the collaborative filtering signal. Specifically, the first-hop neighborhood directly profiles ego user and item nodesi.e., historical items of a user (or interacted users of an item) can be viewed as the pre-existing features of user (or item). The second-hop neighboring nodes of a user (or an item) exhibit similar users w.r.t. behaviors (or similar items w.r.t. audiences). Furthermore, the higher-order paths from a user to an item reflect potential interests of the user on the item. Undoubtedly, mining the inherent patterns in graph structure is helpful to representation learning. We hence devise two operators, node dropout and edge dropout, to create different views of nodes. The operators can be formulated as follows:</p><formula xml:id="formula_8">Z â€² = ğ» (E, ğ‘  â€² (G)), Z â€²â€² = ğ» (E, ğ‘  â€²â€² (G)), ğ‘  â€² , ğ‘  â€²â€² âˆ¼ S,<label>(8)</label></formula><p>where two stochastic selections ğ‘  â€² and ğ‘  â€²â€² are independently applied on graph G, and establish two correlated views of nodes Z â€² and Z â€²â€² . We elaborate the augmentation operators as follows:</p><p>â€¢ Node Dropout (ND). With the probability ğœŒ, each node is discarded from the graph, together with its connected edges.</p><p>In particular, ğ‘  â€² and ğ‘  â€²â€² can be modeled as:</p><formula xml:id="formula_9">ğ‘  â€² (G) = (M â€² âŠ™ V, E), ğ‘  â€²â€² (G) = (M â€²â€² âŠ™ V, E),<label>(9)</label></formula><p>where M â€² , M â€²â€² âˆˆ {0, 1} | V | are two masking vectors which are applied on the node set V to generate two subgraphs. As such, this augmentation is expected to identify the influential nodes from differently augmented views, and make the representation learning less sensitive to structure changes. â€¢ Edge Dropout (ED). The augmentation drops out the edges in graph with a dropout ratio ğœŒ. Two independent processes are represented as:</p><formula xml:id="formula_10">ğ‘  â€² (G) = (V, M â€² âŠ™ E), ğ‘  â€²â€² (G) = (V, M â€²â€² âŠ™ E),<label>(10)</label></formula><p>where M â€² , M â€²â€² âˆˆ {0, 1} | E | are two masking vectors on the edge set E. Only partial connections within the neighborhood contribute to the node representations. As such, coupling these two subgraphs together aims to capture the useful patterns of the local structures of a node, and further endows the representations more robustness against the presence of single interactions, especially the noisy interactions.</p><p>We apply these augmentations on ID embeddings or graph structure per epoch for simplicity -that is, we generate two different views of each node at the beginning of a new training epoch. Note that the dropout and masking ratios remain the same for two independent processes (i.e., ğ‘¡ â€² and ğ‘¡ â€²â€² , or ğ‘  â€² and ğ‘  â€²â€² ). We leaving the different ratios in future work. It is also worthwhile mentioning that only dropout and masking operations are involved, and no any model parameters are added. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contrastive Learning</head><p>Having established the augmented views of nodes, we treat the views of the same node as the positive pairs (i.e., {(z â€² ğ‘¢ , z â€²â€² ğ‘¢ )|ğ‘¢ âˆˆ U}), and the views of any different nodes as the negative pairs (i.e., {(z â€² ğ‘¢ , z â€²â€² ğ‘£ )|ğ‘¢, ğ‘£ âˆˆ U, ğ‘¢ â‰  ğ‘£ }). The auxiliary supervision of positive pairs encourages the consistency between different views of the same node for prediction, while the supervision of negative pairs enforces the divergence among different nodes. Formally, we follow SimCLR <ref type="bibr" target="#b4">[5]</ref> and adopt the contrastive loss, InfoNCE <ref type="bibr" target="#b11">[12]</ref>, to maximize the agreement of positive pairs and minimize that of negative pairs:</p><formula xml:id="formula_11">L ğ‘¢ğ‘ ğ‘’ğ‘Ÿ ğ‘ ğ‘ ğ‘™ = âˆ‘ï¸ ğ‘¢ âˆˆU âˆ’ log exp(ğ‘  (z â€² ğ‘¢ , z â€²â€² ğ‘¢ )/ğœ) ğ‘£ âˆˆU exp(ğ‘  (z â€² ğ‘¢ , z â€²â€² ğ‘£ )/ğœ) ,<label>(11)</label></formula><p>where ğ‘  (â€¢) measures the similarity between two vectors, which is set as cosine similarity function; ğœ is the temperature hyperparameter. Analogously, we obtain the contrastive loss of the item side L ğ‘–ğ‘¡ğ‘’ğ‘š ğ‘ ğ‘ ğ‘™ . Combining these two losses, we get the objective function of selfsupervised task as L ğ‘ ğ‘ ğ‘™ = L ğ‘¢ğ‘ ğ‘’ğ‘Ÿ ğ‘ ğ‘ ğ‘™ + L ğ‘–ğ‘¡ğ‘’ğ‘š ğ‘ ğ‘ ğ‘™ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-task Training</head><p>To improve recommendation with the SSL task, we leverage a multi-task training strategy to jointly optimize the classic recommendation task (cf. Equation ( <ref type="formula" target="#formula_5">5</ref>)) and the self-supervised learning task (cf. Equation ( <ref type="formula" target="#formula_11">11</ref>))</p><formula xml:id="formula_12">L = L ğ‘šğ‘ğ‘–ğ‘› + ğœ† 1Lğ‘ ğ‘ ğ‘™ + ğœ† 2 âˆ¥Î˜âˆ¥ 2 2 , (<label>12</label></formula><formula xml:id="formula_13">)</formula><p>where Î˜ is the set of model parameters in ğ¿ ğ‘šğ‘ğ‘–ğ‘› since ğ¿ ğ‘ ğ‘ ğ‘™ introduces no additional parameters; ğœ† 1 and ğœ† 2 are hyperparameters to control the strengths of SSL and ğ¿ 2 regularization, respectively. The overall learning algorithm of SGL equipped with ED (termed SGL-ED) is summarized in Algorithm 1. Other implementations (i.e., , SGL-ID, SGL-IM, SGL-ND) follow similar workflow. We also consider the alternative optimization -pre-training on L ğ‘ ğ‘ ğ‘™ and fine-tuning on L ğ‘šğ‘ğ‘–ğ‘› . See more details in Section 4.4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Complexity Analyses</head><p>In this subsection, we analyze the complexity of SGL with ED as the strategy and LightGCN as the recommendation model; other choices can be analyzed similarly. Since SGL introduces no </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>To justify the superiority of SGL and reveal the reasons of its effectiveness, we conduct extensive experiments to answer the following research questions: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>We conduct experiments on three widely used benchmark datasets: Yelp2018 <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>, Amazon-Book <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>, and Alibaba-iFashion <ref type="bibr" target="#b5">[6]</ref> <ref type="foot" target="#foot_0">1</ref> . Following <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>, we use the same 10-core setting for Yelp2018 and Amazon-Book. Alibaba-iFashion is more sparse, where we randomly sample 300k users and use all their interactions over the fashion outfits. The statistics of all three datasets are summarized in Table <ref type="table" target="#tab_2">2</ref>. We follow the same strategy described in <ref type="bibr" target="#b43">[44]</ref> to split the interactions into training, validation, and testing with a ratio of 7:1:2.</p><p>For users in the testing set, we follow the all-ranking protocol <ref type="bibr" target="#b43">[44]</ref> to evaluate the top-ğ¾ recommendation performance and report the average Recall@ğ¾ and NDCG@ğ¾ where we set ğ¾ = 20.</p><p>4.1.1 Compared Methods. We compare the proposed SGL with the following CF models:</p><p>â€¢ NGCF <ref type="bibr" target="#b43">[44]</ref>. This is a graph-based CF method largely follows the standard GCN <ref type="bibr" target="#b9">[10]</ref>, including the use of nonlinear activation and feature transformation. Besides, it additionally encodes the second-order feature interaction into the message during message passing. We tune the regularization coefficient ğœ† 2 and the number of GCN layers within the suggested ranges.</p><p>â€¢ LightGCN <ref type="bibr" target="#b16">[17]</ref>. This is the state-of-the-art graph-based CF method which devises a light graph convolution to ease the training difficulty and pursue better generation ability. Similarly, we tune the ğœ† 2 and the number of GCN layers. â€¢ Mult-VAE <ref type="bibr" target="#b26">[27]</ref>. This is an item-based CF method based on the variational auto-encoder (VAE). It uses multinomial likelihood to model user-item implicit feedback data and learns latent-variable model with variational inference. The model is optimized with an additional reconstruction objective, which can be seen as a special case of SSL. We follow the suggested model setting and tune the dropout ratio and ğ›½. We discard potential baselines like MF <ref type="bibr" target="#b31">[32]</ref>, NeuMF <ref type="bibr" target="#b18">[19]</ref>, GC-MC <ref type="bibr" target="#b35">[36]</ref>, and PinSage <ref type="bibr" target="#b47">[48]</ref> since the previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44]</ref> has validated the superiority over the compared ones. Upon LightGCN, we implement four variants of the proposed SGL, named SGL-ID, SGL-IM, SGL-ND and SGL-ED, which are equipped with ID Embedding Dropout, ID Embedding Masking, Node Dropout and Edge Dropout, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1.2</head><p>Hyper-parameter Settings. For fair comparison, all models are trained from scratch which are initialized with the Xavier method <ref type="bibr" target="#b10">[11]</ref>. The models are optimized by the Adam optimizer with  <ref type="table" target="#tab_3">3</ref> shows the result comparison between SGL and LightGCN. We find that:</p><p>â€¢ In most cases, four SGL outperforms LightGCN by a large margin, indicating the superiority of supplementing the recommendation task with self-supervised learning. â€¢ In SGL family, SGL-ED achieves best performance in 12 out of 18 cases and second place in 4 cases, verifying the effectiveness of adopting edge dropout as data augmentation. We attribute this to the power of SGL-ED in capturing the inherent patterns in graph structure. â€¢ Considering two graph structure-based augmentations, SGL-ND is relatively unstable than SGL-ED. For example, on Yelp2018 and Amazon-Book, the results of SGL-ED increases with layers go deeper; whereas, SGL-ND exhibits different patterns: rise-decline on Yelp and decline-rise on Amazon-Book. Node Dropout can be viewed as a special case of Edge Dropout, which discards edges around a few nodes. Hence, dropping high-degree nodes will dramatically change the graph structure, thus exerts influence than the information aggregation in representation learning and makes the training unstable. â€¢ The improvements on Amazon-Book and Alibaba-iFashion are more significant than that on Yelp2018. This might be caused by the characteristics of datasets. Specifically, in Amazon-Book and Alibaba-iFashion, supervision signal from user-item interactions is too sparse to guide the representation learning in LightGCN.</p><p>Benefiting from the self-supervised task, SGL obtains auxiliary supervisions to assist the representation learning. â€¢ Increasing the model depth from 1 layer to 3 layers is able to enhance the performance of SGL. This indicates that exploiting SSL could empower the generalization ability of GCN-based recommender models -that is, the contrastive learning among different nodes is of promise to solving the oversmoothing issue of node representations, further avoiding the overfitting problem of prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.2</head><p>Comparison with the State-of-the-Arts. In Table <ref type="table" target="#tab_4">4</ref>, we summarize the performance comparison with various baselines. As we can see, SGL-ED consistently outperforms other baselines across the board. This again demonstrates the rationality and effectiveness of incorporating the self-supervised learning. Among all the baselines, LightGCN achieves better performance than NGCF and Mult-VAE, which is consistent with the claim of LightGCN paper. The performance of Mult-VAE is on par with NGCF and Yelp on Alibaba-iFashion, while outperforms NGCF on Amazon-Book.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Benefits of SGL (RQ2)</head><p>In this section, we study the benefits of SGL from three dimensions: (1) long-tail recommendation; (2) training efficiency; and (3) robustness to noises. Due to the limited space, we only report the results of SGL-ED, while having similar observations in the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.1</head><p>Long-tail Recommendation. As Introduction mentions, GNN-based recommender models easily suffer from the long-tail problem. To verify whether SGL is of promise to solving the problem, we split items into ten groups based on the popularity, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S G L -E D -L 1 S G L -E D -L 2 S G L -E D -L 3 L i g h t G C N -L 1 L i g h t G C N -L 2 L i g h t G C N -L 3</head><p>(a) Yelp2018 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S G L -E D -L 1 S G L -E D -L 2 S G L -E D -L 3 L i g h t G C N -L 1 L i g h t G C N -L 2 L i g h t G C N -L 3</head><p>(b) Amazon-Book </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S G L -E D -L 1 S G L -E D -L 2 S G L -E D -L 3 L i g h t G C N -L 1 L i g h t G C N -L 2 L i g h t G C N -L 3</head><p>(c) Alibaba-iFashion meanwhile keeping the total number of interactions of each group the same. The larger the GroupID is, the larger degrees the items have. We then decompose the Recall@20 metric of the whole dataset into contributions of single groups, as follows:</p><formula xml:id="formula_14">ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ = 1 ğ‘š ğ‘š âˆ‘ï¸ ğ‘¢=1 10 ğ‘”=1 (ğ‘™ ğ‘¢ ğ‘Ÿğ‘’ğ‘ ) (ğ‘”) âˆ© ğ‘™ ğ‘¢ ğ‘¡ğ‘’ğ‘ ğ‘¡ ğ‘™ ğ‘¢ ğ‘¡ğ‘’ğ‘ ğ‘¡ = 10 âˆ‘ï¸ ğ‘”=1 ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ (ğ‘”)</formula><p>where ğ‘š is the number of users, ğ‘™ ğ‘¢ ğ‘Ÿğ‘’ğ‘ and ğ‘™ ğ‘¢ ğ‘¡ğ‘’ğ‘ ğ‘¡ are the items in the top-N recommendation list and relevant items for user ğ‘¢, respectively. As such, ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ (ğ‘”) measures the recommendation performance over the ğ‘”-th group. We report the empirical results in 2 and find that:</p><p>â€¢ LightGCN is inclined to recommend high-degree items, while leaving long-tail items less exposed. Specifically, although only containing 0.83%, 0.83% and 0.22% of item spaces, the 10-th group contributes 39.72%, 39.92% and 51.92% of the total Recall scores in three datasets, respectively. This admits that, LightGCN hardly learns high-quality representations of long-tail items, due to the sparse interaction signal. Our SGL shows potentials in alleviating this issue: the contributions of the 10-group downgrade to 36.27%, 29.15% and 35.07% in three datasets, respectively. â€¢ Jointly analyzing Table <ref type="table">.</ref> 3 and Fig. <ref type="figure">2</ref>, we find the performance improvements of SGL mainly come from accurately recommending the items with sparse interactions. This again verifies that the representation learning benefits greatly from auxiliary supervisions, so as to establish better representations of these items than LightGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.2</head><p>Training Efficiency. Self-supervised learning has proved its superiority in pre-training natural language model <ref type="bibr" target="#b7">[8]</ref> and graph structure <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref>. Thus, we would like to study its influence on training efficiency. Fig. <ref type="figure">3</ref> shows the training curves of SGL-ED and LightGCN. As the number of epochs increases, the upper subfigures display the changes of training loss, while the bottoms record the performance changes in the testing set. We have the following observations:</p><p>â€¢ Obviously, SGL is much faster to converge than LightGCN on Yelp2018 and Amazon-Book. In particular, SGL arrives at the best performance at the 18-th and 16-th epochs, while LightGCN takes 720 and 700 epochs in these two datasets respectively. This suggests that our SGL can greatly reduce the training time, meanwhile achieves remarkable improvements. â€¢ Although the training curves on Alibaba-iFashion show different patterns from that on Yelp2018 and Amazon-Book, SGL is still faster to converge, compared with LightGCN. This again validates the benefits of SSL tasks, which offer more supervisions than the supervised learning task. â€¢ Another observation is that the origin of the rapid-decline period of BPR loss is slightly later than the rapid-rising period of Recall. This indicates the existence of a gap between the BPR loss and ranking task. We will conduct in-depth research on this phenomenon in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3.3</head><p>Robustness to Noisy Interactions. We also conduct experiments to check SGL's robustness to noisy interactions. Towards this end, we contaminate the training set by adding a certain proportion of adversarial examples (i.e., 5%, 10%, 15%, 20% negative  user-item interactions), while keeping the testing set unchanged.</p><p>Figure <ref type="figure" target="#fig_8">4</ref> shows the results on Yelp2018 and Amazon-Book datasets.</p><p>â€¢ Adding noise data reduces the performance of SGL and LightGCN, where the curve in Figure <ref type="figure" target="#fig_8">4</ref> indicates the performance degradation of each model. Clearly, the performance degradation of SGL is lower than that of LightGCN; moreover, as the noise ratio increases, the gap between two degradation curves becomes more apparent. This suggests that, by comparing differently augmented views of nodes, SGL is able to figure out useful patterns, especially informative graph structures of nodes, and reduce dependence on certain edges. In a nutshell, SGL offers a different angle to denoise false positive interactions in recommendation. â€¢ Focusing on Amazon-Book, the performance of SGL with 20% additional noisy interactions is still superior to LightGCN with noise-free dataset. This further justifies the superiority and robustness of SGL over LightGCN. â€¢ We find that SGL is more robust on Yelp2018. The possible reason may be that Amazon-Book is much sparser than Yelp2018, and adding noisy data exerts more influence on graph structure of Amazon-Book than that of Yelp2018.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Study of SGL (RQ3)</head><p>As the SSL task plays a pivotal role in SGL, we first investigate the impact of hyper-parameters, i.e., SSL weight, temperature and dropout ratio. We adopt the control variates method, that is, tuning one parameter in the range described in Section 4.1.2 with the other two set as the optimal values. We then study the effect of the SSL task on both user and item side. Lastly, we explore the potential of adopting SGL as a pretraining for existing graph-based recommendation model. For better visualization, we ommit the results on iFashion, which have a similar trend. 4.4.1 Effect of SSL Weight ğœ† 1 . Fig. <ref type="figure" target="#fig_9">5</ref> shows the recommendation performance w.r.t. Recall under different ğœ† 1 where larger value means higher contribution of L ğ‘ ğ‘ ğ‘™ . As can be seen, a proper ğœ† 1 (e.g., 0.1) that balances the two tasks can greatly fasten convergence meanwhile achieving satisfying performance. We also observe overfitting problem when ğœ† 1 is large on Amazon-Book, which suggests us to tune this parameter in fine-grained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.2</head><p>Effect of Temperature ğœ. Note that ğœ controls the scale of L ğ‘ ğ‘ ğ‘™ where a bigger ğœ leads to bigger SSL loss. Fig. <ref type="figure">6</ref> shows the curve of model performance as adjusting the value of ğœ. From the figure we can observe that: (1) Large ğœ (e.g., 1.0) will lead to performance drop, which indicates that the similarity function defined in Eq. 11 is insufficient to distinguish positive pair from negative pairs when the value of ğœ is large. (2) On the contrary, setting ğœ with value too small (e.g., 0.1) will also hurt the model performance. Therefore, we suggest tuning ğœ in the range of 0.1 âˆ¼ 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.3</head><p>Effect of Dropout ğœŒ. Fig. <ref type="figure">7</ref> plots the sensitivity analysis of the dropout ratios. We can find that the best performance is achieved when the ratio is in the range of 0.1 âˆ¼ 0.3. This is in line with our intuition that SSL cannot learn sufficient essential characteristics if the two augmented data are not distinct from each other, or losing too much information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.4</head><p>Effect of SSL on User and Item Sides. As the user-item interaction graph is a heterogeneous graph containing two types of nodes: users and items, we formulate the objective function of SSL as the summation of InfoNCE <ref type="bibr" target="#b11">[12]</ref> losses on both user and item sides. Here we conduct ablation studies to verify the influences of user and item nodes. In particular, we create two variants: SGL-EDuser and SGL-ED-item, which separately treat all users and items as negatives in the auxiliary tasks, respectively. Table <ref type="table" target="#tab_5">5</ref> shows the experimental results. As we can see, the joint optimization on both sides achieves better performance on both datasets, than that on single sides. It inspires us to make full use of graph structures. Moreover, we also study the effect of the number of negative samples in the auxiliary task. Two variants are considered: (1) SGL-ED-batch, which differentiates node types and treat users and items in a mini-batch as negative views for users and items, separately, and (2) SGL-ED-merge, which treat nodes within a mini-batch as negative, without differentiating the node types. We report the comparison in Table <ref type="table" target="#tab_5">5</ref> SGL-ED-batch performs better than SGL-ED-merge, which indicates the necessity for distinguishing types of heterogeneous nodes. Moreover, SGL-ED-batch is on a par with SGL-ED that treats the whole spaces of users and items as negative. It suggests that training the SSL task in mini-batching is an efficient alternative.</p><p>4.4.5 Effect of Pre-training. The foregoing experiments have shown the effectiveness of SGL, where the main supervised task and the self-supervised task are jointly optimized. Here we would like to answer the question: Can the recommendation performance benefit from the pre-trained model? Towards this goal, we first pretrain the self-supervised task to obtain the model parameters, use them to initialize LightGN, and then fine the model via optimizing the main task. We term this variant trained as SGL-pre and show the comparison with SGL in Table <ref type="table" target="#tab_5">5</ref>.</p><p>Clearly, although SGL-pre performs worse than SGL-ED on both datasets, the results of SGL-pre are still better than that of LightGCN (cf. Table <ref type="table" target="#tab_3">3</ref>). Our self-supervised task is able to offer a better initialization for LightGCN, which is consistent to the observation in previous studies <ref type="bibr" target="#b4">[5]</ref>. However, the better performance of joint training admits that the representations in the main and auxiliary tasks are mutually enhanced with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Our proposed method involves two tasks: graph-based recommendation and self-supervised learning. In this section, we separately review the related work and discuss their relations to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Graph-based Recommendation</head><p>Previous studies on graph-based recommendation can be categorized into: model-level and graph-level methods, regarding their focus. The model-level methods focus on model design for mining the user-item graph. The research attention has been evolved from the random walk that encodes the graph structure as transition probabilities <ref type="bibr" target="#b1">[2]</ref>, to GCN that propagates user and item embeddings over the graph <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48]</ref>. Recently, attention mechanism is introduced into GCN-based recommendation models <ref type="bibr" target="#b42">[43]</ref>, which learns to weigh the neighbors so as to capture the more informative user-item interactions. A surge of attention has also been dedicated to graph-level methods, which enriches the user-item graph by accounting for side information apart from user-item interactions, which ranges from user social relations <ref type="bibr" target="#b0">[1]</ref>, item co-occurrence <ref type="bibr" target="#b1">[2]</ref>, to user and item attributes <ref type="bibr" target="#b25">[26]</ref>. Recently, Knowledge Graph (KG) is also unified with the user-item graph, which enables considering the detailed types of linkage between items <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Despite the tremendous efforts devoted on these methods, all the existing work adopts the paradigm of supervised learning for model training. This work is in an orthogonal direction, which explores self-supervised learning, opening up a new research line of graph-based recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Self-supervised Learning</head><p>Studies on self-supervised learning can be roughly categorized into two branches: generative models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref> and contrastive models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38]</ref>. Auto-encoding is the most popular generative model which learns to reconstruct the input data, where noises can be intentionally added to enhance model robustness <ref type="bibr" target="#b7">[8]</ref>. Contrastive models learn to compare through a Noise Contrastive Estimation (NCE) objective, which can be in either global-local contrast <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">38]</ref> or global-global contrast manner <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref>. The former focuses on modeling the relationship between the local part of a sample and its global context representation, e.g., stripes to cats in CV <ref type="bibr" target="#b27">[28]</ref>.While the latter directly performs comparison between different samples, which typically requires multiple different views of samples <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref>. There are both pros and cons of the generative model and contrastive model, this work adopts the contrastive model to avoid additional model parameters.</p><p>SSL has also been applied on graph data. For instance, Info-Graph <ref type="bibr" target="#b32">[33]</ref> and DGI <ref type="bibr" target="#b39">[40]</ref> learns node representations according to mutual information between a node and the local structure. In addition, Hu et al. <ref type="bibr" target="#b20">[21]</ref> extend the idea to learn GCN for graph representation. Furthermore, Kaveh et al. <ref type="bibr" target="#b13">[14]</ref> adopt the contrastive model for learning both node and graph representation, which contrasts node representations from one view with graph representation of another view. Besides, GCC <ref type="bibr" target="#b30">[31]</ref> leverages instance discrimination as the pretext task for graph structural information pre-training. These studies are focused on general graphs which cannot be directly used to the user-item graph. Moreover, none of the existing work considers augmentation operators from the perspective of embedding.</p><p>To the best of our knowledge, very limited work exists in combining SSL with recommendation to date. A very recent one is ğ‘† 3 -Rec <ref type="bibr" target="#b48">[49]</ref> for sequential recommendation which utilizes the mutual information maximization principle to learn the correlations among attribute, item, subsequence, and sequence. Another attempt <ref type="bibr" target="#b46">[47]</ref> also adopts a multi-task framework with SSL. However, it differs from our work in: (1) <ref type="bibr" target="#b46">[47]</ref> uses two-tower DNN as encoder, while our work sheds lights on graph-based recommendation, and devised four augmentation operators from two dimensions: ID embeddings and graph structure. (2) <ref type="bibr" target="#b46">[47]</ref> utilizes categorical metadata features as model input, while our work considers a more general collaborative filtering setting with only ID as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this work, we recognized the limitations of graph-based recommendation under general supervised learning paradigm and explored the potential of SSL to solve the limitations. In particular, we proposed a model-agnostic framework SGL to supplement the supervised recommendation task with self-supervised learning on user-item graph. From the embedding matrix and graph structure of the GCN-based models, we devised four types of data augmentation from different aspects to construct the auxiliary contrastive task. We demonstrated SGL on LightGCN and conducted extensive experiments on three benchmark datasets, justifying the advantages of our proposal regarding long-tail recommendation, training convergence and robustness against noisy interactions.</p><p>This work represents an initial attempt to exploit self-supervised learning for recommendation and opens up new research possibilities. In future work, we would like to make SSL more entrenched with recommendation tasks. Going beyond the stochastic selections on embeddings and graph structure, we plan to explore new perspectives, such as counterfactual learning to identify influential data points, to create more powerful data augmentations. Moreover, we will focus on pre-training and fine-tuning in recommendation -that is, to pre-train a model which captures universal and transferable patterns of users across multiple domains or datasets, and fine-tune it on upcoming domains or datasets. Another promising direction is fulfilling the potential of SSL to address the long-tail issue. We hope the development of SGL is beneficial for improving the generalization and transferability of recommender models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ğ‘¢ is the representation of node ğ‘¢ after ğ‘™ aggregation layers, which integrates a (ğ‘™) ğ‘¢ with its previous representation z (ğ‘™âˆ’1) ğ‘¢ . We initiate z (0) ğ‘¢ with ID embedding e ğ‘¢ . There are a number of designs for ğ‘“ aggregate (â€¢) and ğ‘“ combine (â€¢) [10, 13, 39, 46]. (2) Readout layer. Having obtained the representations at different layers, the readout function generates the final representations:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where O = {(ğ‘¢, ğ‘–, ğ‘—)|(ğ‘¢, ğ‘–) âˆˆ O + , (ğ‘¢, ğ‘—) âˆˆ O âˆ’ } is the training data, and O âˆ’ = U Ã— I \ O + is the unobserved interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall system framework of SGL. (1) The first layer illustrates the working flow of the main supervised learning task. (2) The second and third layers show the working flows of SSL task with augmentation on ID embeddings and graph structure, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 : 8 Update</head><label>18</label><figDesc>Learning algorithm for SGL-ED Input: Adjacency matrix of user-item graph, ğœ† 1 , ğœ† 2 , ğœ, ğœŒ 1 while not converge do 2 foreach epoch do 3 Perform Eq. (8) for data augmentation 4 foreach batch do 5 Evaluate L ğ‘šğ‘ğ‘–ğ‘› according to Eq. (5) 6 Evaluate L ğ‘ ğ‘’ğ‘™ ğ‘“ according to Eq. (11) 7Evaluate L according to Eq.<ref type="bibr" target="#b11">(12)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Performance comparison over different item groups between SGL-ED and LightGCN. The suffix in the legend indicates the number of GCN layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Model performance w.r.t. noise ratio. The bar represents Recall, while the line represents the percentage of performance degradation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Model performance as adjusting the SSL weight ğœ† 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Model performance as adjusting ğœ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>â€¢ We devise a new learning paradigm, SGL, which takes node self-discrimination as the self-supervised task to offer auxiliary supervision signal for node representation learning.â€¢ We conduct extensive experiments on three benchmark datasets to demonstrate the superiority of SGL.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The comparison of analytical time complexity between LightGCN and SGL-ED.trainable parameters, the space complexity remains the same as LightGCN<ref type="bibr" target="#b16">[17]</ref>. The time complexity of model inference is also the same, since there is no change on the model structure. In the following part, we will analyze the time complexity of SGL training.Suppose the number of nodes and edges in the user-item interaction graph are |ğ‘‰ | and |ğ¸| respectively. Let ğ‘  denote the number of epochs, ğµ denote the size of each training batch, ğ‘‘ denote the embedding size, ğ¿ denote the number of GCN layers, Ï = 1 âˆ’ ğœŒ denote the keep probability of SGL-ED. The complexity mainly comes from two parts:We summarize the time complexity in training between LightGCN and SGL-ED in Table1. The analytical complexity of LightGCN and SGL-ED is actually in the same magnitude, since the increase of LightGCN only scales the complexity of LightGCN. In practice, taking the Yelp2018 data as an example, the time complexity of SGL-ED (alternative) with Ï of 0.8 is about 3.7x larger than LightGCN, which is totally acceptable considering the speedup of convergence speed we will show in Section 4.3.2. We implement our SGL in TensorFlow and will release our codes upon acceptance. The testing platform is Nvidia Titan RTX graphics card equipped with Inter i7-9700K CPU (32GB Memory). The time cost of each epoch on Yelp2018 is 15.2s and 60.6s for LightGCN and SGL-ED (alternative) respectively, which is consistent with the complexity analyses.</figDesc><table><row><cell>Component</cell><cell cols="2">LightGCN</cell><cell>SGL-ED</cell></row><row><cell>Adjacency Matrix</cell><cell>ğ‘‚ (2 |ğ¸|)</cell><cell></cell><cell cols="2">ğ‘‚ (4 Ï |ğ¸| ğ‘  + 2 |ğ¸|)</cell></row><row><cell>Graph Convolution</cell><cell>ğ‘‚ (2 |ğ¸| ğ¿ğ‘‘ğ‘ </cell><cell cols="2">|ğ¸ | ğµ ) ğ‘‚ (2(1 + 2 Ï) |ğ¸| ğ¿ğ‘‘ğ‘ </cell><cell>|ğ¸ | ğµ )</cell></row><row><cell>BPR Loss</cell><cell cols="2">ğ‘‚ (2 |ğ¸| ğ‘‘ğ‘ )</cell><cell>ğ‘‚ (2 |ğ¸| ğ‘‘ğ‘ )</cell></row><row><cell>Self-supervised Loss</cell><cell>-</cell><cell></cell><cell cols="2">ğ‘‚ (|ğ¸| ğ‘‘ (2 + |ğ‘‰ |)ğ‘ ) ğ‘‚ (|ğ¸| ğ‘‘ (2 + 2ğµ)ğ‘ )</cell></row></table><note>â€¢ Normalization of adjacency matrix. Since we generate two independent sub-graphs per epoch, given the fact that the number of non-zero elements in the adjacency matrices of full training graph and two sub-graph are 2 |ğ¸| , 2 Ï |ğ¸| and 2 Ï |ğ¸| respectively, its total complexity is ğ‘‚ (4 Ï |ğ¸| ğ‘  + 2 |ğ¸|). â€¢ Evaluating self-supervised loss. We only consider the inner product in our analyses. As defined in Equation (11), we treat all other user nodes as negative samples when calculating InfoNCE loss of user side. Within a batch, the complexity of numerator and denominator are ğ‘‚ (ğµğ‘‘) and ğ‘‚ (ğµğ‘€ğ‘‘), respectively, where ğ‘€ is the number of users. And hence the total complexity of both user and item side per epoch is ğ‘‚ (|ğ¸| ğ‘‘ (2 + |ğ‘‰ |)). Therefore, the time complexity of the whole training phase is ğ‘‚ (|ğ¸| ğ‘‘ (2 + |ğ‘‰ |)ğ‘ ).An alternative to reduce the time complexity is treating only the users (or the items) within the batch as negative samples<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">47]</ref>, resulting in total time complexity of ğ‘‚ (|ğ¸| ğ‘‘ (2 + 2ğµ)ğ‘ ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Users #Items #Interactions Density</cell></row><row><cell>Yelp2018</cell><cell>31,668 38,048</cell><cell>1,561,406 0.00130</cell></row><row><cell>Amazon-Book</cell><cell>52,643 91,599</cell><cell>2,984,108 0.00062</cell></row><row><cell cols="2">Alibaba-iFashion 300,000 81,614</cell><cell>1,607,813 0.00007</cell></row><row><cell cols="3">â€¢ RQ1: As compared with the state-of-the-art CF models, how does</cell></row><row><cell cols="3">SGL perform w.r.t. top-ğ‘˜ recommendation?</cell></row><row><cell cols="3">â€¢ RQ2: What are the benefits of performing self-supervised</cell></row><row><cell cols="2">learning in collaborative filtering?</cell><cell></cell></row><row><cell cols="3">â€¢ RQ3: How do different settings influence the effectiveness of the</cell></row><row><cell>proposed SGL?</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison with LightGCN at different layers. The performance of LightGCN on Yelp2018 and Amazon-Book are copied from its original paper. The percentage in brackets denote the relative performance improvement over LightGCN. The bold indicates the best result, while the second-best performance is underlined.</figDesc><table><row><cell cols="2">Dataset</cell><cell cols="2">Yelp2018</cell><cell cols="2">Amazon-Book</cell><cell cols="2">Alibaba-iFashion</cell></row><row><cell>#Layer</cell><cell>Method</cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell><cell>Recall</cell><cell>NDCG</cell></row><row><cell></cell><cell cols="2">LightGCN 0.0631</cell><cell>0.0515</cell><cell>0.0384</cell><cell>0.0298</cell><cell>0.0990</cell><cell>0.0454</cell></row><row><cell></cell><cell>SGL-ID</cell><cell>0.0634(+0.5%)</cell><cell>0.0518(+0.6%)</cell><cell>0.0417(+8.6%)</cell><cell>0.0322(+8.1%)</cell><cell cols="2">0.1141(+15.3%) 0.0540(+18.9%)</cell></row><row><cell>1 Layer</cell><cell>SGL-IM</cell><cell>0.0631(+0%)</cell><cell>0.0513(-0.4%)</cell><cell>0.0429(11.7%)</cell><cell>0.0331(+11.1%)</cell><cell>0.1116(+12.7%)</cell><cell>0.0530(+16.7%)</cell></row><row><cell></cell><cell>SGL-ND</cell><cell cols="3">0.0643(+1.9%) 0.0529(+2.7%) 0.0432(+12.5%)</cell><cell>0.0334(+12.1%)</cell><cell>0.1133(+14.4%)</cell><cell>0.0539(+18.7%)</cell></row><row><cell></cell><cell>SGL-ED</cell><cell>0.0637(+1.0%)</cell><cell>0.0526(+2.1%)</cell><cell cols="3">0.0451(+17.4%) 0.0353(+18.5%) 0.1132(+14.3%)</cell><cell>0.0539(+18.7%)</cell></row><row><cell></cell><cell cols="2">LightGCN 0.0622</cell><cell>0.0504</cell><cell>0.0411</cell><cell>0.0315</cell><cell>0.1066</cell><cell>0.0505</cell></row><row><cell></cell><cell>SGL-ID</cell><cell>0.0659(+5.9%)</cell><cell>0.0539(+6.9%)</cell><cell>0.0436(+6.1%)</cell><cell>0.0342(+8.6%)</cell><cell>0.1089(+2.2%)</cell><cell>0.0517(+2.4%)</cell></row><row><cell>2 Layers</cell><cell>SGL-IM</cell><cell>0.0657(+5.6%)</cell><cell>0.0538(+6.7%)</cell><cell>0.0434(+5.6%)</cell><cell>0.0338(+7.3%)</cell><cell>0.1085(+1.8%)</cell><cell>0.0517(+2.4%)</cell></row><row><cell></cell><cell>SGL-ND</cell><cell>0.0658(+5.8%)</cell><cell>0.0538(+6.7%)</cell><cell>0.0427(+3.9%)</cell><cell>0.0335(+6.3%)</cell><cell>0.1106(+3.8%)</cell><cell>0.0526(+4.2%)</cell></row><row><cell></cell><cell>SGL-ED</cell><cell cols="5">0.0668(+7.4%) 0.0549(+8.9%) 0.0468(+13.9%) 0.0371(+17.8%) 0.1091(+2.3%)</cell><cell>0.0520(+3.0%)</cell></row><row><cell></cell><cell cols="2">LightGCN 0.0639</cell><cell>0.0525</cell><cell>0.0410</cell><cell>0.0318</cell><cell>0.1078</cell><cell>0.0507</cell></row><row><cell></cell><cell>SGL-ID</cell><cell>0.0649(+1.6%)</cell><cell>0.0533(+1.5%)</cell><cell>0.0450(+9.8%)</cell><cell>0.0353(+11.0%)</cell><cell>0.1119(+3.8%)</cell><cell>0.0529(+4.3%)</cell></row><row><cell>3 Layers</cell><cell>SGL-IM</cell><cell>0.0652(+2.0%)</cell><cell>0.0536(+2.1%)</cell><cell>0.0449(+9.5%)</cell><cell>0.0353(+11.0%)</cell><cell>0.1121(+4.0%)</cell><cell>0.0537(+5.9%)</cell></row><row><cell></cell><cell>SGL-ND</cell><cell>0.0644(+0.8%)</cell><cell>0.0528(0.6%)</cell><cell>0.0440(+7.3%)</cell><cell>0.0346(+8.8%)</cell><cell>0.1126(4.5%)</cell><cell>0.0536(+5.7%)</cell></row><row><cell></cell><cell>SGL-ED</cell><cell cols="5">0.0675(+5.6%) 0.0555(+5.7%) 0.0478(+16.6%) 0.0379(+19.2%) 0.1126(+4.5%)</cell><cell>0.0538(+6.1%)</cell></row></table><note>learning rate of 0.001 and mini-batch size of 2048. The early stopping strategy is the same as NGCF and LightGCN. The proposed SGL methods inherit the optimal values of the shared hyper-parameters. For the unique ones of SGL, we tune three ğœ† 1 , ğœ, and ğœŒ within the ranges of {0.005, 0.01, 0.05, 0.1, 0.5, 1.0}, {0.1, 0.2, 0.5, 1.0}, and {0, 0.1, 0.2, â€¢ â€¢ â€¢ , 0.5}, respectively.4.2 Performance Comparison (RQ1)4.2.1 Comparison with LightGCN. Table</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Overall Performance Comparison.</figDesc><table><row><cell>Dataset</cell><cell>Yelp2018</cell><cell>Amazon-Book</cell><cell>Alibaba-iFashion</cell></row><row><cell>Method</cell><cell cols="3">Recall NDCG Recall NDCG Recall NDCG</cell></row><row><cell>NGCF</cell><cell cols="3">0.0579 0.0477 0.0344 0.0263 0.1043 0.0486</cell></row><row><cell cols="4">LightGCN 0.0639 0.0525 0.0411 0.0315 0.1078 0.0507</cell></row><row><cell cols="4">Mult-VAE 0.0584 0.0450 0.0407 0.0315 0.1041 0.0497</cell></row><row><cell>SGL-ED</cell><cell cols="3">0.0675 0.0555 0.0478 0.0379 0.1126 0.0538</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The comparison of different SSL variants</figDesc><table><row><cell>Dataset</cell><cell>Yelp2018</cell><cell>Amazon-Book</cell></row><row><cell>Method</cell><cell cols="2">Recall NDCG Recall NDCG</cell></row><row><cell>SGL-ED-user</cell><cell cols="2">0.0640 0.0523 0.0445 0.0348</cell></row><row><cell>SGL-ED-item</cell><cell cols="2">0.0651 0.0535 0.0442 0.0349</cell></row><row><cell>SGL-ED-batch</cell><cell cols="2">0.0670 0.0549 0.0472 0.0374</cell></row><row><cell cols="3">SGL-ED-merge 0.0671 0.0547 0.0464 0.0368</cell></row><row><cell>SGL-pre</cell><cell cols="2">0.0653 0.0533 0.0429 0.0333</cell></row><row><cell>SGL-ED</cell><cell cols="2">0.0675 0.0555 0.0478 0.0379</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/wenyuer/POG Conference'17, July</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2017" xml:id="foot_1">, Washington, DC, USA Jiancan Wu, et al.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">Conference'17, July 2017, Washington, DC, USA Jiancan Wu, et al.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Context-Aware Friend Recommendation for Location Based Social Networks using Random Walk</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bagci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Karagoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="531" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Video suggestion and discovery for youtube: taking random walks through the view graph</title>
		<author>
			<persName><forename type="first">Shumeet</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Yagnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Aly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="895" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A Generic Coordinate Descent Framework for Learning from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Immanuel</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhargav</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<idno>WWW. 1341-1350</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unifying Knowledge Graph Learning and Recommendation: Towards a Better Understanding of User Preferences</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zikun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>CoRR abs/2002.05709</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">POG: Personalized Outfit Generation for Fashion Recommendation at Alibaba iFashion</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Pfadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2662" to="2670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Power-Law Distributions in Empirical Data</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosma</forename><forename type="middle">Rohilla</forename><surname>Shalizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">E J</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="661" to="703" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised Representation Learning by Predicting Image Rotations</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>HyvÃ¤rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AISTATS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="297" to="304" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contrastive Multi-View Representation Learning on Graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3451" to="3461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAIS: Neural Attentive Item Similarity Model for Recommendation</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2354" to="2366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>WWW. 173-182</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collaborative Filtering for Implicit Feedback Datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning Representations for Automatic Colorization</title>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="577" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction</title>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational Autoencoders for Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning Representations by Maximizing Mutual Information in Variational Autoencoders</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Lotfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Rezaabad</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Vishwanath</surname></persName>
		</author>
		<idno>ISIT. 2729-2734</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Power law distributions in information science: Making the case for logarithmic binning</title>
		<author>
			<persName><forename type="first">Stasa</forename><surname>Milojevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Assoc. Inf. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="2417" to="2425" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno>KDD. 1150-1160</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Investigating and Mitigating Degree-Related Biases in Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<title level="m">Contrastive Multiview Coding. CoRR abs/1906</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">5849</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>CoRR abs/1706.02263</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conditional Image Generation with PixelCNN Decoders</title>
		<author>
			<persName><forename type="first">AÃ¤ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
	<note>Oriol Vinyals, and Alex Graves</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Representation Learning with Contrastive Predictive Coding</title>
		<author>
			<persName><forename type="first">AÃ¤ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>CoRR abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">DropoutNet: Addressing Cold Start in Recommender Systems</title>
		<author>
			<persName><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomi</forename><surname>Poutanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4957" to="4966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Knowledge Graph Convolutional Networks for Recommender Systems</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3307" to="3313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">KGAT: Knowledge Graph Attention Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised Feature Learning via Non-Parametric Instance Discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Self-supervised Learning for Deep Models in Recommendations. CoRR abs</title>
		<author>
			<persName><forename type="first">Tiansheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">Zhiyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichan</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Tjoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieqi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Ettinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2020. 2007. 2020</date>
			<biblScope unit="page">12865</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">SË†3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
