<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Multi-Context Segmentation of Remote Sensing Images based on Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Keiller</forename><surname>Nogueira</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mauro</forename><surname>Dalla Mura</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jocelyn</forename><surname>Chanussot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">William</forename><surname>Robson Schwartz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jefersson</forename><forename type="middle">A</forename><surname>Dos Santos</surname></persName>
						</author>
						<title level="a" type="main">Dynamic Multi-Context Segmentation of Remote Sensing Images based on Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic Segmentation</term>
					<term>Deep Learning</term>
					<term>Convolutional Networks</term>
					<term>Multi-scale</term>
					<term>Multi-context</term>
					<term>Remote Sensing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic segmentation requires methods capable of learning high-level features while dealing with large volume of data. Towards such goal, Convolutional Networks can learn specific and adaptable features based on the data. However, these networks are not capable of processing a whole remote sensing image, given its huge size. To overcome such limitation, the image is processed using fixed size patches. The definition of the input patch size is usually performed empirically (evaluating several sizes) or imposed (by network constraint). Both strategies suffer from drawbacks and could not lead to the best patch size.</p><p>To alleviate this problem, several works exploited multi-context information by combining networks or layers. This process increases the number of parameters resulting in a more difficult model to train. In this work, we propose a novel technique to perform semantic segmentation of remote sensing images that exploits a multi-context paradigm without increasing the number of parameters while defining, in training time, the best patch size. The main idea is to train a dilated network with distinct patch sizes, allowing it to capture multi-context characteristics from heterogeneous contexts. While processing these varying patches, the network provides a score for each patch size, helping in the definition of the best size for the current scenario. A systematic evaluation of the proposed algorithm is conducted using four high-resolution remote sensing datasets with very distinct properties. Our results show that the proposed algorithm provides improvements in pixelwise classification accuracy when compared to state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The increased accessibility to high spatial resolution data provided by new sensor technologies has opened new horizons to the remote sensing community <ref type="bibr" target="#b0">[1]</ref>, allowing a better understanding of the Earth's surface <ref type="bibr" target="#b1">[2]</ref>. Towards such understanding, one of the most important task is semantic labeling (or segmentation) <ref type="bibr" target="#b2">[3]</ref>, which may be stated as a task of assigning a semantic category to every pixel in an image. Semantic segmentation allows the creation of thematic maps aiming to help in the comprehension of a scene <ref type="bibr" target="#b2">[3]</ref>. In fact, semantic labeling has been an essential task for the remote sensing community <ref type="bibr" target="#b3">[4]</ref> given that its outcome, the thematic map, generates essential and useful information capable of assisting in the decision making of a wide range of fields, including environmental monitoring, intelligent agriculture <ref type="bibr" target="#b4">[5]</ref>, disaster relief <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, urban planning <ref type="bibr" target="#b7">[8]</ref>. K. Nogueira, W. R. Schwartz, and J. A. dos Santos are with the Department of Computer Science, Universidade Federal de Minas Gerais, Brazil email: {keiller.nogueira, william, jefersson}@dcc.ufmg.br M. Dalla Mura, and J. Chanussot are with Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, 38000 Grenoble, France email: {mauro.dalla-mura, jocelyn.chanussot}@gipsa-lab.grenoble-inp.fr Fig. <ref type="figure" target="#fig_2">1</ref>: Example showing the importance of multi-context information. In the top case, while smaller contexts may not provide enough information for the understanding of the scene, a large context brings more information that may help the model to identify that it is a road with a car on it. In the bottom scenario, smaller contexts bring enough information for the identification of cars, while a large context may confuse the network and lead it to misclassify a different object as a car.</p><p>Given the importance of such task, several methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> have been proposed for the semantic segmentation of remote sensing images. The current state-of-the-art method for semantic segmentation is based on a resurgent approach, called deep learning <ref type="bibr" target="#b10">[11]</ref>, that can learn specific and adaptable spatial features directly from the images. Specifically, deep learning aims at designing end-to-end trainable neural networks, i.e., systems that map raw input into an output space depending on the task. These systems are capable of learning features and classifiers (in distinct layers) and adjust the parameters, at running time, based on accuracy, giving more importance to one layer than another depending on the problem. This endto-end feature learning (e.g., from image pixels to semantic labels) is the great advantage of deep learning when compared to previous state-of-the-art methods <ref type="bibr" target="#b11">[12]</ref>, such as lowlevel <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref> and mid-level (e.g. Bag of Visual Words <ref type="bibr" target="#b15">[16]</ref>) descriptors.</p><p>Among all networks, a specific type, called Convolutional (Neural) Networks, ConvNets or CNNs <ref type="bibr" target="#b10">[11]</ref>, is the most traditional one for learning visual features in computer vision applications, as well as remote sensing. This type of network relies on the natural stationary property of an image, i.e., the information learned in one part of the image can be used to describe any other region of the image. Furthermore, ConvNets usually obtain different levels of abstraction for the data, ranging from local low-level information in the initial layers arXiv:1804.04020v3 [cs.CV] 23 Apr 2019 (e.g., corners and edges), to more semantic descriptors, midlevel information (e.g., object parts) in intermediate layers, and high-level information (e.g., whole objects) in the final layers.</p><p>Although originally proposed for image classification, to become more suitable for the semantic labeling task, these ConvNets were adapted to output a dense prediction, i.e., to produce another image (usually with the same resolution of the input) that has each pixel associated to a semantic class. Based on this idea, several networks <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref> achieved stateof-the-art for the labeling task in the computer vision domain. Because of their success, these approaches were naturally introduced into the remote sensing scenario. Although somehow successful in this domain, these approaches could be improved if some differences for aerial images were taken into account. Specifically, the main difference concerns the definition of spatial context. In classical computer vision applications, the spatial context is restricted by the scene. In the case of remote sensing images, the context is typically delimited by an input patch (mainly because of memory constraints, given the huge size of remote sensing images). Therefore, the definition of the best input patch size is of vital importance for the network, given that patches of small size could not bring enough information to allow the network to capture the patterns while, larger patches could lead to semantically mixed information, which could affect the performance of the ConvNet. In the literature, the definition of this patch size is usually performed using two strategies: (i) empirically <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b19">[20]</ref>, by evaluating several sizes and selecting the best one, which is a very expensive process, given that, for each size, a new network must be trained (without any guarantee for the best patch configuration), and (ii) imposed <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, in which the patch size is defined by network constraints (i.e., changing the patch size implies modifying the architecture). This could be a potentially serious limitation given that the patch size required by the network could be not even close to the optimal one. Hence, it is clear that both current strategies suffer from drawbacks and could not lead to the best patch size.</p><p>An attempt to alleviate such dependence of the patch size is to aggregate multi-context information <ref type="foot" target="#foot_0">1</ref> . Multi-context paradigm has been proven to be essential for segmentation methods <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>, given that it allows the model to extract and capture patterns of varying granularities, helping the method to aggregate more useful information. Precisely, as presented and explained in (caption of) Figure <ref type="figure" target="#fig_2">1</ref>, smaller contexts may be preferable in some situations while larger ones can be useful in other scenarios. Therefore, several works <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b26">[27]</ref> incorporate the benefits of the multi-context paradigm in their architectures using different approaches. Some of them <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b25">[26]</ref> train several distinct layers or networks, one for each context, and combine them for the final prediction. Others <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref> extract and merge features from distinct layers in order to aggregate multi-context information. Independently of the approach, to aggregate multi-context information, more parameters are included in the final model, resulting in a more complex learning process <ref type="bibr" target="#b10">[11]</ref>.</p><p>In this work, we propose a novel technique to perform semantic segmentation of remote sensing images that exploits the multi-context paradigm without increasing the number of parameters while defining adaptively the best patch size for the inference stage. Specifically, this technique is based upon an architecture composed exclusively on dilated convolutions <ref type="bibr" target="#b27">[28]</ref>, which are capable of processing input patch of varying sizes without distinction, given that they learn the patterns without downsampling the input. In fact, the multicontext information is aggregated to the model by allowing it to be trained using patches of varying sizes (and contexts), a process that increases scale-invariance and reduces overfitting <ref type="bibr" target="#b28">[29]</ref>. This procedure allows the extraction of multicontext information without any combination of distinct networks or layers (a common process of deep learning-based multi-context approaches), resulting in a method with fewer parameters and easier to train. Moreover, during the training stage, the network gives a score (based on accuracy or loss) for each patch size. Then, in the prediction phase, the process selects the patch size with the highest score to perform the segmentation. Therefore, differently from empirically selecting the best patch size which requires a new network trained for each evaluated patch (increasing the computational complexity and training time), the proposed technique evaluates several patches during the training stage and selects the best one for the inference phase doing only a unique training procedure. Aside from the aforementioned advantages, the proposed networks can be fine-tuned for any semantic segmentation application, since they do not depend on the patch size to process the data. This allows other applications to benefit from the patterns extracted by our models, a very interesting feature specially when working with small amounts of labeled data <ref type="bibr" target="#b14">[15]</ref>.</p><p>In practice, these are the contributions of this work:</p><p>• Our main contribution is a novel approach that performs remote sensing semantic segmentation by doing a unique training procedure that aggregates multi-context information while determining the best input patch size for the inference stage, • Network architectures capable of performing semantic segmentation of remote sensing datasets with distinct properties, and that can be trained or fine-tuned for any semantic segmentation application.</p><p>The paper is structured as follows. Related works are presented in Section II while the concept of dilated convolutions is introduced in Section III. We explain the proposed technique in Section IV. Section V presents the experimental protocol and Section VI reports and discusses the obtained results. Finally, in Section VII we conclude the paper and point at promising directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>As introduced, deep learning has made its way into the remote sensing community, mainly due to its success in several computer vision tasks <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b32">[33]</ref>. Towards a better understanding of the Earth's surface, a myriad of techniques <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b26">[27]</ref> have been proposed to perform semantic segmentation in remote sensing images. Based on previous successful models <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b33">[34]</ref>, several of the proposed works exploit the benefits of the multi-context paradigm.</p><p>In <ref type="bibr" target="#b20">[21]</ref>, the authors fine-tuned a deconvolutional network (based on SegNet <ref type="bibr" target="#b17">[18]</ref>) using 256 × 256 fixed size patches. To incorporate multi-context knowledge into the learning process, they proposed a multi-kernel technique at the last convolutional layer. Specifically, the last layer is decomposed into three branches. Each branch processes the same feature maps but using distinct filter sizes generating different outputs which are combined into the final dense prediction. They argue that these different scales smooth the final predictions due to the combination of distinct fields of view and spatial context.</p><p>Sherrah <ref type="bibr" target="#b19">[20]</ref> proposed methods based on fully convolutional networks <ref type="bibr" target="#b16">[17]</ref>. The first architecture was purely based on the fully convolutional paradigm, i.e., the network has several downsampling layers (generating a coarse map) and a final bilinear interpolation layer, which is responsible to restore the coarse map into a dense prediction. In the second strategy, the previous network was adapted by replacing the downsampling layers with dilated convolutions, allowing the network to maintain the full resolution of the image. Finally, the last strategy evaluated by the authors was to fine-tune <ref type="bibr" target="#b14">[15]</ref> pretrained networks over the remote sensing datasets. None of the aforementioned strategies exploit the benefits of the multicontext paradigm. Furthermore, these techniques were evaluated using several input patch sizes with final architectures processing patches with 128 × 128 or 256 × 256 pixels depending on the dataset.</p><p>Marcu et al. <ref type="bibr" target="#b23">[24]</ref> combined the outputs of a dual-stream network in order to aggregate multi-context information for semantic segmentation. Specifically, each network processes the image using patches of distinct size, i.e., one network process 256 × 256 patches (in which the global context is considered) while the other processes 64 × 64 patches (where local context is taken into account). The outputs of these architectures are combined in a later stage using another network. Although they can train the network jointly, in an end-to-end process, the number of parameters is really huge allowing them to use only small values of batch size (10 patches per batch). In <ref type="bibr" target="#b25">[26]</ref>, the authors proposed a multi-context semantic segmentation by combining ConvNets, hand-crafted descriptors, and Conditional Random Fields <ref type="bibr" target="#b34">[35]</ref>. Specifically, they trained three ConvNets, each one with a different patch size (16 × 16, 32 × 32 and 64 × 64 pixels). Features extracted from these networks are combined with hand-crafted ones and classified using random forest classifier. Finally, Conditional Random Fields <ref type="bibr" target="#b34">[35]</ref> are used as a post-processing method in an attempt to improve the final results.</p><p>In <ref type="bibr" target="#b21">[22]</ref>, the authors proposed multi-context methods that combine boundary detection with deconvolution networks (specifically, based on SegNet <ref type="bibr" target="#b17">[18]</ref>). The main contribution of this work is the Class-Boundary (CB) network, which is responsible to help the proposed methods to give more attention to the boundaries. Based on this CB network, they proposed several methods. The first uses three networks that receive as input the same image but with different resolutions (as well as the output of the corresponding CB network) and output the label predictions, which are aggregated, in a subsequent fusion stage, generating the final label. They also experimented fully convolutional architectures <ref type="bibr" target="#b16">[17]</ref> (with several skip layers in order to aggregate multi-context information) and an ensemble of several architectures. All aforementioned networks initially receive 256 × 256 fixed size patches. Maggiori et al. <ref type="bibr" target="#b24">[25]</ref> proposed a multi-context method that performs labeling segmentation based on upsampled and concatenated features extracted from distinct layers of a fully convolutional network <ref type="bibr" target="#b16">[17]</ref>. Specifically, the network, that receives as input patches of 256 × 256 or 512 × 512 pixels (depending on the dataset), is composed of several convolutional and pooling layers, which downsample the input image. Downsampled feature maps extracted from several layers are, then, upsampled, concatenated and finally classified by another convolutional layer. This proposed strategy resembles somehow the DenseNets <ref type="bibr" target="#b35">[36]</ref>, with the final layer having connections to the previous ones. Wang et al. <ref type="bibr" target="#b26">[27]</ref> proposed to extract features from distinct layers of the network to capture multi-context low-and highlevel information. They fine-tuned a ResNet-101 <ref type="bibr" target="#b36">[37]</ref> to extract salient information from 600 × 600 patches. Feature maps are then extracted from intermediate layers, combined with entropy maps, and upsampled to generate the final dense prediction.</p><p>In this work, we perform semantic segmentation by exploiting a multi-context network composed uniquely of dilated convolutions. Three main differences between the proposed approach and the aforementioned works may be pointed out: (i) the proposed technique exploits a fully convolutional network that does not downsample the input data (a common process performed in most works <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>), (ii) the multi-context strategy is exploited during the training process without any modification of the network or combination of several architectures (or layers), and (iii) instead of evaluating possible patch sizes (to find the best one) or to use a patch size determined by network constraints (which could not be the best one), the proposed algorithm determines the best patch size adaptively in training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DILATED CONVNETS</head><p>Dilated convolutions were originally proposed for the computation of wavelet transform <ref type="bibr" target="#b37">[38]</ref> and employed in the deep learning context (as an alternative to deconvolution layers) mainly for semantic segmentation <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In dilated convolutional layers, filter weights are employed differently when compared to standard convolutions. Specifically, filters of this layer may have gaps (or "holes") between their parameters. These gaps, inserted according to the dilation rate r, enlarge the convolutional kernel but preserve the number of trainable parameters since the inserted holes are not considered in the convolution process. Therefore, this dilation rate r can be seen as a parameter responsible to define the final alignment of the kernel weights. Formally, for each location i, the output y of an onedimension dilated convolution given as input a signal x and filter w of length K is calculated as:</p><formula xml:id="formula_0">y[i] = K k=1 x[i + rk]w[k]<label>(1)</label></formula><p>The dilation rate parameter r ∈ N corresponds to the stride with which the input signal is sampled. As illustrated by Figure <ref type="figure">2</ref>, smaller rates result in a more clustered filter (in fact, rate 1 generates a kernel identical to the standard convolution) while larger rates make an expansion of the filter, producing a larger kernel with several gaps. Since this whole process is independent of the input data, changing the dilation rate does not impact in the resolution of the outcome, i.e., in a dilated convolution, independent of the rate, input and output have the same resolution (considering appropriate stride and padding). By enlarging the filter (with such gaps), the network expands its receptive field (since the weights will be arranged in a more sparse shape) but preserves the resolution and no downsampling in the data is performed. Hence, this process has several advantages, such as: (i) supports the expansion of the receptive field without increasing the number of trainable parameters per layer <ref type="bibr" target="#b27">[28]</ref>, which reduces the computational burden, and (ii) preserves the feature map resolution, which may help the network to extract even more useful information from the data, mainly of small objects.</p><p>To better understand the aforementioned advantage, a comparison between dilated and standard convolution is presented in Figure <ref type="figure" target="#fig_1">3</ref>. Given an image, the first network (in red) performs a downsampling operation (that reduces the resolution by a factor of 2) and a convolution, using horizontal Gaussian derivative as the kernel. The obtained low-resolution feature map is then enlarged by an upsampling operation (with a factor of 2) that restores the original resolution but not the information lost during the downsampling process. The second network (blue) computes the response of a dilated convolution on the original image. In this case, the same kernel was used but rearranged with dilation rate r = 2, making both networks have the same receptive field. Although the filter size increases, only non-zero values are taken into account when performing the convolution. Therefore, the number of filter parameters and of operations per position stay constant. Furthermore, it is possible to observe that salient features are better represented by the dilated model since no downsampling is performed over the input data. Top (red) row presents the feature extraction process using a standard convolution over a downsampled image and then an upsample in order to recover the input resolution (a common procedure performed in ConvNets). Bottom (blue) row presents the feature extraction process using dilated convolution with rate r = 2 applied directly to the input (without downsample). The outcomes clearly show the benefits of dilated convolutions over standard ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DYNAMIC MULTI-CONTEXT DILATED CONVOLUTION</head><p>In this section, we present the proposed method for dynamic multi-context semantic segmentation of remote sensing images. The proposed methodology is presented in Section IV-A while the network architectures are described in Section IV-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dynamic Multi-Context Algorithm</head><p>We propose a novel method to perform semantic segmentation of remote sensing images that: (i) exploits the multicontext paradigm without increasing the number of trainable parameters of the network, and (ii) defines, in training time, the best patch size that should be exploited by the network in the test phase.</p><p>As presented in Algorithm 1, the training process receives as input: (i) the data D, where the images and their reference labels come from, (ii) a patch size distribution P, that represents the probability function (kept the same during all the training procedure) from which the patch sizes will come from, (iii) the patch scores S (initialized with zeros), which will be used during the training procedure to accumulate the score of the patch sizes produced by the network, (iv) the network N , which, in this work, can be seen as a function that processes the input batch (X , Y) ∈ D (a tuple of patches and reference semantic labels) with respect to the current weights W, updating them, and outputting a score for the batch v, that can be seen, somehow, as a quality assessment of the patch size relative to the current network, (v) the number of iterations or epochs n.</p><p>The first step of the training procedure is to randomly select a patch size λ k from the distribution P, which may be any valid distribution, such as uniform or multinomial. Then, this patch size λ k is used to create a new batch</p><formula xml:id="formula_1">(X λ k ×λ k , Y λ k ×λ k ) ∈ D.</formula><p>Observe that, at each iteration of the algorithm, a new patch size is selected and a new random batch (using different sites) is sampled based on this size. This batch is then employed to train the network N , i.e., to update its weights W. It is important to emphasize that this training process (performed by the sampled batch) represents only a single step (iteration) of the mini-batch optimization strategy <ref type="bibr" target="#b10">[11]</ref> (and not the full train) that processes one whole batch to then update the network weights W. As aforementioned, for each step of the mini-batch training algorithm, the network N outputs a score for the current batch v, which can be any metric (such as a loss or accuracy) that estimates the performance of the network based on the current batch. This generated score v is used to update the patch scores S, which accumulate, throughout the training procedure, the scores of the patch sizes and are employed in the selection of the best patch size during the inference stage. Note the difference between the patch distribution P and the scores S, i.e., while the former is a distribution employed during the whole training procedure the latter accumulates the scores of the patch sizes given by the network to be employed in the prediction phase. Hence, there is no connection between patch distribution P and the scores S, and an update in S has no impact on P, which is kept fixed throughout the training process.</p><p>All the aforementioned steps are repeated during the training process until the number of iterations n is reached. As it can be noticed, the multi-context information is aggregated to the model by allowing the network to be trained using batches composed of patches of multiple sizes. This process allows the network to capture and extract features by considering distinct context regions, a very important process as presented and explained in (caption of) Figure <ref type="figure" target="#fig_2">1</ref>.</p><p>When the training phase is finalized, the algorithm outputs the updated network N (i.e., its weights W) and the updated patch scores S. The second benefit of the proposed method is almost a direct application of the patch scores S created during the training phase. Precisely, in the prediction phase, scores S over the patch sizes are averaged and analyzed. The best patch size λ * (which corresponds to the highest or lowest score, for accuracy and loss, respectively) is then selected and used to create patches. The network processes these patches (of λ * × λ * pixels) outputting the prediction maps, but no updates in the patch scores S are performed. It is important to highlight that the proposed technique can only choose the best patch size within all possible sizes determined by the patch distribution P, since only the patches within P are evaluated by the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architectures</head><p>As presented in Section III, the properties of the dilated convolutions <ref type="bibr" target="#b27">[28]</ref> make them fit perfectly into the proposed multi-context methodology, given that a network composed of such layers is capable of processing an input of any size without downsampling it. This creates the possibility of processing patches of any size without constraints. Although these layers have the advantage of computing feature responses Require: data D, network N with its weights W, number of iterations n, patch distribution P, and patch scores S (initialized with zeros). Ensure: updated of the network weights W, and patch scores S. for t=1 to n do</p><formula xml:id="formula_2">λ k = P(k) {Randomly select current size} (X λ k ×λ k , Y λ k ×λ k ) ∈ D {Create new batch} v λ k = N (X λ k ×λ k , Y λ k ×λ k ; W) {Continue training} S λ k = S λ k + v λ k {Update scores} end for</formula><p>at the original image resolution, a network composed uniquely of dilated convolutions would be costly to train especially when processing entire (large) scenes. However, as previously mentioned, processing an entire remote sensing image is not possible (because of its huge size) and, therefore splitting the image into small patches is already necessary, which naturally alleviates the training process.</p><p>Though, in this work, we explore networks composed of dilated convolutions, other types of ConvNets could be used, such as fully convolutions <ref type="bibr" target="#b16">[17]</ref> and deconvolutions <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. These networks can also process patches of varying size, but they have restrictions related to a high variation of the patch size. Specifically, these networks need to receive a patch larger enough to allow the generation of a coarse map, that is upsampled to the original size. If the input patch is too small, the network could reach a situation where it is not possible to create the coarse map and, consequently, the final upsampled map. Such problem is overcome by dilated convolutions <ref type="bibr" target="#b27">[28]</ref>, which are allowed to process patches of any size, without distinction, always outputting results with the same resolution of the input data (given proper configurations, such as stride and padding). Such concept is essential to allow the variance of patch sizes, from very small values (such as 7 × 7) to larger ones (for instance, 256 × 256).</p><p>Considering this, a full set of experiments (guided by <ref type="bibr" target="#b39">[40]</ref>) was performed in order to define the best architectures. After the experiments, four networks, illustrated in Figure <ref type="figure">4</ref>, have been selected (based on the accuracy) and extensively evaluated in this work. The first network, presented in Figure <ref type="figure">4a</ref>, is composed of seven layers: six dilated convolutions (that are responsible to capture the patterns of the input images) and a final 1 × 1 convolution layer, which is responsible to generate the dense predictions. There is no pooling or normalization in this network, and all layers have stride 1. Specifically, the first two convolutions have 5×5 filters with dilation rate r 1 and 2, respectively. The following two convolutions have 4 × 4 filters but rate 3 and 4 while the last two convolutions have smaller filters (3×3) but 5 and 6 as dilation rate. Because this network has 6 layers responsible for the feature extraction, it will be referenced as Dilated6. The second network (Figure <ref type="figure">4b</ref>) is based on densely connected networks <ref type="bibr" target="#b35">[36]</ref>, which recently achieved outstanding results on the image classification task. This network is very similar to the first one having the same number of layers and configuration. The main difference between these networks is that a layer receives as input feature maps of all preceding layers. Hence, the last layer has access to all feature maps generated by all other layers of the network. This process allows the network to combine different feature maps with distinct level of abstraction, supporting the capture and learning of a wide range of feature combination. Because this network has 6 layers responsible for the feature extraction and is densely connected, it will be referenced in this work as DenseDilated6. The third network, presented in Figure <ref type="figure">4c</ref>, has the same configuration of the Dilated6, but with pooling layers between each convolutional one. Given a specific combination of stride and padding, no downsampling is performed over the inputs in these pooling layers. Because of the number of layers and the pooling layers, this network will be referenced hereafter as Dilated6Pooling. The last network (Figure <ref type="figure">4d</ref>) is an extension of the previous one, having 8 dilated convolutions instead of only 6. The last two convolutional layers have smaller filters (3 × 3) but 7 and 8 as dilation rate. There are pooling layers between all convolutional ones. Given that this network has 8 dilated convolutional and pooling layers, it will be referenced hereafter as Dilated8Pooling. Although only this network with 8 layers is explored in this work, other variant networks (such as Dilated8 and DenseDilated8) were initially considered but not retained for further experiments due to the similar initial performance and longer training time when compared to the Dilated6 variant networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL SETUP</head><p>In this section, we present the experimental setup. Specifically, Section V-A presents the datasets employed in this work. Baselines are described in Section V-B while the experimental protocol is introduced in Section V-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>To better evaluate the effectiveness of the proposed method, we carried out experiments on four high-resolution remote  <ref type="bibr" target="#b40">[41]</ref> is composed of 5 images taken by the SPOT sensor in 2005 over a famous coffee grower county (Monte Santo) in the State of Minas Gerais, Brazil. Each image has 500 × 500 pixels with green, red, and near-infrared bands (in this order), which are the most useful and representative ones for discriminating vegetation areas <ref type="bibr" target="#b22">[23]</ref>. More specifically, the dataset consists of 1,250,000 pixels classified into two classes: coffee (637,544 pixels or 51%) and non-coffee (612,456 pixels or 49%). Figure <ref type="figure" target="#fig_5">6</ref> presents the images and ground-truths of this dataset.</p><p>This dataset is very challenging for several different reasons, including: (i) high intraclass variance, caused by different crop management techniques, (ii) scenes with distinct plant ages, since coffee is an evergreen culture and, (iii) images with spectral distortions caused by shadows, since the South of Minas Gerais is a mountainous region.</p><p>2) GRSS Data Fusion Dataset: Proposed for the 2014 IEEE GRSS Data Fusion Contest, this dataset <ref type="bibr" target="#b41">[42]</ref> is composed of two (training and testing) fine-resolution visible (RGB) images that cover an urban area near Thetford Mines in Quebec, Canada. Both training and testing images have 0.2 meter of spatial resolution, with the former having 2830 ×3989 and the latter 3769 × 4386 pixels of resolution. Training and testing images, as well as the respective ground-truths, are presented in Figure <ref type="figure" target="#fig_6">7</ref>.</p><p>Pixels are categorized into seven classes: trees, vegetation, road, bare soil, red roof, gray roof, and concrete roof. The dataset is not balanced, as can be seen in Table <ref type="table" target="#tab_0">I</ref>. It is important to highlight that not all pixels are classified into one of these categories, with some pixels considered as uncategorized or unclassified.</p><p>3) Vaihingen Dataset: As introduced, this dataset <ref type="bibr" target="#b42">[43]</ref> was released for the 2D semantic labeling contest of the International Society for Photogrammetry and Remote Sensing 2 http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html The pixel distribution for the labeled images can be seen in Table <ref type="table" target="#tab_1">II</ref>. Each image of this dataset is composed of near-infrared, red and green channels (in this order) and has a spatial resolution of 0.9 meter. A Digital Surface Model (DSM) coregistered to the image data was also provided, allowing the creation of a normalized DSM (nDSM) by <ref type="bibr" target="#b43">[44]</ref>. In this work, we use the spectral information (NIR-R-G) and the nDSM, i.e., the input data for the method has 4 dimensions: NIR-R-G and nDSM. Examples of the Vaihingen Dataset can be seen in Figure <ref type="figure">8</ref>.</p><p>4) Potsdam Dataset: Also proposed for the 2D semantic labeling contest, this dataset <ref type="bibr" target="#b44">[45]</ref> has 38 tiles of the same size (6000 × 6000 pixels), with a spatial resolution of 0.5 meter. From the available patches, 24 are densely annotated (with same classes as for the Vaihingen dataset), in which the pixel distribution is presented in Table <ref type="table" target="#tab_1">II</ref>. Analogously to the Vaihingen dataset, the remaining images are considered the test set and do not have available annotation, requiring submission of the predictions in order to be evaluated. This dataset consists of 4-channel images (near-infrared, red, green and blue), Digital Surface Model (DSM), and normalized DSM (nDSM). In this work, all spectral channels plus the nDSM are used as input for the ConvNet, resulting in a 5dimensional input data. Some samples of these images are presented in Figure <ref type="figure" target="#fig_2">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baselines</head><p>For the coffee dataset, we employed the Cascaded Convolutional Neural Network (CCNN) <ref type="bibr" target="#b4">[5]</ref> as baseline. This method employs a multi-context strategy by aggregating several Con-vNets in order to perform the classification of fixed size tiles towards the final segmentation of the image. For the GRSS Data Fusion Dataset, we employed, as baseline, the work of Santana et al. <ref type="bibr" target="#b45">[46]</ref>. Their algorithm extracts features with many levels of context by exploiting different layers of a pre-trained convolutional network, which are then combined in order to aggregate multi-context information.</p><p>Aside this, for both aforementioned datasets, we also considered as baseline the method conceived by <ref type="bibr" target="#b40">[41]</ref>, in which specific networks are used to perform labeling segmentation using the pixelwise paradigm, i.e., each pixel is classified independently by the classifier. Also, for these two datasets, we considered as baselines: (i) Fully Convolutional Networks (FCN) <ref type="bibr" target="#b16">[17]</ref>. In this case, the pixelwise architectures proposed by <ref type="bibr" target="#b40">[41]</ref> were converted into fully convolutional network and exploited as baseline. (ii) Deconvolutional networks <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Again, the pixelwise architectures proposed by <ref type="bibr" target="#b40">[41]</ref> were converted into deconvolutional network (based on the wellknown SegNet <ref type="bibr" target="#b17">[18]</ref> architecture) and exploited as a baseline in this work. (iii) dilated network <ref type="bibr" target="#b27">[28]</ref>, which is, in this case, the Dilated6Pooling (Figure <ref type="figure">4c</ref>). All these networks were trained traditionally using patches of constant size defined according to a set of experiments of <ref type="bibr" target="#b40">[41]</ref>, i.e., patches of 7×7 and 25×25 for Coffee and GRSS Data Fusion datasets, respectively.</p><p>For the remaining datasets (Vaihingen and Potsdam), we refer to the official results published on the challenge website 3  as baselines for the proposed work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Protocol</head><p>For the Coffee <ref type="bibr" target="#b40">[41]</ref> and the GRSS Data Fusion <ref type="bibr" target="#b41">[42]</ref> datasets, we employed the same protocol of <ref type="bibr" target="#b40">[41]</ref>. Specifically, for the former dataset, we conducted a five-fold cross-validation to assess the performance of the proposed algorithm. In this strategy, five runs are executed, where, at each run, three coffee scenes are used as training while, one is used as validation, and the remaining one is used as test. The reported results are the average metric of the five runs followed by its corresponding standard deviation. For the GRSS Data Fusion dataset, an image was used for training while the other was used for test, since this dataset has a clear definition of training/testing.</p><p>For Vaihingen <ref type="bibr" target="#b42">[43]</ref> and Potsdam <ref type="bibr" target="#b44">[45]</ref> datasets, we followed the protocol proposed by <ref type="bibr" target="#b7">[8]</ref>. For the Vaihingen dataset, 11 out of the 16 annotated images were used to train the network. The 5 remaining images (with <ref type="bibr">IDs 11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34)</ref> were employed to validate and evaluate the segmentation generalization accuracy. For the Potsdam dataset, 18 (out of 24) images were used for training the proposed technique. The remaining 6 images (with IDs 02 12, 03 12, 04 12, 05 12, 06 12, 07 12) were employed for validation of the method.</p><p>Four metrics <ref type="bibr" target="#b46">[47]</ref> were considered to assess the performance of the proposed algorithm: overall and average accuracy, kappa index and F1 score. Overall accuracy is a metric that considers the global aspects of the classification, i.e., it takes into account all correctly classified pixels indistinctly. On the other hand, average accuracy reports the average (per-class) ratio of correctly classified samples, i.e., it outputs an average of the accuracy of each class. Kappa index measures the agreement between the reference and the prediction map. Finally, F1 score is defined as the harmonic mean of precision and recall. These metrics were selected based on their diversity: overall accuracy and kappa are biased toward large classes (relevance of classes with small amount of samples are canceled out by those with large amount) while average accuracy and F1 3 http://www2.isprs.org/commissions/comm2/wg4/vaihingen-2d-semanticlabeling-contest.html and http://www2.isprs.org/commissions/comm2/wg4/ potsdam-2d-semantic-labeling.html. are calculated specifically for each class and, therefore, are independent of class size. Hence, the presented results are always some combination of such metrics in order to provide enough information about the effectiveness of the proposed method.</p><p>The proposed method and network <ref type="foot" target="#foot_1">4</ref> were implemented using TensorFlow <ref type="bibr" target="#b47">[48]</ref>, a framework conceived to allow efficient exploitation of deep learning with Graphics Processing Units (GPUs). All experiments were performed on a 64 bits Intel i7 4960X machine with 3.6GHz of clock and 64GB of RAM memory. Four GeForce GTX Titan X with 12GB of memory, under an 8.0 CUDA version, were employed in this work. Note, however, that each GPU was used independently and that all networks proposed here can be trained using only one GPU. Ubuntu version 16.04.3 LTS was used as operating system.</p><p>As previously stated, a set of experiments (guided by <ref type="bibr" target="#b39">[40]</ref>) was executed to define the hyperparameters. After all the setup experiments, the best values for hyperparameters, presented in Table <ref type="table" target="#tab_2">III</ref>, were determined for each dataset. The number of iterations increases with the complexity of the dataset in order to ensure convergence. In the proposed models, the learning rate, responsible to determine how much an updating step influences the current value of the network weights, starts with a high value and is reduced during the training phase using the exponential decay <ref type="bibr" target="#b47">[48]</ref> with parameters defined according to the last column of Table <ref type="table" target="#tab_2">III</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS AND DISCUSSION</head><p>In this section, we present and discuss the obtained results. Specifically, we first analyze the parameters of the proposed technique: Section VI-A presents the results achieved using different patch distributions, Section VI-B analyzes distinct functions to update the patch size score, and Section VI-C evaluates different ranges for the patch size. Then, a comparison between the dilated and standard convolution is presented in Section VI-D. A convergence analysis of the proposed technique is performed in Section VI-E while a comparison between networks trained with the proposed and standard training techniques is presented in Section VI-F. Finally, a comparison with the state-of-the-art is reported in Section VI-G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Patch Distribution Analysis</head><p>As explained at the beginning of Section IV-A, the algorithm receives as input a list of possible patch sizes and a correspondent distribution. In fact, any distribution could be used, including uniform or multinomial. Given the influence The main difference between the evaluated distributions is related to the prior knowledge of the application. In the uniform distribution, no prior knowledge is assumed, and all patch sizes from the input range have the same probability, taking more time to converge the model. The uniform fixed distribution assumes a good knowledge of the application and only pre-defined patch sizes can be (equally) selected and evaluated, taking less time to converge the model. The multinomial distribution tries to blend previous ideas. Assuming a certain prior knowledge of the application, the multinomial distribution weighs the probabilities allowing the network to give more attention to specific pre-defined patch sizes but without discarding the others. If prior intuition is confirmed, these pre-defined patch sizes are randomly selected more often and the network should converge faster. Otherwise, the proposed process is still able to use other (non-pre-defined) patch sizes and converge the network anyway.</p><p>Results of this analysis can be seen in Table <ref type="table" target="#tab_3">IV</ref>. Note that all experiments were performed using the Coffee dataset <ref type="bibr" target="#b40">[41]</ref>, Dilated6 network (Figure <ref type="figure">4a</ref>), accuracy as score function, and hyperparameters presented in Table <ref type="table" target="#tab_2">III</ref>. In these experiments, patches size varied from 25 × 25 to 50 × 50. Specifically, for the uniform distribution, any value between 25 and 50 has the same probability of being selected, while for the multinomial distribution, all values have some chance to be selected, but these two points have twice the probability. For the uniform fixed, these two patch sizes split the total probability and each one has 50% of being selected. Overall, the variation of the distribution has no serious impact on the final outcome, since results are all very similar. However, given its simplicity and faster convergence, for the remaining of this work, results will be reported using the uniform fixed distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Score Function Analysis</head><p>As introduced in Section IV, at each training iteration an update is performed in the score of patch sizes, which are used in the selection of the best patch size during the testing stage. In this work, we evaluated two possible score functions that could be employed in this step: the loss and the accuracy.</p><p>In the first case, the loss is a measure (obtained using cross entropy <ref type="bibr" target="#b10">[11]</ref>, in this case) that represents the error generated in terms of the ground-truths and the network predictions.</p><p>In the second case, the score is represented by the pixelwise classification accuracy <ref type="bibr" target="#b46">[47]</ref> of the images.</p><p>To analyze the most appropriate score function, experiments were performed varying only this particular parameter and maintaining the remaining ones. Specifically, these experiments were conducted using: the Coffee dataset <ref type="bibr" target="#b40">[41]</ref>, Dilated6 network (Figure <ref type="figure">4a</ref>), uniform fixed distribution (over 25 × 25 and 50×50), and same hyperparameters presented in Table <ref type="table" target="#tab_2">III</ref>. Results can be seen in Table <ref type="table" target="#tab_4">V</ref>. Through the table, it is possible to see that both score functions achieved similar results. However, since accuracy score is more intuitive, for the remaining of this work, results will be reported using this function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Range Analysis</head><p>Although the presented approach is proposed to select automatically the best patch size, in training time, avoiding lots of experiments to adjust such size (as done in several works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b40">[41]</ref>), in this section, the patch size range is analyzed in order to examine the robustness of the method.</p><p>This range is evaluated on all datasets, except Potsdam. Such dataset is very similar to Vaihingen one and, therefore, analysis and decisions made over the latter dataset are also applicable to the Potsdam one. Furthermore, in order to evaluate such dataset, a validation set, created according to <ref type="bibr" target="#b7">[8]</ref>, was employed. Experiments were conducted varying only the patch size range but maintaining the remaining configurations. Particularly, the experiments employed the same hyperparameters (presented in Table <ref type="table" target="#tab_2">III</ref>), Dilated6 network (Figure <ref type="figure">4a</ref>), and uniform fixed distribution.</p><p>Table <ref type="table" target="#tab_4">VI</ref> presents the obtained results. Each dataset was evaluated over several ranges, selected based on previous works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Specifically, each dataset was evaluated in a large range (comprising from small to large sizes) and subsets of such range. Table VI also presents the most selected patch size (for the testing phase) for each experiment, giving some insights about how the proposed method behaves during such step.</p><p>For the Coffee dataset <ref type="bibr" target="#b40">[41]</ref>, obtained results are all very similar making it difficult to define a better or worse range. Hence, any patch size range could be selected for further experiments, showing the robustness of the proposed algorithm which yielded similar results independently of the patch size range. Because of processing time (smaller patches are processed faster), in this case, patch size range 25, 50 was selected and used in all further experiments.</p><p>For remaining datasets, a specific range achieved the best result. For the GRSS Data Fusion dataset <ref type="bibr" target="#b41">[42]</ref>, the best result was obtained when considering the largest range <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr">56,</ref><ref type="bibr">63,</ref><ref type="bibr">70)</ref>, i.e., the range varying from small to large patch sizes. For Vaihingen <ref type="bibr" target="#b42">[43]</ref>, the intermediate range <ref type="bibr" target="#b44">(45,</ref><ref type="bibr">55,</ref><ref type="bibr">65,</ref><ref type="bibr">75,</ref><ref type="bibr">85)</ref> achieved the best result. Therefore, in these cases, such ranges were selected and used in the remaining experiments of this work. However, as can be seen through Table <ref type="table" target="#tab_4">VI</ref>, other ranges also produce competitive results and could be selected and used without significant loss of performance, which confirms the robustness of the proposed method in relation to the patch size range, allowing it to process images without the need of experimentally searching for the best patch size configuration.</p><p>In terms of patch size selection (during the inference phase), the algorithm really varies depending on the experiment. For the Coffee dataset, the most selected patch sizes were 50 and 75, showing a trend towards such interval. For the remaining datasets, larger patches were favored in our experiments. This may be justified by the fact that urban areas have complex interactions and larger patches allow the network to capture more information about the context. Though the best patch size is really dependent on the experiment, current results showed that the proposed approach is able to learn and select the best patch size in processing time producing interesting outcomes when compared to state-of-the-art works, a fact reconfirmed in Section VI-G. TABLE VI: Results of the proposed approach when varying the input range of patch sizes. For Vaihingen, a validation set (created according <ref type="bibr" target="#b7">[8]</ref>) is employed. Bold patch size ranges were selected for all further experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Convolution Operation Analysis</head><p>Although the proposed networks use dilated convolutions, it is possible to recreate such architectures using standard convolution operations. As introduced in Section III, the only difference between these convolution operations is the possibility to have gaps between the filter weights, a special characteristic of the dilated convolutions <ref type="bibr" target="#b27">[28]</ref>. Such aspect makes all the difference since dilated convolution can expand the exploited context (by enlarging the filter weights) without increasing the number of parameters, while standard convolutions are not able to do this since the filters are always grouped (without gaps). This is a great advantage since a deeper network composed of standard convolution operations (without any downsample or upsample operation) would require more layers in order to aggregate a large context, while a network composed of dilated convolutions can expand the context without increasing the number of parameters, requiring fewer layers.</p><p>In order to demonstrate this advantage of dilated convolutions over standard ones, we performed experiments comparing two networks that have exactly the same architecture (Dilated6 -Figure <ref type="figure">4a</ref>) but differ in the convolution operation: while one network uses dilated convolutions, the second architecture employs the standard operation. Since the Dilated6 network does not have pooling layers, the comparison between these networks is totally fair, given that the only difference is the convolution operation type. All datasets were used in this experiment, except Potsdam. This is because the Vaihingen and Potsdam datasets are very similar and analysis performed over one can also be extended to the other. A validation set, created according to <ref type="bibr" target="#b7">[8]</ref>, was used to evaluate the Vaihingen dataset. Experiments were executed preserving all configurations and varying only the convolution type. Particularly, the configuration was defined taken into account previous experiments, i.e., it uses uniform fixed distribution, patch ranging according to Section VI-C, accuracy as score function, and hyperparameters presented in Table <ref type="table" target="#tab_2">III</ref>.</p><p>Results can be seen in Table <ref type="table" target="#tab_6">VII</ref>. Overall, architectures based on dilated convolution outperformed the networks that employ the standard operation. Since the only difference between the networks is the convolution (and, consequently, the exploited context), these results show the advantage of the dilated operations over the standard one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Convergence Analysis</head><p>In this section, we analyze the convergence of the proposed technique. Figure <ref type="figure" target="#fig_4">5</ref> presents the convergence of the datasets using the Dilated6 network, accuracy as score function, uniform fixed distribution, and hyperparameters presented in Table <ref type="table" target="#tab_2">III</ref>. According to the figure, the loss and accuracy vary significantly at the beginning of the process but, with the reduction of the learning rate, the networks converge independently of the use of distinct patch sizes. Moreover, the test/validation accuracy (green line) converges and stabilizes showing that the networks can learn to extract features from patches of multiple sizes while selecting the best patch size for testing. For the Coffee dataset, only the fold 1 is reported. For Vaihingen and Potsdam datasets, the validation set (created according <ref type="bibr" target="#b7">[8]</ref>) is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Performance Analysis</head><p>To analyze the efficiency, in terms of performance and processing time, of the proposed algorithm, several experiments were conducted comparing the same network trained using two distinct methods: (i) the traditional training process <ref type="bibr" target="#b10">[11]</ref>, in which the network is trained using patches of constant size, without any variation. This method is the standard one when it comes to neural networks and is the most exploited in the literature for training deep learning-based techniques. Also, this is the approach that is used to empirically selects the best patch size, which is traditionally done by training several networks, one for each considered patch. (ii) the proposed dynamic training process, in which the network is trained with patches of varying size.</p><p>Two datasets were selected to be evaluated using these training strategies: (i) the GRSS Data Fusion dataset, which has the largest patch size range (according to Section VI-C) allowing a better comparison between the training strategies, and (ii) Vaihingen dataset, which is very similar to Potsdam one and, therefore, allows the conclusions to be applied to this one. To evaluate this dataset, a validation set, created according <ref type="bibr" target="#b7">[8]</ref>, was employed. Specifically, in these experiments, Dilated6 network (Figure <ref type="figure">4a</ref>) is trained using both strategies. For the proposed dynamic training process, previous experiments were taken into account, i.e., it uses uniform fixed distribution, patch ranging according to Section VI-C, accuracy as score function, and hyperparameters presented in Table <ref type="table" target="#tab_2">III</ref>. Concerning the traditional training process, several networks (with same architecture) were trained using each of the possible patch sizes.</p><p>Results of these experiments are presented in Table <ref type="table" target="#tab_7">VIII</ref>. For both datasets, networks trained with the proposed approach outperform the models trained with the traditional training process (independently of the patch size), showing the ability of the proposed method to capture multi-context information from patches of distinct size which improve the performance of the final model. Also, on average, the processing time of the proposed method is lower than the traditional training process, in which the computational time increases with the increase of the patch size, an expected behavior given that the convolution process using large inputs takes more time than using smaller ones.</p><p>Specifically, for the GRSS Data Fusion dataset, considering only models trained with the traditional method, the best result is achieved by the network using patches of 70×70 pixels. This ConvNet took around 160 hours to train using 200,000 iterations and achieved 86.93% of average accuracy. However, the model trained with the proposed dynamic process outperforms this result while taking less time to train. Particularly, Dilated6 network trained using the dynamic process produced 90.13% of average accuracy while taking around 81 hours to train. This improvement in the performance is due to the exploitation of distinct contexts provided by different patch sizes during the training procedure. This process of using distinct patch sizes also speeds up the training, given that small patches (which are processed faster) are also used together with large ones.</p><p>The same conclusions hold for the Vaihingen dataset. Precisely, the best result using the traditional method is achieved by the network trained with patches of 85 × 85 pixels. This ConvNet took around 325 hours to train using 500,000 iterations and achieved 66.96% of average accuracy. However, this result was outperformed by the network trained using the proposed dynamic strategy, while taking less training time. Such model produced 71.96% of average accuracy while taking around 220 hours to train.</p><p>Moreover, the proposed dynamic strategy has another advantage: while the empirical method would require training several networks in order to select the best patch size, resulting in a greater computational time, the proposed strategy combines all patch sizes during the training stage while selecting the best size for the inference phase, requiring only one full procedure to achieve its final result. Hence, overall, the proposed method requires less training time than the empirical approach, while achieving better results.</p><p>G. State-of-the-art Comparison 1) Coffee dataset: Using analysis performed on previous sections, we have conducted several experiments over the Coffee dataset <ref type="bibr" target="#b40">[41]</ref>. Results of these experiments, as well as the state-of-the-art baselines, are presented in Table <ref type="table" target="#tab_8">IX</ref>. In order to allow a visual comparison, prediction maps for the Coffee dataset using different networks trained with the proposed method are presented in Figure <ref type="figure" target="#fig_5">6</ref>.</p><p>Overall, all baselines produced similar results. While the pixelwise network <ref type="bibr" target="#b40">[41]</ref> yielded a slightly worse result with a higher standard deviation, all other baselines reached basically the same level of performance, with a smaller standard deviation. This may be justified by the fact that the pixelwise network does not learn much information about the pixel interaction (since each pixel is processed independently), while the other methods process and classify a set of pixels simultaneously. Because of the similar results, all baselines are comparable. This same behavior may be seen among the networks trained with the proposed methodology. Although these networks achieved comparable results, such models outperformed the baselines. Furthermore, the Dilated6Pooling trained with the proposed dynamic method produced better results than the same network trained with traditional training process (mainly in the Kappa Index). These results show the effectiveness of the proposed technique that produces state-of-the-art outcomes by capturing multi-context information while selecting the best patch size, two great advantages when compared to the traditional training process. 2) GRSS Data Fusion Dataset: We also performed several experiments on the GRSS Data Fusion Contest dataset <ref type="bibr" target="#b41">[42]</ref> considering all analysis carried out in previous sections. Experimental results, as well as baselines, are presented in Table <ref type="table" target="#tab_9">X</ref>. The prediction maps obtained for the test set are presented in Figure <ref type="figure" target="#fig_6">7</ref>.</p><p>Overall, Dilated6 produced the best result among all approaches. In general, networks trained with the proposed method outperformed the baselines. Moreover, the Di-lated6Pooling trained with the proposed dynamic technique outperformed the baseline composed of the same network trained using traditional training process, corroborating with previous conclusions.</p><p>Among the baseline methods, although all of them achieved comparable results, the best outcome was yielded by the Deep Contextual <ref type="bibr" target="#b45">[46]</ref>. This method also leverages from multicontext information, since it combines features extracted from distinct layers of pre-trained ConvNets. When comparing this method with the best result of the proposed technique (Dilated6), one can clearly observe the advantage of the proposed approach, which improves the results for all metrics when compared to the Deep Contextual <ref type="bibr" target="#b45">[46]</ref> approach. This reaffirms the effectiveness of the proposed dynamic method, corroborating with previous conclusions.</p><p>3) Vaihingen Dataset: As introduced in Section V-B, official results for the Vaihingen dataset are reported only by the challenge organization that held some images that are used for testing the submitted algorithms. Therefore, one must submit the outcomes of the proposed algorithm to have them evaluated. In our case, following previous analysis, we submitted five approaches: the first four are related to each network presented in Section IV trained with the 6 classes (which are represented in the official results as UFMG 1 to 4), and the fifth one, represented in the official results as UFMG 5, is the Dilated8 network (Figure <ref type="figure">4d</ref>) trained with only 5 classes, i.e., all labels except the clutter/background one. This last submission is due to the lack of training data for that class which corresponds to only 0.67% of the dataset (as stated in Table <ref type="table" target="#tab_1">II</ref>). It is important to note that all submissions related to the proposed work do not use any post-processing, such as Conditional Random Fields (CRF) <ref type="bibr" target="#b34">[35]</ref>.</p><p>Some official results reported by the organization are summarized in Table <ref type="table" target="#tab_10">XI</ref>. In addition to our results, this table also compiles the best results of each work with enough information to make a fair comparison, i.e., in which the proposed approach is minimally explained. In order to allow a visual comparison, examples of the proposed method, for the validation and test sets, are presented in Figures <ref type="figure">8 and 9</ref>, respectively.</p><p>It is possible to notice that the proposed work yielded competitive results. The best result, in terms of overall accuracy, was 90.3% achieved by DLR 9 <ref type="bibr" target="#b21">[22]</ref> and GSN3 <ref type="bibr" target="#b26">[27]</ref>. Our best result (UFMG 4) appears in fifth place by yielding 89.4% of overall accuracy, outperforming several methods, such as ADL 3 <ref type="bibr" target="#b48">[49]</ref> and RIT L8 <ref type="bibr" target="#b49">[50]</ref>, that also tried to aggregate multi-context information. However, as can be seen in Table <ref type="table" target="#tab_10">XI</ref> and Figure <ref type="figure" target="#fig_7">10a</ref>, while the other approaches have a larger number of trainable parameters, our network has only 2 millions, which makes it less pruned to overfitting and, consequently, easier to train, showing that the proposed method really helps in extracting all feasible information of the data even if using limited architectures (in terms of parameters). In fact, the number of parameters of the network is so relevant that authors of DLR 9 submission <ref type="bibr" target="#b21">[22]</ref>, one of the best results but with a higher number of parameters, do not recommend their proposed method for practical use because of the memory consumption and expensive training phase. Furthermore, the obtained results, that do not have any post-processing, are better than others, such as DST 2 <ref type="bibr" target="#b19">[20]</ref>, that employ CRF as  Aside from this, the proposed work (UFMG 5) achieved the best result (82.5% of F1 Score) in the car class, which is one of the most difficult classes (of this dataset) when compared to others (such as building) because of its composition (small objects) and its high intraclass variance (caused by a great variety of models and colors). This may be justified by the fact that the proposed network does not downsample the input image preserving important details for such classes composed of small objects. However, this submission ignores the clutter/background class, which could be considered as an advantage, making the comparison unfair. But, there are other works doing the same training protocol (i.e., ignoring the clutter/background class), such as INR <ref type="bibr" target="#b24">[25]</ref>. Yet such works have not achieved good accuracy in the car class as the  </p><note type="other">Original Image Ground-Truth Dilated6 DenseDilated6 Dilated6Pooling Dilated8Pooling</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Potsdam Dataset:</head><p>As for the Vaihingen dataset, official results for the Potsdam dataset are reported only by the challenge organization. For this dataset, we have four submissions, one for each network presented in Section IV trained with the 6 classes (which are represented, in the official results, as UFMG 1 to 4). In this dataset, there is no need to disregard the clutter/background class, since it has a sufficient amount of samples (4.96%). As before, all submissions related to the proposed work does not use any post-processing.</p><p>Table <ref type="table" target="#tab_11">XII</ref> summarizes some results reported by the organizers. Again, besides our results, the table also compiles the best results of each work with enough information to make a fair comparison. Visual examples of the proposed method, for the validation and test sets, are presented in Figures <ref type="figure" target="#fig_2">11  and 12</ref>, respectively.</p><p>The proposed work achieved competitive results, appearing in third place according to the overall accuracy. DST 5 <ref type="bibr" target="#b19">[20]</ref> and RIT L7 <ref type="bibr" target="#b49">[50]</ref> are the best result in terms of overall accuracy. However, they have a larger number of trainable parameters when compared to our proposed networks, as seen in Figure <ref type="figure" target="#fig_7">10b</ref>. This outcome corroborates with previous results, reaffirming obtained conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>In this paper, we propose a novel approach based on Convolutional Networks to perform semantic segmentation of remote sensing scenes. The method exploits networks composed uniquely of dilated convolution layers that do not downsample the input. Based on these networks and their no downsampling property, the proposed approach: (i) employs, in the training phase, patches of different sizes, allowing the networks to capture multi-context characteristics given the distinct context size, and (ii) updates a score for each of these patch sizes in order to select the best one during the testing phase.</p><p>We performed experiments on four high-resolution remote sensing datasets with very distinct properties: (i) Coffee dataset <ref type="bibr" target="#b40">[41]</ref>, composed multispectral high-resolution scenes of coffee crops and non-coffee areas, (ii) GRSS Data Fusion dataset <ref type="bibr" target="#b41">[42]</ref>, consisting of very high-resolution of visible spectrum images, (iii) Vaihingen dataset <ref type="bibr" target="#b42">[43]</ref>, composed of multispectral high-resolution images and normalized Digital Surface Model, and (iv) Potsdam dataset <ref type="bibr" target="#b44">[45]</ref>, also composed of multispectral high-resolution images and normalized Digital Surface Model.</p><p>Experimental results have showed that our method is effective and robust. It achieved state-of-the-art results in two datasets (Coffee and GRSS Data Fusion datasets) outperforming several techniques (such as Fully Convolutional <ref type="bibr" target="#b16">[17]</ref> and deconvolutional networks <ref type="bibr" target="#b17">[18]</ref>) that also exploit the multicontext paradigm. This shows the potential of the proposed method in learning multi-context information using patches of multiple sizes.</p><p>For the other datasets (Vaihingen and Potsdam), although the proposed technique did not achieve state-of-the-art, it yielded competitive results. In fact, our approach outperformed some relevant baselines that exploit post-processing techniques (although we did not employ any) and other multi-context strategies. Among all methods, the proposed one has the least number of parameters and is, therefore, less pruned to overfitting and, consequently, easier to train. At the same time, it produces one of the highest accuracies, which shows the effectiveness of the proposed technique in extracting all feasible information from the data using limited (in terms of parameters) architectures. Furthermore, the proposed technique achieved one of the best results for the car class, which is  one of the most difficult classes of these datasets because of its composition (small objects). This demonstrates the benefits of processing the input image without downsampling it, a process that preserves important details for classes that are composed of small objects. Aside from this, the proposed networks can be fine-tuned for any semantic segmentation application, since they do not depend on the patch size to process the data. This allows other applications to benefit from the patterns extracted by our models, a very important process mainly when working with small amounts of labeled data <ref type="bibr" target="#b14">[15]</ref>.</p><note type="other">Image nDSM Ground-Truth Dilated6 DenseDilated6 Dilated6 Pooling Dilated8 Pooling Dilated8 Pooling</note><p>The presented conclusions open opportunities towards a simplified use of deep learning methods for a better understanding of the Earth's surface, which is still needed for some remote sensing applications, such as agriculture or urban planning. In the future, we plan to better analyze the relation between the number of classes in the dataset and the number of parameters in the ConvNet. </p><note type="other">Image nDSM Ground-Truth Dilated6 DenseDilated6 Dilated6 Pooling Dilated8 Pooling Dilated8 Pooling</note></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 Fig. 2 :</head><label>32</label><figDesc>Fig. 2: Example of dilated convolutions. Dilation supports expansion of the receptive field without loss of resolution or coverage of the input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: Comparison between dilated and standard convolutions. Top (red) row presents the feature extraction process using a standard convolution over a downsampled image and then an upsample in order to recover the input resolution (a common procedure performed in ConvNets). Bottom (blue) row presents the feature extraction process using dilated convolution with rate r = 2 applied directly to the input (without downsample). The outcomes clearly show the benefits of dilated convolutions over standard ones.</figDesc><graphic url="image-7.png" coords="4,509.59,160.74,51.29,51.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ALGORITHM 1</head><label>1</label><figDesc>Process of dynamic training a Convolutional Networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 4: Dilated Convolutional Network architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: Convergence of Dilated6 network for all datasets. For the Coffee dataset, only the fold 1 is reported. For Vaihingen and Potsdam datasets, the validation set (created according<ref type="bibr" target="#b7">[8]</ref>) is reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Two images of the Coffee Dataset, their respective ground-truths and the prediction maps generated by the proposed algorithm. Legend -White: Coffee areas. Black: Non Coffee areas.</figDesc><graphic url="image-47.png" coords="13,66.50,138.75,77.10,77.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: The GRSS Data Fusion training and test images, their respective ground-truths and the prediction maps generated by the proposed algorithm. Legend -Black: unclassified. Light purple: road. Light green: trees. Red: red roof. Cyan: gray roof. Dark purple: concrete roof. Dark green: vegetation. Yellow: bare soil.</figDesc><graphic url="image-57.png" coords="13,65.84,432.01,114.84,133.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 :</head><label>10</label><figDesc>Fig.10: Comparison, in terms of overall accuracy and number of trainable parameters, between proposed and existing networks for Vaihingen and Potsdam datasets. Ideal architectures should be in the top left corner, with fewer parameters but higher accuracy. Since the x axis is logarithmic, a change of only 0.3 in this axis is equivalent to more than 1 million new parameters in the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :Fig. 9 :</head><label>89</label><figDesc>Fig. 8: Example predictions for the validation set of the Vaihingen dataset. Legend -White: impervious surfaces. Blue: buildings. Cyan: low vegetation. Green: trees. Yellow: cars. Red: clutter, background. Image nDSM Dilated6 DenseDilated6 Dilated6 Pooling</figDesc><graphic url="image-77.png" coords="15,79.66,262.89,64.25,64.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Fig. 11: Example predictions for the validation set of the Potsdam dataset. Legend -White: impervious surfaces. Blue: buildings. Cyan: low vegetation. Green: trees. Yellow: cars. Red: clutter, background. Image nDSM Dilated6 DenseDilated6 Dilated6 Pooling</figDesc><graphic url="image-107.png" coords="16,79.66,262.89,64.25,64.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Number of pixels per class for the GRSS Data Fusion dataset. coffee crops and non-coffee areas. The others are urban datasets which have the objective of mapping targets such as roads, buildings, and cars. The first one is the GRSS Data Fusion contest dataset (consisting of very high-resolution images), while the others are the Vaihingen and Potsdam datasets, provided in the framework of the 2D semantic labeling contest organized by the ISPRS Commission III 2 and composed of multispectral high-resolution images.</figDesc><table><row><cell></cell><cell>Train</cell><cell></cell><cell>Test</cell><cell></cell></row><row><cell>Classes</cell><cell>#Pixels</cell><cell>%</cell><cell>#Pixels</cell><cell>%</cell></row><row><cell>Road</cell><cell>112,457</cell><cell>19.83</cell><cell>808,490</cell><cell>55.77</cell></row><row><cell>Trees</cell><cell>27,700</cell><cell>4.89</cell><cell>100,528</cell><cell>6.93</cell></row><row><cell>Red roof</cell><cell>45,739</cell><cell>8.05</cell><cell>136,323</cell><cell>9.40</cell></row><row><cell>Grey roof</cell><cell>53,520</cell><cell>9.44</cell><cell>142,710</cell><cell>9.84</cell></row><row><cell>Concrete roof</cell><cell>97,821</cell><cell>17.25</cell><cell>109,423</cell><cell>7.55</cell></row><row><cell>Vegetation</cell><cell>185,242</cell><cell>32.65</cell><cell>102,948</cell><cell>7.10</cell></row><row><cell>Bare soil</cell><cell>44,738</cell><cell>7.89</cell><cell>49,212</cell><cell>3.41</cell></row><row><cell>Total</cell><cell cols="2">567,217 100.00</cell><cell cols="2">1,449,634 100.00</cell></row><row><cell cols="5">sensing datasets with very distinct properties. The first one</cell></row><row><cell cols="5">is an agricultural dataset composed of multispectral high-</cell></row><row><cell>resolution scenes of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>1) Coffee Dataset: This dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Number of pixels per class for ISPRS dataset, i.e., Vaihingen and Potsdam. It is composed by a total of 33 image tiles (with an average size of 2494 × 2064 pixels), that are densely classified into six possible labels: impervious surfaces, building, low vegetation, tree, car, clutter/background. Sixteen of these images have ground-truth available while the remaining ones, considered the test set, do not have available annotation, requiring submission of the predictions in order to be evaluated.</figDesc><table><row><cell></cell><cell cols="2">Vaihingen</cell><cell>Potsdam</cell><cell></cell></row><row><cell>Classes</cell><cell>#Pixels</cell><cell>%</cell><cell>#Pixels</cell><cell>%</cell></row><row><cell>Impervious Surfaces</cell><cell>21,815,349</cell><cell>27.94</cell><cell>245,930,445</cell><cell>28.46</cell></row><row><cell>Building</cell><cell>20,417,332</cell><cell>26.15</cell><cell>230,875,852</cell><cell>26.72</cell></row><row><cell>Low Vegetation</cell><cell>16,272,917</cell><cell>20.84</cell><cell>203,358,663</cell><cell>23.54</cell></row><row><cell>Tree</cell><cell>18,110,438</cell><cell>23.19</cell><cell>126,352,970</cell><cell>14.62</cell></row><row><cell>Car</cell><cell>945,687</cell><cell>1.21</cell><cell>14,597,667</cell><cell>1.69</cell></row><row><cell>Clutter/Background</cell><cell>526,083</cell><cell>0.67</cell><cell>42,884,403</cell><cell>4.96</cell></row><row><cell>Total</cell><cell cols="2">78,087,806 100.00</cell><cell cols="2">864,000,000 100.00</cell></row><row><cell>(ISPRS).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Hyperparameters employed in each dataset.</figDesc><table><row><cell>Datasets</cell><cell>Learning Rate</cell><cell>Weight Decay</cell><cell>Iterations</cell><cell>Exponential Decay (decay/steps)</cell></row><row><cell>Coffee Dataset</cell><cell>0.01</cell><cell>0.001</cell><cell>150,000</cell><cell>0.5/50,000</cell></row><row><cell>GRSS Data Fusion Dataset</cell><cell>0.01</cell><cell>0.005</cell><cell>200,000</cell><cell>0.5/50,000</cell></row><row><cell>Vaihingen Dataset</cell><cell>0.01</cell><cell>0.01</cell><cell>500,000</cell><cell>0.5/50,000</cell></row><row><cell>Potsdam Dataset</cell><cell>0.01</cell><cell>0.01</cell><cell>500,000</cell><cell>0.5/50,000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV :</head><label>IV</label><figDesc>Results over different distributions.</figDesc><table><row><cell></cell><cell>Overall Accuracy</cell><cell>Kappa</cell><cell>Average Accuracy</cell><cell>F1 Score</cell></row><row><cell>Uniform</cell><cell>86.13±2.39</cell><cell>69.39±3.48</cell><cell cols="2">84.81±1.65 84.58±1.90</cell></row><row><cell>Uniform Fixed</cell><cell cols="2">86.27±1.44 69.41±2.01</cell><cell cols="2">84.85±1.66 84.62±1.06</cell></row><row><cell>Multinomial</cell><cell cols="2">86.06±1.68 68.94±2.94</cell><cell cols="2">84.56±2.00 84.39±1.51</cell></row><row><cell cols="5">of this distribution over the proposed algorithm, experiments</cell></row><row><cell cols="5">have been conducted to determine the most appropriate distri-</cell></row><row><cell cols="5">bution. Towards this, we selected and compared three distinct</cell></row><row><cell cols="5">distributions. First is the uniform distribution over a range</cell></row><row><cell cols="5">of values, i.e., given two extreme points, all intermediate</cell></row><row><cell cols="5">values (extremes included) inside this range should have the</cell></row><row><cell cols="5">same probability of being selected. Second is the uniform</cell></row><row><cell cols="5">distribution but over selected values (and not a range). In this</cell></row><row><cell cols="5">case, referenced as uniform fixed, the probability distribution</cell></row><row><cell cols="5">is equally divided into the given values (the remaining in-</cell></row><row><cell cols="5">termediate points have no probability of being selected). The</cell></row><row><cell cols="5">last distribution evaluated is the multinomial. In this case,</cell></row><row><cell cols="5">ordinary values inside a range have the same probability but</cell></row><row><cell cols="5">several given points have twice the chance of being selected.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>Results over different score functions.</figDesc><table><row><cell></cell><cell>Overall Accuracy</cell><cell>Kappa</cell><cell>Average Accuracy</cell><cell>F1 Score</cell></row><row><cell>Accuracy</cell><cell>86.27±1.44</cell><cell>69.41±2.01</cell><cell cols="2">84.85±1.66 84.62±1.06</cell></row><row><cell>Loss</cell><cell>86.15±1.96</cell><cell>69.16±3.41</cell><cell cols="2">84.68±2.02 84.49±1.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII :</head><label>VII</label><figDesc>Results of the Dilated6 network trained using distinct convolution types.</figDesc><table><row><cell>Datasets</cell><cell>Convolution Type</cell><cell>Overall Accuracy</cell><cell>Average Accuracy</cell></row><row><cell>Coffee</cell><cell>Standard Dilated</cell><cell cols="2">84.13±1.28 82.97±0.48 86.27±1.44 84.85±1.66</cell></row><row><cell>GRSS</cell><cell>Standard Dilated</cell><cell>85.70 90.10</cell><cell>85.31 90.13</cell></row><row><cell>Vaihingen</cell><cell>Standard Dilated</cell><cell>86.13 88.66</cell><cell>69.65 71.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VIII :</head><label>VIII</label><figDesc>Comparison between the dilated network trained using the proposed and the traditional method.</figDesc><table><row><cell>Dataset</cell><cell>Training Process</cell><cell>Patch Size</cell><cell>Training Time (hours)</cell><cell>Average Accuracy</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell>35</cell><cell>83.33</cell></row><row><cell></cell><cell>Traditional</cell><cell>28 49</cell><cell>59 86</cell><cell>85.48 85.94</cell></row><row><cell>GRSS Data Fusion</cell><cell></cell><cell>70</cell><cell>160</cell><cell>86.93</cell></row><row><cell></cell><cell></cell><cell>7,14,21,</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Dynamic</cell><cell>28,35,42,49,</cell><cell>81</cell><cell>90.13</cell></row><row><cell></cell><cell></cell><cell>56,63,70</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>45</cell><cell>125</cell><cell>66.29</cell></row><row><cell></cell><cell></cell><cell>55</cell><cell>160</cell><cell>66.77</cell></row><row><cell>Vaihingen</cell><cell>Traditional</cell><cell>65 75</cell><cell>200 260</cell><cell>66.84 66.65</cell></row><row><cell></cell><cell></cell><cell>85</cell><cell>325</cell><cell>66.96</cell></row><row><cell></cell><cell>Dynamic</cell><cell>45,55,65,75,85</cell><cell>220</cell><cell>71.96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IX :</head><label>IX</label><figDesc>Results for the Coffee dataset.</figDesc><table><row><cell>Training Process</cell><cell>Network</cell><cell>Average Accuracy</cell><cell>Kappa</cell></row><row><cell></cell><cell>Pixelwise [41]</cell><cell cols="2">81.72±2.38 62.75±7.42</cell></row><row><cell></cell><cell>CCNN [5]</cell><cell cols="2">82.80±2.30 64.60±4.34</cell></row><row><cell>Traditional</cell><cell>FCN [17]</cell><cell cols="2">83.25±2.47 66.00±3.55</cell></row><row><cell></cell><cell>Deconvolution Network [18]</cell><cell cols="2">82.61±2.05 65.56±3.47</cell></row><row><cell></cell><cell cols="3">Dilated network (Dilated6Pooling) 82.52±1.14 66.14±2.27</cell></row><row><cell></cell><cell>Dilated6</cell><cell cols="2">84.79±1.66 69.41±2.01</cell></row><row><cell>Dynamic</cell><cell>DenseDilated6 Dilated6Pooling</cell><cell cols="2">85.88±2.34 71.51±2.74 85.77±1.74 72.27±1.38</cell></row><row><cell></cell><cell>Dilated8Pooling</cell><cell cols="2">86.67±1.39 73.78±1.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE X :</head><label>X</label><figDesc>Results for the GRSS Data Fusion dataset.</figDesc><table><row><cell>Training Process</cell><cell>Network</cell><cell>Overall Accuracy</cell><cell>Average Accuracy</cell><cell>Kappa Index</cell></row><row><cell></cell><cell>Pixelwise [41]</cell><cell>85.04</cell><cell>86.52</cell><cell>78.18</cell></row><row><cell></cell><cell>FCN [17]</cell><cell>83.27</cell><cell>87.45</cell><cell>76.10</cell></row><row><cell>Traditional</cell><cell>Deconvolution Network [18]</cell><cell>82.15</cell><cell>86.24</cell><cell>75.04</cell></row><row><cell></cell><cell>Dilated network (Dilated6Pooling)</cell><cell>83.96</cell><cell>83.83</cell><cell>76.12</cell></row><row><cell></cell><cell>Deep Contextual [46]</cell><cell>85.45</cell><cell>88.33</cell><cell>79.01</cell></row><row><cell></cell><cell>Dilated6</cell><cell>90.10</cell><cell>90.13</cell><cell>85.22</cell></row><row><cell>Dynamic</cell><cell>DenseDilated6 Dilated6Pooling</cell><cell>88.66 88.05</cell><cell>80.62 86.12</cell><cell>81.80 81.81</cell></row><row><cell></cell><cell>Dilated8Pooling</cell><cell>89.03</cell><cell>85.31</cell><cell>83.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE XI :</head><label>XI</label><figDesc>Official results for the Vaihingen dataset.</figDesc><table><row><cell>Method</cell><cell>#Parameters</cell><cell>Impervious Surface</cell><cell cols="2">F1 Score Building Low Vegetation</cell><cell>Tree</cell><cell>Car</cell><cell>Overall Accuracy</cell></row><row><cell>DLR 9 [22]</cell><cell>806 • 10 6</cell><cell>92.4</cell><cell>95.2</cell><cell>83.9</cell><cell cols="2">89.9 81.2</cell><cell>90.3</cell></row><row><cell>GSN3 [27]</cell><cell>44 • 10 6</cell><cell>92.3</cell><cell>95.2</cell><cell>84.1</cell><cell cols="2">90.0 79.3</cell><cell>90.3</cell></row><row><cell>ONE 7 [21]</cell><cell>28 • 10 6</cell><cell>91.0</cell><cell>94.5</cell><cell>84.4</cell><cell cols="2">89.9 77.8</cell><cell>89.8</cell></row><row><cell>INR [25]</cell><cell>4 • 10 6</cell><cell>91.1</cell><cell>94.7</cell><cell>83.4</cell><cell cols="2">89.3 71.2</cell><cell>89.5</cell></row><row><cell>UFMG 4</cell><cell>2 • 10 6</cell><cell>91.1</cell><cell>94.5</cell><cell>82.9</cell><cell>88.8</cell><cell>81.3</cell><cell>89.4</cell></row><row><cell>UFMG 5</cell><cell>2 • 10 6</cell><cell>91.0</cell><cell>94.6</cell><cell>82.7</cell><cell>88.9</cell><cell>82.5</cell><cell>89.3</cell></row><row><cell>UFMG 1</cell><cell>1.3 • 10 6</cell><cell>90.5</cell><cell>94.1</cell><cell>82.5</cell><cell cols="2">89.0 78.5</cell><cell>89.1</cell></row><row><cell>DST 2 [20]</cell><cell>3.5 • 10 6</cell><cell>90.5</cell><cell>93.7</cell><cell>83.4</cell><cell cols="2">89.2 72.6</cell><cell>89.1</cell></row><row><cell>UFMG 2</cell><cell>0.8 • 10 6</cell><cell>90.7</cell><cell>94.3</cell><cell>82.5</cell><cell cols="2">88.5 77.4</cell><cell>89.0</cell></row><row><cell>UFMG 3</cell><cell>1.3 • 10 6</cell><cell>90.6</cell><cell>93.4</cell><cell>82.4</cell><cell cols="2">88.5 79.8</cell><cell>88.8</cell></row><row><cell>ADL 3 [49]</cell><cell>0.5 • 10 6</cell><cell>89.5</cell><cell>93.2</cell><cell>82.3</cell><cell cols="2">88.2 63.3</cell><cell>88.0</cell></row><row><cell>RIT 2 [51]</cell><cell>138 • 10 6</cell><cell>90.0</cell><cell>92.6</cell><cell>81.4</cell><cell cols="2">88.4 61.1</cell><cell>88.0</cell></row><row><cell>RIT L8 [50]</cell><cell>134 • 10 6</cell><cell>89.6</cell><cell>92.2</cell><cell>81.6</cell><cell cols="2">88.6 76.0</cell><cell>87.8</cell></row><row><cell>UZ 1 [8]</cell><cell>2.5 • 10 6</cell><cell>89.2</cell><cell>92.5</cell><cell>81.6</cell><cell cols="2">86.9 57.3</cell><cell>87.3</cell></row><row><cell cols="8">proposed work. Furthermore, still considering the car class,</cell></row><row><cell cols="8">the second best result (81.3% of F1 Score) is also yielded</cell></row><row><cell cols="8">by our proposed work (UFMG 4), which employs all classes</cell></row><row><cell cols="8">during the training phase, which shows the effectiveness and</cell></row><row><cell cols="8">robustness of our work mainly for classes related to small</cell></row><row><cell>objects.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XII :</head><label>XII</label><figDesc>Official results for the Potsdam dataset.</figDesc><table><row><cell>Method</cell><cell>#Parameters</cell><cell>Impervious Surface</cell><cell cols="2">F1 Score Building Low Vegetation</cell><cell>Tree</cell><cell>Car</cell><cell>Overall Accuracy</cell></row><row><cell>DST 5 [20]</cell><cell>3.5 • 10 6</cell><cell>92.5</cell><cell>96.4</cell><cell>86.7</cell><cell>88.0</cell><cell>94.7</cell><cell>90.3</cell></row><row><cell>RIT L7 [50]</cell><cell>134 • 10 6</cell><cell>91.2</cell><cell>94.6</cell><cell>85.1</cell><cell>85.1</cell><cell>92.8</cell><cell>88.4</cell></row><row><cell>UFMG 4</cell><cell>2 • 10 6</cell><cell>90.8</cell><cell>95.6</cell><cell>84.4</cell><cell>84.3</cell><cell>92.4</cell><cell>87.9</cell></row><row><cell>UFMG 3</cell><cell>1.3 • 10 6</cell><cell>90.5</cell><cell>95.6</cell><cell>83.3</cell><cell>82.6</cell><cell>90.8</cell><cell>87.2</cell></row><row><cell>UFMG 1</cell><cell>1.3 • 10 6</cell><cell>90.1</cell><cell>95.6</cell><cell>83.7</cell><cell>82.4</cell><cell>91.3</cell><cell>87.0</cell></row><row><cell>KLab 2 [52]</cell><cell>44 • 10 6</cell><cell>89.7</cell><cell>92.7</cell><cell>83.7</cell><cell>84.0</cell><cell>92.1</cell><cell>86.7</cell></row><row><cell>UFMG 2</cell><cell>0.8 • 10 6</cell><cell>88.7</cell><cell>95.3</cell><cell>83.1</cell><cell>80.8</cell><cell>90.8</cell><cell>85.8</cell></row><row><cell>UZ 1 [8]</cell><cell>2.5 • 10 6</cell><cell>89.3</cell><cell>95.4</cell><cell>81.8</cell><cell>80.5</cell><cell>86.5</cell><cell>85.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In this work, multi-context (sometimes called multi-scale, according to deep learning recent literature) refers to spatial context difference and, therefore, any method that exploits (direct or indirectly) images with distinct scales is aggregating multi-context information.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">The code has been made publicly available at https://github.com/ keillernogueira/dynamic-rs-segmentation/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially financed by the Pró-Reitoria de Pesquisa da Universidade Federal de Minas Gerais, CNPq (grant 312167/2015-6), CAPES (grant 88881.131682/2016-01), and Fapemig (APQ-00449-17). The authors gratefully acknowledge the support of NVIDIA Corporation with the donation of the GeForce GTX TITAN X GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Remote sensing platforms and sensors: A survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jóźków</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="22" to="36" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Global land cover mapping using earth observation satellite data: Recent progresses and challenges</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yifang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS journal of photogrammetry and remote sensing</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Print</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A survey of semantic segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thoma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06541</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Remote sensing digital image analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Richards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coffee crop recognition using multi-scale convolutional neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberoamerican Congress on Pattern Recognition</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A cloud-integrated web platform for marine monitoring using gis and remote sensing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fustes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cantorna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dafonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Arcay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Iglesias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manteiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="155" to="160" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting convnet diversity for flooding identification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Fadel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Í</forename><forename type="middle">C</forename><surname>Dourado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Werneck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A V</forename><surname>Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Calumby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Torres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1446" to="1450" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dense semantic labeling of subdecimeter resolution images with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="881" to="893" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A review on image segmentation techniques with remote sensing perspective</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPRS Technical Commission VII Symposium -100 Years ISPRS</title>
				<imprint>
			<date type="published" when="2010-01">January 2010</date>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing: A comprehensive review and list of resources</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Magazine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="8" to="36" />
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparative study of global color and texture descriptors for web image retrieval</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Torres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="359" to="380" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Do deep features generalize from everyday objects to remote sensing and aerial scenes domains</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Computer Vision and Pattern Recognition Workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards better exploiting convolutional neural networks for remote sensing scene classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Penatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="539" to="556" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video google: a text retrieval approach to object matching in videos</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2003-10">October 2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for robust semantic pixel-wise labelling</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.07293</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02585</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semantic segmentation of earth observation data using multimodal and multi-scale deep networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lefèvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="180" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Classification with an edge: improving semantic image segmentation with boundary detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marmanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="158" to="172" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiscale classification of remote sensing images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-H</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Philipp-Foliguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Falao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3764" to="3775" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dual local-global contextual pathways for recognition in aerial imagery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05462</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-resolution semantic labeling with convolutional neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Maggiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Charpiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Alliez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7092" to="7103" />
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic labeling of aerial and satellite imagery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Janney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2868" to="2881" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gated convolutional neural network for semantic segmentation in high-resolution images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">446</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015-09">September 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Saliency-guided unsupervised feature learning for scene classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2175" to="2184" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scene classification via a gradient boosting random convolutional network framework</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1793" to="1802" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stacked convolutional denoising auto-encoders for feature representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1017" to="1027" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07386</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2017-06">June 2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A real-time algorithm for signal analysis with the help of the wavelet transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Holschneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kronland-Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tchamitchian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wavelets</title>
		<imprint>
			<biblScope unit="page" from="286" to="297" />
			<date type="published" when="1990">1990</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="437" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning to semantically segment high-resolution remote sensing images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dalla Mura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016-12">December 2016</date>
			<biblScope unit="page" from="3566" to="3571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Processing of multiresolution thermal hyperspectral and digital color data: Outcome of the 2014 ieee grss data fusion contest</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Coillie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gautama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pižurica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Philips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shimoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2984" to="2996" />
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<ptr target="http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-vaihingen.html" />
		<title level="m">International society for photogrammetry and remote sensing (isprs)</title>
				<imprint>
			<date type="published" when="2018-06-18">2018-06-18</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Use of the stair vision library within the isprs 2d semantic labeling benchmark (vaihingen)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gerke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>in ITC</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<ptr target="http://www2.isprs.org/commissions/comm3/wg4/2d-sem-label-potsdam.html" />
		<title level="m">International society for photogrammetry and remote sensing (isprs)</title>
				<imprint>
			<date type="published" when="2018-06-18">2018-06-18</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep contextual description of superpixels for aerial urban scenes classification</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Machado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium</title>
				<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="3027" to="3031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Assessing the accuracy of remotely sensed data: principles and practices</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Congalton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Green</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>software available from tensorflow.org. [Online</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Effective semantic pixel labelling with convolutional networks and conditional random fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sherrah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Janney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van-Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Computer Vision and Pattern Recognition Workshop</title>
				<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="36" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dense semantic labeling of very-high-resolution aerial imagery and lidar with fullyconvolutional neural networks and higher-order crfs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Piramanayagam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Saber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Computer Vision and Pattern Recognition Workshop</title>
				<imprint>
			<date type="published" when="2017-06">June 2017</date>
			<biblScope unit="page" from="1561" to="1570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Classification of remote sensed images using random forests and deep learning framework</title>
		<author>
			<persName><forename type="first">S</forename><surname>Piramanayagam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schwartzkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Saber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Signal Processing for Remote Sensing XXII</title>
				<imprint>
			<publisher>International Society for Optics and Photonics</publisher>
			<date type="published" when="2016-10">October 2016</date>
			<biblScope unit="volume">10004</biblScope>
			<biblScope unit="page">100040L</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Salvaggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="60" to="77" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
