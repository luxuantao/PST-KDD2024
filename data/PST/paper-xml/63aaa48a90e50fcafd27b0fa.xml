<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_VhqYgAq">
					<orgName type="full">Ministry of Science and Technology of the People&apos;s Republic of China</orgName>
				</funder>
				<funder ref="#_MygPBC3">
					<orgName type="full">Science and Technology Commission of Shanghai Municipality</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanhong</forename><surname>Wang</surname></persName>
							<email>yhwang18@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Brain-Inspired Circuits and Systems</orgName>
								<orgName type="department" key="dep2">MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
							<email>zhzhao18@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Brain-Inspired Circuits and Systems</orgName>
								<orgName type="department" key="dep2">MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiaosha</forename><surname>Zou</surname></persName>
							<email>qiaoshazou@fudan.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Brain-Inspired Circuits and Systems</orgName>
								<orgName type="department" key="dep2">MOE Frontiers Center for Brain Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TCAD.2022.3232070</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D EEP neural networks (DNNs) exhibit prominent performance in solving tasks in various domains, such as computer vision <ref type="bibr" target="#b0">[1]</ref>, natural language processing <ref type="bibr" target="#b1">[2]</ref>, and speech recognition <ref type="bibr" target="#b2">[3]</ref>. Recently, the deployment of DNN models on edge devices like mobile phones is emerging, which poses serious challenges to the on-chip computing capability. The domain-specific deep learning accelerator (DLA) is one of the solutions to augment the computing capability with relatively lower power consumption <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Meanwhile, the power constraint means limited hardware resources, among which, the limited memory capacity raises the most significant problem in the model deployment. Even under the circumstance of limited resources, appropriate mapping strategies that can fully utilize the on-chip hardware resources still have a large margin in improving the model execution latency <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>In addition to the constraints of hardware resources, the diversity of DNNs topologies and applications also proves the necessity of mapping. Lacking of automatic mapping tools, the optimal model execution on dedicated DLAs requires enormous manpower. Automatic mapping frameworks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> proposed previously can alleviate the problem to some extent, however, it is still hard to find the optimal strategy when facing continuously evolving and complex network topology. In this work, we propose an automatic mapping framework named AutoMap with an emphasis on a uniform network topology representation and data flow scheduling with limited memory resources.</p><p>In general, the execution flow of a DNN can be structured as a computational graph (CG), which contains nodes representing the operations and edges representing the data flow direction. However, the granularity of operations is not consistent in the software and hardware level as shown in Fig. <ref type="figure" target="#fig_1">1(a)</ref>. Specifically, prevalent deep learning frameworks, such as Pytorch <ref type="bibr" target="#b11">[12]</ref>, Tensorflow <ref type="bibr" target="#b12">[13]</ref>, and Keras <ref type="bibr" target="#b13">[14]</ref> pack several commonly used computations inside certain network structures into one single node in the CG (like Attention, LSTM, GRU, etc.), which can be called the high-level CG. In contrast, the execution of DNNs on DLAs normally performs on a more elaborated level, such as matrix-matrix multiplication, which is called the low-level CG. Some studies use the directed acyclic graphs (DAGs) <ref type="bibr" target="#b8">[9]</ref> as the CG representation. The temporal dependencies between recurrent nodes are hard to capture by DAG representation which is introduced in Section III. Therefore, we propose an innovative representation called extended directed weighted graph (EDWG) to handle the limitations mentioned above.</p><p>Many previous works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b14">[15]</ref> employ a layerwise mapping strategy that focus on the computation in one single layer without considering the data reuse between layers. This strategy causes excessive data movements as data may be swapped between layers, which harms the computation and power efficiency as shown in Fig. <ref type="figure" target="#fig_1">1(c</ref>). Meanwhile, the development of compact networks in applications on edge devices makes it possible to accommodate more layers on DLAs. Therefore, the groupwise mapping method is more efficient by grouping several layers together and reusing the intragroup features on the global buffer of DLA. With this method, we only need to load the initial input feature and store the final output feature from/to DDR. However, this method comes at cost of the complexity of handling data dependencies carefully. The comparison of these two methods is depicted in Fig. <ref type="figure" target="#fig_1">1(d</ref>) with red lines denoting the intergroup connections and black lines representing the intragroup connections.</p><formula xml:id="formula_0">(a) (b) (c) (d)<label>(e)</label></formula><p>As the requirement of the groupwise mapping method, CG Partition is one of the most important step responsible for layer clustering and computation order scheduling. After partitioning, the memory requirement in each group should match the on-chip memory capacity of DLA. Furthermore, the execution dependency between CG graph nodes should be taken into consideration during computation scheduling. Upon fulfilling the two requirements mentioned above, the intergroup connection [as shown in red lines in Fig. <ref type="figure" target="#fig_1">1(d)</ref>] should be as minimal as possible to reduce the runtime latency and power consumption. Previous work <ref type="bibr" target="#b8">[9]</ref> adopts Longest-Path-based method for clustering these nodes into several groups. The Longest-Path-based node clustering can meet the execution dependency requirement since it first divides the nodes into different levels according to their longest-path value from the root (input) node, then clusters them into several groups level by level. However, the intergroup connection may not be minimized. We then propose our two-step dynamic-programming-based partitioner (DPP) to achieve better performance as shown in our experiments.</p><p>As illustrated above, the memory allocation of DNN features is a significant challenge in groupwise mapping method. An optimal memory allocation strategy can reduce the unnecessary off-chip data communication given the limited on-chip memory resources, or minimize the on-chip memory requirement given the computation graph as shown in Fig. <ref type="figure" target="#fig_1">1</ref>(e). Previous work adopts a manual static memory allocation strategy <ref type="bibr" target="#b15">[16]</ref>, which causes excessive memory usage. In this work, we propose a dynamic memory allocator (DMA) to assign the on-chip buffer space for scheduled nodes in a stepby-step manner. During each step, once independent nodes are searched, the corresponding on-chip buffer space will be released in time. In this way, the on-chip buffer can be used effectively and the off-chip communication will be minimized.</p><p>Targeting at the resource-limited DLAs on edge devices, we propose an automatic mapping framework named AutoMap. By solving the problems that previous mapping strategies are facing, the proposed framework has the following contributions.</p><p>1) A new intermediate representation (IR) named EDWG is proposed. This proposed IR not only unifies the highlevel and low-level CG (namely, the CG in the software and hardware aspects), but also realizes the unification of spatial and temporal representations in neural networks via modified edge connections. 2) To cope with the evolving network topologies and save unnecessary off-chip data movements with optimal on-chip memory utilization, we adopt the groupwise mapping method. We further propose the dynamic programming partitioner (DPP) and DMA for better performance. 3) This framework is comprehensively evaluated with four different network models and results are compared with previous mapping strategies on various accelerator architectures. The latency analysis of our framework proves the efficiency of the proposed techniques under the power/hardware constraints. <ref type="foot" target="#foot_0">1</ref>The remainder of this article is organized as follows: Section II introduces the background of DNNs and mappers for accelerators in general. Before digging into details of our proposed framework, we introduce the fundamental concept of EDWG in Section III. Then, the overall flow of the AutoMap framework is illustrated in Section IV. The important techniques leveraged in AutoMap, namely, DPP and DMA, are described in Section V and Section VI, respectively. The thorough evaluation of the framework and technologies is described and analyzed in Section VII. The related works are listed in Section VIII and the whole work is summarized in Section IX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>The procedure of the mapping strategy is closely related to the DNN characteristics including network structures, parameter sizes and temporal dependencies. Therefore, we briefly induce the popular DNN structures first. Then, a typical PEarray-based DLA design is introduced as our fundamental assumption of hardware architecture. Finally, the general flow of an accelerator mapper is explained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Neural Network</head><p>DNNs emerge recently as one of the most powerful machine learning methods. Among them, convolutional neural networks (CNNs) are capable of extracting spatial information and have achieved performance that rivals or even surpasses humans in several compute vision tasks <ref type="bibr" target="#b16">[17]</ref>. Recurrent neural networks (RNNs) generally do well in extracting temporal relations and have been widely applied in time series tasks, such as speech recognition <ref type="bibr" target="#b17">[18]</ref>. To leverage the advantage of both topologies, a mixed structure is proposed in current DNNs. For example, in the image caption task, features extracted by CNNs from images are used as input for RNNs to generate image caption <ref type="bibr" target="#b18">[19]</ref>. Another example is the video classification task, where RNNs fuse the features extracted by CNNs for classification. These hybrid models combining CNNs and RNNs can utilize both the spatial and temporal characteristics, thus, have the capability to handle complex tasks, which is one of the future trends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning Accelerator</head><p>Processing element (PE)-array-based DLA is one of the most popular design styles in recent years <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>. In general, the PE-array-based DLA consists of two computing modules and three types of global buffers as illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. The two computing modules are the PE and the post-PE (PPE). Each PE includes at least one multiplier and one adder for the multiply-and-accumulate (MAC) operation. With dedicated dataflows as described in Fig. <ref type="figure" target="#fig_2">2</ref>, the PE array supports the calculation of a convolution (Conv) layer or a fully connected (FC) layer. The PPE performs the elementwise operations and nonlinear operations, such as activation functions of Bias, BN, Tanh, etc. Three corresponding global buffers are weight, feature, and accumulation buffers (ABs). The weight buffer (WB) is used to store parameters of DNNs. The feature buffer (FB) is responsible for receiving inputs of DNNs, caching the output of the previous layer as interlayer features, and transferring the computation results to off-chip devices (like a DDR in SoC). The AB saves the intermediate accumulation results from the PE array.</p><p>Notice that the connection between computing engines and on-chip buffers may vary between different DLA designs. Some DLAs directly connect their PPEs to the DDR, which extinguishes the possibility of feature reuse between layers. Other DLAs connect PPEs to the FB for efficient feature reuse. In this article, we focus on the latter one because of the large mapspace.</p><p>We list the major DLA instructions and corresponding parameters in Table <ref type="table" target="#tab_0">I</ref>. The DDR2FB, DDR2WB and FB2DDR are data movement instructions between DDR and on-chip buffers. To execute the instruction, we need to provide all the parameters. For example, DDR2FB performs moving feature from DDR to FB. It needs ddr_raddr which indicates DDR address for reading the feature, fb_waddr which indicates FB address for writing the feature and len indicates the feature length. Another two instructions Comp_CONV and Comp_FC are responsible for the computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Mapper</head><p>In the scope of this article, a Mapper is responsible for binding and scheduling the fine-grained computation of DNNs to the DLA hardware. It is analogous to the Compiler, which is in charge of translating the software program of applications to the execution code for general-purpose processors like CPU and GPU. The Mapper takes the software-defined DNN models and generates the DLA hardware-compatible mapping, with the goal of reducing execution latency and energy consumption. In general, there are four main steps as we summarized in groupwise mapping, namely, Model Parsing, CG Partition, subCG Scheduling and subCG Mapping. At the very beginning, Model Parsing is responsible to translate the DNN model described in high-level CG into low-level CG, given the operation support list of the dedicated DLA. Then, in CG Partition, the whole CG needs to be divided into several nonoverlapping parts called subCGs according to the on-chip buffer size. Inside each subCG, the optimal execution order considering the data dependencies should be determined individually in subCG SCheduling. Finally, the allocation and binding of storage and computation resources are performed in subCG Mapping. The detailed procedures and design considerations of each step are described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EDWG: UNIFIED CG REPRESENTATION</head><p>In this section, we introduce how our EDWG can be used as an unified representation of DNN CG. Then, we elaborate the Model Parser, which realizes the automatic transformation from a high-level EDWG to a low-level EDWG, given the specific hardware-supported operator set. At last, we introduce our Model Decomposer, a preprocessing for large nodes in the low-level EDWG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Spatial-Temporal Unification</head><p>The data dependencies in DNNs can be divided into two types, spatial dependency and temporal dependency. The data dependency among network layers in the same time step is called spatial dependency while the data dependency in the different time step is called temporal dependency. The former is the most prevalent dependency type in DNN models, while the latter typically appears in RNNs. We take a simple two-layer RNN as an example with the traditional DAG representation shown in Fig. <ref type="figure" target="#fig_3">3(a)</ref>. Typically, the input X of RNN is a Tensor with three dimensions, i.e., time steps, batch size, and input size <ref type="bibr" target="#b11">[12]</ref>. In order to represent the temporal dependency, a compromised method is to expand the DAG along time steps as shown in Fig. <ref type="figure" target="#fig_3">3(b</ref>). However, this is essentially a transformation from temporal dependency to spatial dependency, and both the nodes and edges in the representation are redundant. We propose an EDWG expression allowing cycles in the graph, and exploit the edge direction for spatial dependency and weights for temporal dependency. To be specific, the weight of edge w indicates the computation of the destination node relies on the output of the source node which is generated w time steps ago. As shown in Fig. <ref type="figure" target="#fig_3">3(c</ref>), RNN2 spatially relies on RNN1, thus, has a directed connection from RNN1 with weight equal to 0. RNN1 temporally relies on itself, thus, has a cyclic connection to itself with the weight equals -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Two-Level Unification</head><p>As indicated in Table <ref type="table" target="#tab_0">II</ref> in Fig. <ref type="figure" target="#fig_3">3</ref>, EDWG maintains two data structures, a node list denoted as nodes and a graph. Each computational node contains the parameters and shape information and is uniquely assigned an index in the EDWG. The graph contains all the spatial and temporal dependencies as shown in Fig. <ref type="figure" target="#fig_3">3(c</ref>). The high-level EDWG can be directly parsed from different platform formats, e.g., pth <ref type="bibr" target="#b11">[12]</ref>, h5 <ref type="bibr" target="#b13">[14]</ref>, pb <ref type="bibr" target="#b12">[13]</ref>. As illustrated above, the trained model from software platforms contains many encapsulated nodes. To get the corresponding low-level EDWG, each node in the high-level EDWG should be lowered into hardware operators. Only if all nodes are low-level types, the EDWG can be considered a low-level EDWG. We have listed the high-level nodes and low-level nodes which are currently supported in our AutoMap in Table <ref type="table" target="#tab_0">II</ref> in Fig. <ref type="figure" target="#fig_3">3</ref>. This table can be easily extended when new computation nodes are registered and the parsing rules for lowering the high-level nodes are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Parser</head><p>The open neural network exchange (ONNX) <ref type="bibr" target="#b19">[20]</ref> format is a standard model representation format in the software domain that aims to make the model transfer between different platforms easier. Comprehensive software packages are provided to convert the prevalent model formats into ONNX. In our mapper, we directly use ONNX as the input model format. Model Parser is responsible for converting the ONNX format into the high-level EDWG representation, and then transforming each node in the high-level EDWG into fine-grained operators to generate the low-level EDWG. For example, given the RNN node parsing rules, the Model Parser will convert the two-layer RNNs in Fig. <ref type="figure" target="#fig_3">3(c</ref>) into a more hardware-friendly low-level EDWG in Fig. <ref type="figure" target="#fig_3">3(e)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. EDWG Decomposer</head><p>The goal of the EDWG Decomposer is to decompose large nodes that can not fit in the WB of DLA. There are two implementations here, input decomposition (ID) and output decomposition (OD). Assume that the node W hx is a large node that needs to be decomposed, and the decomposition results of the two methods are in Fig. <ref type="figure" target="#fig_3">3</ref>(f). The ID method splits the large node along the input channel into several small nodes. Meanwhile, the input feature X should also be split accordingly. The outputs from the small nodes should be added to get the correct result. The OD method splits the large node along the output channel into several small nodes, and then the results of these small nodes need to be concatenated for final outputs.</p><formula xml:id="formula_1">(a) (b) (d) (c) (e)<label>(f)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OVERALL FLOW OF AUTOMAP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Setup and Notations</head><p>AutoMap aims to provide an end-to-end framework for automatically optimizing and mapping DNNs to the DLA. Representing the CG as an EDWG, our AutoMap has a hierarchical organization with many techniques, such as model parsing, node fusion, DP sorting, DP grouping, and DMA. We use Python to build the whole framework and realize techniques mentioned above. AutoMap takes a DNN model and the hardware description (WB size, FB size, AB size, and the PE array size.) as inputs, then generates the executable binary file consisting of DLA instructions (listed in Table <ref type="table" target="#tab_0">I</ref>) as well as the arranged data file for DDR.</p><p>As shown in Fig. <ref type="figure" target="#fig_4">4</ref>, the AutoMap flow is organized hierarchically into three levels with correspondence to the memory hierarchy in the DLA. In the first of Model-Level, an EDWG is partitioned into subEDWGs according to the size of WB. The instruction with parameters of ddr_raddr and ddr_waddr (in Table <ref type="table" target="#tab_0">I</ref>) is generated. The second is minimal executable unit (MEU)-level, where nodes are fused and the computation order and memory space are assigned for each fused node. The third is maximum computation unit (MCU)-level, working on the split tiles of each fused node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model-Level Flow</head><p>The Model-Level is the topmost level for handling the input files and generating the final configurations. The main procedure follows six steps as shown in  which is elaborated in Section V. The fourth step is the MEU Scheduler, where the partitioned Input_i and subEDWG_i will be combined into different MEUs. Then, these MEUs are sorted and assigned orders. The fifth is the DDR Allocator. We assume that when running on the SoC, all the initial model data and input data are prestored in the off-chip DDR. Besides, inter-MEU features are also transferred to the DDR for caching. The DDR address needs to be reasonably assigned to these data. In addition, the mismatch of bit width DDR and on-chip buffers requires data rearrangement. Consequently, the data needs to be rearranged to generate the DDR Binary file. After the MEU-Level delivering all the configurations, the Model Wrapper integrates them by adding some DDR-DLA data interaction configurations. Finally, the configuration file for the whole model is generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. MEU-Level Flow</head><p>As illustrated in Section II-B, PE Array and PPE are two main modules for computation. The post-processing nodes, such as BN, Bias, and Tanh can be processed in the PPE pipeline while computational nodes, such as Conv and FC are executed in the PE Array. Thus, the intermediate data between a computational node and a post-processing node does not need to be stored in the FB. In the MEU-Level, we have to find the execution group sequence which is consisted of fused node (shown as FNode in Fig. <ref type="figure" target="#fig_2">2</ref>) that can be pipelined. The first step in MEU-Level Node Fusion is responsible for this and will be introduced elaborately in the following. In the second step, FNode Sorter will sort the FNodes according to data dependencies in the subEDWG. Then, FB Allocator will assign the FB space for each fused node.</p><p>The concept of Node Fusion (also as operator fusion) was previously proposed to fuse multiple successive operators into a single one in order to reduce memory accesses, and thereby speed up the DNN inference and reduce the energy consumption <ref type="bibr" target="#b10">[11]</ref>. There are mainly two types of fusion strategies. The first strategy is to combine multiple post processing (PP) operators (elementwise operations, e.g., add) into one fused PP operator. The second one is to fuse the MAC operator with the subsequent elementwise operators. These ideas are initially designed for GPU acceleration and have achieved 1.2? to 2? speedup as shown in TVM <ref type="bibr" target="#b10">[11]</ref>.</p><p>In fact, the Node Fusion is also effective on DLAs. We leverage the combination of two strategies mentioned above. Each fused node follows the form of "MAC-PP-? ? ? -PP." For example, in a CNN model, three layers (convolution layer, batch normalization (BN) layer, and tanh activation function layer) connecting in sequence are fused to get the fused node "Conv-BN-Tanh," which is illustrated in Fig. <ref type="figure" target="#fig_6">5</ref>. The fused node is deployed as an intact operator on DLAs, thus, the intermediate data movement and execution time can be reduced. We evaluate the speedup caused by Node Fusion on the DLA. The results are shown as histogram in Fig. <ref type="figure" target="#fig_6">5</ref>, which indicates that Node Fusion can result in a 1.5? to 2.5? speedup.</p><p>In the MEU-level, the whole computation subgraph is assumed to be executed fully on chip without excessive offchip communication. Therefore, the FB Allocator is crucial to reduce DDR data transformation. Accordingly, we have proposed our DMA which is elaborated in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. MCU-Level Flow</head><p>MCU-Level is responsible for the tiling and execution of a fused node. Since the AB may not able to hold all the output of the fused node, tiling is necessary to make each tile suitable for AB. Each tile corresponds to a single instruction of MCU-Level in Table <ref type="table" target="#tab_0">I</ref>. Based on the assigned FB addresses from the MEU-Level, the instruction parameters like fb_raddr and fb_waddr should be increased with the tile's order. The accumulation result of each tile will move from the AB to the FB in a pipeline way to make space for the next tile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EDWG PARTITIONER</head><p>The EDWG Partitioner is a core step in the Model-Level which is responsible for splitting a large EDWG into subED-WGs. Besides meeting the constraints of on-chip memory, a EDWG Partitioner can perform better by considering hardware execution cost such as off-chip data movement, runtime latency, etc. Thus, we propose a DPP and realize the algorithm with three steps: DP Sorting, DP Partitioning and EDWG Reconstruction. Note that these techniques can also be used in other mapping levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DP Sorting</head><p>The DP-based sorting algorithm operates on the low-level EDWG to get the optimal computing order, which targets the effective utilization of the on-chip buffer. We illustrate the algorithm with an example in Fig. <ref type="figure" target="#fig_7">6</ref>(a). The example EDWG is shown on the left side, in which v i represents a node and the edge value represents the feature size. The algorithm is described on the right side to find the optimal sorted node set V. The iterative searching process begins with the final output node. If a node is not in V and all its children nodes are already in V, it is called a candidate node. In each iteration, all candidate nodes are selected. The evaluation is performed on each candidate with an associated cost function. Then, the node with the smallest cost will be inserted to the head of list V.</p><p>During each iteration, for each candidate, a subgraph is constructed with V ? {v i }. The cost is calculated by the memory consumption of all input features for the corresponding subgraph. For example, given v 7 ? V and the candidate of v 5 , the cost is calculated as 16 + 6 = 22. Similarly, the cost for candidate v 6 is 14 + 4 = 18. Thus, v 6 is now added to list V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DP Grouping</head><p>The DP-based Grouping is responsible for dividing the sorted node list V into disjoint groups due to the WB size limitation. As introduced in Section II-C and shown in Fig. <ref type="figure" target="#fig_2">2</ref>, different grouping strategies have enormous impacts on the execution latency and energy consumption as introducing different off-chip data movements. Thus, minimizing the data movements is the most important optimization objective. We implement this grouping algorithm in combination with the DP algorithm, which is shown as the pseudocode in Fig. <ref type="figure" target="#fig_7">6(b)</ref>.</p><p>The input of the entire algorithm is V, and the output is the segmented result. It is represented by P <ref type="bibr" target="#b0">[1]</ref> in Fig. <ref type="figure" target="#fig_7">6(b)</ref>. During the process, we maintain two lists P and MinCost (lines 1 and 2) of length |V| + 1, and fill and update the value from back to front. At nth iteration, P[n] records the optimal grouping result for node set v n-1 , . . . , v |V|-1 , while MinCost[n] is the minimum cost. In the first level loop, n is decremented from |V| to 1 (line 3). In the second-level loop, m is incremented from n to |V| (line 4). We compare the cost of {{v n , . . . , v m }}? P[m + 1] with the current MinCost[n], choose the grouping scheme with smaller cost, and update MinCost[n] and P[n] (lines 5-9). At last, we get the final result as P <ref type="bibr" target="#b0">[1]</ref>.</p><p>The input of the entire algorithm is V from DP sorting, and the output is the grouping result as represented by P <ref type="bibr" target="#b0">[1]</ref> in Fig. <ref type="figure" target="#fig_7">6(b</ref>). We maintain two lists of P and MinCost during the iterations. The empty list of P has the size of |V| + 1 and each element in P is a list of nonoverlapping node sets after grouping. For example, at the iteration step n, the element value of P[n] is updated, which contains the list of node set as</p><formula xml:id="formula_2">[{v n-1 , v n-2 }, ..{.., v |V|-3 , v |V|-2 , v |V|-1 }]. A list MinCost with</formula><p>zero values of length |V| + 1 is used to remember the minimum cost of each iteration. All the lists are updated in a backward fashion. The algorithm has two nested loops. In the outer loop, index n is decreased from |V| to 1 (line 3). Meanwhile, the index m is increased from n to |V| (line 4) in the inner loop. During each iteration, we compare the cost of {{v n , . . . , v m }} ? P[m + 1] with the current MinCost <ref type="bibr">[n]</ref>. Then, the grouping scheme with the smallest cost is selected and the corresponding value of MinCost[n] and P[n] (lines 5-9) are recorded. At last, we get the final result as P <ref type="bibr" target="#b0">[1]</ref>.</p><p>For the calculation of cost, we use the estimation of offchip data movements. Using f (S i ) to represent the data size of both input and output features of node set S i , we get</p><formula xml:id="formula_3">Cost(P[1]) = f (S 0 ) + f (S 1 ) + ? ? ? In addition, we have Cost(P[n]) = Cost(P[m + 1]) + f ({v n , . . . , v m + 1}).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. EDWG Reconstruction</head><p>In the former steps, all nodes in the original EDWG are assigned into different groups temporally ignoring their connections. In this step, we reorganize all nodes in each group following the EDWG construction rule and name it subEDWG (represented as EDWG_i in Fig. <ref type="figure" target="#fig_4">4</ref>). The reconstruction includes restoring the connection among nodes, and identifying input nodes and output nodes in each subEDWG. In addition, we also generate the data dependencies among all subEDWGs here to provide references for generating data movement instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Complexity Analysis</head><p>The DP Sorting and DP Grouping are two core steps in our DPP. Compared with the standard DP algorithm, the DPP reduces complexity which makes it possible for partitioning a large graph with thousands of nodes. The standard DP can finish the partition for a linear CG with cascaded N nodes in a O(N 2 ) time complexity. However, for nonlinear CGs, like CG that contains multi-output/multi-input nodes, the time complexity will be exponential. Our proposed DPP is a two-step partitioning strategy. In the first step, we sort the nodes through DP considering the off-chip data movement. The corresponding complexity is O(N * d ave ), where d ave is the average degree in a CG and is typically much more smaller than N. In the second step, we perform the standard DP partition on the sorted nodes. Thus, the complexity for our DPP is</p><formula xml:id="formula_4">O(N * d ave ) + O(N 2 ) = O(N 2 ).</formula><p>Compared with the Longestpath-based Sorting and DP Grouping method proposed by Zheng's work <ref type="bibr" target="#b8">[9]</ref>, our DPP greatly reduces the off-chip data movement. The comparison results are shown in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DYNAMIC MEMORY ALLOCATOR</head><p>There are several methods to reduce memory access overhead in different levels, such as maximizing the parallelism <ref type="bibr" target="#b20">[21]</ref>, efficient utilization of memory bandwidth <ref type="bibr" target="#b21">[22]</ref>, and increasing the on-chip data reuse, and so on. The memory structures used by DLAs vary from work to work. For example, Kang's work <ref type="bibr" target="#b20">[21]</ref> and Zheng's work <ref type="bibr" target="#b8">[9]</ref> assume the on-chip buffer design with multiple banks. Therefore, Zheng et al. <ref type="bibr" target="#b8">[9]</ref> performed the memory allocation in a larger granularity, that is, in the bank group. While in this work, we target at the space allocation in a single physical bank. As shown in Fig. <ref type="figure" target="#fig_8">7</ref>(b), a physical bank can contain several logical banks to work concurrently. With the divergence of data size in different DNN layers, the space allocation in one single physical bank is important with both area and efficiency consideration. Note that our work is orthogonal to the optimization of parallelism with multibanks <ref type="bibr" target="#b20">[21]</ref>.</p><p>In the groupwise mapping strategy, to reduce unnecessary off-chip data movement, the most complex and important part is handling feature reuse from previous layers. As described in Section II-B, FB acts as the key component during feature reuse and its space should be carefully allocated. In contrast, the WB, the AB, and even DDR can use a relatively simple allocation strategy as they have little margin for data reuse between layers.</p><p>We propose a DMA to reduce the DDR-DLA data moving considering the data locality. Previously, the stacking allocator is prevalent that simply stacks the newly incoming data whenever there is free space. However, in the edge device scenario, on-chip memory resource is scarce and not all data should stay on-chip. Specifically, the intermediate feature generated from previous layers could be dynamically cleared when no data dependencies exists. For example, when all the child nodes of v i are computed, the output feature of v i in the FB can be cleared. Therefore, a DMA that performs space allocation and release according to the nodes' dependency is adopted in this work.</p><p>The pseudocode of the dynamic memory allocation algorithm and one specific example is indicated in Fig. <ref type="figure" target="#fig_9">8</ref>. In our algorithm, a dynamic CG (denoted as dyn_EDWG) is maintained for checking the dependency. After each step, the edges between the current node and its parent nodes will be cut from the dynamic CG, causing some isolated nodes, whose outputs will be immediately released from the FB. The EDWG in this example shows an exact execution block in WaveNet <ref type="bibr" target="#b22">[23]</ref>. After sorting nodes with DP Sorting as introduced in Section V-A, the computation order is labeled on the EDWG with red circles, which also corresponds to the allocation step. Notice that Conv3 and Add1 are fused and, therefore, both numbered with 4 because of the "MAC-PP" pattern. In step 1, the feature data of Input is put into the FB. Note that after previous mapping stages, the feature size is guaranteed to be smaller than the FB capacity. In step 2, the output of Conv1 is placed immediately after the Input as it will be used by steps 3, 4, and 10. After step 2, the Input will no longer be referred to in the following steps. Then, the space of Input will be released, and the output of Conv2 can be, therefore, placed in the FB. In step 4, the Conv3 and Add1 are both processed, and the result of Add1 is put in the FB. At this time, the dependencies of Conv2 are all resolved and the corresponding space can be released. The process is iteratively executed until the end of EDWG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. EVALUATION</head><p>In this section, we evaluate the benefit of each proposed optimization technique and compare the mapping framework performance with previous work. We first introduce the experimental settings. Then, the performance evaluations with dedicated metrics of our proposed DPP and DMA are performed. Benefiting from the reduction of external memory access with proposed techniques, the mapping framework is proved to increase the utilization rate of the PE Array and achieve 1.4?-3.5? speedup on various DNNs. In addition, the experiment results show that our proposed framework can also be used to guide the design space exploration of accelerator architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>Software Implementation: Our AutoMap is implemented with Python3. The description of hardware and trained models in the ONNX format are taken as the input. The output contains two parts: 1) the generated instructions as an executable configuration file and 2) the rearranged network parameters as the binary file in DDR.</p><p>Hardware Baselines: We choose different PE Array designs from NVDLA <ref type="bibr" target="#b3">[4]</ref>, TPU <ref type="bibr" target="#b4">[5]</ref>, Sticker <ref type="bibr" target="#b5">[6]</ref> and systolic architecture <ref type="bibr" target="#b6">[7]</ref> as hardware backbones to perform evaluation. For a fair comparison, the scale of the PE array is normalized to 16*16. We establish a cycle-accurate simulator consisting of the behavior modeling of both accelerators and external memory. The analytical model of the DLA's timing behavior refers to Zheng's work <ref type="bibr" target="#b8">[9]</ref>.</p><p>Network Benchmarks: In order to evaluate the techniques on diverse networks, several kinds of basic structures like GRU, LSTM and CNN should be covered. In a consequence, we select four different models for verification, as LPCNet <ref type="bibr" target="#b23">[24]</ref>, DSPNet <ref type="bibr" target="#b24">[25]</ref>, WaveNet <ref type="bibr" target="#b22">[23]</ref>, and ODSRLSTM <ref type="bibr" target="#b25">[26]</ref>. Their </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DP-Based EDWG Partitioner</head><p>Baselines: As illustrated in Section V, our proposed DP EDWG Partitioner (DPP) is divided into third steps: 1) DP sorting; 2) DP grouping; and 3) EDWG reconstruction. We also implement the layerwise partitioner (LWP) and longestpath-based partitioner (LPP) <ref type="bibr" target="#b8">[9]</ref> as baselines for comparison.</p><p>Metrics: Our partitioner can reduce off-chip data movement, resulting in the reduction of runtime and power consumption of DLA. Intuitively, we take the off-chip data movement as the metric in our evaluation.</p><p>Fig. <ref type="figure" target="#fig_10">9</ref>(a)-(d) shows experimental results on four networks and it is seen that our DPP algorithm performs better in all the networks. Specifically, the DPP achieves 71.9%, 90.8%, 87.9%, and 82.2% off-chip data movement reduction over LWP for LPCNet, DSPNet, WaveNet, and ODSRLSTM, respectively. Besides, we also achieve the noble results over LPP that are 42.3%, 43.3%, 50.0%, and 10.1%. As introduced in Table <ref type="table" target="#tab_1">III</ref>, the basic structure and size of networks vary significantly. It indicates DPP has good compatibility.</p><p>In addition, the on-chip memory is a key factor of the area cost of DLA. Therefore, we also evaluate our DPP algorithm performance under various WB sizes. In Fig. <ref type="figure" target="#fig_10">9</ref>, when the WB size gets larger, the off-chip data movement gets smaller for both LPP and DPP. This is because large memory can accommodate more layers in each subEDWG. However, it is hard for the LWP algorithm to benefit from large memory because of the layerwise calculation mode. The results show that given the same amount of data movements, our DPP has  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Dynamic Memory Allocator</head><p>Baselines: As illustrated in Section VI, our proposed DMA is mainly used for FB. We also implement simple memory allocator (SMA) and ordered memory allocator (OMA) mechanisms. For SMA, the memory allocation steps include loading the input features from DDR, caching the intermediate results in the FB, and transferring the output feature to the DDR. For OMA, each computing node reads features directly from the FB, and the output features are directly put in the free space of FB in order. When the space is fully occupied, features with further references are transferred to the DDR for storage and others are discarded.</p><p>Metrics: We take the number of off-chip data movements in the MEU level as the evaluation metric. The excess off-chip data movement occurs only when there is no free space in the FB. For clarity, we assume the weight and initial feature of subEDWG are preloaded on chip, and only compare the excess off-chip data movement, which is shown as MEU off-chip movement in Fig. <ref type="figure" target="#fig_11">10</ref>.</p><p>In Fig. <ref type="figure" target="#fig_11">10</ref>(a)-(d), we compare the off-chip data movements of SMA, OMA and DMA under different FB sizes. The results show that for SMA, the off-chip data movement is stably high. However, both OMA and our DMA can effectively reduce data transfers with increasing performances when the FB gets larger. Apparently, when the FB is large enough, no off-chip transmission is needed. Our DMA can utilize the space more efficiently, which is manifested in the fact that under the same capacity, the DMA method requires fewer off-chip transfers. The results also indicate that during hardware designs, allocating a smaller FB is feasible with the DMA algorithm. For example, for LPCNet, FB is designed to be 64 kB to eliminate the data movement, but OMA requires 128 kB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Framework Speed Up Evaluation</head><p>GEMM on the PE Array: Aiming at different acceleration targets, the PE structures and connection styles vary in different designs, resulting in manifold dataflows. For example, Google's TPU <ref type="bibr" target="#b4">[5]</ref> belongs to the weight-stationary featuresystolic dataflow, of which weights are located in each PE and the input feature streams in cycle-by-cycle. Sticker <ref type="bibr" target="#b5">[6]</ref> follows the weight-share feature-share dataflow where the weight is shared by PEs in the same column and the input feature is shared with PEs in the same rows. NVDLA <ref type="bibr" target="#b3">[4]</ref> follows the weight-stationary feature-share dataflow, where the weight is located in each PE and the feature is shared among PEs in the same row. Systolic architecture <ref type="bibr" target="#b6">[7]</ref> follows weight-systolic feature-systolic dataflow, where both weights and the input feature stream in cycle-by-cycle.</p><p>Baselines: To evaluate the end-to-end acceleration of our DPP+DMA, we set up two baselines for comparison. The first is the strategy of LWP + SMA. The second is the combination of Depth-First-search Partitioner (LPP) + OMA.</p><p>Metrics: We use the end-to-end execution speedup after mapping networks to the DLA as metrics. In our evaluation, the end-to-end runtime includes timing consumption of PE Computation, PE Propagation, PPE Propagation, and Off-chip Data Movement. The runtime breakdown and the PE utilization are also analyzed in detail.</p><p>We conduct a comprehensive evaluation on four architectures with the same hardware scale. Specifically, the PE array is unified as 16*16, and the DDR line width is assumed to be 64 bits. The experimental results are shown in Fig. <ref type="figure" target="#fig_12">11</ref>, indicating that our proposed method can achieve 1.27?-3.45? speedup. We notice that the performance variation is small in terms of the architecture differences. While the  network topology has a great influence on the runtime speedup because of mapping space. The most complex network attains a noticeable speedup, which is shown by 2.99?-3.45? speedup for DSPNet. The LPCNet and ODSRLSTM have medium complexity, thus, they gain the moderate speedup with 1.36?-1.85?. Although WaveNet is the largest network, its structure is simple, so it obtains the smallest speedup with 1.27?.</p><p>Most of these speedups come from the reduction of offchip data movements. To better illustrate this observation, we perform a runtime breakdown of two examples. The first is mapping the DSPNet onto NVDLA, and the runtime analysis under three settings are shown in Fig. <ref type="figure" target="#fig_13">12(b)-(d)</ref>. When the LWP+SMA solution is adopted, the off-chip data movement occupies most of the runtime (53%), while the core PE computation only occupies 46%, which poses a severe bottleneck of the acceleration capability. The previous LPP+OMA and our DPP+DMA solutions can increase the time of PE computation to 58% and 62%, respectively, while the off-chip data movement only contributes 35%. On another network, DSPNet, our method directly increases the computation ratio from 20% to 64%. In addition, it is observed that the latency variation between architectures comes from the PE propagation variation, which is only a small proportion during runtime. This is the reason why all architectures have similar results in Fig. <ref type="figure" target="#fig_12">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Framework Energy Saving Evaluation</head><p>Baselines: The baselines are the same as Section VII-D. Metrics: We establish an energy model which contains DRAM access energy, SRAM access energy and 16-bit fixpoint MAC computing energy simulated from CACTI, Destiny, and TSMC 65-nm GP Process, respectively. Then, we evaluate the end-to-end energy consumption for the four networks' execution on DLA. For clarity, we normalize the energy consumption with regard to the LWP+SMA results.</p><p>We only show the results on NVDLA dataflow in since the DLA dataflows have little impact in this experiments. As shown in Fig. <ref type="figure" target="#fig_14">13</ref>, our DPP+DMA can achieve 47%-75% of energy saving compared to the basic LWP+SMA. When compared with the previous LPP+OMA, our DPP+DMA can also achieves about 20% more energy saving. As illustrated above, the energy saving benefits from the DRAM access reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. RELATED WORK</head><p>The emerging DLAs pose significant challenges on the mapping of DNN models to dedicated hardwares, especially, when considering the power-constraint edge devices. Previous works have proposed several mapping and optimization techniques to explore the large mapspace.</p><p>SMAUG <ref type="bibr" target="#b9">[10]</ref> provides a complete tool-chain to emulate the end-to-end model execution on accelerator hardwares. It includes a Python-based software interface, a C++-based operator optimizer, and an accelerator simulator backend. Compiling techniques, such as blocking/tiling/parallelism, are provided in the operator optimizer, which is in the operator level. For comparison, our work covers optimization not only on the operator level but also on the graph level and even subgraph level. Timeloop <ref type="bibr" target="#b26">[27]</ref>, as a similar work, only performs optimization on the mapping of every single layer in DNNs.</p><p>TVM <ref type="bibr" target="#b10">[11]</ref> provides both the graph-level and the operatorlevel optimization. It emphasizes optimizing DL workloads for general purpose CPU and GPU while ours focuses on DLAs.</p><p>From the graph-level, TVM uses operator fusion to reduce the CPU-GPU data movement, which is also leveraged by us as introduced in Section IV-C. From the operator-level, TVM uses Relay as the high-level representation and TVM IR as the low-level representation. In our work, we use EDWG to realize the unified expression. TVM also optimizes for GPU parallelism. Our work targets the PE-array-based DLA and leaves the parallelism task (like mapping networks to the chiplet containing several DLAs) as future work.</p><p>Zheng's <ref type="bibr" target="#b8">[9]</ref> work adopts the groupwise mapping and is the most similar work to ours. However, it varies largely from ours mainly in three aspects. First, they only target CNNs and use a DAG to express a CG. Our work considers the large variation of network structures, such as LSTM, GRU, etc. Compared to CNNs, these structures have timing dependencies, resulting in more constraints during mapping. Second, they propose a two-step partitioner. In the first step, they sort all the nodes by their depth (the length of the longest path from the network input node to it). As a result, the arbitrarily connected CG is transformed into a linear one. In the second step, the sorted nodes are partitioned in a fixed order. We use the DP sorting to sort the nodes and have achieved better results as shown in Section VII-B. Finally, their memory allocator divides the FB into multiple blocks and dynamically assigns them to each operator for storing the output feature, whose granularity is relatively coarse. We assign the address of FB in a finer granularity and, thus, memory space can be efficiently utilized.</p><p>Burrello et al. <ref type="bibr" target="#b27">[28]</ref> also provides an automatic end-toend deployment platform for DNNs. They target at the low-cost IoT MCUs while we target at the PE-Array-based domain-specific accelerators (DLAs). The execution speedup is realized by reducing the data transfers in both works. However, they focus on the tiling in one single layer, while we take more efforts in the optimization in the computation graph level. TensorFlow Lite Micro <ref type="bibr" target="#b28">[29]</ref> provides an opensource machine learning inference framework for running deep-learning models on embedded systems. They put efforts into lowering the resource requirements and minimizing the runtime without considering the dedicated hardware resource mapping and binding. MAESTRO <ref type="bibr" target="#b29">[30]</ref> acts as a simulation platform that estimates the performance of DLAs given the DNN model, hardware descriptions, and mapping strategies for early stage design space and map-space exploration. Instead of performing early stage estimation, our work can generate the final executable configuration files after mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>In this article, we described an automatic mapper for the end-to-end mapping a neural network to a PE-array-based DLA on the edge scenario. Our proposed CG expression named EDWG realizes both spatial-temporal unification and high-low CG unification. We also proposed DPP and DMA for model partition and memory address allocation. Both of them have been evaluated and proved to have a better performance in speeding up the NN execution on DLAs. In addition, our methods can also save energy consumption and reduce area cost due to the effective usage of the on-chip memory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>AutoMap: Automatic Mapping of Neural Networks to Deep Learning Accelerators for Edge Devices Yanhong Wang , Zihao Zhao, Xu Jin, Graduate Student Member, IEEE, Haotian Zheng, Maohua Nie , Qiaosha Zou , Member, IEEE, and C.-J. Richard Shi , Fellow, IEEE Abstract-Emerging deep neural networks (DNNs) have been emerging in applications (object detection, automatic speech recognition, etc.) deployed on edge devices. To improve the energy efficiency of edge devices, domain-specific deep learning accelerators (DLAs) are designed with limited on-chip resources. The manifold DLA designs and evolving DNN topologies bring challenges for applications mapping and scheduling on hardware resources. In this article, we propose an automatic DNN mapping framework named AutoMap, given the hardware backend information. First, a computational graph representation called extended directed weighted graph (EDWG) is proposed, which realizes unified expression for both spatial and temporal network interlayer connections. Second, an associated partitioner is implemented for splitting an EDWG into subEDWGs, which incorporates the on-chip memory constraint and facilitates weight data reuse on chip. Finally, a dynamic memory allocation strategy is utilized to alleviate the feature storing burden introduced by the multivarious network sizes and connections. Compared to the baseline mapping methods, experimental results show that our proposed automatic mapping framework can help to speedup the execution of several DNNs on state-of-the-art DLAs, ranging from 1.27? to 3.45?. The utilization of the PE array can increase from 20% to 64%. Index Terms-Deep learning accelerator (DLA), deep neural network (DNN), extended directed weighted graph (EDWG), mapper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Challenges exist in the mapping flow of DNNs to DLA. (a) The flow of mapping DNNs to DLA. (b) Challenge 1: Gaps between Software Language/Hardware Language. (c) Challenge 2: Excessive data movement in the traditional layer-wise mapping. (d) Challenge 3: Partition and Schedule for group-wise mapping. (e) Challenge 4: Schedule layers and Allocate the Global Buffer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Left: The illustration of four steps in mapping. DDR2FB means that the Input Node needs to be fetched from DDR to FB. DDR2WB means that the Computational (Comp.) Node's weight needs to be transferred from DDR to WB. FB2DDR means that the Output Node needs to be moved from FB to DDR. Right: The main structure of the DLA and SoC platform. The DDR Binary and Model Conf . are generated from the Mapper.</figDesc><graphic url="image-51.png" coords="3,371.75,53.21,174.50,150.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) 2-layer RNNs with DAG representation. X is a input node containing feature with three dimension, i.e., time steps, batch size, and input size [12]. (b) 2-layer RNNs with DAG representation and the computation is expanded in time steps. (c) Our EDWG representation. (d) Parsing rules for high-level node RNN. (e) Low-level EDWG after the Model Parser performed parsing on the high-level EDWG on (d). (e) Two methods for large node decomposition. B:Batch, T:time step, IS:input size, and OS:output size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>In the first and second steps, we use the Model Parser and Model Decomposer introduced in Section III to get the low-level EDWG. In the third step, the EDWG Partitioner divides the EDWG into subEDWGs subject to the WB size of the DLA with the proposed dynamic programming (DP)-based Partitioner,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Multilevel data flow of the proposed AutoMap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Example for (a) node fusion and (b) normalized speedup on DLAs. The PE array is set to 32?32 and DDR bandwidth is set to 64 bits. Both weights and feature are quantified to 16 bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Illustration of DP Sorting. (b) Pseudocode for DP Grouping algorithm.</figDesc><graphic url="image-58.png" coords="7,60.99,53.36,490.44,189.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) Multibank on-chip buffer design in some related works. (b) Data communication pattern between a physical bank and the PE array. (c) Each feature are distributed in all logical banks after memory allocation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Pseudocode of the allocator and an example for it. The number with red circle on the EDWG indicates the computation order. The FB of ten steps indicates the address changes. The DCG are abbreviation for Dynamic CG. The sequence indicates the changing process.</figDesc><graphic url="image-59.png" coords="9,60.99,53.24,490.44,156.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Offchip data movement for different networks with different partition algorithms. (a) LPCNet. (b) DSPNet. (c) WaveNet. (d) ODSRLSTM.</figDesc><graphic url="image-61.png" coords="10,60.99,192.05,490.44,101.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Offchip data movement for different networks with different allocators. (a) LPCNet. (b) DSPNet. (c) WaveNet. (d) ODSRLSTM. lower WB capacity requirement during hardware design, indicating a smaller chip area. For example, DSPNet's result in Fig. 9(b) indicates that our DPP has similar data movements with 64 kB in WB while the LPP requires 160 kB of WB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Runtime analysis on four DLA architectures for proposed methods compared with previous methods. (a) NVDLA. (b) TPU. (c) Sticker. (d) Systolic.</figDesc><graphic url="image-62.png" coords="11,60.99,53.20,490.44,111.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Runtime breakdown and comparison of PE utilization under different strategies. (a) Legend. (b) (LWP + SMA) 1 . (c) (LPP + OMA) 1 . (d) (DPP + DMA) 1 . (e) (LWP + SMA) 2 . (f) (LPP + OMA) 2 . (g) (DPP + DMA) 2 .</figDesc><graphic url="image-63.png" coords="11,60.99,195.69,490.44,81.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Framework energy saving evaluation.</figDesc><graphic url="image-64.png" coords="11,314.00,325.84,247.32,151.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I MAIN</head><label>I</label><figDesc>INSTRUCTIONS OF DLA</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III INFORMATION</head><label>III</label><figDesc>OF FOUR NETWORK STRUCTURES structures, parameters, and operation counts are summarized in TableIII.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The framework is public in https://github.com/DLA-explorers/AutoMap.Authorized licensed use limited to: Tsinghua University. Downloaded on January 01,2024 at 07:41:21 UTC from IEEE Xplore. Restrictions apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Authorized licensed use limited to: Tsinghua University. Downloaded on January 01,2024 at 07:41:21 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported in part by the <rs type="funder">Ministry of Science and Technology of the People's Republic of China</rs> under Grant <rs type="grantNumber">2018YFB2202604</rs>, and in part by the <rs type="funder">Science and Technology Commission of Shanghai Municipality</rs> under Grant <rs type="grantNumber">2018SHZDZX01</rs>. This article was recommended by <rs type="person">Associate Editor M. Shafique</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_VhqYgAq">
					<idno type="grant-number">2018YFB2202604</idno>
				</org>
				<org type="funding" xml:id="_MygPBC3">
					<idno type="grant-number">2018SHZDZX01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>His research interests include hardware and software co-design for low-power deep learning accelerator architecture and system-of-chip design for AI accelerator.</p><p>Xu Jin (Graduate Student Member, IEEE) received the B.Sc. degree in microelectronic science and engineering from the Huazhong University of Science and Technology, Wuhan, China, in 2020. He is currently pursuing the Ph.D. degree with Fudan University, Shanghai, China.</p><p>His research interests include hardware and software optimization for low-power and high-efficiency embedded systems, deep learning, and speech signal processing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Haotian</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object detection with deep learning: A review</title>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3212" to="3232" />
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pre-trained models for natural language processing: A survey</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. China Technol. Sci</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1872" to="1897" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speech recognition using deep neural networks: A systematic review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Nassif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shahin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Attili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Azzeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shaalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="19143" to="19165" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">NVDLA deep learning accelerator</title>
		<ptr target="http://nvdla.org" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>NVIDIA</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 44th Annu. Int. Symp. Comput. Archit</title>
		<meeting>44th Annu. Int. Symp. Comput. Archit</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">STICKER: An energy-efficient multi-sparsity compatible accelerator for convolutional neural networks in 65-nm CMOS</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSSC.2019.2946771</idno>
		<ptr target="https://doi.org/10.1109/JSSC.2019.2946771" />
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid State Circuits</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="465" to="477" />
			<date type="published" when="2020-02">Feb. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automated systolic array architecture synthesis for high throughput CNN inference on FPGAs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 54th Annu. Design Autom</title>
		<meeting>54th Annu. Design Autom</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Systolicarray deep-learning acceleration exploring pattern-indexed coordinateassisted sparsity for real-time on-device speech processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><forename type="middle">R</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Great Lakes Symp. VLSI, 2021</title>
		<meeting>Great Lakes Symp. VLSI, 2021</meeting>
		<imprint>
			<biblScope unit="page" from="353" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient scheduling of irregular network structures on CNN accelerators</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput.-Aided Design Integr. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3408" to="3419" />
			<date type="published" when="2020-11">Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SMAUG: End-to-end full-stack simulation infrastructure for deep learning workloads</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TVM: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th USENIX Symp. Oper. Syst. Design Implement</title>
		<meeting>13th USENIX Symp. Oper. Syst. Design Implement</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th USENIX Symp</title>
		<meeting>12th USENIX Symp</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An efficient hardware design for accelerating sparse CNNs with NAS-based models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput.-Aided Design Integr. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="597" to="613" />
			<date type="published" when="2022-03">Mar. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ShiDianNao: Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 42nd Annu. Int. Symp. Comput. Archit</title>
		<meeting>42nd Annu. Int. Symp. Comput. Archit</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="92" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SAIL: A deep-learning-based system for automatic gait assessment from TUG videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Human-Mach. Syst</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="110" to="122" />
			<date type="published" when="2022-02">Feb. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Performance evaluation of deep neural networks applied to speech recognition: RNN, LSTM and GRU</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shewalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Soft Comput. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="235" to="245" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An overview of image caption generation methods</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Intell. Neurosci</title>
		<imprint>
			<biblScope unit="page">3062706</biblScope>
			<date type="published" when="2020-01">Jan. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">ONNX github repository</title>
		<ptr target="https://github.com/onnx/onnx" />
		<imprint>
			<date type="published" when="2022-03-19">Mar. 19, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-bank on-chip memory management techniques for CNN accelerators</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1181" to="1193" />
			<date type="published" when="2022-05">May 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ROMANet: Fine-grained reuse-driven off-chip memory access management and data organization for deep neural network accelerators</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V W</forename><surname>Putra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hanif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shafique</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Very Large Scale Integr. (VLSI) Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="702" to="715" />
			<date type="published" when="2021-04">Apr. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">WaveNet: A generative model for raw audio</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">LPCNet: Improving neural speech synthesis through linear prediction</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Skoglund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust., Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5891" to="5895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A hybrid DSP/deep learning approach to real-time full-band speech enhancement</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Valin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 20th Int. Workshop Multimedia Signal Process. (MMSP)</title>
		<meeting>IEEE 20th Int. Workshop Multimedia Signal ess. (MMSP)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An 8.93 TOPS/W LSTM recurrent neural network accelerator featuring hierarchical coarse-grain sparsity for on-device speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kadetotad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Berisha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1877" to="1887" />
			<date type="published" when="2020-07">Jul. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Timeloop: A systematic approach to DNN accelerator evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp</title>
		<meeting>IEEE Int. Symp</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="304" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DORY: Automatic end-to-end deployment of real-world DNNs on low-cost IoT MCUs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Burrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garofalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bruschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tagliavini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Conti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1253" to="1268" />
			<date type="published" when="2021-08">Aug. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">TensorFlow lite micro: Embedded machine learning on TinyML systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08678</idno>
		<imprint>
			<date type="published" when="2020-10">Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MAESTRO: A data-centric approach to understand reuse, performance, and hardware cost of DNN mappings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chatarasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2020-06">May/Jun. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
