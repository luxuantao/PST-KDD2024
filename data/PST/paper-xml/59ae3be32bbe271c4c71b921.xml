<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A deep learning architecture for temporal sleep stage classification using multivariate and multimodal time series</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stanislas</forename><surname>Chambon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTCI</orgName>
								<orgName type="institution" key="instit2">Telecom ParisTech</orgName>
								<orgName type="institution" key="instit3">Université Paris-Saclay</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Research &amp; Algorithms Team</orgName>
								<orgName type="institution" key="instit2">Rythm inc</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathieu</forename><forename type="middle">N</forename><surname>Galtier</surname></persName>
							<email>mathieu@rythm.co</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Research &amp; Algorithms Team</orgName>
								<orgName type="institution" key="instit2">Rythm inc</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pierrick</forename><forename type="middle">J</forename><surname>Arnal</surname></persName>
							<email>pierrick@rythm.co</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Research &amp; Algorithms Team</orgName>
								<orgName type="institution" key="instit2">Rythm inc</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gilles</forename><surname>Wainrib</surname></persName>
							<email>gilles.wainrib@ens.fr</email>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">DATA Team</orgName>
								<orgName type="department" key="dep2">Département d&apos;Informatique</orgName>
								<orgName type="institution">École Normale Supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
							<email>dre.gramfort@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LTCI</orgName>
								<orgName type="institution" key="instit2">Telecom ParisTech</orgName>
								<orgName type="institution" key="instit3">Université Paris-Saclay</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">Université Paris-Saclay</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">CEA</orgName>
								<orgName type="institution" key="instit2">Université Paris-Saclay</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">french Association Nationale de la Recherche et de la Technologie (ANRT) under Grant</orgName>
								<address>
									<postCode>2015 / 1005</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="laboratory">Laboratoire Traitement et Communication de l&apos;Information (LTCI)</orgName>
								<orgName type="institution">Research &amp; Algorithms Team</orgName>
								<address>
									<addrLine>Tele-com ParisTech</addrLine>
									<settlement>Rythm, Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Université Paris-Saclay</orgName>
								<address>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Research &amp; Algorithms Team</orgName>
								<address>
									<settlement>Rythm, Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="department">Département d&apos;Informatique</orgName>
								<address>
									<settlement>Ecole Nor-male Supérieure, Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="department">Paris and Inria</orgName>
								<orgName type="laboratory">Alexandre Gramfort is with LTCI</orgName>
								<orgName type="institution">Université Paris-Saclay</orgName>
								<address>
									<addrLine>Télécom ParisTech</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution">Université Paris-Saclay</orgName>
								<address>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A deep learning architecture for temporal sleep stage classification using multivariate and multimodal time series</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7F3F805E7D9547BB7E1E57CDDE7298F9</idno>
					<idno type="DOI">10.1109/TNSRE.2018.2813138</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2018.2813138, IEEE Transactions on Neural Systems and Rehabilitation Engineering UNDER REVIEW AS A JOURNAL PAPER AT IEEE TRANSACTION 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2018.2813138, IEEE Transactions on Neural Systems and Rehabilitation Engineering UNDER REVIEW AS A JOURNAL PAPER AT IEEE TRANSACTION 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sleep stage classification</term>
					<term>multivariate time series</term>
					<term>deep learning</term>
					<term>spatio-temporal data</term>
					<term>transfer learning</term>
					<term>EEG</term>
					<term>EOG</term>
					<term>EMG</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sleep stage classification constitutes an important preliminary exam in the diagnosis of sleep disorders. It is traditionally performed by a sleep expert who assigns to each 30 s of signal a sleep stage, based on the visual inspection of signals such as electroencephalograms (EEG), electrooculograms (EOG), electrocardiograms (ECG) and electromyograms (EMG).</p><p>We introduce here the first deep learning approach for sleep stage classification that learns end-to-end without computing spectrograms or extracting hand-crafted features, that exploits all multivariate and multimodal Polysomnography (PSG) signals (EEG, EMG and EOG), and that can exploit the temporal context of each 30 s window of data. For each modality the first layer learns linear spatial filters that exploit the array of sensors to increase the signal-to-noise ratio, and the last layer feeds the learnt representation to a softmax classifier. Our model is compared to alternative automatic approaches based on convolutional networks or decisions trees. Results obtained on 61 publicly available PSG records with up to 20 EEG channels demonstrate that our network architecture yields state-of-the-art performance. Our study reveals a number of insights on the spatio-temporal distribution of the signal of interest: a good trade-off for optimal classification performance measured with balanced accuracy is to use 6 EEG with 2 EOG (left and right) and 3 EMG chin channels. Also exploiting one minute of data before and after each data segment offers the strongest improvement when a limited number of channels is available. As sleep experts, our system exploits the multivariate and multimodal nature of PSG signals in order to deliver state-of-the-art classification performance with a small computational cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Sleep stage identification, a.k.a. sleep scoring or sleep stage classification, is of great interest to better understand sleep and its disorders. Indeed, the construction of an hypnogram, the sequence of sleep stages over a night, is often involved, as a preliminary exam, in the diagnosis of sleep disorders such as insomnia or sleep apnea <ref type="bibr" target="#b0">[1]</ref>. Traditionally, this exam is performed as follows. First a subject sleeps with a medical device which performs a polysomnography (PSG), i.e., it records electroencephalography (EEG) signals at different locations over the head, electrooculography (EOG) signals, electromyography (EMG) signals, and eventually more. Second, a human sleep expert looks at the different time series recorded over the night and assigns to each 30 s time segment a sleep stage following a reference nomenclature such as American Academy of Sleep Medicine (AASM) rules <ref type="bibr" target="#b1">[2]</ref> or Rechtschaffen and Kales (RK) rules <ref type="bibr" target="#b2">[3]</ref>. Regarding the AASM rules, 5 stages are identified: Wake (W), Rapid Eye Movements (REM), Non REM1 (N1), Non REM2 (N2) and Non REM3 (N3) also known as slow wave sleep or even deep sleep. They are characterized by distinct time and frequency patterns and they also differ in proportions over a night. For instance, transitory stages such as N1 are less frequent than REM or N2. In the case of AASM rules, the transitions between two different stages are also documented and the transition rules may modulate the final decision of a human scorer. Indeed, some transitions are prohibited or others are strengthened depending on the occurence of some events such as arousal, K-complex or spindles regarding the transition N1-N2 <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Although very precious information is collected thanks to this exam, sleep scoring is a tedious and time consuming task which is furthermore subject to the scorer subjectivity and variability <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>The use of automatic sleep scoring methods or at least an automatic assistance has been investigated for several years and has driven much interest. From a statistical machine learning perspective, the problem is an imbalanced multi-class prediction problem. State-of-the-art automatic approaches can be classified into two categories depending on whether the features used for classification are extracted using expert knowledge or if they are learnt from the raw signals. Methods of the first category rely on a priori knowledge about the signals and events that enables to design hand-crafted features (see <ref type="bibr" target="#b6">[7]</ref> for a very extensive list of references). Methods in the second category consist in learning appropriate feature representations from transformed data <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref> or directly from raw data with convolutional neural networks <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>. Recently, another method was proposed to perform sleep stage classification onto radio waves signals, with an adversarial deep neural network <ref type="bibr" target="#b13">[14]</ref>.</p><p>One of the main statistical learning challenges is the imbalanced nature of the classification task which has important practical implications for this application. Typically sleep stages such as N1 are rare compared to N2 stages. When learning a predictive algorithm with very imbalanced classes, what classically happens is that the resulting system tends to never predict the rarest classes. One way to address this issue is to reweight the model loss function so that the cost of making an error on a rare sample is larger <ref type="bibr" target="#b14">[15]</ref>. With an online training approach as used with neural networks, one way to achieve this is to employ balanced sampling, i.e. to feed the network with batches of data which contain as many data points from each class <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b12">[13]</ref>. This indeed prevents the predictive models to be biased towards the most frequent stages. Yet, such a strategy raises the question of the choice of the evaluation metric used. The standard Accuracy metric (Acc.) considers that any prediction mistake has the same cost. Imagine that N2 would represent 90 % of the data, predicting always N2 would lead to a 90 % accuracy, which is obviously bad. A natural way to better evaluate a model in the presence of imbalanced classes is to use the Balanced Accuracy (B. Acc.) metric. With this metric the cost of a mistake on a sample of type N2 is inversely proportional to the fraction of samples of type N2 in the data. By doing so, every sleep stage has the same impact on the final figure of merit <ref type="bibr" target="#b15">[16]</ref>.</p><p>Another statistical learning challenge concerns the way transition rules are handled. Indeed, as the transition rules may impact the final decision of a scorer, a predictive model might take them into account in order to increase its performance. Doing so is possible by feeding the final classifier with the features from the neighboring time segments <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b12">[13]</ref>. This is referred to as temporal sleep stage classification.</p><p>A number of public sleep datasets contain PSG records with several EEG channels, and additional modalities such as EOG or EMG channels <ref type="bibr" target="#b16">[17]</ref>. Although these modalities are used by human experts for sleep scoring, seldom are they considered by automatic systems <ref type="bibr" target="#b15">[16]</ref>. Focusing only on the EEG modality, it is natural to think that the multivariate nature of EEG data does carry precious information. This can be exploited at least to cope with electrode removal or bad channels, and thus improve the robustness of the prediction algorithm. However, this can also be exploited as a leverage to improve the predictive capacities of the algorithm. Indeed, the EEG community has designed a number of methods to increase the signal-to-noise ratio (SNR) of an effect of interest from a full array of sensors. Among these methods are so called linear spatial filters and include classical techniques such as PCA/ICA <ref type="bibr" target="#b17">[18]</ref>, Common Spatial Patterns for BCI applications <ref type="bibr" target="#b18">[19]</ref> or beamforming methods for source local-ization <ref type="bibr" target="#b19">[20]</ref>. Less classically and more recently various deep learning approaches have been proposed to learn from EEG data <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b23">[24]</ref> and some of these contributions use a first layer that boils down to a spatial filter <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b29">[30]</ref>. Note that using a deep neural network to learn a feature representation and classify sleep stages on data coming from multiple sensors has been recently investigated in parallel of our work <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Yet our study further investigates and quantifies how much using a spatial filtering step enhances the prediction performance.</p><p>This paper is organized as follows. First we introduce our end-to-end deep learning approach to perform temporal sleep stage classification using multivariate time series coming from multiple modalities (EEG, EOG, EMG). We furthermore detail how the temporal context of each segment of data can be exploited by our model. Then, we benchmark our approach on publicly available data and compare it to state-of-the-art sleep stage classification methods. Finally, we explore the dependencies of our approach regarding the spatial context, the temporal context and the amount of training data at hand. Notation: We denote by X ∈ R C×T a segment of multivariate time series with its label y ∈ Y which maps to the set {W, N 1, N 2, N 3, REM }. Here, X corresponds to a sample lasting 30 seconds and Y = y ∈ R 5 + :</p><p>5 i=1 y i = 1 corresponds to the probability simplex. Precisely, each label is encoded as a vector of R 5 with 4 coefficients equal to 0 and a single coefficient equal to 1 which indicates the sleep stage.</p><p>Here C refers to the number of channels and T to the number of time steps.</p><formula xml:id="formula_0">S k t = {X t-k , • • • , X t , • • • , X t+k }</formula><p>stands for an ordered sequence of 2k + 1 neighboring segments of signal. X k = (R C×T ) 2k+1 is the space of 2k + 1 neighboring segments of signal. Finally, stands for the categorical cross entropy loss function. Given a true label y ∈ Y and a predicted label p ∈ Y it is defined as: (y, p) = -5 i=1 y i log p i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MATERIAL AND METHODS</head><p>In this section, we present a deep learning architecture to perform temporal sleep stage classification from multivariate and multimodal time series. We first define formally the classification problem addressed here. Then we present the network architecture used to predict without temporal context (k = 0). Then we describe the time distributed multivariate network proposed to perform temporal sleep stage classification (k &gt; 0). Finally, we present and discuss the alternative state-of-the-art methods used for comparison in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Machine learning problem</head><p>In this paragraph, we formalize in mathematical terms the temporal classification task considered here. Let k be a nonnegative integer. Let f : X k -→ Y stand for a predictive model that belongs to a parametric set denoted F. Here f takes as input an ordered sequence of 2k + 1 neighboring segments of signal, and outputs a probability vector p ∈ Y. For simplicity the parameters of the network are not written. The machine learning problem tackled then reads:</p><formula xml:id="formula_1">f = arg min f ∈F E x,y∈X k ×Y [ (f (x), y)] .<label>(1)</label></formula><p>Equation ( <ref type="formula" target="#formula_1">1</ref>) implies that the parameters of the neural network f are optimized by minimizing the expected value of the categorical cross entropy between the output of this network f (x) and the true label y.</p><p>Whenever k &gt; 0 the neural network has access to the temporal context of the segment of signal to classify, it is the temporal sleep stage classification problem, and when k = 0 the problem boils down to the standard formulation of sleep stage classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multivariate Network Architecture</head><p>The deep network architecture we propose to perform sleep stage classification from multivariate time series without temporal context (k = 0) has three key features: linear spatial filtering to estimate so called virtual channels, convolutive layers to capture spectral features and separate pipelines for EEG/EOG and EMG respectively. This network constitutes a general feature extractor we denote by Z : R C×T → R D , where D is the size of the estimated feature space. Our network can handle various number of input channels and several modalities at the same time. The general architecture is represented in Fig. <ref type="figure" target="#fig_0">1</ref>. We now detail the different blocks of the network, which are summarized in Tab. I. The first layer of the network is a time-independent linear operation that outputs a set of virtual channels, each obtained by linear combination of the original input channels. It implements a spatial filtering driven by the classification task to perform <ref type="bibr" target="#b24">[25]</ref>- <ref type="bibr" target="#b29">[30]</ref>. In our experiments, the number of virtual channels was set to the number of input channels making the first layer a multiplication with a square matrix. This square matrix plays the same role as the unmixing matrix estimated by ICA algorithms. This step will be further discussed in the discussion. Note that this first layer based on spatial filters can be implemented with a 2D valid convolution with kernels of shape (C, 1), see layer 3 in Tab. I.</p><p>Following this linear operation, the dimensions are permuted, see layer 4 in Tab. I. Then two blocks of temporal convolution followed by non-linearity and max pooling are consecutively applied. The parameters have been set for signals sampled at 128 Hz. In this case the number of time steps is T = 128 × 30 = 3840. Each block first convolves its input signal with 8 estimated kernels of length 64 with stride 1 (∼ 0.5 s of record) before applying a rectified linear unit, a.k.a. ReLU non-linearity x → max(x, 0) <ref type="bibr" target="#b30">[31]</ref>. The outputs are then reduced along the time axis with a max pooling layer (size of 16 without overlap). The output of the two convolution blocks is finally passed through a dropout layer <ref type="bibr" target="#b31">[32]</ref> which randomly prevents updates of 25% of its output neurons at each gradient step.</p><p>As represented in Fig. <ref type="figure" target="#fig_0">1</ref>, we process jointly the EEG and EOG time series since these modalities are comparable in magnitudes and both measure similar signals, namely electric potential up to a few hundreds of microvolts on the surface of the scalp. The same idea is used by EEG practitioners when the EOG channels are kept in the ICA decomposition to better reject EOG artifacts <ref type="bibr" target="#b32">[33]</ref>. The EMG time series which have different statistical and spectral properties are processed in a parallel pipeline.</p><p>The resulting outputs are then concatenated to form the feature space of dimension D before being fed into a final layer with 5 neurons and a softmax non-linearity to obtain a probability vector which sums to one. This final layer is referred to as a softmax classifier <ref type="bibr" target="#b33">[34]</ref>. Let a ∈ R 5 be the pre-activation of the last layer. The output of the network is a vector p ∈ Y. p is obtained as: p i = exp(a i )/ 5 j=1 exp(a j ). .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Time Distributed Multivariate Network</head><p>In this paragraph, we describe the Time Distributed Multivariate Network we propose to perform temporal sleep stage classification (k &gt; 0). It builds on the Multivariate Network Architecture presented previously and distributes it in time to take into account the temporal context. Indeed a sample of class N2 is very likely to be close to another N2 sample, but also to an N1 or an N3 sample <ref type="bibr" target="#b1">[2]</ref>.</p><p>To take into account the statistical properties of the signals before and after the sample of interest, we propose to aggregate the different features extracted by Z on a number of time segments preceding or following the sample of interest. More formally, let</p><formula xml:id="formula_2">S k t = {X t-k , • • • , X t , • • • , X t+k } ∈ X k</formula><p>be a sequence of 2k + 1 neighboring samples (k samples in the past and k samples in the future). Distributing in time the features extractor consists in applying Z to each sample in S k t and aggregating the 2k + 1 outputs forming a vector of size D(2k + 1). Then, the obtained vector is fed into the final softmax classifier. This is summarized in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training</head><p>The minimization in (1) is done with an online procedure based on stochastic gradient descent using mini batches of data. Yet, to be able to learn to discriminate under-represented classes (typically W and N1 stages), and since we are interested in optimizing the balanced accuracy, we propose to balance the distribution of each class in minibatches of size 128. As we have 5 classes it means that during training, each batch has about 20% of samples of each class. The Adam optimizer <ref type="bibr" target="#b34">[35]</ref> is used for optimization with the following parameters α = 0.001 (learning rate), β 1 = 0.9, β 2 = 0.999 and = 10 -8 .</p><p>An early stopping callback on the validation loss with patience of 5 epochs was used to stop the training process when no improvements were detected. Weights were initialized with a normal distribution with mean µ = 0, and standard </p><formula xml:id="formula_3">y t X t k X t+k X t Fig. 2. Time distributed architecture to process a sequence of inputs S k t = {X t-k , • • • , Xt, • • • , X t+k } with k = 1.</formula><p>X k stands for the multivariate input data over 30 s that is fed into the feature extractor Z. Features are extracted from consecutive 30 s samples:</p><formula xml:id="formula_4">X t-k , • • • , Xt, • • • , X t+k . Then the obtained features are aggregated [z t-k , • • • , zt, • • • , z t+k ].</formula><p>The resulting aggregation of features is finally fed into a classifier to predict the label yt associated to the sample Xt.</p><p>deviation σ = 0.1. Those values were obtained empirically by monitoring the loss during training. The implementation was written in Keras <ref type="bibr" target="#b35">[36]</ref> with a Tensorflow backend <ref type="bibr" target="#b36">[37]</ref>.</p><p>The training of the time distributed network was done in two steps. First, we trained the multivariate network, especially its feature extractor part Z t without temporal context (k = 0). The trained model was then used to set the weights of the feature extractor distributed in time. Second, we freezed the weights of the feature extractor distributed in time and we trained the final softmax classifier with aggregated features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>In this section, we first introduce the dataset and the preprocessing steps used. Then, we present the different features extractors of the literature which we use in our benchmark. We then present the experiments which aim at (i) establishing a general benchmark of our feature extractor against state-of-the art approaches in univariate (single derivation) and bivariate (2 channels) contexts, (ii) studying the influence of the spatial context, (iii) evaluating the gain obtained by using the temporal context and (iv) evaluating the impact of the quantity of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data and preprocessing steps</head><p>Data used in our experiments is the publicly available MASS dataset -session 3 <ref type="bibr" target="#b16">[17]</ref>. It corresponds to 62 night records, each one coming from a different subject. Because of preprocessing issues we removed the record 01-03-0034. Each record contains data from 20 EEG channels which were referenced with respect to the A2 electrode. We did not modify the referencing scheme, hence removed the A2 electrode from our study. Each record also includes signals from 2 EOG (horizontal left and right) and 3 EMG channels (chin channels) that we considered as additional modalities.</p><p>The time series from all the available sensors were first lowpass filtered with a 30 Hz cutoff frequency. Then they were downsampled to a sampling rate of 128 Hz. The downsampling step speeds up the computations for the neural networks, while keeping the information up to 64 Hz (Nyquist frequency). Downsampling and low / band pass filtering are commonly used preprocessing steps <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b15">[16]</ref>. The data extraction and the filtering steps were performed with the MNE software <ref type="bibr" target="#b37">[38]</ref>. The filter employed was a zero-phase finite impulse response (FIR) filter with transition bandwidth of approximately 7 Hz. Sleep stages were marked according to the AASM rules by a single sleep expert per record <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b16">[17]</ref>. When investigating the use of temporal context by feeding the predictors with sequences of consecutive samples S k , we used zero padding to complete the samples at the beginning and at the end of the night. This enables to feed the models with all the samples of a night record while keeping fixed the dimension of the input batches.</p><p>The time series fed into the different neural networks were additionnaly standardized. Indeed, for each channel, every 30 s sample is standardized individually such that it has zero mean and unit variance. For the specific task of sleep stage classification this is particularly relevant since records are carried out over nearly 8 hours. During such a long period the recording conditions vary such as skin humidity, body temperature, body movements or even worse electrode contact loss. Giving to each 30 s time series the same first and second order moments enables to cope with this likely covariate shift that may occur during a night record. This operation only rescales the frequency powers in every frequency band, without altering their relative amplitudes where the discriminant information for the considered sleep stage classification task lies (see Parseval's theorem). Note that this preprocessing step can be done online before feeding the network with a batch of data.</p><p>Cross-validation was used to have an unbiased estimate of the performance of our model on unseen records. To reduce variance in the reported scores, the data were randomly split 5 times between train, validation and testing set. The splits were performed with respect to records in order to guarantee that a record used in the training set was never used in the validation or the testing set. For each split, 41 records were included in the training set, 10 records in the validation set and 10 records in the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Related work and compared approaches</head><p>We now introduce the three state-of-the-art approaches that we used for comparison with our approach: a gradient boosting classifier <ref type="bibr" target="#b38">[39]</ref> trained on hand-crafted features and two convolutional networks trained on raw univariate time series following the approach of <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Features based approach</head><p>The Gradient Boosting model was learnt on hand-crafted features: time domain features and frequency domain features computed for each input sensor as described in <ref type="bibr" target="#b15">[16]</ref>. More precisely, we extracted from each channel the power and relative power in 5 bands: δ (0.5 -4.5 Hz), θ (4.5 -8.5 Hz), α (8.5 -11.5 Hz), σ (11.5 -15.5 Hz), β (15.5 -30 Hz), giving both 5 features. We furthermore extracted power ratios between these bands (which amount for 5 × 4/2 = 10 supplementary features) and spectral entropy features as well as statistics such as mean, variance, skewness, kurtosis, 75% quantile. This gives in the end a set of 26 features per channel.</p><p>The implementation used is from the XGBoost package <ref type="bibr" target="#b39">[40]</ref>, which internally employs decisions trees. This model is known for its high predictive performance, robustness to outliers, robustness to unbalanced classes and parallel search of the best split. Training was performed by minimizing also the categorical cross entropy. The training set was balanced using under sampling. The maximum number of trees in the model was set to 1000. An early stopping callback on the validation categorical cross entropy with patience equal to 10 was used to stop the training when no improvement was observed. Training never led to more than 1000 trees in a model.</p><p>The model has several hyper-parameters that need to be tuned to improve classification performances and cope with unbalanced classes. To find the best hyper-parameters for each experiment, we performed random searches with the hyperopt Python package <ref type="bibr" target="#b40">[41]</ref>. Concretely, we considered only the data from the training and validation subjects at hand. For each set of hyper-parameters, we trained and evaluated the classifier on data from 5 different splits of training and evaluation subjects (80% for training 20% for evaluation). The search was done with 50 sets of hyper-parameters and the set which achieved the best balanced accuracy averaged on the 5 splits was selected. The following parameters were tuned: learning rate in interval 10 -4 , 10 -1 , the minimum weight of a child tree in set {1, 2, • • • , 10}, the maximum depth of trees in {1, 2, • • • , 10}, the regularization parameter in [0, 1], the subsampling parameter in [0.5, 1], the sampling level of columns by tree in [0.5, 1].</p><p>2) Convolutional networks on raw univariate time series We reimplemented and benchmarked 2 end-to-end deep learning approaches. We detail each of them in the following paragraphs and explain how we used these methods.</p><p>a) Tsinalis et al. 2016: The approach by Tsinalis et al. 2016 <ref type="bibr" target="#b10">[11]</ref> is a deep convolution network that processes univariate time series (a single EEG signal). It was reimplemented according to the paper details. The approach originally takes into account the temporal context, by feeding the network with 150 s of signals, i.e. the sample to classify plus the 2 previous and 2 following samples. When used without temporal context in the experiments, the network is fed with 30 s samples.</p><p>Training was performed by minimizing the categorical cross entropy, and a similar balanced sampling strategy with Adam optimizer was used. An additional 2 regularization set to 0.01 was applied onto the convolution filters <ref type="bibr" target="#b10">[11]</ref>. The code was written in Keras <ref type="bibr" target="#b35">[36]</ref> with a Tensorflow backend <ref type="bibr" target="#b36">[37]</ref>.</p><p>b) Supratak et al. 2017: The approach by Supratak et al. 2017 <ref type="bibr" target="#b11">[12]</ref> is also an end-to-end deep convolutional network which contains two blocks: a feature extractor that processes the frequency content of the signal and a recurrent neural network that processes a sequence of consecutive 30 s samples of signal. The feature extractor processes low frequency information and high frequency information into two distinct convolutional sub-neural networks before merging the feature representations. The resulting tensor is then fed into a softmax classifier. This block is trained with balanced sampling. Then the feature extractor is linked to a recurrent neural network composed of 2 bi-LSTM layers. The whole architecture is fed with sequences of 25 consecutive 30 s samples from the same record.</p><p>The first block was used for comparison in our experiment. Its training was performed by minimizing the categorical crossentropy, and a balanced sampling strategy with Adam optimizer was used. The code was written in Keras <ref type="bibr" target="#b35">[36]</ref> with a Tensorflow backend <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment 1: Comparison of feature extractors on the Fz / Cz channels</head><p>In this experiment, we perform a general benchmark of our feature extractor against hand-crafted features classified with Gradient Boosting, and the two network architectures just described <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. The purpose of this experiment is to benchmark different feature representations on a similar spatial context, Fz-Cz, without using the temporal context, and to emphasize the benefits of processing multivariate time series instead of a pre-computed derivation.</p><p>Only time series coming from the channels Fz and Cz are considered here. First, the four predictive models were fed with the time series or the features from the derivation Fz-Cz that was computed manually. Second, our approach was fed with the time series from the derivations Fz-A2 and Cz-A2, i.e., the original time series of the dataset with pre-computed references. This version of our approach is referred to as Proposed approach -multivariate. No temporal context was used for this experiment (k = 0).</p><p>Finally, the experiment was carried out using balanced sampling at training time. For Gradient Boosting, an under sampling strategy was used to balance the training and the validation sets.</p><p>The performance of the different algorithms is evaluated with general classification metrics: Accuracy, Balanced Accuracy, Cohen Kappa, F1 score. Furthermore, run time metrics were computed such as: the number of parameters, the total training time, the training time per pass over the train set (called epoch), the prediction time per record (nearly 1k samples). These metrics are reported in Fig. <ref type="figure" target="#fig_1">3</ref>. Finally per class metrics were used: F1, Precision, Sensitivity, Specificity along with confusion matrices (C.M.). The C.M. were obtained by (i) normalizing the C.M. evaluated per testing subject such that its rows sum up to 1, (ii) computing the average C.M. over all testing subjects. These metrics are reported in Fig. <ref type="figure">4</ref> Accuracy  Furthermore, the proposed feature extractor trained on the Fz-A2, Cz-A2 channels, i.e. that is fed with multivariate time series, significantly outperforms its univariate counterpart and the other feature extractors which receive univariate time series. Processing two channels instead of a single induces a limited increase in number of parameters, training and prediction run time.</p><p>Besides, in Fig. <ref type="figure">4</ref>., the univariate proposed method, trained on Fz-Cz, yields equal or higher diagonal coefficients in its confusion matrix than the other feature extractors for sleep stages W, N1, N3. Supratak et al. 2017 outperforms the different univariate approaches on N1 and N3.</p><p>Moreover, the multivariate proposed approach yields higher diagonal coefficients in its confusion matrix than its univariate counterpart and the other feature extractor, except for N1 where Supratak et al. 2017 exhibits the highest classification accuracy. The analysis of the other per-class metrics agree with these facts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiment 2: More sensors increase performance</head><p>In this experiment, we investigated the influence of the multivariate spatial context on the performance of our approach. We considered 7 different configurations of EEG sensors which varied both in the number of recording sensors from 2 to 20 as well as in their positions over the head. We report the classification results for each configuration in Fig. <ref type="figure" target="#fig_2">5</ref>. One observes that both Gradient Boosting and our approach benefit from the increased number of EEG sensors. However, the B. Acc. obtained with our approach does not improve once we have 6 well distributed channels. This is certainly due to the redundancy of the EEG channels, yet more channels could make on some data the model more robust to the presence of bad sensors. First, this demonstrates that it is worth adding more EEG sensors, but up to a certain point. Second, it shows that our approach exploits well the multivariate nature of signals to improve classification performances. Third, it shows that the channel agnostic features extractor, i.e. the use of the spatial projection and the features extractor is a good option to fully exploit the spatial distribution of the sensors.</p><p>Restricting the number of EEG channels to 6 and 20, we further investigated the influence of additionnal modalities (EOG, EMG). Classification results are provided in Fig. <ref type="figure" target="#fig_3">6</ref>.</p><p>Considering additional modalities also increases the classification performances of the considered classifiers. It gives them a significative boost of performance, especially when the EMG modality is considered. This means that both approaches W N1 N2 N3 REM f1 0.41 0.16 0.79 0.68 0.56 Gradient Boosting  successfully integrate the new features with the previous ones. This suggests that our feature extractor was sufficiently data agnostic and versatile to handle both modalities. Finally, it again stresses the importance of considering the spatial context, here the additionnal modalities, to improve classification performances.</p><p>Interestingly, the boost of performance is more important in the 6 channel setting rather than in the 20 channel setting. We further observe that both EEG configuration with EOG and EMG modalities reach the same performances. Thus, the use of additional modalities compensate the use of a larger spatial context in this situation. Practically speaking, to obtain the highest performances at a reduced computational cost, one shall consider few well located EEG sensors with additional modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Experiment 3: Temporal context boosts performance</head><p>In this experiment, we investigate the influence of the temporal context on the classification performances and demonstrate that considering the data from the neighboring samples increases classification performances especially if the spatial context is limited. We also report what is the impact of temporal context on confusion matrices, and also on the matrices of transition probabilities between sleep stages. The coefficient P ij of the transition matrix P ∈ R 5 is equal to the probability of going from a sleep stage i to a sleep stage j.</p><p>We considered the spatial configurations with 2 frontal EEG channels, 6 EEG channels, and 6 EEG channels plus 2 EOG and 3 EMG channels. We varied the size of the temporal input sequence S k from k = 0, i.e. without temporal context, up to k = 5. The classification results are reported in Fig. <ref type="figure" target="#fig_4">7</ref>.</p><p>We furthermore evaluated the spatial configuration with only 2 frontal EEG channels for which we report the average confusion matrices as well as the average transition matrices of the predicted hypnograms. We additionally included the transition matrix of the true hypnogram according to the labels given by the sleep expert. The matrices are presented in Fig <ref type="figure" target="#fig_5">8</ref>. We observe in Fig. <ref type="figure" target="#fig_4">7</ref> that considering the close temporal context induces a boost in classification performances whereas considering a too large temporal context induces a decrease in performance. The gain strongly depends on the spatial context taken into account. Indeed, our model trained on 2 frontal channels with -30/ + 30 s of context achieves similar  performances than with the 6 EEG channel montage without temporal context. On the other hand, when considering an extended spatial context, the gain due to the temporal context turns out to be limited, as the performances of our approach or Gradient Boosting with the 6 EEG channels + 2 EOG and 3 EMG channels suggest.</p><p>The finer analysis operated on the confusion matrices and transition matrices indicates a trade-off when integrating some temporal context: integrating the close temporal context brings benefits in the detection of some sleep stages specifically (N1, N2, REM) but a too large temporal context has a negative effect on the detection of W and N3 as emphasized by Fig. <ref type="figure" target="#fig_5">8</ref>.</p><p>Besides, the transition matrices of predictions compared to the true transition matrix in Fig. <ref type="figure" target="#fig_5">8</ref> indicate that processing a larger temporal context smooths the hypnogram.</p><p>corresponds to an increase of the diagonal coefficient in the transition matrices. As a consequence, the transition probabilities from stages W, N1, N2 and REM are improved but on the other hand, the transition probabilities from N3 (especially from N3 to N3) are negatively impacted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experiment 4: More training data boost performance</head><p>In this experiment, we investigated the influence of the quantity of data on the classification performances of our approach. To do this we considered the spatial configurations with 2 frontal EEG channels, 6 EEG channels, and 6 EEG channels plus 2 EOG and 3 EMG channels. Concretely, we varied the number of training records n in {3, 12, 22, 31, 41}. We considered the same number of records for validation and testing as previously, i.e. 10. We furthermore carried out the experiments over 5 random splits of training, validation and testing subjects. The classification results are reported in Fig. <ref type="figure" target="#fig_6">9</ref>. Every algorithm with any spatial context exhibits an increase in performance when there is more training data. Gradient Boosting is more resilient than the proposed approach to the little data situation especially with a large spatial context. On the other hand, our deep learning model exhibits stronger increase in performance as a function of the quantity of data.</p><p>Furthermore, it appears that having few training records but an extended spatial context delivers as good performances as with many training records and few channels. Said differently, a rich spatial context can compensate for the scarcity of training data. Indeed, the input configuration with 6 EEG channels plus 2 EOG and 3 EMG channels with only 12 training subjects (right sub-figure) reaches the same performance as the 2 EEG channels input configuration (left sub-figure) with 41 training subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Experiment 5: Opening the model box</head><p>In this experiment, we aimed at understanding what the deep neural network learns. More precisely, we want to understand how the predictor relates a specific frequency content to the different sleep stages. We did so by occluding almost the whole frequency content, except a specific frequency band and monitoring the classification performances of the network while predicting on the filtered data. Such an operation, referred to as occlusion sensitivity has been successfully used to better understand how deep neural networks classify images <ref type="bibr" target="#b41">[42]</ref>.</p><p>We occluded almost the whole frequency domain and just kept a specific frequency band: either δ (0.5-4.5 Hz), θ (4.5-8.5 Hz), α (8.5 -11.5 Hz), σ (11.5 -15.5 Hz) or β (15.5 -30 Hz). Each time, we took the neural network trained on the original signal, and made it predict on signals obtained after applying a band-pass filter with cutoff frequencies given by the considered frequency band. This means that for any filtered sample, the frequency content outside this frequency band was removed. We compared the predictions on the filtered signals with the original labels. The confusion matrices associated to the different band-pass filters are reported in Fig. <ref type="figure" target="#fig_9">10</ref>. 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.99 0.00 0.01 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 β (15.5, 30) Hz Using the network on filtered signals enables to reveal the relationship between a specific frequency content and the sleep stages predicted by the network. Indeed, when only the delta band is kept, the network assigns N2 or N3 to all the samples. This implies that the network associates a low frequency content to N2 and N3 stages where there are actually low frequency events such as slow oscillations or K-complex.</p><p>Similarly, we observe that when the network predicts on signals where only the alpha band is kept, the network predicts mostly W. This is in agreement with the rules human scorers follow. A similar approach could be performed with much finer frequency bands.</p><p>Thus, despite the black-box nature of the proposed approach, this occluding procedure allows to open the box and to reveal interesting insights about how the model relates a particular frequency content to the different sleep stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION</head><p>In this section, we discuss the architecture characteristics of our approach and put them in perspective with state-ofthe art methods. We furthermore discuss the use of temporal context to take into account transitions between sleep stage and discuss its use for applications. Finally, we discuss points about the training of the proposed architecture and how this one can meet a specific need.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Spatial filtering</head><p>The proposed architecture was designed to handle a multivariate input thanks to a spatial filtering step. This step is motivated by the fact that a linear combination of the input channels should enhance the information useful for the task, and so even more if the spatial filters are optimized via back propagation on the training data. Motivated by simplicity, we chose the number of virtual channels equal to the number of input channels. Yet, this constitutes a degree of freedom one may play with to increase the performances of the network as was explored in <ref type="bibr" target="#b25">[26]</ref>.</p><p>As a comparison, <ref type="bibr" target="#b8">[9]</ref> averages the input time series to obtain a single one which is then fed into a 1D convolutional network. This can be seen as a particular case of our spatial filtering step where the number of virtual channels is equal to 1 and where the unique spatial filter coefficients are fixed to 1/C, with C the number of input channels. On the contrary, <ref type="bibr" target="#b4">[5]</ref> proposed an approach that also takes as input a multivariate time series but does not perform a particular spatial processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature extractor architecture</head><p>The proposed feature extractor exhibits a simple and versatile 2 layer architecture. Considering fewer or more layers was explored but did not deliver any extra gain in performance. We furthermore opted to perform spatial and temporal convolutions strictly separately. By doing so we replaced possible 2D expensive convolutions by a 1D spatial convolution and a 1D temporal convolution. Such a low rank spatio-temporal convolution strategy turned out to be successful in our exper-Regarding the dimensions of the convolution filters and pooling regions, our approach was motivated by the ability of neural networks to learn a hierarchical representation of input data, extracting low level and small scale patterns in the shallow layers and more complex and large scale patterns in the deep layers. Our strategy is quite different from <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> which use large temporal convolution filters. Despite the use of smaller filters, Fig. <ref type="figure" target="#fig_1">3</ref> and Fig. <ref type="figure" target="#fig_9">10</ref> demonstrate that our architecture is able to discriminate stages with low frequency content, such as N 3, from stages with higher frequency content such as N 2 due to the presence of spindles, or even from W and N 1 with the presence of α (8 -12Hz) bursts. Besides, our proposed architecture turns out to be data agnostic and handles well both EEG, EOG and EMG signals as shown by the results of experiment 2, see Fig. <ref type="figure" target="#fig_2">5</ref> and<ref type="figure" target="#fig_3">Fig 6</ref>.</p><p>Yet it is to be noticed, that recent approaches use even smaller convolution filters, of size 2, 3, 5, or 7 <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>On the contrary they also use a larger number of features maps from 64 up to 512 <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The use of small filters in combination with a larger number of features maps is worth investigating and quantifying and might result in more signal agnostic neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Number of parameters</head><p>The complexity of the proposed network and its number of parameters are quite small thanks to specific architecture choices. The overall network does not exhibit more than ∼ 10 4 parameters when considering an extended spatial context, and not more than ∼ 10 5 parameters when considering both an extended spatial context and an extended temporal context. This is quite simple and compact compared to the recent approaches in <ref type="bibr" target="#b10">[11]</ref> which has up to ∼ 14.10 7 parameters and <ref type="bibr" target="#b11">[12]</ref> which exhibits ∼ 6.10 5 parameters for the feature extractor and 2.10 7 parameters for the sequence learning part using BiLSTM. This significant difference with <ref type="bibr" target="#b10">[11]</ref> is mainly due to our choice of using small convolution filters (64 time steps after low pass filtering and downsampling), large pooling regions (pooling over 16 time steps) according to the 128 Hz sampling frequency and removing the penultimate fully connected layers before the final softmax classifier. Such a strategy has already been successul in computer vision <ref type="bibr" target="#b42">[43]</ref> and EEG <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Classification metrics</head><p>The proposed approach yields equal (univariate) or higher (multivariate) classification metrics than the other benchmarked feature extractors while presenting a limited training run time per epoch or prediction time per night record (cf. Fig. <ref type="figure" target="#fig_1">3</ref>). The analysis of per class metrics shows that the proposed approach might not reach the highest performance on every stages (cf. Fig. <ref type="figure">4</ref>). Indeed, Supratak et al. 2017 outperforms on N1, and Gradient Boosting exhibits a similar accuracy in N3. However, the proposed approach performs globally well and appears to be quite robust in comparison to the other approaches.</p><p>The proposed approach is particularly good at detecting W (high sensitivity 0.85 and specificity close to 1). This characteristic might be particularly interesting for clinical applications where a diagnosis of fragmented sleep might rely on the detection of W.</p><p>In order to measure the relevance of our approach for different types of subjects, we monitored the balanced accuracy of a subject as a function of the sleep fragmentation index (total number of awakenings and sleep stage shifts divided by total sleep time) <ref type="bibr" target="#b43">[44]</ref>. The results (not shown) did not exhibit a particular correlation between this measure of sleep quality and the classification performances. This indicates that the proposed approach could be used for clinical purposes with patients whose sleep exhibit abnormal structures.</p><p>Unfortunately, the different classification performances cannot be compared with inter-scorer agreement on this dataset since the night records have only been annotated by a single expert. Yet, a 0.80 agreement has been reported between scorers <ref type="bibr" target="#b5">[6]</ref>. Furthermore, <ref type="bibr" target="#b4">[5]</ref> monitored the classification accuracy of their model as a function of the consensus from 1 to 6 scorers. The reported curve was linearly increasing from 0.76 accuracy for 1 scorer up to 0.87 accuracy for a 6 scorer consensus. We shall reproduce such an experiment with the proposed approach in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Temporal context and transitions</head><p>Our architecture allows naturally to learn from the temporal context as it only relies on the aggregation of temporal features and a softmax classifier. Such a choice, enabled us to measure the influence of the close temporal context and better understand its impact. It differs from the approaches proposed by <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> as our features extractor always receives 30 s of signals, and is therefore applied to a sequence of neighboring 30 s samples. On the contrary, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref> extended the feature extractor input window to s, respectively 120 s. In <ref type="bibr" target="#b11">[12]</ref>, a temporal context of 25 neighboring 30 s samples is processed.</p><p>Our experiment on temporal context highlights a tradeoff when integrating some temporal context: integrating some temporal context brings benefits in the detection of some sleep stages specifically (N1, N2, REM) but a too large temporal context has a negative effect on the detection of W and N3 stages as emphasized by Fig. <ref type="figure" target="#fig_5">8</ref>. This naturally translates to the balanced accuracy scores which exhibit a significative increase for small temporal context and no increase, or even a decrease, for large temporal context (cf. Fig. <ref type="figure" target="#fig_4">7</ref>). Looking at the transitions matrices, it appears that more temporal context smoothes the hypnograms which might be detrimental to the quality of the system. For these reasons, temporal context should be used, but its width must be cross-validated.</p><p>Besides, some subjects might exhibit abnormal sleep structures related to a sleep disorder <ref type="bibr" target="#b5">[6]</ref>. There is thus a tradeoff between boosting the classification performance by integrating as much context as possible and not over-fitting sleep transitions in order to not miss a sleep disorder related to a fragmented sleep. This is an additional argument in favor of cross-validating the temporal context width.</p><p>An extension of our approach, for example to capture complex stage transitions or long term dependencies would be to employ a recurrent network architecture. Along these lines recent approaches have proposed more complex strategies to integrate the temporal context with LSTM unit cells or Bi-LSTM unit cells <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Integrating our feature extractor with such recurrent networks remains to be done and should lead to further performance improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Influence of dataset</head><p>Figure <ref type="figure" target="#fig_6">9</ref> raises an important question: how much data is needed to establish a correct benchmark of predictive models for sleep stage classification? This is particularly interesting concerning the deep learning approaches. Indeed, the Gradient Boosting handles quite well the small data situation and does not exhibit a huge increase in performances with the increase of the number of training records. On the contrary our approach delivers particularly good performances if enough training data are available. Extrapolation of the learning curves (performance as a function of the number of training records) in Fig. <ref type="figure" target="#fig_6">9</ref> suggests that one could expect better performances if more data were accessible. This forces us to reconsider the way we compare predictive models when training dataset sizes differ between experiments since the quantity of training data plays the role of a hyper-parameter for some algorithms like ours. Some algorithms become indeed better when more data are available (see for example Fig. <ref type="figure" target="#fig_0">1</ref> in <ref type="bibr" target="#b45">[46]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Choice of sampling and metrics</head><p>Our approach was particularly motivated by the accurate detection of any sleep stage independently to its proportion. To achieve this goal, all approaches have been trained using balanced sampling and evaluated with balanced metrics (except for experiment 1 where more metrics have been used). We observed that the choice of sampling strategies employed during online learning impacts the evaluation metrics and conversely the choice of metrics should motivate the choice of sampling strategies. Indeed, balanced sampling should be used to optimize the balanced accuracy of the model. On the other hand, random sampling should be used to boost the accuracy. The use of balanced sampling has been reportedly used or commented in previous works <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>.</p><p>Nonetheless, for a specific clinical application, one may decide that errors on a minor stage, such as N 1, are not so dramatic and hence prefer to train the network with random batches of data. On the contrary, one might want to discriminate as accurately as possible N 1 stages from W or REM and therefore one should use balanced sampling, or over sampling of N1.</p><p>Sampling strategy and evaluation metrics is a degree of freedom one can play with to adapt the network for his own experimental or clinical purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this study we introduced a deep neural network to perform temporal sleep stage classification from multimodal and multivariate time series. The model pools information from different sensors thanks to a linear spatial filtering operation and builds a hierarchical features representation of PSG data thanks to temporal convolutions. It additionally pools information from different modalities processed with separate pipelines.</p><p>The proposed approach in this paper exhibits strong classification performances compared to the state-of-the-art with a little run time and computational cost. This makes the approach a potential good candidate for being used in a portable device and performing online sleep stage classification.</p><p>Our approach enables to quantify the use of multiple EEG channels and additional modalities such as EOG and EMG. Interestingly, it appears that a limited number of EEG channels (6 EEG: F3, F4, C3, C4, O1, O2) gives performances similar to 20 EEG channels. Furthermore, using EMG channels boosts the model performances.</p><p>The use of temporal context is analyzed and quantified and appears to give significant increase in performance when the spatial context is limited. It is to be noticed that the temporal context as explored in this paper might not be directly suitable for online prediction, but it is easily usable for offline prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Network general architecture: the network processes C EEG/EOG channels and C EMG channels through separate pipelines. For each modalitity, it performs spatial filtering and applies convolutions, non linear operations and max pooling (MP) over the time axis. The outputs of the different pipelines are finally concatenated to feed a softmax classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. General classification and run time metrics of several feature extractors benchmarked on the Fz-Cz derivation or Fz-A2, Cz-A2 channels.The proposed approach trained on Fz-A2, Cz-A2 channels obtained higher classification performance than the other feature extractor trained on the Fz-Cz derivation, included its univariate counted-part while having a very low number of parameters and run time at training and prediction time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Influence of channel selection on the classification performances: increasing the number of EEG sensors increases B. Acc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Influence of additional modalities on the classification performances: adding EOG and EMG induces a boost in performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Influence of temporal context: considering the close temporal context induces a boost in performance especially when the spatial context is limited. From left to right: spatial configuration with 2 frontal EEG channels, 6 EEG channels, 6 EEG channels plus 2 EOG and 3 EMG channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Influence of temporal context on the confusion matrices (top row) and the transition matrices (bottom row). Including more temporal context induces an increase of performance in the discrimination of stages N1, N2 and REM whereas it induces a slight decrease in the discrimination of W and N3 when the temporal context is too wide. Including more temporal context smooths the hypnogram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Influence of the number of training records: the more training records the better performances are.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>α ( 8</head><label>8</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Prediction on filtered data: confusion matrices associated to unfiltered and filtered signals from testing records.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DETAILED</head><label>I</label><figDesc>ARCHITECTURE FOR THE FEATURE EXTRACTOR FOR C EEG CHANNELS WITH TIME SERIES OF LENGTH T . THE SAME ARCHITECTURE IS EMPLOYED FOR C EMG CHANNELS. WHEN BOTH EEG / EOG AND EMG ARE CONSIDERED, THE OUTPUTS OF THE DROPOUT LAYERS ARE CONCATENATED AND FED INTO THE FINAL CLASSIFIER. THE NUMBER OF PARAMETERS OF THE FINAL DENSE LAYER BECOMES THUS EQUAL TO</figDesc><table><row><cell></cell><cell></cell><cell>Layer</cell><cell cols="2">Layer Type</cell><cell cols="3"># filters # params</cell><cell></cell><cell>size</cell><cell>stride</cell><cell>Output dimension</cell><cell>Activation</cell><cell>Mode</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(C, T)</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>Reshape</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(C, T, 1)</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell cols="2">Convolution 2D</cell><cell>C</cell><cell cols="2">C * C</cell><cell></cell><cell>(C, 1)</cell><cell>(1, 1)</cell><cell>(1, T, C)</cell><cell>Linear</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>Permute</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(C, T, 1)</cell></row><row><cell></cell><cell>Features</cell><cell>5</cell><cell cols="2">convolution 2D</cell><cell>8</cell><cell cols="2">8 * 64 + 8</cell><cell></cell><cell>(1, 64) (1, 1)</cell><cell>(C, T, 8)</cell><cell>Relu</cell><cell>same</cell></row><row><cell></cell><cell>Extractor</cell><cell>6</cell><cell cols="2">maxpooling 2D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(1, 16) (1, 16)</cell><cell>(C, T // 16, 8)</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell cols="2">convolution 2D</cell><cell>8</cell><cell cols="3">8 * 8 * 64 + 8</cell><cell>(1, 64) (1, 1)</cell><cell>(C, T // 16, 8)</cell><cell>Relu</cell><cell>same</cell></row><row><cell></cell><cell></cell><cell>8</cell><cell cols="2">maxpooling 2D</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(1, 16) (1, 16)</cell><cell>(C, T // 256, 8)</cell></row><row><cell></cell><cell></cell><cell>9</cell><cell>Flatten</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(C * (T // 256) * 8)</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell cols="2">Dropout (50%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(C * (T // 256) * 8)</cell></row><row><cell></cell><cell>Classifier</cell><cell>11</cell><cell>Dense</cell><cell></cell><cell></cell><cell cols="3">5 * (C * T // 256 * 8)</cell><cell>5</cell><cell>Softmax</cell></row><row><cell></cell><cell cols="2">Sequence of inputs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>F3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EEG Channels</cell><cell>F4 C3 C4 O1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>O2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>-60</cell><cell>-30</cell><cell>0</cell><cell></cell><cell>30</cell><cell>60</cell><cell>Time (s)</cell><cell>90</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Z</cell><cell>Z</cell><cell>Z</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>z t k</cell><cell>z t</cell><cell>z t+k</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Classifier</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>5 × ((C + C ) × (T // 256) × 8).</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic analysis of single-channel sleep EEG: validation in healthy individuals</title>
		<author>
			<persName><forename type="first">C</forename><surname>Berthomier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Drouot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herman-Stoïca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berthomier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bokar-Thire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mattout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-P. D'</forename><surname>Ortho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sleep</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1587" to="1595" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The AASM Manual for the Scoring of Sleep and Associated Events: Rules, Terminology and Technical Specification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Iber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ancoli-Israel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Quan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A manual of standardized terminology, techniques and scoring system for sleep stages of human subjects</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><surname>Hobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electroencephalography and Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">644</biblScope>
			<date type="published" when="1969-06">June 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName><forename type="first">O</forename><surname>Tsinalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Sleep Stage Scoring Using Time-Frequency Analysis and Stacked Sparse Autoencoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="1587" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The use of neural networks in the analysis of sleep stages and the diagnosis of narcolepsy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Stephansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ambati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E M</forename><surname>Iv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hogl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stefani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pizza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Plazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Antelmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Perrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Kuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Kushida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Peppard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B D</forename><surname>Jennum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sørensen</surname></persName>
		</author>
		<author>
			<persName><surname>Mignot</surname></persName>
		</author>
		<idno>abs/1710.02094</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Hout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the American Academy of sleep Medicine Inter-scorer Reliability Program: Sleep Stage Scoring</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="447" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><surname>Aboalayon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Faezipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Almuhammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moslehpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sleep Stage Classification Using EEG Signal Analysis: A Comprehensive Survey and New Investigation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">272</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for interpretable analysis of EEG sleep stage scoring</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vilamala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<idno>abs/1710.00633</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SLEEPNET: automated sleep staging system via deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goparaju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Westover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1707.08262</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Supratak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.06421v1</idno>
		<title level="m">Mixed Neural Network Approach for Temporal Sleep Stage Classification</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Tsinalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.01683</idno>
		<title level="m">Automatic Sleep Stage Scoring with Single-Channel EEG Using Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepsleepnet: a model for automatic sleep stage scoring based on raw single-channel eeg</title>
		<author>
			<persName><forename type="first">A</forename><surname>Supratak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A convolutional neural network for sleep stage scoring from raw single-channel eeg</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vercueil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Payen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">PrePrint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning sleep stages from radio signals: A conditional adversarial architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bianchi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">Aug 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research. International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009-09">Sept 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Machines and Sleeping Brains: Automatic Sleep Stage Classification using Decision-Tree Multi-Class Support Vector Machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lajnef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ruby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-E</forename><surname>Aguera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Eichenlaub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kachouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jerbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">250</biblScope>
			<biblScope unit="page" from="94" to="105" />
			<date type="published" when="2015-11">November. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Montreal Archive of Sleep Studies: an open-access resource for instrument benchmarking and exploratory research</title>
		<author>
			<persName><forename type="first">C</forename><surname>O'reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Sleep Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="628" to="635" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recipes for the Linear Analysis of EEG</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Spence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Gerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="326" to="341" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimizing spatial filters for robust EEG single-trial analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lemm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="56" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Localization of Brain Electrical Activity via Linearly Constrained Minimum Variance Spatial Filtering</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D V</forename><surname>Veen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Drongelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yuchtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="867" to="880" />
			<date type="published" when="1997-09">Sept 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classification of Patterns of EEG Synchronization for Seizure Prediction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kuzniecky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1927" to="1940" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling electroencephalography waveforms with semi-supervised deep belief nets: fast classification and anomaly measurement</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Wulsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Litt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neural Engineering</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36015</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">EEG Based Emotion Classification using Deep Belief Networks</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bashivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yeasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional Neural Network with embedded Fourier Transform for EEG Classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cecotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gräser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008-12">Dec 2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for P300 Detection with Application to Brain-Computer Interfaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cecotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gräser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="433" to="445" />
			<date type="published" when="2011-03">Mar. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using Convolutional Neural Networks to Recognize Rhythm Stimuli from Electroencephalography Recordings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Grahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1449" to="1457" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional Neural Network for Multi-Category Rapid Serial Visual Presentation BCI</title>
		<author>
			<persName><forename type="first">R</forename><surname>Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Geva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">146</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Stober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sternin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Grahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04306v4</idno>
		<title level="m">Deep Feature Learning for EEG Recordings</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">EEGNet: A Compact Convolutional Network for EEG-based Brain-Computer Interfaces</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Lawhern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Solon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Waytowich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Lance</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08024v2</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dropout : A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic Removal of Eye Movement and Blink Artifacts from EEG Data using Blind Component Separation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Joyce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Gorodnitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kutas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="325" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Learning</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Keras</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MNE software for processing MEG and EEG data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luessi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Engemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Strohmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brodbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Parkkonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hämäläinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="446" to="460" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Greedy function approximation: A gradient boosting machine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">September 6-12, 2014. 2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Striving for Simplicity: The All Convolutional Net</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An alternative measure of sleep fragmentation in clinical practice : the sleep fragmentation index</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haba-Rubio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ibanez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sforza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sleep Medicine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="577" to="581" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scaling to very very large corpora for natural language disambiguation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL &apos;01</title>
		<meeting>the 39th Annual Meeting on Association for Computational Linguistics, ACL &apos;01<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
