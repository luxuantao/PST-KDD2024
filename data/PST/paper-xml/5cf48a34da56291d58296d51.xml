<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Representation Learning for Attributed Multiplex Heterogeneous Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University ‡ DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University ‡ DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
							<email>zhangjianwei.zjw@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
							<email>yang.yhx@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="department">Hongxia Yang</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
							<email>jingren.zhou@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University ‡ DAMO Academy</orgName>
								<address>
									<settlement>Alibaba Group</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hongxia Yang</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>11 pages</addrLine>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Representation Learning for Attributed Multiplex Heterogeneous Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3292500.3330964</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Network embedding</term>
					<term>Multiplex network</term>
					<term>Heterogeneous network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Network embedding (or graph embedding) has been widely used in many real-world applications. However, existing methods mainly focus on networks with single-typed nodes/edges and cannot scale well to handle large networks. Many real-world networks consist of billions of nodes and edges of multiple types, and each node is associated with different attributes. In this paper, we formalize the problem of embedding learning for the Attributed Multiplex Heterogeneous Network and propose a unified framework to address this problem. The framework supports both transductive and inductive learning. We also give the theoretical analysis of the proposed framework, showing its connection with previous works and proving its better expressiveness. We conduct systematical evaluations for the proposed framework on four different genres of challenging datasets: Amazon, YouTube, Twitter, and Alibaba 1 . Experimental results demonstrate that with the learned embeddings from the proposed framework, we can achieve statistically significant improvements (e.g., 5.99-28.23% lift by F1 scores; p ≪ 0.01, t−test) over previous state-of-the-art methods for link prediction.</p><p>The framework has also been successfully deployed on the recommendation system of a worldwide leading e-commerce company, Alibaba Group. Results of the offline A/B tests on product recommendation further confirm the effectiveness and efficiency of the framework in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Mathematics of computing → Graph algorithms; • Computing methodologies → Learning latent representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Network embedding <ref type="bibr" target="#b3">[4]</ref>, or network representation learning, is a promising method to project nodes in a network to a lowdimensional continuous space while preserving network structure and inherent properties. It has attracted tremendous attention recently due to significant progress in downstream network learning tasks such as node classification <ref type="bibr" target="#b0">[1]</ref>, link prediction <ref type="bibr" target="#b38">[39]</ref>, and community detection <ref type="bibr" target="#b7">[8]</ref>. DeepWalk <ref type="bibr" target="#b26">[27]</ref>, LINE <ref type="bibr" target="#b34">[35]</ref>, and node2vec <ref type="bibr" target="#b9">[10]</ref> are pioneering works that introduce deep learning techniques into network analysis to learn node embeddings. NetMF <ref type="bibr" target="#b28">[29]</ref> gives a theoretical analysis of equivalence for the different network embedding algorithms, and later NetSMF <ref type="bibr" target="#b27">[28]</ref> gives a scalable solution via sparsification. Nevertheless, they were designed to handle only the homogeneous network with single-typed nodes and edges. More recently, PTE <ref type="bibr" target="#b33">[34]</ref>, metapath2vec <ref type="bibr" target="#b6">[7]</ref>, and HERec <ref type="bibr" target="#b30">[31]</ref> are proposed for heterogeneous networks. However, real-world network-structured applications, such as e-commerce, are much more complicated, comprising not only multi-typed nodes and/or edges but also a rich set of attributes. Due to its significant importance and challenging requirements, there have been tremendous attempts in the literature to investigate embedding learning for complex networks. Depending on the network topology (homogeneous or heterogeneous) and attributed property (with or without attributes), we categorize six different types of networks and summarize their relative comprehensive developments, respectively, in Table <ref type="table" target="#tab_0">1</ref>. These six categories include HOmogeneous N etwork (or HON), Attributed HOmogeneous N etwork (or AHON), HEterogeneous N etwork (or HEN), Attributed HEterogeneous N etwork (or AHEN), Multiplex HEterogeneous N etwork (or MHEN), and Attributed Multiplex HEterogeneous N etwork (or AMHEN). As can be seen, among them the AMHEN has been least studied.</p><p>In this paper, we focus on embedding learning for AMHENs, where different types of nodes might be linked with multiple different types of edges, and each node is associated with a set of different attributes. This is common in many online applications. For example, in the four datasets that we are working with, there are 20.3% (Twitter), 21.6% (YouTube), <ref type="bibr" target="#b14">15</ref>.1% (Amazon) and <ref type="bibr" target="#b15">16</ref>.3% (Alibaba) of the linked node pairs having more than one type of edges respectively. As an instance, in an e-commerce system, users may have several types of interactions with items, such as click, conversion, add-to-cart, add-to-preference. Figure <ref type="figure" target="#fig_0">1</ref> illustrates such an example. Obviously, "users" and "items" have intrinsically different properties and shall not be treated equally. Moreover, different user-item interactions imply different levels of interests and should be treated differently. Otherwise, the system cannot precisely capture the user's behavioral patterns and preferences and would be insufficient for practical use. Not merely because of the heterogeneity and multiplicity, in practice, dealing with AMHEN poses several unique challenges:</p><p>• Multiplex Edges. Each node pair may have multiple different types of relationships. It is important to be able to borrow strengths from different relationships and learn unified embeddings. • Partial Observations. The real networked data are usually partially observed. For example, a long-tailed customer may only present few interactions with some products. Most existing network embedding methods focus on the transductive settings, and cannot handle the long-tailed or cold-start problems. • Scalability. Real networks usually have billions of nodes and tens or hundreds of billions of edges <ref type="bibr" target="#b39">[40]</ref>. It is important to develop learning algorithms that can scale well to large networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single</head><p>Single / LINE <ref type="bibr" target="#b34">[35]</ref> node2vec <ref type="bibr" target="#b9">[10]</ref> NetMF <ref type="bibr" target="#b28">[29]</ref> NetSMF <ref type="bibr" target="#b27">[28]</ref> Attributed Homogeneous Network (AHON) TADW <ref type="bibr" target="#b40">[41]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single</head><p>Single Attributed LANE <ref type="bibr" target="#b15">[16]</ref> AANE <ref type="bibr" target="#b14">[15]</ref> SNE <ref type="bibr" target="#b19">[20]</ref> DANE <ref type="bibr" target="#b8">[9]</ref> ANRL <ref type="bibr" target="#b43">[44]</ref> Heterogeneous Network (HEN) PTE <ref type="bibr" target="#b33">[34]</ref> Multi Single / metapath2vec <ref type="bibr" target="#b6">[7]</ref> HERec <ref type="bibr" target="#b30">[31]</ref> Attributed HEN (AHEN)</p><formula xml:id="formula_0">HNE [3] Multi Single Attributed Multiplex Heterogeneous Network (MHEN) PMNE [22] Single Multi / MVE [30] MNE [43] mvn2vec [32] GATNE-T Multi Multi Attributed MHEN (AMHEN) GATNE-I Multi Multi Attributed</formula><p>To address the above challenges, we propose a novel approach to capture both rich attributed information and to utilize multiplex topological structures from different node types, namely General Attributed Multiplex HeTerogeneous Network Embedding, or abbreviated as GATNE. The key features of GATNE are the following:</p><p>• We formally define the problem of attributed multiplex heterogeneous network embedding, which is a more general representation for real-world networks. • GATNE supports both transductive and inductive embeddings learning for attributed multiplex heterogeneous networks. We also give the theoretical analysis to prove that our transductive model is a more general form than existing models (e.g., MNE <ref type="bibr" target="#b42">[43]</ref>). • Efficient and scalable learning algorithms for GATNE have been developed. Our learning algorithms are able to handle hundreds of million nodes and billions of edges efficiently.</p><p>We conduct extensive experiments to evaluate the proposed models on four different genres of datasets: Amazon, YouTube, Twitter, and Alibaba. Experimental results show that the proposed framework can achieve statistically significant improvements (∼5.99-28.23% lift by F1 scores on Alibaba dataset; p ≪0.01, t−test) over state-of-the-art methods. We have deployed the proposed model on Alibaba's distributed system and apply the method to Alibaba's recommendation engine. Offline A/B tests further confirm the effectiveness and efficiency of our proposed models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review related state-of-the-arts for network embedding, heterogeneous network embedding, multiplex heterogeneous network embedding, and attributed network embedding.</p><p>Network Embedding. Works in network embedding mainly consist of two categories, graph embedding (GE) and graph neural network (GNN). Representative works for GE include DeepWalk <ref type="bibr" target="#b26">[27]</ref> which generates a corpus on graphs by random walk and then trains a skip-gram model on the corpus. LINE <ref type="bibr" target="#b34">[35]</ref> learns node presentations on large-scale networks while preserving both firstorder and second-order proximities. node2vec <ref type="bibr" target="#b9">[10]</ref> designs a biased random walk procedure to efficiently explore diverse neighborhoods. NetMF <ref type="bibr" target="#b28">[29]</ref> is a unified matrix factorization framework for theoretically understanding and improving DeepWalk and LINE. For popular works in GNN, GCN <ref type="bibr" target="#b18">[19]</ref> incorporates neighbors' feature representations into the node feature representation using convolutional operations. GraphSAGE <ref type="bibr" target="#b10">[11]</ref> provides an inductive approach to combine structural information with node features. It learns functional representations instead of direct embeddings for each node, which helps it work inductively on unobserved nodes during training.</p><p>Heterogeneous Network Embedding. Heterogeneous networks examine scenarios with nodes and/or edges of various types. Such networks are notoriously difficult to mine because of the bewildering combination of heterogeneous contents and structures. The creation of a multidimensional embedding of such data opens the door to the use of a wide variety of off-the-shelf mining techniques for multidimensional data. Despite the importance of this problem, limited efforts have been made on embedding a scalable network of dynamic and heterogeneous data. HNE <ref type="bibr" target="#b2">[3]</ref> jointly considers the contents and topological structures in networks and represents different objects in heterogeneous networks to unified vector representations. PTE <ref type="bibr" target="#b33">[34]</ref> constructs large-scale heterogeneous text network from labeled information and different levels of word co-occurrence information, which is then embedded into a lowdimensional space. metapath2vec <ref type="bibr" target="#b6">[7]</ref> formalizes meta-path based random walk to construct the heterogeneous neighborhood of a node and then leverages a heterogeneous skip-gram model to perform node embeddings. HERec <ref type="bibr" target="#b30">[31]</ref> uses a meta-path based random walk strategy to generate meaningful node sequences to learn network embeddings that are first transformed by a set of fusion functions and subsequently integrated into an extended matrix factorization (MF) model.</p><p>Multiplex Heterogeneous Network Embedding. Existing approaches usually study networks with a single type of proximity between nodes, which only captures a single view of a network. However, in reality, there usually exist multiple types of proximities between nodes, yielding networks with multiple views, or multiplex network embedding. PMNE <ref type="bibr" target="#b21">[22]</ref> proposes three methods to project a multiplex network into a continuous vector space. MVE <ref type="bibr" target="#b29">[30]</ref> embeds networks with multiple views in a single collaborated embedding using attention mechanism. MNE <ref type="bibr" target="#b42">[43]</ref> uses one common embedding and several additional embeddings of each edge type for each node, which are jointly learned by a unified network embedding model. Mvn2vec <ref type="bibr" target="#b31">[32]</ref> explores the feasibility to achieve better embedding results by simultaneously modeling preservation and collaboration to represent semantic meanings of edges in different views respectively. Attributed Network Embedding. Attributed network embedding aims to seek for low-dimensional vector representations for nodes in a network, such that original network topological structure and node attribute proximity can be preserved in such representations. TADW <ref type="bibr" target="#b40">[41]</ref> incorporates text features of vertices into network representation learning under the framework of matrix factorization. LANE <ref type="bibr" target="#b15">[16]</ref> smoothly incorporates label information into the attributed network embedding while preserving their correlations. AANE <ref type="bibr" target="#b14">[15]</ref> enables a joint learning process to be done in a distributed manner for accelerated attributed network embedding. SNE <ref type="bibr" target="#b19">[20]</ref> proposes a generic framework for embedding social networks by capturing both the structural proximity and attribute proximity. DANE <ref type="bibr" target="#b8">[9]</ref> can capture the high nonlinearity and preserve various proximities in both topological structure and node attributes. ANRL <ref type="bibr" target="#b43">[44]</ref> uses a neighbor enhancement autoencoder to model the node attribute information and an attribute-aware skip-gram model based on the attribute encoder to capture the network structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>Denote a network G = (V, E), where V is a set of n nodes and E is a set of edges between nodes. Each edge e i j = (v i , v j ) is associated with a weight w i j ≥ 0, indicating the strength of the relationship between v i and v j . In practice, the network could be either directed or undirected. If G is directed, we have e i j e ji and w i j w ji ;</p><p>if G is undirected, we have e i j ≡ e ji and w i j ≡ w ji . Notations are summarized in Table <ref type="table" target="#tab_1">2</ref>. Definition 1 (Heterogeneous Network). A heterogeneous network <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref> is a network G = (V, E) associated with a node type mapping function ϕ : V → O and an edge type mapping function ψ : E → R, where O and R represent the set of all node types and the set of all edge types, respectively. Each node v ∈ V belongs to a particular node type. Similarly, each edge e ∈ E is categorized into a specific edge type. If |O| + |R| &gt; 2, the network is called heterogeneous; otherwise homogeneous.</p><p>Notice that in a heterogeneous network, an edge e can no longer be denoted as e i j since there may be multiple types of edges between node v i and v j . Under such situations, an edge is denoted as e (r ) i j , where r corresponds to a certain edge type. Definition 2 (Attributed Network). An attributed network <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref> is a network G endowed with an attribute representation A, i.e., G = (V, E, A). Each node v i ∈ V is associated with some types of feature vectors. A = {x i |v i ∈ V} is the set of node features for all nodes, where x i is the associated node feature of node v i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 3 (Attributed Multiplex Heterogeneous Network</head><p>). An attributed multiplex heterogeneous network is a network G = (V, E, A), E = r ∈R E r , where E r consists of all edges with edge type r ∈ R, and |R| &gt; 1. We separate the network for every edge type or view r ∈ R as G r = (V, E r , A).</p><p>An example of AMHEN is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, which contains 2 node types and 4 edge types. The two node types are user and item with different attributes. Given the above definitions, we can formally define our problem for representation learning on networks.</p><p>Problem 1 (AMHEN Embedding). Given an AMHEN G = (V, E, A), the problem of AMHEN embedding is to give a unified low-dimensional space representation of each node v on every edge type r . The goal is to find a function f r : V → R d for every edge type r , where d ≪ |V |.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we first explain the proposed GATNE framework in the transductive context <ref type="bibr" target="#b18">[19]</ref>. The resultant model is named as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating Overall Node Embedding</head><p>Base Embedding Edge Embedding GATNE-T only uses network structure information while GATNE-I considers both structure information and node attributes. The output layer of heterogeneous skip-gram specifies one set of multinomial distributions for each node type in the neighborhood of the input node v. In this example, V = V1 ∪ V2 ∪ V3 and K1, K2, K3 specify the size of v's neighborhood on each node type respectively. GATNE-T. We also give a theoretical discussion about the connection of GATNE-T with the newly trending models, e.g., MNE. To deal with the partial observation problem, we further extend the model to the inductive context <ref type="bibr" target="#b41">[42]</ref> and present a new inductive model named as GATNE-I. For both models, we present efficient optimization algorithms.</p><formula xml:id="formula_1">0 1 0 0 1 1 0 0 1 0 0 1 0 1 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Attributes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GATNE-T GATNE-I</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transductive Model: GATNE-T</head><p>We begin with embedding learning for attributed multiplex heterogeneous networks in the transductive context, and present our model GATNE-T. More specifically, in GATNE-T, we split the overall embedding of a certain node v i on each edge type r into two parts: base embedding, edge embedding as shown in Figure <ref type="figure" target="#fig_1">2</ref>. The base embedding of node v i is shared between different edge types. The k-th level edge embedding u</p><formula xml:id="formula_2">(k) i,r ∈ R s , (1 ≤ k ≤ K) of node v i</formula><p>on edge type r is aggregated from neighbors' edge embeddings as:</p><formula xml:id="formula_3">u (k ) i,r = aддreдator ({u (k −1) j,r , ∀v j ∈ N i,r }),<label>(1)</label></formula><p>where N i,r is the neighbors of node v i on edge type r . The initial edge embedding u (0) i,r for each node v i and each edge type r is randomly initialized in our transductive model. Following Graph-SAGE <ref type="bibr" target="#b10">[11]</ref>, the aддreдator function can be a mean aggregator as:</p><formula xml:id="formula_4">u (k ) i,r = σ ( Ŵ(k) • mean({u (k−1) j,r , ∀v j ∈ N i,r })),<label>(2)</label></formula><p>or other pooling aggregators such as max-pooling aggregator:</p><formula xml:id="formula_5">u (k ) i,r = max({σ ( Ŵ(k) pool u (k −1) j,r + b(k) pool ), ∀v j ∈ N i,r }),<label>(3)</label></formula><p>where σ is an activation function. We denote the K-th level edge embedding u</p><formula xml:id="formula_6">(K )</formula><p>i,r as edge embedding u i,r , and concatenate all the edge embeddings for node v i as U i with size s-by-m, where s is the dimension of edge-embeddings:</p><formula xml:id="formula_7">U i = (u i,1 , u i,2 , . . . , u i,m ).<label>(4)</label></formula><p>We use self-attention mechanism <ref type="bibr" target="#b20">[21]</ref> to compute the coefficients a i,r ∈ R m of linear combination of vectors in U i on edge type r as:</p><formula xml:id="formula_8">a i,r = softmax(w T r tanh (W r U i )) T ,<label>(5)</label></formula><p>where w r and W r are trainable parameters for edge type r with size d a and d a × s respectively and the superscript T denotes the transposition of the vector or the matrix. Thus, the overall embedding of node v i for edge type r is:</p><formula xml:id="formula_9">v i,r = b i + α r M T r U i a i,r ,<label>(6)</label></formula><p>where b i is the base embedding for node v i , α r is a hyper-parameter denoting the importance of edge embeddings towards the overall embedding and M r ∈ R s×d is a trainable transformation matrix.</p><p>Connection with Previous Work. We choose MNE <ref type="bibr" target="#b42">[43]</ref>, a recent representative work for MHEN, as the base model for multiplex heterogeneous networks to discuss the connection between our proposed model and previous work. In GATNE-T, we use the attention mechanism to capture the influential factors between different edge types. We theoretically prove that our transductive model is a more general form of MNE and improves the expressiveness. For MNE, the overall embedding ṽi,r of node</p><formula xml:id="formula_10">v i on edge type r ∈ R is ṽi,r = b i + α r X T r o i,r ,<label>(7)</label></formula><p>where X r is a edge-specific transformation matrix. And for GATNE-T, the overall embedding for node v i on edge type r is:</p><formula xml:id="formula_11">v i,r = b i + α r M T r U i a i,r = b i + α r M T r m p=1 λ p u i,p ,<label>(8)</label></formula><p>where λ p denotes the p-th element of a i,r and is computed as:</p><formula xml:id="formula_12">λ p = exp(w T r tanh (W r u i,p )) t exp(w T r tanh (W r u i,t )) .<label>(9)</label></formula><p>Theorem 4.1. For any r ∈ R, there exist parameters w r and W r , such that for any o i,1 , . . . , o i,m , and corresponding matrix X r , with dimension s for each o i, j and size s × d for X r , there exist u i,1 , . . . , u i,m , and corresponding matrix M r , with dimension s + m for each u i, j and size (s + m) × d for M r , that satisfy v i,r ≈ ṽi,r .</p><p>Proof. For any t, let u i,t = (o T i,t , u ′T i,t ) T concatenated by two vectors, where the first s dimension is o i,t , and u ′ i,t is an mdimensional vector. Let u ′ i,t,k denote the k-th dimension of u ′ i,t , then take u ′ i,t,k = δ t k , where δ is the Kronecker delta function as</p><formula xml:id="formula_13">δ i j = 1, i = j; δ i j = 0, i j. (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>Let W r be all zero, except for the element on row 1 and column s +r with a large enough number M; therefore tanh(W r u i,p ) becomes a vector with its 1 st dimension approximately being δ r p , and other dimensions being 0. Take w r as a vector with its 1 st dimension being M, then exp(w T r tanh (W r u i,p )) ≈ exp(Mδ r p ), so λ p ≈ δ r p . Finally we take M r being X r at its first s × d dimension, and 0 on the following m × d dimension, and we can get v i,r ≈ ṽi,r . □</p><p>Thus the parameter space of MNE is almost included by our model's parameter space and we can conclude that GATNE-T is a generalization of MNE, if edge embeddings can be trained directly. However, in our model, the edge embedding u is generated from single or multiple layers of aggregation. We give more discussions about the aggregation case.</p><p>Effects of Aggregation. In the GATNE-T model, the edge embedding u (k ) is computed by aggregating the edge embedding u (k −1) of its neighbors as:</p><formula xml:id="formula_15">u (k ) i,r = σ ( Ŵ(k) • mean({u (k −1) j,r , v j ∈ N i,r })).<label>(11)</label></formula><p>The mean aggregator is basically a matrix multiplication,</p><formula xml:id="formula_16">mean k (v i ) = mean({u (k −1) j,r , v j ∈ N i,r }) = (U (k−1) r N r ) i , (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>where N r is the neighborhood matrix on edge type r , U</p><formula xml:id="formula_18">(k ) r = (u (k ) 1,r , ..., u (k)</formula><p>n,r ) is the k-th level edge embedding of all nodes in the graph on edge type r , and (U</p><formula xml:id="formula_19">(k −1) r N r ) i denotes the i t h column of U (k −1) r</formula><p>N r . N r can be a normalized adjacency matrix. The mean operator of Equation ( <ref type="formula" target="#formula_15">11</ref>) can be weighted and the neighborhood matrix N i,r can be sampled. Take Ŵ(k) = I, where I is an identity matrix, then u</p><formula xml:id="formula_20">(k ) i,r = σ (mean k (v i )). If N r is of full rank, then for any O r = (o 1,r , . . . , o n,r ), there exists U (k−1) r such that U (k−1) r N r = O r .</formula><p>If the activation function σ is an automorphism, i.e., σ : R → R and N r is of full rank, we can use the construction method described in Theorem 4.1 to construct u i,r and the above method to construct each level of edge embeddings u</p><formula xml:id="formula_21">(K −1) i,r , ..., u<label>(0)</label></formula><p>i,r subsequently. Therefore, our model is still a more general form that can generalize the MNE model, when all the neighborhood matrices N r and the activation function σ are invertible in all levels of aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Inductive Model: GATNE-I</head><p>The limitation of GATNE-T is that it cannot handle unobserved data. However, in many real-world applications, the networked data is often partially observed <ref type="bibr" target="#b41">[42]</ref>. We then extend our model to the inductive context and present a new model named GATNE-I. The model is also illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. Specifically, we define the base embedding b i as a parameterized function of v i 's attribute x i as b i = h z (x i ), where h z is a transformation function and z = ϕ(i) is node v i 's corresponding node type. Notice that nodes with different types may have different dimensions of their attributes x i . The transformation function h z can have different forms such as a multi-layer perceptron <ref type="bibr" target="#b25">[26]</ref>. Similarly, the initial edge embeddings u (0) i,r for node i on edge type r should be also parameterized as the function of attributes x i as u (0) i,r = g z,r (x i ), where g z,r is also a transformation function that transforms the feature to an edge embedding of node v i on the edge type r and z is v i 's corresponding node type. To be more specific, for the inductive model, we also add an extra attribute term to the overall embedding of node v i on type r :</p><formula xml:id="formula_22">v i,r = h z (x i ) + α r M T r U i a i,r + β r D T z x i ,<label>(13)</label></formula><p>where β r is a coefficient and D z is a feature transformation matrix on v i 's corresponding node type z. The difference between our Algorithm 1: GATNE Input: network G = (V, E, A), embedding dimension d, edge embedding dimension s, learning rate η, negative samples L, coefficient α, β. Output: overall embeddings v i,r for all nodes on every edge type r 1 Initialize all the model parameters θ . 2 Generate random walks on each edge type r as P r .</p><p>3 Generate training samples {(v i , v j , r )} from random walks P r on each edge type r . i,r directly for each node, we train transformation functions h z and g z,r that transforms the raw feature x i to b i and u (0) i,r , which works for nodes that did not appear during training as long as they have corresponding raw features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Optimization</head><p>We discuss how to learn the proposed transductive and inductive models. Following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref>, we use random walk to generate node sequences and then perform skip-gram <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> over the node sequences to learn embeddings. Since each view of the input network is heterogeneous, we use meta-path-based random walks <ref type="bibr" target="#b6">[7]</ref>. To be specific, given a view r of the network, i.e., G r = (V, E r , A) and a meta-path scheme</p><formula xml:id="formula_23">T : V 1 → V 2 → • • • V t • • • → V l ,</formula><p>where l is the length of the meta-path scheme, the transition probability at step t is defined as follows:</p><formula xml:id="formula_24">p(v j |v i , T) =        1 |N i,r ∩V t +1 | (v i , v j ) ∈ E r , v j ∈ V t +1 , 0 (v i , v j ) ∈ E r , v j V t +1 , 0 (v i , v j ) E r ,<label>(14)</label></formula><p>where v i ∈ V t and N i,r denotes the neighborhood of node v i on edge type r . The flow of the walker is conditioned on the predefined meta path T. The meta-path-based random walk strategy ensures that the semantic relationships between different types of nodes can be properly incorporated into skip-gram model <ref type="bibr" target="#b6">[7]</ref>. Supposing the random walk with length l on edge type r follows a path P = (v p 1 , . . . , v p l ) such that (v</p><formula xml:id="formula_25">p t −1 , v p t ) ∈ E r (t = 2 . . . l), denote v p t 's context as C = {v p k |v p k ∈ P, |k −t | ≤ c, t k },</formula><p>where c is the radius of the window size. Thus, given a node v i with its context C of a path, our objective is to minimize the following negative log-likelihood:</p><formula xml:id="formula_26">− log P θ {v j |v j ∈ C}|v i = v j ∈C − log P θ (v j |v i ),<label>(15)</label></formula><p>where θ denotes all the parameters. Following metapath2vec <ref type="bibr" target="#b6">[7]</ref> we use the heterogeneous softmax function which is normalized with respect to the node type of node v j . Specifically, the probability of v j given v i is defined as:</p><formula xml:id="formula_27">P θ (v j |v i ) = exp(c T j • v i,r ) k ∈V t exp(c T k • v i,r ) ,<label>(16)</label></formula><p>where v j ∈ V t , c k is the context embedding of node v k and v i is the overall embedding of node v i for edge type r . Finally, we use heterogeneous negative sampling to approximate the objective function − log P θ (v j |v i ) for each node pair (v i , v j ) as:</p><formula xml:id="formula_28">E = − log σ (c T j • v i,r ) − L l =1 E v k ∼P t (v) [log σ (−c T k • v i,r )],<label>(17)</label></formula><p>where σ (x) = 1/(1 + exp(−x)) is the sigmoid function, L is the number of negative samples correspond to a positive training sample, and v k is randomly drawn from a noise distribution P t (v) defined on node v j 's corresponding node set V t . We summarize our algorithm in Algorithm 1. The time complexity of our random walk based algorithm is O(nmdL) where n is the number of nodes, m is the number of edge types, d is overall embedding size, L is the number of negative samples per training sample (L ≥ 1). The memory complexity of our algorithm is O(n(d +m ×s)) with s being the size of edge embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we first introduce the details of four evaluation datasets and the competitor algorithms. We focus on the link prediction task to evaluate the performances of our proposed methods compared to other state-of-the-art methods. Parameter sensitivity, convergence, and scalability are then discussed. Finally, we report the results of offline A/B tests of our method on Alibaba's recommendation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We work on three public datasets and the Alibaba dataset for the link prediction task. Amazon Product Dataset<ref type="foot" target="#foot_0">2</ref>  <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> includes product metadata and links between products; YouTube dataset<ref type="foot" target="#foot_1">3</ref>  <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref> consists of various types of interactions; Twitter dataset 4 [6] also contains various types of links. Alibaba dataset has two node types, user and item (or product), and includes four types of interactions between users and items. Since some of the baselines cannot scale to the whole graph, we evaluate performances on sampled datasets. The statistics of these four sampled datasets are summarized in Table <ref type="table" target="#tab_2">3</ref>. Notice that n-types and e-types in the table denote node types and edge types, respectively. Amazon. In our experiments, we only use the product metadata of Electronics category, including the product attributes and coviewing, co-purchasing links between products. The product attributes include the price, sales-rank, brand, and category.</p><p>YouTube. YouTube dataset is a multiplex bidirectional network dataset that consists of five types of interactions between 15,088 YouTube users. The types of edges include contact, shared friends, shared subscription, shared subscriber, and shared favorite videos between users.</p><p>Twitter. Twitter dataset is about tweets related to the discovery of the Higgs boson between 1st and 7th, July 2012. It is made up of four directional relationships between more than 450,000 Twitter users. The relationships are re-tweet, reply, mention, and friendship/follower relationships between Twitter users.</p><p>Alibaba. Alibaba dataset consists of four types of interactions including click, add-to-preference, add-to-cart, and conversion between two types of nodes, user and item. The sampled Alibaba dataset is denoted as Alibaba-S. We also provide the evaluation of the whole dataset on Alibaba's distributed cloud platform; the full dataset is denoted as Alibaba.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Competitors</head><p>We categorize our competitors into the following four groups. The overall embedding size is set to 200 for all methods. The specific hyper-parameter settings for different methods are listed in the Appendix.</p><p>Network Embedding Methods. The compared methods include DeepWalk <ref type="bibr" target="#b26">[27]</ref>, LINE <ref type="bibr" target="#b34">[35]</ref>, and node2vec <ref type="bibr" target="#b9">[10]</ref>. As these methods can only deal with HON, we feed separate graphs with different edge types to them and obtain different node embeddings for each separate graph.</p><p>Heterogeneous Network Embedding Methods. We focus on the representative work metapath2vec <ref type="bibr" target="#b6">[7]</ref>, which is designed to deal with the node heterogeneity. When there is only one node type in the network, metapath2vec degrades to DeepWalk. For Alibaba dataset, the meta-path schemes are set to U − I − U and I − U − I , where U and I denote User and Item respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multiplex Heterogeneous Network Embedding Methods.</head><p>The compared methods include PMNE <ref type="bibr" target="#b21">[22]</ref>, MVE <ref type="bibr" target="#b29">[30]</ref>, MNE <ref type="bibr" target="#b42">[43]</ref>. We denote the three methods of PMNE as PMNE(n), PMNE(r) and PMNE(c) respectively. MVE uses collaborated context embeddings and applies an attention mechanism to view-specific embedding. 4 https://snap.stanford.edu/data/higgs-twitter.html MNE uses one common embedding and several additional embeddings for each edge type, which are jointly learned by a unified network embedding model.</p><p>Attributed Network Embedding Methods. The compared method is ANRL <ref type="bibr" target="#b43">[44]</ref>. ANRL uses a neighbor enhancement autoencoder to model the node attribute information and an attributeaware skip-gram model based on the attribute encoder to capture the network structure.</p><p>Our Methods. Our proposed methods include GATNE-T and GATNE-I. GATNE-T considers the network structure and uses base embeddings and edge embeddings to capture the influential factors between different edge types. GATNE-I considers both the network structure and the node attributes, and learns an inductive transformation function instead of learning base embeddings and meta embeddings for each node directly. For Alibaba dataset, we use the same meta-path schemes as metapath2vec. For some datasets without node attributes, we also generate node features for them. Due to the size of the Alibaba dataset with more than 40 million nodes and 500 million edges and the scalabilities of the other competitors, we only compare our GATNE model with DeepWalk, MVE, and MNE. Specific implementations can be found in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Analysis</head><p>Link prediction is a common task in both academia and industry. For academia, it is widely used to evaluate the quality of network embeddings obtained by different methods. In the industry, link prediction is a very demanding task since in real-world scenarios we are usually facing graphs with partial links, especially for ecommerce companies that rely on the links between their users and items for recommendations. We hide a set of edges/non-edges from the original graph and train on the remaining graph. Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref>, we create a validation/test set that contains 5%/10% randomly selected positive edges respectively with the equivalent number of randomly selected negative edges for each edge type. The validation set is used for hyper-parameter tuning and early stopping. The test set is used to evaluate the performance and is only run once under the tuned hyper-parameter. We use some commonly used evaluation criteria, i.e., the area under the ROC curve (ROC-AUC) <ref type="bibr" target="#b11">[12]</ref> and the PR curve (PR-AUC) <ref type="bibr" target="#b4">[5]</ref> in our experiments. We also use F1 score as the other metric for evaluation. To avoid the thresholding effect <ref type="bibr" target="#b36">[37]</ref>, we assume that the number of hidden edges in the test set is given <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref>. All of these metrics are uniformly averaged among the selected edge types.</p><p>Quantitative Results. The experimental results of three public datasets and Alibaba-S are shown in Table <ref type="table" target="#tab_3">4</ref>. GATNE outperforms all sorts of baselines in the various datasets. GATNE-T obtains better performance than GATNE-I on Amazon dataset as the node attributes are limited. The node attributes of Alibaba dataset are abundant so that GATNE-I obtains the best performance. ANRL is very sensitive to the weak node attributes and obtains the worst result on Amazon dataset. The different node attributes of users and items also limit the performance of ANRL on Alibaba-S dataset. On YouTube and Twitter datasets, GATNE-I performs similarly to GATNE-T as the node attributes of these two datasets are the node embeddings of DeepWalk, which are generated by the network       achieves better performance than GATNE-T on extremely largescale real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Millions of Training Batches</head><p>Scalability Analysis. We investigate the scalability of GATNE that has been deployed on multiple workers for optimization. While GATNE-T converges slower and its training speed is about to reach a limit when the number of workers being larger than 100.</p><p>Besides the state-of-the-art performance, GATNE is also scalable enough to be adopted in practice.</p><p>Parameter Sensitivity. We investigate the sensitivity of different hyper-parameters in GATNE, including overall embedding dimension d and edge embedding dimension s. Figure <ref type="figure" target="#fig_6">4</ref> illustrates the performance of GATNE when altering the base embedding dimension d or edge embedding dimension s from the default setting (d = 200, s = 10). We can conclude that the performance of GATNE is relatively stable within a large range of base/edge embedding dimensions, and the performance drops when the base/edge embedding dimension is either too small or too large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Offline A/B Tests</head><p>We deploy our inductive model GATNE-I on Alibaba's distributed cloud platform for its recommendation system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In our paper, we formalized the attributed multiplex heterogeneous network embedding problem and proposed GATNE to solve it with both transductive and inductive settings. We split the overall node embedding of GATNE-I into three parts: base embedding, edge embedding, and attribute embedding. The base embedding and attribute embedding are shared among edges of different types, while the edge embedding is computed by aggregation of neighborhood information with the self-attention mechanism. Our proposed methods achieve significantly better performances compared to previous state-of-the-art methods on link prediction tasks across multiple challenging datasets. The approach has been successfully deployed and evaluated on Alibaba's recommendation system with excellent scalability and effectiveness. We use the codes released by the corresponding author in the GitHub 13 . As for Alibaba dataset, we re-implemented it on the Alibaba distributed cloud platform.</p><p>A.2.4 Attributed Network Embedding Methods.</p><p>• ANRL <ref type="bibr" target="#b43">[44]</ref>. We use the codes from Alibaba's GitHub 14 . As YouTube and Twitter datasets do not have node attributes, we generate node attributes for them. To be specific, we use the node embeddings (200 dimensions) of DeepWalk as the input node features on these datasets for ANRL. For Alibaba-S and Amazon dataset, we use raw features as attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Datasets</head><p>Our experiment evaluates on five datasets, including four datasets and Alibaba dataset. Due to the limitation of memory and computation resources on a single Linux server, the four datasets are subgraphs sampled from the original datasets for training and evaluation. Table <ref type="table" target="#tab_7">6</ref> shows the statistics of the original public datasets.</p><p>• Amazon is a dataset of product reviews and metadata from Amazon. In our experiments, we only use the product metadata, including the product attributes and co-viewing, copurchasing links between products. The node type set of Amazon is O = {product } and the edge type set of Amazon is R = {also_bouдht, also_viewed }, which denotes two products are co-bought or co-viewed by the same user respectively. The products of Amazon are split into many categories. The number of products in all the categories is so large that we use the Electronics category of products for experimentation. The number of products in Electronics is still large for many algorithms; therefore, we extract a connected subgraph from the whole graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Discussion</head><p>As for research on network embedding, many people use link prediction or node classification tasks for evaluating the representation of network embeddings. However, although there are many commonly used public datasets, like Twitter or YouTube dataset, none of them provide a "standard" separation for train, validation, and test for different tasks. This causes different results on the same dataset for different evaluation separations so the results from previous papers cannot be directly used, and researchers have to re-implement and run all baselines themselves, reducing their attention on improving their model.</p><p>Here we appeal on researchers to provide the standardized dataset, which contains a standard separation of train, validation and test sets as well as the full dataset. Therefore researchers can evaluate their method based on a standard environment, and results across papers can be compared directly. This also helps to increase the reproducibility of research.</p><p>Future Work. Apart from the heterogeneity of networks, the dynamics of networks are also crucial to network representation learning. There are three ways to capture the dynamic information of networks. Firstly, we can add dynamic information into node attributes. For example, we can use methods like LSTM <ref type="bibr" target="#b13">[14]</ref> to capture the dynamic activities of users. Secondly, the dynamic information, such as the timestamp of each interaction, can be considered as the attributes of edges. Thirdly, we may consider the several snapshots of networks representing the dynamic evolution of networks. We leave representation learning for the dynamic attributed multiplex heterogeneous network as our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The left illustrates an example of an attributed multiplex heterogeneous network. Users in the left part of the figure are associated with attributes, including gender, age, and location. Similarly, items in the left part of the figure include attributes such as price and brand. The edge types between users and items are from four interactions, including click, addto-preference, add-to-cart and conversion. The three subfigures in the middle represent three different ways of setting up the graphs, including HON, MHEN, and AMHEN from the bottom to the top. The right part shows the performance improvement of the proposed models over DeepWalk on the Alibaba dataset. As can be seen, GATNE-I achieves a +28.23% performance lift compared to DeepWalk.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of GATNE-T and GATNE-I models.GATNE-T only uses network structure information while GATNE-I considers both structure information and node attributes. The output layer of heterogeneous skip-gram specifies one set of multinomial distributions for each node type in the neighborhood of the input node v. In this example, V = V1 ∪ V2 ∪ V3 and K1, K2, K3 specify the size of v's neighborhood on each node type respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 while not converged do 5 foreach 7 Sample 8</head><label>578</label><figDesc>(v i , v j , r ) ∈ training samples do 6 Calculate v i,r using Equation (6) or (13) L negative samples and calculate objective function E using Equation (17) Update model parameters θ by ∂E ∂θ . transductive and inductive model mainly lies on how the base embedding b i and initial edge embeddings u (0) i,r are generated. In our transductive model, the base embedding b i and initial edge embedding u (0) i,r are directly trained for each node based on the network structure, and the transductive model cannot handle nodes that are not seen during training. As for our inductive model, instead of training b i and u (0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) The convergence curve for GATNE-T and GATNE-I models on Alibaba dataset. The inductive model converges faster and achieves better performance than the transductive model. (b) The training time decreases as the number of workers increases. GATNE-I takes less training time to converge compared with GATNE-T.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The performance of GATNE-T and GATNE-I on Alibaba-S when changing base/edge embedding dimensions exponentially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc> shows the speedup w.r.t. the number of workers on the Alibaba dataset. The figure shows that GATNE is quite scalable on the distributed platform, as the training time decreases significantly when we add up the number of workers, and finally, the inductive model takes less than 2 hours to converge with 150 distributed workers. We also find that GATNE-I's training speed increases almost linearly as the number of workers increases but less than 150.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The network types handled by different methods.</figDesc><table><row><cell></cell><cell>Heterogeneity</cell><cell></cell></row><row><cell>Network Type</cell><cell>Method</cell><cell>Attribute</cell></row><row><cell></cell><cell>Node Type Edge Type</cell><cell></cell></row><row><cell></cell><cell>DeepWalk [27]</cell><cell></cell></row><row><cell>Homogeneous</cell><cell></cell><cell></cell></row><row><cell>Network (HON)</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Notations.</figDesc><table><row><cell cols="2">Notation Description</cell></row><row><cell>G</cell><cell>the input network</cell></row><row><cell>V, E</cell><cell>the node/edge set of G</cell></row><row><cell>O, R</cell><cell>the node/edge type set of G</cell></row><row><cell>A</cell><cell>the attribute set of G</cell></row><row><cell>n</cell><cell>the number of nodes</cell></row><row><cell>m</cell><cell>the number of edge types</cell></row><row><cell>r</cell><cell>an edge type</cell></row><row><cell>d</cell><cell>the dimension of base/overall embeddings</cell></row><row><cell>s</cell><cell>the dimension of edge embeddings</cell></row><row><cell>v</cell><cell>a node in the graph</cell></row><row><cell>N</cell><cell>the neighborhood set of a node on an edge type</cell></row><row><cell>b, u, c, v</cell><cell>the base/edge/context/overall embedding of a node</cell></row><row><cell>h, g</cell><cell>transformation functions in our inductive approach</cell></row><row><cell>x</cell><cell>the attribute of a node</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Statistics of Datasets.</figDesc><table><row><cell>Dataset</cell><cell># nodes</cell><cell># edges</cell><cell cols="2"># n-types # e-types</cell></row><row><cell>Amazon</cell><cell>10,166</cell><cell>148,865</cell><cell>1</cell><cell>2</cell></row><row><cell>YouTube</cell><cell>2,000</cell><cell>1,310,617</cell><cell>1</cell><cell>5</cell></row><row><cell>Twitter</cell><cell>10,000</cell><cell>331,899</cell><cell>1</cell><cell>4</cell></row><row><cell>Alibaba-S</cell><cell>6,163</cell><cell>17,865</cell><cell>2</cell><cell>4</cell></row><row><cell>Alibaba</cell><cell cols="2">41,991,048 571,892,183</cell><cell>2</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Performance comparison of different methods on four datasets.</figDesc><table><row><cell></cell><cell></cell><cell>Amazon</cell><cell></cell><cell></cell><cell>YouTube</cell><cell></cell><cell></cell><cell>Twitter</cell><cell></cell><cell></cell><cell>Alibaba-S</cell><cell></cell></row><row><cell></cell><cell cols="2">ROC-AUC PR-AUC</cell><cell>F1</cell><cell cols="2">ROC-AUC PR-AUC</cell><cell>F1</cell><cell cols="2">ROC-AUC PR-AUC</cell><cell>F1</cell><cell cols="2">ROC-AUC PR-AUC</cell><cell>F1</cell></row><row><cell>DeepWalk</cell><cell>94.20</cell><cell>94.03</cell><cell>87.38</cell><cell>71.11</cell><cell>70.04</cell><cell>65.52</cell><cell>69.42</cell><cell>72.58</cell><cell>62.68</cell><cell>59.39</cell><cell>60.62</cell><cell>56.10</cell></row><row><cell>node2vec</cell><cell>94.47</cell><cell>94.30</cell><cell>87.88</cell><cell>71.21</cell><cell>70.32</cell><cell>65.36</cell><cell>69.90</cell><cell>73.04</cell><cell>63.12</cell><cell>62.26</cell><cell>63.40</cell><cell>58.49</cell></row><row><cell>LINE</cell><cell>81.45</cell><cell>74.97</cell><cell>76.35</cell><cell>64.24</cell><cell>63.25</cell><cell>62.35</cell><cell>62.29</cell><cell>60.88</cell><cell>58.18</cell><cell>53.97</cell><cell>54.65</cell><cell>52.85</cell></row><row><cell>metapath2vec</cell><cell>94.15</cell><cell>94.01</cell><cell>87.48</cell><cell>70.98</cell><cell>70.02</cell><cell>65.34</cell><cell>69.35</cell><cell>72.61</cell><cell>62.70</cell><cell>60.94</cell><cell>61.40</cell><cell>58.25</cell></row><row><cell>ANRL</cell><cell>71.68</cell><cell>70.30</cell><cell>67.72</cell><cell>75.93</cell><cell>73.21</cell><cell>70.65</cell><cell>70.04</cell><cell>67.16</cell><cell>64.69</cell><cell>58.17</cell><cell>55.94</cell><cell>56.22</cell></row><row><cell>PMNE(n)</cell><cell>95.59</cell><cell>95.48</cell><cell>89.37</cell><cell>65.06</cell><cell>63.59</cell><cell>60.85</cell><cell>69.48</cell><cell>72.66</cell><cell>62.88</cell><cell>62.23</cell><cell>63.35</cell><cell>58.74</cell></row><row><cell>PMNE(r)</cell><cell>88.38</cell><cell>88.56</cell><cell>79.67</cell><cell>70.61</cell><cell>69.82</cell><cell>65.39</cell><cell>62.91</cell><cell>67.85</cell><cell>56.13</cell><cell>55.29</cell><cell>57.49</cell><cell>53.65</cell></row><row><cell>PMNE(c)</cell><cell>93.55</cell><cell>93.46</cell><cell>86.42</cell><cell>68.63</cell><cell>68.22</cell><cell>63.54</cell><cell>67.04</cell><cell>70.23</cell><cell>60.84</cell><cell>51.57</cell><cell>51.78</cell><cell>51.44</cell></row><row><cell>MVE</cell><cell>92.98</cell><cell>93.05</cell><cell>87.80</cell><cell>70.39</cell><cell>70.10</cell><cell>65.10</cell><cell>72.62</cell><cell>73.47</cell><cell>67.04</cell><cell>60.24</cell><cell>60.51</cell><cell>57.08</cell></row><row><cell>MNE</cell><cell>90.28</cell><cell>91.74</cell><cell>83.25</cell><cell>82.30</cell><cell>82.18</cell><cell>75.03</cell><cell>91.37</cell><cell>91.65</cell><cell>84.32</cell><cell>62.79</cell><cell>63.82</cell><cell>58.74</cell></row><row><cell>GATNE-T</cell><cell>97.44</cell><cell>97.05</cell><cell>92.87</cell><cell>84.61</cell><cell>81.93</cell><cell>76.83</cell><cell></cell><cell>91.77</cell><cell>84.96</cell><cell>66.71</cell><cell>67.55</cell><cell>62.48</cell></row><row><cell>GATNE-I</cell><cell>96.25</cell><cell>94.77</cell><cell>91.36</cell><cell>84.47</cell><cell>82.32</cell><cell>76.83</cell><cell>92.04</cell><cell>91.95</cell><cell>84.38</cell><cell>70.87</cell><cell>71.65</cell><cell>65.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The experimental results on Alibaba dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ROC-AUC PR-AUC</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell cols="2">DeepWalk</cell><cell></cell><cell>65.58</cell><cell>78.13</cell><cell>70.14</cell></row><row><cell></cell><cell></cell><cell>MVE</cell><cell></cell><cell></cell><cell>66.32</cell><cell>80.12</cell><cell>72.14</cell></row><row><cell></cell><cell></cell><cell>MNE</cell><cell></cell><cell></cell><cell>79.60</cell><cell>93.01</cell><cell>84.86</cell></row><row><cell></cell><cell></cell><cell cols="2">GATNE-T</cell><cell></cell><cell>81.02</cell><cell>93.39</cell><cell>86.65</cell></row><row><cell></cell><cell></cell><cell cols="2">GATNE-I</cell><cell></cell><cell>84.20</cell><cell>95.04</cell><cell>89.94</cell></row><row><cell>0</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table 5 lists the experimental results of Alibaba dataset.Convergence Analysis. We analyze the convergence properties of our proposed models on Alibaba dataset. The results, as shown in Figure3(a), demonstrate that GATNE-I converges faster and</figDesc><table><row><cell>GATNE scales very well and achieves state-of-the-art performance</cell></row><row><cell>on Alibaba dataset, with 2.18% performance lift in PR-AUC, 5.78%</cell></row><row><cell>in ROC-AUC, and 5.99% in F1-score, compared with best results</cell></row><row><cell>from previous state-of-the-art algorithms. The GATNE-I performs</cell></row><row><cell>better than GATNE-T model in the large-scale dataset, suggesting</cell></row><row><cell>that the inductive approach works better on large-scale attributed</cell></row><row><cell>multiplex heterogeneous networks, which is usually the case in</cell></row><row><cell>real-world situations.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The training dataset has about 100 million users and 10 million items, with 10 billion interactions between them per day. We use the model to generate embedding vectors for users and items. For every user, we use K nearest neighbor (KNN) with Euclidean distance to calculate the top-N items that the user is most likely to click. The experimental goal is to maximize top-N hit-rate. Under the framework of A/B tests, we conduct an offline test on GATNE-I, MNE, and DeepWalk. The results demonstrate that GATNE-I improves hit-rate by 3.26% and 24.26% compared to MNE and DeepWalk, respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Statistics of Original Datasets. We use the codes from MNE's GitHub13 . The probability of traversing layers of PMNE(c) is set to 0.5.• MVE<ref type="bibr" target="#b29">[30]</ref>. MVE uses collaborated context embeddings and applies an attention mechanism to view-specific embeddings. The code of MVE was received from the corresponding author by email. The embedding dimensions for each view is set to 200. The number of training samples for each epoch is set to 100 million and the number of epochs is set to 10. As for Alibaba dataset, we re-implemented this method on the Alibaba distributed cloud platform. • MNE [43]. MNE uses one common embedding and several additional embeddings of each edge type for each node, which are jointly learned by a unified network embedding model. The additional embedding size for MNE is set to 10.</figDesc><table><row><cell>Dataset # nodes</cell><cell># edges</cell><cell cols="2"># n-types # e-types</cell></row><row><cell cols="2">Amazon 312,320 7,500,100</cell><cell>1</cell><cell>4</cell></row><row><cell cols="2">YouTube 15,088 13,628,895</cell><cell>1</cell><cell>5</cell></row><row><cell cols="2">Twitter 456,626 15,367,315</cell><cell>1</cell><cell>4</cell></row><row><cell>MNE[43].</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>YouTube is a multi-dimensional bidirectional network dataset consists of 5 types of interactions between 15,088 YouTube users. The types of edges include contact, shared friends, shared subscription, shared subscriber, and shared favorite videos between users. It is a multiplex network with |O| = 1 and |R| = 5. • Twitter is a dataset about tweets posted on Twitter about the discovery of the Higgs boson between 1st and 7th, July 2012. It is made up of 4 directional relationships between more than 450,000 Twitter users. The relationships are retweet, reply, mention, and friendship/follower relationship between Twitter users. It is a multiplex network with |O| = 1 and |R| = 4. • Alibaba consists of 4 types of interactions which including click, add-to-preference, add-to-cart, and conversion between two types of nodes, user and item. The node type set of Alibaba is O = {user, item} and the size of the edge type set is |R| = 4. The whole graph of Alibaba is so large that we cannot evaluate the performances of different methods on it by a single machine. We extract a subgraph from the whole graph for comparison with different methods, denoted as Alibaba-S. By the way, we also provide the evaluation of the whole graph on the Alibaba's distributed cloud platform, the full graph is denoted as Alibaba.</figDesc><table /><note>13 https://github.com/HKUST-KnowComp/MNE 14 https://github.com/cszhangzhen/ANRL •</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">http://jmcauley.ucsd.edu/data/amazon/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">http://socialcomputing.asu.edu/datasets/YouTube</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Qibin Chen, Ming Ding, Chang Zhou, and Xiaonan Fang for their comments. The work is supported by the NSFC for Distinguished Young Scholar (61825602), NSFC (61836013), and a research fund supported by Alibaba Group.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>In the appendix, we first give the implementation notes of our proposed models. The detailed descriptions of datasets and the parameter configurations of all methods are then given. Finally, we discuss the questions about fair comparison and our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Implementation Notes</head><p>Running Environment. The experiments in this paper can be divided into two parts. One is conducted on four datasets using a single Linux server with 4 Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz, 512G RAM and 8 NVIDIA Tesla V100-SXM2-16GB. The codes of our proposed models in this part are implemented with TensorFlow 5 1.12 in Python 3.6. The other part is conducted on the full Alibaba dataset using Alibaba's distributed cloud platform which contains thousands of workers. Every two workers share an NVIDIA Tesla P100 GPU with 16GB memory. Our proposed models are implemented with TensorFlow 1.4 in Python 2.7 in this part.</p><p>Details. Our codes used by single Linux server can be split into three parts: random walk, model training and evaluation. The random walk part is implemented referring to the corresponding part of DeepWalk 6 and metapath2vec 7 . The training part of the model is implemented referring to the word2vec part of TensorFlow tutorials 8 . The evaluation part uses some metric functions from scikit-learn 9 including roc_auc_score, f1_score, preci-sion_recall_curve, auc. Our model parameters are updated and optimized by stochastic gradient descent with Adam updating rule <ref type="bibr" target="#b16">[17]</ref>. The distributed version of our proposed models is implemented based on the coding rules of Alibaba's distributed cloud platform in order to maximize the distribution efficiency. High-level APIs, such as tf.estimator and tf.data, are used for the higher coefficient of utilization of computation resources in the Alibaba's distributed cloud platform.</p><p>Function Selection. Many different aggregator functions in Equation <ref type="bibr" target="#b0">(1)</ref>, such as the mean aggregator (Cf. Equation ( <ref type="formula">2</ref>)) or pooling aggregator (Cf. Equation ( <ref type="formula">3</ref>)), achieve similar performance in our experiments. Mean aggregator is finally used to be reported in the quantitative experiments in our model. We use the linear transformation function as the parameterized function of attributes h z and g z,r in Equation ( <ref type="formula">13</ref>) of our inductive model GATNE-I.</p><p>Parameter Configuration. Our base/overall embedding dimension d is set to 200 and the dimension of edge embedding s is set to 10. The number of walks for each node is set to 20 and the length of walks is set to 10. The window size is set to 5 for generating node contexts. The number of negative samples L for each positive training sample is set to 5. The number of maximum epochs is set to 50 and our models will early stop if ROC-AUC on the validation set does not improve in 1 training epoch. The coefficient α r and β r are all set to 1 for every edge type r . For Alibaba dataset, the node types include U and I representing User and Item respectively. The meta-path-schemes of our methods are set to U − I −U and I −U − I . 5 https://www.tensorflow.org/ 6 https://github.com/phanein/deepwalk 7 https://ericdongyx.github.io/metapath2vec/m2v.html 8 https://www.tensorflow.org/tutorials/representation/word2vec 9 https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics</p><p>We use the default setting of Adam optimizer in TensorFlow; the learning rate is set to lr = 0.001. For offline A/B test in section 5.4, we use N = 50.</p><p>Code and Dataset Releasing Details. The codes of our proposed models on the single Linux server (based on Tensorflow 1.12), together with our partition of the three public datasets and the Alibaba-S dataset are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Compared Methods</head><p>We give the detailed running configuration about all compared methods as follows. The embedding size is set to 200 for all methods. For random-walk based methods, the number of walks for each node is set to 20 and the length of walks is set to 10. The window size is set to 5 for generating node contexts. The number of negative samples for each training pairs is set to 5. The number of iterations for training the skip-gram model is set to 100. The code sources and other specific hyper-parameter settings of compared methods are explained below.</p><p>A.2.1 Network Embedding Methods.</p><p>• DeepWalk <ref type="bibr" target="#b26">[27]</ref>. For public and Alibaba-S datasets, we use the codes from the corresponding author's GitHub 6 . For Alibaba dataset, we re-implemented DeepWalk on Alibaba distributed cloud platform. • LINE <ref type="bibr" target="#b34">[35]</ref>. The codes of LINE are from the corresponding author's GitHub 10 . We use the LINE(1st+2nd) as the overall embeddings. The embedding size is set to 100 both for firstorder and second-order embeddings. The number of samples is set to 1000 million. • node2vec <ref type="bibr" target="#b9">[10]</ref>. The codes of node2vec are from the corresponding author's GitHub 11 . Node2vec adds two parameters to control the random walk process. The parameter p is set to 2 and the parameter q is set to 0.5 in our experiments.</p><p>A.2.2 Heterogeneous Network Embedding Methods.</p><p>• metapath2vec <ref type="bibr" target="#b6">[7]</ref>. The codes provided by the corresponding author are only for specific datasets and could not directly generalize to other datasets. We re-implement metapath2vec for networks with arbitrary node types in Python based on the original C++ codes 12 . As the number of node types of three public datasets is one, metapath2vec degrades to DeepWalk in these three datasets. For Alibaba dataset, the node types include U and I representing User and Item respectively. The meta-path-schemes are set to U − I − U and I − U − I .</p><p>A.2.3 Multiplex Heterogeneous Network Embedding Methods.</p><p>• PMNE <ref type="bibr" target="#b21">[22]</ref>. PMNE proposes three different methods to apply node2vec on multiplex networks. We denote their network aggregation algorithm, result aggregation algorithm, and layer co-analysis algorithm as PMNE(n), PMNE(r), and PMNE(c) respectively in accord with the denotations of </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2011. classification in social networks</title>
		<author>
			<persName><forename type="first">Smriti</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social network data analytics</title>
				<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="115" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gãĳnnemann</surname></persName>
		</author>
		<idno>ICLR&apos;18</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Heterogeneous network embedding via deep architectures</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;15</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The relationship between Precision-Recall and ROC curves</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Goadrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;06</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The anatomy of a scientific rumor</title>
		<author>
			<persName><forename type="first">Manlio</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenico</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mougel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirco</forename><surname>Musolesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2980</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;17</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Community detection in graphs</title>
		<author>
			<persName><forename type="first">Santo</forename><surname>Fortunato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics reports</title>
		<imprint>
			<biblScope unit="volume">486</biblScope>
			<biblScope unit="page" from="75" to="174" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Attributed Network Embedding</title>
		<author>
			<persName><forename type="first">Hongchang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3364" to="3370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;16</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;17</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The and use of the area under a receiver operating characteristic (ROC) curve</title>
		<author>
			<persName><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><forename type="middle">J</forename><surname>Hanley</surname></persName>
		</author>
		<author>
			<persName><surname>Mcneil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="29" to="36" />
			<date type="published" when="1982">1982. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;16</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accelerated attributed network embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM&apos;17. SIAM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Label informed attributed network embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;17</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="731" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>ICLR&apos;17</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Attributed social network embedding. TKDE</title>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2257" to="2270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cicero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Principled multilayer network embedding</title>
		<author>
			<persName><forename type="first">Weiyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sailung</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingli</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDMW&apos;17</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;15</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
	<note>Qinfeng Shi, and Anton Van Den Hengel</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>ICLR&apos;13</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;13</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushmita</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><surname>Mitra</surname></persName>
		</author>
		<title level="m">Multilayer Perceptron, Fuzzy Sets, Classifiaction</title>
				<imprint>
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;14</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">NetSMF: Large-Scale Network Embedding as Sparse Matrix Factorization</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In WWW&apos;19</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM&apos;18</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="459" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An Attention-based Collaboration Framework for Multi-View Network Representation Learning</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM&apos;17</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1767" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Heterogeneous Information Network Embedding for Recommendation</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangqiu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06597</idno>
		<title level="m">mvn2vec: Preservation and Collaboration in Multi-View Network Embedding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pathselclus: Integrating meta-path selection with user-guided object clustering in heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDD</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pte: Predictive text embedding through large-scale heterogeneous text networks</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD&apos;15</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Uncovering cross-dimension group structures in multi-dimensional networks</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM workshop on Analysis of Dynamic Networks</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="568" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large scale multi-label classification via metalabeler</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suju</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">K</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW&apos;09</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Uncoverning groups via heterogeneous interaction analysis</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xufei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM&apos;09. IEEE</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="503" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Link prediction in relational data</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Fai</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;04</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</title>
		<author>
			<persName><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="839" to="848" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;15</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2111" to="2117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;16</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scalable Multiplex Network Embedding</title>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3082" to="3088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ANRL: Attributed Network Representation Learning via Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinggang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3155" to="3161" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
