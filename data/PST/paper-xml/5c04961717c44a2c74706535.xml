<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bidirectional Handshaking LSTM for Remaining Useful Life Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-09-27">27 September 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>Elsheikh</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Bidirectional Handshaking LSTM for Remaining Useful Life Prediction</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Soumaya</forename><surname>Yacout</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Bidirectional Handshaking LSTM for Remaining Useful Life Prediction</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohamed-Salah</forename><surname>Ouali</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Bidirectional Handshaking LSTM for Remaining Useful Life Prediction</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bidirectional Handshaking LSTM for Remaining Useful Life Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-09-27">27 September 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">0A08BA3D135D5D32D03E1CB9E633CB0E</idno>
					<idno type="DOI">10.1016/j.neucom.2018.09.076</idno>
					<note type="submission">Received date: 1 March 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Remaining useful life prediction</term>
					<term>bidirectional handshaking</term>
					<term>long short-term memory</term>
					<term>asymmetric objective function</term>
					<term>target generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unpredictable failures and unscheduled maintenance of physical systems increases production resources, produces more harmful waste for the environment, and increases system life cycle costs. Efficient Remaining Useful Life (RUL) estimation can alleviate such an issue. The RUL is predicted by making use of the data collected from several types of sensors that continuously record different indicators about a working asset, such as vibration intensity or exerted pressure. This type of continuous monitoring data is sequential in time, as it is collected at a certain rate from the sensors during the asset's work. Long Short-Term Memory (LSTM) neural network models have been demonstrated to be efficient throughout the literature when dealing with sequential data because of their ability to retain a lot of information over time about previous states of the system. This paper proposes using a new LSTM architecture for predicting the RUL when given short sequences of monitored observations with random initial wear. By using LSTM, this paper proposes a new objective function that is suitable for the RUL estimation problem, as well as a new target generation approach for training LSTM networks, which requires making lesser assumptions about the actual degradation of the system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><note type="other">1</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p> Novel bidirectional LSTM architecture.  Novel asymmetric objective function for making safe predictions.  Novel approach for generating remaining useful life targets for training.  Improved performance for short sequences with random starts.  The turbofan engine dataset from NASA's repository is used for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T 1 Introduction</head><p>Remaining Useful Life (RUL) prediction is the attempt to predict the remaining period of normal operation, at a certain level of performance, for a physical system <ref type="bibr" target="#b0">[1]</ref>. There has been numerous research on RUL production in the literature. Nevertheless, there is no one approach that is universal because of the variability in the physics of different systems, their surrounding conditions, initial working conditions, and the physics of the acquisition devices <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref>. The initial condition of the working asset or its subcomponents affect the asset's RUL. Poor initial conditions put the asset at a higher risk of earlier failure, which in turn means shorter RUL <ref type="bibr" target="#b2">[3]</ref>.</p><p>Previous research can be mainly classified into physical modelling approaches and data-driven approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Physical modelling achieves the best performance if it can be accurately formulated. This is an almost impossible task as a result of the large amount of interacting variables that can affect the physical system directly and indirectly, which are mostly unknown <ref type="bibr" target="#b0">[1]</ref>. Moreover, these variables interact with each other in highly complex, non-linear ways that cannot be anticipated <ref type="bibr" target="#b1">[2]</ref>.</p><p>Accurate RUL prediction allows users of a physical system to benefit the most from its working lifetime, and to make efficient decisions regarding maintenance and replacement <ref type="bibr" target="#b5">[6]</ref>. In turn, this means more profit and fewer losses due to unexpected faults or failures <ref type="bibr" target="#b4">[5]</ref>. Due to the random nature of the system behavior, there is always the possibility that the predicted RUL will either be earlier or later than the actual lifetime. In this case, it is safer to have earlier rather than later predictions to avoid catastrophic failures <ref type="bibr" target="#b6">[7]</ref>.</p><p>Data-driven approaches offer good approximations for a system's failure mechanism based on historical data. These approaches vary in their capacity to learn complex systems <ref type="bibr" target="#b0">[1]</ref>. The monitored signals captured by sensors are acquired at a certain rate and in a chronological order from the working system. Therefore, processing sequential data such as this and continuously predicting the RUL is a problem that is suitable for sequential modelling. Data-driven approaches handle sequential data differently; either</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 4 intrinsically, such as Hidden Markov Models (HMM) <ref type="bibr" target="#b7">[8]</ref>, by windowing such as Convolutional Neural Networks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> and most machine learning techniques, or by transformations to compress variable length sequences in a set of informative features <ref type="bibr" target="#b10">[11]</ref>. The latter two approaches do not harness much of the sequential information as a result of their limited scope of observation in time and compression of information. On the other hand, models that are sequential in nature capture sequential dependencies between various observations in time in a more compact manner.</p><p>There is also another type of non-sequential data-driven modelling which depend on Key Performance Indicators (KPIs) which try to focus on some important aspects of the production system that is crucial to the system such as fuel consumption or the amount of produced power for engine systems <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>Although these approaches are meant for diagnosis, they can yet be used for prognosis <ref type="bibr" target="#b13">[14]</ref>. Although these approaches are statistically well formulated, they are either computationally demanding since they require matrix inversion or memory demanding for kernel calculations.</p><p>One of the most common sequential modelling techniques is the Recurrent Neural Networks (RNN) <ref type="bibr" target="#b14">[15]</ref>.</p><p>The main advantage of RNNs over HMMs is that HMMs have a finite, discrete set of states to represent the system, while RNN theoretically has no such limitations <ref type="bibr" target="#b15">[16]</ref>. RNN and its more recent variations such as Long Short-Term Memory (LSTM) networks <ref type="bibr" target="#b16">[17]</ref> and the Gated Recurrent Unit networks <ref type="bibr" target="#b17">[18]</ref> have shown some success in various domains that have a sequential nature <ref type="bibr" target="#b18">[19]</ref>, or that can be processed sequentially <ref type="bibr" target="#b19">[20]</ref>.</p><p>There is very limited work in the literature using RNN and its variants for RUL prediction <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b20">[21]</ref>, and even fewer publications that actually report performance and scores on real test scenarios. Very recent publications, as in <ref type="bibr" target="#b21">[22]</ref>, use LSTM and discuss how the data is prepared for the RUL prediction task, but they use a subset of the training set as the test data, which means they know the full testing sequence to allow it to produce detailed results about the performance of the LSTM over time. Nevertheless, they did not report or prepare the network to work with short sequences of sensor observations. Also, another very recent attempt was the use of an ensemble of Echo State Networks (ESN) <ref type="bibr" target="#b22">[23]</ref>. However, none of the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 5 previous publications report results using bidirectional RNNs <ref type="bibr" target="#b23">[24]</ref>, which process input sequences in both directions. This architecture has shown improvements in different recognition applications that are sequential in nature <ref type="bibr" target="#b24">[25]</ref>. Yet, this type of architecture requires certain modifications to suit the RUL prediction problem, which is different than recognition.</p><p>All of the aforementioned sequential modelling techniques are supervised and therefore require training on targets. Since the RUL cannot be assumed to be degrading linearly at every working cycle, especially if the working asset starts at a new condition, some papers have made assumptions about the nature of the actual RUL by using the piece-wise degradation function, which starts constant and degrades according to a certain power function <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>. The point of inflection from constant to degradation is determined by inspection <ref type="bibr" target="#b25">[26]</ref> or by tuning a detector model <ref type="bibr" target="#b21">[22]</ref>. Such assumptions restrict the model to making predictions given a full history of the working asset until failure. This implies that partial maintenance procedures causing the asset to start at an improved health condition are not taken into account, and such maintenance actions induce only minimal repair. This paper presents four main contributions in order to predict the RUL:</p><p>1-Predicting the RUL from short observation sequences with random starts, since the initial condition of physical systems is usually unknown due to manufacturing deficiencies, replacements of parts of the system, and non-ideal maintenance. Hence, the network is trained to anticipate such requirements. Conventionally the initial condition is either considered to be given <ref type="bibr" target="#b26">[27]</ref>, or estimated as a separate model <ref type="bibr" target="#b27">[28]</ref>.</p><p>2-Proposing a new safety-oriented objective function for the LSTM network to train the network to favor safer, earlier prediction rather than later prediction, since all well-known accuracy measures are either symmetric, such as the Mean Squared Error (MSE) and Mean Absolute Error (MAE), or rectified such as hinge loss <ref type="bibr" target="#b28">[29]</ref>, and both types are not suitable for making safe predictions.</p><p>3-Proposing a new target RUL generation procedure for the training process of the network. Instead of relying on the manual examination of the data <ref type="bibr" target="#b25">[26]</ref> or tuning other models <ref type="bibr" target="#b21">[22]</ref>, the proposed Since there are no intermediate predictions required for a given sequence of observations, and only the RUL needs to be predicted after the given sequence is observed, then there is no need to process the sequence simultaneously in both directions.</p><p>Instead, the proposed architecture processes the observed sequence in both directions sequentially by processing the sequence in the forward direction, and then using the LSTM final states to initialize the backward processing cells. This architecture forces the network to obtain two different yet linked mappings of the observation sequences to the desired RUL. The reason is that each cell is a function of the current input and the previous state, and both are different for LSTMs processing in opposite directions, unlike all-forward LSTM architectures, where all LSTMs have the same input sequence but different previous states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Long Short-Term Memory Cell</head><p>An LSTM cell was proposed to overcome the limitations of training the classical RNN <ref type="bibr" target="#b16">[17]</ref>. Instead of having the output of the RNN cell be a non-linear function of the weighted sum of the current inputs and previous output, the LSTM uses storage elements to pass information from the past outputs to current outputs.</p><p>The LSTM has three control signals, such that each is a non-linear function activated by a weighted sum of the current input observation and previous hidden state as shown in equations 1-6. The forget gate decides whether to retain or forget the previous state of the LSTM. The input gate decides whether to update the state of the LSTM using the current input or not, and the output gate decides whether to pass on the hidden state to the next iteration or not. The new state stored in the</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T 7</formula><p>LSTM is the sum of the new gated input and the gated previous state as shown in equation 5.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> illustrates how the gates and inputs interact.</p><p>(</p><p>Where , and are the trainable weights and biases, respectively, for each gating signal indicated by , is the hidden layer activation of the previous iteration, while is the current hidden layer activation. Similar to the hidden states, the cell states are defined. The current input is , and the gate activations are as described previously. Finally, is the elementwise multiplication operator.</p><p>The training of the LSTM using backpropagation is much more stable than the classical RNN and can theoretically retain information for prolonged periods of time <ref type="bibr" target="#b16">[17]</ref>. This is beneficial when attempting to make forecasts about the future by learning from long sequences of historical data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bidirectional LSTM</head><p>Bidirectional RNNs are a modification to the conventional RNNs to process sequences of observations in both directions, starting from the first input observation to the last, and starting from the last observation back to the first <ref type="bibr" target="#b23">[24]</ref>. This requires the processed sequence to be buffered into windows of observations to allow processing in both directions. At any point in time, the network makes use of the earlier observations processed by the forward LSTM cells until the point of prediction, as well as the upcoming observations processed by the backward LSTM cells to make the prediction. This approach is beneficial when making intermediate predictions, like recognizing phonemes in speech recognition <ref type="bibr" target="#b24">[25]</ref>. This is</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T 8</formula><p>different from the prediction task, where predictions are required ahead of the whole given sequence. This requirement is the trigger for the new proposed architecture for bidirectional LSTMs.</p><p>3 The Proposed Methodology</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bidirectional Handshaking LSTM (BHSLSTM)</head><p>The aim of the proposed approach is to extract as much information as possible from a given subsequence of observations to make predictions about the RUL of the system. Processing the given sequence in the forward direction, which means in the same sequence as they appeared in the system through several LSTM cells, produces a summary vector, which contains the final output after passing the given sequence through the cells.</p><p>The proposed handshaking approach initializes another set of LSTM cells that process the given sequence in reverse order, starting with the last observation and ending up with another summary vector at the first observation. This allows the LSTM network to have more insights when identifying the trend of the sequence in both directions. Moreover, the handshaking procedure allows the learning process to be collaborative between the forward and backward units, and therefore provides better results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Safety-Oriented Objective Function</head><p>The objective function is the error function at the output of the network that needs to be minimized. The networks are trained by minimizing the error between the network predictions and the actual RUL in the training data. As mentioned earlier, the main goal of RUL estimation is to anticipate upcoming failures before they occur to avoid unnecessary downtime, as well as additional maintenance costs. That is why</p><formula xml:id="formula_3">A C C E P T E D M A N U S C R I P T 9</formula><p>the Scoring Function (SF) proposed by <ref type="bibr" target="#b29">[30]</ref> favors early estimations rather than late ones, as shown in the following equation, by assuming that :</p><p>{ Where and are the penalty factors for the SF, such that , thus the penalty increases when the factors increase. In order to comply with the requirements of the scoring function, and in general any safety measure for prediction, the network is trained with an objective function that penalizes unsafe predictions, where , at a much higher cost. The SF is not a suitable objective function, as raising the error measure to an exponential is not a well chain-rule-differentiable function because the exponential term must be calculated for every weight update, which makes it computationally expensive. It also can either overflow, resulting in very large updates, or vanish, resulting in no updates at all due to the multiple applications of the exponential term.</p><p>Therefore, in this paper, we propose using an approximation of this function as shown in Figure <ref type="figure" target="#fig_5">3</ref>, which also favors earlier predictions rather than later ones. This approximation is more suitable, unlike conventional objective functions, which are either symmetric, such as the MSE and MAE functions, or rectified, which means that they are one-side, favoring those such as the hinge functions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Target RUL Generation</head><p>The target RUL used for training the network is another challenge, since the actual state of health of a system is not given and is usually unknown. Hence, different suggestions about the nature of degradation were given by different authors <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>. The core idea for their assumptions is that for healthy systems, the degradation is not noticeable, and all system behaviors look alike, making the RUL estimation an impractical task. Therefore, it is assumed that the RUL will be piecewise continuous, such that at the beginning of life, the RUL is constant and then starts decreasing.</p><p>The rate of decrease and the point where the system starts degrading are the two main concerns. The author in <ref type="bibr" target="#b25">[26]</ref> made an assumption based upon a manual study of the data that the system will start degrading at 130 cycles and will degrade linearly afterwards. The authors of <ref type="bibr" target="#b21">[22]</ref> suggested using an anomaly detector to identify the point where the system starts degrading, which signals the start of degradation after three triggers, and they tried several power law degradation functions. They had to tune the anomaly detector, the number of triggers, and the power of the degradation function parameters using a grid search until they achieved good validation results.</p><p>Our proposed target preparation is based on the behavior of the sensors' readings of the system. This is a real physical phenomenon that the previous researchers tried to approximate. We start the preparation of the target by taking the moving average smoothed version of all the sensor readings that show a trend, scale them down, and shift their values to start from 1 and end at 0. The sensor readings that show an upward trend are inverted before this step by subtracting all of the sensor readings from the maximum value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 11 Now, each sensor reading has a value ranging from 1 to 0, which is assumed to represent the ratio of the current health condition relative to the starting health condition at 1 and failure at 0. The lifespan of an asset is the working time, starting from a healthy condition until failure in a run-to-failure dataset. Hence, the RUL at any instant of the working time is a fraction of the lifespan, which is proportional to the health condition at that instant. To get the values to correspond with the remaining useful life, each of these ratios are then multiplied by the lifespan, which is the number of life cycles that the working asset took before failure starting from a healthy condition, as given in the training data. Next, for each sensor, there is the estimation of the actual RUL. The final RUL is chosen from the minimum RUL among all of the estimations to favor safer early predictions.</p><p>Finally, all RUL values above the minimum lifespan in the training set are truncated to that value. This truncation was shown to be useful in several previous works <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Figure <ref type="figure" target="#fig_6">4</ref> illustrates this procedure for one of the sensors' readings.</p><p>In summary, the point at which the system starts degrading and the degradation profile are dependent on the sensor readings and requires no tuning or any assumptions about the function of the degradation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Data Preparation</head><p>Since the network should be prepared to make predictions given a short sequence of observations, the training data must accommodate such requirements. That is why the training sequences are chunked into consecutive overlapping windows of observation sequences, each with a length equal to the minimum anticipated input sequence, which corresponds to the maximum desired number of cycles before the first RUL prediction is given. Each of these input sequences has an output corresponding to the RUL. These windows will allow the network to experience observation sequences from systems starting at different initial health states. The RUL predictions are made using only a single window of observation, so after each window with a certain number of observations, the RUL is predicted without making use of any other observation that came before or after the window. This approach relieves the RUL prediction model 4 Experiments and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmark Dataset Overview</head><p>The dataset used for the experiments is the NASA turbofan engine degradation simulation dataset, known as the Commercial Modular Aero-Propulsion System Simulation (CMAPSS) <ref type="bibr" target="#b29">[30]</ref>. The dataset has four simulation settings. Two of them have single operating conditions and the other two have multiple operating conditions. The datasets are given in the form of training and testing subsets. The training sets have run-to-failure series so the tests stop some time prior to complete failure, thus it is required to forecast the RUL. The time-series contain 26 sensor readings, as well as operating condition indicators as described in <ref type="bibr" target="#b29">[30]</ref>.</p><p>The datasets used in the experiments are the 1st and 3rd, which contain only one operating condition. The 3rd dataset has multiple failure modes. These datasets are chosen since they can be generalized to multiple operating conditions if the operating modes are given by training a model for each operating condition.</p><p>Only the sensors that show trends were considered in the analysis, namely sensors number 4, 7, 8, 11, 12, 13, 15, 17, 20, and 21 as described in <ref type="bibr" target="#b21">[22]</ref>. Another piece of information is added to the sensor readings, which is the forward difference of these readings. Since all sensor readings show a trend, such series cannot be considered stationary; hence, we use the forward difference as a detrending procedure to make the analysis of the time series more feasible <ref type="bibr" target="#b32">[33]</ref>. The raw sensor readings, as well as the difference, are used as the input to the network. There is no feature extraction step as has been proposed by some of the works of literature such as <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T 13</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Measures</head><p>In order to properly assess the performance of different network architectures, objective functions, and target RUL approximations, a set of performance measures should be defined.</p><p>The measures used in this paper are the asymmetric SF, discussed in <ref type="bibr" target="#b29">[30]</ref>, which penalizes late predictions more than early ones. Assuming that , then the SF has the same for as equation 7 with , and .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{</head><p>The RUL predictions are considered correct if is in the range [-13, 10] as discussed in <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b34">[35]</ref>. The percentage of correct RUL is formulated as follows, when the correct indicator ( )</p><p>, and otherwise</p><formula xml:id="formula_4">∑ ( )</formula><p>Where is the number of testing samples.</p><p>The Mean Absolute Error (MAE) is defined as follows ∑| |</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Evaluation Using Different Network Architectures</head><p>Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref> illustrate the performance measures of different network structures on the validation and training sets for datasets 1 and 3, respectively. The sequences are chosen to be of length 30, and the training set is windowed using this size. A random 20% subset is selected as the validation set. Each structure is run three times and the performance of the ones that perform the best run in the validation set is used to evaluate the performance of the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T 14</head><p>The number of LSTM cells in each architecture is selected to be the same: a total of 256 cells for dataset 1 and 512 for dataset 3. The number of LSTM cells is inspired by the results in <ref type="bibr" target="#b21">[22]</ref>. Increasing the number of cells increases the representational capacity of the network to approximate complex functions, yet makes it prone to overfitting. On the other hand, decreasing the number of neurons can cause underfitting, and that is why the number of neurons is estimated by trial and error on a validation set. The number of neurons in the feedforward network on top of the LSTM cells has 1024 neurons for dataset 3, and 512 for dataset 1. A final linear neuron is placed at the output of the network to predict the RUL. Since the focus in this paper is to show the effect of the architecture of the network on the accuracy of the prediction, the number of LSTM cells and feedforward neurons remained constant across the experiments.</p><p>For the experiments in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref>, the ASE objective function is used with , and , which were chosen by trial and error using the validation set performance of dataset 1. Early stopping is used with a tolerance of a maximum of 2 epochs without improvement in the training performance. The batch size used for training is 32. The optimizer used is the RMSprop algorithm <ref type="bibr" target="#b35">[36]</ref>.</p><p>In the testing phase only the last 30 observations, which is the minimum observation sequence required to be tested, are considered for the prediction task in order to assess the performance, given short sequences of random initial wear.</p><p>The results in Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref> illustrate that the BHSLTM outperforms the other structures on the test set performance, although it is not always the best on the training dataset, which means that this structure can generalize better than the other structure.</p><p>To assess the effectiveness of the state initialization of the backward processing cells by the final state of the forward processing cells used in the proposed architecture, an experiment is conducted on dataset 1 without the proposed handshake. The results are given in Table <ref type="table" target="#tab_2">3</ref>. They show an overall performance degradation, as well as the final performance on the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>15   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Evaluation Using the Mean Squared Error with BHSLTM</head><p>To assess the performance improvements when using the proposed asymmetric objective function, another set of experiments using the proposed BHLSTM is conducted on datasets 1 and 3 using the MSE objective function.    Figure <ref type="figure" target="#fig_9">5</ref> shows two merits for the ASE objective function trained network. First, its forecasts are mostly safe and come before the actual RUL. Even the forecasts that come after the actual RUL, like around cycle 80 for engine number 45 and cycle 60 for engine number 57, are not too late. The second merit is that the network trained using the ASE objective approximates the actual RUL curve much better than the other network using the MSE objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Performance Evaluation Using the Piecewise Linear Target RUL</head><p>Finally, to assess the performance improvement when using the proposed target RUL generation procedure with the fixed piecewise RUL proposed by <ref type="bibr" target="#b25">[26]</ref>, the BHLSTM is trained with such RUL for dataset 1 and the results are given in Table <ref type="table" target="#tab_5">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Performance Comparison with Other Techniques</head><p>In this sub-section we present the results that are published by other researchers who solved the RUL prediction problem for the CMAPSS datasets. Although not all of them report the same metrics on all of the datasets, the scoring function represents a common ground for comparison. Table <ref type="table" target="#tab_6">7</ref> shows the performance comparison between some of the techniques mentioned in literature.</p><p>In addition to the results reported in literature, experiments using the DB-KIT <ref type="bibr" target="#b12">[13]</ref> on the same data used</p><p>for training the neural network in terms of windowed input and targets generated by the proposed approach for RUL generation as the KPI as in <ref type="bibr" target="#b13">[14]</ref>. The techniques experimented are the Total Partial Least Squares (TPLS), the Modified Partial Least Squares (MPLS), and the Locally Weighted Projection Regression (LWPR). The number of latent variables required for the TPLS and MPLS were selected using a 5-fold cross-validation to choose the best performing hyperparameter. The kernel-based techniques were not feasible on the computer used for the experiments due to the high dimensionality of the input data ( ) and the considerable number of training windows in the datasets (17731 for dataset 1 and 21820 for dataset 3).</p><p>Results in Table <ref type="table" target="#tab_6">7</ref> are presented either as a range, or as a single value. N/A indicates that the information is not available. The proposed BHLSTM does not take into consideration the total previous history when making a prediction. Thus, its ability to make predictions for engines with random initial states.</p><p>Moreover, the performance of the BHLSTM is very close to the best reported results in the literature on dataset 1.</p><p>Moreover, the proposed technique used all sensor readings that show trends, without trying to make any feature selection that largely impacts the performance <ref type="bibr" target="#b36">[37]</ref>, nor to optimize the networks' hyperparameters. This means that the proposed network is able to properly learn the correct mapping from inputs to outputs requiring lesser effort from the analyst. The focus in this paper is to show the impact of  As for dataset 3, the performance is still comparable but less efficient due to the lack of information about the operating conditions from the input. This is a more general assumption since the operating conditions of machines are not always explicitly known, nor readable as a continuous input <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>.</p><p>The KPI-based techniques show almost comparable performance to each other on dataset 3, while the LWPR is the best performing on dataset 1. These models provide a simple representation for the problem but on the account of the performance. So, they can be used to give an approximation for the physics of the system or identification of the most important factors acquired by the monitoring system but cannot be well relied upon for direct RUL prediction. The proposed Bidirectional Handshaking LSTM (BHLSTM) network architecture uses a bidirectional sequence processing approach in a sequential manner. The forward processing LSTM units pass their final states to the backward processing units instead of simultaneously partially processing the sequence up to the time of prediction, which is more suitable for making intermediate predictions rather than making a single prediction after observing a sequence, such as in the case of RUL prediction. The summary vectors of both directions of the BHLSTM are concatenated for the higher classification layers.</p><p>This paper also proposed a new, asymmetric objective function that penalizes late predictions more than earlier ones, thereby ensuring safer predictions. This is in contrast to the commonly used mean squared error objective function, which is a symmetric function.</p><p>Finally, the proposed target generation for the RUL training requires no assumptions about the degradation function part, nor the point at which the degradation starts. The proposed approach uses the sensor readings to estimate the health of the system, which is then mapped to the target RUL. This decreases the amount of parameter tuning required by previous works of literature.</p><p>The experiments conducted in this paper show that the proposed modifications outperform the conventional network architectures and objective functions when combined and when used alone. This means that each of the proposed approaches can be used along with other types of neural networks, as well as for training other machine learning models.</p><p>The results also indicate that the proposed modifications allow the network to make robust predictions when compared to other techniques in literature, even though it uses only small chunks of information      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>predicted RUL uses the given sensor readings for defining an approximation for the actual RUL.4-Proposing a new bidirectional LSTM network architecture that suits the RUL prediction problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>Figure 1 LSTM cell diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>Figure 2 illustrates the architecture when using a single LSTM cell in the forward direction and another for the backward direction. The figure shows how the final state of the forward processing cell initializes the state of the backward processing cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>Figure 2 BHLSTM diagram for a single forward and a single backward LSTM cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>When, the proposed asymmetric objective functions are defined as follows, the Asymmetric Squared Error (ASE){And the Asymmetric Absolute Error (AAE) weights for early and late predictions respectively such that . It is recommended to add more penalty during training to force the network to comply to the desired safety constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3</head><label>3</label><figDesc>Figure 3 Comparison between the scoring function (left), the proposed asymmetric squared objective function (middle), and the asymmetric absolute objective function (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4</head><label>4</label><figDesc>Figure 4 RUL target generation procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>to have all observations start from full health until the moment of prediction, which is sometimes not available.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>effect of the objective function on the estimation of the RUL, Figure5illustrates the estimations of a BHLSTM network trained with ASE and MSE along with the actual target RUL for two of the training engines. The predictions at each cycle use only the previous 30 sensor readings, not the whole sequence up to the point of forecast.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5</head><label>5</label><figDesc>Figure 5 Comparison between RUL forecasts using ASE and MSE objective functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>the new proposed architecture, objective function, and target generation on performance of conventional A C C E P T E D M A N U S C R I P T 19 LTSM networks used for RUL prediction, and to decrease the number of assumptions required by the analyst.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>20 5</head><label>20</label><figDesc>Discussion and ConclusionIn this paper, a new Long Short-Term Memory (LSTM) network architecture, objective training function, and training target generation are proposed. These proposals aim at solving the problem of making Remaining Useful Life (RUL) predictions for physical systems using short sequences of observations with random starting health conditions more efficient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7</head><label>7</label><figDesc>Figure 7 BHLSTM diagram for a single forward and a single backward LSTM cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8</head><label>8</label><figDesc>Figure 8 Comparison between the scoring function (left), the proposed asymmetric squared objective function (middle), and the asymmetric absolute objective function (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Filter</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 9</head><label>9</label><figDesc>Figure 9 RUL target generation procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Dataset 1 performance measures for different network architectures.</figDesc><table><row><cell>Model</cell><cell>Train ASE</cell><cell>Train SF</cell><cell>Validation ASE</cell><cell>Validation SF</cell><cell>Test SF</cell><cell>Test Accuracy</cell><cell>Test MAE</cell></row><row><cell></cell><cell>4554.56</cell><cell>218.77</cell><cell>3670.12</cell><cell>152.288</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BHSLSTM (128)</cell><cell>5589.12</cell><cell>287.31</cell><cell>5092.67</cell><cell>301.78</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>4593.86</cell><cell>235.17</cell><cell>4323.77</cell><cell cols="2">145.54 376.64</cell><cell>63</cell><cell>11.7</cell></row><row><cell></cell><cell>4515.8</cell><cell>213.67</cell><cell>3961.57</cell><cell>224.97</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2 Layers LSTM</cell><cell>5094.9</cell><cell>253.22</cell><cell>5056.98</cell><cell>335.82</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(128)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>4884.82</cell><cell>228.97</cell><cell>4032.45</cell><cell cols="2">151.4 1120.1</cell><cell>50</cell><cell>14.96</cell></row><row><cell></cell><cell>5273.7</cell><cell>244.1</cell><cell>6186.3</cell><cell>378.1</cell><cell>464.8</cell><cell>57</cell><cell>13.7</cell></row><row><cell>1 Layer LSTM</cell><cell>7543.53</cell><cell>512.6</cell><cell>7839.87</cell><cell>652.16</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(256)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5129.47</cell><cell>237.5</cell><cell>6195.96</cell><cell>450.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5182.67</cell><cell>243.47</cell><cell>5452.87</cell><cell>375.47</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BLSTM (128)</cell><cell>4926.5</cell><cell>221.93</cell><cell>4475.2</cell><cell>213.23</cell><cell>542.6</cell><cell>58</cell><cell>13</cell></row><row><cell></cell><cell>5288.86</cell><cell>241.2</cell><cell>4377.4</cell><cell>243.95</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Dataset 3 performance measures for different network architectures.</figDesc><table><row><cell>Model</cell><cell>Train ASE</cell><cell>Train SF</cell><cell>Validation ASE</cell><cell>Validation SF</cell><cell>Test SF</cell><cell>Test Accuracy</cell><cell>Test MAE</cell></row><row><cell></cell><cell>6409</cell><cell>485</cell><cell>8747</cell><cell>1677</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BHSLSTM (256)</cell><cell>7914</cell><cell>766</cell><cell>6196</cell><cell>670</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5767</cell><cell>464</cell><cell>5373</cell><cell>527</cell><cell>1422</cell><cell>52</cell><cell>15.79</cell></row><row><cell>2 Layers LSTM (256)</cell><cell>6550.89 7584.9</cell><cell>507 1096</cell><cell>5747 5828</cell><cell>414.45 519</cell><cell>4931</cell><cell>45</cell><cell>18.36</cell></row><row><cell></cell><cell>5859</cell><cell>366.14</cell><cell>6668.7</cell><cell>530.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1 Layer LSTM (512)</cell><cell>8458.5 10907.65</cell><cell>699 997.56</cell><cell>10213 9238</cell><cell>502.4 1181.9</cell><cell>8158</cell><cell>42</cell><cell>23</cell></row><row><cell></cell><cell cols="2">11435 1160.25</cell><cell>10633</cell><cell>1480</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>7947.42</cell><cell>581.52</cell><cell>7158.19</cell><cell cols="2">321.5 4715.3</cell><cell>43</cell><cell>19.855</cell></row><row><cell>BLSTM (256)</cell><cell>8400.29</cell><cell>756.75</cell><cell>7061.85</cell><cell>581.25</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>6962</cell><cell>538</cell><cell cols="2">7944 867.7306</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Bidirectional LSTM performance without the proposed handshake procedure for dataset 1.</figDesc><table><row><cell>Train ASE</cell><cell cols="2">Train SF Validation ASE</cell><cell>Validation SF</cell><cell>Test SF</cell><cell cols="2">Test Accuracy Test MAE</cell></row><row><cell>5680.26</cell><cell>285.75</cell><cell>7161.5</cell><cell>596.55</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4745.56</cell><cell>216.18</cell><cell>4038.54</cell><cell>202.35</cell><cell>581.5</cell><cell>53</cell><cell>13.59</cell></row><row><cell>4827.67</cell><cell>219.8</cell><cell>4653.3</cell><cell>245.56</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>BHLSTM performance using the MSE objective function for dataset 1.</figDesc><table><row><cell>Train ASE</cell><cell cols="2">Train SF Validation ASE</cell><cell>Validation SF</cell><cell>Test SF</cell><cell cols="2">Test Accuracy Test MAE</cell></row><row><cell>152.57</cell><cell>113</cell><cell>201.76</cell><cell>111.22</cell><cell>2407.8</cell><cell>26</cell><cell>20.4</cell></row><row><cell>145.36</cell><cell>106.16</cell><cell>165.58</cell><cell>144.156</cell><cell></cell><cell></cell><cell></cell></row><row><cell>161.98</cell><cell>126.3</cell><cell>229.69</cell><cell>177.1</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>BHLSTM performance using the MSE objective function for dataset 3.By comparing the performance of the BHLTSM in Table1 and Table 2, to those in Table4and Table5a significant drop in performance on the test datasets is noted. As a means for a visual assessment of the</figDesc><table><row><cell>Train ASE</cell><cell cols="2">Train SF Validation ASE</cell><cell>Validation SF</cell><cell>Test SF</cell><cell cols="2">Test Accuracy Test MAE</cell></row><row><cell>181.8</cell><cell>230.4</cell><cell>224.05</cell><cell>389.39</cell><cell></cell><cell></cell><cell></cell></row><row><cell>193.32</cell><cell>287.15</cell><cell>413.183</cell><cell>324.72</cell><cell></cell><cell></cell><cell></cell></row><row><cell>142.17</cell><cell>140.68</cell><cell>200</cell><cell>273.54</cell><cell>6875</cell><cell>28</cell><cell>35.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc>BHLSTM performance on dataset 1 using the piecewise linear RUL.Again, by comparing the results of the BHLSTM in Table1with Table6we can see that the performance of the test set deteriorates.</figDesc><table><row><cell>Train ASE</cell><cell cols="2">Train SF Validation ASE</cell><cell>Validation SF</cell><cell>Test SF</cell><cell cols="2">Test Accuracy Test MAE</cell></row><row><cell>11668.16</cell><cell>1672.75</cell><cell>10300.94</cell><cell>1806.76</cell><cell></cell><cell></cell><cell></cell></row><row><cell>10044.38</cell><cell>1478.14</cell><cell>10413.44</cell><cell>1384.8</cell><cell></cell><cell></cell><cell></cell></row><row><cell>11344.18</cell><cell>1720.16</cell><cell>10288.53</cell><cell>2072.7</cell><cell>616.1</cell><cell>54</cell><cell>14.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc>Comparison between different the performance of different techniques in literature</figDesc><table><row><cell>Technique</cell><cell>Dataset 1 Score</cell><cell>Dataset 1 Accuracy</cell><cell>Dataset 3 Score</cell><cell>Dataset 3 Accuracy</cell></row><row><cell>Multi-Objective DBN Ensemble [38]</cell><cell>334.23</cell><cell>N/A</cell><cell>421.91</cell><cell>N/A</cell></row><row><cell>DBN [38]</cell><cell>417.59</cell><cell>N/A</cell><cell>442.43</cell><cell>N/A</cell></row><row><cell>LASSO [38]</cell><cell>653.85</cell><cell>N/A</cell><cell>1058.36</cell><cell>N/A</cell></row><row><cell>ETR [38]</cell><cell>1667.86</cell><cell>N/A</cell><cell>2240.7</cell><cell>N/A</cell></row><row><cell>KNR [38]</cell><cell>729.32</cell><cell>N/A</cell><cell>1030.29</cell><cell>N/A</cell></row><row><cell>Gradient Boosting [38]</cell><cell>474.01</cell><cell>N/A</cell><cell>576.72</cell><cell>N/A</cell></row><row><cell>SVR [39]</cell><cell>[388, 538]</cell><cell>[54, 64]</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>RULCLIPPER [37]</cell><cell>[310, 440]</cell><cell>[56, 64]</cell><cell>[480, 632]</cell><cell>[55, 63]</cell></row><row><cell>Extreme Learning Machine and Fuzzy clustering [40]</cell><cell>1046</cell><cell>48</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>CNN [41]</cell><cell>1290</cell><cell>N/A</cell><cell>1600</cell><cell>N/A</cell></row><row><cell>Deep LSTM [42]</cell><cell>338</cell><cell>N/A</cell><cell>852</cell><cell>N/A</cell></row><row><cell>TPLS</cell><cell>3914.1</cell><cell>29</cell><cell>3463.8</cell><cell>21</cell></row><row><cell>MPLS</cell><cell>2203.1</cell><cell>36</cell><cell>3648.4</cell><cell>23</cell></row><row><cell>LWPR</cell><cell>1647.8</cell><cell>33</cell><cell>3848.6</cell><cell>23</cell></row><row><cell>BHLSTM</cell><cell>376.64</cell><cell>63</cell><cell>1422</cell><cell>52</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Prognostic modelling options for remaining useful life estimation by industry</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Sikorska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hodkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1803" to="1836" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">State of the art and taxonomy of prognostics approaches , trends of prognostics applications and open issues towards maturity at different technology readiness levels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gouriveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zerhouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="214" to="236" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Challenges, issues, and lessons learned chasing the &apos;Big P&apos;: Real predictive prognostics part 1</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Calvello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Aerospace Conference Proceedings</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="3610" to="3619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prognostic and diagnostic monitoring of complex systems for product lifecycle management: Challenges and opportunities</title>
		<author>
			<persName><forename type="first">V</forename><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Chem. Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1253" to="1263" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Practical options for selecting data-driven or physics-based prognostics algorithms with reviews</title>
		<author>
			<persName><forename type="first">D</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliab. Eng. Syst. Saf</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="223" to="236" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Remaining useful life estimation -A review on the statistical data driven approaches</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">213</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Metrics for Offline Evaluation of Prognostic Performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Celaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Progn. Heal. Manag</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Poceedings of theIEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Convolution Networks for Images, Speech, and Time-Series</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">3361</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>Handb. brain theory neural networks</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Remaining useful life estimation in prognostics using deep convolution neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliab. Eng. Syst. Saf</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A review on time series data mining</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eng. Appl. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="164" to="181" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recent results on key performance indicator oriented fault detection using the DB-KIT toolbox</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IECON 2017 -43rd Annual Conference of the IEEE Industrial Electronics Society</title>
		<meeting>IECON 2017 -43rd Annual Conference of the IEEE Industrial Electronics Society</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7103" to="7108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparison of KPI related fault detection algorithms using a newly developed MATLAB toolbox : DB-KIT</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Industrial Electronics Society, IECON 2016-42nd Annual Conference of the IEEE</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7149" to="7154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Health index-based prognostics for remaining useful life predictions in electrical machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Habibullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nadarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2633" to="2644" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks and Learning Machines, Third</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning Precise Timing with LSTM Recurrent Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Urgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">On the Properties of Neural Machine Translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Encoder-Decoder Approaches</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LSTM: A Search Space Odyssey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A Simple Way to Initialize Recurrent Networks of Rectified Linear Units</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Approach for fault prognosis using recurrent neural network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Manuf</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Remaining useful life estimation of engineered systems using vanilla LSTM neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ensemble of optimized echo state networks for remaining useful life prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rigamonti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Roychoudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">281</biblScope>
			<biblScope unit="page" from="121" to="138" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards End-To-End Speech Recognition with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR Workshop Conf. Proc</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent Neural Networks for Remaining Useful Life Estimation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">O</forename><surname>Heimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PHM 2008. International Conference on</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Prognostics and Health Management</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Intelligent condition-based prediction of machinery reliability</title>
		<author>
			<persName><forename type="first">A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Banjevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K S</forename><surname>Jardine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1600" to="1614" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A novel method using adaptive hidden semi-Markov model for multi-sensor monitoring equipment health prognosis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mech. Syst. Signal Process</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="217" to="232" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On The Algorithmic Implementation of Multiclass Kernel-based Vector Machines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Damage Propagation Modeling for Aircraft Engine Prognostics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Eklund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Prognostics and Health Management</title>
		<meeting>IEEE International Conference on Prognostics and Health Management</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Similarity-Based Prognostics Approach for Engineered Systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Prognostics and Health Management</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluation of neural networks in the subject of prognostics as compared to linear regression model</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Riad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Elminir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Elattar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Eng. Technol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">06</biblScope>
			<biblScope unit="page" from="52" to="58" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Trend time-series modeling and forecasting with neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="808" to="816" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A recurrent neural network based health indicator for remaining useful life prediction of bearings</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="page" from="98" to="109" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Metrics for evaluating performance of prognostic techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Prognostics and Health Management</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Variants of RMSProp and Adagrad with Logarithmic Regret Bounds</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mukkamala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Investigating computational geometry for failure prognostics in presence of imprecise health indicator: Results and comparisons on C-MAPPS datasets</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ramasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Europen confernce of the prognostics and health management society</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multiobjective Deep Belief Networks Ensemble for Remaining Useful Life Estimation in Prognostics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Direct Remaining Useful Life Estimation Based on Support Vector Regression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Khelif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chebel-Morello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Laajili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fnaiech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zerhouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2276" to="2285" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A new multivariate approach for prognostics based on extreme learning machine and fuzzy clustering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gouriveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zerhouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2626" to="2639" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Network Based Regression Approach for Estimation of Remaining Useful Life</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sateesh Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Database Systems for Advanced Applications</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="214" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory Network for Remaining Useful Life estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ristovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farahat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Prognostics and Health Management (ICPHM)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="88" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">RUL prediction using moving trajectories between SVM hyper planes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Galar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fuqing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -Annual Reliability and Maintainability Symposium</title>
		<meeting>-Annual Reliability and Maintainability Symposium</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-maintenance and engineering immune systems: Towards smarter machines and manufacturing systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elmeligy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Control</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
