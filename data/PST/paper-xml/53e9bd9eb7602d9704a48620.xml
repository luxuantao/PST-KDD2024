<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cherry: Checkpointed Early Resource Recycling in Out-of-order Microprocessors £</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">F</forename><surname>Martínez</surname></persName>
							<email>martinez@csl.cornell.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jose</forename><surname>Renau</surname></persName>
							<email>renau@cs.uiuc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
							<email>michael.huang@ece.rochester.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Rochester</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Milos</forename><surname>Prvulovic</surname></persName>
							<email>prvulovi@cs.uiuc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
							<email>torrellas@cs.uiuc.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Systems Laboratory</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cherry: Checkpointed Early Resource Recycling in Out-of-order Microprocessors £</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents CHeckpointed Early Resource RecYcling (Cherry), a hybrid mode of execution based on ROB and checkpointing that decouples resource recycling and instruction retirement. Resources are recycled early, resulting in a more efficient utilization. Cherry relies on state checkpointing and rollback to service exceptions for instructions whose resources have been recycled. Cherry leverages the ROB to (1) not require in-order execution as a fallback mechanism, (2) allow memory replay traps and branch mispredictions without rolling back to the Cherry checkpoint, and (3) quickly fall back to conventional out-of-order execution without rolling back to the checkpoint or flushing the pipeline.</p><p>We present a Cherry implementation with early recycling at three different points of the execution engine: the load queue, the store queue, and the register file. We report average speedups of 1.06 and 1.26 in SPECint and SPECfp applications, respectively, relative to an aggressive conventional architecture. We also describe how Cherry and speculative multithreading can be combined and complement each other.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Modern out-of-order processors typically employ a reorder buffer (ROB) to retire instructions in order <ref type="bibr" target="#b17">[18]</ref>. In-order retirement enables precise bookkeeping of the architectural state, while making out-of-order execution transparent to the user. When, for example, an instruction raises an exception, the ROB continues to retire instructions up to the excepting one. At that point, the processor's architectural state reflects all the updates made by preceding instructions, and none of the updates made by the excepting instruction or its successors. Then, the exception handler is invoked.</p><p>One disadvantage of typical ROB implementations is that individual instructions hold most of the resources that they use until they retire. Examples of such resources are load/store queue entries and physical registers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>. As a result, an instruction that completes early holds on to these resources for a long time, even if it does not need them anymore. Tying up unneeded resources limits performance, as new instructions may find nothing left to allocate.</p><p>To tackle this problem, we propose CHeckpointed Early Resource RecYcling (Cherry). Cherry is a mode of execution that decouples £ This work was supported in part by the National Science Foundation under grants CCR-9970488, EIA-0081307, EIA-0072102, and CHE-0121357; by DARPA under grant F30602-01-C-0078; and by gifts from IBM, Intel, and Hewlett-Packard.</p><p>the recycling of the resources used by an instruction and the retirement of the instruction. Resources are released early and gradually and, as a result, they are utilized more efficiently. For a processor with a given level of resources, Cherry's early recycling can boost the performance; alternatively, Cherry can deliver a given level of performance with fewer resources.</p><p>While Cherry uses the ROB, it also relies on state checkpointing to roll back to a correct architectural state when exceptions arise for instructions whose resources have already been recycled. When this happens, the processor re-executes from the checkpoint in conventional out-of-order mode (non-Cherry mode). At the time the exception re-occurs, the processor handles it precisely. Thus, Cherry supports precise exceptions. Moreover, Cherry uses the cache hierarchy to buffer memory system updates that may have to be undone in case of a rollback; this allows much longer checkpoint intervals than a mechanism limited to a write buffer.</p><p>At the same time, Cherry leverages the ROB to (1) not require inorder execution as a fallback mechanism, (2) allow memory replay traps and branch mispredictions without rolling back to the Cherry checkpoint, and (3) quickly fall back to conventional out-of-order execution without rolling back to the checkpoint or even flushing the pipeline.</p><p>To illustrate the potential of Cherry, we present an implementation on a processor with separate structures for the instruction window, ROB, and register file. We perform early recycling at three key points of the execution engine: the load queue, the store queue, and the register file. To our knowledge, this is the first proposal for early recycling of load/store queue entries in processors with load speculation and replay traps. Overall, this Cherry implementation results in average speedups of 1.06 for SPECint and 1.26 for SPECfp applications, relative to an aggressive conventional architecture with an equal amount of such resources.</p><p>Finally, we discuss how to combine Cherry and Speculative Multithreading (SM) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. These two checkpoint-based techniques complement each other: while Cherry uses potentially unsafe resource recycling to enhance instruction overlap within a thread, SM uses potentially unsafe parallel execution to enhance instruction overlap across threads. We demonstrate how a combined scheme reuses much of the hardware required by either technique.</p><p>This paper is organized as follows: Section 2 describes Cherry in detail; Section 3 explains the three recycling mechanisms used in this work; Section 4 presents our setup to evaluate Cherry; Section 5 shows the evaluation results; Section 6 presents the integration of Cherry and SM; and Section 7 discusses related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CHERRY: CHECKPOINTED EARLY RESOURCE RECYCLING</head><p>The idea behind Cherry is to decouple the recycling of the resources consumed by an instruction and the retirement of the instruction. A Cherry-enabled processor recycles resources as soon as they become unnecessary in the normal course of operation. As a result, resources are utilized more efficiently. Early resource recycling, however, can make it hard for a processor to achieve a consistent architectural state if needed. Consequently, before a processor enters Cherry mode, it makes a checkpoint of its architectural registers in hardware (Section 2.1). This checkpoint may be used to roll back to a consistent state if necessary.</p><p>There are a number of events whose handling requires gathering a precise image of the architectural state. For the most part, these events are memory replay traps, branch mispredictions, exceptions, and interrupts. We can divide these events into two groups:</p><p>The first group consists of memory replay traps and branch mispredictions. A memory replay trap occurs when a load is found to have issued to memory out of order with respect to an older memory operation that overlaps <ref type="bibr" target="#b0">[1]</ref>. When the event is identified, the offending load and all younger instructions are re-executed (Section 3.1.1). A branch misprediction squashes all instructions younger than the branch instruction, after which the processor initiates the fetching of new instructions from the correct path.</p><p>The second group of events comprises exceptions and interrupts. In this paper we use the term exception to refer to any synchronous event that requires the precise architectural state at a particular instruction, such as a division by zero or a page fault. In contrast, we use interrupt to mean asynchronous events, such as I/O or timer interrupts, which are not directly associated with any particular instruction.</p><p>The key aspect that differentiates these two groups is that, while memory replay traps and branch mispredictions are a common, direct consequence of ordinary speculative execution in an aggressive out-of-order processor, interrupts and exceptions are extraordinary events that occur relatively infrequently.</p><p>As a result, the philosophy of Cherry is to allow early recycling of resources only when they are not needed to support (the relatively common) memory replay traps and branch mispredictions. However, recycled resources may be needed to service extraordinary events, in which case the processor restores the checkpointed state and restarts execution from there (Section 2.3.3).</p><p>To restrict resource recycling in this way, we identify a ROB entry as the Point of No Return (PNR). The PNR corresponds to the oldest instruction that can still suffer a memory replay trap or a branch misprediction (Figure <ref type="figure" target="#fig_0">1</ref>). Early resource recycling is allowed only for instructions older than the PNR.</p><p>Instructions that are no older than the PNR are called reversible. In these instructions, when memory replay traps, branch mispredictions, or exceptions occur, they are handled as in a conventional outof-order processor. It is never necessary to roll back to the checkpointed state. In particular, exceptions raised by reversible instructions are precise.</p><p>Instructions that are older than the PNR are called irreversible. Such instructions may or may not have completed their execution. However, some of them may have released their resources. In the event that an irreversible instruction raises an exception, the processor has to roll back to the checkpointed state. Then, the processor executes in conventional out-of-order mode (non-Cherry or normal mode) until the exception re-occurs. When the exception re-occurs, it is handled in a precise manner as in a conventional processor. Then, the processor can return to Cherry mode if desired (Section 2.3.3).</p><p>As for interrupts, because of their asynchronous nature, they are always handled without any rollback. Specifically, processor execu- of No Return (PNR). We assume a circular ROB implementation with Head and Tail pointers <ref type="bibr" target="#b17">[18]</ref>. tion seamlessly falls back to non-Cherry mode (Section 2.1). Then, the interrupt is processed. After that, the processor can return to Cherry mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No Return</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head</head><p>The position of the PNR depends on the particular implementation and the types of resources recycled. Conservatively, the PNR can be set to the oldest of (1) the oldest unresolved branch instruction (Í ), and (2) the oldest memory instruction whose address is still unresolved (Í Å ). Instructions older than oldest´Í Í Å µ are not subject to reply traps or squashing due to branch misprediction.</p><p>If we define Í Ä and Í Ë as the oldest load and store instruction, respectively, whose address is unresolved, the PNR expression becomes oldest´Í Í Ä Í Ë µ. In practice, a more aggressive definition is possible in some implementations (Section 3).</p><p>In the rest of this section, we describe Cherry as follows: first, we introduce the basic operation under Cherry mode; then, we describe needed cache hierarchy support; next, we address events that cause the squash and re-execution of instructions; finally, we examine an important Cherry parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic Operation under Cherry Mode</head><p>Before a processor can enter Cherry mode, a checkpoint of the architectural register state has to be made. A simple support for checkpointing includes a backup register file to keep the checkpointed register state and a retirement map at the head of the ROB. Of course, other designs are possible, including some without a retirement map <ref type="bibr" target="#b22">[23]</ref>.</p><p>With this support, creating a checkpoint involves copying the architectural registers pointed to by the retirement map to the backup register file, either eagerly or lazily. If it is done eagerly, the copying can be done in a series of bursts. For example, if the hardware supports four data transfers per cycle, 32 architectural values can be backed up in eight processor cycles. Note that the backup registers are not designed to be accessed by conventional operations and, therefore, they are simpler and take less silicon than the main physical registers. If the copying is done lazily, the physical registers pointed to by the retirement map are simply tagged. Later, each of them is backed up before it is overwritten.</p><p>While the processor is in Cherry mode, the PNR races ahead of the ROB head (Figure <ref type="figure" target="#fig_0">1</ref>), and early recycling takes place in the irreversible set of instructions. As in non-Cherry mode, the retirement map is updated as usual as instructions retire. Note, however, that the retirement map may point to registers that have already been recycled and used by other instructions. Consequently, the true architectural state is unavailable-but reconstructible, as we explain below.</p><p>Under Cherry mode, the processor boosts the IPC through more efficient resource utilization. However, the processor is subject to exceptions that may cause a costly rollback to the checkpoint. Consequently, we do not keep the processor in Cherry mode indefinitely. Instead, at some point, the processor falls back to non-Cherry mode.</p><p>This can be accomplished by simply freezing the PNR. Once all instructions in the irreversible set have retired, and thus the ROB head has caught up with the PNR, the retirement map reflects the true architectural state. By this time, all the resources that were recycled early would have been recycled in non-Cherry mode too. This collapse step allows the processor to fall back to non-Cherry mode smoothly. Overall, the checkpoint creation, early recycling, and collapse step is called a Cherry cycle.</p><p>Cherry can be used in two ways. One way is to enter Cherry mode only as needed, for example when the utilization of one of the resources (physical register file, load queue, etc.) reaches a certain threshold. This situation may be caused by an event such as a longlatency cache miss. Once the pressure on the resources falls below a second threshold, the processor returns to non-Cherry mode. We call this use on-demand Cherry.</p><p>Another way is to run in Cherry mode continuously. In this case, the processor kepts executing in Cherry mode irrespective of the regime of execution, and early recycling takes place all the time. However, from time to time, the processor needs to take a new checkpoint in order to limit the penalty of an exception in an irreversible instruction. To generate a new checkpoint, we simply freeze the PNR as explained before. Once the collapse step is completed, a new checkpoint is made, and a new Cherry cycle starts. We call this use rolling Cherry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cache Hierarchy Support</head><p>While in Cherry mode, the memory system receives updates that have to be discarded if the processor state is rolled back to the checkpoint. To support long Cherry cycles, we must allow such updates to overflow beyond the processor buffers. To make this possible, we keep all these processor updates within the local cache hierarchy, disallowing the spill of any such updates into main memory. Furthermore, we add one Volatile bit in each line of the local cache hierarchy to mark the updated lines.</p><p>Writes in Cherry mode set the Volatile bit of the cache line that they update. Reads, however, are handled as in non-Cherry mode. 1  Cache lines with the Volatile bit set may not be displaced beyond the outermost level of the local cache hierarchy, e.g. L2 in a two-level structure. Furthermore, upon a write to a cached line that is marked dirty but not Volatile, the original contents of the line are written back to the next level of the memory hierarchy, to enable recovery in case of a rollback. The cache line is then updated, and remains in state dirty (and now Volatile) in the cache.</p><p>If the processor needs to roll back to the checkpoint while in Cherry mode, all cache lines marked Volatile in its local cache hierarchy are gang-invalidated as part of the rollback mechanism. Moreover, all Volatile bits are gang-cleared. On the other hand, if the processor successfully falls back to non-Cherry mode, or if it creates a new checkpoint while in rolling Cherry, we simply gang-clear the Volatile bits in the local cache hierarchy.</p><p>These gang-clear and gang-invalidation operations can be done in a handful of cycles using inexpensive custom circuitry. Figure <ref type="figure" target="#fig_1">2</ref> shows a bit cell that implements one Volatile bit. It consists of a standard 6-T SRAM cell with one additional transistor for gangclear (inside the dashed circle). Assuming a ¼ ½ m TSMC process, 1 Read misses that find the requested line marked Volatile in a lower level of the cache hierarchy also set (inherit) the Volatile bit. This is done to ensure that the lines with updates are correctly identified in a rollback. and using SPICE to extract the capacitance of a line that would gangclear 8Kbits (one bit per 64B cache line in a 512KB cache), we estimate that the gang-clear operation takes 6-10 FO4s <ref type="bibr" target="#b12">[13]</ref>. If the delay of a processor pipeline stage is about 6-8 FO4s, gang-clearing can be performed in about two processor cycles. Gang-invalidation simply invalidates all lines whose Volatile bit is set (by gang-clearing their Valid bits). Finally, we consider the case of a cache miss in Cherry mode that cannot be serviced due to lack of evictable cache lines (all lines in the set are marked Volatile). In general, if no space can be allocated, the application must roll back to the checkpoint. To prevent this from hapenning too often, one may bound the length of a Cherry cycle, after which a new checkpoint is created. However, a more flexible solution is to include a fully associative victim cache in the local cache hierarchy, that accommodates evicted lines marked Volatile. When the number of Volatile lines in the victim cache exceeds a certain threshold, an interrupt-like signal is sent to the processor. As with true interrupts (Section 2.3.2), the processor proceeds with a collapse step and, once in non-Cherry mode, gang-clears all Volatile bits. Then, a new Cherry cycle may begin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Squash and Re-Execution of Instructions</head><p>The main events that cause the squash and possible re-execution of in-flight instructions are memory replay traps, branch mispredictions, interrupts, and exceptions. We consider how each one is handled in Cherry mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Memory Replay Traps and Branch Mispredictions</head><p>Only instructions in the reversible set (Figure <ref type="figure" target="#fig_0">1</ref>) may be squashed due to memory replay traps or branch mispredictions. Since resources have not yet been recycled in the reversible set, these events can be handled in Cherry mode conventionally. Specifically, in a replay trap, the offending load and all the subsequent instructions are replayed; in a branch misprediction, the instructions in the wrong path are squashed and the correct path is fetched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Interrupts</head><p>Upon receiving an interrupt while in Cherry mode, the hardware automatically initiates the transition to non-Cherry mode by entering a collapse step. Once non-Cherry mode is reached, the processor handles the interrupt as usual.</p><p>Note that Cherry handles interrupts without rolling back to the checkpoint. The only difference with respect to a conventional processor is that the interrupt may have a slightly higher response time. Depending on the application, we estimate the increase in the response time to range from tens to a few hundred nanoseconds. Such an increase is tolerable for typical asynchronous interrupts. In the unlikely scenario that this extra latency is not acceptable, the simplest solution is to disable Cherry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Exceptions</head><p>A processor running in Cherry mode handles all exceptions precisely. An exception is processed differently depending on whether it occurs on a reversible or an irreversible instruction (Figure <ref type="figure" target="#fig_0">1</ref>). When it occurs on a reversible one, the corresponding ROB entry is marked. If the instruction is squashed before the PNR gets to it (e.g. it is in the wrong path of a branch), the (false) exception will have no bearing on Cherry. If, instead, the PNR reaches that ROB entry while it is still marked, the processor proceeds to exit Cherry mode (Section 2.1): the PNR is frozen and, as execution proceeds, the ROB head eventually catches up with the PNR. At that point, the processor is back to non-Cherry mode and, since the excepting instruction is at the ROB head, the appropriate handler is invoked.</p><p>If the exception occurs on an irreversible instruction, the hardware automatically rolls back to the checkpointed state and restarts execution from there in non-Cherry mode. Rolling back to the checkpointed state involves aborting any outstanding memory operations, gang-invalidating all cache lines marked Volatile, gangclearing all Volatile bits, restoring the backup register file, and starting to fetch instructions from the checkpoint. The processor executes in conventional out-of-order mode (non-Cherry mode) until the exception re-occurs. At that point, the exception is processed normally, after which the processor can re-enter Cherry mode.</p><p>It is possible that the exception does not re-occur. This may be the case, for example, for page faults in a shared-memory multiprocessor environment. Consequently, we limit the number of instructions that the processor executes in non-Cherry mode before returning to Cherry mode. One could remember the instruction that caused the exception and only re-execute in non-Cherry mode until such instruction retires. However, a simpler, conservative solution that we use is to remain in non-Cherry mode until we retire the number of instructions that are executed in a Cherry cycle. Section 2.4 discusses the optimal size of a Cherry cycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4">OS and Multiprogramming Issues</head><p>Given that the operating system performs I/O mapped updates and other hard-to-undo operations, it is advisable not to use Cherry mode while in the OS kernel. Consequently, system calls and other entries to the kernel automatically exit Cherry mode.</p><p>However, Cherry blends well with context switching and multiprogramming. If a timer interrupt mandates a context switch for a process, the processor bails out of Cherry mode as described in Section 2.3.2, after which the resident process can be preempted safely. If it is the process who yields the CPU (e.g. due to a blocking semaphore), the system call itself exits Cherry mode, as described above. In no case is a rollback to the checkpoint necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Optimal Size of a Cherry Cycle</head><p>The size of a Cherry cycle is crucial to the performance of Cherry.</p><p>In what follows, we denote by Ì the duration of a Cherry cycle ignoring any Cherry overheads. For Cherry to work well, Ì has to be within a range. If Ì is too short, performance is hurt by the overhead of the checkpointing and the collapse step. If, instead, Ì is too long, both the probability of suffering an exception within a Cherry cycle and the cost of a rollback are high. The optimal Ì is found somewhere in between these two opposing conditions. We now show how to find the optimal Ì . For simplicity, in the following discussion we assume that the IPC in Cherry and non-Cherry mode stays constant at ×¡IPC and IPC, respectively, where × denotes the average overhead-free speedup delivered by the Cherry mode.</p><p>We can express the per-Cherry-cycle overhead Ì Ó of running in Cherry mode as:</p><formula xml:id="formula_0">Ì Ó • Ô<label>(1)</label></formula><p>where is the overhead caused by checkpointing and by the reduced performance experienced in the subsequent collapse step, Ô is the probability of suffering a rollback-causing exception in a Cherry cycle, and is the cost of suffering such an exception. If exceptions occur every Ì cycles, with Ì Ì , we can rewrite:</p><formula xml:id="formula_1">Ô Ì Ì<label>(2)</label></formula><p>Notice that the expression for Ô is conservative, since it assumes that all exceptions cause rollbacks. In reality, only exceptions triggered by instructions in the irreversible set cause the processor to roll back, and thus the actual Ô would be lower.</p><p>To calculate the cost of suffering such an exception, we assume that when exceptions arrive, they do so half way into a Cherry cycle. In this case, the cost consists of re-executing half Cherry cycle at non-Cherry speed, plus the incremental overhead of executing (for the first time) another half Cherry cycle at non-Cherry speed rather than at Cherry speed. Recall that, after suffering an exception, we execute the instructions of one full Cherry cycle in non-Cherry mode (Section 2.3.3). Consequently:</p><formula xml:id="formula_2">× Ì ¾ • ´× ½µ Ì ¾<label>(3)</label></formula><p>The optimal Ì is the one that minimizes Ì Ó Ì . Substituting Equation 2 and Equation 3 in Equation <ref type="formula" target="#formula_0">1</ref>, and dividing by Ì yields:</p><formula xml:id="formula_3">Ì Ó Ì Ì • × ½ ¾ Ì Ì<label>(4)</label></formula><p>This expression finds a minimum in:</p><formula xml:id="formula_4">Ì × Ì × ½ ¾ (5)</formula><p>Figure <ref type="figure" target="#fig_2">3</ref> plots the relative overhead Ì Ó Ì against the duration Ì of an overhead-free Cherry cycle (Equation <ref type="formula" target="#formula_3">4</ref>). For that experiment, we borrow from our evaluation section (Section 5): 3.2GHz processor, ½ ns (60 cycles), and × ½ ¼ . Then, we plot curves for duration of interval between exceptions Ì ranging from ¾¼¼ s to ½¼¼¼ s. The minimum in each curve yields the optimal Ì (Equation <ref type="formula">5</ref>). As we can see from the figure, for the parameters assumed, the optimal Ì hovers around a few microseconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EARLY RESOURCE RECYCLING</head><p>To illustrate Cherry, we implement early recycling in the load/store unit (Section 3.1) and register file (Section 3.2). Early recycling could be applied to other resources as well. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Load/Store Unit</head><p>Typical load/store units comprise one reorder queue for loads and one for stores <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>. Either reorder queue may become a performance bottleneck if it fills up. In this section we first discuss a conventional design of the queues, and then we propose a new mechanism for early recycling of load/store queue entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Conventional Design</head><p>The processor assigns a Load Queue (LQ) entry to every load instruction in program order, as the instruction undergoes renaming. The entry initially contains the destination register. As the load executes, it fills its LQ entry with the appropriate physical address and issues the memory access. When the data are obtained from the memory system, they are passed to the destination register. Finally, when the load finishes and reaches the head of the ROB, the load instruction retires. At that point, the LQ entry is recycled.</p><p>Similarly, the processor assigns Store Queue (SQ) entries to every store instruction in program order at the renaming stage. As the store executes, it generates the physical address and the data value, which are stored in the corresponding SQ entry. An entry whose address and data are still unknown is said to be empty. When both address and data are known, and the corresponding store instruction reaches the head of the ROB, the update is sent to the memory system. At that point, the store retires and the SQ entry is recycled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Address Disambiguation and Load-Load Replay</head><p>At the time a load generates its address, a disambiguation step is performed by comparing its physical address against that of older SQ entries. If a fully overlapping entry is found, and the data in the SQ entry are ready, the data are forwarded to the load directly. However, if the accesses fully overlap but the data are still missing, or if the accesses are only partially overlapping, the load is rejected, to be dispatched again after a number of cycles. Finally, if no overlapping store exists in the SQ that is older than the load, the load requests the data from memory at once.</p><p>The physical address is also compared against newer LQ entries. If an overlapping entry is found, the newer load and all its subsequent instructions are replayed, to eliminate the chance of an intervening store by another device causing an inconsistency.</p><p>This last event, called load-load replay trap, is meaningful only in environments where more than one device can be accessing the same memory region simultaneously, as in multiprocessors. In uniproces-sor environments, such a situation could potentially be caused by processor and DMA accesses. However, in practice, it does not occur: the operating system ensures mutual exclusion of processor and DMA accesses by locking memory pages as needed. Consequently, load-load replay support is typically not necessary in uniprocessors.</p><p>Figure <ref type="figure" target="#fig_3">4</ref>(a) shows an example of a load address disambiguation and a check for possible load-load replay traps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Store-Load Replay</head><p>Once the physical address of a store is resolved, it is compared against newer entries in the LQ. The goal is to detect any exposed load, namely a newer load whose address overlaps with that of the store, without an intervening store that fully covers the load. Such a load has consumed data prematurely, either from memory or from an earlier store. Thus, the load and all instructions following it are aborted and replayed. This mechanism is called store-load replay trap <ref type="bibr" target="#b0">[1]</ref>.</p><p>Figure <ref type="figure" target="#fig_3">4</ref>(b) shows an example of a check for possible store-load replay traps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Design with Early Recycling</head><p>Following Cherry's philosophy, we want to release LQ and SQ entries as early as it is possible to do so. In this section, we describe the conditions under which this is the case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimized LQ</head><p>As explained before, a LQ entry may trigger a replay trap if an older store (or older load in multiprocessors) resolves to an overlapping address. When we release a LQ entry, we lose the ability to compare against its address (Figure <ref type="figure" target="#fig_3">4</ref>). Consequently, we can only release a LQ entry when such a comparison is no longer needed because no replay trap can be triggered.</p><p>To determine whether or not a LQ entry is needed to trigger a replay trap, we use the Í Ä and Í Ë pointers to the ROB (Section 2). Any load that is older than Í Ë cannot trigger a store-load replay trap, since the physical addresses of all older stores are already known.</p><p>Furthermore, any load that is older than Í Ä cannot trigger a loadload replay trap, because the addresses of all older loads are already known.</p><p>In a typical uniprocessor environment, only store-load replay traps are relevant. Consequently, as the Í Ë moves in the ROB, any loads that are older than Í Ë release their LQ entry. In a multiprocessor or other multiple-master environment, both store-load and loadload replay traps are relevant. Therefore, as the Í Ë and Í Ä move in the ROB, any loads that are older than oldest´Í Ä Í Ë µ release their LQ entry. <ref type="foot" target="#foot_0">2</ref> To keep the LQ simple, LQ entries are released in order.</p><p>Early recycling of LQ entries is not limited by Í ; it is fine for load instructions whose entry has been recycled to be subject to branch mispredictions. Moreover, it is possible to service exceptions and interrupts inside the irreversible set without needing a rollback. This is because LQ entries that are no longer needed to detect possible replay traps can be safely recycled without creating a side effect. In light of an exception or interrupt, the recycling of such a LQ entry does not alter the processor state needed to service that exception or interrupt. Section 3.3 discusses how this blends in a Cherry implementation with recycling of other resources. Since LQ entries are recycled early, we partition the LQ into two structures. The first one is called Load Reorder Queue (LRQ), and supports the address checking functionality of the conventional LQ. Its entries are assigned in program order at renaming, and recycled according to the algorithm just described. Each entry contains the address of a load.</p><p>The second structure is called Load Data Queue (LDQ), and supports the memory access functionality of the conventional LQ. LDQ entries are assigned as load instructions begin execution, and are recycled as soon as the data arrive from memory and are delivered to the appropriate destination register (which the entry points to). Because of their relatively short-lived nature, it is reasonable to assume that the LDQ does not become a bottleneck as we optimize the LRQ. LDQ entries are not assigned in program order, but the LDQ must be addressable by transaction id, so that the entry can be found when the data come back from memory.</p><p>Finally, note that, even for a load that has had its LRQ entry recycled, address checking proceeds as usual. Specifically, when its address is finally known, it is compared against older stores for possible data forwarding. This case only happens in uniprocessors, since in multiprocessors the PNR for LQ entries depends on oldest´Í Ä Í Ë µ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimized SQ</head><p>When we release a SQ entry, we must send the store to the memory system. Consequently, we can only release a SQ entry when the old value in the memory system is no longer needed. For the latter to be true, it is sufficient that: (1) no older load is pending address disambiguation, (2) no older load is subject to replay traps, and (3) the store is not subject to squash due to branch misprediction. Condition <ref type="bibr" target="#b0">(1)</ref> means that all older loads have already generated their address and, therefore, located their "supplier", whether it is memory or an older store. If it is memory, recall that load requests are sent to memory as soon as their addresses are known. Therefore, if our store is older than Í Ä , it is guaranteed that all older loads that need to fetch their data from memory have already done so. Condition <ref type="bibr" target="#b1">(2)</ref> implies that all older loads are older than oldest´Í Ë µ (typical uniprocessor) or oldest´Í Ä Í Ë µ (multiprocessor or other multiple-master system), as discussed above. Finally, condition (3) implies that the store itself is older than Í . Therefore, all conditions are met if the store itself is older than oldest´Í Ä Í Ë Í µ. <ref type="foot" target="#foot_1">3</ref>There are two additional implementation issues related to accesses to overlapping addresses. They are relevant when we send a store to the memory system and recycle its SQ entry. First, we would have to compare its address against all older entries in the SQ to ensure that stores to overlapping addresses are sent to the cache in program order. To simplify the hardware, we eliminate the need for such a comparison by simply sending the updates to the memory system (and recycling the SQ entries) in program order. In this case, in-order updates to overlapping addresses are automatically enforced.</p><p>Second, note that a store is not sent to the memory system until all previous loads have been resolved (store is older than Í Ä ). One such load may be to an address that overlaps with that of the store.</p><p>Recall that loads are sent to memory as soon as their addresses are known. The LRQ entry for the load may even be recycled. This case is perfectly safe if the cache system can ensure the ordering of the accesses. This can be implemented in a variety of ways (queue at the MSHR, reject store, etc.), whose detailed implementation is out of the scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Register File</head><p>The register file may become a performance bottleneck if the processor runs out of physical registers. In this section, we briefly discuss a conventional design of the register file, and then propose a mechanism for early recycling of registers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Conventional Design</head><p>In our design, we use a register map at the renaming stage of the pipeline and one at the retirement stage. At renaming, instructions pick one available physical register as the destination for their operation and update the renaming map accordingly. Similarly, when the instruction retires, it updates the retirement map to reflect the architectural state immediately after the instruction. Typically, the retirement map is used to support precise exception handling: when an instruction raises an exception, the processor waits until such instruction reaches the ROB head, at which point the processor has a precise image of the architectural state before the exception. Physical registers holding architectural values are recycled when a retiring instruction updates the retirement map to point away from them. Thus, once a physical register is allocated at renaming for an instruction, it remains "pinned" until a subsequent instruction supersedes it at retirement. However, a register may become dead much earlier: as soon as it is superseded at renaming, and all its consumer instructions have read its value. From this moment, and until the superseding instruction retires, the register remains pinned in case the superseding instruction is rolled back for whatever reason, e.g. due to branch misprediction or exception. This effect causes a suboptimal utilization of the register file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Design with Early Recycling</head><p>Following Cherry's philosophy, we recycle dead registers as soon as possible, so that they can be reallocated by new instructions. However, we again need to rely on checkpointing to revert to a correct state in case of an exception in an instruction in the irreversible set.</p><p>We recycle a register when the following two conditions hold. First, the instruction that produces the register and all those that consume it must be (1) executed and (2) both free of replay traps and not subject to branch mispredictions. The latter implies that they are older than oldest´Í Ë Í µ (typical uniprocessor) or oldest´Í Ä Í Ë Í µ (multiprocessor or other multiple-master system), as discussed above.</p><p>The second condition is that the instruction that supersedes the register is not subject to branch mispredictions (older than Í ).</p><p>Squashing such an instruction due to a branch misprediction would have the undesirable effect of reviving the superseded register. Notice, however, that the instruction can harmlessly be re-executed due to a memory replay trap, and thus ordering constraints around Í Ä Í Ë are unnecessary. In practice, to simplify the implementation, we also require that the instruction that supersedes the register be older than oldest´Í Ë Í µ (typical uniprocessor) or oldest´Í Ä Í Ë Í µ (multiprocessor or other multiple-master system).</p><p>In our implementation, we augment every physical register with a Superseded bit and a Pending count. This support is similar to <ref type="bibr" target="#b16">[17]</ref>. The Superseded bit marks whether the instruction that supersedes the register is older than oldest´Í Ë Í µ (or oldest´Í Ä Í Ë Í µ in multiprocessors), which implies that so are all consumers. The Pending count records how many instructions among the consumers and producer of this register are older than oldest´Í Ë Í µ (or oldest´Í Ä Í Ë Í µ in multiprocessors) and have not yet completed execution. A physical register can be recycled only when the Superseded bit is set and the Pending count is zero. Finally, we also assume that instructions in the ROB keep, as part of their state, a pointer to the physical register that their execution supersedes. This support exists in the MIPS R10000 processor <ref type="bibr" target="#b22">[23]</ref>.</p><p>As an instruction goes past oldest´Í Ë Í µ (or oldest´Í Ä Í Ë Í µ in multiprocessors), the proposed new bits in the register file are acted upon as follows: (1) If the instruction has not finished execution, the Pending count of every source and destination register is incremented; (2) irrespective of whether the instruction has finished execution, the Superseded bit of the superseded register, if any, is set; (3) if the superseded register has both a set Superseded bit and a zero Pending count, the register is added to the free list.</p><p>Additionally, every time that an instruction past oldest´Í Ë Í µ (or oldest´Í Ä Í Ë Í µ in multiprocessors), finishes executing, it decrements the Pending count of its source and destination registers. If the Pending count of a register reaches zero and its Superseded bit is set, that register is added to the free list.</p><p>Overall, in Cherry mode, register recycling occurs before the retirement stage. Note that, upon a collapse step, the processor seamlessly switches from Cherry to non-Cherry register recycling. This is because, at the time the irreversible set is fully collapsed, all early recycled registers in Cherry (and only those) would have also been recycled in non-Cherry mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Putting It All Together</head><p>In this section we have applied Cherry's early recycling approach to three different types of resources: LQ entries, SQ entries, and registers. When considered separately, each resource defines its own PNR and irreversible set. Table <ref type="table" target="#tab_0">1</ref> shows the PNR for each of these three resources.</p><p>When combining early recycling of several resources, we define  the dominating PNR as the one which is farthest from the ROB head at each point in time. Exceptions on instructions older than that PNR typically require a rollback to the checkpoint; exceptions on instructions newer than that PNR can simply trigger a collapse step so that the processor falls back to non-Cherry mode.</p><p>However, it is important to note that our proposal for early recycling of LQ entries is a special case: it guarantees precise handling of extraordinary events even when they occur within the irreversible set (Section 3.1.2). As a result, the PNR for LQ entries need not be taken into account when determining the dominating PNR. Thus, for a Cherry processor with recycling at these three points, the dominating PNR is the newest of the PNRs for SQ entries and for registers. In a collapse step, the dominating PNR is the one that freezes until the ROB head catches up with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION SETUP Simulated Architecture</head><p>We evaluate Cherry using execution-driven simulations with a detailed model of a state-of-the-art processor and its memory subsystem. The baseline processor modeled is an eight-issue dynamic superscalar running at 3.2GHz that has two levels of on-chip caches. The details of the Baseline architecture modeled are shown in Table 2. In our simulations, the latency and occupancy of the structures in the processor pipeline, caches, bus, and memory are modeled in detail.  The processor has separate structures for the ROB, instruction window, and register file. When an instruction is issued, it is placed in both the instruction window and the ROB. Later, when all the input operands are available, the instruction is dispatched to the functional units and is removed from the instruction window.</p><p>In our simulations, we break down the execution time based on the reason why, for each issue slot in each cycle, the opportunity to insert a useful instruction into the instruction window is missed (or not). If, for a particular issue slot, an instruction is inserted into the instruction window, and that instruction eventually graduates, that slot is counted as busy. If, instead, an instruction is available but is not inserted in the instruction window because a necessary resource is unavailable, the missed opportunity is attributed to such a resource. Example of such resources are load queue entry, store queue entry, register, or instruction window entry. Finally, instructions from mispredicted paths and other overheads are accounted for separately.</p><p>We also simulate four enhanced configurations of the Baseline architecture: Base2, Base3, Base4, and Limit. Going from Baseline to Base2, we simply add 32 load queue entries, 32 store queue entries, 32 integer registers, and 32 FP registers. The same occurs as we go from Base2 to Base3, and from Base3 to Base4. Limit has an unlimited number of load/store queue entries and integer/FP registers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cherry Architecture</head><p>We simulate the Baseline processor with Cherry support (Cherry). We estimate the cost of checkpointing the architectural registers to be 8 cycles. Moreover, we use simulations to derive an average overhead of 52 cycles for a collapse step. Consequently, becomes 60 cycles. If we set the duration of an overhead-free Cherry cycle (Ì ) to 5 s, the overhead becomes negligible. Under these conditions, equation 4 yields a total relative overhead (Ì Ó Ì ) of at most one percent, if the separation between exceptions (Ì ) is 448 s or more. Note that, in equation 4, we use an average overhead-free Cherry speedup (×) of 1.06. This number is what we obtain for SPECint applications in Section 5. In our evaluation, however, we do not model exceptions. Neglecting them does not introduce significant inaccuracy, given that we simulate applications in steady state, where page faults are infrequent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applications</head><p>We evaluate Cherry using most of the applications of the SPEC CPU2000 suite <ref type="bibr" target="#b4">[5]</ref>. The first column of Table <ref type="table" target="#tab_4">3</ref> in Section 5.1 lists these. Some applications from the suite are missing; this is due to limitations in our simulation infrastructure. For these applications, it is generally too time-consuming to simulate the reference input set to completion. Consequently, in all applications, we skip the initialization, and then simulate 750 million instructions. If we cannot identify the initialization code, we skip the first 500 million instructions before collecting statistics. The applications are compiled with -O2 using the native SGI MIPSPro compiler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall Performance</head><p>Figures <ref type="figure">5 and 6</ref> show the speedups obtained by the Cherry, Base2, Base3, Base4, and Limit configurations over the Baseline system. The figures correspond to the SPECint and SPECfp applications, respectively, that we study. For each application, we show two bars. The leftmost one (R) uses the realistic branch prediction scheme of Table <ref type="table" target="#tab_2">2</ref>. The rightmost one (P) uses perfect branch prediction for both the advanced and Baseline systems. Note that, even is this case, Cherry uses Í .</p><p>The figures show that Cherry yields speedups across most of the applications. The speedups are more modest in SPECint applications, where Cherry's average performance is between that of Base2 and Base3. For SPECfp applications, the speedups are higher. In this case, the average performance of Cherry is close to that of Base4 and Limit. Overall, with the realistic branch prediction, the average speedup of Cherry on SPECint and SPECfp applications is 1.06 and 1.26, respectively.</p><p>If we compare the bars with realistic and perfect branch prediction, we see that some SPECint applications experience significantly higher speedups when branch prediction is perfect. This is the case for both Cherry and enhanced non-Cherry configurations. The reason is that an increase in available resources through early recycling (Cherry) or by simply adding more resources (Base2 to Base4 and Limit) increases performance when these resources are successfully re-utilized by instructions that would otherwise wait. Thus, if branch prediction is poor, most of these extra resources are in fact wasted by speculative instructions whose execution is ultimately moot. In perlbmk, for example, the higher speedups attained when all configurations (including Baseline) operate with perfect branch prediction is due to better resource utilization. On the other hand, SPECfp applications are largely insensitive to this effect, since branch prediction is already very successful in the realistic setup.</p><p>In general, the gains of Cherry come from recycling resources. To understand the degree of recycling, Table <ref type="table" target="#tab_4">3</ref> characterizes the irreversible set and other related Cherry parameters. The data corresponds to realistic branch prediction. Specifically, the second column shows the average fraction of ROB entries that are used. The next three columns show the size of the irreversible set, given as a fraction of the used ROB. Recall that the irreversible set is the distance between the PNR and the ROB head (Figure <ref type="figure" target="#fig_0">1</ref>). Since the irreversible set depends on the resource being recycled, we give separate numbers for register, LQ entry, and SQ entry recycling. As indicated in Section 3.3, the PNR in uniprocessors is oldest´Í Ë Í µ for registers, Í Ë for LQ entries, and oldest´Í Ä Í Ë Í µ for SQ entries.</p><p>Finally, the last column shows the average duration of the collapse step. Recall from Section 3.3 that it involves identifying the newest of the PNR for registers and for SQ entries, and freezing it until the ROB head catches up with it.  Consider the SPECint applications first. The irreversible set for the LQ entries is very large. Its average size is about 77% of the used ROB. This shows that Í Ë moves far ahead of the ROB head. On the other hand, the irreversible set for the registers is much smaller. Its average size is about 29% of the used ROB. This means that oldest´Í Ë Í µ is not far from the ROB head. Consequently, Í Figure <ref type="figure">5</ref>: Speedups delivered by the Cherry, Base2, Base3, Base4, and Limit configurations over the Baseline system, for the SPECint applications that we study. For each application, the R and P bars correspond to realistic and perfect branch prediction, respectively.</p><formula xml:id="formula_5">P Cherry Limit Base4 Base3 Base2<label>1.0 1.1 1.2 1.3 1.4 1.5 1.6 Speedup applu R P apsi R P art R P equake R P mesa R P mgrid R P swim R P wupwise R P Average R</label></formula><p>Figure <ref type="figure">6</ref>: Speedups delivered by the Cherry, Base2, Base3, Base4, and Limit configurations over the Baseline system, for the SPECfp applications that we study. For each application, the R and P bars correspond to realistic and perfect branch prediction, respectively.</p><p>is the pointer that keeps the PNR from advancing. In these applications, branch conditions often depend on long-latency instructions and, as a result, they remain unresolved for a while. Finally, the irreversible set for the SQ entries is even smaller. Its average size is about 20%. In this case, PNR is given by oldest´Í Ä Í Ë Í µ and it shows that Í Ä further slows down the move of the PNR. In these applications, load addresses often depend on long-latency instructions too.</p><p>In contrast, SPECfp applications have fewer conditional branches and they are resolved faster. Furthermore, load addresses follow a more regular pattern and are also resolved earlier. As a consequence, the PNRs for register and SQ entries move far ahead of the ROB head. The result is that Cherry delivers a much higher speedup for SPECfp applications (Figure <ref type="figure">6</ref>) that for SPECint (Figure <ref type="figure">5</ref>).</p><p>We note that the larger irreversible sets for the SPECfp applications imply a higher cost for the collapse step. Specifically, the average collapse step goes from 167 to 528 cycles as we go from SPECint to SPECfp applications. A long collapse step increases the term in Equation <ref type="formula" target="#formula_3">4</ref>, which forces Ì to be longer. The Baseline bars show that of the three potential bottlenecks targeted in this paper, the LQ is by far the most serious one. Lack of LQ entries causes a large stall in SPECint and, especially, SPECfp applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Contribution of Resources</head><p>Our proposal of early recycling of LQ entries is effective in both the SPECint and SPECfp applications. Our optimization reduces most of the LQ stall. It unleashes extra ILP, which in turn puts more pressure on the SQ, register file, and other resources. Even though Cherry does recycle some SQ entries and physical registers, the net effect of our oprimizations is an increased level of saturation on these two resources for both SPECint and SPECfp applications.</p><p>One reason why Cherry is not as effective in recycling SQ entries and registers is that their PNRs are constrained by more conditions. Indeed, the PNR for registers is oldest´Í Ë Í µ, while the one for SQ entries is oldest´Í Ä Í Ë Í µ. In particular, Í limits the impact of Cherry noticeably.</p><p>Overall, to enhance the impact of Cherry, we can improve in two different ways. First, we can design techniques to advance the PNR for SQ entries and registers more aggressively. However, this may increase the risk of a rollback. Second, recycling within the current irreversible set can be done more aggressively. This adds complexity, and may also increase the risk of rollbacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Resource Utilization</head><p>To gain a better insight into the performance results of Cherry, we measure the usage of each of the targeted resources. Figure <ref type="figure" target="#fig_10">9</ref> shows cumulative distributions of usage for each of the resources. From top to bottom, the charts refer to LQ entries, SQ entries, integer registers, and floating-point registers. In each chart, the horizontal axis is   the cumulative percentage of time that a resource is allocated below the level shown in the vertical axis. Each chart shows the distribution for the Baseline, Limit, and two Cherry configurations. The latter correspond to the real number of allocated entries (CherryR) and the effective number of allocated entries (CherryE). The effective entries include both the entries that are actually occupied and those that would have been occupied had they not been recycled. The difference between CherryE and CherryR shows how effective Cherry is in recycling a given resource. Finally, the area under each curve is proportional to the average usage of a resource.</p><p>The top row of Figure <ref type="figure" target="#fig_10">9</ref> shows that LQ entry recycling is very effective. Under Baseline, the LQ is full about 45% and 65% of the time in SPECint and SPECfp applications, respectively. With Cherry, in more than half of the time, all the LQ entries are recycled. We see that the LQ is almost full less than 15% of the time. Moreover, the effective number of LQ entries is significantly larger than the actual size of the LQ.</p><p>The second row of Figure <ref type="figure" target="#fig_10">9</ref> shows that, as expected, the recycling of SQ entries is less effective. In SPECint applications, the effective size of the SQ under Cherry surpasses the actual size of that resource significantly in only 6% of the time. However, the potential demand for SQ entries (in the Limit configuration) is much larger. The situation in SPECfp applications is slightly different. The SQ entries are recycled somewhat more effectively.</p><p>The last two rows of Figure <ref type="figure" target="#fig_10">9</ref> show the usage of integer (third row) and floating-point (bottom row) registers. In the SPECint applications, the recycling of registers is not very effective. The reason for this is the same as for SQ entries: the PNR is unable to sufficiently advance to permit effective recycling. In contrast, the PNR advances quite effectively in SPECfp applications. The resulting degree of register recycling is very good. Indeed, the effective number of integer registers approaches the potential demand. The potential demand for floating-point registers is larger and is difficult to meet. However, the effective number of floating-point registers in Cherry is larger than the actual size of the register file 50% of the time. In particular, it is more than twice the actual size of the register file 15% of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">COMBINING CHERRY AND SPECULATIVE MULTITHREADING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Similarities and Differences</head><p>Speculative multithreading (SM) is a technique where several tasks are extracted from a sequential code and executed speculatively in parallel <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Value updates by speculative threads are buffered, typically in caches. If a cross-thread dependence violation is detected, updates are discarded and the speculative thread is rolled back to a safe state. The existence of at least one safe thread at all times guarantees forward progress. As safe threads finish execution, they propagate their nonspeculative status to successor threads. Cherry and SM are complementary techniques: while Cherry uses potentially unsafe resource recycling to enhance instruction overlap within a thread, SM uses potentially unsafe parallel execution to enhance instruction overlap across threads. Furthermore, Cherry and SM share much of their hardware requirements. Consequently, combining these two schemes becomes an interesting option.</p><p>Cherry and SM share two important primitives. The first one is support to checkpoint the processor's architectural state before entering unsafe execution, and to roll back to it if the program state becomes inconsistent. The second primitive consists of support to buffer unsafe memory state in the caches, and either merge it with the memory state when validated, or invalidate it if proven corrupted.</p><p>Naturally, both SM and Cherry have additional requirements of their own. SM often tags cached data and accesses with a thread ID, which identifies the owner or originator thread. Furthermore, SM needs hardware or software to check for cross-thread dependence violations. On the other hand, Cherry needs support to recycle load/store queue entries and registers, and to maintain the PNR pointer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Combined Scheme</head><p>In a processor that supports both SM and Cherry execution, we propose to exploit both schemes by enabling/disabling speculative execution and Cherry mode in lockstep. Specifically, as a thread becomes speculative, it also enters Cherry mode, and when it successfully completes the speculative section, it also completes the Cherry cycle. Moreover, if speculation is aborted, so is the Cherry cycle, and vice versa. We now show that this approach has the advantage of reusing hardware support.</p><p>Enabling and disabling the two schemes in lockstep reuses the checkpoint and the cache support. Indeed, a single checkpoint is required when the thread enters both speculative execution and Cherry mode at once. As for cache support, SM typically tags each cache line with a Read and Write bit which, roughly speaking, are set when the speculative thread reads or writes the line, respectively. On the other hand, Cherry tags cache lines with the Volatile bit, which is set when the thread writes the line. Consequently, the Write and Volatile bits can be combined into one.</p><p>With such support, when the thread is speculative, any write sets the Write/Volatile bit. When the thread becomes nonspeculative, all Read bits in the cache are gang-cleared. Then, the processor exits Cherry mode and also gang-clears all Write/Volatile bits.</p><p>Special consideration has to be given to cache overflow situations. Under SM alone, the speculative thread stalls when the cache is about to overflow. Under Cherry mode alone, an interrupt informs the processor when the number of Volatile lines in the victim cache exceeds a certain threshold. This advance notice allows the processor to return to non-Cherry mode immediately without overflowing the cache. When we combine both SM and Cherry mode, we stall the processor as soon as the advance notice is received. When the thread later becomes nonspeculative, the thread can resume and immediately return to non-Cherry. Thanks to stalling when the advance notice was received, there is still some room in the cache for the thread to complete the Cherry cycle and not overflow. This strategy is likely to avoid an expensive rollback to the checkpoint.</p><p>Another consideration related to the previous one is the treatment of the advance warning interrupt when combining Cherry and SM. Note that the advance notice interrupt in Cherry requires no special handling. Indeed, any interrupt triggers the ending of the current Cherry-the advance warning interrupt is special only in that it is signaled when the cache is nearly full. However, when Cherry and SM are combined, the advance warning interrupt has to be recognized as such, so that the stall can be performed before the processor's interrupt handling logic can react to it. This differs from the way other interrupts are handled in SM, where interrupts are typically handled by squashing the speculative thread and responding to the interrupt immediately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Our work combines register checkpointing and reorder buffer (ROB) to allow precise exceptions, fast handling of frequent instruction replay events, and recycling of load and store queue entries and registers. Previous related work can be divided into the following four categories.</p><p>The first category includes work on precise exception handling. Hwu and Patt <ref type="bibr" target="#b6">[7]</ref> use checkpointing to support precise exceptions in out-of-order processors. On an exception, the processor rolls back to the checkpoint, and then executes code in order until the excepting instruction is met. Smith and Pleszkun <ref type="bibr" target="#b17">[18]</ref> discuss several methods to support precise exceptions. The Reorder Buffer (ROB) and the History Buffer are presented, among other techniques.</p><p>The second category includes work related to register recycling. Moudgill et al. <ref type="bibr" target="#b16">[17]</ref> discuss performing early register recycling in out-of-order processors that support precise exceptions. However, the implementation of precise exceptions in <ref type="bibr" target="#b16">[17]</ref> relies on either checkpoint/rollback for every replay event, or a history buffer that restricts register recycling to only the instruction at the head of that buffer. In contrast, Cherry combines the ROB and checkpointing, allowing register recycling and, at the same time, quick recovery from frequent replay events using the ROB, and precise exception handling using checkpointing. Wallace and Bagherzadeh <ref type="bibr" target="#b21">[22]</ref>, and later Monreal et al. <ref type="bibr" target="#b15">[16]</ref> delay allocation of physical registers to the execution stage. This is complementary to our work, and can be combined with it to achieve even better resource utilization. Lozano and Gao <ref type="bibr" target="#b11">[12]</ref>, Martin et al. <ref type="bibr" target="#b14">[15]</ref>, and Lo et al. <ref type="bibr" target="#b10">[11]</ref> use the compiler to analyze the code and pass on dead register information to the hardware, in order to deallocate physical registers. The latter approaches require instruction set support: special symbolic registers <ref type="bibr" target="#b11">[12]</ref>, register kill instructions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, or cloned versions of opcodes that implicitly kill registers <ref type="bibr" target="#b10">[11]</ref>. Our approach does not require changes in the instruction set or compiler support; thus, it works with legacy application binaries.</p><p>The third category of related work would include work that recycles load and store queue entries. Many current processors support speculative loads and replay traps <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref> and, to the best of our knowledge, this is the first proposal for early recycling of load and store queue entries in such a scenario.</p><p>The last category includes work that, instead of recycling resources early to improve utilization, opts to build larger structures for these resources. Lebeck et al. <ref type="bibr" target="#b9">[10]</ref> propose a two-level hierarchical instruction window to keep the effective sizes large and yet the primary structure small and fast. The buffering of the state of all the in-flight instructions is achieved through the use of two-level register files similar to <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>, and a large load/store queue. Instead, we focus on improving the effective size of resources while keeping their actual sizes small. We believe that these two techniques are complementary, and could have an additive effect.</p><p>Finally, we notice that, concurrently to our work, Cristal et al. <ref type="bibr" target="#b1">[2]</ref> propose the use of checkpointing to allow early release of unfinished instructions from the ROB and subsequent out-of-order commit of such instructions. They also leverage this checkpointing support to enable early register release. As a result, a large virtual ROB that tolerates long-latency operations can be constructed from a small physical ROB. This technique is compatible with Cherry, and both schemes could be combined for greater overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">SUMMARY AND CONCLUSIONS</head><p>This paper has presented CHeckpointed Early Resource RecYcling (Cherry), a mode of execution that decouples the recycling of the resources used by an instruction and the retirement of the instruction. Resources are recycled early, resulting in a more efficient utilization. Cherry relies on state checkpointing to service exceptions for instructions whose resources have been recycled. Cherry leverages the ROB to (1) not require in-order execution as a fallback mechanism, (2) allow memory replay traps and branch mispredictions without rolling back to the Cherry checkpoint, and (3) quickly fall back to conventional out-of-order execution without rolling back to the checkpoint or flushing the pipeline. Furthermore, Cherry enables long checkpointing intervals by allowing speculative updates to reside in the local cache hierarchy.</p><p>We have presented a Cherry implementation that targets three resources: load queue, store queue, and register files. We use simple rules for recycling these resources. We report average speedups of 1.06 and 1.26 on SPECint and SPECfp applications, respectively, relative to an aggressive conventional architecture. Of the three techniques, our proposal for load queue entry recycling is the most effective one, particularly for integer codes.</p><p>Finally, we have described how to combine Cherry and speculative multithreading. These techniques complement each other and can share significant hardware support.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Conventional ROB and Cherry ROB with the Point</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of the implementation of a Volatile bit with a typical 6-T SRAM cell and an additional transistor for gang-clear (inside the dashed circle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of Cherry overheads for different intervals between exceptions (Ì ) and overhead-free Cherry cycle durations (Ì ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Actions taken on load (a) and store (b) operations in the conventional load/store unit assumed in this paper. Ä and ÄÜ stand for Load, while Ë and Ë Ü stand for Store.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figures 7 and 8</head><label>8</label><figDesc>Figures 7 and 8 show the contribution of different components to the execution time of the SPECint and SPECfp applications, respectively. Each application shows the execution time for three configurations, namely Baseline, Cherry, and Limit. The execution times are normalized to Baseline. The bars are broken down into busy time (Busy) and different types of processor stalls due to: lack of physical registers (Regs), lack of SQ entries (SQ), lack of load queue entries (LQ). A final category (Other) includes other losses, including those due to branch mispredictions or lack of entries in the instruction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Breakdown of the execution time of the SPECint applications for the Baseline (B), Cherry (C), and Limit (L) configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Breakdown of the execution time of the SPECfp applications for the Baseline (B), Cherry (C), and Limit (L) configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Cumulative distribution of resource usage in SPECint (left) and SPECfp (right) applications. The horizontal axis is the cumulative percentage of time that a resource is used below the level shown in the vertical axis. The resources are, from top to bottom: LQ entries, SQ entries, integer physical registers, and floating-point physical registers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>PNR for each of the example resources that are recycled early under Cherry.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Baseline architecture modeled. In the table, MSHR,</figDesc><table><row><cell>RAS, FSB and RT stand for Miss Status Handling Register,</cell></row><row><cell>Return Address Stack, Front-Side Bus, and Round-Trip time</cell></row><row><cell>from the processor, respectively. Cycle counts refer to pro-</cell></row><row><cell>cessor cycles.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Characterizing the irreversible set and other related</figDesc><table><row><cell>Cherry parameters.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Note that the LQ entry for the load equal to oldest´Í Ä Í Ë µ cannot trig- ger a replay trap and, therefore, can also be released. However, for simplicity, we ignore this case.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Note that it is safe to update the memory system if the store is equal to oldest´Í Ä Í Ë Í µ. However, for simplicity, we ignore this case.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Rajit Manohar, Sanjay Patel, and the anonymous reviewers for useful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Alpha 21264/EV67 Microprocessor Hardware Reference Manual</title>
		<imprint>
			<date type="published" when="2000-09">September 2000</date>
			<pubPlace>Shrewsbury, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Compaq Computer Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Large virtual ROBs by processor checkpointing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Llosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>González</surname></persName>
		</author>
		<idno>UPC-DAC-2002- 39</idno>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
		<respStmt>
			<orgName>Universitat Politècnica de Catalunya</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple-banked register file architectures</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Topham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
				<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-06">June 2000</date>
			<biblScope unit="page" from="316" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data speculation support for a chip multiprocessor</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-10">October 1998</date>
			<biblScope unit="page" from="58" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SPEC CPU2000: Measuring CPU performance in the new millennium</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="28" to="35" />
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The microarchitecture of the Pentium 4 processor</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Upton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carmean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roussel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Technology Journal</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Checkpoint repair for out-of-order execution machines</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
				<meeting><address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987-06">June 1987</date>
			<biblScope unit="page" from="18" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adapting the SPEC 2000 benchmark suite for simulation-based computer architecture research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinosowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Workload Characterization</title>
				<meeting><address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-09">September 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A chip-multiprocessor architecture with speculative multithreading</title>
		<author>
			<persName><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="866" to="880" />
			<date type="published" when="1999-09">September 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A large, fast instruction window for tolerating cache misses</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Lebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koppanalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
				<meeting><address><addrLine>Anchorage, AK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Software-directed register deallocation for simultaneous multithreaded processors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="922" to="933" />
			<date type="published" when="1999-09">September 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting short-lived variables in superscalar processors</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
				<meeting><address><addrLine>Ann Arbor, MI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-12">November-December 1995</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Manohar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
		</imprint>
	</monogr>
	<note>Personal communication</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clustered speculative multithreaded processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Marcuello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>González</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Supercomputing</title>
				<meeting><address><addrLine>Rhodes, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="365" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploiting dead value information</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
				<meeting><address><addrLine>Research Triangle Park, NC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-12">December 1997</date>
			<biblScope unit="page" from="125" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delaying physical register allocation through virtual-physical registers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Monreal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Viñals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
				<meeting><address><addrLine>Haifa, Israel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-11">November 1999</date>
			<biblScope unit="page" from="186" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Register renaming and dynamic speculation: An alternative approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moudgill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pingali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassiliadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
				<meeting><address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-12">December 1993</date>
			<biblScope unit="page" from="202" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Implementing precise interrupts in pipelined processors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Pleszkun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="562" to="573" />
			<date type="published" when="1988-05">May 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiscalar processors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Breach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
				<meeting><address><addrLine>Santa Margherita Ligure, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="414" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The potential for using thread-level data speculation to facilitate automatic parallelization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Steffan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High-Performance Computer Architecture</title>
				<meeting><address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-02">January-February 1998</date>
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">POWER4 system microarchitecture</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Dodson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sinharoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="25" />
			<date type="published" when="2002-01">January 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A scalable register file architecture for dynamically scheduled processors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bagherzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Parallel Architectures and Compilation Techniques</title>
				<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996-10">October 1996</date>
			<biblScope unit="page" from="179" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The MIPS R10000 superscalar microprocessor</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Yeager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="28" to="40" />
			<date type="published" when="1996-04">April 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-level hierarchical register file organization for VLIW processors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zalamea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Llosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ayguadé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
				<meeting><address><addrLine>Monterey, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-12">December 2000</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
