<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-09-09">9 Sep 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xing</forename><surname>Wu</surname></persName>
							<email>wuxing@kuaishou.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Kuaishou Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaochen</forename><surname>Gao</surname></persName>
							<email>gaochaochen@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liangjun</forename><surname>Zang</surname></persName>
							<email>zangliangjun@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jizhong</forename><surname>Han</surname></persName>
							<email>hanjizhong@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
							<email>wangzhongyuan@kuaishou.com</email>
							<affiliation key="aff2">
								<orgName type="department">Kuaishou Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Songlin</forename><surname>Hu</surname></persName>
							<email>husonglin@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09-09">9 Sep 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2109.04380v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive learning has been attracting much attention for learning unsupervised sentence embeddings. The current state-of-the-art unsupervised method is the unsupervised Sim-CSE (unsup-SimCSE). Unsup-SimCSE takes dropout as a minimal data augmentation method, and passes the same input sentence to a pre-trained Transformer encoder (with dropout turned on) twice to obtain the two corresponding embeddings to build a positive pair. As the length information of a sentence will generally be encoded into the sentence embeddings due to the usage of position embedding in Transformer, each positive pair in unsup-SimCSE actually contains the same length information. And thus unsup-SimCSE trained with these positive pairs is probably biased, which would tend to consider that sentences of the same or similar length are more similar in semantics. Through statistical observations, we find that unsup-SimCSE does have such a problem. To alleviate it, we apply a simple repetition operation to modify the input sentence, and then pass the input sentence and its modified counterpart to the pre-trained Transformer encoder, respectively, to get the positive pair. Additionally, we draw inspiration from the community of computer vision and introduce a momentum contrast, enlarging the number of negative pairs without additional calculations. The proposed two modifications are applied on positive and negative pairs separately, and build a new sentence embedding method, termed Enhanced Unsup-SimCSE (ESimCSE). We evaluate the proposed ESimCSE on several benchmark datasets w.r.t the semantic text similarity (STS) task. Experimental results show that ESimCSE outperforms the state-of-the-art unsup-SimCSE by an average Spearman correlation of 2.02% on BERT-base.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The large-scale pre-trained language model <ref type="bibr" target="#b8">(Devlin et al., 2018;</ref><ref type="bibr" target="#b15">Liu et al., 2019)</ref>, represented by BERT, benefits many downstream supervised tasks through finetuning methods. However, when applying BERT's native sentence embeddings directly for semantic similarity tasks without labeled data, the performance is hardly satisfactory <ref type="bibr" target="#b10">(Gao et al., 2021;</ref><ref type="bibr" target="#b21">Yan et al., 2021)</ref>. Recently, researchers have proposed using contrastive learning to learn better unsupervised sentence embeddings. Contrastive learning aims to learn effective sentence embeddings based on the assumption that effective sentence embeddings should bring similar sentences closer while pushing away dissimilar ones. It generally uses various data augmentation methods to randomly generate different views for each sentence, and assumes a sentence is semantically more similar to its augmented counterpart than any other sentence. The current state-of-the-art method is unsup-SimCSE <ref type="bibr" target="#b10">(Gao et al., 2021)</ref>, which generates the state-of-the-art unsupervised sentence embeddings and performs on par with previously supervised counterparts. Unsup-SimCSE implicitly hypothesizes dropout acts as a minimal data augmentation method. Specifically, unsup-SimCSE composes N sentences in a batch and feeds each sentence to the pre-trained BERT twice with two independently sampled dropout masks. Then the embeddings derived from the same sentence constitute a "positive pair", while those derived from two different sentences constitute a "negative pair".</p><p>Using dropout as a minimal data augmentation method is simple and effective, but there is a weak point. Pretrained language models are built on Transformer blocks, which will encode the length information of a sentence through position embeddings. And thus a positive pair derived from the same sentence would contain the same length in-Figure <ref type="figure">1</ref>: The schematic diagram of the ESimCSE method. Unlike the unsup-SimCSE, ESimCSE performs word repetition operations on the batch so that the lengths of positive pairs vary without changing the semantics of sentences. This mechanism weakens the same-length hint for the model when predicting positive pairs. In addition, ESimCSE also maintains several preceding mini-batches' model outputs in a queue, termed momentum contrast, which can expand the negative pairs involved in loss calculation. This mechanism allows pairs to be compared more sufficiently in contrastive learning. formation, while a negative pair derived from two different sentences generally would contain different length information. Therefore, positive pairs and negative pairs are different in the length information they contained, which can act as a feature to distinguish them. Specifically, due to such a difference, the semantic similarity model trained with these pairs can be biased, which probably considers that two sentences of the same or similar lengths are more similar in semantics.</p><p>To confirm the impact of the length difference, we evaluate on standard semantic textual similarity (STS) tasks with the unsup-SimCSE-BERT base model published by <ref type="bibr" target="#b10">(Gao et al., 2021)</ref>. We partition STS task datasets into groups based on the sentence pairs' length difference, and calculate the corresponding semantic similarity with spearman correlation separately. As shown in Table <ref type="table">1</ref>, as the length difference increases, the performance of unsup-SimCSE gets worse. The performance of unsup-SimCSE on sentences with similar length (≤ 3) far exceeds the performance on sentences with a larger difference in length (&gt; 3).</p><p>To alleviate this problem, we propose a simple but effective enhancement method to unsup- SimCSE. For each positive pair, we expect to change the length of a sentence without changing its semantic meaning. Existing methods to change the length of a sentence generally use random insertion and random deletion. However, inserting randomly selected words into a sentence may introduce extra noise, which will probably distort the meaning of the sentence; deleting keywords from a sentence will also change its semantics substantially. Therefore, we propose a safer method, termed "word repetition", which randomly duplicates some words in a sentence. For example, as shown in Table <ref type="table">2</ref>, the original sentence is "I like this apple because it looks so fresh and I think it should be delicious." Random insertion may generate "I don't like this apple because but it looks so not fresh and I think it should be dog delicious." , and random deletion may generate "I this apple because it looks so and I think it should be.". Both deviate far from the meaning of the original sentence.</p><formula xml:id="formula_0">Dataset length diff ≤ 3 length diff &gt; 3 STS12 0.</formula><p>On the contrary, the method of "word repetition" may get "I like like this apple because it looks so so fresh and and I think it should be delicious.", or "I I like this apple apple because it looks looks so fresh fresh and I think it should be delicious delicious." Both keep the meaning of the original sentence quite well.</p><p>Apart from the optimization above for positive pairs construction, we further explore how to optimize the construction of negative pairs. Since contrastive learning is carried out between positive pairs and negative pairs, theoretically more negative pairs can lead to better comparison between the pairs <ref type="bibr" target="#b7">(Chen et al., 2020)</ref>. And thus a potential optimization direction is to leverage more negative pairs, encouraging the model towards more refined learning. However, according to <ref type="bibr" target="#b10">(Gao et al., 2021)</ref>, a larger batch size is not always a better choice. For example, as show in Figure <ref type="figure" target="#fig_0">2</ref>, for the unsup-SimCSE-BERT base model, the optimal batch size is 64, and other settings of the batch size will lower the performance. Therefore, we tend to figure out how to expand the negative pairs more effectively. In the community of computer vision, to alleviate the GPU memory limitation when expanding the batch size, a feasible way is to introduce the momentum contrast <ref type="bibr" target="#b11">(He et al., 2020)</ref>, which is also applied to natural language understanding <ref type="bibr" target="#b9">(Fang et al., 2020)</ref>. Momentum contrast allows us to reuse the encoded embeddings from the immediate preceding mini-batches to expand the negative pairs, by maintaining a queue: which always enqueue the sentence embeddings of the current minibatches and meanwhile dequeue the "oldest" ones. As the enqueued sentence embeddings come from the preceding mini-batches, we keep a momentumupdated model by taking the moving-average of its parameters and use the momentum model to generate enqueued sentence embeddings. Note that, we turn off dropout when using the momentum encoder, which can narrow the gap between training and prediction.</p><p>The above two optimizations are proposed separately for building positive and negative pairs. We finally combine both with unsup-SimCSE, which is termed Enhanced SimCSE (ESimCSE). We illustrate the schematic diagram of ESimCSE in Figure <ref type="figure">1</ref>. The proposed ESimCSE is evaluated on the semantic text similarity (STS) task with 7 STS-B test sets. Experimental results show that ESimCSE can substantially improve the similarity measuring performance in different model settings over the previous state-of-the-art unsup-SimCSE. Specifically, ESimCSE gains an average increase of Spearman's correlation over unsup-SimCSE by +2.02% on BERT base , +0.90% on BERT large , +0.87% on RoBERTa base , +0.55% on RoBERTa large , respectively.</p><p>Our contributions can be summarized as follows:</p><p>• We observe that unsup-SimCSE constructs each positive pair with two sentences of the same length, which can bias the learning process. We propose a simple but effective "word repetition" method to alleviate the problem.</p><p>• We propose to use the momentum contrast method to increase the number of negative pairs involved in the loss calculation, which encourages the model towards more refined learning.</p><p>• We conduct extensive experiments on several benchmark datasets w.r.t semantic text similarity task. The experimental results well demonstrate that both proposed optimizations bring substantial improvements to unsup-SimCSE.</p><p>2 Background: Unsup-SimCSE</p><p>Given a set of paired sentences</p><formula xml:id="formula_1">x i , x + i m i=1</formula><p>, where</p><p>x i and x + i are semantically related and will be re-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Text Similarity original sentence I like this apple because it looks so fresh and I think it should be delicious.</p><p>1.0 random insertion I don't like this apple because but it looks so not fresh and I think it should be dog delicious.</p><p>0.76 random deletion I like this apple because it looks so fresh and I think it should be delicious.</p><p>0.77 word repetition I like like this apple because it looks so so fresh and and I think it should be delicious.</p><p>1.0 word repetition I I like this apple apple because it looks looks so fresh fresh and I think it should be delicious delicious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0.98</head><p>Table <ref type="table">2</ref>: An example of different methods to change the length of a sentence. The similarity scores are predicted by official released "unsup-simcse-bert-base-uncased" model.</p><p>ferred to positive pairs. The core idea of unsup-SimCSE is to use identical sentences to build the positive pairs, i.e., x + i = x i . Note that in Transformer, there is a dropout mask placed on fullyconnected layers and attention probabilities. And thus the key ingredient is to feed the same input x i to the encoder twice by applying different dropout masks z i and z + i and output two separate sentence embeddings to build a positive pair as follows:</p><formula xml:id="formula_2">h i = f θ (x i , z i ) , h + i = f θ x i , z + i (1)</formula><p>With h i and h + i for each sentence in a mini-batch with batch size N , the contrastive learning objective w.r.t x i is formulated as follows,</p><formula xml:id="formula_3">i = − log e sim(h i ,h + i )/τ N j=1 e sim(h i ,h + j )/τ (2)</formula><p>where τ is a temperature hyperparameter and sim (h i , h i ) is the similarity metric, which is typically the cosine similarity function as follows,</p><formula xml:id="formula_4">sim h i , h + i = h i h + i h i • h + i (3)</formula><p>3 Proposed ESimCSE: Enhanced unsup-SimCSE</p><p>In this section, we first introduce the word repetition method to construct better positive pairs. Then we introduce the momentum contrast method to expand negative pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word Repetition</head><p>The word repetition mechanism randomly duplicates some words/sub-words in a sentence. Here we take sub-word repetition as an example. Given a sentence s, after processing by a sub-word tokenizer, we get a sub-word sequence x = {x 1 , x 2 , ..., x N }, N being the length of sequence.</p><p>We define the number of repeated tokens as</p><formula xml:id="formula_5">dup len ∈ [0, max(2, int(dup rate * N ))] (4)</formula><p>where dup rate is the maximal repetition rate, which is a hyperparameter. Then dup len is a randomly sampled number in the set defined above, which will introduce more diversity when extending the sequence length. After dup len is determined, we use uniform distribution to randomly select dup len sub-words that need to be repeated from the sequence, which composes the dup set as follows,</p><formula xml:id="formula_6">dup set = unif orm(range = [1, N ], num = dup len) (5) For example, if the 1th sub-word is in dup set, then sequence x becomes x + = {x 1 , x 1 , x 2 , ..., x N }.</formula><p>And different from unsup-SimCSE which passes x to the pre-trained BERT twice, E-SimCSE passes x and x + independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Momentum Contrast</head><p>The momentum contrast allows us to reuse the encoded sentence embeddings from the immediate preceding mini-batches by maintaining a queue of a fixed size. Specifically, the embeddings in the queue are progressively replaced. When the output sentence embeddings of the current mini-batch is enqueued, the "oldest" ones in the queue are removed if the queue is full. Note that we use a momentum-updated encoder to encode the enqueued sentence embeddings. Formally, denoting  the parameters of the encoder as θ e and those of the momentum-updated encoder as θ m , we update θ m in the following way,</p><formula xml:id="formula_7">STS12 STS13 STS14 SICK15 STS16 STS-B SICK-R train 0 0 0 0 0 5,</formula><formula xml:id="formula_8">θ m ← λθ m + (1 − λ)θ e (6)</formula><p>where λ ∈ [0, 1) is a momentum coefficient parameter. Note that only the parameters θ e are updated by back-propagation. And here we introduce θ m to generate sentence embeddings for the queue, because the momentum update can make θ m evolve more smoothly than θ e . As a result, though the embeddings in the queue are encoded by different encoders (in different "steps" during training), the difference among these encoders can be made small. With sentence embeddings in the queue, the loss function of ESimCSE is further modifed as follows,</p><formula xml:id="formula_9">i = − log e sim(h i ,h + i )/τ N j=1 e sim(h i ,h + j )/τ + M m=1 e sim(h i ,h + m) /τ</formula><p>(7) where h + m is denotes a sentence embedding in the momentum-updated queue, and M is the size of the queue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation Setup</head><p>Following unsup-SimCSE, we use 1-million sentences randomly drawn from English Wikipedia for training<ref type="foot" target="#foot_0">1</ref> . Then we conduct our experiments on 7 standard semantic textual similarity (STS) tasks. The detail statistics are shown in Table <ref type="table" target="#tab_2">3</ref>. STS12-STS16 datasets do not have train or development sets, and thus we evaluate the models on the development set of STS-B to search for better settings of the hyper-parameters. The SentEval toolkit<ref type="foot" target="#foot_1">2</ref> is used for evaluation. For the compared baseline unsup-SimCSE, we download the officially published model checkpoints<ref type="foot" target="#foot_2">3</ref> and reproduce evaluation results with the suggested hyper-parameters in dev/test mode. experiments are conducted on Nvidia 3090 GPUs.</p><p>Semantic Textual Similarity Tasks Semantic textual similarity measures the semantic similarity of any two sentences. STS 2012-2016 <ref type="bibr" target="#b4">(Agirre et al., 2012</ref><ref type="bibr" target="#b5">(Agirre et al., , 2013</ref><ref type="bibr" target="#b1">(Agirre et al., , 2014</ref><ref type="bibr" target="#b0">(Agirre et al., , 2015</ref><ref type="bibr" target="#b2">(Agirre et al., , 2016) )</ref> and STS-B <ref type="bibr" target="#b6">(Cer et al., 2017)</ref> are widely used semantic textual similarity benchmark datasets, which measure the semantic similarity of two sentences with the cosine similarity of the corresponding sentence embeddings. After deriving the semantic similarities of all pairs in the test set, we follow unsup-SimCSE to use Spearman correlation to measure the correlation between the ranks of predicted similarities and the ground-truth. For a set of size n, the n raw scores X i , Y i are converted to its corresponding ranks rg X i , rg Y i , then the Spearman correlation is defined as follows</p><formula xml:id="formula_10">r s = cov (rg X , rg Y ) σ rg X σ rg Y<label>(8)</label></formula><p>where cov (rg X , rg Y ) is the covariance of the rank variables, σ rg X and σ rg Y are the standard deviations of the rank variables. Spearman correlation has a value between -1 and 1, which will be high when the ranks of predicted similarities and the ground-truth are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>We start from pre-trained checkpoints of BERT(uncased) or RoBERTa(cased) using both the base and the large versions, and we add an MLP layer on top of the [CLS] representation to get the sentence embedding. We implement ESimCSE based on Huggingface's transformers package 4 . And we train our models for one epoch by using the Adam optimizer with the batch size = 64 and the hyper-parameter temperature τ = 0.05 in Eq. ( <ref type="formula">3</ref>). The learning rate is set as 3e-5 for ESimCSE-BERT base model and 1e-5 for other models. The dropout rate is p = 0.1 for base models, p = 0.15 for large models. For the momentum contrast, we empirically choose a relatively large momentum λ = 0.995. In addition, we evaluate the model every 125 training steps on the development set of STS-B and keep the best checkpoint for the final evaluation on test sets. We use sub-word repetition instead of word repetition, which will be further discussed in the ablation study section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>Table <ref type="table" target="#tab_4">4</ref> shows the best results obtained on the STS-B development sets. We highlight the highest numbers among models with the same pre-trained encoder as bold. ♣ denotes the evaluation results from the official published model by <ref type="bibr" target="#b10">(Gao et al., 2021)</ref>   <ref type="bibr" target="#b10">(Gao et al., 2021)</ref>.</p><p>improves the measurement of semantic textual similarity in different settings of base models over the previous state-of-the-art unsup-SimCSE. Specifically, our proposed ESimCSE outperforms unsup-SimCSE by +2.02% on BERT base , +0.90% on BERT large , +0.87% on RoBERTa base , +0.55% on RoBERTa large , respectively. We also explore how much improvement it can bring to unsup-SimCSE when only using word repetition or momentum contrast. As shown in table <ref type="table" target="#tab_9">6 and 7</ref>, either word repetition or momentum contrast can bring substantial improvements to unsup-SimCSE. It means that both proposed methods to enhance the positive pairs and negative pairs are effective. Better yet, these two modifications can be superimposed (ESimCSE) to get further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Ablation Study</head><p>This section investigates how different dropout rates, repetition rates, sentence-length-extension methods, and momentum contrast queue size affect ESimCSE's performance. We only change one hyperparameter at a time. All results use our ESimCSE-BERT base model and are evaluated on the development set of STS-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effect of Dropout Rate</head><p>Dropout is the key ingredient to the unsup-SimCSE model, so different dropout rates p are crucial to the model's performance. According to <ref type="bibr" target="#b10">(Gao et al., 2021)</ref>, the optimal dropout rate for unsup-SimCSE-BERT base is p = 0.1. Considering that ESimCSE additionally introduces word repetition and momentum contrast mechanisms, we re-examine the impact of different dropouts on its performance. We experiment on three typical dropout rates, and the results are shown in the table 8. Specifically, when the dropout is 0.1, it achieves the best performance on the STS-B development set. When the dropout increases to 0.15, the performance is close to that of 0.1, with no significant drop. And even when the dropout reaches 0.2, the performance drops by nearly 1%, but it still outperforms unsup-SimCSE. The experimental results kind of show the robustness of the superiority of the proposed ESimCSE over unsup-SimCSE, in terms of dropout rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effect of Repetition Rate</head><p>Word repetition can bring improvement by diversifying the length difference of positive pairs in   the proposed ESimCSE. Intuitively, few repetitions have a limited impact on the diversity of length difference, while many repetitions will distort the semantics of sentences, making positive pairs not solid enough. To quantitatively study the effect of repetition rate on the model performance, we slowly increase the repetition rate parameter dup rate from 0.08 to 0.36, with each increase by 0.04. As shown in Table <ref type="table" target="#tab_11">9</ref>, when dup rate = 0.32, ESimCSE-BERT base achieves the best performance, a larger or smaller dup rate will cause performance degradation, which is consistent with our intuition. Although there are small fluctuations, most of the results of the proposed ESimCSE still exceed the best results of unsup-SimCSE-BERT base .</p><formula xml:id="formula_11">Model STS12 STS13 STS14 SICK15 STS16 STS-B SICK-R Avg.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of Sentence-Length-Extension Method</head><p>In addition to sub-word repetition, we also explore three other methods to increase sentence length: word repetition, inserting stop-words and inserting [MASK] <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>. The implementation of word repetition is similar to sub-word repetition, except that the repetition operation occurs before tokenization. For example, given a word "microbiology", word repetition will produce "microbiology microbiology", while sub-word repetition will produce "micro micro ##biology" or "micro ##biology ##biology". Inserting stop-words is another wordlevel expansion method. The selection of insertion position is the same as the method of word repetition, except that the selected word is no longer repeated, but a random stop-word is inserted instead. Inserting [MASK] is similar, which inserts a [MASK] token after the selected word. It is similar to the pre-training input of BERT. We can regard [MASK] as a dynamic context-compatible word placeholder. As shown in Table <ref type="table" target="#tab_12">10</ref>, sub-word repetition achieves the best performance, and word repetition can also bring a good improvement, which shows that more fine-grained repetition can better alleviate the bias brought by the length difference of positive pairs. Inserting [MASK] can also bring a small improvement, but inserting stop words will slightly decrease effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Effect of Queue Size in Momentum Contrast</head><p>The size of the momentum contrast queue determines the number of negative pairs involved in the loss calculation. Without considering the time cost and the limitation of GPU memory, can a larger queue size lead to better performance? We take the BERT base as the base model for ESimCSE and experiment with the queue size equals to different multiples of the batch size. The experimental results are listed in    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Unsupervised sentence representation learning has been widely studied. <ref type="bibr" target="#b19">(Socher et al., 2011;</ref><ref type="bibr" target="#b12">Hill et al., 2016;</ref><ref type="bibr" target="#b14">Le and Mikolov, 2014)</ref> propose to learn sentence representation according to the internal structure of each sentence. <ref type="bibr" target="#b13">(Kiros et al., 2015;</ref><ref type="bibr" target="#b16">Logeswaran and Lee, 2018)</ref> predict the surrounding sentences of a given sentence based on the distribution hypothesis. <ref type="bibr" target="#b18">(Pagliardini et al., 2017)</ref> propose Sent2Vec, a simple unsupervised model allowing to compose sentence embeddings using word vectors along with n-gram embeddings. Recently, contrastive learning has been explored in unsupervised sentence representation learning and has become a promising trend <ref type="bibr" target="#b22">(Zhang et al., 2020;</ref><ref type="bibr" target="#b20">Wu et al., 2020;</ref><ref type="bibr" target="#b17">Meng et al., 2021;</ref><ref type="bibr" target="#b10">Gao et al., 2021;</ref><ref type="bibr" target="#b21">Yan et al., 2021)</ref>. Those contrastive learning based methods for sentence embeddings are generally based on the assumption that a good semantic representation should be able to bring similar sentences closer while pushing away dissimilar ones. Therefore, those methods use various data augmentation methods to randomly generate two different views for each sentence and design an effective loss function to make them closer in the semantic representation space. Among these contrastive methods, the most related ones to our work are unsup-ConSERT and unsup-SimSCE. ConSERT explores various effective data augmentation strategies(e.g., adversarial attack, token shuffling, Cutoff, dropout) to generate different views for contrastive learning and analyze their effects on unsupervised sentence representation transfer. Unsup-SimSCE, the current state-of-the-art unsupervised method uses only standard dropout as minimal data augmentation, and feed an identical sentence to a pretrained model twice with independently sampled dropout masks to generate two distinct sentence embeddings as a positive pair. Unsup-SimSCE is very simple but works surprisingly well, performing on par with previously supervised counterparts. However, we find that unsup-SimCSE constructs each positive pair with two sentences of the same length, which can mislead the learning of sentence embeddings. So we propose a simple but effective method temed "word repetition" to alleviate it. We also propose to use the momentum contrast method to increase the number of negative pairs involved in the loss calculation, which encourages the model towards more refined learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we propose optimizations to construct positive and negative pairs for unsup-SimCSE and combine them with unsup-SimCSE, which is termed ESimCSE. Through extensive experiments, the proposed ESimCSE achieves considerable improvements on standard semantic text similarity tasks over unsup-SimCSE.</p><p>As unsup-SimCSE treats all negative pairs the same importance. Some negative pairs are quite different from positive pairs, while others are relatively close to positive pairs. This distinction will be helpful for embedding retrieval tasks but not reflected in the objective function of unsup-SimCSE. Therefore, in the future, we will focus on designing a more refined objective function to improve the discrimination between different negative pairs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The performance trend on STS-B development set when batch size changes for unsup-SimCSE-BERT base model.</figDesc><graphic url="image-2.png" coords="3,72.00,62.81,226.77,151.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="2,100.35,62.81,396.85,249.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Data statistics of standard semantic textual similarity (STS) tasks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Sentence embedding performance on seman-</figDesc><table /><note>tic textual similarity (STS) development sets in terms of Spearman's correlation, with BERT base , BERT large , RoBERTa base , RoBERTa large as base models. ♣ : results from official published model by</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>unsup-SimCSE-BERT base ♣ 68.40 82.41 74.38 80.91 78.56 76.85 72.23 76.25 ESimCSE-BERT base 73.40 83.27 77.25 82.66 78.81 80.17 72.30 78.27 (+2.02) unsup-SimCSE-BERT large ♣ 70.88 84.16 76.43 84.50 79.76 79.26 73.88 78.41 ESimCSE-BERT large 73.21 85.37 77.73 84.30 78.92 80.73 74.89 79.31 (+0.90)</figDesc><table><row><cell cols="2">unsup-SimCSE-RoBERTa base ♣ 70.16 81.77 73.24 81.36 80.65 80.22 68.56 76.57</cell></row><row><cell>ESimCSE-RoBERTa base</cell><cell>69.90 82.50 74.68 83.19 80.30 80.99 70.54 77.44 (+0.87)</cell></row><row><cell cols="2">unsup-SimCSE-RoBERTa large ♣ 72.86 83.99 75.62 84.77 81.80 81.98 71.26 78.90</cell></row><row><cell>ESimCSE-RoBERTa large</cell><cell>73.20 84.93 76.88 84.86 81.21 82.79 72.27 79.45 (+0.55)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Sentence embedding performance on 7 semantic textual similarity (STS) test sets, in terms of Spearman's correlation, with BERT base , BERT large , RoBERTa base , RoBERTa large as base models. ♣ : results from official published model by<ref type="bibr" target="#b10">(Gao et al., 2021)</ref>..</figDesc><table><row><cell>Model</cell><cell>STS-B</cell></row><row><cell cols="2">unsup-SimCSE-BERT base ♣ 82.45</cell></row><row><cell>+ word repetition</cell><cell>84.09 (+1.64)</cell></row><row><cell>+ momentum contrast</cell><cell>83.98 (+1.53)</cell></row><row><cell>ESimCSE-BERT base</cell><cell>84.85 (+2.40)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Improvement on STS-B development sets that word repetition or momentum contrast brings to unsup-SimCSE.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 11 .</head><label>11</label><figDesc>The optimal result is reached when the queue size was 2.5 times the batch size. A smaller or larger queue size will</figDesc><table><row><cell>Model</cell><cell>STS12 STS13 STS14 SICK15 STS16 STS-B SICK-R Avg.</cell></row><row><cell cols="2">unsup-SimCSE-BERT base ♣ 68.40 82.41 74.38 80.91 78.56 76.85 72.23 76.25</cell></row><row><cell>+ word repetition</cell><cell>69.79 83.43 75.65 82.44 79.43 79.44 71.86 77.43 (+1.18)</cell></row><row><cell>+ momentum contrast</cell><cell>71.41 82.23 74.94 82.99 79.85 79.48 71.85 77.54 (+1.29)</cell></row><row><cell>ESimCSE-BERT base</cell><cell>73.40 83.27 77.25 82.66 78.81 80.17 72.30 78.27 (+2.02)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Improvements on 7 STS test sets that word repetition or momentum contrast brings to unsup-SimCSE.</figDesc><table><row><cell>p</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell></row><row><cell cols="4">STS-B 84.85 84.75 83.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Effects of different dropout probabilities p on the STS-B development set in terms of Spearman's correlation.</figDesc><table><row><cell cols="2">dup rate 0.08</cell><cell>0.12</cell><cell>0.16</cell><cell>0.2</cell></row><row><cell>STS-B</cell><cell cols="4">83.5 83.62 82.01 83.01</cell></row><row><cell cols="2">dup rate 0.24</cell><cell>0.28</cell><cell>0.32</cell><cell>0.36</cell></row><row><cell>STS-B</cell><cell cols="4">84.24 82.96 84.85 83.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Effects of repetition rate p on the STS-B development set in terms of Spearman's correlation.</figDesc><table><row><cell cols="2">Length-extension Method STS-B</cell></row><row><cell>unsup-SimCSE-BERT base</cell><cell>82.45</cell></row><row><cell>Inserting Stop-words</cell><cell>81.72</cell></row><row><cell>Inserting [MASK]</cell><cell>83.08</cell></row><row><cell>Word Repetition</cell><cell>84.40</cell></row><row><cell>Sub-word Repetition</cell><cell>84.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Effects of sentence-length-extension method on the STS-B development set in terms of Spearman's correlation.reduce the effect. It is intuitive because the introduction of momentum contrast encourages more negative pairs to participate in the loss calculation so that the positive pairs can be compared more sufficiently. But a too large queue size also reduces the benefit. We guess that is because the negative pairs in the momentum contrast are generated by the past "steps" during training, and a larger queue will use the outputs of more outdated encoder models which are quite different from the current one. And thus that will reduce the reliability of the loss calculation.</figDesc><table><row><cell>Queue Size</cell><cell>STS-B</cell></row><row><cell>1 × batch size</cell><cell>83.83</cell></row><row><cell cols="2">1.5 × batch size 83.81</cell></row><row><cell>2 × batch size</cell><cell>83.03</cell></row><row><cell cols="2">2.5 × batch size 84.85</cell></row><row><cell>3 × batch size</cell><cell>82.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Effects of queue size of momentum contrast on the STS-B development set in terms of Spearman's correlation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://huggingface.co/datasets/princeton-nlp/datasetsfor-simcse/resolve/main/wiki1m for simcse.txt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">2 https://github.com/facebookresearch/SentEval</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/princeton-nlp/SimCSE</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/huggingface/transformers,version 4.2.1.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Montse</forename><surname>Maritxalar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<title level="m">Semeval-2015 task 2: Semantic textual similarity, english, spanish and pilot on interpretability</title>
				<imprint>
			<date type="published" when="2015">2015. SemEval 2015</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
	<note>Proceedings of the 9th international workshop on semantic evaluation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 10: Multilingual semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international workshop on semantic evaluation</title>
				<meeting>the 8th international workshop on semantic evaluation</meeting>
		<imprint>
			<date type="published" when="2014">2014. SemEval 2014</date>
			<biblScope unit="page" from="81" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 1: Semantic textual similarity, monolingual and cross-lingual evaluation</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">German</forename><surname>Rigau Claramunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval-2016. 10th International Workshop on Semantic Evaluation</title>
				<imprint>
			<date type="published" when="2016-06-16">2016. 2016 Jun 16-17</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>San Diego</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ACL (Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="497" to="511" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semeval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">* SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
				<imprint>
			<date type="published" when="2012">2012. SemEval 2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="385" to="393" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">* sem 2013 shared task: Semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second joint conference on lexical and computational semantics (* SEM)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
	<note>and the shared task: semantic textual similarity</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cert: Contrastive self-supervised learning for language understanding</title>
		<author>
			<persName><forename type="first">Hongchao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12766</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<title level="m">Simcse: Simple contrastive learning of sentence embeddings</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03483</idno>
		<title level="m">Learning distributed representations of sentences from unlabelled data</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Russ R Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02893</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08473</idno>
		<title level="m">Coco-lm: Correcting and contrasting text sequences for language model pretraining</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised learning of sentence embeddings using compositional n-gram features</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02507</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic pooling and unfolding recursive autoencoders for paraphrase detection</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Clear: Contrastive learning for sentence representation</title>
		<author>
			<persName><forename type="first">Zhuofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15466</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Consert: A contrastive framework for self-supervised sentence representation transfer</title>
		<author>
			<persName><forename type="first">Yuanmeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rumei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An unsupervised sentence embedding method by mutual information maximization</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruidan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuozhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwan</forename><surname>Hui Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12061</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
