<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAT: Beyond Efficient Transformer for Content-Aware Anomaly Detection in Event Sequences</title>
				<funder ref="#_NJUu2HC #_GjtD4tX #_s2rXbaU">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shengming</forename><surname>Zhang</surname></persName>
							<email>shengming.zhang@rutgers.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yanchi</forename><surname>Liu</surname></persName>
							<email>yanchi@nec-labs.com</email>
						</author>
						<author>
							<persName><forename type="first">Xuchao</forename><surname>Zhang</surname></persName>
							<email>xuczhang@nec-labs.com</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
							<email>weicheng@nec-labs.com</email>
						</author>
						<author>
							<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
							<email>haifeng@nec-labs.com</email>
						</author>
						<author>
							<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
							<email>hxiong@rutgers.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Rutgers University Newark</orgName>
								<address>
									<region>New Jersey</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">NEC Labs America</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>New Jersey</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">NEC Labs America</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>New Jersey</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">NEC Labs America</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>New Jersey</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">NEC Labs America</orgName>
								<address>
									<settlement>Princeton</settlement>
									<region>New Jersey</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Rutgers University Newark</orgName>
								<address>
									<region>New Jersey</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<address>
									<addrLine>10 pages</addrLine>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CAT: Beyond Efficient Transformer for Content-Aware Anomaly Detection in Event Sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539155</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>anomaly detection</term>
					<term>event sequence modeling</term>
					<term>one-class classification</term>
					<term>content-aware sequential pattern mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is critical and important to detect anomalies in event sequences, which becomes widely available in many application domains. Indeed, various efforts have been made to capture abnormal patterns from event sequences through sequential pattern analysis or event representation learning. However, existing approaches usually ignore the semantic information of event content. To this end, in this paper, we propose a self-attentive encoder-decoder transformer framework, Content-Aware Transformer (CAT), for anomaly detection in event sequences. In CAT, the encoder learns preamble event sequence representations with content awareness, and the decoder embeds sequences under detection into a latent space, where anomalies are distinguishable. Specifically, the event content is first fed to a content-awareness layer, generating representations of each event. The encoder accepts preamble event representation sequence, generating feature maps. In the decoder, an additional token is added at the beginning of the sequence under detection, denoting the sequence status. A one-class objective together with sequence reconstruction loss is collectively applied to train our framework under the label efficiency scheme. Furthermore, CAT is optimized under a scalable and efficient setting. Finally, extensive experiments on three real-world datasets demonstrate the superiority of CAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Anomaly detection; Neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The expansion of new communication technologies and services, along with an increasing number of interconnected network devices contributes to making computer networks ever larger and more complex as intertwined systems <ref type="bibr" target="#b14">[15]</ref>. The arising system complexities make it even more challenging to maintain precise system management and lead to serious system vulnerabilities, as security incidents may occur more frequently. Abnormal states caused by malicious attackers could lead to ramifications including severe loss of economy and social well-being <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>. Anomaly detection, which aims at uncovering abnormal system behaviors in a timely manner, thus plays an important role in incident management of large-scale systems. With the ubiquitous sensors and networks, large-scale systems continuously generate monitoring data which contains rich information reflecting the runtime status of systems.</p><p>One type of the most important monitoring data is formatted as event sequences. An event sequence is defined as an ordered series of events, where each event is or can be mapped to a discrete symbol belonging to a finite alphabet, often with content describing the event <ref type="bibr" target="#b5">[6]</ref>. Event sequences can be seen in many real-world scenarios. For example, a computer program is a sequence of system command events, an email user possesses a sequence of sending/receiving email events. Different types of event data possess rich content information as well, e.g. the content of system command events is the machine-generated log files, and the subject, contents of emails denote as content of sending/receiving email events, which provides deep insights on the event status. We aim at detecting anomalous patterns of event sequences with contents considered, e.g. if a computing machine is running under an abnormal state or if an email user is a spammer. Figure <ref type="figure" target="#fig_0">1</ref> shows an illustrative example of anomaly detection in event sequences. Event messages are collected within a detect time interval, forming event sequences. The event sequences are then projected in a latent space, where normal event sequences and anomalous ones are distinguishable. Over the past decades, scholars have made great efforts on addressing anomaly detection in event sequences. Their methodologies can be grouped into three categories: (i) Pattern recognition approaches that reports event sequences with above-threshold dissimilarities as anomalies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>. These study outcomes take event alphabet sequence as input and treat each event as an independent dimension. However, sequential patterns and content information are not considered in these approaches. (ii) Sequential pattern learning approaches that use fixed sliding-window over event sequence and predict the next event based on the observation window <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref>. However, content information is not included in these approaches. The next-event-prediction scheme with fixed observation window size cannot capture sequential patterns outside the scope of sliding window as well. (iii) Representation learning approaches that map event contents to templates and generate event content representation for each template <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. However, they use simple aggregation of each event content token's representation as the event embedding. The templates capture limited semantic information and the dependency between tokens are not considered.</p><p>In this paper, we propose to approach the problem by introducing a self-attentive encoder-decoder transformer model to capture both sequential pattern and semantic information in event sequences. Although transformer has been successfully applied on a wide-range of research problems involving sequential data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b43">44]</ref>, directly applying existing transformer models to the event sequence anomaly detection problem remains at least three challenges: (i) Existing transformer models are typically designed for sequence forecasting, while our problem targets anomaly detection. (ii) Most existing transformer models cannot naturally capture the content information in event sequences, which we need to strengthen. (iii) Another key issue of existing transformer models is the efficiency and scalability issue. Detecting anomalies typically need to be executed over the air. It indicates the detection procedure needs to be conducted under a timely and online scheme that efficiency needs to be specifically addressed. Similarly, sometimes the events in a detect interval can be densely populated that the event sequence length is long.</p><p>The commonly used one-at-a-time forecasting scheme brings about scalability concerns, which need to be addressed as well.</p><p>To overcome the aforementioned deficits of existing transformer models, we propose a Content-Aware Transformer (CAT) to address the anomaly detection problem in event sequences. CAT adopts a self-attentive encoder-decoder transformer architecture, where the encoder learns event sequence representations, and the decoder embeds event sequences into a latent space, where anomalies are distinguishable. Specifically, we first design a content-awareness layer to process the event content information, generating representations of each event. The event representations are then grouped as event representation sequences as the input of the encoder and decoder. In the encoder, preamble event sequences are encoded as feature maps for the decoder's reference. In the decoder, a special token that represents the event sequence status is added to the start of the decoder's input representation sequence, denoted as <ref type="bibr">[SEQ]</ref>. During the training stage, the decoded representation distribution of the [SEQ] token for all the event sequences are bounded under a one-class objective, forming an optimal hypersphere. In the testing stage, any event sequence whose decoded [SEQ] representation lies outside the hypersphere is considered as anomaly. Such training scheme requires only normal event sequences for training, which addresses label efficiency. Furthermore, our decoder is designed to simultaneously forecast all the remaining events, and the multihead attention module is substituted as a sparse one so that the scalability and efficiency issues are addressed. Finally, extensive experiments on three real-world datasets are conducted to show the effectivenss of CAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Event Sequence Analysis: The analysis of event sequences has received considerable attention in the literature in the past decades. An event sequence typically refers to a finite-length sequence of events, where each event belongs to a finite alphabet <ref type="bibr" target="#b5">[6]</ref>. One of the most common types of event sequence data is the monitoring and fault diagnosis of dynamic systems, such as log data of operating system calls <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37]</ref>. Beyond that, event sequence data is observed in other domains as well, e.g. biological sequences such as DNA and protein sequences <ref type="bibr" target="#b9">[10]</ref> and sensor data of operational systems such as aircrafts or drones <ref type="bibr" target="#b10">[11]</ref>. Some instances of the event sequence data have limited event alphabet size, such as the protein sequences (close to 20 amino acid bases for protein), while others may have a large size, such as the sensor data (1,000 for the aircraft data <ref type="bibr" target="#b4">[5]</ref>). Some types of event sequence data need preprocessing of raw data in order to be matched into a finite alphabet space, e.g. template extractions for the log data of system calls. Beyond the alphabet information, event data contains rich content information that could potentially be used for analysis, such as the fingerprint or the structure of a protein sequence and the textual information of the system calls log data, etc. It remains challenging for event sequence analysis due to its multivariate characteristic, especially for the type of textual content information.</p><p>Event Sequence Anomaly Detection: The problem of detecting anomalies in event sequences has long been a hot topic in research communities. In general, three formulations are defined based on different definitions of anomalies, i.e., sequence-based, contiguous subsequence-based, and pattern frequency-based anomaly detection <ref type="bibr" target="#b5">[6]</ref>. Early studies detect anomalies based on similarity measurements, where test sequences are compared with training ones based on similarity scores <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39]</ref>. Other techniques include window-based techniques <ref type="bibr" target="#b42">[43]</ref>, hidden markov models-based techniques [8] and pattern frequency-based techniques <ref type="bibr" target="#b23">[24]</ref>. In terms of learning schemes, while a line of research applies supervised learning scheme for anomaly detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref>, it is more realistic to apply unsupervised method that only normal event sequences are used during the training stage for label efficiency concerns <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40]</ref>. In this paper, we mainly focus on the unsupervised learning scheme.</p><p>Deep Learning-based Anomaly Detection: A great deal of attention has been drawn to deep learning over the past decade because of its superiority in terms of model expressness and performance. There has been a line of research that applies deep learning techniques for event sequence anomaly detection under the unsupervised learning scheme. More specifically, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref> take event template index sequence as input and train in a next-event-templateindex prediction manner with long short term memory (LSTM <ref type="bibr" target="#b17">[18]</ref>) networks; <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42]</ref> detect anomalies with transformer <ref type="bibr" target="#b34">[35]</ref>-based models; <ref type="bibr" target="#b26">[27]</ref> aggregates word embeddings with pre-trained language models, e.g. Word2Vec <ref type="bibr" target="#b28">[29]</ref> and Glove <ref type="bibr" target="#b31">[32]</ref> to represent log events. Due to the rare-distribution of anomalies in a typical dataset, a one-class classification objective naturally fits for the task of anomaly detection and have gained attention by the community, including OC-SVM <ref type="bibr" target="#b32">[33]</ref>, LogBERT <ref type="bibr" target="#b16">[17]</ref> and OC4Seq <ref type="bibr" target="#b35">[36]</ref>. OC-SVM is a kernel-based approach with one-class objective, LogBERT and OC4Seq are deep neural network-based approaches that introduces a one-class objective as part of their loss functions. LogBERT and OC4Seq both report to succeed in the anomaly detection task. However, they need to generate templates of events and consider limited semantic information of event content. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>In this section, we give formal definitions of some important terminologies pertinent to our work. Table <ref type="table" target="#tab_0">1</ref> summaries the mathematical notations appeared in this paper. </p><formula xml:id="formula_0">? ? = ? (RNN(? ? -1 , ? ? ))<label>(1)</label></formula><p>The recurrent-based message passing indicates that the hidden representation of the current state is related to both its input and the hidden representation of the previous state. ? (?) is the activation function. The second is the convolutional-based message passing: Attention(?, ?, ? ) = softmax(</p><formula xml:id="formula_1">? ? = ? (Conv1d([? ? -? , ..., ? ? -1 , ? ? ]))<label>(2)</label></formula><formula xml:id="formula_2">?? ? ?? ? ? )?<label>(3)</label></formula><p>The ?, ?, ? denote to packed matrices of queries, keys and values, ? ? is a scaling factor. If it is under a self-attention scheme, ?, ?, ? can be substituted as the input representation ? . Figure <ref type="figure" target="#fig_1">2</ref> shows and illustration of the attention-based "encoder-decoder" transformer, which will be explained in detail in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CAT FRAMEWORK</head><p>In this section, we introduce a novel self-attentive encoder-decoder transformer framework, Content-Aware Transformer (CAT) for event sequence anomaly detection. In CAT, the encoder captures preamble sequential pattern with content awareness and the decoder bounds the event sequence distribution for anomaly detection.</p><p>In particular, we propose to address three major challenges:</p><p>? C1: Existing transformer-based approaches cannot naturally capture the content information in event sequences, and although some of the existing anomaly detection models consider semantic information, they are based on general word language models. Thus we state Challenge 1 (C1): How to design a transformer-based model that could capture comprehensive content information of event sequences? ? C2: Existing transformer models are typically designed for the sequence forecasting tasks, while our aim is anomaly detection. Thus we state Challenge 2 (C2): How to design a transformer model specifically for the anomaly detection problem in event sequences?</p><p>? C3: We expect our designed transformer can handle long sequences in a responsive manner. However, the self-attention mechanism of transformer limits the efficiency and the oneat-a-time forecasting scheme thresholds the capacity for long sequences. Thus we state Challenge 3 (C3): How to design a scalable and efficient transformer for event sequence anomaly detection?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>Our proposed CAT framework is under a self-attentive encoderdecoder transformer architecture. In order to capture comprehensive semantic information of events, we build a content-awareness layer to extract semantic patterns. In the encoder, preamble event sequences are given as inputs, being encoded as feature maps. In the decoder, a special sequence token [SEQ] integrates at the beginning of input sequence, denoting the sequence state. We apply a oneclass objective to bound the decoded [SEQ] token representations together with a reconstruction loss for sequence forecasting. By using a collective sequence forecasting scheme, our CAT framework could deal with event sequences of longer length, and by substituting the original self-attentional module to a sparse attention block, we address the efficiency issue. Figure <ref type="figure" target="#fig_3">3</ref> shows the overall architecture of CAT. We will present the details of CAT in following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Self-Attentional Encoder-Decoder Paradigm</head><p>In terms of training scheme, the designed transformer can be trained either directly with a task-specific loss <ref type="bibr" target="#b11">[12]</ref>, or with an "encoderdecoder" structure <ref type="bibr" target="#b34">[35]</ref>. Figure <ref type="figure" target="#fig_1">2</ref> shows the general architecture of the self-attentional encoder-decoder paradigm. For a typical "encoder-decoder" structure, the encoder generates hidden representations ? ? of inputs ? ? , while the decoder generates output </p><p>Here ? ? ? ,? ? ? ,? ? ? are projection matrices for each head ?, ? ? denotes to the projection matrix of the concatenated multi-heads. Each of the multi-head attention layer is followed by a fully connected feed-forward network:</p><formula xml:id="formula_4">FFN(?) = ReLU(?? 1 + ? 1 )? 2 + ? 2<label>(5)</label></formula><p>In the decoder part, a step-by-step dynamic decoding process is usually applied that predicts output representations from ?-th to (? + 1)-th step, until reaching the end of the sequence. In the following sections, we will describe concretely how to design the encoder and decoder for the task of event sequence anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Encoder: Sequential Transformer with Content-Awareness</head><p>The encoder is specialized for extracting sequential patterns of preamble event sequences with content-awareness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Content-Awareness</head><p>Layer. We design a content-awareness layer in order to capture comprehensive semantic patterns of events. Given an event ?, we add one special classification token, [???] at the start of the event token sequence, and another special token</p><p>[???] at the end of the sequence. The content-awareness layer starts with a pre-trained BERT and being further fine-tuned over the event corpus throughout the training process. We propose the following objective to capture the semantic patterns:</p><formula xml:id="formula_5">L ??? = argmin ? - |? | ?? ? =1 ? ?? ?=1 ? ? ?,? log ? ?,? (??? ? |? \? ; ?)<label>(6)</label></formula><p>Here ? is the vocabulary size, ? ? ?,? is a binary indicator of 0 and 1 that if or not observation ? of token ? is equal to the current class ?, ? ?,? is the predicted language model (LM) probability of the current class ?, ? \? = {??? 1 , ..., ??? ? -1 , ??? ? +1 , ..., ??? |? | } is the token sequence excluding the current token ?, ? is the set of model parameters of the content-awareness layer.</p><p>In terms of extracting event representations, we extract the event's representation directly via regression over their [CLS] token, noted as ? [??? ] . Figure <ref type="figure" target="#fig_4">4</ref> shows the architecture of the contentawareness layer. The designed content-awareness layer properly address Challenge 1 (C1).  <ref type="formula">7</ref>, we substitute the original fully connected feed-forward network as a 1-D convolutional filter (kernel width=3) with the ELU(?) activation function, which is proved to be effective in <ref type="bibr" target="#b8">[9]</ref>. We also downsamples the input by adding a MaxPool layer with stride=2. This serves as two purposes that it decreases the memory usage as well as distilling redundant values drawn from the self-attention mechanism. Similar technique is proved to be effective for long-sequence forecasting in <ref type="bibr" target="#b43">[44]</ref>. The encoded long-term and short-term representations (? ? and ? {?,? } ) are concatenated as the final feature map:</p><formula xml:id="formula_6">Y = [? ? ] ? [? ?,? ] ???=1<label>(8)</label></formula><p>where ? ? ,? {?,? } denote to the corresponding encoded feature maps of ? ? and ? {?,? } , [?] ? [?] ???=1 is the concatenation operator of two matrices on the column-wise dimension. The encoded feature map Y will serve as Key and Value inputs of the attention block in the decoder, which is to be described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Decoder: Auto-Regression Preserved Long Sequence Anomaly Detection</head><p>Our decoder is specifically designed for event sequence anomaly detection in a form of the auto-regression preserved long sequence forecasting together with a one-class anomaly detection objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Generative</head><p>Inference for Long Sequence Forecasting. The input sequence of decoder ? ? is typically chronologically after the preamble sequence ? of encoder, either with or without event overlaps. Similar to the encoder, the event sequence is first fed into the content-awareness layer, generating the event representation sequence ? ? ? . Instead of using the original one-at-a-time forecasting procedure introduced in <ref type="bibr" target="#b34">[35]</ref>, we employ a generative inference that utilizes a one-time inference for prediction. Specifically, we select a sub-sequence of ? ? ? as the start sequence, which is denoted as</p><formula xml:id="formula_7">? ? ? \? ? = {? [??? ] 1 , ? [??? ]<label>2</label></formula><p>, ..., ?</p><p>[??? ]</p><p>? ? -1 }. The remaining part of sequence, i.e. the prediction sequence is substituted as padded zeros, noted as the padding matrix ? {? ? ,? ? } 0 . The decoder infers the zeropadded prediction sequence by one forward procedure rather than the conventional left-to-right decoding with one position offset, which is time consuming. In order to capture the characteristic of the whole decode event sequence, we add representation of one special sequence token, ? [??? ] at the start of the decode event representation sequence, forming the complete decoder input X as:</p><formula xml:id="formula_8">X = [? [??? ] ] ? [? ? ? \? ? ] ? [? {? ? ,? ? } 0 ] ???=1<label>(9)</label></formula><p>For each stack of the decoder layer, it first comes with a masked selfattention sub-layer that preserves the auto-regressive property. It is implemented by masking out (setting to -?) all illegal dot-product attention values. The procedure in the decoder that forwards from ?-th layer into (? + 1)-th layer is:</p><formula xml:id="formula_9">? ?+1 ? ? = FFN(Attention( Mask-Attention (? ? ? ), Y, Y))<label>(10)</label></formula><p>In the attention sub-layer, Y serves as the packed Key and Value matrices in equation 3. The output of the last stacked layer is followed by a fully connected layer that transfers the shape back to X for comparison. The compete decoded output Z is denoted as:</p><formula xml:id="formula_10">Z = [? [??? ] ] ? [? ? ? \? ? ] ? [? {? ? ,? ? } ] ???=1<label>(11)</label></formula><p>It consists of three parts, ? [??? ] is the decoded output of [SEQ] token, ? ? ? \? ? is the decoded output sequence of the start sequence ? ? ? \? ? and ? {? ? ,? ? } is the decoded output sequence for the padded prediction sequence. By comparing the ground-truth and forecast result of prediction sequence, we propose the reconstruction loss L ??? as:</p><formula xml:id="formula_11">L ??? = argmin ? 1 |? {? ? ,? ? } | |? {? ? ,? ? } | ?? ?=? ? ?? ? {? ? ,? ? } -? ? {? ? ,? ? } ? 2<label>(12)</label></formula><p>Here ? ? {? ? ,? ? } and ? ? {? ? ,? ? } are the ?-th event of ? {? ? ,? ? } and ? {? ? ,? ? } , |? {? ? ,? ? } | is the number of event in ? {? ? ,? ? } , which is |? ? | -? ? + 1. ? is the set of model parameters.</p><p>Our designed reconstruction loss L ??? avoids the use of softmax function that maps events to their corresponding alphabets. Getting rid of event alphabet mapping is beneficial for at least two aspects: (i) Increase the forecasting speed when the alphabet space of events is very large, such as the sensor data of drones. (ii) Easy to deal with out-of-vocabulary (oov) events or events without straight access of corresponding alphabets, e.g. the system calls log data needs parsing to obtain template indices, which could be inaccurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Event</head><p>Sequence Anomaly Detection with One-Class Objective Function. In order to enable the self-attentive encoder-decoder transformer model for the anomaly detection task in event sequences, we adopt a one-class objective function, which can be trained with objects of only one class but can indicate if a new object belongs to this class or not. Specifically, we treat the decoded output of our added [SEQ] token as the latent representation of an event sequence. A natural assumption is that all normal event sequences' latent representations should distribute closely with each other, forming a hypersphere. A one-class objective aims at finding a hypersphere with minimum volume that could include all the normal event sequences' latent representations. During the testing stage, any sequence whose latent representation lies outside the hypersphere boundary is classified as anomaly. We propose our loss function L ?? as:</p><formula xml:id="formula_12">L ?? = argmin ? ?? ? ?? [??? ] ? -a? 2 (<label>13</label></formula><formula xml:id="formula_13">)</formula><p>Here a is the center of all decoded output of [SEQ] tokens in the training set S, where ?</p><p>[??? ] ? corresponds to token representation of sequence ? ? ? S.</p><p>Such form of loss function, although being commonly used in other works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36]</ref>, only forces latent representations to be close with each other, but not forming a decision boundary. Some ad-hoc heuristics have been introduced to fix this issue, such as selecting the latent representation whose distance to center ranks at certain percentile as the decision boundary's radius <ref type="bibr" target="#b16">[17]</ref>. Instead, we choose to find a "soft-margin" decision boundary based on the trained latent representations by solving the optimization problem <ref type="bibr" target="#b33">[34]</ref>:</p><formula xml:id="formula_14">argmin ?,?,? ? 2 + ? ?? ? ? ?<label>(14)</label></formula><p>with the constraint:</p><formula xml:id="formula_15">?? * [??? ] ? -a * ? 2 ? ? 2 + ? ? , ? ? ? 0 ? ? ? S ??<label>(15)</label></formula><p>? is a parameter that controls the trade-off between the volume ? and the errors ?. ? * [??? ] ? and a * are the trained latent representations and center. The designed one-class objective together with decision boundary optimization properly address Challenge 2 (C2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Scalable and Efficient Optimization</head><p>Till now, we have fully presented the general architecture of our proposed CAT framework. In this subsection, we will introduce how we take care of both scalability and efficiency issues. Some of the techniques have been elaborated in the previous sections. We list all the efficiency-related designs as bullet points bellow:</p><p>? Our encoder-decoder transformer-based model CAT has no need for creating multiple observation instances of one event sequence, making it irrelevant to sequence length. ? We substitute fully connected feed-forward network modules (FFN) to 1-D convolutional filters, followed by a Maxpool layer that further down-samples the size. ? The decoder of CAT performs a one-time inference for predicting all events rather than the conventional left-to-right decoding with one position offset, and is optimized under a ?2-norm reconstruction loss, making it less sensitive to sequence length and irrelevant to event alphabet space. ? We adopt a sparse version of the self-attention module that preserves top-K valued attentions introduced in <ref type="bibr" target="#b43">[44]</ref>. Together with all the efficiency-oriented settings, our CAT is rather efficient in comparison with other event sequence anomaly detection approaches, effectively addressing Challenge 3 (C3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Data Source</head><p>We start by collecting event sequence data sources to carry out our experiments. Log data of system calls is one of the most studied event sequences. The topic of log anomaly detection has also long been discussed with a decent amount of baselines and public datasets. Among several potential candidates, we choose three publicly available popular datasets which are examined by peer researchers. In this paper, we use log data to show the effectiveness of our approach and it can also be generalized to other domains. Table <ref type="table" target="#tab_1">2</ref> shows the statistics of the datasets. Hadoop Distributed File System (HDFS <ref type="bibr" target="#b39">[40]</ref>): This dataset was generated by a Hadoop-based map-reduce cloud environment using benchmark workloads, and manually labeled through handcrafted rules to identify the anomalies. The logs are sliced into traces according to block ids. Each trace associated with a specific block id is assigned a groundtruth label (normal/anomaly). It contains 11,175,629 log messages, of which 284,818 are anomalies.</p><p>BlueGene/L (BGL <ref type="bibr" target="#b30">[31]</ref>): BGL is an open dataset of logs collected from a BlueGene/L supercomputer system at Lawrence Livermore National Labs (LLNL) in Livermore, California, with 131,072 processors and 32,768GB memory. The log contains alert and non-alert messages identified by alert category tags. This dataset contains 4,747,936 log messages, of which 348,460 are anomalous.</p><p>ThunderBird <ref type="bibr" target="#b30">[31]</ref>: Thunderbird is an open dataset of logs collected from a Thunderbird supercomputer system at Sandia National Labs (SBNL) in Albuquerque, with 9,024 processors and 27,072GB memory. The label information also comes from alert and non-alert messages identified by alert category tags. We sample a sub-dataset with 20M log messages, 758,562 of which are anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head><p>Baselines: To better demonstrate the performance of our CAT model, we include a wide range of state-of-the-art baseline algorithms. Note that our experiments are all under the label efficiency setting that only normal log sequences are used during the training stage. The baselines can be divided into four groups: (i) methods using pattern recognition and similarity comparisons (PCA, iForest, LogCluster, Invariants Mining) (ii) methods utilizing one-class objective functions (OC-SVM, OC4Seq) (iii) content-irrelevant deep methods that only use log template indices as input (DeepLog, LogBERT) (iv) methods with content-awareness (Logsy, AutoEncoder, LogAnomaly, DeepLog w/ Log2Vec). The descriptions of baselines, detailed evaluation protocols and experimental setup can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Overall Performance.</head><p>To demonstrate the effectiveness of our model, we first compare our CAT with all the baseline methods on performing log sequence anomaly detections. Table <ref type="table" target="#tab_2">3</ref> presents the experimental results, from which we can see some interesting facts: (i) The performance of CAT surpasses the baseline methods in terms of F-1 score, AUC score and AUPR score on BGL and Thunderbird datsets. It demonstrates the superior capability of our proposed framework on anomaly detection task in event sequences.</p><p>(ii) Although some methods achieves very high precision score such as DeepLog, LogAnomaly, Deeplog w/ Log2Vec in HDFS dataset and LogCluster in BGL dataset, some methods achieves very high recall score such as OC-SVM, Logsy in HDFS datset and OC-SVM, DeepLog, LogAnomaly, DeepLog w/ Log2Vec in Thunderbird dataset, their F-1 score is relatively low, which indicates that either the high-precision anomaly detection approaches let go of too many anomalies or the high-recall approaches raise too many false alarms. (iii) Some approaches could only do well in one out of three datasets, e.g. LogCluster performs well in Thunderbird dataset with 95.91% in F-1 score. However, its performance is very poor in HDFS dataset with only 18.77% in F-1 score; (iv) although the OC-SVM model performs well under ranking evaluation metrics (98.13% in AUC score) in HDFS dataset, the classification metric scores are very low (5.15% in F-1 score), which indicates that although OC-SVM could properly sort all the test sequeces in HDFS dataset, its decision boundary needs to be improved. (v) The performance of LogAnomaly and DeepLog w/ Log2Vec consistently outperforms DeepLog, which illustrates the benefit of considering semantic content information for event sequence anomaly detection. (vi) The performance of Logsy consistently performs well in Recall score in all three datasets, which illustrates the benefit of using transformerbased architecture to process the semantic information. However, as it is not designed for sequence anomaly detection, the Precision score keeps low that raises too many false alarms.</p><p>The unbalanced performance of other baseline approaches illustrates our model's robustness on both classification and ranking metrics in all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Ablation Study.</head><p>We also conduct some ablation experiments to check how each part of our model affects the final results. We experiment by ablating different parts of our framework, and included in the results in Table <ref type="table" target="#tab_2">3</ref>. To clarify, CAT ? is CAT with untrained content-awareness layer (fixed pre-trained BERT) in order to see the effectiveness of our learned semantic information. CAT ? is CAT with conventional dense attentive module and left-to-right forecasting with one position offset at a time. From the results, we can draw the following arguments: (i) the comparison between CAT ? and CAT shows that our designed content-awareness layer help improve the model performance to a significant margin; (ii) the comparison between CAT ? and other baselines demonstrates that even without log-specific semantic information, our proposed encoder-decoder transformer-based model together with specifically designed loss functions still outperforms other baselines to certain degrees; (iii) the comparison between CAT ? and CAT shows that our scalable setting does not harm the overall performance and even gains a significant improvement in Thunderbird dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Scalability Study.</head><p>In order to compare the scalability for deep neural network-based approaches, we show the runtime comparisons between LogAnomaly, OC4Seq (LSTM-based approaches), LogBERT (bert-like approach without decoder), AutoEncoder (Conventional Transformer-based approach) and our models (CAT ? and CAT. Figure <ref type="figure" target="#fig_6">5</ref> shows the average training epoch time of all approaches in different encoder preamble sequence length (? ? ) and decoder predict sequence length (? {? ? ,? ? } ). All experiments are conducted on the Thunderbird dataset. We observe that our  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Case Study</head><p>From Table <ref type="table" target="#tab_2">3</ref> we can see that our CAT model gains significant improvements comparing to our ablated dense model CAT ? in the Thunderbird dataset. To further illustrate the insights, we conduct a case study over the Thunderbird dataset. Figure <ref type="figure" target="#fig_7">6</ref> shows the histogram of sequence lengths and log template indices (alphabets) distribution (doughnut charts) of normal and anomaly sequences.</p><p>From the histogram we can see that the sequence length distribution of normal and anomaly sequences differs a lot: The 5% and 95% percentile of normal log sequence length is 30 and 667, while for anomaly sequences they are 138 and 1484. Such long-sequence patterns cannot be captured under a short fixed observation window, which may be the reason for the failure of observation windowbased approaches in the Thunderbird dataset. As for the alphabet distribution, two doughnut charts show the alphabet count distributions of normal and anomaly sequences. The alphabet indices [SEQ] token is added at the start of decode sequence representing sequence state. The decoder forecasts prediction sequence in a one-time auto-regression preserved inference. A one-class objective function is utilized enabling the transformer for the task of anomaly detection. The scalability and efficiency concerns are properly strengthened by applying a list of efficiency-specific designs. The proposed CAT model provided a general solution to process content-rich event sequences. Finally, extensive experimental results on three real-world log event sequence datasets demonstrated CAT's superiority over other baselines for anomaly detection in event sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX Description of Baselines</head><p>? PCA <ref type="bibr" target="#b39">[40]</ref>: It is a Principle Component Analysis (PCA)-based approach that extracts major components of input sequence features and report above-threshold testing sequences as anomalies.</p><p>? iForest <ref type="bibr" target="#b22">[23]</ref>: It is a tree-based model that explicitly isolates anomalies instead of profiles normal points without any distance or density measures. ? LogCluster <ref type="bibr" target="#b21">[22]</ref>: It is a cluster-based approach that utilizes a knowledge base to check if the log sequences occurred before and only examine unseen logs. ? Logsy <ref type="bibr" target="#b29">[30]</ref>: It is a transformer-based approach for single log event anomaly detection. Logsy is not designed for sequence, but its transformer-based architecture sheds similarity with us.</p><p>? AutoEncoder <ref type="bibr" target="#b13">[14]</ref>: It is an auto-encoder-based approach that generates latent representations of log events and uses Isolation Forest (iForest) for anomaly detection.</p><p>? LogAnomaly <ref type="bibr" target="#b27">[28]</ref>: It is a LSTM-based model that generates word embeddings over log messages with strengthen of synonyms and antonyms, and use summation of word embeddings as log representation. ? DeepLog w/ Log2Vec <ref type="bibr" target="#b26">[27]</ref>: Log2Vec is a log-specific word embedding method that generates log specific semantic representations, and feed them into DeepLog for anomaly detection.</p><p>Evaluation Protocols We use Drain 1 as parser to extract templates of logs, generating structured log files. For HDFS dataset, the log messages are grouped into sequences by the BlockId. For BGL and Thunderbird dataset, a five-minute sliding window is applied to generate log sequences. A log sequence is anomaly if it contains any labeled anomaly log messages. We split the generated sequences into 3:1:4, where 30% as training set, 10% as validation set, 60% as test set. Note that we are conducting all the experiments under the label efficiency setting, i.e. there are only normal log sequences in the training set.</p><p>We compare the results on two categories of evaluation metrics. The first is classification correctness, including Precision, Recall 1 https://github.com/logpai/logparser and F-1 score. The other group is ranking metrics, including the AUC and AUPR score.</p><p>Experimental Setup For the Logsy baseline approach, we conduct it over each log message within a sequence, and report the sequence as anomaly if any one of the log message is classified as anomaly by Logsy. For observation window-based approach, we select the corresponding window-size that is best reported in their corresponding papers, e.g. for DeepLog and DeepAnomaly, the window-size is set to 10. For next-log prediction approach, we set the candidate number as the best reported in the corresponding paper as well, e.g. for HDFS data, the number of candidates is set to 10. For BGL and Thunderbird, we set the number of candidates that could best produce results in terms of F-1 score. The best number of candidates for BGL and Thunderbird is 50. For LSTM-based approaches, the number of layers is set to be 2 and the number of hidden units is set to 256. For transformer-based approaches, the number of layers for both encoder and decoder is set to be 2. The head number of multi-head attention is set to be 8, and the hidden state dimension is set to 256. For the dynamic encoder/decoder sequence length experiments, since LogBERT, LogAnomaly and OC4Seq does not have decoder structure, the change of decoder sequence length is identical to the change of window size. For oneclass objective-based approaches, we use the proposed method of generating decision boundaries. We implement our model using PyTorch. The length of preamble sequence, start sequence and predict sequence length is set as {32,16,32} in HDFS dataset, {64,32,256} in BGL dataset and {64,32,512} in Thunderbird dataset. For any sequence with length shorter than our model input length, we pad it using a specific [PAD] token. The key parameters of our approach are set to be consistent with baselines for fair comparisons, e.g. hidden state dimension is set fo 256, number of layers 2 and multihead attension 8. For our CAT model, our content-awareness layer uses a pre-trained BERT model from Transformers python library. For baselines, we use the template index sequence as input and use tf-idf feature with the same dimension of our model. In the process of model training, we use the Adam optimizer for parameter optimization. We set learning rate at 0.01 and batch size at 32. The batch size is adjusted during the scalability experiments to reach the GPU capacity. All the extra parameters of the baselines are set up similarly as our method and carefully tuned to ensure fair comparisons. The implementations of all the baselines are from public repositories, e.g. LogBERT<ref type="foot" target="#foot_1">2</ref> , OC-SVM <ref type="foot" target="#foot_2">3</ref> , AutoEncoder<ref type="foot" target="#foot_3">4</ref> and OC4Seq <ref type="foot" target="#foot_4">5</ref> . The implementation of our CAT model is also available as a public repository <ref type="foot" target="#foot_5">6</ref> .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrations of the content-aware event sequence anomaly detection task.</figDesc><graphic url="image-2.png" coords="2,77.82,83.68,192.20,170.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustrations of the encoder-decoder transformer.</figDesc><graphic url="image-3.png" coords="3,353.99,83.69,168.17,149.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 3 . 1 .Definition 3 . 2 .Definition 3 . 3 .</head><label>313233</label><figDesc>Event: An event is considered as a sequence of tokens. Formally, an event ? = {??? 1 , ??? 2 , ..., ??? |? | }, where ??? ? represents the ?-th tokenized words/symbols or numbers, |? | is the total number of tokens. Event sequence: An event sequence can be described as a consecutive sequence of events ordered chronologically within an observed time window. Formally, an event sequence ? = {? 1 , ? 2 , ..., ? |? | }, where ? ? represents the ?-th event, |? | is the total number of events sampled within a time window. Event sequence anomaly detection: Given a set of event sequences S = {? 1 , ? 2 , ..., ? | S | } as the training set, An anomaly detection model learns key patterns that given a new event sequence, the model could indicate if it is a normal sequence or an anomaly. If it is under an unsupervised learning scheme, the training event sequence set S will only contain normal sequences. Definition 3.4. Sequential Neural Networks: The problem of neural sequential pattern learning refers to using Deep Neural Network-based approaches to extract knowledge from sequential data. Many popular models are devised to construct hidden representations ? ? for the input representations ? ? , where ? refers to the ?-th indexed token of sequence. There are mainly three popular types of message passing modules. The first is recurrent-based message passing:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The architecture of the Content-Aware Transformer (CAT). Event sequence ? ? is under anomaly detection. ? is the preamble event sequence. Blue cells are event representations extracted by the content-awareness layer. The encoder (the above part) encodes preamble sequences into feature maps. A special token [SEQ] is added to the start of decoder input sequence (orange cell), denoting the sequence status. Part of the decoder input is padded as zeros for forecasting. The one-class objective L ?? bounds all sequences' [SEQ] token, and the reconstruction loss L ??? optimizes sequence forecasting.</figDesc><graphic url="image-4.png" coords="4,65.75,83.68,480.50,167.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Architecture of the content-awareness layer.</figDesc><graphic url="image-5.png" coords="5,77.82,83.69,192.19,106.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 3 . 2</head><label>32</label><figDesc>Sequential Layer. For a given input of preamble event sequence ? = {? 1 , ? 2 , ..., ? |? | }, we generate the event representation sequence ? ? = {? to regression over [CLS] token of event ? ? . We stack a subsequence of ? ? , denoted as ? {?,? } = {? refers as local, indicating the sub-sequence encodes information from local perspective. Such setting ensures both long-term and short-term memories are preserved. The procedure that forwards from ?-th layer into (? + 1)-th layer is: ? ?+1 ? = MaxPool ( ELU( Conv1d( Attention(? ? ? , ? ? ? , ? ? ? )))) (7) Comparing to Equation 4 and Equation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training epoch time comparisons with different encoder/decoder sequence length.</figDesc><graphic url="image-6.png" coords="8,53.80,258.11,240.24,115.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Histogram of sequence lengths and log template indices (alphabets) distribution of normal and anomaly log sequences in Thunderbird dataset.</figDesc><graphic url="image-7.png" coords="8,317.96,258.11,240.24,119.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mathematical Notations {??? 1 , ??? 2 , ..., ??? |? | } an event ? is a sequence of tokens. ? ? = {? 1 , ? 2 , ..., ? |? | } sequence of chronologically ordered events. S S = {? 1 , ? 2 , ..., ? |S | } set of training event sequences. Projection matrices and bias terms, e.g. ? ? ,? ? , ? 1 , ? 2 . ???=1 Concatenation of two matrices at column-wise dimension. L Loss objectives, including L ??? , L ??? , L ?? . Representation of additional sequence token. ? ? Representation sequence of ? from index one to |? |. ? {?,? } Representation sequence of ? from index ? to |? |. ? ?\? Representation sequence of ? from index one to ? -1. Zero-padded representation sequence of ? from index ? to |? |. ? ? , ? {?,? } Encoded feature maps. Decoded output sequence of ? from index ? to |? |. ? ?\? Decoded output sequence of ? from index one to ? -1.</figDesc><table><row><cell cols="2">Symbol</cell><cell>Description</cell></row><row><cell cols="2">???</cell><cell>Tokenized words, symbols or numbers.</cell></row><row><cell cols="3">? ? = ? ? Input representation of ?-th indexed token of sequence.</cell></row><row><cell cols="2">? ?</cell><cell>Hidden representation of ?-th indexed token of sequence.</cell></row><row><cell cols="2">?, ?, ?</cell><cell>Packed matrices of queries, keys and values for attention.</cell></row><row><cell cols="2">? , ?</cell></row><row><cell cols="2">?, ?</cell><cell>Model parameters.</cell></row><row><cell cols="2">? (?)</cell><cell>Activation function, e.g. ReLU(?), ELU(?).</cell></row><row><cell cols="2">[?] ? [?] ? [??? ]</cell><cell>Representation of additional classification token.</cell></row><row><cell cols="2">? [??? ]</cell></row><row><cell>?</cell><cell>{?,? } 0</cell></row><row><cell cols="2">? [??? ]</cell><cell>Decoded output of [SEQ] token.</cell></row><row><cell cols="2">? {?,? }</cell></row><row><cell cols="2">X</cell><cell>Complete input of decoder.</cell></row><row><cell cols="2">Y</cell><cell>Concatenated feature maps.</cell></row><row><cell cols="2">Z</cell><cell>Complete decoded output.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of three datasets.</figDesc><table><row><cell></cell><cell>HDFS</cell><cell>BGL</cell><cell>Thunderbird</cell></row><row><cell># Log Messages</cell><cell cols="2">11,172,157 4,747,963</cell><cell>20,000,000</cell></row><row><cell># Anomalies</cell><cell>284,818</cell><cell>348,460</cell><cell>758,562</cell></row><row><cell># Total Sequences</cell><cell>575,061</cell><cell>24,294</cell><cell>59,391</cell></row><row><cell># Anomaly Sequences</cell><cell>16,873</cell><cell>1,637</cell><cell>22,164</cell></row><row><cell>Average Length</cell><cell>19</cell><cell>195</cell><cell>336</cell></row><row><cell>Alphabet Size</cell><cell>49</cell><cell>762</cell><cell>1,185</cell></row><row><cell>Unseen Alphabets</cell><cell>28</cell><cell>591</cell><cell>173</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Overall performance.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell>HDFS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BGL</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Thunderbird</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Classification Metrics</cell><cell cols="2">Ranking Metrics</cell><cell cols="3">Classification Metrics</cell><cell cols="2">Ranking Metrics</cell><cell cols="3">Classification Metrics</cell><cell cols="2">Ranking Metrics</cell></row><row><cell>Algorithm</cell><cell>?????????</cell><cell>??????</cell><cell>? -1</cell><cell>?? ?</cell><cell>?? ??</cell><cell cols="2">????????? ??????</cell><cell>? -1</cell><cell>?? ?</cell><cell>?? ??</cell><cell>?????????</cell><cell>??????</cell><cell>? -1</cell><cell>?? ?</cell><cell>?? ??</cell></row><row><cell>PCA</cell><cell>95.26%</cell><cell>49.28%</cell><cell>64.96%</cell><cell>81.77%</cell><cell>58.09%</cell><cell>35.20%</cell><cell>17.09%</cell><cell>23.01%</cell><cell>52.18%</cell><cell>29.91%</cell><cell>89.81%</cell><cell>58.46%</cell><cell>70.82%</cell><cell>76.57%</cell><cell>70.96%</cell></row><row><cell>iForest</cell><cell>8.29%</cell><cell>10.59%</cell><cell>9.30%</cell><cell>54.17%</cell><cell>2.57%</cell><cell>25.56%</cell><cell>93.61%</cell><cell>40.15%</cell><cell>48.35%</cell><cell>26.20%</cell><cell>44.71%</cell><cell>96.21%</cell><cell>61.05%</cell><cell>50.00%</cell><cell>45.66%</cell></row><row><cell>LogCluster</cell><cell>10.72%</cell><cell>75.28%</cell><cell>18.77%</cell><cell>79.11%</cell><cell>8.73%</cell><cell>97.37%</cell><cell>18.70%</cell><cell>31.38%</cell><cell>59.25%</cell><cell>41.64%</cell><cell>97.65%</cell><cell>94.23%</cell><cell>95.91%</cell><cell>96.16%</cell><cell>94.65%</cell></row><row><cell>Invariants Mining</cell><cell>89.99%</cell><cell>87.04%</cell><cell>87.01%</cell><cell>93.34%</cell><cell>76.06%</cell><cell>64.75%</cell><cell>64.62%</cell><cell>64.69%</cell><cell>75.19%</cell><cell>52.04%</cell><cell>76.22%</cell><cell>58.72%</cell><cell>66.33%</cell><cell>71.67</cell><cell>63.59%</cell></row><row><cell>OC-SVM</cell><cell>2.64%</cell><cell>100.00%</cell><cell>5.15%</cell><cell>98.13%</cell><cell>79.72%</cell><cell>50.74%</cell><cell>60.38%</cell><cell>55.14%</cell><cell>55.56%</cell><cell>48.73%</cell><cell>62.29%</cell><cell>99.89%</cell><cell>76.73%</cell><cell>74.54%</cell><cell>62.27%</cell></row><row><cell>OC4Seq</cell><cell>83.62%</cell><cell>71.91%</cell><cell>77.30%</cell><cell>85.72%</cell><cell>60.91%</cell><cell>78.53%</cell><cell>92.71%</cell><cell>85.01%</cell><cell>95.45%</cell><cell>73.31%</cell><cell>87.83%</cell><cell>87.48%</cell><cell>87.71%</cell><cell>96.31%</cell><cell>87.52%</cell></row><row><cell>DeepLog</cell><cell>98.79%</cell><cell>47.44%</cell><cell>64.07%</cell><cell>73.72%</cell><cell>48.38%</cell><cell>74.73%</cell><cell>94.44%</cell><cell>83.42%</cell><cell>96.63%</cell><cell>70.92%</cell><cell>78.49%</cell><cell>99.96%</cell><cell>87.95%</cell><cell>91.73%</cell><cell>78.45%</cell></row><row><cell>LogBERT</cell><cell>79.23%</cell><cell>69.40%</cell><cell>73.99%</cell><cell>84.45%</cell><cell>55.80%</cell><cell>88.22%</cell><cell>94.67%</cell><cell>91.30%</cell><cell>97.12%</cell><cell>83.68%</cell><cell>88.07%</cell><cell>88.39%</cell><cell>88.30%</cell><cell>94.06%</cell><cell>88.07%</cell></row><row><cell>Logsy</cell><cell>24.13%</cell><cell>98.08%</cell><cell>38.75%</cell><cell>94.30%</cell><cell>23.74%</cell><cell>58.43%</cell><cell>94.46%</cell><cell>72.13%</cell><cell>95.14%</cell><cell>55.53%</cell><cell>24.13%</cell><cell>97.98%</cell><cell>37.70%</cell><cell>94.32%</cell><cell>23.73%</cell></row><row><cell>AutoEncoder</cell><cell>84.87%</cell><cell>83.55%</cell><cell>84.17%</cell><cell>97.15%</cell><cell>83.25%</cell><cell>82.63%</cell><cell>92.41%</cell><cell>87.19%</cell><cell>95.48%</cell><cell>76.79%</cell><cell>81.96%</cell><cell>66.38%</cell><cell>73.39%</cell><cell>95.36%</cell><cell>89.77%</cell></row><row><cell>LogAnomaly</cell><cell>99.05%</cell><cell>48.93%</cell><cell>65.59%</cell><cell>74.58%</cell><cell>50.06%</cell><cell>79.89%</cell><cell>93.80%</cell><cell>86.33%</cell><cell>96.08%</cell><cell>75.34%</cell><cell>79.96%</cell><cell>100.00%</cell><cell>88.84%</cell><cell>92.63%</cell><cell>79.94%</cell></row><row><cell>DeepLog w/ Log2Vec</cell><cell>99.13%</cell><cell>54.35%</cell><cell>70.29%</cell><cell>77.10%</cell><cell>55.22%</cell><cell>83.43%</cell><cell>92.90%</cell><cell>87.93%</cell><cell>95.79%</cell><cell>77.86%</cell><cell>80.51%</cell><cell>100.00%</cell><cell>89.21%</cell><cell>92.91%</cell><cell>80.48%</cell></row><row><cell>CAT  ?</cell><cell>95.42%</cell><cell>70.56%</cell><cell>81.13%</cell><cell>85.23%</cell><cell>68.13 %</cell><cell>88.60%</cell><cell>94.61%</cell><cell>91.48%</cell><cell>97.08%</cell><cell>84.01%</cell><cell>90.51%</cell><cell>99.00%</cell><cell>94.57%</cell><cell>97.54%</cell><cell>89.88%</cell></row><row><cell>CAT  ?</cell><cell>88.08%</cell><cell>90.97%</cell><cell>89.49%</cell><cell>95.32%</cell><cell>80.36%</cell><cell>88.79%</cell><cell>94.78%</cell><cell>91.70%</cell><cell>97.21%</cell><cell>84.28%</cell><cell>94.19%</cell><cell>98.33%</cell><cell>96.22%</cell><cell>96.61%</cell><cell>93.38%</cell></row><row><cell>CAT</cell><cell>88.37%</cell><cell>90.63%</cell><cell>89.47%</cell><cell>95.10%</cell><cell>80.38%</cell><cell>88.93%</cell><cell cols="3">95.94% 92.33% 97.79%</cell><cell>85.41%</cell><cell>99.35%</cell><cell>98.97%</cell><cell cols="2">99.16% 96.89%</cell><cell>99.24%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>? Invariants Mining<ref type="bibr" target="#b24">[25]</ref>: It is an algorithm to automatically discover sparse and integer program invariants with clear physical meanings in logs.? OC-SVM<ref type="bibr" target="#b32">[33]</ref>: It is a Support Vector Machine (SVM)-based method with a one-class objective. A kernel is applied to the input sequence and then a soft-margin decision boundary is obtained by solving a constrained optimization problem. ? OC4Seq [36]: It is a Long Short-Tem Memory (LSTM)-based approach with one-class objective loss functions of both global and local perspectives. ? DeepLog [13]: It is a LSTM-based model that predicts next log template index based on observation window instances. ? LogBERT [17]: It is a BERT-like model that trains log template index sequence under a masked language model loss and a one-class objective.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0"><p>CONCLUSIONSIn this paper, we investigated the anomaly detection problem in event sequences which is of great economic value and managerial meaning. Specifically, we proposed the Content-Aware Transformer (CAT) framework to detect anomalies in event sequences. We designed a content-awareness layer that could capture comprehensive event semantic information, generating the semantic event representation sequence. A self-attentive encoder-decoder transformer-based architecture is utilized, where in the encoder, preamble event representation sequence are taken as input and encoded as concatenated feature map. In the decoder, a special</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/HelenGuohx/logbert</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/logpai/loglizer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/logpai/deep-loglizer</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/wzwtrevor/Multi-Scale-One-Class-Recurrent-Neural-Networks</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/mmichaelzhang/CAT</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This research was partially supported by the <rs type="funder">National Science Foundation (NSF)</rs> via the grant number <rs type="grantNumber">III-2006387</rs>, <rs type="grantNumber">III-2040799</rs>, and <rs type="grantNumber">III-1814510</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NJUu2HC">
					<idno type="grant-number">III-2006387</idno>
				</org>
				<org type="funding" xml:id="_GjtD4tX">
					<idno type="grant-number">III-2040799</idno>
				</org>
				<org type="funding" xml:id="_s2rXbaU">
					<idno type="grant-number">III-1814510</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ramp loss K-Support Vector Classification-Regression; a robust and sparse multi-class approach to the intrusion detection problem</title>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mojtaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hosseini</forename><surname>Bamakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="113" to="126" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fingerprinting the datacenter: automated classification of performance crises</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moises</forename><surname>Goldszmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armando</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><forename type="middle">B</forename><surname>Woodard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Andersen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="111" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Similarity measures for categorical data: A comparative evaluation</title>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Boriah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 SIAM international conference on data mining</title>
		<meeting>the 2008 SIAM international conference on data mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LOF: identifying density-based local outliers</title>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rg</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2000 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Anomaly detection and diagnosis algorithms for discrete symbol sequences with applications to airline safety</title>
		<author>
			<persName><forename type="first">Suratna</forename><surname>Budalakoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashok</forename><forename type="middle">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">Eric</forename><surname>Otey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="101" to="113" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Anomaly detection for discrete sequences: A survey</title>
		<author>
			<persName><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="823" to="839" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">LogTransfer: Cross-system log anomaly detection for software systems with transfer In ISSRE</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenglin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangrui</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weibin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="37" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient anomaly detection by modeling privilege flows using hidden Markov model</title>
		<author>
			<persName><forename type="first">Sung-Bae</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyuk-Jang</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">computers &amp; security</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">Djork-Arn?</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">NPS@: network protein sequence analysis</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Combet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Blanchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Geourjon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilbert</forename><surname>Deleage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in biochemical sciences</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="147" to="150" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous drone racing? the UZH-FPV drone racing dataset</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Delmerico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Titus</forename><surname>Cieslewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henri</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Faessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6713" to="6719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeplog: Anomaly detection and diagnosis from system logs through deep learning</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guineng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1285" to="1298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised log message anomaly detection</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Farzad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gulliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICT Express</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="229" to="237" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comprehensive survey on network anomaly detection</title>
		<author>
			<persName><forename type="first">Gilberto</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">Jpc</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luiz</forename><surname>Fernando Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><forename type="middle">Lemes</forename><surname>Al-Muhtadi</surname></persName>
		</author>
		<author>
			<persName><surname>Proen?a</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Telecommunication Systems</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="447" to="489" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dimensions of cyber-attacks: Cultural, social, economic, and political</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anup</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Sousan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Laplante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Technology and Society Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="28" to="38" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Logbert: Log anomaly detection via bert</title>
		<author>
			<persName><forename type="first">Haixuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Joint Conference on Neural Networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hitanomaly: Hierarchical transformers for anomaly detection in system log</title>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carol</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongzhi</forename><surname>Luan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Network and Service Management</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2064" to="2076" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Data mining approaches for intrusion detection</title>
		<author>
			<persName><forename type="first">Wenke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Stolfo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Failure prediction in ibm bluegene/l event logs</title>
		<author>
			<persName><forename type="first">Yinglung</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramendra</forename><surname>Sahoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh IEEE International Conference on Data Mining (ICDM 2007)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="583" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Log clustering based problem identification for online service systems</title>
		<author>
			<persName><forename type="first">Qingwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuewei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="102" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Tony</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">2008 eighth ieee international conference on data mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
	<note>Isolation forest</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A statistical pattern based feature extraction method on system call traces for anomaly detection</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Japkowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfa</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Software Technology</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mining Invariants from Console Logs for System Problem Detection</title>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Network security situation awareness based on network simulation</title>
		<author>
			<persName><forename type="first">Song-Song</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="512" to="517" />
			<date type="published" when="2014">2014. 2014</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A semantic-aware representation framework for online log analysis</title>
		<author>
			<persName><forename type="first">Weibin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenglin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Zaiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 29th International Conference on Computer Communications and Networks (ICCCN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs</title>
		<author>
			<persName><forename type="first">Weibin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenglin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="4739" to="4745" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-attentive classification-based anomaly detection in unstructured logs</title>
		<author>
			<persName><forename type="first">Sasho</forename><surname>Nedelkoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmin</forename><surname>Bogatinovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Acker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Odej</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1196" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What supercomputers say: A study of five system logs</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Oliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Stearley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th annual IEEE/IFIP international conference on dependable systems and networks (DSN&apos;07)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="575" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><surname>Robert Pw Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-scale one-class recurrent neural networks for discrete event sequence anomaly detection</title>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3726" to="3734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detecting intrusions using system calls: Alternative data models</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Warrender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Pearlmutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 IEEE symposium on security and privacy</title>
		<meeting>the 1999 IEEE symposium on security and privacy</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="133" to="145" />
		</imprint>
	</monogr>
	<note>Cat. No. 99CB36344</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The impact of cyber attacks on the private sector. Briefing Paper</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Association for International Affair</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A survey of distance and similarity measures used within network intrusion anomaly detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><forename type="middle">J</forename><surname>Weller-Fahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><forename type="middle">A</forename><surname>Borghetti</surname></persName>
		</author>
		<author>
			<persName><surname>Sodemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="70" to="91" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Detecting large-scale system problems by mining console logs</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armando</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles</title>
		<meeting>the ACM SIGOPS 22nd symposium on Operating systems principles</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="117" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semi-supervised log-based anomaly detection via probabilistic label estimation</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1448" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Log Sequence Anomaly Detection Based on Local Information Extraction and Globally Sparse Transformer Model</title>
		<author>
			<persName><forename type="first">Chunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiyi</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Network and Service Management</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="4119" to="4133" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Performance evaluation of social network anomaly detection using a moving window-based scan method</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srijan</forename><surname>Driscoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">D</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">J</forename><surname>Fricker</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">H</forename><surname>Spitzner</surname></persName>
		</author>
		<author>
			<persName><surname>Woodall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quality and Reliability Engineering International</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1699" to="1716" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName><forename type="first">Haoyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieqi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wancai</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
