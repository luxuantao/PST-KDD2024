<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-Time Communication Analysis for On-Chip Networks with Wormhole Switching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Real-Time Systems Research Group</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Alan</forename><surname>Burns</surname></persName>
							<email>burns@cs.york.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Real-Time Systems Research Group</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-Time Communication Analysis for On-Chip Networks with Wormhole Switching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6FCD442CF35011DA1EC10957D029612B</idno>
					<idno type="DOI">10.1109/NOCS.2008.11</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we discuss a real-time on-chip communication service with a priority-based wormhole switching policy. A novel off-line schedulability analysis approach is presented. By evaluating diverse inter-relationships among the traffic-flows, this approach can predict the packet network latency based on two quantifiable different delays: direct interference from higher priority traffic-flows and indirect interference from other higher priority traffic-flows. Due to the inevitable existence of parallel interference, we prove that the general problem of determining the exact schedulability of real-time traffic-flow over the onchip network is NP-hard. However the results presented do form an upper bound. In addition, an error in a previous published scheduling approach is illustrated and remedied. Utilizing this analysis scheme, we can flexibly evaluate at design time the schedulability of a set of traffic-flows with different QoS requirements on a real-time SoC/NoC communication platform.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>With the development of semiconductor technology over the last fifteen year, it is possible to offer more than many tens of million of transistors on a single chip. Under this condition, designers are developing ICs integrating complex heterogeneous functional elements into a single device, known as a System-on-Chip (SoC).</p><p>Generally, SoC is an integrated circuit that implements most or all of the functions of a complete electronic system. In such a system, different components need a standard approach to support on-chip communication. Early SoCs employed busses or a point-to-point approach to fulfil the information exchange demands. However, with the rapid increase in the number of blocks to be connected and the increase in performance demands, busses and point-to-point based platforms suffer from limited scalability and quickly become a communication bottleneck <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>. On-chip packet-switched networks have recently been proposed as a significant solution for complex communication of SoCs. Network on Chip (NoC) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b3">[4]</ref> is an architectural paradigm for scalable on-chip interconnection architectures. This architecture offers a general and fixed communication platform which can be reused for a large number of SoC designs.</p><p>Networks as a subject has been studied for decades. However, the situation for NoC is different from off-chip networks meaning that we can not deploy general networks on SoC platforms directly. NoCs differ from off-chip networks mainly in that they are more constrained and less non-deterministic <ref type="bibr" target="#b2">[3]</ref>. The structure of an off-chip network or general network is in principle unknown, ie. topology is not fixed and behaviours of nodes are not predictable. So the protocols need enough adaptability to meet various requirements. Some results used in traditional networks can't be employed directly and must be re-evaluated. A few distinctive limitations are unique for on-chip networks, namely, minimal energy consumption, and small size <ref type="bibr" target="#b11">[12]</ref> (both computation and storage functions implemented in a small silicon area). Therefore many network design choices need to be modified so that the implementation cost as well as speed/throughput performance is acceptable.</p><p>The new on-chip communication architecture needs to provide different levels of service for various application components on the same network. One kind of communication, namely real-time communication, has very stringent requirements, the correctness relies on not only the communication result but also the completion time bound. For a packet transmitted over the network, this time bound is denoted by the packet network latency. A data packet received by a destination too late could be useless. For instance, the signal message packet or control message packet of an application requires timely delivery. The worst case acceptable time metric is defined to be the deadline of the packet. A traffic-flow is a packet stream which traverses the same route from the source to the destination and requires the same grade of service along the path. For hard real-time traffic-flows, it is necessary that all the packets generated by the traffic-flow must be delivered before their deadlines even under worst case scenarios. In another words, the maximum network latency for each packet can not exceed its deadline. A set of real-time traffic-flows over the network are termed schedulable if all the packets belonging to these traffic-flows meet their deadlines under any arrival order of the packet set.</p><p>As a popular switching control technique, wormhole switching <ref type="bibr" target="#b18">[19]</ref> has been widely applied for on-chip networks due to its greater throughput and smaller buffering requirement <ref type="bibr" target="#b11">[12]</ref>. However, few works have been done to analyze the real-time packet schedulablility for wormhole switching networks. In order to support real-time requirements, particularly satisfying its deadline bound, predictable behaviour of the network service is essential. But the situation for on-chip wormhole networks is partially non-deterministic due to the contentions in communication. In on-chip networks, several tasks running on different nodes exchange information periodically. During a transmission period, one transmitted packet shares the resources, such as buffers or physical links, with other packets. When several packets try to access the same resource at the same time, contention occurs and the network only can serve one packet and suspend the others based on some arbitration policy. Once a packet becomes blocked, it can block other packets, which can in turn block other packets, and so on. The exact analysis of congestion in this situation is hard <ref type="bibr" target="#b1">[2]</ref> due to the possibility of a packet becoming blocked at several routers during its journey from source to destination. The contention problem leads to packet delays and even missed deadlines. Therefore, it is necessary to give a scheduling strategy and analysis approach to predict whether all the real-time packets can meet their timing properties.</p><p>In this paper we explore real-time communication in wormhole switching for on-chip networks. We assume the prioritybased transmission preemption method <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>. A novel analyzable approach, the worst case network latency evaluation, is presented, within which a broad class of realtime communication services can be explored and developed. By evaluating diverse inter-relationships and service attributes among the traffic-flows, our model can predict the packet transmission latency for a given traffic-flow based on two quantifiable different delays: direct interference from higher priority traffic-flows and indirect interference from other higher priority traffic-flows. Due to the inevitable existence of parallel interference, we prove that the general problem of determining the exact schedulability of real-time traffic-flows over the on-chip network is NP-hard. We also prove that the real maximum network latency is bounded by the theoretical calculation in our model. By using this approach, we can flexibly evaluate at design time the schedulability of a trafficflow set with different quality of service (QoS) requirements in a real-time SoC/NoC communication platform.</p><p>The rest of this paper is organized as follows: section II introduces the major features of wormhole switching. A preemptive arbitration structure is deployed to implement priority-driven transmission scheduling. A novel real-time communication model and associated analysis are represented in sections III and IV. Section V discusses the limitation of our model when there exists parallel interference. Finally, section VI concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. WORMHOLE SWITCHING IN REAL-TIME</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>COMMUNICATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Wormhole switching</head><p>It is a major challenge for NoCs to provide real-time support. The area and energy constraints determine that the cut-through switching approach (wormhole switching) is the more practical deployment strategy than the store-and-forward switching (packet switching) policy <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>. In a wormhole switching network, data is encapsulated into a packet format for network transmission. So for convenience in discussing, a packet is treated as the basic information unit throughout this paper. Each packet in a wormhole network is divided into a number of fixed size flits <ref type="bibr" target="#b18">[19]</ref>. The header flit takes the routing information and governs the route. As the header advances along the specified path, the remaining flits follow in a pipeline way. If the header flit encounters a link already in use, it is blocked until the link becomes available. In this situation, all the flits of the packet will remain in the router along the path and only a small flits buffer is required in each router. But this blocking will decrease the available resource for other communication traffic-flows and reduce the network resource efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. A General NoC Platform</head><p>A general communication infrastructure for the wormhole on-chip network is illustrated in Figure <ref type="figure" target="#fig_1">1</ref>. A set of routers and point-to-point links interconnecting the routers are organized in a mesh structure. Each router has one or several intellectual property (IP) modules which hold tasks for execution. These tasks, executing on different IPs, communicate with each other by transmitting packets through the on-chip interconnection network. Two unidirectional links, one for each direction, connecting two routers realize the full-duplex transmission media. The network uses dimension-order X-Y routing, which is simple and easy to be implemented in the regular topology. The virtual channels (VCs) technique <ref type="bibr" target="#b5">[6]</ref> is deployed which decouples resource allocation by providing multiple buffers for each physical link in the network. Each of these buffers is considered as a virtual channel and can hold one or more flits of a packet. By combining with the virtual channels technique, the transmitting packet can bypass a blocked one. This strategy efficiently utilizes the network resource (link bandwidth) and improves the performance with a very small buffer overhead <ref type="bibr" target="#b4">[5]</ref>.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> also illustrates a number of traffic-flows loaded on this NoC platform. For example, τ 1 starts in router 7 and passes through routers 11 and 15 before terminating in router 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Priority preemptive arbitration</head><p>In conventional wormhole routers, data flits held by VCs access the output link based on first-come first-service arbitration. This scheme is suitable for non-real-time networks since it is fair and produces good average performance. But in real-time communication, the network must ensure each realtime packet meets its deadline bound. Priority arbitration is proposed to resolve this problem <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>. We employ the flit-level preemption method implemented by using virtual channels, which have similar structure as in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>. The arbitration with priority method uses priority preemption to provide delivery guarantees for hard deadline packets. We assume there are as many virtual channels as priority levels at each output port. Each virtual channel is assigned a different priority. An output port structure of a router is shown in Figure <ref type="figure" target="#fig_0">2</ref>. The traffic-flows loaded in the wormhole network have priorities associated with them. Each packet generated by a traffic-flow inherits the corresponding priority of the trafficflow. A packet with priority i can only request the virtual channels associated with priority value i. At any time the packet with the highest priority always gets the privilege to access the output link. In addition, a higher priority packet can also preempt a lower priority packet during its transmission. Since the real-time traffic-flows between different routers in a specific on-chip network is known a prior, a global priority assignment policy should be ameanable to off-line analysis <ref type="bibr" target="#b1">[2]</ref>. Consider n traffic-flows, each one with an associated virtual channel containing flits. The arbiter fetches flits from these queues according to priority arbitration and forwards them over a shared output link. If the highest priority packet can not send data because it is blocked elsewhere in the network, the next highest priority packet can access the output link. The allowable service time for a traffic-flow is all the time intervals at which no higher priority traffic-flow competes for the same physical link.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Current works</head><p>Wormhole switching achieves high throughput performance with less buffer requirement comparing with packet-switched technique <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>. But it also introduces unpredictable network delay <ref type="bibr" target="#b7">[8]</ref>. Hard real-time communication, on the contrary, requires the timing to be predictable even under the worst case situation. Besides this, the network resource allocation and scheduling for the real-time traffic-flow should be analysable during the design phase. Predictability of performance is essential for NoCs design to take early decisions before actual implementation. Fortunately, the communication pattern of a SoC is determined during a pre-configuration period; interconnection topology and characteristics of traffic pattern are foreseeable. Therefore, we need an off-line static evaluation approach to ensure the packet network latency never violates its timing bound. Utilizing this approach, we also can plan and explore the distribution of the real-time applications over the network to produce a very effective mapping.</p><p>The first work to explore the packet's timing property in wormhole switching was published by Li and Mutka <ref type="bibr" target="#b15">[16]</ref> in 1994. Utilizing the priority strategy, for a wormhole network with the same number of virtual channels as the number of priority levels, a packet can request only a virtual channel which is numbered lower than or equal to its priority <ref type="bibr" target="#b15">[16]</ref>. Song et al <ref type="bibr" target="#b19">[20]</ref> proposed a flow control approach to avoid the priority inversion problem. By flit-level preemption, the different priority traffic-flows can be catered for by a very small number of virtual channels. However the upper bound of network latency for each packet in the network are not delivered by Song and Li's methods. Balakrishnan et al <ref type="bibr" target="#b1">[2]</ref> proposed a quite naive and simple approach -lumped link to address this problem. All the links the traffic-flow travels are lumped as one shared resource -like a bus structure. Static priority preemptive policy is adopted to assure at any time only the highest priority traffic-flow can access the link resources. However, due to lumping, direct and indirect contentions are treated in the same way, Balakrishnan's result is sufficient but pessimistic <ref type="bibr" target="#b17">[18]</ref>. Hary et al <ref type="bibr" target="#b9">[10]</ref> utilized the same model proposed in <ref type="bibr" target="#b1">[2]</ref> but ignored indirect competition, the result of <ref type="bibr" target="#b9">[10]</ref> is optimistic. In this paper we treat the indirect contentions as interference jitter and get an upper bound on network latency. The analysis and relative example are represented in section IV. The analysis by Lu et al <ref type="bibr" target="#b17">[18]</ref> takes account of the parallel interference in disjoint traffic-flows and tries to minimize the direct interferences of higher priority traffic-flows. But this parallel consideration is not reasonable when worst case network latency is desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. REAL-TIME COMMUNICATION MODEL AND ASSUMPTIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. System model and attributes</head><p>The packet level analysis approach of real-time communication in general networks <ref type="bibr" target="#b21">[22]</ref> in the absence of buffer restrictions is not suitable for wormhole networks. We need a new analysis model for real-time wormhole switching communication. Some conditions, assumptions and explanations are essential.</p><p>A wormhole switching real-time network Γ comprises n real-time traffic-flows Γ ={τ 1 , τ 2 , . . . τ n }. Each traffic-flow τ i is characterized by attributes</p><formula xml:id="formula_0">τ i = (P i , C i , T i , D i , J R i ).</formula><p>We assume that all the traffic-flows which require timely delivery are periodic 1 . The length of time between releases of successive packets of τ i is a constant, which is called the period T i for this traffic-flow. Each traffic-flow τ i has a priority value P i . All the packets that belong to the τ i inherits the same priority P i . The value 1 denotes the highest priority and larger integers denote lower priorities. We assume the trafficflow is prioritized by any possible priority assignment policy. The issue of priority assignment is beyond the scope of this paper. Each real-time traffic-flow has deadline D i constraint which means all the packets belonging to this traffic-flow have the restriction that it should be delivered from a source router to a destination router within a certain delay bound even in the worst case situation. Our model has the same restriction as <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b17">[18]</ref> that each traffic-flow's deadline must be less than or equal to its period, D i ≤ T i for all τ i ∈ Γ. J R i is the release jitter <ref type="bibr" target="#b0">[1]</ref> denotes the maximum deviation of successive packets released from its period. If a packet from τ i is generated at time a, then it will be released for transmission by time a+J R i and have an absolute deadline of a + D i .</p><p>The basic network latency happens when no traffic-flow contention exists. The basic network latency is determined by its source/destination routing distance, packet size, link bandwidth and some additional protocol control overheads. For real-time evaluation purpose, we use the term maximum basic network latency. Let H denote the hops between source and destination nodes and S indicate the constant processing delay in each router. Let the maximum packet size belonging to τ i be L max i and the flit size for wormhole switching be f size , the maximum basic network latency C i <ref type="bibr" target="#b7">[8]</ref> is given by:</p><formula xml:id="formula_1">C i = L max i + L add f size • f size /B link + H • S<label>(1)</label></formula><p>where L add is the additional data for wormhole switching such as header and tail flit information. Here we only consider the longest possible, or maximum, basic network latency for evaluation. If the real-time traffic-flow can meet its deadline with the maximum basic network latency scenario, it will meet the deadline for any other basic network latency scenario. Note that the network latency here does not consider contention. The competing interventions can disturb and extend the packet network latency. The competing interventions and related worst case evaluation are discussed in section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inter-relationships between traffic-flows</head><p>To capture the relations between traffic-flows and the physical links of the network, we formalize the mesh network topology defined as a directed graph G : V × E. V is a set, whose elements are called nodes, each node v i denotes one router in the mesh network. E is a set of ordered pairs of vertices, called edges. An edge e x,y = {v x → v y } is considered to be a real physical link from router v x to router v y ; v x is called the source and v y is called the destination. We define a mapping space from the traffic-flow set to the physical links Γ → E. Given a set of n traffic-flows Γ, we can map them to the target network. The routing i of each traffic-flows τ i is denoted by the ordered pairs of edges, i = {e 1,2 , e 2,3 , . . . , e n-1,n }. If a traffic-flow τ i shares at least one link with τ j , the intersection set between them is i ∩ j . If i ∩ j = ∅, τ i and τ j are disjoint. For the case in Figure <ref type="figure" target="#fig_1">1</ref>, the routing of τ 1 , τ 2 , τ 3 and τ 4 are 1 = {e 7,11 , e 11,15 , e 15,14 }, 2 = {e 13,9 , e 9,5 , e 5,1 , e 1,2 }, 3 = {e 15,14 , e 14,13 , e 13,9 } and 4 = {e 13,9 , e 9,5 , e 5,1 }. The intersection sets between them are 1 ∩ 2 = 1 ∩ 4 = ∅, 1 ∩ 3 = {e 15,14 }, 2 ∩ 3 = {e 13,9 }, 2 ∩ 4 = {e 13,9 , e 9,5 , e 5,1 } and 3 ∩ 4 = {e 13,9 }.</p><p>The packet advances when it receives the bandwidth of all the links along the path. To determine the upper bound of network latency for a real-time traffic-flow, the maximum basic network latency and contention interference need to be measured. The maximum basic network latency can be calculated by Eq.( <ref type="formula" target="#formula_1">1</ref>). Therefore the important factor dominating the latency upper bound is interference. Under the priority arbitration policy, only traffic-flows with higher priority than the current one can cause interference. So we should find all the higher priority traffic-flows which could affect the observed one and calculate the interference upper bound by analyzing the characteristics of these higher priority traffic-flows.</p><p>Kim et al <ref type="bibr" target="#b12">[13]</ref> introduced two kinds of interferences to deal with the relation between the traffic-flows, direct interference and indirect interference and corresponding direct and indirect interference sets S D i and S I i of the observed traffic-flow τ i . The direct interference relation means the higher priority traffic-flow has at least one physical link in common with the observed traffic-flow. Thus, these traffic-flows will force a direct contention with the observed one. S D i includes all the traffic-flows which meet the following condition:</p><formula xml:id="formula_2">S D i = {τ k | k ∩ i = ∅ and P k &gt; P i for all τ k ∈ Γ}.</formula><p>With the indirect interference relation, on the contrary, the two traffic-flows do not share any physical link but there is (are) intervening traffic-flow(s) between the given two traffic-flows. S I i includes the higher priority traffic-flows that do not share any links with τ i but share at least one link with a traffic-flow in S D i , S I i = {τ k | k ∩ i = ∅ and k ∩ j = ∅, j ∈ S D i and P k &gt; P i for all τ k ∈ Γ}. For each traffic-flow from higher priority to lower priority, the set S i consisting of all traffic-flows with direct/indirect interference is constructed:</p><formula xml:id="formula_3">S i = S I i + S D i .</formula><p>An example is shown in Figure <ref type="figure" target="#fig_1">1</ref>. Four prioritized trafficflows sorted from high to low priority are τ 1 , τ 2 , τ 3 and τ 4 . Flows τ 1 and τ 2 have no shared links with any other higher priority traffic-flow so no direct or indirect interference, With the assumption and definition of the communication model, we will give, in the next section, a determinant upper bound on the schedulability of real-time traffic-flows.</p><formula xml:id="formula_4">S 1 = S 2 = S I 1 = S D 1 = S I 2 = S D 2 = ∅. Flow</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. NETWORK LATENCY UPPER BOUND ANALYSIS</head><p>An efficient approach is necessary in order to evaluate all the possible competing interferences imposed by all the higher priority traffic-flows. We find that the observed traffic-flow has a relationship of resource competition with its relevant higher priority traffic-flows. This is similar to the processor resource model <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b20">[21]</ref> in real-time scheduling, in which all the tasks contends for the shared processor resource. In wormhole switching networks, the shared physical communication links are also contended for by the associated traffic-flows. Utilizing the preemptive fixed priority scheduling policy, we can analyze this model following the real-time scheduling approach in single-processors.</p><p>We introduce the concept of worst case network latency which is inspired by worst case response time (WCRT) <ref type="bibr" target="#b13">[14]</ref> proposed in real-time system scheduling. A traffic-flow is schedulable if and only if the network latency of all the packets belonging to this traffic-flow is no more than its deadline. If we can find the worst case network latency of this traffic-flow, we can judge whether this traffic-flow is schedulable. Generally, we need to find the condition which can trigger the trafficflow's worst case network latency. Liu and Layland in their seminal paper <ref type="bibr" target="#b16">[17]</ref> identified two major conditions to achieve the worst case response:</p><p>• All the tasks execute for their worst-case execution time and all tasks are subsequently released at their maximum rate. • The task is released at the critical instant. The critical instant is the time that the task is requested simultaneously with requests of all higher priority tasks. We borrow the concept of critical instant to apply it for real-time scheduling in wormhole switching. The worst case network latency is assumed to occur when the packet from the observed traffic-flow is fired simultaneously with all the packets from higher priority traffic-flows with their maximal release rates.</p><p>We have discussed that the worst case network latency is primarily determined by the interference after computing the maximum basic network latency. Next, we need to quantify the analysis based on two distinguishing interferences:</p><p>• Direct interference from higher priority traffic-flows.</p><p>• Indirect interference from other higher priority trafficflows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Interference from direct higher priority</head><p>For an observed traffic-flow τ i , the network latency R i of a packet released from τ i is:</p><formula xml:id="formula_5">R i = B + C i + I i (2)</formula><p>where I i is the interference summation from the higher priority traffic-flow(s). The maximum basic network latency C i is constant and known a prior by static analysis (Eq.( <ref type="formula" target="#formula_1">1</ref>)). B is the maximum blocking time by any lower priority trafficflow which has already begun transmission. The maximum blocking happens when a higher priority packet arrives just after a lower priority packet starts its service. Consider our flitlevel preemptive scheduling strategy, the higher priority packet waits at most one flit time and then starts its transmission at each hop output port. The maximum blocking time is represented by B = f size × H/B link . The flit size and link bandwidth is constant after on-chip network configuration. Thus, the blocking time could be regarded as a constant parameter and incorporated in the basic network latency C i . The network latency equation Eq.( <ref type="formula">2</ref>) is simplified into</p><formula xml:id="formula_6">R i = C i + I i<label>(3)</label></formula><p>We assume the packet from the observed traffic-flow is released simultaneously with all the packets from higher priority traffic-flows, this triggers the worst case network latency based on the condition of critical instant. Without loss of generality, we assume this time instant is at time 0. Until the time instant R i when the packet is accepted completely by the receiver, during the time interval [0, R i ], the maximum possible direct competition interference from higher priority traffic-flows in S D i to a packet from τ i when release jitter is considered is:</p><formula xml:id="formula_7">I i = ∀τj ∈S D i R i + J R j T j C j<label>(4)</label></formula><p>The packet from τ i may be blocked by more than one packet from each τ j , τ j ∈ S D i , since the packet releases are periodic. The Ri+J R j Tj is the maximum number of packets a trafficflow can release during the time interval [0, R i ]. Using Eq.( <ref type="formula" target="#formula_7">4</ref>) to substitute I i in Eq.( <ref type="formula">2</ref>), we can produce:</p><formula xml:id="formula_8">R i = ∀τj ∈S D i R i + J R j T j C j + C i<label>(5)</label></formula><p>We find the variable R i appears on both sides of the Eq.( <ref type="formula" target="#formula_8">5</ref>). This equation can be solve using an iterative technique <ref type="bibr" target="#b0">[1]</ref>. Let r n+1 i be the (n + 1) th iterative value generated from the equation:</p><formula xml:id="formula_9">r n+1 i = ∀τj ∈S D i r n i + J R j T j C j + C i<label>(6)</label></formula><p>The iteration starts with r 0 i = C i and terminates when r n+1 i = r n i . This iteration also should halt if r n+1 i &gt; D i , which denotes the deadline miss for this packet. By this iterative technique, the worst case network latency with direct interference can be calculated (R i =r n+1 i =r n i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Interference from indirect higher priority</head><p>The model indicated by Eq.( <ref type="formula" target="#formula_8">5</ref>) only considers the direct interference from higher priority traffic-flow. Indirect interference also needs to be taken into account.  When τ 1 , . . . , τ j as indirect higher priority traffic-flows are taken into account, even though they do not share any physical link with τ n , we find their services still can impose an extra interference on τ n . The time-line graph in Figure <ref type="figure" target="#fig_3">4</ref> shows such a situation. The solid up arrow in the graph indicates the release time of a packet from a traffic-flow. A packet served by the network for some time is depicted as a shaded rectangle. The preemption of a packet is depicted as a white rectangle. The bold circle denotes the complete packet received by the destination. We assume τ i is released with the other higher priority packets at the same time 0, the packets from τ 1 , . . . , τ j will contend with τ i . This contention delays the start time of τ i until time t i , t i is the start time of τ i first transmission service. At the time 0 + t i , τ n is released, the packet from τ i immediately preempts τ n . It is easy to find that τ i imposes the interference C i upon the τ n during the time interval [t, R i ]. At the time 0 + T i , τ i is released again but this time all the higher priority traffic-flows τ 1 , . . . , τ j only send a very small packet or even do not send any packet. Flow τ i in this situation does not suffer any interference from them and gets network service immediately. From the view of τ n , the time interval between two successive releases from τ i is only (T i -t i ). Consider our original assumption, the worst case network latency occurs when all the packets from the higher priority traffic-flows are released periodically with the minimum packet release interval T . The maximum interference a packet from τ n suffered from a higher priority traffic-flow is calculated by</p><formula xml:id="formula_10">Rn+J R i Ti</formula><p>. But in this case, the minimum interval between subsequent preemptions from τ i is only (T i -t i ) which is less than the original minimum interval assumption T i . Note that this phenomenon can only occur when considering indirect interferences.</p><p>Theorem 1: The upper bound of interference suffered by τ n from direct higher priority traffic-flow τ i is:</p><formula xml:id="formula_11">R n + R i -C i + J R i T i C i (7)</formula><p>when the indirect interference is considered. Proof: Let s i denote the packet release time from τ i . In this analysis assumption, without loss of generality, the first packet is released at time 0. Therefore, each packet from τ i is generated periodically at the time instant 0, T i , 2T i , . . ., kT i . s i,k = (k -1)T i , where k is the sequential number of the packets. But a real application does not always meet this constraint and has application release jitter. More specifically, the release time s i,k satisfies:</p><formula xml:id="formula_12">(k -1)T i ≤ s i,k ≤ (k -1)T i + J R i (8)</formula><p>In addition, in Figure <ref type="figure" target="#fig_3">4</ref>, we observe that the possible interference from higher priority packets also defers its starting service time. If the worst case network latency for τ i is R i , the upper bound of start service time is R i -C i . The real service start time for each packet satisfies:</p><formula xml:id="formula_13">(k -1)T i ≤ s i,k ≤ (k -1)T i + J R i + R i -C i (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>Now we evaluate the maximum interference suffered by τ n from τ i in a given time interval. Here we assume the start service time of τ i is a = R i -C i + 0, this is the upper bound of the start service time since it is released. A packet from τ n is released simultaneously with τ i and b is the corresponding completion time of this release. The worst case interference occurs when most packets from τ i are released since τ n is released. Figure <ref type="figure" target="#fig_4">5</ref> illustrates this situation: The number of preemptions by τ i is given by the positive integer number g between the interval [a, b], g ∈ N. The last release of τ i should fall into the interval before the completion of τ n , g is the largest value that satisfies:</p><formula xml:id="formula_15">a -(R i -C i ) -J R i + (g -1)T i &lt; b<label>(10)</label></formula><p>or, equivalently,</p><formula xml:id="formula_16">g &lt; J R i + R i -C i + b -a T i + 1<label>(11)</label></formula><p>The largest positive integer number satisfying this inequality is given by</p><formula xml:id="formula_17">g = J R j + R i -C i + b -a T i (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>The interval [a, b] marks the worst case network latency of τ n , b -a = R n . Therefore, the interference upper bound from τ i is:</p><formula xml:id="formula_19">J R i + R i -C i + R n T i C i<label>(13)</label></formula><p>The packet from τ n will experience worst latency than what predicted by Eq.( <ref type="formula" target="#formula_8">5</ref>) on account of the indirect interference from τ 1 , . . . , τ j which delays τ i and further force more hits on τ n . Therefore, the worst case network latency does occur not when the packet is released simultaneously with higher priority packets but at the point when the packet from the observed traffic-flow is released at the same time as the higher priority packets finish waiting and start to receive service.</p><p>This deviation induced by higher priority interference between consequtive releases is called interference jitter, using symbol J I to denote the interference jitter of traffic-flows. The interference jitter of a traffic-flow is the maximum deviation between two successive packet start service times which can be obtained by computing the difference between the maximum and minimum value of packet start service times. Consider the situation that no higher priority packet is sent in a period, the minimum packet start service time becomes zero. Accordingly, the interference jitter of the traffic-flow is the maximum number of start service time which can be given by an upper bound:</p><formula xml:id="formula_20">J I i = R i -C i<label>(14)</label></formula><p>Note that not all the traffic-flows suffer interference jitter, this only happens when the observed traffic-flow τ n has indirect interference, namely, J I i exists if and only if S D i ∩ S I n = ∅. As a result, the worst case network latency in case of interference jitter and release jitter are calculated as follows:</p><formula xml:id="formula_21">R n = ∀τi∈S D n R n + J R i + J I i T i C i + C n<label>(15)</label></formula><p>We modify our traffic-flow attributes with the six-tuple (C i , P i , T i , D i , J R i , J I i ) for τ i . Differing from the conclusion of <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[10]</ref>, here we treat the indirect interference as interference jitter of direct higher priority traffic-flow and obtain an tighter upper bound for worst case network latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. A case example</head><p>Let us revisit the example given in Figure <ref type="figure" target="#fig_1">1</ref>. The interrelations between these traffic-flows have been examined in section III-B. The attributes of the traffic-flows are shown in Table <ref type="table">I</ref>. The time units are not necessary in this analysis as long as all the traffic-flows use the same base. TRAFFIC-FLOWS DESCRIPTION Since the higher priority traffic-flow always forces interference to the lower priority traffic-flow and extends the latter's network latency, we sort the traffic-flows based on their priority and proceed to analyze them from the highest priority one by one. τ 1 , τ 2 do not suffer any contention and receives the worst case network latency equal to their maximum basic latency, R 1 =C 1 = 2, R 2 = C 2 =1. Thus τ 1 and τ 2 are schedulable. τ 3 shares the physical link with the higher priority traffic-flows τ 1 and τ 2 , S D 3 = {τ 1 , τ 2 }, S I 3 = ∅. The network latency for τ 3 according to Eq.( <ref type="formula" target="#formula_8">5</ref>):</p><formula xml:id="formula_22">R 0 3 = 3 R 1 3 = 3 + 3 6 2 + 3 5 1 = 3 + 2 + 1 = 6 R 2 3 = 3 + 6 6 2 + 6 5 1 = 3 + 2 + 2 = 7 R 3 3 = 3 + 7 6 2 + 7 5 1 = 3 + 4 + 2 = 9 R 4 3 = 3 + 9 6 2 + 9 5 1 = 3 + 4 + 2 = 9</formula><p>The recurrence stops at R 3 = 9 which is less than the deadline 10. The worst case network latency of τ 3 is 9.</p><p>Flow τ 4 suffers both direct and indirect interferences with</p><formula xml:id="formula_23">S D 4 ={τ 2 , τ 3 }, S I 4 ={τ 1 }.</formula><p>Based on the principle proposed in Section IV-B when indirect interference exists, we treat indirect interference as interference jitter and therefore update the attributes of our example. The interference jitter of traffic-flow τ 3 referred to τ 4 equals R 3 -C 3 = 6. Eq.( <ref type="formula" target="#formula_21">15</ref>) becomes</p><formula xml:id="formula_24">R 4 = C 4 + R4 T2 C 2 + R4+J I 3 T3 C 3</formula><p>which stops at R 4 = 13. So, the worst case network latency of τ 4 is 13 with both direct and indirect interference. Again, the traffic-flow meets its deadline. Note that if τ 1 is considered as direct interference (Balakrishnan's analysis approach <ref type="bibr" target="#b1">[2]</ref>), the worst case network latency calculated above is 19 which means it misses the deadline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. TIGHTNESS OF THE ANALYSIS</head><p>By abstracting the communication resources and finding the inter-relations among the traffic-flows, we have succeeded in transforming the real-time wormhole scheduling approach into an analyzable model. In this section, we show that our analysis approach can produce an upper bound of network latency for all situations. We also show that the exact worst case network latency evaluation in the parallel interference situation, as a general problem, is NP-hard. A case example is used to illustrate that the latency upper bound is not tight when where is parallel interference. Suppose the relations of traffic-flows shown in Figure <ref type="figure" target="#fig_6">6</ref> with the attributes in Table <ref type="table">II</ref>. Relying on the critical instant assumption, flow τ 3 experiences its maximum network latency when released simultaneously with τ 1 and τ 2 . The worst case network latency for τ 1 , τ 2 , τ 3 are 1, 3, 9 according to Eq.( <ref type="formula" target="#formula_21">15</ref>). But we find that during the analysis of τ 3 , τ 2 is forced to compete with τ 1 in this model even though they do not share  Lu et al in his paper <ref type="bibr" target="#b17">[18]</ref> found this phenomenon that realtime transmission scheduling can be parallel for disjoint concurrent contentions. Figure <ref type="figure" target="#fig_8">7</ref>(A) illustrates this concurrency. If all traffic-flows release the packets simultaneously, τ 1 and τ 2 are executed at the same time in this scheduling sequence. This parallel interference reflects the fact that no real link resource is shared between the direct higher priority traffic-flows of the observed one. For τ i , this parallel interference phenomenon exists when the following condition is met: τ j , τ k ∈ S D i and j ∩ k = ∅. Lu et al <ref type="bibr" target="#b17">[18]</ref> also gave an approach to analyze this phenomenon. The worst case network latency is assumed to occur when all the traffic-flows are released simultaneously and the disjoint traffic-flows are scheduled in parallel. The corresponding network latency of packet in τ 1 , τ 2 and τ 3 are 1, 3, 8 in Lu's model which are smaller than previous results. However we must point out the analysis by Lu et al <ref type="bibr" target="#b17">[18]</ref> is defective. Even though this parallel interference can possibly reduce the intervention to the lower priority traffic-flow since many of them are not likely to occur at the same time. But the worst case latency does not always occur when all the traffic-flows are released at the same time. Figure <ref type="figure" target="#fig_8">7(B</ref>) is a contradictory case against this assumption. The network releases τ 1 and τ 3 at the same time, τ 2 is ready just before τ 1 accomplishes its transmission. τ 2 consequently holds the link resource until service completion. After that τ 3 starts its packet transferring. In this situation, the lower priority traffic-flow τ 3 suffers more interference. The network latency of τ 3 is 9 more than the upper bound value produced in Lu's analysis.</p><p>The parallel interference phenomenon does not appear in all the traffic-flow set loaded on the network. However, this possible parallelism clearly complicates the analysis progress. In general scheduling, when the worst case release conditions are not easily determined, the analysis is usually intractable. Let us re-visit our analysis model, for τ i , the total interferences generated by all direct higher priority traffic-flows are the sequence summation of each traffic-flow,</p><formula xml:id="formula_25">I i = j∈S D i Ri+J R j Tj .</formula><p>This assumption can produce a tight result for each trafficflow when there is no parallel interference from direct higher priority traffic-flows; the traffic-flow set in Figure <ref type="figure" target="#fig_1">1</ref> is a case. However this assumption ignores the possible simultaneous communication service. The following proof shows that this approach is not tight and only an upper bound of network latency can be produced when parallel interference exists.</p><p>Theorem 2: The real worst case network latency is no more than the calculation result accomplished by Eq.( <ref type="formula" target="#formula_8">5</ref>) and Eq.( <ref type="formula" target="#formula_21">15</ref>) when parallel interference exists.</p><p>Proof: Similar to the scheduling in the single processor model, this model also implies that at any time only one traffic-flow can win the access right of the shared resource and execute the service. Let t 1 , t 2 indicates the time instant after network startup. During any time interval [t 1 , t 2 ] where t 1 &lt; t 2 , the maximum required service time of all the higher priority traffic-flows (namely, interference to the observed traffic) is no more than ∀j∈S D i t2-t1+J R j +J I j Tj C j . Since parallel service exist, more service opportunity can be supported by the network in any time interval. This may accelerate consumption of the required service from higher priority traffic-flows and finally shortens the whole interference imposing on the observed one. The network latency is exactly equal to the interference of all the higher priority traffic-flows plus service time of itself, Eq.( <ref type="formula">2</ref>). Consequently, the worst case network latency in the real network is no more than the calculation result under the assumption of no parallel interference.</p><p>Theorem 2 implies our model and analysis approach is only sufficient but not necessary when parallel interference exists. In another words, if a traffic-flow can pass this schedulability test then it can meet its deadline. But if it fails this test, no similar conclusion can be made. Next, we give a proof that it is not possible to produce a polynomial-time necessary and sufficient test unless P=NP. </p><formula xml:id="formula_26">=(C 1 , P 1 , T 1 , D 1 , J R 1 , J I 1 , s 1 ), τ 2 = (C 2 , P 2 , T 2 , D 2 , J R 2 , J I 2 , s 2 ), . . ., τ n+1 =(C n+1 , P n+1 , T n+1 , D n+1 , J R n+1 , J I n+1 , s n+1</formula><p>). s i is the release time of τ i and τ 1 with highest priority, τ n+1 with lowest priority. First N traffic-flows are disjoint with each other and communication in parallel. The observed one τ n+1 with lower priority contends with first N higher priority traffic-flows, so S D n+1 = {τ 1 , τ 2 , . . . , τ n }. In order to determine the network latency, we need to take account all the possible free gap intervals for the observed traffic-flow. The case in Figure <ref type="figure" target="#fig_8">7</ref>(B) implies the worst case network latency no longer occurs when all the trafficflows are released simultaneously. Thus, we need to examine all the possible packet release sequences to achieve the worst case network latency calculation. We now shown that to solve this problem when parallel interference exists is NP-hard.</p><p>Lemma 1: For traffic-flows set Γ meeting the conditions shown in Figure <ref type="figure" target="#fig_9">8</ref>, the observed traffic-flow τ n+1 is schedulable on the network if and only if all the deadlines are met during time interval [0, t 1 ], where t 1 = s + P , s = max{s 1 , . . . , s n+1 } and P = lcm{P 1 , . . . , P n+1 }.</p><p>Proof: ⇒ : If the observed traffic-flow is schedulable on the network, relying on the schedulable condition, all the deadlines are met since it was released. Thus, all deadlines in the interval [0, t 1 ] are met. ⇐ : At any time instant t i , the network state is denotes by E(e 1 , . . . , e n ) ti , where e i is the amount of time for τ i has finished transmission service since last release. The time interval [0, s + P ] is separated into two parts [0, s) and [s, s+P]. After s, all the traffic-flows are released and executed in parallel except τ n+1 . Let t i ∈ [s, s + P ] and t j = t i + j × P for j is positive integer and j &gt; 0, it is not difficult to see the network state E ti = E tj is always true. This means the schedule progress repeats itself every P units of time after s. Since all deadline of τ n+1 in [s, s+P] are met, all the deadline after s+P should also be met. As the assumption, the deadline of τ n+1 in [0, s] are met. Thus τ n+1 is schedulable in this network.</p><p>Strictly speaking, we need to check all the infinite scheduling sequences after packet releases of the traffic-flow set to ensure the deadline is always met. Lemma 1 gives us an easy method to estimate whether the traffic-flow set is schedulable in finite schedule time interval [0, s+P ]. If all the deadlines in [0, s + P ] are met, then this traffic-flow set is a valid schedule. If we know period, maximum basic network latency and release time of all packets in a traffic-flow set in advance, we can find all the available gap intervals in [0, s+P] for observed traffic-flow with a polynomial time algorithm. Determining the schedulability of observed traffic-flow, namely finding the worst case network latency when knowing all the available gap intervals in advance is also taking polynomial time. Now we prove that if the traffic-flow set with arbitrary release time, finding the worst case network latency is intractable. Utilizing the K Simultaneous Congruences <ref type="bibr" target="#b14">[15]</ref> which has been shown to be the NP-complete, our problem can be proved to be intractable by reducing the current problem to a known NPcomplete problem in polynomial time.</p><p>K Simultaneous Congruences: Given N ordered pairs of positive integers (a 1 ,b 1 ), . . . , (a n , b n ) and a positive integer K ( 2 ≤ K ≤ N ). Is there a subset of ≥ K ordered pairs (a i,1 , b i,1 ), . . . , (a i, , b i, ) such that there is a positive integer x with the property that x = a i,j + p j × b i,j for each 1 ≤ j ≤ ? Theorem 3: For a traffic-flow set with arbitrary release time, the problem of determining schedulability when parallel interference exists is NP-hard.</p><p>Proof: The proof includes two steps, first, we try to prove finding all the gap intervals for the observed trafficflow is NP-hard. Suppose n sorted pairs of positive integers (a 1 ,b 1 ), . . ., (a n ,b n ) with constraint a i ≥ b i -1 and a positive integer K, we construct n parallel traffic-flows communications (τ 1 , . . . , τ n ) with attributes T i = D i = b i , C i = b i -1, and s i +C i = a i . In our construction, each traffic-flow in each period only exports one free time unit. For this traffic-flow set, finding the gap interval on the network only and only if all the N traffic-flows output free time units simultaneously. Thus, the K Simultaneous Congruences problem has a solution if and only if the gap intervals can be found in the constructed model. Since this construction progress can be done in polynomial time, the problem of finding all the gap intervals is as hard as the K Simultaneous Congruences problem. We assume a traffic-flow τ n+1 which has the lowest priority comparing with τ 1 , . . . , τ n . Determining the schedulability of τ n+1 when knowing all the gap intervals is polynomial time complexity, so the problem of determining schedulability in the parallel interference situation is NP-hard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>The new on-chip communication architectures need to provide different levels of service for various components on the same network. The requirement of real-time applications needs a scheduling strategy and analysis approach to predict whether all the real-time packets can meet their timing bounds. In this paper, we introduce an analysis approach for real-time on-chip communication with wormhole switching and fixed priority scheduling. The worst case network latency upper bound for each traffic-flow can be achieved for all situations. When parallel interference exists in a real network, we show that the exact determinant of schedulable for traffic-flow sets is NP-hard. Utilizing this analysis scheme, we can flexibly evaluate the schedulability of traffic-flow sets with different QoS requirements in a real-time communication platform at the design phase.</p><p>The future work will involve the issues of priority assignment and the practical consideration of having less virtual channel and priority levels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Priority Arbitration Output Port</figDesc><graphic coords="3,102.92,308.73,169.41,127.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>τ 3 competes with τ 1</head><label>1</label><figDesc>and τ 2 and gets S I 3 = ∅ and S 3 = S D 3 = {τ 1 , τ 2 }. Flow τ 4 directly contends with τ 2 and τ 3 and indirectly suffers interference from τ 1 , S D 4 = {τ 2 , τ 3 }, S I 4 = {τ 1 } and S 4 = {τ 1 , τ 2 , τ 3 }. Note that if one traffic-flow is both contending directly and indirectly with an observed one, then this trafficflow will be regarded as generating direct contention only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A Case of Indirect Interference</figDesc><graphic coords="5,316.01,519.42,217.15,86.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The Problem of Indirect Interference</figDesc><graphic coords="6,92.12,124.54,191.01,147.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Upper Bound Analysis of Indirect Interference</figDesc><graphic coords="6,314.76,375.65,219.15,68.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Parallel Interference Case</figDesc><graphic coords="7,331.40,508.30,185.88,89.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>any physical link; and this will force an extra delay on τ 3 . The problem is when we schedule all the traffic-flows with the resource competing relationship, we always ideally assume at any time instant only one traffic-flow can be served by the subset of the network which hosts the interfering flows service.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Scheduling Sequence With Parallel Interference</figDesc><graphic coords="8,87.25,245.53,198.58,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. A General Parallel Interference Model</figDesc><graphic coords="8,310.27,548.54,228.12,51.02" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This periodic restriction is for presentation reason, the analysis can be extended to sporadic traffic-flows where T is the minimum inter-arrival interval.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Applying new scheduling theory to static priority pre-emptive scheduling</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Audsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tindell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Wellings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Software Engineering Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="284" to="292" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A priority-driven flow control mechanism for real-time traffic in multiprocessor networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ozguner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="664" to="678" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Powering networks on chips</title>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISSS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Networks on Chips: A New SoC Paradigm</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="70" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of research and practices of network-on-chip</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bjerregaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computer Survey</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Virtual-channel flow control</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="194" to="205" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Route packets, not wires: On-chip interconnection networks</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th Design Automation Conference (DAC)</title>
		<meeting>the 38th Design Automation Conference (DAC)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="684" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interconnection Networks: An Engineering Approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yalamanchili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Institute of Electrical &amp; Electronics Enginee</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Future trends in SoC interconnect</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bainbridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLSI Design, Automation and Test</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="183" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feasibility test for real-time communication using wormhole routing</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Hary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ozguner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Proceedings -Computers and Digital Techniques</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="273" to="278" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On-chip networks: A scalable, communication-centric embedded system design paradigm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Henkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakradhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLSID &apos;04: Proceedings of the 17th International Conference on VLSI Design</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">845</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey of efficient on-chip communications for soc</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kavaldjiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J M</forename><surname>Smit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CTIT</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A real-time communication method for wormhole switching networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPP &apos;98: Proceedings of the 1998 International Conference on Parallel Processing</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="527" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fixed priority scheduling of periodic task sets with arbitrary deadlines</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lehoczky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Real-Time Systems Symposium</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="201" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the complexity of fixed-priority scheduling of periodic real-time tasks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Performance Evaluation</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="237" to="250" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Priority based real-time communication for large scale wormhole networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mutka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Symposium on Parallel Processing</title>
		<meeting>the 8th International Symposium on Parallel Processing<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="433" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scheduling algorithms for multiprogramming in a hard-real-time environment</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">W</forename><surname>Layland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="61" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feasibility analysis of messages for onchip networks using wormhole routing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jantsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASP-DAC &apos;05: Proceedings of the 2005 conference on Asia South Pacific design automation</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey of wormhole routing techniques in direct networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="62" to="76" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Throttle and preempt: A new flow control for real-time communications in wormhole networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPP &apos;97: Proceedings of the international Conference on Parallel Processing</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="198" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An extendible approach for analyzing fixed priority hard real-time tasks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Tindell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Wellings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Real-Time System</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="151" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Service disciplines for guaranteed performance service in packet-switching networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="1374" to="1396" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
