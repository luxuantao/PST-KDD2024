<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structural and Semantic Contrastive Learning for Self-supervised Node Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-17">17 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
							<email>&lt;kaize.ding@asu.edu&gt;</email>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yancheng</forename><surname>Wang</surname></persName>
							<email>&lt;yancheng.wang@asu.edu&gt;.</email>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yingzhen</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Yancheng Wang</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Structural and Semantic Contrastive Learning for Self-supervised Node Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-17">17 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2202.08480v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Contrastive Learning (GCL) recently has drawn much research interest for learning generalizable, transferable and robust node representations in a self-supervised fashion. In general, the contrastive learning process in GCL is performed on top of the representations learned by a graph neural network (GNN) backbone, which transforms and propagates the node contextual information based on its local neighborhoods. However, existing GCL efforts have severe limitations in terms of both encoding architecture, augmentation, and contrastive objective, making them commonly inefficient and ineffective to use in different datasets. In this work, we go beyond the existing unsupervised GCL counterparts and address their limitations by proposing a simple yet effective framework S 3 -CL. Specifically, by virtue of the proposed structural and semantic contrastive learning, even a simple neural network is able to learn expressive node representations that preserve valuable structural and semantic patterns. Our experiments demonstrate that the node representations learned by S 3 -CL achieve superior performance on different downstream tasks compared to the state-of-the-art GCL methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due to the high-dimensional, structure-complex and heterogeneous properties of real-world graphs <ref type="bibr">(Sun et al., 2019a)</ref>, data labeling in the graph domain tends to be labor-intensive and resource-expensive. To relieve the burdensome reliance on human-annotated labels, self-supervised node representation learning has drawn much research interests lately. Inspired by the success in the vision and language domains, contrastive learning has been widely adopted by recent advances of graph neural networks (GNNs) to improve their Typically, graph contrastive learning (GCL) methods learn representations by creating two augmented views of a graph element and maximizing the agreement between representations of the two views. By using non-semantic labels, GCL is able to provide generalizable, transferable and robust node representations for various downstream tasks <ref type="bibr" target="#b37">(You et al., 2020;</ref><ref type="bibr" target="#b8">Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr" target="#b6">Du et al., 2021)</ref>, becoming a prevailing paradigm in self-supervised node representation learning. In general, existing GCL methods differ mostly in their encoding architecture, augmentation design, and contrastive objective. However, those modules commonly require careful design and laborious customization for each specific dataset, which largely limits the usage of current GCL frameworks in practice. Otherwise, those methods may easily lose their efficacy due to the following inherent limitations:</p><p>(i) Shallow Encoding Architecture. As the canonical encoding backbone of existing GCL frameworks, conventional GNNs focus on the local neighborhoods and their shallow property could hamper the success of GCL. For real-world graphs, nodes sharing similar semantics may not be geographically close, it is essential to consider both local and global structure information when encoding the nodes. However, directly stacking multiple GNN layers will lead to over-smoothing <ref type="bibr">(Chen et al., 2020a)</ref>, thus how to enlarge the receptive fields of the encoding backbone when performing contrastive learning on graphs requires urgent research efforts; (ii) Arbitrary Augmentation Design. Most GCL efforts adopt arbitrary augmentations on the input graph to obtain different augmented views in various scales, which may unexpectedly change both structural and semantic patterns of the graph, leading to degraded performance <ref type="bibr">(Zhu et al., 2021;</ref><ref type="bibr" target="#b23">Park et al., 2021)</ref>. For instance, if a hub node is simply removed/perturbed, it will affect a substantial amount of other nodes when learning their node representations. Hence, proposing a principled and noise-resistant graph augmentation function is necessary; (iii) Semanticless Contrastive Objective. Existing GCL methods for selfsupervised node representation learning mainly focus on instance-level contrasting to improve node-wise discrimi-nation, while the semantic structure of the input graph is largely ignored. Enhancing the intra-cluster compactness and inter-cluster separability is critical for improving the quality of self-supervised or unsupervised learning <ref type="bibr" target="#b18">(Li et al., 2020)</ref>, yet how to achieve this without prior knowledge of labels remains unattended.</p><p>In this paper, we address the aforementioned issues by proposing a simple yet effective GCL framework, namely, S 3 -CL (Simple Neural Networks with Structural and Semantic Contrastive Leanring). By learning with the proposed two new contrastive losses, our approach outperforms other GCL counterparts with a much simpler and parameterless encoding backbone, such as an MLP or even a one-layer neural network. Our structural contrastive learning module not only avoids the negative effects of arbitrary graph augmentations, but also enables larger receptive fields when learning the representation of each node. Specifically, based on the idea of graph diffusion, we first conduct structure augmentation on the input graph to generate multiple augmented views at different diffusion degrees. Then by performing contrastive learning on the node representations learned from the local and a high-order views, the encoder is able to improve node-wise discrimination by exploiting both local and global structure knowledge. In the meantime, the semantic contrastive learning module further enhances intra-cluster compactness and inter-cluster separability to better capture the data semantics. Specifically, it infers the clusters among nodes and their corresponding prototypes by a new Bayesian non-parametric algorithm, and then perform semantic contrastive learning to enforce those nodes that are semantically similar to cluster around their corresponding cluster prototypes in the latent space. By jointly optimizing the structural and semantic losses, the pre-trained encoder network can learn highly expressive node representations for different downstream tasks without using any humanannotated labels. Our major contributions are:</p><p>? We develop a new GCL framework S 3 -CL, which can learn expressive node representations by using a much simpler and parameter-less learning backbone. ? We propose the structural and semantic contrastive learning losses, which can be used for effectively capturing the structural and semantic patterns of the input graph in a self-supervised fashion. ? We conduct extensive experiments to show that our approach is able to achieve superior performance on different downstream tasks over the state-of-the-art GCL methods on multiple benchmark datasets.  <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref>, graph attention networks (GAT) <ref type="bibr" target="#b32">(Veli?kovi? et al., 2018)</ref>, and the others <ref type="bibr" target="#b7">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b36">Xu et al., 2019;</ref><ref type="bibr" target="#b35">Wu et al., 2019;</ref><ref type="bibr">Chen et al., 2020c)</ref>. Despite the remarkable success, conventional GNNs tend to perform worse as they become deeper with more GNN layers. As a result, recent GNNs usually have shallow architectures, which risks overlooking the global structural information, and exploiting only the local neighborhood information of nodes. To solve this problem, researchers start to design GNNs with broader message passing range, by proposing techniques such as decoupling transformation and propagation operations <ref type="bibr">(Klicpera et al., 2019a;</ref><ref type="bibr" target="#b40">Zhu &amp; Koniusz, 2021)</ref>, applying superior normalization <ref type="bibr">(Chen et al., 2020b;</ref><ref type="bibr" target="#b0">Chamberlain et al., 2021)</ref>, and many others <ref type="bibr">(Sun et al., 2019b)</ref>. However, few efforts have been devoted to capturing both local and global structure information in self-supervised node representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Graph Contrastive Learning. Inspired by the recent success of contrastive learning in computer vision and natural language processing, a series of works have been proposed to apply contrastive learning to graph-structured data. Most of proposed graph contrastive learning approaches create different views of the unlabeled input graph and aim at maximizing agreement between representations of the two views. Deep graph Infomax (DGI) <ref type="bibr" target="#b33">(Veli?kovi? et al., 2019)</ref> is first method that contrast the patch representations with high-level graph representations by maximizing their mutual information. InfoGraph <ref type="bibr">(Sun et al., 2019a)</ref> obtains representations for whole graphs by contrasting the graphlevel representations with the substructure-level representations. MVGRL <ref type="bibr" target="#b8">(Hassani &amp; Khasahmadi, 2020)</ref> adopts graph diffusion to generate an augmented view, and contrast representations of first order neighbors with a graph diffusion. GCC <ref type="bibr" target="#b26">(Qiu et al., 2020)</ref> and GRACE <ref type="bibr">(Zhu et al., 2020a)</ref> create the augmented views by sampling subgraphs. MERIT <ref type="bibr" target="#b11">(Jin et al., 2021)</ref> adopts a siamese self-distillation network, and performs contrastive learning across views and networks at the same time. Nonetheless, existing GCL methods usually rely on arbitrary augmentations and are also ineffective to capture semantic structure of graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head><p>An attributed graph with N nodes can be formally represented by G = (V, E, X), where V = {v 1 , v 2 , . . . , v N } and E ? V ? V denote the set of nodes and edges respectively. Let A ? {0, 1} N ?N be the adjacency matrix of graph G.</p><p>A ij = 1 if and only if (v i , v j ) ? E. ? stands for the adjacency matrix for a graph with added self-loops I. We let D and D denote the diagonal degree matrix of A and ? respectively. The attribute matrix X ? R N ?D is used to describe the features of nodes. The i-th row of X is the feature of node v i . Hence, an attributed graph can be described as G = (X, A) for simplicity. The problem of self-supervised (unsupervised) node representation learning can be formulated as follows:</p><p>PROBLEM DEFINITION 1 Self-supervised Node Representation Learning. Given an attributed graph G = (X, A), we aim to learn a graph encoder f ? : R N ?D ? R N ?N ? R N ?D , without the access to any humanannotated label information, such that generated node representations H ? R N ?D = f ? (X, A) can be leveraged by different downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>In this paper, we propose a novel graph contrastive learning framework S 3 -CL for self-supervised node representation learning. Our proposed framework consists of the following components:</p><p>? An encoder network, which is built with a one-layer neural network. It maps the features of each node v i to a lowdimensional representation vector h i ? R D . ? A structural contrastive learning module, which improves node-wise discrimination by maximizing the agreement between two augmented views from the structure of the input graph. ? A semantic contrastive learning module, which encourages intra-cluster compactness and inter-cluster separability in the latent space by maximizing the agreement between each node and its estimated cluster prototype.</p><p>The overall framework is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. By learning with our proposed structural and semantic contrastive learning losses, we are able to learn expressive node representations that preserve both structure and semantic knowledge of the input graph, with only a simple one-layer neural encoder network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Structural Contrastive Learning</head><p>In unsupervised representation learning, contrastive learning methods <ref type="bibr" target="#b9">(He et al., 2020;</ref><ref type="bibr">Chen et al., 2020d)</ref> treat each sample as a distinct class and aims to achieve instancewise discrimination. In a similar manner, existing GCL methods achieve node-wise discrimination by maximizing the agreement between the representations of the same graph element in different augmented views.</p><p>Structure Augmentation via Graph Diffusion. To better preserve the structure knowledge and prevent the augmen-tations on graphs from behaving arbitrarily, we propose to adopt Graph Diffusion <ref type="bibr">(Klicpera et al., 2019b;</ref><ref type="bibr" target="#b35">Wu et al., 2019)</ref> to perform reliable augmentations on the input graph from the structural perspective. Compared to arbitrarily modifying the graph structure, such as perturbing edges or nodes, adopting graph diffusion as the augmentation function can not only preserve the structure knowledge, but also mitigate the noises in the original graph structure. In general, the graph diffusion process can be computed by:</p><formula xml:id="formula_0">S = ? l=0 ? l T l ? R N ?N ,<label>(1)</label></formula><p>where T ? R N ?N is a generalized transition matrix and ? l is the weighting coefficient which determines the ratio of l th -hop neighborhood. Different choices of ? l and T lead to different forms of filters. In our work, we take ?sym = D-1/2 ? D-1/2 as the transition matrix T. By simply setting the weighting coefficient for the l-hops neighborhoods ? l = 1 and all the other weighting coefficients as 0, we can obtain a special case of diffusion matrix as S (l) = ?l sym . Note that other meaningful choices of weighting coefficients can also be used in our framework, and we leave this for future work.</p><p>Afterwards, we can conduct feature propagation <ref type="bibr" target="#b35">(Wu et al., 2019)</ref> using the augmented graph structure and further compute the node representations using the one-layer encoder network f ? (?) as:</p><formula xml:id="formula_1">H (l) = f ? (S (l) X) = ReLU(S (l) X?),<label>(2)</label></formula><p>where ? ? R N ?D is the weight matrix. This way the computed node representations H (l) can encode the feature information from l-hops neighborhoods in the graph.</p><p>In order to better exploit both local and global structure information in the contrastive learning process, we perform multiple augmentations on the graph structure by setting l with different values and learn the node representations from those augmented views. Specifically, H (1) is learned from a local view as only information passing between direct neighbors is enabled, while {H (l) } L l=2 are learned from a set of high-order views that encode the global structure information at different levels. In our structural contrastive learning, we aim to maximize the agreement between the representations of each node learned from the local view and a high-order view by maximizing their mutual information. Following previous works on instance-level contrastive learning <ref type="bibr">(Chen et al., 2020c)</ref>, we apply an projection head g ? (?) to the representations computed by the encoder network, instead of directly contrasting the output of the encoder network. Thus the representations we contrast in our structural contrastive learning can be denoted by {U (l) } L l=1 , where U (l) = g ? (H (l) ), and g ? (?) is a two-layer MLP in our implementation. Structural Contrastive Objective. To maximize the mutual information between two different views, its lowerbounds are usually used as objectives for contrastive learning. In our work, we adopt InfoNCE <ref type="bibr" target="#b21">(Oord et al., 2018)</ref> to estimate the lower bound of the mutual information between the representations learned from a local view U<ref type="foot" target="#foot_0">1</ref> and a specific high-order view U (l) of the input graph:</p><formula xml:id="formula_2">? ? ! ? ? ? Momentum Update ? ? (#) ? (%) ? ? (&amp;) ? ' Pseudo Labels Latent Space ? ()* ? ? Semantic Prototypes ? ,-. Node Representation ? (?, ? # ) (?, ? % ) (?, ? &amp; ) Node-wise Contrasting ? ? ? ? ? ! ? " ? # ? Label</formula><formula xml:id="formula_3">L (l) str = - 1 N N i=1 log exp(u (1) i ? u (l) i /? 1 ) M j=0 exp(u (1) i ? u (l) j /? 1 ) ,<label>(3) where u</label></formula><p>(1) i and u</p><p>(l)</p><p>i denote the i-th row of U (1) and U (l) respectively. Note that {u (l) j } M j=0 contain one positive example and M negative examples of other nodes. Thus, the loss function of structural contrastive loss can be defined as:</p><formula xml:id="formula_4">L str = L l=2 L (l)</formula><p>str .</p><p>(4) By performing the above structural contrastive learning loss, the encoder network is able to encourage accurate node-wise discrimination during the learning process. The final node representations H can be compute by feeding l) X to the encoder network for the sake of preserving both local and global structure information.</p><formula xml:id="formula_5">X = 1 L L l=1 S (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semantic Contrastive Learning</head><p>Despite the structural contrastive learning module can provide better node-wise discrimination by exploiting the structure knowledge from different augmented views, it cannot capture the data semantics of the input graph.</p><p>To solve this problem, we further propose a semantic contrastive learning module that explicitly captures data semantic structure by encouraging intra-cluster compactness and inter-cluster separability. Specifically, it iteratively infer the clusters among nodes and the corresponding prototypes, and perform semantic contrastive learning to promote those nodes that are semantically similar clustering around their corresponding cluster prototypes in the latent space. The representation of the cluster prototypes are denoted by a matrix C ? R K?D , where K is the number of prototypes inferred from the data. We use c k to denote the k-th row of C, which is the representation of the k-th prototype in the latent space. The prototype assignments or pseudo labels 1 of nodes are denoted by Z = {z i } n i=1 , where z i ? {1, ..., K} is the pseudo label of node v i .</p><p>Bayesian Non-parametric Prototype Inference. A key component of our semantic contrastive learning module is to infer highly representative cluster prototypes. However, the optimal number of cluster is unknown under the setting of self-supervised node representation learning, thus it is intractable to directly adopt clustering methods such as K-means to cluster the nodes. To address this issue, we propose a Bayesian non-parametric prototype inference algorithm to approximate the optimal number of clusters and compute the cluster prototypes. We build a Dirichlet Process Mixture Model (DPMM) and assume the distribution of node representations is a Gaussian Mixture Model (GMM), whose components share the same fixed covariance matrix ?I. Each component is used to model the prototype of a cluster. The DPMM model can be defined as:</p><formula xml:id="formula_6">G ? DP(G 0 , ?) ? i ? G for i = 1, ..., N h i ? N (? i , ?I) for i = 1, ..., N<label>(5)</label></formula><p>where G is a Gaussian distribution draw from the Dirichlet process DP(G 0 , ?), and ? is the concentration parameter for DP(G 0 , ?). ? i is the mean of the Gaussian sampled for node representation h i . G 0 is the prior over means of the Gaussians. We take G 0 to be a zero-mean Gaussian N (0, ?I), where ?I is the covariance matrix.</p><p>Next, we use a collapsed Gibbs sampler <ref type="bibr" target="#b27">(Resnik &amp; Hardisty, 2010)</ref> to infer the components of the GMM with the DPMM. The Gibbs sampler iteratively sample pseudo labels for the nodes given the means of the Gaussian components, and sample the means of the Gaussian components given the pseudo labels of the nodes. When the variance of the Gaussian components ? ? 0, the process to sample pseudo labels becomes deterministic <ref type="bibr" target="#b17">(Kulis &amp; Jordan, 2011)</ref>. Let K denote the number of inferred prototypes at current iteration step, the prototype assignment update can be formulated as:</p><formula xml:id="formula_7">z i = arg min k {d ik } , for i = 1, ..., N d ik = ||h i -c k || 2 for k = 1, ..., K ? for k = K + 1,<label>(6)</label></formula><p>where d ik is the metric to determine the pseudo labels of node representation h i . ? is the margin to initialize a new prototype. In practice, we choose the value of ? by performing cross-validation on each dataset. With the formulation in Equation ( <ref type="formula" target="#formula_7">6</ref>), a node will be assigned to the prototype modeled by the component corresponding to the closest mean of Gaussian, unless the squared Euclidean distance to the closest mean is greater than ?. After obtaining the pseudo labels, the cluster prototype representations can be computed by:</p><formula xml:id="formula_8">c k = zi=k h i zi=k 1</formula><p>, for k = 1, ..., K.</p><p>Note that we iteratively update prototype assignments and prototype representations till convergence, and we set the number prototypes K to be the number of inferred prototypes K then. The algorithm for prototype inference is summarized as Algorithm 2 in Appendix.</p><p>Prototype Refinement via Label Propagation. Considering the fact that the pseudo labels inferred by the Bayesian non-parametric algorithm could be inaccurate, we further refine the pseudo labels generated by the Gibbs sampler based on label propagation <ref type="bibr" target="#b39">(Zhou et al., 2004)</ref>. This way we can smooth the noisy pseudo labels and refine the cluster prototype representations by leveraging structure knowledge. Firstly, we convert the prototype assignments Z to a one-hot pseudo label matrix Z ? R N ?K , where Z ij = 1 if and only if z i = k. Following the idea of Personalized PageRank (PPR) <ref type="bibr">(Klicpera et al., 2019a)</ref>, the pseudo labels after T aggregation steps Z (T ) are updated by:</p><formula xml:id="formula_10">Z (t+1) = (1 -?) ?sym Z (t) + ?Z (0) ,<label>(8)</label></formula><p>where Z (0) = Z and ? can be considered as the teleport probability in PPR. Next, we convert the propagated results Z (T ) back to hard pseudo labels by setting</p><formula xml:id="formula_11">z i = arg max k Z (T )</formula><p>ik for i ? {1, ..., N }.</p><p>After refining the pseudo labels Z with label propagation, we use the mean of the node representations in each cluster as the cluster prototype representation, which is computed by c k = zi=k h i / zi=k 1.</p><p>Semantic Contrastive Objective. Given the prototype assignment Z and prototype representation C, our semantic contrastive learning aims to find the network parameter ? that maximizes the log-likelihood defined as:</p><formula xml:id="formula_12">Q(?) = N i=1 log p(x i |?, C), (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>where p is the probability density function. As p(x i |?, C) = K k=1 log p(x i , z i = k|?, C), we get</p><formula xml:id="formula_14">Q(?) = N i=1 K k=1 log p(x i , z i = k|?, C). (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>The variational lower bound of Q(?) is given by</p><formula xml:id="formula_16">Q(?) ? N i=1 K k=1 q(k|x i ) log p(x i , z i = k|?, C) q(k|x i ) = N i=1 K k=1 q(k|x i ) log p(x i , z i = k|?, C) - N i=1 K k=1 q(k|x i ) log q(k|x i ).<label>(11)</label></formula><p>where q(k|x i ) = p(z i = k|x i , ?, C) denotes the posterior of z i . Since the second term above is a constant, we can maximize the log-likelihood Q(?) by minimizing the function E(?) as follows:</p><formula xml:id="formula_17">E(?) = - N i=1 K k=1 q(k|x i ) log p(x i , z i = k|?, C). (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>By letting q(k|x i ) = 1 {zi=k} , E(?) can be computed by -N i=1 log p(x i , z i |?, C). Under the assumption of a uniform prior distribution of x i over different prototypes, we have p(x i , z i |?, C) ? p(x i |z i , ?, C). The distribution around each prototype generated by our DPMM model is a Gaussian. If we apply 2 normalization on the representation of nodes and prototypes, we can estimate p(x i |z i , ?, C) by</p><formula xml:id="formula_19">p(x i |z i , ?, C) = exp(h i ? c zi /? 2 ) K k=1 exp(h i ? c k /? 2 ) , (<label>13</label></formula><formula xml:id="formula_20">)</formula><p>where ? 2 ? ? 2 , and ? is the variance of the Gaussians in the DPMM model defined by Equation ( <ref type="formula" target="#formula_6">5</ref>). h i and c zi are the representations of x i and z i -th prototype. Thus, E(?) can be minimized by minimizing the loss function as follows:</p><formula xml:id="formula_21">L sem = - 1 N N i=1 log exp(h i ? c zi /? 2 ) K k=1 exp(h i ? c k /? 2 ) . (<label>14</label></formula><formula xml:id="formula_22">)</formula><p>Algorithm 1 Learning algorithm of S 3 -CL Input: Attribute matrix X; Adjacency matrix A; Aggregation degree L Output: Encoder network parameter ? 1: Initialize encoder parameter ? and ? 2: while not converge do 3: Obtain representation of augmented views {H (l) } L l=2 4: Calculate loss Lstr by Eq.( <ref type="formula">4</ref>) 5: Obtain prototype representations C and prototype assignments Z by Eq.( <ref type="formula" target="#formula_7">6</ref>) and Eq.( <ref type="formula" target="#formula_9">7</ref>) 6: Refine C and Z via label propagation 7: Calculate loss Lsem by Eq.( <ref type="formula" target="#formula_21">14</ref>) 8: L = ?Lstr + (1 -?)Lsem 9: Update ? by minimizing L 10: Update momentum encoder ? by Eq.( <ref type="formula" target="#formula_25">16</ref>) 11: end while 12: return ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model Learning</head><p>Given the proposed S 3 -CL learning framework, our goal is to learn expressive node representations that preserve both valuable structural and semantic knowledge without any semantic labels. In this section, we will introduce the overall loss function, and also the optimization of the proposed framework with regard to the network parameters, prototype assignments and prototype representations.</p><p>Overall Loss. To train our model in an end-to-end fashion and learn the encoder f ? (?), we jointly optimize both the structural and semantic contrastive learning losses. The overall objective function is defined as:</p><formula xml:id="formula_23">L = ?L str + (1 -?)L sem , (<label>15</label></formula><formula xml:id="formula_24">)</formula><p>where we aim to minimize L during training, and ? is a balance factor to control the contribution of each loss.</p><p>Notably, in the semantic contrastive learning, the computed pseudo labels Z can be utilized in the negative example sampling process to avoid sampling bias issue <ref type="bibr" target="#b5">(Chuang et al., 2020)</ref> in our structural contrastive learning. We select negative samples in Equation (3) for each node from nodes assigned to different prototypes. Detailed analysis on negative example sampling can be found in Appendix C.2.</p><p>Model Optimization via EM. We adopt EM algorithm to alternately estimate the posterior distribution p(z i |x i , ?, C) and optimize the network parameters ?. We describe the details for the E-step and M-step applied in our methods as follows:</p><p>? E-step. In this step, we aim to estimate the posterior distribution p(z i |x i , ?, C). To achieve that, we fix the network parameter ?, and estimate the prototypes C and the prototype assignment Z. For the sake of the stability of training of the encoder network, we apply our Bayesian non-parametric prototype inference algorithm on the node representations computed by a momentum encoder fol-lowing <ref type="bibr" target="#b9">(He et al., 2020)</ref>, which is H = f ? ( X), The parameter of the momentum encoder ? is updated by the moving average of ? by:</p><formula xml:id="formula_25">? = (1 -m) ? ? + m ? ? ,<label>(16)</label></formula><p>where m ? [0, 1) is the momentum coefficient. ? M-step. Given the posterior distribution computed by the E-step, we aim to maximize the expectation of loglikelihood Q(?), by directly optimizing the semantic contrastive loss function L sem . In order to perform structural and semantic contrastive learning at the same time, we instead optimize a joint overall loss function as formulated in Equation ( <ref type="formula" target="#formula_23">15</ref>).</p><p>After the self-supervised pre-training on the unlabeled input graph, the pre-trained encoder can be directly used to generate node representations for various downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>Benchmarks. We evaluate S 3 -CL on five public benchmarks that are widely used for node representation learning, namely Cora, Citeseer <ref type="bibr" target="#b28">(Sen et al., 2008)</ref>, Pubmed <ref type="bibr" target="#b20">(Namata et al., 2012)</ref>, Coauthor CS and ogbn-arxiv <ref type="bibr" target="#b10">(Hu et al., 2020)</ref>. Cora, Citeseer and Pubmed are three most widely used citation networks. Coauthor CS is co-authorship graph. The ogan-arxiv is a directed citation graph. We summarize the statistics of the datasets in Table <ref type="table" target="#tab_1">1</ref>. Among the five benchmarks we use, ogbn-arxiv is considered to have larger scale, and is more challenging to deal with. Compared Methods. To demonstrate the effectiveness of our proposed method, five state-of-the-arts graph selfsupervised learning methods are compared in our experiments, including DGI <ref type="bibr" target="#b33">(Veli?kovi? et al., 2019)</ref>, MV-GRL <ref type="bibr" target="#b8">(Hassani &amp; Khasahmadi, 2020)</ref>, GMI <ref type="bibr" target="#b25">(Peng et al., 2020)</ref>, GRACE <ref type="bibr">(Zhu et al., 2020b)</ref>, and MERIT <ref type="bibr" target="#b11">(Jin et al., 2021)</ref>. As we consider node classification as our downstream task, we also include five representative supervised node classification methods, namely LP <ref type="bibr" target="#b42">(Zhu et al., 2003)</ref>, GCN <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b32">(Veli?kovi? et al., 2018)</ref>, and SGC <ref type="bibr" target="#b35">(Wu et al., 2019)</ref>, as baselines for the evaluation on node classification.</p><p>To evaluate the performance of pre-trained model for node clustering, we compare S 3 -CL against methods includ-Methods Cora CiteSeer PubMed Coauthor CS ogbn-arxiv SUPERVISED MLP 55.2 ? 0.4 46.5 ? 0.5 71.4 ? 0.3 76.5 ? 0.3 55.5 ? 0.2 LP <ref type="bibr" target="#b42">(Zhu et al., 2003)</ref> 68.0 ? 0.5 45.3 ? 0.6 63.0 ? 0.3 74.3 ? 0.0 68.3 ? 0.0 GCN <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref> 81.7 ? 0.4 70.5 ? 0.3 79.4 ? 0.4 91.8 ? 0.1 71.7 ? 0.3 GAT <ref type="bibr" target="#b32">(Veli?kovi? et al., 2018)</ref> 83.0 ? 0.7 72.5 ? 0.7 79.0 ? 0.3 90.5 ? 0.7 73.2 ? 0.2 SGC <ref type="bibr" target="#b35">(Wu et al., 2019)</ref> 81.5 ? 0.2 73.1 ? 0.1 79.7 ? 0.4 91.5 ? 0.3 69.8 ? 0.2 SELF-SUPERVISED + FINE-TUNING DGI <ref type="bibr" target="#b33">(Veli?kovi? et al., 2019)</ref> 81.7 ? 0.6 71.5 ? 0.7 77.3 ? 0.6 90.0 ? 0.3 67.1 ? 0.4 GMI <ref type="bibr" target="#b25">(Peng et al., 2020)</ref> 82.7 ? 0.2 73.0 ? 0.3 80.1 ? 0.2 91.0 ? 0.0 69.6 ? 0.3 MVGRL <ref type="bibr" target="#b8">(Hassani &amp; Khasahmadi, 2020)</ref> 82.9 ? 0.7 72.6 ? 0.7 79.4 ? 0.3 91.3 ? 0.1 71.3 ? 0.2 GRACE <ref type="bibr">(Zhu et al., 2020b)</ref> 80.0 ? 0.4 71.7 ? 0.6 79.5 ? 1.1 90.1 ? 0.8 71.1 ? 0.2 MERIT <ref type="bibr" target="#b11">(Jin et al., 2021)</ref> 83.1 ? 0.6 74.0 ? 0.7 80.1 ? 0.4 92.4 ? 0.4 71.7 ? 0.1 S 3 -CL (ours)</p><p>84.5 ? 0.4 74.6 ? 0.4 80.8 ? 0.3 93.1 ? 0.4 72.8 ? 0.3  <ref type="bibr" target="#b19">(Lloyd, 1982)</ref> 49.2 32.1 22.9 54.0 30.5 27.8 59.5 31.5 28.1 GAE <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2016)</ref> 59.6 42.9 34.7 40.8 17.6 12.4 67.2 27.7 27.9 ARGA <ref type="bibr" target="#b22">(Pan et al., 2018)</ref> 64.0 44.9 35.2 57.3 35.0 34.1 66.8 30.5 29.5 ARVGA <ref type="bibr" target="#b22">(Pan et al., 2018)</ref> 64.0 45.0 37.4 54.4 26.1 24.5 69.0 29.0 30.6 GALA <ref type="bibr" target="#b24">(Park et al., 2019)</ref> 74.5 57.6 53.1 69.3 44.1 44.6 69.3 32.7 32.1 DBGAN <ref type="bibr" target="#b38">(Zheng et al., 2020)</ref> 74.8 56.0 54.0 67.0 40.7 41.4 69.4 32.4 32.7 DGI <ref type="bibr" target="#b33">(Veli?kovi? et al., 2019)</ref> 55.4 41.1 32.7 51.4 31.5 32.6 58.9 27.7 315 MVGRL <ref type="bibr" target="#b8">(Hassani &amp; Khasahmadi, 2020)</ref> 73.2 56.2 51.9 68.1 43.2 43.4 69.3 34.4 32.3 MERIT <ref type="bibr" target="#b11">(Jin et al., 2021)</ref> 73.6 57.1 52.8 68.9 43.9 44.1 69.5 34.7 32.8 S 3 -CL (ours)</p><p>75.1 60.7 56.6 71.2 46.3 48.5 71.3 36.0 34.7</p><p>Table <ref type="table">3</ref>. Node clustering performance comparison on benchmark datasets.</p><p>ing variational GAE (VGAE) <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2016)</ref>, marginalized GAE (MGAE) <ref type="bibr" target="#b34">(Wang et al., 2017)</ref>, adversarially regularized GAE (ARGA) and VGAE (ARVGA) <ref type="bibr" target="#b22">(Pan et al., 2018)</ref>, GALA <ref type="bibr" target="#b24">(Park et al., 2019)</ref>, DBGAN <ref type="bibr" target="#b38">(Zheng et al., 2020)</ref>, MVGRL <ref type="bibr" target="#b8">(Hassani &amp; Khasahmadi, 2020)</ref>, and MERIT <ref type="bibr" target="#b11">(Jin et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation Results</head><p>Node Classification. To evaluate the trained encoder network, we adopt a linear evaluation protocol by training a separate logistic regression classifier on top of the learned node representations. We follow DGI <ref type="bibr" target="#b33">(Veli?kovi? et al., 2019)</ref> for the evaluation protocol on node classification. The mean classification accuracy with standard deviation on the test nodes after 10 runs of training is reported. For the experiments with ogbn-arxiv, we follow the OGB benchmarking protocol <ref type="bibr" target="#b10">(Hu et al., 2020)</ref>. To avoid out-of-memory issue when evaluating MVGRL, GRACE, and MERIT, we subsample 512 nodes as negative samples for each node during the self-supervised pretraining.</p><p>The node classification results of different methods are reported in Table <ref type="table" target="#tab_2">2</ref>. We can clearly see that S 3 -CL outperforms previous state-of-the-arts self-supervised node rep-resentation learning methods across the five public benchmarks. Such superiority mainly stems from two factors: (i) our approach S 3 -CL grants each node the access to information of nodes in a larger neighborhood; (ii) S 3 -CL infers the semantic information of nodes, and enforce intra-class compactness and inter-class separability on the node representation. With the help of these extra information, node representation generated by S 3 -CL are more informative and distinctive. Without the access to label, S 3 -CL even outperform supervised methods like SGC and GAT.</p><p>Node Clustering. To further evaluate the node representation learned by S 3 -CL, we perform experiments on node clustering. We follow the same evaluation protocol as in <ref type="bibr" target="#b8">(Hassani &amp; Khasahmadi, 2020)</ref>. K-means is applied on the learned node representation to get clustering results. We use accuracy (ACC), normalized mutual information (NMI), and adjusted rand index (ARI) to measure the performance of clustering. We report the averaged clustering results over 10 times of execution.</p><p>The clustering results are displayed in Table <ref type="table">3</ref>. Our approach achieve remarkable performance gain over previous methods. For example, the ARI on Citeseer is improved by 4.4% against MERIT. Such improvement greatly attributes to the fact that S 3 -CL explores the semantic information of nodes instead of enforcing node-wise discrimination alone as previous GCL methods. Thus, the node representation learned by S 3 -CL works well for simple clustering algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation &amp; In-depth Analysis</head><p>Effect of Contrastive Modules. To validate the effectiveness of the structural contrastive learning and semantic contrastive learning in S 3 -CL, we conduct experiments on Citesser, Cora, and Pubmed with two variants of S 3 -CL, each of which has one of the contrastive learning component removed. The results are shown in Table <ref type="table" target="#tab_3">4</ref>. We can observe that the performance of S 3 -CL degrade when any of the component is removed. Effect of Graph Diffusion. We further investigate the effect of the graph diffusion S in the structural contrastive learning of S 3 -CL, by altering the diffusion steps L. A larger L embed information from further neighbors into the node representation. To demonstrate the power of our method in utilizing global structural information, we compare S 3 -CL against SGC with different number of layers L. The results are shown in Figure <ref type="figure" target="#fig_2">2</ref>. We can clearly observe that when L is larger than 5, the performance of SGC starts to drop. However, S 3 -CL consistently achieve improved performance till L = 15.  Effect of Label Propagation. Next, we study the effect of the propagation steps (T ) in our semantic graph contrastive learning. For comparison, we add a baseline model that learn node representation with the same neural network and label propagation scheme as our method, denoted as NN+LP.</p><p>We also compare our method with GCN. Different from S 3 -CL, the training for GCN and NN+LP is supervised by a cross-entropy loss under the semi-supervised node classifi-cation setting <ref type="bibr" target="#b14">(Kipf &amp; Welling, 2017)</ref>. As we can observe from the results in Figure <ref type="figure" target="#fig_3">3</ref>, GCN and NN+LP can achieve similar performance as S 3 -CL when T is relatively small. However, if we further increase the number of propagation steps to T &gt; 5, the performance of GCN breaks down. Though the performance of NN+LP remains stable, it still fall behind S 3 -CL, which shows that S 3 -CL benefits from large receptive field. Representation Visualization. To show the quality of the node representation learned by S 3 -CL, we use t-SNE to visualize the node representation. For comparison, we also visualize the node representation generated by previous SOTA graph contrastive learning method MERIT. The visualization results are shown in Figure <ref type="figure" target="#fig_4">4</ref>. Each mark represents the representation of a node, and the color of the mark denotes its ground truth label. From the figure, we can observe that though some classes can be identified by MERIT, the boundaries between different classes are hard to tell. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a new GCL framework named S 3 -CL, which is able to effectively capture both structural and semantic knowledge from the input graph for selfsupervised node representation learning. By jointly optimizing the structural and semantic contrastive learning losses, the encoder network built with simple neural networks learns expressive node representations for different downstream tasks without using any human-annotated labels. Extensive experiments demonstrate that S 3 -CL outperforms the state-of-the-art GCL counterparts on multiple benchmark graph datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>We implement our proposed framework in Pytorch and optimize it with Adam <ref type="bibr" target="#b12">(Kingma &amp; Ba, 2014)</ref>. All experiments are conducted on a Nvidia Tesla v100 16GB GPU. We set aggregation degree L to 10 in our data augmentation. The output dimension of our encoder network is fixed to 512. The number of hidden units for the MLP is set to 2048. We set the number of negative samples M in L str to 512. For the training of S 3 -CL, we first pre-train the encoder network by minimizing the structural loss function L str and initialize the model parameters with the pre-trained weights. After that, we optimize the model as illustrated in Algorithm 1. We tune the balance factor ? within {0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8}.</p><p>The value of ? in our Bayesian non-parametric prototype inference algorithm is selected from {0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0, 5}. For all the baseline methods, we use Adam as the optimizer. We grid search for the learning rate in {1 ? 10 -5 , 5 ? 10 -5 , 1 ? 10 -4 , 5 ? 10 -4 , 1 ? 10 -3 , 5 ? 10 -3 , 1 ? 10 -2 , 5 ? 10 -2 , 1 ? 10 -1 , 5 ? 10 -1 } on different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Bayesian Non-parametric Prototype Inference Algorithm</head><p>We summarize the algorithm to infer Prototype in Algo-rithm2.</p><p>Algorithm 2 Bayesian Non-parametric Prototype Inference for i = 1 to N do 4:</p><p>Update the pseudo label zi according to Eq.( <ref type="formula" target="#formula_7">6</ref>) 5:</p><p>If min k d ik &gt; ?, set K = K + 1 6: end for 7:</p><p>for k = 1 to K do 8:</p><p>Compute the prototype representation c k = z i =k hi 9:</p><p>end for 10: until convergence 11: Set K = K 12: return C, Z, and K</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experimental Results</head><p>C.1. Study on Prototype Inference.</p><p>In the semantic-level contrastive learning of DGCL, we propose a Bayesian non-parametric to infer prototypes of node representations. During the training, a new prototype will be instantiated if the distance between all existing prototype representations and the representation of a node is larger than a threshold ?. To verify the effectiveness of the proto-type inference algorithm, we design a baseline model that adopts K-means clustering algorithm to infer prototypes as in <ref type="bibr" target="#b29">(Snell et al., 2017)</ref>. For a fair comparison, label propagation is also applied to refine the prototypes obtained by the baseline model. The results showed in Tabel 5 demonstrate that the proposed Bayesian non-parametric prototype inference algorithm is good enough to capture semantic information of the nodes, even without the knowledge on the number of classes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Effect of Negative Example Sampling</head><p>We study the influence of the size of sampled negative examples in the structural contrastive loss of S 3 -CL. The results are shown in Table <ref type="table" target="#tab_5">6</ref>. We can see that as the number of sampled negative examples increase from a small value, the performance of S 3 -CL can be improved. However, further improving the number of sampled negative examples to M &gt; 512 does not lead to better performance.  C.3. Effect of Balance Factor.</p><p>We study the effect of the balance factor ? in the overall loss function on Citeseer and Cora. The results from Figure <ref type="figure" target="#fig_6">5</ref> show that S 3 -CL usually achieve the best results with balance factor ? around 0.4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of the overall framework S 3 -CL for self-supervised node representation learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Effect analysis on the number of diffusion steps (L) of augmentation in the structural contrastive learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Effect analysis on the number of propagation steps (T ) of label propagation in the semantic contrastive learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Representation visualization on the Citeseer dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Input:</head><label></label><figDesc>Node representation h1, ..., hN ; threshold to generate new prototype ? Output: Prototype representation C, pseudo labels Z, and number of clusters K 1: Initialize K = 1, zi = 1 and c1 = 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Effect analysis on the value of balance factor (?) in the overall loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>to learn the representations of nodes in a neighborhood aggregation manner, usually referred to as Message Passing Neural Networks (MPNNs). The representation of a node is learnt by transforming, aggregating node features from its neighbors. Following this scheme, different GNN architectures have been proposed, including graph convolutional networks (GCNs)</figDesc><table /><note><p>Graph Neural Networks. Graph Neural Networks (GNNs) emerged as a powerful tool to learn expressive node representations for graph data. Most prevailing GNNs are built</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The statistics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="2">Features Classes</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell><cell>7</cell></row><row><cell>CiteSeer</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell><cell>6</cell></row><row><cell>PubMed</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell></row><row><cell>Coauthor CS</cell><cell>18,333</cell><cell>81,894</cell><cell>6,805</cell><cell>15</cell></row><row><cell>ogbn-arxiv</cell><cell cols="2">169,343 1,166,243</cell><cell>128</cell><cell>40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Node classification performance comparison on benchmark datasets.</figDesc><table><row><cell>Methods</cell><cell>Cora ACC NMI ARI ACC NMI ARI ACC NMI ARI Citeseer Pubmed</cell></row><row><cell>K-means</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on contrastive components.</figDesc><table><row><cell>Method</cell><cell>CiteSeer</cell><cell>Cora</cell><cell>Pubmed</cell></row><row><cell cols="4">w/o structural 73.1?0.2 83.3?0.3 80.0? 0.3</cell></row><row><cell>w/o semantic</cell><cell cols="3">71.9?0.4 82.2?0.5 79.3? 0.2</cell></row><row><cell>S 3 -CL</cell><cell cols="3">74.6?0.4 84.5?0.4 80.8?0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on prototype inference.</figDesc><table><row><cell>Datasets</cell><cell>Citeseer</cell><cell>Cora</cell><cell>Pubmed</cell></row><row><cell>?</cell><cell>0.15</cell><cell>0.20</cell><cell>0.35</cell></row><row><cell>Estimated K</cell><cell>6</cell><cell>8</cell><cell>3</cell></row><row><cell>Classes</cell><cell>6</cell><cell>7</cell><cell>3</cell></row><row><cell>S 3 -CL</cell><cell cols="3">74.6?0.4 84.5?0.4 80.8?0.3</cell></row><row><cell>Baseline</cell><cell cols="3">74.6?0.3 84.8?0.6 80.7?0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Effect analysis on the value of balance factor (?) in the overall loss function.</figDesc><table><row><cell>M</cell><cell>Citeseer</cell><cell>Cora</cell><cell>Pubmed</cell></row><row><cell>64</cell><cell cols="3">74.1? 0.3 84.0?0.2 80.3?0.3</cell></row><row><cell>256</cell><cell cols="3">74.4? 0.4 84.2?0.4 80.3?0.2</cell></row><row><cell>512</cell><cell cols="3">74.6? 0.4 84.5?0.4 80.8?0.3</cell></row><row><cell cols="4">1024 74.6? 0.6 84.6?0.7 80.7?0.6</cell></row><row><cell cols="4">2048 74.5? 0.2 84.6?0.5 80.8?0.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use those two terms in this paper interchangeably.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rowbottom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gorinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Grand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10934</idno>
		<title level="m">Graph neural diffusion</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In AAAI, 2020a</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15421</idno>
		<title level="m">Scalable graph neural networks via bidirectional propagation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yen-Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00224</idno>
		<title level="m">Debiased contrastive learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graphgt: Machine learning datasets for deep graph generation and transformation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Varala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Angirekula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Khasahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-scale contrastive siamese networks for selfsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05682</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph auto-encoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1111.0352</idno>
		<title level="m">Revisiting k-means: New algorithms via bayesian nonparametrics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Least squares quantization in pcm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on information theory</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on MLG</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Metropolis-hastings data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Symmetric graph convolutional autoencoder for unsupervised graph representation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6519" to="6528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Gibbs sampling for the uninitiated</title>
		<author>
			<persName><forename type="first">P</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hardisty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Maryland Univ College Park Inst for Advanced Computer Studies</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Adagcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05081</idno>
		<title level="m">Adaboosting graph convolutional networks into deep models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep graph infomax. In ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Marginalized graph autoencoder for graph clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Mgae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distribution-induced bidirectional generative adversarial network for graph representation learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Simple spectral graph convolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph neural networks with heterophily</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Semisupervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep Graph Contrastive Representation Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep graph contrastive representation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
