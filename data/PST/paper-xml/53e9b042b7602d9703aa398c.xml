<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Deep Feature Learning for Deformable Registration of MR Brain Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guorong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minjeong</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaozong</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shu</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Radiology and BRIC</orgName>
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Deep Feature Learning for Deformable Registration of MR Brain Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">69D3A781B1FEBAB57CFF77B39E0AAE30</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Establishing accurate anatomical correspondences is critical for medical image registration. Although many hand-engineered features have been proposed for correspondence detection in various registration applications, no features are general enough to work well for all image data. Although many learning-based methods have been developed to help selection of best features for guiding correspondence detection across subjects with large anatomical variations, they are often limited by requiring the known correspondences (often presumably estimated by certain registration methods) as the ground truth for training. To address this limitation, we propose using an unsupervised deep learning approach to directly learn the basis filters that can effectively represent all observed image patches. Then, the coefficients by these learnt basis filters in representing the particular image patch can be regarded as the morphological signature for correspondence detection during image registration. Specifically, a stacked two-layer convolutional network is constructed to seek for the hierarchical representations for each image patch, where the high-level features are inferred from the responses of the low-level network. By replacing the hand-engineered features with our learnt data-adaptive features for image registration, we achieve promising registration results, which demonstrates that a general approach can be built to improve image registration by using data-adaptive features through unsupervised deep learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deformable image registration is very important in many neuroscience and clinical studies to normalize the individual subjects to the reference space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The principle behind image registration is to reveal the anatomical correspondences by maximizing the feature similarities between two images. Thus, image registration often relies on hand-engineered features, e.g., Gabor filters, to drive deformable registration <ref type="bibr" target="#b2">[3]</ref>. However, the pitfall of these hand-engineered image features is that they are not guaranteed to work well for all image data, especially for correspondence detection. For example, it is not effective to use the responses of Gabor filters as the image features to help identify the point in the uniform white matter region of MR (Magnetic Resonance) brain images. Accordingly, learning-based methods have been proposed recently to select a set of best features from a large feature pool for characterizing each image point <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. The criterion is usually set to require the feature vectors on the corresponding points to be (1) discriminative against other non-corresponding points and (2) consistent with the corresponding points across training samples <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>. Then the learnt best features can often improve the registration accuracy and robustness. However, the current learning-based methods require many known correspondences in the training data, which have to be approximated by certain registration methods. Thus, besides being stuck in this chicken-and-egg causality dilemma, these supervised learning-based methods could also be affected by the quality of provided correspondences due to limited accuracy of employed registration methods.</p><p>To address these limitations, we aim to seek for the independent bases derived directly from the image data by unsupervised learning. Specifically, we first consider a feature space consisting of all possible image patches. Then, we aim to learn a set of independent bases that are able to well represent all image patches. Next, the coefficients derived from the patch representation by these learnt bases can be regarded as the morphological signature to characterize each point for correspondence detection.</p><p>Inspired by the recent progress in machine learning, we adopt a stacked convolutional ISA (Independent Subspace Analysis) method <ref type="bibr" target="#b6">[7]</ref> to learn the hierarchical representations for patches from MR brain images. Generally speaking, ISA is an extension of ICA (Independent Component Analysis) to derive image bases for image recognition and pattern classification <ref type="bibr" target="#b7">[8]</ref>. To overcome the limitation of high dimensional data in video processing, Le et al. <ref type="bibr" target="#b6">[7]</ref> introduced the deep learning techniques such as stacking and convolution <ref type="bibr" target="#b8">[9]</ref> to build a layered convolutional neural network that progressively performs ISA in each layer for unsupervised learning. In our application, we deploy the stacked convolutional ISA method to learn the hierarchical representations for the high-dimensional 3D image patches, thus allowing us to establish accurate anatomical correspondences by using hierarchical feature representations (which include not only the low-level image features, but also the high-level features inferred from large-scale image patches).</p><p>To show the advantage of unsupervised feature learning in image registration, we integrate our learnt features, from 60 MR brain images, into multi-channel demons <ref type="bibr" target="#b9">[10]</ref> and also a feature-based registration method <ref type="bibr" target="#b10">[11]</ref>. Through the evaluation on IXI dataset with 83 manually labeled ROIs and also the ADNI dataset, the performances of both state-of-the-art registration methods have been improved substantially, compared with their counterpart methods using hand-engineered image features. These results also show a general way of improving image registration by using hierarchical feature representations through unsupervised deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivations</head><p>Since there are no universal image features that can work well with all image data, learning-based methods were recently developed to learn the best features for all image points to guide registration. Specifically, in the training stage, all sample images are first registered to the particular template by a certain state-of-the-art registration algorithm. Then, the correspondences from the estimated deformation fields are regarded as ground truth. Next, the procedure of feature selection is performed on each point to pick up the best features, so that the similarities between corresponding points can be preferably increased <ref type="bibr" target="#b3">[4]</ref>. In the application stage, each subject point has to be pre-calculated with all kinds of features used in the training stage, and then its correspondence is established by using the learnt best features for each target template point under registration. However, these learning-based image registration methods have the following limitations:</p><p>1) The correspondences provided for training may be inaccurate. A typical example for elderly brain images is shown in the top row of Fig. <ref type="figure" target="#fig_0">1</ref>, where the deformed subject image (Fig. <ref type="figure" target="#fig_0">1(c</ref>)) is far from well-registered with template (Fig. <ref type="figure" target="#fig_0">1(a)</ref>), especially for the ventricles. Thus, it is difficult to learn meaningful features for allowing accurate correspondence detection.</p><p>2) The best features are often learnt only at the template space. Once the template image is changed, the whole learning procedure needs to be redone, which is time consuming. 3) Current learning-based methods are not straightforward to include new image features for training, unless repeating the whole training procedure again. 4) Considering the computational cost, the best features are learnt only from a few types of image features (e.g., only 3 types of image features, each with 4 scales, as used in <ref type="bibr" target="#b5">[6]</ref>), which limits the discriminative power of the learnt features.</p><p>To overcome the above limitations, we propose the following unsupervised learning approach to learn the hierarchical representations for image patches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unsupervised Learning by Independent Subspace Analysis</head><p>Here, we use to denote a particular image patch, which is arranged into the column vector with length , i.e., , , … , . The superscript 1, … , denotes the index of all image patches from the training MR brain images. In the classic feature extraction, a set of filters ,…, are hand-designed to extract features from , where each is a column vector ( , , … , ) and totally filters are used. A certain feature can be computed by the dot product of and , i.e., , . ISA is an unsupervised learning algorithm that automatically learns the basis filters from image patches . As an extension of ICA, the responses , are not required to be all mutually independent in ISA. Instead, these responses can be divided into several groups, each of which is called independent subspace <ref type="bibr" target="#b7">[8]</ref>. Then, the responses are dependent inside each group, but dependencies among different groups are not allowed. Thereby, similar features can be grouped into the same subspace to achieve invariance. We use matrix</p><formula xml:id="formula_0">, ,…, ,</formula><p>,…, to represent the subspace structure of all observed responses , , where each entry , indicates whether basis vector is associated with the subspace. Here, denotes for the dimensionality of subspace of response , . It is worth noting that the matrix is fixed when training ISA <ref type="bibr" target="#b6">[7]</ref>.</p><p>The graphical depiction of ISA is shown in Fig. <ref type="figure">2(a)</ref>. Given image patches (in the bottom of Fig. <ref type="figure">2(a)</ref>), ISA learns optimal (in the middle of Fig. <ref type="figure">2</ref>(a)) via finding independent subspaces (indicated by the pink dots in Fig. <ref type="figure">2(a)</ref>) by solving: arg min ∑ ∑ ; , , . . ,</p><p>where ; , ∑ , is the activation of particular in ISA.</p><p>The orthonormal constraint is used to ensure the diversity of the basis filters . Batch projected gradient descent is used to solve Eq. 1, which is free of tweaking with learning rate and convergence criterion <ref type="bibr" target="#b6">[7]</ref>. Given the optimized and any image patch , it is straightforward to obtain the activation of in each subspace, i.e., ; , ,… . Note that is regarded as the representation coefficient vector of a particular patch with the learnt basis filters , which will be used as the morphological signature for the patch during the registration.</p><p>To ensure the accurate correspondence detection, multi-scale image features are necessary to use, especially for the ventricle example shown in Fig. <ref type="figure" target="#fig_0">1</ref>. However, it also raises a problem of high-dimensionality in learning features from the large-scale image patches. To this end, we follow the approach used in video data analysis <ref type="bibr" target="#b6">[7]</ref> by constructing a two-layer network, as show in Fig. <ref type="figure">2(b)</ref>, for scaling up the ISA to the large-scale image patches. Specifically, we first train the ISA in the first layer based on the image patches with smaller scale. After that, a sliding window (with the same scale in the first layer) convolutes with each large-scale patch to get a sequence of overlapped small-scale patches (shown in Fig. <ref type="figure">2(c)</ref>). The combined responses of these overlapped patches through the first layer ISA (a sequence of blue triangles in Fig. <ref type="figure">2(b)</ref>) are whitened by PCA and then used as the input (pink triangles in Fig. <ref type="figure">2(b)</ref>)</p><p>to the second layer that is further trained by another ISA. In this way, high-level understanding of large-scale image patch can be perceived from the low-level image features detected by the basis filters in the first layer. It is apparent that this hierarchical patch representation is fully data-adaptive, thus free of requirement on known correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2. Graphical depiction of ISA and the stacked convolutional ISA network</head><p>The basis filters learnt in the first layer, from 60 MR brains, are shown in Fig. <ref type="figure">3</ref>, where we show only a 2D slice to represent each 3D filter. Most of them look like Gabor filters that can detect edges in different orientations. Given the stacked 2-layer convolutional ISA network, the input image patch is now extracted in a large scale. The hierarchical representation coefficient is calculated as follows: (1) extract a set of overlapped small-scale patches from by sliding window (Fig. <ref type="figure">2(c</ref>)); (2) calculate the response of each small-scale patch in the 1 st layer ISA; (3) combine the responses in step (2) and further reduce the dimension by learnt PCA; (4) calculate the response in the 2 nd layer ISA as the hierarchical representation coefficients for patch . In registration, we extract image patch in the large scale for each underlying point and use as the morphological signature to detect correspondence. Here, we use normalized cross correlation as the similarity measurement between two representation coefficient vectors. The performance of our learnt features is shown in Fig. <ref type="figure" target="#fig_0">1(f)</ref>, where, for a template point (indicated by red cross in Fig. <ref type="figure" target="#fig_0">1(a)</ref>), we can successfully find its corresponding point in the subject image even with large ventricle. Other hand-engineered features either detect too many non-corresponding points (when using entire intensity patch as the feature vector in Fig. <ref type="figure" target="#fig_0">1(d)</ref>) or have too low responses and thus miss the correspondence (when using SIFT features in Fig. <ref type="figure" target="#fig_0">1(e)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Improving Deformable Image Registration with Learnt Features</head><p>Without loss of generalization, we show two examples of integrating the learnt features by ISA into the state-of-the-art registration methods. First, it is straightforward to deploy multi-channel demons <ref type="bibr" target="#b11">[12]</ref> by regarding each channel with the element in . Second, we replace the hand-engineered attribute vector (i.e., local intensity histogram) in a feature-ba learnt features, while keepin tration. In the following, w methods after equipping wi Fig. <ref type="figure">3</ref>. The learnt bases f</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we demonst evaluating the registration images from ADNI datase we randomly sample ~15,0 layers are 13 13 13 an filters in the 1 st layer and th and 150, respectively signature used for regi For comparison, we set method <ref type="bibr" target="#b10">[11]</ref> as the baseline each channel of multi-chan features in HAMMER, whi Since PCA is also an unsu the Eigen vectors, we fur patches with multi-channel show the better performanc ISA, we deploy PCA on 7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiment on IXI</head><p>IXI dataset 2 consists of 30 s FSL software package (http for all subjects to the templ above methods, respectivel 1 Source code can be downloa 2 http://biomedic.doc Main.Datasets ased registration method, i.e., HAMMER <ref type="bibr" target="#b10">[11]</ref> 1 , with ng the same optimization mechanism for deformable reg we will show the registration improvement for these t ith our learnt features.</p><p>filters (13 13) in the first layer from 60 MR brain images trate the performance of unsupervised learning method accuracy w/o learnt features. Specifically, we use 60 M et (http://adni.loni.ucla.edu/) for training. For each ima 000 image patches, where the patch size for the 1 st and nd 21 21 21, respectively. The number of learnt ba he dimensionality of subspace in the 2 nd layer are 3 y. Therefore, the overall dimensionality of morpholog istration is 150 for each image patch . diffeomorphic demons <ref type="bibr" target="#b1">[2]</ref> and the HAMMER registrat e methods. Next, we integrate the learnt image features i nnel demons and also replace the hand-engineered im ch are denoted below as M+ISA, and H+ISA, respectiv upervised dimensionality reduction method by calculat rther integrate the PCA-based dimension-reduced im l demons (M+PCA) and HAMMER (H+PCA), in orde ce by deep learning. To keep similar number of features T in tion th 6 h of wml 83 ROIs, and also their overall Dice ratio. Specifically, the overall Dice ratios are 78.5% by Demons, 75.2% by M+PCA, 79.0 by M+ISA, 78.9% by HAMMER, 75.4% by H+PCA, and 80.1% by H+ISA, respectively. The detailed Dice ratios in 10 typical brain structures are also shown in Fig. <ref type="figure" target="#fig_1">4</ref>. It is clear that the registration accuracy is improved by the learnt image features, compared to the baseline methods. The performances by M+PCA and H+PCA are worse than the baseline methods, because PCA assumes Gaussian distribution of image patches and may be not able to model the actual complicated patch distribution. Furthermore, we apply the paired t-test on Dice ratios by the above 6 registration methods. We found that M+ISA and H+ISA have significant improvements over their respective baseline methods (p&lt;0.05) in 37 and 68 out of 83 ROIs in IXI dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiment on ADNI Dataset</head><p>In this experiment, we randomly select 20 MR images from ADNI dataset (different with the training images). The preprocessing steps include skull removal, bias correction, and intensity normalization. All subject images are linearly registered with template image by FLIRT. After that, we deploy 6 deformable registration methods to further normalize all subjects onto the template space. First, we show the Dice ratios on 3 tissue types (ventricle, gray mater, and white matter) by 6 registration methods in Table <ref type="table" target="#tab_1">1</ref>, where H+ISA method achieves the highest Dice ratio as in IXI dataset. It is worth noting that Demons achieves very high overlap (although still lower than H+ISA), because its registration is guided by intensities which are also used for tissue segmentation in our experiment (by FAST in FSL software package). Thus, it is not very fair for the feature-based registration method, although H+ISA still gets best. Second, since ADNI also provides the labeled hippocampi, we further compare the overlap ratio of hippocampus by the registration methods using different features, i.e., our learnt ISA features and PCA features. Taking HAMMER as example, H+ISA achieves overall 2.74% improvement, compared to both H+PCA and HAMMER which have similar performance. On the other hand, M+ISA achieves overall 0.19% 0.24% improvements compared to M+PCA and the original Demons, respectively. We also apply the paired ttest upon M+ISA v.s. Demons and H+ISA v.s. HAMMER, and found that only H+ISA has significant improvement ( 0.05) over the baseline method (HAMMER).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have presented using the unsupervised learning method to explore the optimal image features for deformable image registration. In particular, a stacked convolutional ISA network is built to learn the hierarchical basis filters from a number of image patches in the MR brain images, thus the learnt basis filters are fully adaptive to both global and local image appearances. After incorporating these learnt image features into the existing state-of-the-art registration methods, we achieved promising registration results, showing that a general registration approach could be built by using hierarchical and data-adaptive features through unsupervised deep learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a-c) Comparison of template, subject, and deformed subject. (d-f) Similarity between a template point (indicated by red cross) and all subject points, measured with hand-engineered features (d, e) and the features learnt by unsupervised learning, respectively.</figDesc><graphic coords="3,86.55,356.16,267.76,195.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Dice ratios for 10 typical ROIs in IXI dataset</figDesc><graphic coords="7,84.99,193.41,270.88,133.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>7 7  patches and keep &gt;70% energy of image patche</figDesc><table><row><cell>our</cell></row><row><cell>gis-</cell></row><row><cell>two</cell></row><row><cell>d by</cell></row><row><cell>MR</cell></row><row><cell>age,</cell></row><row><cell>2 nd</cell></row><row><cell>asis</cell></row><row><cell>300</cell></row><row><cell>gical</cell></row><row><cell>tion</cell></row><row><cell>into</cell></row><row><cell>mage</cell></row><row><cell>ely.</cell></row><row><cell>ting</cell></row><row><cell>mage</cell></row><row><cell>er to</cell></row><row><cell>s as</cell></row><row><cell>es.</cell></row><row><cell>Dataset</cell></row><row><cell>subjects, each with 83 manually delineated ROIs. FLIRT</cell></row><row><cell>p://fsl.fmrib.ox.ac.uk) is used to perform affine registrat</cell></row><row><cell>late space. Then, these images are further registered wit</cell></row><row><cell>ly. In this way, we can calculate the Dice ratio for each</cell></row><row><cell>aded at http://www.nitrc.org/projects/hammerw</cell></row><row><cell>.ic.ac.uk/brain-development/index.php?n=</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>The Dice ratios of VN, GM, and WM on ADNI dataset. (unit: %)</figDesc><table><row><cell>Methods</cell><cell>Ventricle</cell><cell>Gray Matter</cell><cell>White Matter</cell><cell>Overall</cell></row><row><cell>Demons</cell><cell>93.2</cell><cell>78.0</cell><cell>89.7</cell><cell>86.9</cell></row><row><cell>M+PCA</cell><cell>84.5</cell><cell>71.6</cell><cell>80.5</cell><cell>78.9</cell></row><row><cell>M+ISA</cell><cell>88.9</cell><cell>76.5</cell><cell>87.8</cell><cell>84.4</cell></row><row><cell>HAMMER</cell><cell>91.5</cell><cell>72.5</cell><cell>82.4</cell><cell>82.1</cell></row><row><cell>H+PCA</cell><cell>90.6</cell><cell>71.9</cell><cell>83.5</cell><cell>82.0</cell></row><row><cell>H+ISA</cell><cell>95.0</cell><cell>78.6</cell><cell>88.1</cell><cell>87.3</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image registration methods: A survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zitová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Flusser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="977" to="1000" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffeomorphic demons: efficient non-parametric image registration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vercauteren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="61" to="S72" />
			<date type="published" when="2009-01">1, suppl. 1. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Local frequency representations for robust multimodal image registration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Vermuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Marroquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="462" to="469" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DRAMMS: Deformable registration via attribute matching and mutualsaliency weighting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="622" to="639" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning-based deformable registration of MR brain images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1145" to="1157" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning best features and deformation statistics for hierarchical registration of MR brain images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPMI 2007</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Lelieveldt</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4584</biblScope>
			<biblScope unit="page" from="160" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1705" to="1720" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional network for images, speech, and time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Handbook of Brain Theory and Neural Networks</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Registration of 4D cardiac CT sequences under trajectory constraints with multichannel diffeomorphic demons</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peyrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1351" to="1368" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image registration by local histogram matching</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1161" to="1172" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Registration of 4D time-series of cardiac images with multichannel Diffeomorphic Demons</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Peyrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delingette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sermesant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2008, Part II</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Axel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Fichtinger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Székely</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">5242</biblScope>
			<biblScope unit="page" from="972" to="979" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
