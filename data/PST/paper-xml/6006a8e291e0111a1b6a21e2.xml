<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-16">16 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
							<email>h.yin1@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
							<email>jundong@virginia.edu</email>
						</author>
						<author>
							<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
							<email>qinyong.wang@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Nguyen</forename><surname>Quoc</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Viet</forename><surname>Hung</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
							<email>xiangliang.zhang@kaust.edu.sa</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">The University of Queensland</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Griffith University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-16">16 Jan 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2101.06448v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Social Recommendation</term>
					<term>Self-supervised Learning</term>
					<term>Hypergraph Learning</term>
					<term>Graph Convolutional Network</term>
					<term>Recommender System</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Social relations are often used to improve recommendation quality and most existing social recommendation models exploit pairwise relations to mine potential user preferences. However, real-life interactions among users are very complicated and user relations can be high-order. Hypergraph provides a natural way to model complex high-order relations, while its potential for social recommendation is under-explored. In this paper, we fill this gap and propose a multi-channel hypergraph convolutional network to enhance social recommendation by leveraging high-order user relations. Technically, each channel in the network encodes a hypergraph that depicts a common high-order user relation pattern via hypergraph convolution. By aggregating the embeddings learned through multiple channels, we obtain comprehensive user representations to generate recommendation results. However, the aggregation operation might also obscure the inherent characteristics of different types of high-order connectivity information. To compensate for the aggregating loss, we innovatively integrate self-supervised learning into the training of the hypergraph convolutional network to regain the connectivity information. The experimental results on multiple real-world datasets show that the proposed model outperforms the SOTA methods, and the ablation study verifies the effectiveness of the multi-channel setting and the selfsupervised task. The implementation of our model is available via https://github.com/Coder-Yu/RecQ.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Over the past decade, the social media boom has dramatically changed people's ways of thinking and behaving. It has been revealed that people may alter their attitudes and behaviors in response to what they perceive their friends might do or think, which is known as the social influence <ref type="bibr" target="#b6">[7]</ref>. Meanwhile, there are also studies <ref type="bibr" target="#b23">[24]</ref> showing that people tend to build connections with others who have similar preferences with them, which is called the homophily. Based on these findings, social relations are often integrated into recommender systems to mitigate the data sparsity issue <ref type="bibr" target="#b30">[31]</ref>. Generally, if a user has few interactions with items, the * Corresponding author and having equal contribution with the first author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Friends</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Friends</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Friends</head><p>Buy Buy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Friends Buy Buy</head><p>Figure <ref type="figure">1</ref>: The common types of high-order user relations in social recommender systems.</p><p>model would rely on her friends' preferences to generate better recommendations. Following this paradigm, a large number of social recommendation models have been developed <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59]</ref> and have shown stronger performance compared with general recommendation models.</p><p>Recently, graph neural networks (GNNs) <ref type="bibr" target="#b40">[41]</ref> have achieved great success in a wide range of areas. Owing to their powerful capability in modeling relational data, GNNs-based models also have shown prominent performance in social recommendation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b55">56]</ref>. However, a key limitation of these GNNs-based social recommendation models is that they only exploit the simple pairwise user relations and ignore the ubiquitous high-order relations among users. Although the long-range dependencies of relations (i.e. transitivity of friendship), which are also considered high-order, can be captured by using k graph neural layers to incorporate features from k-hop social neighbors, these GNNs-based models are unable to formulate and capture the complex high-order user relation patterns (as shown in Fig. <ref type="figure">1</ref>) beyond pairwise relations. For example, it is natural to think that two users who are socially connected and also purchased the same item have a stronger relationship than those who are only socially connected, whereas the common purchase information in the former is often neglected in previous social recommendation models.</p><p>Hypergraph <ref type="bibr" target="#b3">[4]</ref>, which generalizes the concept of edge to make it connect more than two nodes, provides a natural way to model complex high-order relations among users. Despite the great advantages over the simple graph in user modeling, the strengths of hypergraph are under-explored in social recommendation. In this paper, we fill this gap by investigating the potentials of fusing hypergraph modeling and graph convolutional networks, and propose a Multi-channel Hypergraph Convolutional Network (MHCN) to enhance social recommendation by exploiting high-order user relations. Technically, we construct hypergraphs by unifying nodes that form specific triangular relations, which are instances of a set of carefully designed triangular motifs (shown in Fig. <ref type="figure" target="#fig_0">2</ref>). As we define multiple categories of motifs which concretize different types of high-order relations such as 'having a mutual friend', 'friends purchasing the same item', and 'strangers but purchasing the same item' in social recommender systems, each channel of the proposed hypergraph convolutional network undertakes the task of encoding a different motif-induced hypergraph. By aggregating multiple user embeddings learned through multiple channels, we can obtain the comprehensive user representations which are considered to contain multiple types of high-order relation information and have the great potential to generate better recommendation results with the item embeddings.</p><p>However, despite the benefits of the multi-channel setting, the aggregation operation might also obscure the inherent characteristics of different types of high-order connectivity information <ref type="bibr" target="#b51">[52]</ref>, as different channels would learn embeddings with varying distributions on different hypergraphs. To address this issue and fully inherit the rich information in the hypergraphs, we innovatively integrate a self-supervised task <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref> into the training of the multi-channel hypergraph convolutional network. Unlike existing studies which enforce perturbations on graphs to augment the ground-truth <ref type="bibr" target="#b50">[51]</ref>, we propose to construct self-supervision signals by exploiting the hypergraph structures, with the intuition that the comprehensive user representation should reflect the user node's local and global high-order connectivity patterns in different hypergraphs. Concretely, we leverage the hierarchy in the hypergraph structures and hierarchically maximizes the mutual information between representations of the user, the user-centered sub-hypergraph, and the global hypergraph. The mutual information here measures the structural informativeness of the sub-and the whole hypergraph towards inferring the user features through the reduction in local and global structure uncertainty. Finally, we unify the recommendation task and the self-supervised task under a primary &amp; auxiliary learning framework. By jointly optimizing the two tasks, the performance of the recommendation task achieves significant gains.</p><p>Overall, the major contributions of this paper are summarized as follows:</p><p>• We investigate the potentials of fusing hypergraph modeling and graph neural networks in social recommendation by exploiting multiple types of high-order user relations under a multi-channel setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Social Recommendation</head><p>As suggested by the social science theories <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>, users' preferences and decisions are often influenced by their friends. Based on this fact, social relations are integrated into recommender systems to alleviate the issue of data sparsity. Early exploration of social recommender systems mostly focuses on matrix factorization (MF), which has a nice probabilistic interpretation with Gaussian prior and is the most used technique in social recommendation regime.</p><p>The extensive use of MF marks a new phase in the research of recommender systems. A multitude of studies employ MF as their basic model to exploit social relations since it is very flexible for MF to incorporate prior knowledge. The common ideas of MF-based social recommendation algorithms can be categorized into three groups: co-factorization methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44]</ref>, ensemble methods <ref type="bibr" target="#b18">[19]</ref>, and regularization methods <ref type="bibr" target="#b21">[22]</ref>. Besides, there are also studies using socially-aware MF to model point-of-interest <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>, preference evolution <ref type="bibr" target="#b36">[37]</ref>, item ranking <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b58">59]</ref>, and relation generation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>Over the recent years, the boom of deep learning has broadened the ways to explore social recommendation. Many research efforts demonstrate that deep neural models are more capable of capturing high-level latent preferences <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>. Specifically, graph neural networks (GNNs) <ref type="bibr" target="#b60">[61]</ref> have achieved great success in this area, owing to their strong capability to model graph data. GraphRec <ref type="bibr" target="#b8">[9]</ref> is the first to introduce GNNs to social recommendation by modeling the user-item and user-user interactions as graph data. DiffNet <ref type="bibr" target="#b38">[39]</ref> and its extension DiffNet++ <ref type="bibr" target="#b37">[38]</ref> model the recursive dynamic social diffusion in social recommendation with a layer-wise propagation structure. Wu et al. <ref type="bibr" target="#b39">[40]</ref> propose a dual graph attention network to collaboratively learn representations for two-fold social effects. Song et al. develop DGRec <ref type="bibr" target="#b31">[32]</ref> to model both users' session-based interests as well as dynamic social influences. Yu et al. <ref type="bibr" target="#b55">[56]</ref> propose a deep adversarial framework based on GCNs to address the common issues in social recommendation. In summary, the common idea of these works is to model the user-user and user-item interactions as simple graphs with pairwise connections and then use multiple graph neural layers to capture the node dependencies. Despite their success, we argue that these GNN-based methods are unable to model the high-order user relation information beyond simple pairwise interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hypergraph in Recommender Systems</head><p>Hypergraph <ref type="bibr" target="#b3">[4]</ref> provides a natural way to model complex highorder relations and has been extensively employed to tackle various problems. With the development of deep learning, some studies combine GNNs and hypergraphs to enhance representation learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>. However, despite the great capacity in modeling complex data, the potentials of hypergraph for improving recommender systems have been rarely explored. There are only several studies focusing on the combination of these two topics. Bu et al. <ref type="bibr" target="#b4">[5]</ref> introduce hypergraph learning to music recommender systems, which is the earliest attempt. The most recent combinations are HyperRec <ref type="bibr" target="#b35">[36]</ref> and DHCF <ref type="bibr" target="#b14">[15]</ref>, which borrow the strengths of hypergraph neural networks to model the short-term user preference for nextitem recommendation and the high-order correlations among users and items for general collaborative filtering, respectively. As for the applications in social recommendation, HMF <ref type="bibr" target="#b59">[60]</ref> uses hypergraph topology to describe and analyze the interior relation of social network in recommender systems, but it does not fully exploit high-order social relations since HMF is a hybrid recommendation model. LBSN2Vec <ref type="bibr" target="#b44">[45]</ref> is a social-aware POI recommendation model that builds hyperedges by jointly sampling friendships and check-ins with random walk, but it focuses on connecting different types of entities instead of exploiting the high-order social network structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Self-Supervised Learning</head><p>Self-supervised learning <ref type="bibr" target="#b13">[14]</ref> is an emerging paradigm to learn with the ground-truth samples obtained from the raw data. It was firstly used in the image domain <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b56">57]</ref>. The latest advances in this area extend self-supervised learning to graph representation learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>. These studies mainly develop self-supervision tasks from the perspective of investigating graph structure. Node properties like degree and proximity, which are seen as local structure information, are often used as the ground truth to fully exploit the unlabeled data <ref type="bibr" target="#b15">[16]</ref>. Meanwhile, global structure information like node pair distance is also harnessed to facilitate representation learning <ref type="bibr" target="#b32">[33]</ref>. Besides, contrasting congruent and incongruent views of graphs with mutual information maximization <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref> is another way to set up a self-supervised task, which has shown promising results compared with supervised baselines in many situations.</p><p>As the research of self-supervised learning is still in its infancy, there are only several works combining it with recommender systems <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b61">62]</ref>. These efforts either mine self-supervision signals from future/surrounding sequential data <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b42">43]</ref>, or mask attributes of items/users to learn correlations of the raw data <ref type="bibr" target="#b61">[62]</ref>. However, these thoughts cannot be easily adopted to social recommendation where temporal factors and attributes may not be available. The most relevant work to ours is GroupIM <ref type="bibr" target="#b29">[30]</ref>, which maximizes mutual information between representations of groups and group members to overcome the sparsity problem of group interactions. As the group can be seen as a special social clique, this work can be a corroboration of the effectiveness of social selfsupervision signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED MODEL 3.1 Preliminaries</head><p>Let 𝑈 = {𝑢 1 , 𝑢 2 , ..., 𝑢 𝑚 } denote the user set (|𝑈 | = 𝑚), and 𝐼 = {𝑖 1 , 𝑖 2 , ..., 𝑖 𝑛 } denote the item (|𝐼 | = 𝑛). I (𝑢) is the set of user consumption in which items consumed by user 𝑢 are included. 𝑹 ∈ R 𝑚×𝑛 is a binary matrix that stores user-item interactions. For each pair (𝑢, 𝑖), 𝑟 𝑢𝑖 = 1 indicates that user 𝑢 consumed item 𝑖 while 𝑟 𝑢𝑖 = 0 means that item 𝑖 is unexposed to user 𝑢, or user 𝑢 is not interested in item 𝑖. In this paper, we focus on top-K recommendation, and r𝑢𝑖 denotes the probability of item 𝑖 to be recommended to user 𝑢. As for the social relations, we use 𝑺 ∈ R 𝑚×𝑚 to denote the relation matrix which is asymmetric because we work on directed social networks. In our model, we have multiple convolutional layers, and we use {𝑷 (1) , 𝑷 (2) , • • • , 𝑷 (𝑙) } ∈ R 𝑚×𝑑 and {𝑸 (1) , 𝑸 (2) , • • • , 𝑸 (𝑙) } ∈ R 𝑛×𝑑 to denote the user and item embeddings of size 𝑑 learned at each layer, respectively. In this paper, we use bold capital letters to denote matrices and bold lowercase letters to denote vectors.</p><p>Definition 1: Let 𝐺 = (𝑉 , 𝐸) denote a hypergraph, where 𝑉 is the vertex set containing 𝑁 unique vertices and 𝐸 is the edge set containing 𝑀 hyperedges. Each hyperedge 𝜖 ∈ 𝐸 can contain any number of vertices and is assigned a positive weight 𝑊 𝜖𝜖 , and all the weights formulate a diagonal matrix 𝑾 ∈ R 𝑀×𝑀 . The hypergraph can be represented by an incidence matrix 𝑯 ∈ R 𝑁 ×𝑀 where 𝐻 𝑖𝜖 = 1 if the hyperedge 𝜖 ∈ 𝐸 contains a vertex 𝑣 𝑖 ∈ 𝑉 , otherwise 0. The vertex and edge degree matrices are diagonal matrices denoted by 𝑫 and 𝑳, respectively, where 𝐷 𝑖𝑖 = 𝑀 𝜖=1 𝑊 𝜖𝜖 𝐻 𝑖𝜖 ; 𝐿 𝜖𝜖 = 𝑁 𝑖=1 𝐻 𝑖𝜖 . It should be noted that, in this paper, 𝑊 𝜖𝜖 is uniformly assigned 1 and hence 𝑾 is an identity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Channel Hypergraph Convolutional Network for Social Recommendation</head><p>In this section, we present our model MHCN, which stands for Multi-channel Hypergraph Convolutional Network. In Fig. <ref type="figure" target="#fig_2">3</ref>, the schematic overview of our model is illustrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Hypergraph Construction.</head><p>To formulate the high-order information among users, we first align the social network and user-item interaction graph in social recommender systems and then build hypergraphs over this heterogeneous network. Unlike prior models which construct hyperedges by unifying given types of entities <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b44">45]</ref>, our model constructs hyperedges according to the graph structure. As the relations in social networks are often directed, the connectivity of social networks can be of various types. In this paper, we use a set of carefully designed motifs to depict the common types of triangular structures in social networks, which guide the hypergraph construction. Motif, as the specific local structure involving multiple nodes, is first introduced in <ref type="bibr" target="#b24">[25]</ref>. It has been widely used to describe complex structures in a wide range of networks including the social network. In this paper, we only focus on triangular motifs because of the ubiquitous triadic closure in social networks, but our model can be seamlessly extended to handle on more complex motifs and we leave it as our future work. Fig. <ref type="figure" target="#fig_0">2</ref> shows all the used triangular motifs. It has been revealed that M 1 − M 7 are crucial for social computing <ref type="bibr" target="#b2">[3]</ref>, and we further design M 8 − M 10 to involve user-item interactions to complement. Given motifs M 1 − M 10 , we categorize them into three groups according to the underlying semantics. M 1 − M 7 summarize all the possible triangular relations in explicit social networks and describe the high-order social connectivity like 'having a mutual friend'. We name this group 'Social Motifs'. M 8 − M 9 represent the compound relation, that is, 'friends purchasing the same item'. This type of relation can be seen as a signal of strengthened tie, and we name M 8 − M 9 'Joint Motifs'. Finally, we should also consider users who have no explicit social connections. So, M 10 is non-closed and defines the implicit high-order social relation that users who are not socially connected but purchased the same item. We name M 10 'Purchase Motif '. Under the regulation of these three types of motifs, we can</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Social Motifs</head><p>Joint Motifs Purchase Motif Follow Purchase construct three hypergraphs that contain different high-order user relation patterns. We use the incidence matrices 𝑯 𝑠 , 𝑯 𝑗 and 𝑯 𝑝 to represent these three motif-induced hypergraphs, respectively, where each column of these matrices denotes a hyperedge. For example, in Fig. <ref type="figure" target="#fig_2">3</ref>, {𝑢 1 , 𝑢 2 , 𝑢 3 } is an instance of M 4 , and we use 𝑒 1 to denote this hyperedge. Then, according to definition 1, we have</p><formula xml:id="formula_0">𝐻 𝑠 𝑢 1 ,𝑒 1 = 𝐻 𝑠 𝑢 2 ,𝑒 1 = 𝐻 𝑠 𝑢 3 ,𝑒 1 = 1.</formula><p>Additionally, in this paper, we work on directed graphs, but we can easily transfer our model to undirected graphs by only using M 4 , M 8 and M 10 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Multi-Channel Hypergraph Convolution.</head><p>In this paper, we use a three-channel setting, including 'Social Channel (s)', 'Joint Channel (j)', and 'Purchase Channel (p)', in response to the three types of triangular motifs, but the number of channels can be adjusted to adapt to more sophisticated situations. Each channel is responsible for encoding one type of high-order user relation pattern. As different patterns may show different importances to the final recommendation performance, directly feeding the full base user embeddings 𝑷 (0) to all the channels is unwise. To control the information flow from the base user embeddings 𝑷 (0) to each channel, we design a pre-filter with self-gating units (SGUs), which is defined as:</p><formula xml:id="formula_1">𝑷 (0) 𝑐 = 𝑓 𝑐 gate (𝑷 (0) ) = 𝑷 (0) ⊙ 𝜎 (𝑷 (0) 𝑾 𝑐 𝑔 + 𝒃 𝑐 𝑔 ),<label>(1)</label></formula><p>where 𝑾 𝑐 𝑔 ∈ R 𝑑×𝑑 , 𝒃 𝑐 𝑔 ∈ R 𝑑 are parameters to be learned, 𝑐 ∈ {𝑠, 𝑗, 𝑝} represents the channel, ⊙ denotes the element-wise product and 𝜎 is the sigmoid nonlinearity. The self-gating mechanism effectively serves as a multiplicative skip-connection <ref type="bibr" target="#b7">[8]</ref> that learns a nonlinear gate to modulate the base user embeddings at a featurewise granularity through dimension re-weighting, then we obtain the channel-specific user embeddings 𝑷 (0) 𝑐 . Referring to the spectral hypergraph convolution proposed in <ref type="bibr" target="#b9">[10]</ref>, we define our hypergraph convolution as:</p><formula xml:id="formula_2">𝑷 (𝑙+1) 𝑐 = 𝑫 −1 𝑐 𝑯 𝑐 𝑳 −1 𝑐 𝑯 ⊤ 𝑐 𝑷 (𝑙) 𝑐 . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>The difference is that we follow the suggestion in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>  𝑐 defines the message passing from nodes to hyperedges and then premultiplying 𝑯 𝑐 is viewed to aggregate information from hyperedges to nodes. However, despite the benefits of Table <ref type="table">1</ref>: Computation of motif-induced adjacency matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motif Matrix Computation</head><p>𝑨</p><formula xml:id="formula_4">𝑀 𝑖 = M 1 𝑪 = (𝑼 𝑼 ) ⊙ 𝑼 𝑇 𝑪 + 𝑪 ⊤ M 2 𝑪 = (𝑩𝑼 ) ⊙ 𝑼 𝑇 + (𝑼 𝑩) ⊙ 𝑼 𝑇 + (𝑼 𝑼 ) ⊙ 𝑩 𝑪 + 𝑪 ⊤ M 3 𝑪 = (𝑩𝑩) ⊙ 𝑼 + (𝑩𝑼 ) ⊙ 𝑩 + (𝑼 • 𝑩) ⊙ 𝑩 𝑪 + 𝑪 ⊤ M 4 𝑪 = (𝑩𝑩) ⊙ 𝑩 𝑪 M 5 𝑪 = (𝑼 𝑼 ) ⊙ 𝑼 + (𝑼 𝑼 𝑇 ) ⊙ 𝑼 + (𝑼 𝑇 𝑼 ) ⊙ 𝑼 𝑪 + 𝑪 ⊤ M 6 𝑪 = (𝑼 𝑩) ⊙ 𝑼 + (𝑩𝑼 𝑇 ) ⊙ 𝑼 𝑇 + (𝑼 𝑇 𝑼 ) ⊙ 𝑩 𝑪 M 7 𝑪 = (𝑼 𝑇 𝑩) ⊙ 𝑼 𝑇 + (𝑩𝑼 ) ⊙ 𝑼 + (𝑼 𝑼 𝑇 ) ⊙ 𝑩 𝑪 M 8 𝑪 = (𝑹𝑹 𝑇 ) ⊙ 𝑩 𝑪 M 9 𝑪 = (𝑹𝑹 𝑇 ) ⊙ 𝑼 𝑪 + 𝑪 ⊤ M 10 𝑪 = 𝑹𝑹 𝑇 𝑪</formula><p>hypergraph convolution, there are a huge number of motif-induced hyperedges (e.g. there are 19,385 social triangles in the used dataset, LastFM), which would cause a high cost to build the incidence matrix 𝑯 𝑐 . But as we only exploit triangular motifs, we show that this problem can be solved in a flexible and efficient way by leveraging the associative property of matrix multiplication. Following <ref type="bibr" target="#b57">[58]</ref>, we let 𝑩 = 𝑺 ⊙ 𝑺 𝑇 and 𝑼 = 𝑺 − 𝑩 be the adjacency matrices of the bidirectional and unidirectional social networks respectively. We use 𝑨 𝑀 𝑘 to represent the motif-induced adjacency matrix and (𝑨 𝑀 𝑘 ) 𝑖,𝑗 = 1 means that vertex 𝑖 and vertex 𝑗 appear in one instance of M 𝑘 . As two vertices can appear in multiple instances of M 𝑘 , (𝑨 𝑀 𝑘 ) 𝑖,𝑗 is computed by:</p><formula xml:id="formula_5">(𝑨 𝑀 𝑘 ) 𝑖,𝑗 = #(𝑖, 𝑗 occur in the same instance of M 𝑘 ). (3)</formula><p>Table <ref type="table">1</ref> shows how to calculate 𝑨 𝑀 𝑘 in the form of matrix multiplication. As all the involved matrices in Table <ref type="table">1</ref> are sparse matrices, 𝑨 𝑀 𝑘 can be efficiently calculated. Specifically, the basic unit in Table <ref type="table">1</ref> is in a general form of 𝑿 𝒀 ⊙ 𝒁 , which means 𝑨 𝑀 1 to 𝑨 𝑀 9 may be sparser than 𝒁 (i.e. 𝑩 or 𝑼 ) or as sparse as 𝒁 . 𝑨 𝑀 10 could be a little denser, but we can filter out the popular items (we think consuming popular items might not reflect the users' personalized preferences) when calculating 𝑨 𝑀 10 and remove the entries less than a threshold (e.g. 5) in 𝑨 𝑀 10 to keep efficient calculation. For symmetric motifs, 𝑨 𝑀 = 𝑪, and for the asymmetric ones 𝑨 𝑀 = 𝑪 + 𝑪 𝑇 . Obviously, without considering self-connection, the summation of 𝑨 𝑀 1 to 𝑨 𝑀 7 is equal to 𝑯 𝑠 𝑯 𝑠⊤ , as each entry of 𝑯 𝑠 𝑯 𝑠⊤ ∈ R 𝑚×𝑚 also indicates how many social triangles contain the node pair represented by the row and column index of the entry. Analogously, the summation of 𝑨 𝑀 8 to 𝑨 𝑀 9 is equal to 𝑯 𝑗 𝑯 𝑗⊤ without self-connection and 𝑨 𝑀 10 is equal to 𝑯 𝑝 𝑯 𝑝⊤ . Taking the calculation of 𝑨 𝑀 1 as an example, it is evident that 𝑼 𝑼 constructs a unidirectional path connecting three vertices, and the operation ⊙𝑼 makes the path a closed-loop, which is an instance of 𝑨 𝑀 1 . As 𝑨 𝑀 10 also contains the triangles in 𝑨 𝑀 8 and 𝑨 𝑀 9 . So, we remove the redundance from 𝑨 𝑀 10 . Finally, </p><formula xml:id="formula_6">u 2 u 3 u 4 s p s j i 2 u 5 u 6 s s G s G j G p MI (u 2 , s s , G s ) MI (u 2 , s j , G j ) MI (u 2 , s p , G p ) G s G j G p sum dot</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Conv</head><p>Self-Gating Self-Gating Self-Gating we use 𝑨 𝑠 = 7 𝑘=1 𝑨 𝑀 𝑘 , 𝑨 𝑗 = 𝑨 𝑀 8 + 𝑨 𝑀 9 , and 𝑨 𝑝 = 𝑨 𝑀 10 − 𝑨 𝑗 to replace 𝑯 𝑠 𝑯 𝑠⊤ , 𝑯 𝑗 𝑯 𝑗⊤ , and 𝑯 𝑝 𝑯 𝑝⊤ in Eq. ( <ref type="formula" target="#formula_2">2</ref>), respectively. Then we have a transformed hypergraph convolution, defined as:</p><formula xml:id="formula_7">𝑷 (𝑙+1) 𝑐 = D−1 𝑐 𝑨 𝑐 𝑷 (𝑙) 𝑐 ,<label>(4)</label></formula><p>where D𝑐 ∈ R 𝑚×𝑚 is the degree matrix of 𝑨 𝑐 . Obviously, Eq (2) is equivalent to Eq (4), and can be a simplified substitution of the hypergraph convolution. Since we follow the design of LightGCN which has subsumed the effect of self-connection, and thus skipping self-connection in adjacency matrix does not matter too much. In this way, we bypass the individual hyperedge construction and computation, and greatly reduce the computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Learning Comprehensive User</head><p>Representations. After propagating the user embeddings through 𝐿 layers, we average the embeddings obtained at each layer to form the final channel-specific user representation:</p><formula xml:id="formula_8">𝑷 * 𝑐 = 1 𝐿+1 𝐿 𝑙=0 𝑷 (𝑙)</formula><p>𝑐 to avoid the over-smoothing problem <ref type="bibr" target="#b12">[13]</ref>. Then we use the attention mechanism <ref type="bibr" target="#b33">[34]</ref> to aggregate three channel-specific user embeddings. For each user 𝑢, a triplet (𝛼 𝑠 , 𝛼 𝑗 , 𝛼 𝑝 ) is learned to weigh the different contributions of three channel-specific embeddings to the final recommendation performance. The attention function 𝑓 att is defined as:</p><formula xml:id="formula_9">𝒑 * = ∑︁ 𝑐 ∈ {𝑠,𝑗,𝑝 } 𝛼 𝑐 𝑙 ∑︁ 𝑙=0 𝒑 * 𝑐 , 𝛼 𝑐 = 𝑓 att (𝑝 * 𝑐 ) = exp(𝒂 ⊤ • 𝑾 𝑎𝑡𝑡 𝒑 * 𝑐 ) 𝑐 ′ ∈ {𝑠,𝑗,𝑝 } exp(𝒂 ⊤ • 𝑾 𝑎𝑡𝑡 𝒑 * 𝑐 ′ ) ,<label>(5)</label></formula><p>where 𝒂 ∈ R 𝑑 and 𝑾 𝑎𝑡𝑡 ∈ R 𝑑×𝑑 are trainable parameters. By employing this attention mechanism, we can selectively aggregate information from different channel-specific user embeddings to form the comprehensive user embeddings. Note that, since the explicit social relations are noisy and isolated relations are not a strong signal of close friendship <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>, we discard those relations which are not part of any instance of defined motifs. So, we do not have a convolution operation directly working on the explicit social network 𝑺. Besides, in our setting, the hypergraph convolution cannot directly aggregate information from the items (we do not incorporate the items into 𝑨 𝑗 and 𝑨 𝑝 ). To tackle this problem, we additionally perform simple graph convolution on the user-item interaction graph to encode the purchase information and complement the multi-channel hypergraph convolution. The simple graph convolution is defined as:</p><formula xml:id="formula_10">𝑷 (𝑙+1) 𝑟 = 𝑫 −1 𝑢 𝑹𝑸 (𝑙) , 𝑷<label>(0)</label></formula><formula xml:id="formula_11">𝑟 = 𝑓 𝑟 gate (𝑷 (0) ), 𝑸 (𝑙+1) = 𝑫 −1 𝑖 𝑹 ⊤ 𝑷 (𝑙) 𝑚 , 𝑷 (𝑙) 𝑚 = ∑︁ 𝑐 ∈ {𝑠,𝑗,𝑝 } 𝛼 𝑐 𝒑 (𝑙) 𝑐 + 𝑷 (𝑙) 𝑟 ,<label>(6)</label></formula><p>where 𝑷</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(𝑙)</head><p>𝑟 is the gated user embeddings for simple graph convolution, 𝑷 (𝑙) 𝑚 is the combination of the comprehensive user embeddings and 𝑷 (𝑙) 𝑟 , and 𝑫 𝑢 ∈ R 𝑚×𝑚 and 𝑫 𝑖 ∈ R 𝑛×𝑛 are degree matrices of 𝑹 and 𝑹 ⊤ , respectively. Finally, we obtain the final user and item embeddings 𝑷 and 𝑸 defined as:</p><formula xml:id="formula_12">𝑷 = 𝑷 * + 1 𝐿 + 1 𝐿 ∑︁ 𝑙=0 𝑷 (𝑙) 𝑟 , 𝑸 = 1 𝐿 + 1 𝐿 ∑︁ 𝑙=0 𝑸 (𝑙) ,<label>(7)</label></formula><p>where 𝑷 (0) and 𝑸 (0) are randomly initialized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Model Optimization.</head><p>To learn the parameters of MHCN, we employ the Bayesian Personalized Ranking (BPR) loss <ref type="bibr" target="#b28">[29]</ref>, which is a pairwise loss that promotes an observed entry to be ranked higher than its unobserved counterparts:</p><formula xml:id="formula_13">L 𝑟 = ∑︁ 𝑖 ∈I (𝑢),𝑗∉I (𝑢) − log 𝜎 r𝑢,𝑖 (Φ) − r𝑢,𝑗 (Φ) + 𝜆∥Φ∥ 2 2 , (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>where Φ denotes the parameters of MHCN, r𝑢,𝑖 = 𝒑 ⊤ 𝑢 𝒒 𝑖 is the predicted score of 𝑢 on 𝑖, and 𝜎 (•) here is the sigmoid function. Each time a triplet including the current user 𝑢, the positive item 𝑖 purchased by 𝑢, and the randomly sampled negative item 𝑗 which is disliked by 𝑢 or unknown to 𝑢, is fed to MHCN. The model is optimized towards ranking 𝑖 higher than 𝑗 in the recommendation list for 𝑢. In addition, 𝐿 2 regularization with the hyper-parameter 𝜆 is imposed to reduce generalized errors. Figure <ref type="figure">4</ref>: Hierarchical mutual information maximization on hypergraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Enhancing MHCN with Self-Supervised Learning</head><p>Owing to the exploitation of high-order relations, MHCN shows great performance (reported in Table <ref type="table" target="#tab_6">3 and 4</ref>). However, a shortcoming of MHCN is that the aggregation operations (Eq. 5 and 6) might lead to a loss of high-order information, as different channels would learn embeddings with varying distributions on different hypergraphs <ref type="bibr" target="#b51">[52]</ref>. Concatenating the embeddings from different channels could be the alternative, but it uniformly weighs the contributions of different types of high-order information in recommendation generation, which is not in line with the reality and leads to inferior performance in our trials. To address this issue and fully inherit the rich information in the hypergraphs, we innovatively integrate self-supervised learning into the training of MHCN.</p><p>In the scenarios of representation learning, self-supervised task usually either serves as a pretraining strategy or an auxiliary task to improve the primary task <ref type="bibr" target="#b15">[16]</ref>. In this paper, we follow the primary &amp; auxiliary paradigm, and set up a self-supervised auxiliary task to enhance the recommendation task (primary task). The recent work Deep Graph Infomax (DGI) <ref type="bibr" target="#b34">[35]</ref> is a general and popular approach for learning node within graph-structured data in a self-supervised manner. It relies on maximizing mutual information (MI) between node representations and corresponding high-level summaries of graphs. However, we consider that the graph-node MI maximization stays at a coarse level and there is no guarantee that the encoder in DGI can distill sufficient information from the input data. Therefore, with the increase of the graph scale, the benefits brought by MI maximization might diminish. For a better learning method which fits our scenario more, we inherit the merits of DGI to consider mutual information and further extend the graph-node MI maximization to a fine-grained level by exploiting the hierarchical structure in hypergraphs.</p><p>Recall that, for each channel of MHCN, we build the adjacency matrix 𝑨 𝑐 to capture the high-order connectivity information. Each row in 𝑨 𝑐 represents a subgraph of the corresponding hypergraph centering around the user denoted by the row index. Then we can induce a hierarchy: 'user node ← user-centered sub-hypergraph ← hypergraph' and create self-supervision signals from this structure. Our intuition of the self-supervised task is that the comprehensive user representation should reflect the user node's local and global high-order connectivity patterns in different hypergraphs, and this goal can be achieved by hierarchically maximizing the mutual information between representations of the user, the user-centered sub-hypergraph, and the hypergraph in each channel. The mutual information measures the structural informativeness of the suband the whole hypergraph towards inferring the user preference through the reduction in local and global structure uncertainty.</p><p>To get the sub-hypergraph representation, instead of averaging the embeddings of the users in the sub-hypergraph, we design a readout function 𝑓 out 1 : R 𝑘×𝑑 → R 𝑑 , which is permutationinvariant and formulated as:</p><formula xml:id="formula_15">𝒛 𝑐 𝑢 = 𝑓 𝑜𝑢𝑡 1 (𝑷 𝑐 , 𝒂 𝑐 𝑢 ) = 𝑷 𝑐 𝒂 𝑐 𝑢 𝑠𝑢𝑚(𝒂 𝑐 𝑢 ) ,<label>(9)</label></formula><p>where 𝑷 𝑐 = 𝑓 𝑐 gate (𝑷 ) is to control the participated magnitude of 𝑷 to avoid overfitting and mitigate gradient conflict between the primary and auxiliary tasks<ref type="foot" target="#foot_0">1</ref> , 𝒂 𝑐 𝑢 is the row vector of 𝑨 𝑐 corresponding to the center user 𝑢, and 𝑠𝑢𝑚(𝒂 𝑐 𝑢 ) denotes how many connections in the sub-hypergraph. In this way, the weight (importance) of each user in the sub-hypergraph is considered to form the sub-hypergraph embedding 𝒛 𝑢 . Analogously, we define the other readout function 𝑓 out 2 : R 𝑚×𝑑 → R 𝑑 , which is actually an average pooling to summarize the obtained sub-hypergraph embeddings into a graphlevel representation:</p><formula xml:id="formula_16">𝒉 𝑐 = 𝑓 𝑜𝑢𝑡 2 (𝒁 𝑐 ) = AveragePooling(𝒁 𝑐 ).<label>(10)</label></formula><p>We follow DGI and use InfoNCE <ref type="bibr" target="#b25">[26]</ref> as our learning objective to maximize the hierarchical mutual information. But we find that, compared with the binary cross-entropy loss, the pairwise ranking loss, which has also been proved to be effective in mutual information estimation <ref type="bibr" target="#b16">[17]</ref>, is more compatible with the recommendation task. We then define the objective function of the self-supervised task as follows:</p><formula xml:id="formula_17">L 𝑠 = − ∑︁ 𝑐 ∈ {𝑠,𝑗,𝑝 } ∑︁ 𝑢 ∈𝑈 log 𝜎 (𝑓 𝐷 (𝒑 𝑐 𝑢 , 𝒛 𝑐 𝑢 ) − 𝑓 𝐷 (𝒑 𝑐 𝑢 , z𝑐 𝑢 )) + ∑︁ 𝑢 ∈𝑈 log 𝜎 (𝑓 𝐷 (𝒛 𝑐 𝑢 , 𝒉 𝑐 ) − 𝑓 𝐷 (z 𝑐 𝑢 , 𝒉 𝑐 )) . (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>𝑓 𝐷 (•) : R 𝑑 × R 𝑑 ↦ −→ R is the discriminator function that takes two vectors as the input and then scores the agreement between them. We simply implement the discriminator as the dot product between two representations. Since there is a bijective mapping between 𝑷 𝑐 and 𝒁 𝑐 , they can be the ground truth of each other. We corrupt 𝒁 𝒄 by both row-wise and column-wise shuffling to create negative examples Z𝑐 . We consider that, the user should have a stronger connection with the sub-hypergraph centered with her (local structure), so we directly maximize the mutual information between their representations. By contrast, the user would not care all the other users too much (global structure), so we indirectly maximize the mutual information between the representations of the user and the complete hypergraph by regarding the sub-hypergraph as the mediator. Compared with DGI which only maximizes the mutual information between node and graph representations, our hierarchical design can preserve more structural information of the hypergraph into the user representations (comparison is shown in Section 4.3). Figure <ref type="figure">4</ref> illustrates the hierarchical mutual information maximization. Finally, we unify the objectives of the recommendation task (primary) and the task of maximizing hierarchical mutual information (auxiliary) for joint learning. The overall objective is defined as:</p><formula xml:id="formula_19">L = L 𝑟 + 𝛽L 𝑠 , (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>where 𝛽 is a hyper-parameter used to control the effect of the auxiliary task and L 𝑠 can be seen as a regularizer leveraging hierarchical structural information of the hypergraphs to enrich the user representations in the recommendation task for a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Complexity Analysis</head><p>In this section, we discuss the complexity of our model. Model size. The trainable parameters of our model consist of three parts: user and item embeddings, gate parameters, and attention parameters. For the first term, we only need to learn the 0 𝑡ℎ layer user embeddings 𝑷 (0) ∈ R 𝑚×𝑑 and item embeddings 𝑸 (0) ∈ R 𝑛×𝑑 . As for the second term, we employ seven gates, four for MHCN and three for the self-supervised task. Each of the gate has parameters of size (𝑑 + 1) × 𝑑, while the attention parameters are of the same size. To sum up, the model size approximates (𝑚 + 𝑛 + 8𝑑)𝑑 in total. As min(𝑚, 𝑛) ≫ 𝑑, our model is fairly light.</p><p>Time complexity. The computational cost mainly derives from four parts: hypergraph/graph convolution, attention, self-gating, and mutual information maximization. For the multi-channel hypergraph convolution through 𝐿 layers, the propagation consumption is less than O (|𝑨 + |𝑑𝐿), where |𝑨 + | denotes the number of elements in 𝑨, and here</p><formula xml:id="formula_21">|𝑨 + | = max(|𝑨 + 𝑠 |, |𝑨 + 𝑗 |, |𝑨 + 𝑝 |).</formula><p>Analogously, the time complexity of the graph convolution is O (|𝑹 + |𝑑𝐿). As for the attention and self gating mechanism, they both contribute O (𝑚𝑑<ref type="foot" target="#foot_1">2</ref> ) time complexity. The cost of mutual information maximization is mainly from 𝑓 𝑜𝑢𝑡 1 , which is O (|𝑨 + |𝑑). Since we follow the setting in <ref type="bibr" target="#b12">[13]</ref> to remove the learnable matrix for linear transformation and the nonlinear activation function, the time complexity of our model is much lower than that of previous GNNs-based social recommendation models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS</head><p>In this section, we conduct extensive experiments to validate our model. The experiments are unfolded by answering the following three questions: (1) Does MHCN outperform the state-of-the-art baselines? (2) Does each component in MHCN contribute? (3) How do the hyper-parameters (𝛽 and the depth of MHCN) influence the performance of MHCN?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Protocol</head><p>Datasets. Three real-world datasets: LastFM 2 , Douban<ref type="foot" target="#foot_2">3</ref> , and Yelp <ref type="bibr" target="#b46">[47]</ref> are used in our experiments. As our aim is to generate Top-K recommendation, for the dataset of Douban which is based on explicit ratings, we leave out ratings less than 4 and assign 1 to the rest. The statistics of the datasets is shown in Table <ref type="table" target="#tab_3">2</ref>. We perform 5-fold cross-validation on the three datasets and report the average results.</p><p>Baselines. We compare MHCN with a set of strong and commonlyused baselines including MF-based and GNN-based models:</p><p>• BPR <ref type="bibr" target="#b28">[29]</ref> is a popular recommendation model based on Bayesian personalized ranking. It models the order of candidate items by a pairwise ranking loss.</p><p>• SBPR <ref type="bibr" target="#b58">[59]</ref> is a MF based social recommendation model which extends BPR and leverages social connections to model the relative order of candidate items. • LightGCN <ref type="bibr" target="#b12">[13]</ref> is a GCN-based general recommendation model that leverages the user-item proximity to learn node representations and generate recommendations, which is reported as the state-of-the-art method. • GraphRec <ref type="bibr" target="#b8">[9]</ref> is the first GNN-based social recommendation model that models both user-item and user-user interactions.</p><p>• DiffNet++ <ref type="bibr" target="#b37">[38]</ref> is the latest GCN-based social recommendation method that models the recursive dynamic social diffusion in both the user and item spaces. • DHCF <ref type="bibr" target="#b14">[15]</ref> is a recent hypergraph convolutional network-based method that models the high-order correlations among users and items for general recommendation. Two versions of the proposed multi-channel hypergraph convolutional network are investigated in the experiments. MHCN denotes the vanilla version and 𝑺 2 -MHCN denotes the self-supervised version. Metrics. To evaluate the performance of all methods, two relevancybased metrics Precision@10 and Recall@10 and one ranking-based metric NDCG@10 are used. We perform item ranking on all the candidate items instead of the sampled item sets to calculate the values of these three metrics, which guarantees that the evaluation process is unbiased. Settings. For a fair comparison, we refer to the best parameter settings reported in the original papers of the baselines and then use grid search to fine tune all the hyperparameters of the baselines to ensure the best performance of them. For the general settings of all the models, the dimension of latent factors (embeddings) is empirically set to 50, the regularization coefficient 𝜆 = 0.001, and the batch size is set to 2000. We use Adam to optimize all these models. Section 4.4 reports the influence of different parameters (i.e.</p><p>𝛽 and the depth) of MHCN, and we use the best parameter settings in Section 4.2, and 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Recommendation Performance</head><p>In this part, we validate if MHCN outperforms existing social recommendation baselines. Since the primary goal of social recommendation is to mitigate data sparsity issue and improve the recommendation performance for cold-start users. Therefore, we respectively • Although DHCF is also based on hypergraph convolution, it does not show any competence in all the cases. We are unable to reproduce its superiority reported in the original paper <ref type="bibr" target="#b14">[15]</ref>. There are two possible causes which might lead to its failure. Firstly, it only exploits the user-item high-order relations. Secondly, the way to construct hyperedges is very impractical in this model, which leads to a very dense incidence matrix. The model would then encounter the over-smoothing problem and heavy computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we conduct an ablation study to validate if each component positively contributes to the final recommendation performance.</p><p>Pr ec @ 10 R ec @ 10 N D C G @ 10 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Investigation of Multi-Channel</head><p>Setting. We first investigate the multi-channel setting by removing any of the three channels from 𝑆 2 -MHCN and leaving the other two to observe the changes of performance. Each bar in the plots (except complete) represents the case that the corresponding channel is removed, while complete means no module has been removed. From Fig. <ref type="figure">5</ref>, we can observe that removing any channel would cause performance degradation. But it is obvious that purchase channel contributes the most to the final performance. Without this channel, 𝑆 2 -MHCN falls to the level of LightGCN shown in Table <ref type="table" target="#tab_4">3</ref>. By contrast, removing Social channel or Joint channel would not have such a large impact on the final performance. Comparing Social channel with Joint channel, we can observe that the former contributes slightly more on LastFM and Yelp, while the latter, in terms of the performance contribution, is more important on Douban.</p><p>To further investigate the contribution of each channel when they are all employed, we visualize the attention scores learned along with other model parameters, and draw a box plot to display the distributions of the attention weights. According to Fig. <ref type="figure">6</ref>, we can observe that, for the large majority of users in LastFM, Social channel has limited influence on the comprehensive user representations. In line with the conclusions from Fig. <ref type="figure">5</ref>, Purchase channel plays the most important role in shaping the comprehensive user representations. The importance of Joint channel falls between the other two. The possible reason could be that, social relations are usually noisy and the users who are only socially connected might not always share similar preferences.</p><p>4.3.2 Investigation of Self-supervised Task. To investigate the effectiveness of the hierarchical mutual information maximization (MIM), we break this procedure into two parts: local MIM between the user and user-centered sub-hypergraph, and global MIM between the user-centered sub-hypergraph and hypergraph. We then run MHCN with either of these two to observe the performance changes. We also compare hierarchical MIM with the node-graph MIM used in DGI to validate the rationality of our design. We implement DGI by referring to the original paper <ref type="bibr" target="#b34">[35]</ref>. The results are illustrated in Fig. <ref type="figure" target="#fig_4">7</ref>, and we use Disabled to denote the vanilla MHCN. Unlike the bars in Fig. <ref type="figure">6</ref>, each bar in Fig. <ref type="figure" target="#fig_4">7</ref> represents the case where only the corresponding module is used. As can be seen, hierarchical MIM shows the best performance while local MIM achieves the second best performance. By contrast, global MIM contributes less but it still shows better performance on Douban Yelp when compared with DGI. Actually, DGI almost rarely contributes on the latter two datasets and we can hardly find a proper parameter that can make it compatible with our task. On some metrics, training MHCN with DGI even lowers the performance.</p><p>According to these results, we can draw a conclusion that the selfsupervised task is effective and our intuition for hierarchical mutual information maximization is more reasonable compared with the node-graph MIM in DGI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Sensitivity Analysis</head><p>In this section, we investigate the sensitivity of 𝛽 and 𝐿.</p><p>As we adopt the primary &amp; auxiliary paradigm, to avoid the negative interference from the auxiliary task in gradient propagating, we can only choose small values for 𝛽. We search the proper value in a small interval and empirically set it from 0.001 to 0.5. We then start our attempts from 0.001, and proceed by gradually increasing the step size. Here we report the performance of 𝑆 2 -MHCN with eight representative 𝛽 values {0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.5}. As can be seen in Fig. <ref type="figure">8</ref>, with the increase of the value of 𝛽, the performance of 𝑆 2 -MHCN on all the datasets rises. After reaching the peak when 𝛽 is 0.01 on all the datasets, it steadily declines. According to Fig. <ref type="figure">8</ref>, we can draw a conclusion that even a very small 𝛽 can promote the recommendation task, while a larger 𝛽 would mislead it. The benefits brought by the self-supervised task could be easily neutralized and the recommendation task is sensitive to the magnitude of self-supervised task. So, choosing a small value is more likely to facilitate the primary task when there is little prior knowledge about the data distribution. Finally, we investigate the influence of 𝐿 to find the optimal depth for 𝑆 2 -MHCN. We stack hypergraph convolutional layers from 1layer to 5-layer setting. According to Fig. <ref type="figure">9</ref>, the best performance of 𝑆 2 -MHCN is achieved when the depth of 𝑆 2 -MHCN is 2. With the continuing increase of the number of layer, the performance of 𝑆 2 -MHCN declines on all the datasets. Obviously, a shallow structure fits 𝑆 2 -MHCN more. A possible reason is that 𝑆 2 -MHCN aggregates high-order information from distant neighbors. As a result, it is more prone to encounter the over-smoothing problem with the increase of depth. This problem is also found in DHCF <ref type="bibr" target="#b14">[15]</ref>, which is based on hypergraph modeling as well. Considering the over-smoothed representations could be a pervasive problem in hypergraph convolutional network based models, we will work against it in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Recently, GNN-based recommendation models have achieved great success in social recommendation. However, these methods simply model the user relations in social recommender systems as pairwise interactions, and neglect that real-world user interactions can be high-order. Hypergraph provides a natural way to model high-order user relations, and its potential for social recommendation has not been fully exploited. In this paper,we fuse hypergraph modeling and graph neural networks and then propose a multichannel hypergraph convolutional network (MHCN) which works on multiple motif-induced hypergraphs to improve social recommendation. To compensate for the aggregating loss in MHCN, we innovatively integrate self-supervised learning into the training of MHCN. The self-supervised task serves as the auxiliary task to improve the recommendation task by maximizing hierarchical mutual information between the user, user-centered sub-hypergraph, and hypergraph representations. The extensive experiments conducted on three public datasets verify the effectiveness of each component of MHCN, and also demonstrate its state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Triangle motifs used in our work. The green circles denote users and the yellow circles denote items.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An overview of the proposed model(1-layer). Each triangle in the left graph is a hyperedge and also an instance of defined motifs. 𝐺 𝑠 , 𝐺 𝑗 and 𝐺 𝑝 denote the three motif-induced hypergraphs constructed based on social, joint, and purchase motifs, respectively. 𝑠 𝑠 , 𝑠 𝑗 , and 𝑠 𝑝 in the three dotted ellipses denote three ego-networks with 𝑢 2 as the center, which are subgraphs of 𝐺 𝑠 , 𝐺 𝑗 and 𝐺 𝑝 , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Contributions of each channel on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Investigation of Hierarchical Mutual Information Maximization on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Influence of the magnitude of hierarchical MIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>to remove the learnable matrix for linear transformation and the nonlinear activation function (e.g. leaky ReLU). By replacing 𝑯 𝑐 with any of 𝑯 𝑠 , 𝑯 𝑗 and 𝑯 𝑝 , we can borrow the strengths of hypergraph convolutional networks to learn user representations encoded high-order information in the corresponding channel. As 𝑫 𝑐 and 𝑳 𝑐 are diago-</figDesc><table><row><cell cols="2">nal matrices which only re-scale embeddings, we skip them in the</cell></row><row><cell cols="2">following discussion. The hypergraph convolution can be viewed</cell></row><row><cell cols="2">as a two-stage refinement performing 'node-hyperedge-node' fea-</cell></row><row><cell cols="2">ture transformation upon hypergraph structure. The multiplication</cell></row><row><cell>operation 𝑯 ⊤ 𝑐 𝑷</cell><cell>(𝑙)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Dataset Statistics</figDesc><table><row><cell cols="5">Dataset #User #Item #Feedback #Relation Density</cell></row><row><cell>LastFM</cell><cell>1,892 17,632</cell><cell>92,834</cell><cell>25,434</cell><cell>0.28%</cell></row><row><cell>Douban</cell><cell>2,848 39,586</cell><cell>894,887</cell><cell>35,770</cell><cell>0.79%</cell></row><row><cell>Yelp</cell><cell>19,539 21,266</cell><cell>450,884</cell><cell>363,672</cell><cell>0.11%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>General recommendation performance comparison. MHCN 𝑺 2 -MHCN Improv. 𝑺 2 -Improv.</figDesc><table><row><cell cols="8">Dataset Metric GraphRec P@10 17.385% DiffNet++ DHCF LightGCN LastFM BPR SBPR 15.606% 16.491% 18.485% 16.877% 19.205% R@10 18.020% 15.821% 16.703% 18.737% 17.131% 19.480%</cell><cell>19.625% 19.945%</cell><cell>20.052% 20.375%</cell><cell>4.410% 4.594%</cell><cell>2.175% 2.155%</cell></row><row><cell></cell><cell>N@10</cell><cell>0.21173</cell><cell cols="2">0.18953 0.20216</cell><cell>0.22310</cell><cell>0.20744</cell><cell>0.23392</cell><cell>0.23834</cell><cell>0.24395</cell><cell>4.287%</cell><cell>2.156%</cell></row><row><cell></cell><cell>P@10</cell><cell>17.021%</cell><cell cols="2">15.673% 15.993%</cell><cell>17.532%</cell><cell>16.871%</cell><cell>17.780%</cell><cell>18.283%</cell><cell>18.506%</cell><cell>4.083%</cell><cell>1.220%</cell></row><row><cell>Douban</cell><cell>R@10</cell><cell>5.916%</cell><cell>5.160%</cell><cell>5.322%</cell><cell>6.205%</cell><cell>5.755%</cell><cell>6.247%</cell><cell>6.556%</cell><cell>6.681%</cell><cell>6.947%</cell><cell>1.906%</cell></row><row><cell></cell><cell>N@10</cell><cell>0.19051</cell><cell cols="2">0.17476 0.17821</cell><cell>0.19701</cell><cell>0.18655</cell><cell>0.19881</cell><cell>0.20694</cell><cell>0.21038</cell><cell>5.819%</cell><cell>1.662%</cell></row><row><cell></cell><cell>P@10</cell><cell>2.323%</cell><cell>2.002%</cell><cell>2.192%</cell><cell>2.480%</cell><cell>2.298%</cell><cell>2.586%</cell><cell>2.751%</cell><cell>3.003%</cell><cell>16.125%</cell><cell>9.160%</cell></row><row><cell>Yelp</cell><cell>R@10</cell><cell>6.075%</cell><cell>5.173%</cell><cell>5.468%</cell><cell>6.354%</cell><cell>5.986%</cell><cell>6.525%</cell><cell>6.862%</cell><cell>7.885%</cell><cell>17.247%</cell><cell>14.908%</cell></row><row><cell></cell><cell>N@10</cell><cell>0.04653</cell><cell cols="2">0.03840 0.04314</cell><cell>0.04833</cell><cell>0.04700</cell><cell>0.04998</cell><cell>0.05356</cell><cell>0.06061</cell><cell>21.268%</cell><cell>13.162%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Cold-start recommendation performance comparison.</figDesc><table><row><cell cols="3">Dataset Metric GraphRec</cell><cell>BPR</cell><cell>SBPR</cell><cell cols="7">DiffNet++ DHCF LightGCN MHCN 𝑺 2 -MHCN Improv. 𝑺 2 -Improv.</cell></row><row><cell></cell><cell>P@10</cell><cell>4.662%</cell><cell>3.784%</cell><cell>4.573%</cell><cell>5.102%</cell><cell>3.974%</cell><cell>4.809%</cell><cell>5.466%</cell><cell>5.759%</cell><cell>12.877%</cell><cell>5.360%</cell></row><row><cell>LastFM</cell><cell>R@10</cell><cell>18.033%</cell><cell cols="2">15.240% 18.417%</cell><cell>21.365%</cell><cell>16.395%</cell><cell>20.361%</cell><cell>23.354%</cell><cell>24.431%</cell><cell>14.350%</cell><cell>4.611%</cell></row><row><cell></cell><cell>N@10</cell><cell>0.14675</cell><cell cols="2">0.12460 0.15141</cell><cell>0.16031</cell><cell>0.14285</cell><cell>0.15044</cell><cell>0.17218</cell><cell>0.19138</cell><cell>19.381%</cell><cell>11.151%</cell></row><row><cell></cell><cell>P@10</cell><cell>2.007%</cell><cell>1.722%</cell><cell>1.935%</cell><cell>2.230%</cell><cell>1.921%</cell><cell>2.134%</cell><cell>2.343%</cell><cell>2.393%</cell><cell>7.309%</cell><cell>2.133%</cell></row><row><cell>Douban</cell><cell>R@10</cell><cell>8.215%</cell><cell>7.178%</cell><cell>8.084%</cell><cell>8.705%</cell><cell>7.977%</cell><cell>8.317%</cell><cell>9.646%</cell><cell>10.632%</cell><cell>22.136%</cell><cell>10.227%</cell></row><row><cell></cell><cell>N@10</cell><cell>0.05887</cell><cell cols="2">0.04784 0.05716</cell><cell>0.06767</cell><cell>0.05533</cell><cell>0.06037</cell><cell>0.06771</cell><cell>0.07113</cell><cell>5.113%</cell><cell>5.052%</cell></row><row><cell></cell><cell>P@10</cell><cell>1.355%</cell><cell>1.232%</cell><cell>1.286%</cell><cell>1.475%</cell><cell>1.314%</cell><cell>1.504%</cell><cell>1.545%</cell><cell>1.747%</cell><cell>14.108%</cell><cell>13.074%</cell></row><row><cell>Yelp</cell><cell>R@10</cell><cell>5.901%</cell><cell>5.468%</cell><cell>5.720%</cell><cell>6.635%</cell><cell>5.876%</cell><cell>6.753%</cell><cell>6.838%</cell><cell>7.881%</cell><cell>12.264%</cell><cell>15.253%</cell></row><row><cell></cell><cell>N@10</cell><cell>0.03896</cell><cell cols="2">0.03448 0.03671</cell><cell>0.04237</cell><cell>0.03826</cell><cell>0.04273</cell><cell>0.04354</cell><cell>0.05143</cell><cell>15.703%</cell><cell>18.121%</cell></row><row><cell cols="6">conduct experiments on the complete test set and the cold-start</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">test set in which only the cold-start users with less than 20 interac-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">tions are contained. The experimental results are shown in Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3 and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>The improvement is calculated by subtracting the best performance value of the baselines from that of 𝑆 2 -MHCN and then using the difference to divide the former. Analogously, 𝑆 2 -improvement is calculated by comparing the values of the performance of MHCN and and 𝑺 2 -MHCN. According to the results, we can draw the following conclusions:</figDesc><table><row><cell></cell><cell>block (i.e. MF-based vs. MF-based, GNNs-based vs. GNNs-based),</cell></row><row><cell></cell><cell>social recommendation models are still competitive and by and</cell></row><row><cell></cell><cell>large outperform the corresponding general recommendation</cell></row><row><cell></cell><cell>models except LightGCN.</cell></row><row><cell></cell><cell>• LightGCN is a very strong baseline. Without considering the two</cell></row><row><cell></cell><cell>variants of MHCN, LightGCN shows the best or the second best</cell></row><row><cell></cell><cell>performance in most cases. This can be owed to the removal</cell></row><row><cell></cell><cell>of the redundant operations including the nonlinear activation</cell></row><row><cell></cell><cell>function and transformation matrices. The other baselines such</cell></row><row><cell>• MHCN shows great performance in both the general and cold-start recommendation tasks. Even without self-supervised learn-ing, it beats all the baselines by a fair margin. Meanwhile, self-</cell><cell>as GraphRec might be limited by these useless operations, and fail to outperform LightGCN, though the social information is incorporated.</cell></row><row><cell>supervised learning has great ability to further improve MHCN.</cell><cell></cell></row><row><cell>Compared with the vanilla version, the self-supervised version</cell><cell></cell></row><row><cell>shows decent improvements in all the cases. Particularly, in the</cell><cell></cell></row><row><cell>cold-start recommendation task, self-supervised learning brings</cell><cell></cell></row><row><cell>significant gains. On average, 𝑺 2 -MHCN achieves about 5.389%</cell><cell></cell></row><row><cell>improvement in the general recommendation task and 9.442%</cell><cell></cell></row><row><cell>improvement in the cold-start recommendation task compared</cell><cell></cell></row><row><cell>with MHCN. Besides, it seems that, the sparser the dataset, the</cell><cell></cell></row><row><cell>more improvements self-supervised learning brings.</cell><cell></cell></row><row><cell>• GNN-based recommendation models significantly outperform</cell><cell></cell></row><row><cell>the MF-based recommendation models. Even the general rec-</cell><cell></cell></row><row><cell>ommendation models based on GNNs show much better perfor-</cell><cell></cell></row><row><cell>mance than MF-based social recommendation models. However,</cell><cell></cell></row><row><cell>when compared with the counterparts based on the same building</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Self-supervised task has its own parameters of self-gating, and does not share the same gate parameters with MHCN.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://files.grouplens.org/datasets/hetrec2011/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">http://smiles.xjtu.edu.cn/Download/download_Douban.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was supported by ARC Discovery Project (Grant No. DP190101985 and DP170103954). Jundong Li is supported by National Science Foundation (NSF) under grant No. 2006844.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torr</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08150</idno>
		<title level="m">Hypergraph convolution and hypergraph attention</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Higher-order organization of complex networks</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Austin R Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Gleich</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="page" from="163" to="166" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hypergraph theory. An introduction. Mathematical Engineering</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>Bretto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Music recommendation by unified hypergraph: combining social media information and music content</title>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shulong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
				<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="391" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Social influence: Compliance and conformity</title>
		<author>
			<persName><forename type="first">B</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">J</forename><surname>Cialdini</surname></persName>
		</author>
		<author>
			<persName><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="591" to="621" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hypergraph neural networks</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3558" to="3565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02474</idno>
		<title level="m">Recommender Systems Based on Generative Adversarial Networks: A Problem-Driven Perspective</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TrustSVD: collaborative filtering with both the explicit and implicit influence of user trust and of item ratings</title>
		<author>
			<persName><forename type="first">Guibing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Yorke-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="123" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual Channel Hypergraph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Shuyi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanwan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2020" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10141</idno>
		<title level="m">Self-supervised learning on graphs: Deep insights and new direction</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">RankMI: A Mutual Information Maximizing Ranking Loss</title>
		<author>
			<persName><forename type="first">Mete</forename><surname>Kemertas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leila</forename><surname>Pishdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afsaneh</forename><surname>Fazly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="14362" to="14371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Heterogeneous Graph Neural Model for Cold-Start Recommendation</title>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiqiao</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2029" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to recommend with social trust ensemble</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><surname>Michael R Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to recommend with trust and distrust relationships</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Recommender Systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sorec: social recommendation using probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM conference on Information and knowledge management</title>
				<meeting>the 17th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="931" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>Michael R Lyu</surname></persName>
		</author>
		<author>
			<persName><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM international conference on Web search and data mining</title>
				<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Disentangled Self-Supervision in Sequential Recommenders</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="483" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Miller Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Smithlovin</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalev</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph Representation Learning via Graphical Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence</title>
				<meeting>the twenty-fifth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hari</forename><surname>Sundaram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03736</idno>
		<title level="m">GroupIM: A Mutual Information Maximization Framework for Neural Group Recommendation</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Sparsity, scalability, and distribution in recommender systems</title>
		<author>
			<persName><forename type="first">Badrul</forename><surname>Munir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarwar</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Session-based social recommendation via dynamic graph attention networks</title>
		<author>
			<persName><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
				<meeting>the Twelfth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="555" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Multi-Stage Self-Supervised Learning for Graph Convolutional Networks on Graphs with Few Labels</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.11038</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Next-item Recommendation with Sequential Hypergraphs</title>
		<author>
			<persName><forename type="first">Jianling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1101" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling the evolution of users&apos; preferences and social links in social networking services</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junping</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00844</idno>
		<title level="m">DiffNet++: A Neural Influence and Interest Diffusion Network for Social Recommendation</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR abs/1904.10322</idno>
		<title level="m">A Neural Influence Diffusion Model for Social Recommendation</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dual graph attention networks for deep latent representation of multifaceted social effects in recommender systems</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2091" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Self-Supervised Hypergraph Convolutional Networks for Sessionbased Recommendation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06852</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Self-Supervised Reinforcement Learning forRecommender Systems</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Arapakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joemon M</forename><surname>Jose</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05779</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Social collaborative filtering by trust</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1633" to="1647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Revisiting user mobility and social relationships in lbsns: a hypergraph embedding approach</title>
		<author>
			<persName><forename type="first">Dingqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingqing</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Cudre-Mauroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2147" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Discovering interpretable geo-social communities for user behavior prediction</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hongzhi Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 32nd International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="942" to="953" />
		</imprint>
	</monogr>
	<note>Data Engineering (ICDE)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Social influence-based group representation learning for group recommendation</title>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Hongzhi Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE 35th International Conference on Data Engineering (ICDE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="566" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Overcoming Data Sparsity in Group Recommendation</title>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Hongzhi Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adapting to User Interest Drift for POI Recommendation</title>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Hongzhi Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2566" to="2581" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Joint event-partner recommendation in event-based social networks</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hongzhi Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 34th International Conference on Data Engineering (ICDE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="929" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning with Augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mixed pooling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Dingjun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiqiu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on rough sets and knowledge technology</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="364" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adaptive Implicit Friends Identification over Heterogeneous Network for Social Recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="357" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hybrid attacks on model-based social recommender systems</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenge</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhao</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">483</biblScope>
			<biblScope unit="page" from="171" to="181" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Generating reliable friends via adversarial training to improve social recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Data Mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="768" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Enhance Social Recommendation with Adversarial Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02340</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Ranking Users in Social Networks With Higher-Order Structures</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="232" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Leveraging social connections to improve personalized ranking for collaborative filtering</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management</title>
				<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A novel social network hybrid recommender system based on hypergraph topologic structure</title>
		<author>
			<persName><forename type="first">Xiaoyao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="985" to="1013" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07873</idno>
		<title level="m">Sˆ3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
