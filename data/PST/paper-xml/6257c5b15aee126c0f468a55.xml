<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPARSE ATTENTION WITH LEARNING-TO-HASH</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
							<email>zhiqings@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
							<email>yiming@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shinjae</forename><surname>Yoo</surname></persName>
							<email>sjyoo@bnl.gov</email>
							<affiliation key="aff1">
								<orgName type="institution">Brookhaven National Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SPARSE ATTENTION WITH LEARNING-TO-HASH</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those ANN methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the ANN methods such as Locality-Sensitive Hashing (LSH) are randomized and cannot fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely LHA (Learningto-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of LHA is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre-trained Transformer models. Our experiments on evaluation of the WikiText-103 dataset for language modeling, the GLUE benchmark for natural language understanding, and the Lang-Range-Arena benchmark for multiple tasks (text/image classification, retrieval, etc.) show the superior performance of LHA over other strong Transformer variants.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The Transformer architecture <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> has been successfully applied to various tasks, including natural language processing <ref type="bibr" target="#b34">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b9">Devlin et al., 2018;</ref><ref type="bibr" target="#b7">Dai et al., 2019;</ref><ref type="bibr" target="#b16">Liu et al., 2019;</ref><ref type="bibr" target="#b43">Yang et al., 2019)</ref>, computer vision <ref type="bibr" target="#b3">(Carion et al., 2020;</ref><ref type="bibr" target="#b10">Dosovitskiy et al., 2020)</ref>, and time series forecasting <ref type="bibr" target="#b46">(Zhou et al., 2020;</ref><ref type="bibr" target="#b41">Wu et al., 2021)</ref>. Such a success is mainly due to the self-attention component, which enables each token to directly interact with any other tokens in the entire sequence. But self-attention has a quadratic time and space complexity with respect to the sequence length and hence does not scale efficiently to long sequences as a result.</p><p>To address this inefficiency problem, one of the solutions is to approximate the full attention matrix with a sparse one, as the softmax operation is dominated by the largest elements. Some recent efforts focus on dynamic learning of sparse attention patterns via Approximate Nearest Neighbors (ANN) approaches, including Locality Sensitive Hashing (LSH) <ref type="bibr" target="#b13">(Kitaev et al., 2020;</ref><ref type="bibr" target="#b8">Daras et al., 2020)</ref> and mini-batch spherical k-means <ref type="bibr" target="#b28">(Roy et al., 2021;</ref><ref type="bibr" target="#b38">Wang et al., 2020a)</ref>. The queries and keys are hashed or clustered into different buckets, hoping that the queries and keys in the same bucket are similar with a high probability. The effectiveness of those ANN approaches rely on the assumption that the (transformed) query and key vectors should lie in the same space, which could be sub-optimal in dealing with different sparsity patterns, as analyzed in this paper (Section 3.2). Besides, the hash functions in LSH are randomized and data-agnostic, which cannot fully utilize the rich information in real-world data distributions.</p><p>In this paper, we address the above limitations of existing ANN-based methods for attention sparsification. Firstly, we analyze two imbalance issues in LSH-produced sparse attention patterns, i.e., unbalanced hash bucket sizes and unbalanced query-key ratios. Secondly, we design a new metric called attention utility to quantify how well the sparse patterns approximate the full attention, and we show that ANN-derived sparse patterns are substantially inferior to their counterparts. Thirdly, we propose a novel solution, namely Learning-to-Hash Attention (LHA), for dynamic attention sparsification with enhanced model expressiveness. LHA directly optimizes our newly defined attention utility metric in an end-to-end manner via separate learnable hash functions for queries and keys, respectively. As for reducing the computational complexity in the training phase, LHA uses unbiased kernelized attention techniques <ref type="bibr" target="#b5">(Choromanski et al., 2020;</ref><ref type="bibr" target="#b24">Peng et al., 2021)</ref> to efficiently approximate the attention utilities. Similar to other sparse attention models <ref type="bibr" target="#b13">(Kitaev et al., 2020;</ref><ref type="bibr" target="#b28">Roy et al., 2021)</ref>, LHA reduces the overall complexity of self-attention from O(N 2 ) to O(N 1.5 ) for sequence length N . Our experiments in a wide range of tasks on the evaluation benchmarks for language modeling, natural language understanding, and Long-Range-Arena show that LHA achieves better performance compared to strong transformer baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Related work can be roughly divided into three categories, i.e., location-based sparse attention, content-based sparse attention, and dense approximation of attention, as outlined below.</p><p>Location-based sparse attention methods aim to improve the computational efficiency by using pre-specified global or local sparsification patterns over token locations. <ref type="bibr" target="#b15">Liu et al. (2018)</ref> proposed to alternate coarse attention layers and local attention layers. <ref type="bibr" target="#b4">Child et al. (2019)</ref> used a strided sparse attention pattern in image generation. <ref type="bibr" target="#b29">Sukhbaatar et al. (2019)</ref> imposed sparsity based on the predicted temporal window size for each token. Other methods <ref type="bibr" target="#b45">(Zhang et al., 2021;</ref><ref type="bibr" target="#b1">Beltagy et al., 2020;</ref><ref type="bibr" target="#b0">Ainslie et al., 2020;</ref><ref type="bibr" target="#b44">Zaheer et al., 2020)</ref> used a pre-specified subset of locations in the input as the global memory, and only allow non-local attentions from this subset to all the other tokens. Location-based sparse attention cannot leverage more flexible content-based interactions among arbitrary positions, as a limitation.</p><p>Content-based sparse attention allows more flexible sparse patterns than location-based ones. <ref type="bibr" target="#b20">Malaviya et al. (2018)</ref> used sparsemax to obtain a sparse attention matrix, while <ref type="bibr" target="#b6">(Correia et al., 2019)</ref> used entmax. These methods require to compute the full attention matrix before sparsification and hence cannot reduce the quadratic computation complexity. Approximate Nearest Neighbor (ANN) methods address this limitation by calculating the content-based sparse patterns in advance <ref type="bibr" target="#b28">(Roy et al., 2021;</ref><ref type="bibr" target="#b13">Kitaev et al., 2020;</ref><ref type="bibr" target="#b8">Daras et al., 2020;</ref><ref type="bibr" target="#b38">Wang et al., 2020a</ref>) (which will be discussed more in Section 3.1). Those methods usually apply an ANN module as the shared hash function to both queries and keys. <ref type="bibr" target="#b35">Vyas et al. (2020)</ref> and <ref type="bibr" target="#b46">Zhou et al. (2020)</ref> sparsified the attention maps by eliminating redundant queries. <ref type="bibr" target="#b31">Tay et al. (2020b)</ref> designed a differentiable sorting algorithm of internal representations to enable efficient quasi-global local attention. Contemporary to our work, SparseFinder <ref type="bibr" target="#b33">(Treviso et al., 2021)</ref> learns sparse attention patterns that approximate entmax attention, but its bucketing strategies are still based on ANN approaches. In contrast, LHA directly predicts a bucketing strategy (i.e., learnable hash functions) that maximizes the attention utility.</p><p>Another line of research explored low-rank or kernelized dense approximation of attention matrices, instead of computing the attention scores exactly for only a few pairs. <ref type="bibr" target="#b39">Wang et al. (2020b)</ref> applied a low-rank decomposition to the attention matrix. <ref type="bibr" target="#b42">Xiong et al. (2021)</ref> approximated the attention matrix with Nyström approximation. <ref type="bibr" target="#b12">Katharopoulos et al. (2020)</ref> utilized the association property of Key-Query-Value multiplication and reduce the quadratic complexity to linear complexity with kernelized approximation to the softmax operation. <ref type="bibr" target="#b5">Choromanski et al. (2020)</ref> and <ref type="bibr" target="#b24">Peng et al. (2021)</ref> further proposed an unbiased approximation of softmax with random Fourier features.</p><p>Our work in this paper is directly related to the second category, i.e., content-based attention sparsification. Specially, we address the limitations of existing ANN-based methods by modeling queries and keys in separate vector spaces and by proposing a novel approach to learn the hash functions for attention sparsification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RE-EXAMINATION OF CONTENT-BASED SPARSE PATTERNS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">PRELIMINARY</head><p>The self-attention mechanism <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> can be formulated as the weighted sum of the value vectors V ∈ R N ×d h where the weights are calculated using query vectors Q ∈ R N ×d h and key vectors K ∈ R N ×d h as:</p><formula xml:id="formula_0">Attention(Q, K, V ) = A • V = softmax QK T √ d h • V,<label>(1)</label></formula><p>where A denotes the matrix of normalized attention weights, d h is the dimension of hidden representations, and N is the sequence length. self-attention has a quadratic time and space complexity with respect to the sequence length and hence does not scale efficiently to long sequences.</p><p>Content-based sparse attention methods usually apply randomized hash functions or a clustering algorithm to queries {Q i } and keys {K j }, and hope that similar queries and keys are hashed or clustered into the same bucket. The queries can thus only attend to the keys if both are in the same bucket. Formally, a content-based sparse attention strategy with B hash buckets is defined as:</p><formula xml:id="formula_1">Sparse-Attention(Q i , K, V ) = j:h Q (Qi)=h K (Kj ) Āij V j ,<label>(2)</label></formula><p>where h K , h Q : R d h → [B] are the hash functions for keys and queries, and Āij ∝ A ij is the re-normalized attention weights such that ∀i, j:h Q (Qi)=h K (Kj ) Āij = 1. In general <ref type="bibr" target="#b13">(Kitaev et al., 2020;</ref><ref type="bibr" target="#b28">Roy et al., 2021)</ref>, calculating the hash function and performing local attention for each query have the time complexity of O(B) and O(N/B), respectively. Thus, the overall complexity<ref type="foot" target="#foot_0">1</ref> of self-attention can be reduced from</p><formula xml:id="formula_2">O(N 2 ) to O(N •B+N 2 /B) ≈ O(N 1.5 ) when B ≈ N/B ≈ √ N .</formula><p>Since the hash functions are not differentiable, Approximate Nearest Neighbor (ANN) methods are used to derive an effective content-based hash function. Reformer <ref type="bibr" target="#b13">(Kitaev et al., 2020)</ref> applies Locality Sensitive Hashing (LSH) to the tied queries and keys, where several hyper-planes are randomly generated to divide tokens into different buckets. SMYRF <ref type="bibr" target="#b8">(Daras et al., 2020)</ref> improves Reformer by introducing asymmetric transformation to queries and keys, i.e.,</p><formula xml:id="formula_3">F (Q i ) = Q i ; 0; M 2 Q + M 2 K − ||Q i || 2 2 , G(K j ) = K j ; M 2 Q + M 2 K − ||K j || 2 2 ; 0 ,<label>(3)</label></formula><p>where <ref type="bibr" target="#b28">(Roy et al., 2021)</ref> and Cluster-former <ref type="bibr" target="#b38">(Wang et al., 2020a</ref>) use mini-batch spherical k-means to partition tokens into different clusters.</p><formula xml:id="formula_4">M Q = max Qi ||Q i || 2 and M K = max Kj ||K j || 2 , such that ||F (Q i ) − G(K j )|| 2 2 = const − Q i • K j . Routing Transformer</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">BUCKET IMBALANCE ISSUES</head><p>Previous content-based sparse attention models take it for granted that the ANN-derived sparse pattern can effectively approximate the full attention. However, it is only verified via empirical evaluation on the down-stream tasks yet, which cannot reflect the true attention map approximation ability. Notice that there are two necessary conditions for sparse attention to work effectively and efficiently:</p><p>1. The number of queries and the number of keys in each bucket should be reasonably balanced, as queries should attend to enough keys to get a good approximation of the full-attention;</p><p>2. The bucket sizes should be nearly equal in order to effectively reduce the overall complexity.</p><p>We first analyze how badly the two conditions would be violated by LSH<ref type="foot" target="#foot_1">2</ref> , a typical ANN method. We apply LSH to 10 attention heads in the 3 rd layer of a Transformer<ref type="foot" target="#foot_2">3</ref> pre-trained on language modeling and obtain the results shown in Figure <ref type="figure" target="#fig_0">1</ref> (up). We can see that the imbalance issue not only exists in the query-key ratios, but also in the bucket sizes. To go a step further, we apply LSH to all 16 × 10 = 160 attention heads in the pre-trained Transformer and find that around 61.3% buckets have the query-key imbalance problem, where the query-key ratios are either greater than 2:1 or smaller than 1:2. Around 35.9% buckets have the bucket size imbalance problem, where the bucket sizes are twice greater than half smaller than the expected bucket size of 512. Clearly, neither of the two aforementioned conditions are well satisfied in this LSH-sparsified Transformer model.</p><p>There can be several possible reasons that cause the imbalance problem: 1) the Euclidean distance metric in ANN methods does not monotonically decrease with the dot-product metric used in attention mechanism; 2) the queries and keys are from different distribution and not normalized, and thus restricts the effectiveness of ANN methods. To investigate the root cause, we apply the SMYRF asymmetric transformation (Equation <ref type="formula" target="#formula_3">3</ref>) to queries and keys, which creates a monotonic relation between Euclidean distances and dot-products. The new analysis results are shown in Figure <ref type="figure" target="#fig_0">1</ref> (down).</p><p>We can see that the asymmetric transformation would only exacerbate the imbalance problem, with respect to both query-key ratios and bucket sizes. Therefore, we can conclude that the root cause of the imbalance problem is the mismatch between the query and key distributions, that could be further magnified by the asymmetric transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PROPOSED METRIC: ATTENTION UTILITY</head><p>The above analysis shows the imbalance problem in ANN-derived hashing strategies. To further quantify the approximation quality of sparse attention patterns, we utilize the concept of Attention Biclustering from <ref type="bibr" target="#b8">(Daras et al., 2020)</ref>, which considers a practical case <ref type="bibr" target="#b13">(Kitaev et al., 2020;</ref><ref type="bibr" target="#b28">Roy et al., 2021)</ref> where all clusters strictly contain the same number of queries and keys:</p><formula xml:id="formula_5">∀b ∈ [B], |{Q i | h Q (Q i ) = b}| = |{K j | h K (K j ) = b}| = N B .<label>(4)</label></formula><p>We denote with C the set of all possible assignments in B balanced non-overlapping clusters:</p><formula xml:id="formula_6">C = {C 1 , C 2 , . . . , C T },</formula><p>where T is the number of possible assignments. We can then define the attention utility AU of each assignment C t as:</p><formula xml:id="formula_7">AU(C t ) = i,j:(Qi,Kj )∈Ct A ij , where C t = {(Q i , K j )|h Ct Q (Q i ) = h Ct K (K j )}.<label>(5)</label></formula><p>The attention utility represents the aggregation of the sparse attention weights in an assignment. As we should keep as much sparse attention weights as possible to better approximate a full attention map, attention utility quantifies how well a sparse attention approximates the full one. In fact, we can show that it is computationally intractable to find the optimal attention utility: Theorem 1. Finding the assignment that achieves the optimal attention utility, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arg max</head><p>Ct∈C</p><formula xml:id="formula_8">AU(C t ) = (Qi,Kj )∈Ct A ij (6) is NP-hard.</formula><p>This theorem is a corollary of the NP-hardness of Attention Biclustering in <ref type="bibr" target="#b8">(Daras et al., 2020)</ref>, and it motivates us to develop a learning-based approach to optimize the attention utility. Notice that similar to other sparse attention models <ref type="bibr" target="#b13">(Kitaev et al., 2020;</ref><ref type="bibr" target="#b28">Roy et al., 2021)</ref>, the proposed attention utility metric currently does not take causal masks into consideration. Developing a new metric that can take arbitrary attention masks into account is left for future work.</p><p>To measure the attention utility of ANN-derived sparse attention patterns, we follow <ref type="bibr" target="#b13">(Kitaev et al., 2020;</ref><ref type="bibr" target="#b8">Daras et al., 2020)</ref> and adaptively set hash boundaries to create balanced clusters. The analyzed models include LSH, LSH with asymmetric transformation (i.e., SMYRF), and our proposed Learningto-Hash (L2H) method which will be introduced in the next section. We also calculate the empirical upper bound by aggregating the attention from each query to its top keys w.r.t dot-products. We present a box-plot of averaged attention utilities for all 16 Transformer layer in Figure <ref type="figure" target="#fig_1">2</ref>. We can see that the LSH methods obtain significantly less attention utilities than the upper bound or L2H. As a conclusion, the ANN-derived sparse patterns are arguably sub-optimal when used as hash functions for sparse attention. We believe this also partially explains why most of these approaches need to impose extra constraints (e.g. queries and keys are tied or normalized). In the next section, we propose an alternative choice of the hash functions to tackle this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LEARNING-TO-HASH ATTENTION (LHA)</head><p>We propose a novel Learning-to-Hash Attention model in this section. The key idea is to learn separate parameterized hash functions for queries and keys, respectively, thus the sparse pattern in LHA can be no longer limited to distance-based hash functions such as LSH or online k-means and adapt to the mismatch of query and key distributions. Besides, we empirically find that LHA can be used as a plug-and-play replacement for dense attention layers, which makes LHA applicable to the wide range of pre-trained Transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LEARNING-TO-HASH FOR SPARSE ATTENTION</head><p>We first remind the readers of our definitions of the sparse attention:</p><formula xml:id="formula_9">Sparse-Attention(Q i , K, V ) = j:h Q (Qi)=h K (Kj ) Āij V j (7) where h K , h Q : R d h → [B]</formula><p>are the hash functions for keys and queries, and Āij ∝ A ij is the renormalized attention weights such that ∀i, j:h Q (Qi)=h K (Kj ) Āij = 1. Inspired by the Learning-to-Hash methods <ref type="bibr" target="#b37">(Wang et al., 2017)</ref>, we implement the learnable hash functions h</p><formula xml:id="formula_10">K , h Q : R d h → [B] by defining parameterized functions H Q , H K : R d h → R B , such that: h Q (Q i ) = arg max b∈{1,...,B} [H Q (Q i )] b , h K (K j ) = arg max b∈{1,...,B} [H K (K j )] b ,<label>(8)</label></formula><p>where H Q and H K can be any parameterized functions such as MLPs.</p><p>Notice that our formulation is a generalization of the non-learnable sparse attention mechanisms in several previous content-based sparse attention models. To reproduce the symmetric LSH-based sparse attention scheme in Reformer <ref type="bibr" target="#b13">(Kitaev et al., 2020)</ref> or the mini-batch k-means scheme in Routing Transformer <ref type="bibr" target="#b28">(Roy et al., 2021)</ref>, we can set:</p><formula xml:id="formula_11">H Q (x) = H K (x) = x[R; −R] or H Q (x) = H K (x) = xµ (9) where R ∈ R d h × B 2 is a random matrix, [•;</formula><p>•] denotes the concatenation of two vectors, and µ ∈ R d h ×B are the exponentially moving averaged cluster centroids shared by keys and queries. The asymmetric transformation in SMYRF <ref type="bibr" target="#b8">(Daras et al., 2020)</ref> can also be reproduced by:</p><formula xml:id="formula_12">H Q (x) = 1 F (x) • a + b r , H K (x) = 1 G(x) • a + b r (10)</formula><p>where a is a random vector, b is a random scalar, r is a scalar parameter, 1(•) : [B] → {0, 1} B is the one-hot operation, and F (•) and G(•) are defined as in Equation <ref type="formula" target="#formula_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">OPTIMIZATION OBJECTIVE FOR LEARNING-TO-HASH</head><p>Previous ANN-derived sparse attention models construct their hash functions by either randomization or k-mean clustering. In contrast, our hash functions are fully learnable. However, as we use the arg max operation to get the hashing buckets, the parameterized projections in our hash functions cannot be trained in an end-to-end manner. To resolve this issue, we guide the training of our learnable hash functions directly by our proposed attention utility (See Equation <ref type="formula" target="#formula_7">5</ref>). Notice that the attention utility of each query Q i can be written as j:h Q (Qi)=h K (Kj ) A ij . We first calculate the possible attention utilities ψ (i) that query Q i can obtain when allocated to all B hash buckets:</p><formula xml:id="formula_13">ψ (i) =   j:h K (Kj )=0 A ij , • • • , j:h K (Kj )=B−1 A ij  <label>(11)</label></formula><p>Ideally, our hash function would allocate Q i to the bucket that maximize the attention utility, such that Q i can attend to the most salient keys:</p><formula xml:id="formula_14">h Q (Q i ) = arg max b∈[N b ] ψ (i) b = arg max b∈[B] h K (Kj )=b A ij<label>(12)</label></formula><p>To achieve this goal, since ψ (i) is naturally a normalized categorical distribution, we can simply convert H Q (Q i ) into a distribution and use the KL divergence between the predicted distribution softmax(H Q (Q i )) and the desired distribution ψ (i) as the optimization objective:</p><formula xml:id="formula_15">L Qi = KL(ψ (i) softmax(H Q (Q i )))<label>(13)</label></formula><p>On the other hand, the possible attention utilities ψ (j) that a key K i obtains when allocated to all B buckets can be written as:</p><formula xml:id="formula_16">ψ (j) =   i:h Q (Qi)=0 A ij , • • • , i:h Q (Qj )=B−1 A ij  <label>(14)</label></formula><p>While ψ (j) is no longer a normalized distribution, we can normalize it and similarly define the optimization objective L Kj also as a KL divergence.</p><p>During the training stage, our final optimization objective is a convex combination of the task-specific objective and the learning-to-hash objectives (i.e., {L Qi } and {L Kj }) for all queries and keys. Figure <ref type="figure" target="#fig_2">3</ref> in the appendix illustrates the joint training diagram of LHA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">APPROXIMATE ATTENTION UTILITIES</head><p>The remaining problem is how to efficiently compute ψ (i) and ψ (j) . A naive way requires calculating the dot products between all query-key pairs with O(N 2 ) complexity. Inspired by the recent advances in kernelized attention <ref type="bibr" target="#b12">(Katharopoulos et al., 2020;</ref><ref type="bibr" target="#b5">Choromanski et al., 2020;</ref><ref type="bibr" target="#b24">Peng et al., 2021)</ref>, we use random Fourier features <ref type="bibr" target="#b27">(Rahimi et al., 2007;</ref><ref type="bibr" target="#b5">Choromanski et al., 2020)</ref> to approximate ψ (i) in an unbiased manner. Let us define φ : R d h → R 2D as the Positive Randomized Features (PRFs) <ref type="bibr" target="#b5">(Choromanski et al., 2020)</ref> such that</p><formula xml:id="formula_17">E[φ(x) • φ(y)] = exp x T y √ d h<label>(15)</label></formula><p>We can approximate ψ (i) with the following formula:</p><formula xml:id="formula_18">ψ (i) b = j:h K (Kj )=b A ij ∝ j:h K (Kj )=b exp Q i • K j √ d h ∝ E   φ(Q i ) • j:h K (Kj )=b φ(K j )   (16)</formula><p>for each bucket b ∈ [B]. Since h K (K j ) does not change for the queries, we can simply pre-compute j:h K (Kj )=b φ(K j ) once to save computation, which reduces the complexity of computing {ψ (i) } for all queries from O(N 2 ) to O(N • B) ≈ O(N 1.5 ). The attention utilities for keys ψ (j) can be efficiently approximated in a similar way<ref type="foot" target="#foot_3">4</ref> :</p><formula xml:id="formula_19">ψ (i) b = i:h Q (Qi)=b A ij ∝ ∼ E     j:h Q (Qi)=b φ(Q i ) φ(Q i ) • N j=1 φ(K j )   • φ(K j )  <label>(17)</label></formula><p>4.4 IMPLEMENTATION DETAILS Practical content-based sparse attention models require that each hash bucket has the same size, which is crucial in terms of computational efficiency on modern hardwares.Therefore, we follow Roy et al. ( <ref type="formula">2021</ref>) and sort the tokens with regard to normalized hash scores, i.e., softmax(H Q (Q i )) and softmax(H K (K i )), in each bucket. The hash bucket membership is then determined by the top-k threshold, where k = N B is the bucket size. Since such a bucketing strategy no longer guarantees the validation of attention bi-clustering, as each query or each key can be assigned to zero or more than one clusters, we further empirically enlarge the hash bucket sizes by √ 2× to increase the recall of queries and keys. A pseudo-code implementation for LHA can be found in the appendix.</p><p>In causally masked attentions, the queries cannot attend to keys behind, as is usually the case in language modeling. When we use separate hash functions {h Q (Q i )} and {h K (K j }) for queries and keys, respectively, it is possible that for some query Q i , there exists no such key K j in the same bucket that j ≤ i. This would cause serious numerical instability in our implementation. To tackle this problem, inspired by <ref type="bibr" target="#b13">Kitaev et al. (2020)</ref> and <ref type="bibr" target="#b28">Roy et al. (2021)</ref>, we tie the key hashes with query hashes in the case of causal attentions by constructing a joint hash function:</p><formula xml:id="formula_20">h K (K i ) = h Q (Q i ) = arg max b∈[N b ] [H Q (Q i ) b + H K (K i ) b ]<label>(18)</label></formula><p>We find that this strategy empirically works better than using other tricks to fix this "no-attentiontarget" problem. Our solution is different from Routing Transformer <ref type="bibr" target="#b28">(Roy et al., 2021)</ref> or Reformer <ref type="bibr" target="#b13">(Kitaev et al., 2020)</ref>, which impose an extra constraint that queries and keys are tied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we conduct experiments to verify the effectiveness of our approach on several benchmark datasets covering language modeling, natural language understanding, and Long-Range-Arena. Due to the space limitations, the detailed hyper-parameter settings are presented in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">LANGUAGE MODELING</head><p>Wikitext-103 <ref type="bibr" target="#b21">(Merity et al., 2016</ref>) is a large-scale dataset for testing long term dependencies in word-level language models. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We use this dataset as a probe dataset to perform various ablations to tease apart the effect of various hyper-parameter choices on the model performance.</p><p>We follow the base setting of the state-of-the-art Transformer-XL <ref type="bibr" target="#b7">(Dai et al., 2019)</ref>  per-layer sinusoidal absolute positional encoding <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>. The LSH Transformer is implemented in our code base by simply replacing the learnable hash functions by random projections. A sequence length of 1536 is used in both training and evaluation stages for our models. In ablation study, we vary 1) the type of non-local attention heads, 2) the type of learnable hash function in LHA (i.e., linear or two-layer MLP), 3) the number of non-local attention heads, and 4) the number of non-local attention layers. We also consider a baseline where the hash functions generate content-independent random numbers. The experimental results are presented in Table <ref type="table" target="#tab_0">1</ref>.</p><p>From the table, we can see that a smaller attention size (384 compared to 768 in local Transformer) would decrease the model performance by 0.9, while adding LSH or LHA heads can improve the performance by 0.3 and 1.0, respectively. Surprisingly, we find that local Transformer is a very strong baseline for language modeling that outperforms a pure LHA Transformer. We can also see that a two-layer MLP works better than a linear function as the learnable hash functions in LHA. The best performance is achieved when we set half of the heads to local attention and the other half to non-local LHA for each layer. Finally, we can see that LHA consistently underperforms the untrainable LSH counterparts under four different settings. For the test set evaluation, we use the best setting from the ablation study. We can see that the proposed LHA model outperforms Transformer-XL, which further validate that LHA can help modeling long-term dependency for language modeling tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">NATURAL LANGUAGE UNDERSTANDING</head><p>To show the ability of our model to approximate arbitrarily complicated attention distributions, we evaluate our proposed method on the approximation of RoBERTa model <ref type="bibr" target="#b16">(Liu et al., 2019)</ref> on the  <ref type="bibr" target="#b36">(Wang et al., 2018)</ref>. Following the common practice, the maximum sequence length is set to 128. To show the effectiveness of LHA, we choose two competitive baselines in literature: SMYRF <ref type="bibr" target="#b8">(Daras et al., 2020)</ref> and Fast Clustered Attention (FCA) <ref type="bibr" target="#b35">(Vyas et al., 2020)</ref>. We also produce an LSH-based sparse attention baseline. We use a pure LHA model similar to variant (c) in Section 5.1.</p><p>We summarize the performance per task in Table <ref type="table" target="#tab_1">2</ref>. We report accuracy for all tasks except STS-B, where we report Pearson correlation. From the table, we can see that LHA performs as good as full attention for all the GLUE tasks, and that LHA outperforms all other methods in the average GLUE score, and has a smaller computational costs in its sparse attention part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">LONG-RANGE-ARENA BENCHMARK</head><p>Long-Range-Arena (LRA) benchmark <ref type="bibr" target="#b32">(Tay et al., 2020c</ref>) is a recently proposed benchmark focused on evaluating model quality under long-context scenarios for Transformer variants. We follow the apples-to-apples setting<ref type="foot" target="#foot_4">5</ref> of LRA benchmark and compare our method against other efficient attention variants. We use an LHA/local hybrid variant similar to variant (d) in Section 5.1.</p><p>We consider the tasks of ListOps (Nangia &amp; Bowman, 2018) (LO), byte-level IMDb reviews text classification <ref type="bibr" target="#b19">(Maas et al., 2011)</ref> (IMDb), byte-level document retrieval on ACL Anthology Network (AAN) <ref type="bibr" target="#b25">(Radev et al., 2013)</ref>, and CIFAR10 <ref type="bibr" target="#b14">(Krizhevsky et al., 2009)</ref> image classification on sequences of pixels (Image). The results are shown in Table <ref type="table" target="#tab_2">3</ref> and the brief descriptions of the compared baselines can be found in Section 2. From the table, we can see that LHA achieves consistent improvements over previous efficient attention models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION &amp; FUTURE WORK</head><p>In this paper, we address the limitations of ANN-based sparse attention methods and propose the Learning-to-Hash Attention (LHA) as our new solution. Specifically, LHA leverages separate learnable hash functions for queries and keys, respectively, and utilizes kernelized techniques for efficient approximation of attention utilities. The experiments on language modeling, natural language understanding, text classification, and image classification demonstrated the effectiveness of LHA.</p><p>For future work, we would like to validate the effectiveness of LHA model on much larger language modeling datasets, such as <ref type="bibr">PG-19 (Rae et al., 2019)</ref>.  For analysis, we pre-trained a 16-layer and 10-head Transformer on the WikiText-103 language modeling benchmark <ref type="bibr" target="#b21">(Merity et al., 2016)</ref>. A per-layer sinusoidal absolute positional encoding <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> is injected to the queries and keys before self-attention.</p><p>For Figure <ref type="figure" target="#fig_0">1</ref>, we analyzed the queries and keys for the first 1024 tokens in the WikiText-103 validation data, and set the number of LSH buckets B = 4. We apply LSH to all the 10 attention heads in the 3 rd Transformer layer. For Figure <ref type="figure" target="#fig_1">2</ref>, we follow the same experimental setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 LEARNING-TO-HASH</head><p>During the training stage, our final optimization objective L is a convex combination of the taskspecific objective L task (e.g., cross-entropy loss in language modeling) and learning-to-hash objectives for all keys and queries, i.e., {L Qi } and {L Kj } (See equation 13).</p><formula xml:id="formula_21">L = (1 − λ)L task + λ N • L • H N i=1 L h=1 H h=1 (L L,H Qi + L L,H Kj )<label>(19)</label></formula><p>where λ, N , L, H denote the loss coefficient, the sequence length, the number of attention layers, and the number of attention heads per layer, respectively. Empirically, we set Λ = 0.05 and found it work well across different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 LANGUAGE MODELING</head><p>We use the base setting of Transformer-XL <ref type="bibr" target="#b7">(Dai et al., 2019)</ref> in our experiments, which consists of 16 Transformer layers. For each layer, the hidden size is set to 410, and the number of attention heads is set to 10. The dimension of feed-forward layer is set to 2100. All codes are implemented based on Flax <ref type="bibr">(Heek et al., 2020)</ref> in JAX <ref type="bibr" target="#b2">(Bradbury et al., 2018)</ref>. The dropout ratio is set to 0.2. The batch size is set to 32. We use AdamW <ref type="bibr" target="#b18">(Loshchilov &amp; Hutter, 2017)</ref> as the optimizer, and set (β 1 , β 2 ) to (0.9, 0.999). The peak learning rate is set to 3.5e-4. The model is trained for 20k steps with a 2k-step warm-up stage with a cosine learning rate decay <ref type="bibr" target="#b17">(Loshchilov &amp; Hutter, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 NATURAL LANGUAGE UNDERSTANDING</head><p>We use the base setting of RoBERTa <ref type="bibr" target="#b16">(Liu et al., 2019)</ref> in our experiments, which consists of 12 Transformer layers. For each layer, the hidden size is set to 768, and the number of attention heads is set to 12. The dimension of feed-forward layer is set to 3072. The RoBERTa pre-trained checkpoint is taken from the Transformers <ref type="bibr" target="#b40">(Wolf et al., 2020)</ref> library. We use AdamW <ref type="bibr" target="#b18">(Loshchilov &amp; Hutter, 2017)</ref> as the optimizer, and set (β 1 , β 2 ) to (0.9, 0.98). To fine-tune the pre-trained models, we search the optimization hyper-parameters in a search space including different batch sizes (16/32/48), learning rates ((1-5) * 1e-5), and the number of epochs (3-5). A 10% warm-up schedule and linear learning rate decay <ref type="bibr" target="#b9">(Devlin et al., 2018)</ref> is applied. We follow the apples-to apples setting<ref type="foot" target="#foot_5">12</ref> of LRA benchmark and compare our method against other efficient attention variants. Specifically, we use a 4-layer Transformer with 256 hidden size and 1024 feed-forward layer size for the IMDb text classification task <ref type="bibr" target="#b19">(Maas et al., 2011)</ref>, a 4-layer Transformer with 256 hidden size and 1024 feed-forward layer size for the ListOps task <ref type="bibr" target="#b25">(Radev et al., 2013)</ref>, a 4-layer Transformer with 128 hidden size and 512 feed-forward layer size for the document retrieval task <ref type="bibr" target="#b23">(Nangia &amp; Bowman, 2018)</ref>, and a 4-layer Transformer with 128 hidden size and 64 feed-forward layer size for the CIFAR10 image classification task <ref type="bibr" target="#b14">(Krizhevsky et al., 2009)</ref>. The models are trained for 20k steps with a 2k-step warm-up stage with a cosine learning rate decay <ref type="bibr" target="#b17">(Loshchilov &amp; Hutter, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 TIME SERIES FORECASTING</head><p>We follow the experimental setup of Informer <ref type="bibr" target="#b46">(Zhou et al., 2020)</ref>. Specifically, the input length of recurrent component is chosen from {24, 48, 96, 168, 336, 720} for the ETTh1, and chosen from <ref type="bibr">{24, 48, 96, 192, 288, 672}</ref> for the ETTm dataset. The layer of encoder is chosen from {6, 4, 3, 2} and the layer of decoder is set as 2. The head number of multi-head attention is chosen from {8, 16}, and the dimension of multi-head attention's output is set as 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 COMPUTING INFRASTRUCTURE</head><p>All the model training are conducted on a machine with 4 NVIDIA Ampere A100 40GB GPUs and 64 AMD EPYC 7713 64-Core Processor in a Slurm <ref type="bibr" target="#b44">(Yoo et al., 2003)</ref> system. The evaluation of the inference throughput is performed on a stand-alone machine with 1 NVIDIA Tesla V100 32GB GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ADDITIONAL EXPERIMENTS ON TIME SERIES FORECASTING</head><p>We also evaluate our model on time series forecasting tasks. We use the ETT (Electricity Transformer Temperature) dataset <ref type="bibr" target="#b46">(Zhou et al., 2020)</ref>, which contains 2-year data from two separated counties in China. ETTh 1 is a dataset for 1-hour-level, while ETTm 1 is a dataset for 15-minute-level. Each data point consists of the target value "oil temperature" and 6 power load features. The train/val/test is 12/4/4 months. The Mean Squared Error (MSE) metric and Mean Average Error (MAE) metric are used as the evaluation metrics. We use an LHA/local hybrid variant similar to variant (d) in Section 5.1.</p><p>In univariate forecasting setting, all the seven features are used as input and "oil temperature" is the prediction target. The univariate evaluation results can be found in Table <ref type="table" target="#tab_4">4</ref>. We can see that LHA significantly improve the performance of state-of-the-art for most settings, while slightly underperforms Informer <ref type="bibr" target="#b46">(Zhou et al., 2020)</ref> in two settings of ETTh 1 dataset. This verifies the effectiveness of LHA on time series data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F TRAINING DYNAMICS</head><p>We plot the training dynamics of the KL divergence (i.e., L Q and L K ) and the negative entropy of the bucket-wise attention utilities (i.e., negative entropy of ψ for queries and ψ for keys) in Figure <ref type="figure">4</ref>. KL divergence is the optimization objective of LHA, while negative entropy measures the diversity of bucket-wise attention utility. The model we use is a pure LHA model with 16 layers and 10 LHA heads, trained on WikiText-103 for 6000 steps. Each LHA head has 8 hash buckets. From the plots, we can see that the values of negative entropy consistently increase during training for both queries and keys, while the values of KL divergence first increase (due to the increase of the diversity of bucket-wise attention utility), and then decrease (due to learning-to-hash optimization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G ADDITIONAL ANALYSIS ON TRAINING &amp; INFERENCE EFFICIENCY</head><p>To further analyze the efficiency of the proposed LHA compared to softmax attention, we measure the latency of LHA and softmax attention for both training and inference stage, varying by sequence length. Notice that in the inference stage, the LHA model needs not to calculate the learning-to-hash losses (i.e., the KL divergence terms). We study a single-layer attention, which has 10 attention heads and the hidden size of 410. Figure <ref type="figure" target="#fig_3">5</ref> illustrates the latency in both training and inference stage for softmax attention and LHA with different attention size. We can see that LHA can achieve more significant speedup when facing longer sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H ADDITIONAL ANALYSIS ON THE BUCKET IMBALANCE ISSUES</head><p>To further analyze how the bucket imbalance issues are alleviated by the proposed LHA, we plot the histograms of bucket sizes and query-key ratios for all 16 × 10 = 160 attention heads in a pre-trained Transformer. The results are shown in Figure <ref type="figure" target="#fig_4">6</ref>. We can see that learning-to-hash attention (l2h) has more buckets close to the optimal bucket size (i.e., 512) and has more balanced query-key ratios for hash buckets.</p><p>Notice that to compare the statistics of the bucket size and query-key ratios between LSH and LHA, we fine-tune learnable hash functions for the same pre-trained Transformer. Notice that in this experiment, we directly use the highest-ranked bucket as the hash bucket for each query and key, instead of using a token sorting strategy <ref type="bibr" target="#b28">(Roy et al. 2021)</ref> to maintain the same bucket sizes. When calculating the query-key ratios, we do not count the buckets which have no queries or keys.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: We show the unbalanced bucket sizes and unbalanced query-key ratios with statistics of 10 attention heads in the 3 rd layer of a pre-trained Transformer. For each head, we assign the queries and keys of first 1024 tokens in the WikiText-103 validation data into 4 LSH buckets. The buckets are sorted for each attention head according to total bucket sizes (i.e., #query + #key).</figDesc><graphic url="image-2.png" coords="4,146.35,146.22,316.80,63.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: We show the attention utility evaluation of four different sparse attention patterns. For L2H and LSHs, we assign the queries and keys of first 1024 tokens in the WikiText-103 validation data into 4 hash buckets. The causal mask is not used when calculating the attention weights.</figDesc><graphic url="image-3.png" coords="5,146.35,81.86,316.80,95.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The joint training diagram of LHA, where queries and keys generate attention weights to train the learnable hash functions, while the hash functions generate the sparse attention patterns, on which queries and keys are trained towards down-stream tasks.A EXPERIMENTAL SETTINGS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The training and inference latency of LHA and softmax attention. The latency is measured on a single NVIDIA Tesla A100 GPU with a batch size of 1.</figDesc><graphic url="image-11.png" coords="19,309.96,296.14,194.04,132.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: We show the histogram of hash bucket statistics for all 16 × 10 = 160 attention heads in the a pre-trained 16-layer Transformer. For each head, we assign the queries and keys of first 1024 tokens in the WikiText-103 validation data into 4 LSH buckets. (left) The histogram of the bucket sizes for all 160 × 4 = 640 buckets. (right) The histogram of the query-key ratios for all 160 × 4 = 640 buckets. The dashed red lines denote the optimal case where each hash bucket has the same size and same number of queries and keys.</figDesc><graphic url="image-10.png" coords="19,108.00,298.13,194.04,130.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation studies on the WikiText-103 validation data in the base setting. Lower perplexity (PPL) is better. All the models have a total of 16 attention layers and 10 heads. Non-Local (NL, i.e., LSH or LHA) layers when present are always added at the top of the model. Attention size denotes either local window size or hash bucket size. † denotes that the results are taken from Transformer-XL<ref type="bibr" target="#b7">(Dai et al., 2019)</ref>.</figDesc><table><row><cell></cell><cell cols="8">label NL Heads NL Layers Hash Func. Att. Size #Param Valid PPL Test PPL</cell></row><row><cell></cell><cell>-</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell>768</cell><cell>151M</cell><cell>22.89</cell><cell>-</cell></row><row><cell>Local Transformer</cell><cell>-</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell>384</cell><cell>151M</cell><cell>23.82</cell><cell>-</cell></row><row><cell></cell><cell>-</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell>640</cell><cell>151M</cell><cell>23.09 †</cell><cell>24.0 †</cell></row><row><cell></cell><cell>-</cell><cell>10</cell><cell>16</cell><cell>Rand. Linear</cell><cell>384</cell><cell>153M</cell><cell>24.64</cell><cell>-</cell></row><row><cell>LSH Transformer</cell><cell>--</cell><cell>5 10</cell><cell>16 8</cell><cell>Rand. Linear Rand. Linear</cell><cell>384 384</cell><cell>153M 153M</cell><cell>23.51 23.76</cell><cell>--</cell></row><row><cell></cell><cell>-</cell><cell>5</cell><cell>8</cell><cell>Rand. Linear</cell><cell>384</cell><cell>153M</cell><cell>23.53</cell><cell>-</cell></row><row><cell></cell><cell>(a)</cell><cell>10</cell><cell>16</cell><cell>Rand.</cell><cell>384</cell><cell>153M</cell><cell>26.05</cell><cell>-</cell></row><row><cell></cell><cell>(b)</cell><cell>10</cell><cell>16</cell><cell>Linear</cell><cell>384</cell><cell>153M</cell><cell>24.22</cell><cell>-</cell></row><row><cell>LHA Transformer</cell><cell>(c)</cell><cell>10</cell><cell>16</cell><cell>MLP</cell><cell>384</cell><cell>153M</cell><cell>24.00</cell><cell>-</cell></row><row><cell></cell><cell>(d)</cell><cell>5</cell><cell>16</cell><cell>MLP</cell><cell>384</cell><cell>153M</cell><cell>22.79</cell><cell>23.2</cell></row><row><cell></cell><cell>(e)</cell><cell>10</cell><cell>8</cell><cell>MLP</cell><cell>384</cell><cell>153M</cell><cell>23.00</cell><cell>-</cell></row><row><cell></cell><cell>(f)</cell><cell>5</cell><cell>8</cell><cell>MLP</cell><cell>384</cell><cell>153M</cell><cell>22.84</cell><cell>-</cell></row></table><note>model, which contains 16 Transformer layers with 10 heads per layer. For local attentions, we use the relative positional encoding<ref type="bibr" target="#b7">(Dai et al., 2019)</ref>, while for non-local attentions (i.e., LSH or LHA), we use the</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance of methods per task on the GLUE benchmark, where C and # denote the number of queries/keys per cluster and hashing rounds, respectively. † denotes that the results are taken from their original papers.</figDesc><table><row><cell cols="2">RoBERTa-base -</cell><cell>-</cell><cell>60.9</cell><cell>87.6</cell><cell>88.7</cell><cell>92.7</cell><cell>91.6</cell><cell>68.5</cell><cell>94.5</cell><cell>90.0</cell><cell>84.3</cell></row><row><cell>SMYRF †</cell><cell cols="2">2 16</cell><cell>58.9</cell><cell>82.3</cell><cell>85.7</cell><cell>89.5</cell><cell>89.3</cell><cell>64.5</cell><cell>93.1</cell><cell>87.8</cell><cell>81.4</cell></row><row><cell>SMYRF †</cell><cell cols="2">2 32</cell><cell>58.8</cell><cell>85.0</cell><cell>87.7</cell><cell>91.1</cell><cell>89.7</cell><cell>68.6</cell><cell>93.2</cell><cell>89.7</cell><cell>83.0</cell></row><row><cell>FCA †</cell><cell>-</cell><cell>-</cell><cell>59.8</cell><cell>79.4</cell><cell>43.6</cell><cell>74.6</cell><cell>89.4</cell><cell>49.8</cell><cell>94.4</cell><cell>78.9</cell><cell>71.2</cell></row><row><cell>i-FCA †</cell><cell>-</cell><cell>-</cell><cell>60.1</cell><cell>88.0</cell><cell>87.3</cell><cell>93.0</cell><cell>91.5</cell><cell>70.4</cell><cell>94.7</cell><cell>90.0</cell><cell>84.4</cell></row><row><cell>LSH</cell><cell cols="2">1 11</cell><cell>61.5</cell><cell>86.3</cell><cell>88.7</cell><cell>91.1</cell><cell>90.2</cell><cell>68.0</cell><cell>93.3</cell><cell>88.5</cell><cell>83.5</cell></row><row><cell>LSH</cell><cell cols="2">1 22</cell><cell>61.5</cell><cell>86.8</cell><cell>89.7</cell><cell>91.9</cell><cell>90.8</cell><cell>68.4</cell><cell>93.9</cell><cell>88.7</cell><cell>84.0</cell></row><row><cell>LSH</cell><cell cols="2">1 45</cell><cell>61.8</cell><cell>87.5</cell><cell>88.7</cell><cell>92.7</cell><cell>91.2</cell><cell>68.1</cell><cell>94.0</cell><cell>89.4</cell><cell>84.2</cell></row><row><cell>LHA</cell><cell cols="2">1 11</cell><cell>61.2</cell><cell>85.7</cell><cell>89.1</cell><cell>91.3</cell><cell>90.5</cell><cell>68.0</cell><cell>93.7</cell><cell>89.0</cell><cell>83.6</cell></row><row><cell>LHA</cell><cell cols="2">1 22</cell><cell>62.0</cell><cell>86.8</cell><cell>88.0</cell><cell>92.4</cell><cell>91.1</cell><cell>68.6</cell><cell>94.8</cell><cell>89.6</cell><cell>84.2</cell></row><row><cell>LHA</cell><cell cols="2">1 45</cell><cell>62.3</cell><cell>87.7</cell><cell>89.1</cell><cell>92.8</cell><cell>91.4</cell><cell>68.5</cell><cell>94.6</cell><cell>89.5</cell><cell>84.5</cell></row></table><note># C CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B AVG</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy on LO, IMDb, AAN, and Image in Long Range Arena benchmark. Best and second best model per task is shown in boldface and underlined. Throughput is evaluated on IMDb and relative to the vanilla transformer's. * and † denote being statistically significantly better (p &lt; 0.05 in one-tail proportion test) than vanilla Transformer and the second best model. ‡ denotes that the throughput comparison are run on a single NVIDIA Tesla V100 32GB GPU, while previous results<ref type="bibr" target="#b32">(Tay et al., 2020c)</ref> are reported on 4 × 4 TPU V3 chips. ♥, ♦, and ♠ denotes low-rank/kernelized attention, content-based sparse attention, and location-based sparse attention, respectively.</figDesc><table><row><cell>Accuracy (↑)</cell><cell>Throughput (↑)</cell></row></table><note>* † 62.8 * † 45.2 * 53.1 1.0 ‡ 1.1 ‡ 1.3 ‡ 1.5 ‡ GLUE (General Language Understanding Evaluation) dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Univariate long sequence time series forecasting results on two datasets (lower is better). †denotes that the results are taken from<ref type="bibr" target="#b46">(Zhou et al., 2020)</ref>.</figDesc><table><row><cell cols="2">Models</cell><cell>Ours -LHA</cell><cell>Informer †</cell><cell>LogTrans †</cell><cell>Reformer †</cell><cell>DeepAR †</cell><cell>Prophet †</cell></row><row><cell cols="2">Metric</cell><cell>MSE MAE</cell><cell>MSE MAE</cell><cell>MSE MAE</cell><cell>MSE MAE</cell><cell>MSE MAE</cell><cell>MSE MAE</cell></row><row><cell></cell><cell>24</cell><cell cols="6">0.060 0.191 0.098 0.247 0.103 0.259 0.222 0.389 0.107 0.280 0.115 0.275</cell></row><row><cell>ETTh1</cell><cell cols="7">48 168 0.158 0.328 0.183 0.346 0.187 0.355 1.522 1.191 0.239 0.422 1.224 0.763 0.088 0.236 0.158 0.319 0.161 0.322 0.284 0.445 0.162 0.327 0.168 0.330 336 0.219 0.399 0.222 0.387 0.230 0.398 1.860 1.124 0.445 0.552 1.549 1.820</cell></row><row><cell></cell><cell cols="7">720 0.265 0.437 0.269 0.435 0.273 0.463 2.112 1.436 0.658 0.707 2.735 3.253</cell></row><row><cell>ETTm1</cell><cell cols="7">24 48 96 288 0.121 0.277 0.401 0.554 0.411 0.572 1.108 1.245 0.948 0.795 0.452 0.574 0.016 0.102 0.030 0.137 0.065 0.202 0.095 0.228 0.091 0.243 0.120 0.290 0.021 0.112 0.069 0.203 0.078 0.220 0.249 0.390 0.219 0.362 0.133 0.305 0.054 0.180 0.194 0.372 0.199 0.386 0.920 0.767 0.364 0.496 0.194 0.396</cell></row><row><cell></cell><cell cols="7">672 0.403 0.572 0.512 0.644 0.598 0.702 1.793 1.528 2.437 1.352 2.747 1.174</cell></row><row><cell cols="5">A.5 LONG-RANGE-ARENA BENCHMARK</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Notice that we only consider the setting of single-round hashing in our paper, but our analysis and the proposed LHA method can be generalized to the multi-round hashing setting.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We use the same LSH technique as in<ref type="bibr" target="#b13">(Kitaev et al., 2020)</ref>, except that we do not impose extra constraints to queries and keys. This is because we would like to develop a plug-and-play replacement for dense attention layers without imposing extra constraints for queries and keys.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The detailed experimental setting can be found in the appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"> Please refer to (Choromanski et al., 2020)  for how PRFs stabilize attention renormalization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">www.github.com/google-research/long-range-arena#apples-to-apples-setting</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_5">www.github.com/google-research/long-range-arena#apples-to-apples-setting</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research used Perlmutter supercomputer of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231 using NERSC award NERSC DDR-ERCAP0022110.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Published as a conference paper at ICLR 2022 Table <ref type="table">5</ref>: Multivariate long sequence time series forecasting results on two datasets (lower is better). †denotes that the results are taken from <ref type="bibr" target="#b46">(Zhou et al., 2020)</ref>. In multivariate forecasting setting, all the seven features are the prediction targets. The multivariate evaluation results can be found in Table <ref type="table">5</ref>. We can see that LHA significantly improve the performance of state-of-the-art for all settings, which further demonstrates the effectiveness of the proposed LHA method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C PROOF</head><p>We follow the notations in <ref type="bibr" target="#b8">(Daras et al., 2020)</ref>. Let C B the set of all possible assignments in B balanced non-overlapping clusters. A specific assignment is denoted by C B t and there are T possible such assignments:</p><p>C B t = {c 1 , c 2 , ..., c B } :</p><p>We have the following lemma, which is referred as the max-mass problem: Lemma 1. <ref type="bibr" target="#b8">(Daras et al., 2020)</ref> The optimization problem:</p><p>is NP-hard.</p><p>The main idea of the proof is to show that solving polynomially the above problem would mean that we could also solve in polynomial time the 3-DM, which is known to be NP-complete. Please refer to <ref type="bibr" target="#b8">(Daras et al., 2020)</ref> for the detailed constructive proofs. Next, we show how to use this lemma to prove Theorem 1.</p><p>Proof of Theorem 1. Let {Q i } and {K j } denote the query and key sets that we consider for computing the attention utility. We can first construct the new query set Q and key set K such that</p><p>This can be achieved by applying SVD to the matrix of</p><p>. Next, we construct another query set Q , such that</p><p>The problem of finding the optimal attention utility, i.e.,</p><p>is thus equivalent to the problem in Lemma 1, with Q and K as the query and key sets, which is proven to be NP-hard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ALGORITHM</head><p>We present a detailed pseudo-code implementation for LHA in Algorithm 1.</p><p>Algorithm 1 Single-layer Single-head Learning-to-Hash Attention (LHA) 1: Queries, Keys and Values: Q, K, V ∈ R N ×d h 2: Query/Key Hash Functions: H Q , H K 3: Attention Utility Loss Weight: λ 4: if causal mask then 5:</p><p>the query, key, and value vectors of the attention mechanism, where N is the sequence length and d h is the hidden size. Let φ : R d h → R 2D be Positive Randomized Features (PRFs) <ref type="bibr" target="#b5">(Choromanski et al., 2020)</ref> such that:</p><p>We can approximate the aggregated attention utility (Eq. 5) by: ψ</p><p>where ∝ ∼ denotes that the attention approximation converges under the positive random feature condition <ref type="bibr" target="#b12">(Katharopoulos et al., 2020)</ref>. Next, the Learning-to-Hash objectives for each query and each key can be defined as:</p><p>The final optimization objective L is a convex combination of the task-specific objective L task (e.g., cross-entropy loss in language modeling) and learning-to-hash objectives for all keys and queries in each attention head for each layer:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Etc: Encoding long and structured inputs in transformers</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaclav</forename><surname>Cvicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Jax: composable transformations of python+ numpy programs</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Version 0.1, 55</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Krzysztof Marcin Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><surname>Kaiser</surname></persName>
		</author>
		<ptr target="https://github.com/Edward-Sun/Learning-to-Hash-Attention7https://github.com/kimiyoung/transformer-xl8https://www.tensorflow.org/datasets/catalog/glue9https://github.com/google-research/long-range-arena10https://github.com/zhouhaoyi/Informer202011https://huggingface.co/roberta-base" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptively sparse transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gonçalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André Ft</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2174" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><surname>Smyrf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05315</idno>
		<title level="m">Efficient attention using asymmetric clustering</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Flax: A neural network library and ecosystem for jax</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Heek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Rondepierre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Van Zee</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Version 0.3, 3, 2020</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04451</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10198</idno>
		<title level="m">Generating wikipedia by summarizing long sequences</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
				<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sparse and constrained attention for neural machine translation</title>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André Ft</forename><surname>Martins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08241</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On evaluation of adversarial perturbations for sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Miguel</forename><surname>Pino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.06620</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Listops: A diagnostic dataset for latent tree learning</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06028</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02143</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Random feature attention</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Vahed Qazvinian, and Amjad Abu-Jbara. The acl anthology network corpus. Language Resources and Evaluation</title>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dragomir R Radev</surname></persName>
		</author>
		<author>
			<persName><surname>Muthukrishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="919" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Anna</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05507</idno>
		<title level="m">Compressive transformers for long-range sequence modelling</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07799</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00743</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse sinkhorn attention</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="9438" to="9447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04006</idno>
		<title level="m">Long range arena: A benchmark for efficient transformers</title>
				<imprint>
			<date type="published" when="2020">2020c</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Treviso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">António</forename><surname>Góis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erick</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André Ft</forename><surname>Martins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12188</idno>
		<title level="m">Predicting attention sparsity in transformers</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast transformers with clustered attention</title>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey on learning to hash</title>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><surname>Heng Tao Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="769" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06097</idno>
		<title level="m">Cluster-former: Clustering-based sparse transformer for long-range dependency encoding</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting</title>
		<author>
			<persName><forename type="first">Haixu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiehui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13008</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">omformer: A nystr\&quot; om-based algorithm for approximating self-attention</title>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Nystr\</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03902</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Slurm: Simple linux utility for resource management</title>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morris</forename><forename type="middle">A</forename><surname>Jette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Grondona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
	</analytic>
	<monogr>
		<title level="m">Workshop on job scheduling strategies for parallel processing</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003. 2020</date>
			<biblScope unit="page" from="44" to="60" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Big bird: Transformers for longer sequences</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Poolingformer: Long document modeling with pooling attention</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weisheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancheng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04371</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Informer: Beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName><forename type="first">Haoyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieqi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wancai</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07436</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
