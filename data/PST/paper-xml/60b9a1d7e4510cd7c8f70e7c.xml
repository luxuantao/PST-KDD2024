<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhanced Graph Learning for Collaborative Filtering via Mutual Information Maximization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yonghui</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Knowledge Engineering with Big Data</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Knowledge Engineering with Big Data</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Knowledge Engineering with Big Data</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Knowledge Engineering with Big Data</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Knowledge Engineering with Big Data</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Knowledge Engineering with Big Data</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
							<email>zhang1028kun@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Knowledge Engineering with Big Data</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Knowledge Engineering with Big Data</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Knowledge Engineering with Big Data</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Hefei Comprehensive National Science Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Key Laboratory of Knowledge Engineering with Big Data</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Hefei University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhanced Graph Learning for Collaborative Filtering via Mutual Information Maximization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3404835.3462928</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Collaborative Filtering</term>
					<term>Recommendation</term>
					<term>Graph Learning</term>
					<term>Mutual Information Maximization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural graph based Collaborative Filtering (CF) models learn user and item embeddings based on the user-item bipartite graph structure, and have achieved state-of-the-art recommendation performance. In the ubiquitous implicit feedback based CF, users' unobserved behaviors are treated as unlinked edges in the user-item bipartite graph. As users' unobserved behaviors are mixed with dislikes and unknown positive preferences, the fixed graph structure input is missing with potential positive preference links. In this paper, we study how to better learn enhanced graph structure for CF. We argue that node embedding learning and graph structure learning can mutually enhance each other in CF, as updated node embeddings are learned from previous graph structure, and vice versa (i.e., newly updated graph structure are optimized based on current node embedding results). Some previous works provided approaches to refine the graph structure. However, most of these graph learning models relied on node features for modeling, which are not available in CF. Besides, nearly all optimization goals tried to compare the learned adaptive graph and the original graph from a local reconstruction perspective, whether the global properties of the adaptive graph structure are modeled in the learning process is still unknown. To this end, in this paper, we propose an enhanced graph learning network (EGLN ) approach for CF via mutual information maximization. The key idea of EGLN is two folds: First, we let the enhanced graph learning module and the node embedding module iteratively learn from each other without any feature input. Second, we design a local-global consistency optimization function to capture the global properties in the enhanced graph learning process. Finally, extensive experimental results on three real-world datasets clearly show the effectiveness of our proposed model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Collaborative filtering provides personalized recommendations by collectively learning users' preference from user-item historical interaction behaviors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. In most recommendation scenarios, it is more common for users to express their preferences with implicit feedback (e.g., viewing a movie, visiting a restaurant, and pining a picture) rather than explicit ratings. Under the ubiquity of implicit feedback, users' limited observed behaviors are denoted as positive preference set, while negative and unobserved positive preferences are mixed together to form a large unobserved preference set <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. Therefore, how to learn users' preferences from implicit feedback based CF has become an important topic in both academia and industry.</p><p>State-of-the-art CF models rely on embedding techniques for the recommendation, due to the flexibility, and relatively high performance of these embedding models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref>. Earlier works treated user-item implicit feedback as a user-item binary rating matrix, with the observed values as 1, and the unobserved values as 0, and adopted matrix factorization techniques for the user and item embedding learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>. Bayesian Personalized Ranking (BPR) is a popular pairwise ranking matrix factorization based recommendation model that is specifically designed for implicit feedback. By projecting both users and items into a low latent space, BPR designs a pairwise ranking goal, assuming that a user prefers an observed item to an unobserved item <ref type="bibr" target="#b25">[26]</ref>. As users' behaviors are naturally represented as a user-item bipartite graph, neural graph collaborative filtering models have been proposed to better model user-item bipartite graph structure <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33]</ref>. These neural graph models perform graph convolution by updating user and item embeddings at current layer based on the aggregation of linked entity embeddings at previous layer. Furthermore, motivated by the advances of neural mutual information techniques for capturing structured information <ref type="bibr" target="#b31">[32]</ref>, researchers propose to learn node embeddings of graph-structured data by maximizing the mutual information between node representations and the corresponding summarized global representations of the graph structure <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>. These neural graph models inject the graph structure in user and item embedding learning, and enhance representation learning with mutual information maximization on graph-structured data.</p><p>In fact, the performance of neural graph recommendation models relies on the input data of the user-item bipartite graph. In implicit feedback based recommendation, directly transforming the observed user-item behaviors as links is the default choice for current neural graph models. However, we argue that the fixed graph construction process neglects the unique properties of the implicit feedback recommendation by treating all unobserved behaviors as negative feedback. In practice, unobserved behaviors are mixed with negative feedback and unknown positive feedback. Simply treating all unobserved feedback as negative edges that are not linked in the user-item bipartite graph neglects the difference between the true negative and false negative behaviors. Such a default fixed graph structure is obviously noisy with missing false negative interactions, and would lead to suboptimal performance especially when users have sparse interaction records.</p><p>In this paper, we argue that instead of adopting a fixed useritem graph structure for embedding learning in CF, we also need to learn an adaptive user-item graph structure to better serve CF. This is a non-trivial task as we do not know whether a missing link from users' unobserved behaviors is true negative or false negative. Some researchers in the machine learning community also study a similar problem of learning better graph structures to facilitate downstream tasks. Researchers propose to learn graph topology and node embeddings from a unified perspective, either by edge reweighting <ref type="bibr" target="#b30">[31]</ref> with self-attention or parameterized node similarity with input node features <ref type="bibr" target="#b18">[19]</ref>. These models do not suit our task for two reasons. First, all of these models rely on the input node features for adaptive graph learning, while CF does not contain any node feature information. Second, most of these adaptive graph learning techniques use local optimization functions for graph learning, e.g., the learned node embeddings from adaptive graph need to reconstruct the original graph with first-order reconstruction errors. However, as the graph local-global structure matters, how to keep the local-global consistency of the adaptively learned graph is still unknown.</p><p>To this end, in this paper, we propose an enhanced graph learning approach for CF. We formulate the approach with two mutually enhanced parts: enhanced graph learning module and node embedding learning module. The rationale is that, we can design a better graph learning module with previously learned user and item embeddings, and a better user and item embedding learning with updated adaptive graph structure. Specifically, we first let the graph learning module and the node embedding learning module iteratively learn from each other without any feature input. As users' observed behaviors are ground-truth positive preferences, and the missing behaviors are mixed with false negative (i.e., unknown positive) preferences, the graph learning module is a residual graph learning to pick possible unknown preferences with confidence weights. Next, we design an enhanced graph optimization function, with the local-global consistency of the enhanced graph is also modeled to serve better graph structure and node embedding learning. The proposed model can be trained end-to-end. Finally, we perform extensive experimental results on three real-world datasets, and the experimental results clearly demonstrate the superiority and effectiveness of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Collaborative Filtering. CF based approaches make personalized recommendations by collectively analyzing user-item behaviors <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>. Learning accurate user and item embeddings is the default architecture for modern recommender systems due to its flexibility and relatively high performance <ref type="bibr" target="#b27">[28]</ref>. After that, the preference of a user to an item can be predicted as the inner product of the corresponding user and item embeddings. Most traditional CF models take a user-item matrix as input, and learn free user and item latent embeddings based on matrix factorization techniques <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>. Recently, researchers argued that user behaviors can be naturally represented as a user-item bipartite graph structure. Thanks to the powerful modeling ability of graph neural networks for modeling complex graph structure <ref type="bibr" target="#b17">[18]</ref>, researchers proposed to learn user and item embeddings from the bipartite graph. Many graph based recommendation models, such as GC-MC <ref type="bibr" target="#b29">[30]</ref>, Pin-Sage <ref type="bibr" target="#b37">[38]</ref>, NGCF <ref type="bibr" target="#b32">[33]</ref>, LR-GCCF <ref type="bibr" target="#b3">[4]</ref> and LightGCN <ref type="bibr" target="#b11">[12]</ref> have been proposed. Different neural graph recommendation models vary in the input graph data and the graph aggregation process. E.g., NGCF exploited the user-item bipartite graph structure as input, and injected the high-order collaborative signals for better user/item embedding learning with iterative graph convolutions <ref type="bibr" target="#b32">[33]</ref>. Pin-Sage took item-item correlation graph with item content features as input, and learned graph-smoothed item embeddings <ref type="bibr" target="#b37">[38]</ref>. LR-GCCF <ref type="bibr" target="#b3">[4]</ref> and LightGCN <ref type="bibr" target="#b11">[12]</ref> simplified neural graph models with linear graph convolutions by considering the uniqueness of CF, and achieved state-of-the-art performance. Neural graph based recommendation models have shown dominating recommendation performance. Nearly all neural graph recommendation models are implemented with a fixed sparse user-item graph. In practice, the user/item embeddings learned from the input graph are predictable for the possible missing links of the original user-item graph. Based on this natural idea, instead of learning embeddings from fixed user-item graph at once, we consider how to iteratively perform graph learning based on current learned embeddings to enhance user-item graph for better recommendation.</p><p>Graph Learning based Models. Graph convolutional networks perform graph convolutions to integrate vertex features and the input graph topology for node embedding learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Here, the graph structure is constructed by human experts or precomputed by the k-nearest neighbors of each node <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. As the input graph structure is not perfect and may contain missing or noisy edges, the graph structure is not optimized for graph based downstream tasks. GAT is proposed to attentively reweight edge importance based on the self-attention mechanism <ref type="bibr" target="#b30">[31]</ref>. Researchers proposed to learn adaptive graph structure to facilitate downstream graph based tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39]</ref>. These approaches learned distance metrics for graph Laplacian matrix with parameterized similarity calculation. The parameterized similarity calculation is based on input node features, such as the covariance matrix of input features <ref type="bibr" target="#b19">[20]</ref>, feature weight vector <ref type="bibr" target="#b15">[16]</ref>. E.g., given input features and a preliminary graph structure, GLCN is proposed to learn revised edge weights with weight vectors of input features <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b4">[5]</ref>. Since the input graph is not always available in real-world scenarios, researchers proposed to learn discrete and spare graph structure from scratch by solving a bilevel program that learns a discrete probability on the edges of the graph <ref type="bibr" target="#b6">[7]</ref>. Since this model needs to learn the possible link of every node pair, the time complexity is square of the nodes, which prevents it from being applied to large graphs. In summary, all of these works showed promising results of learning and revising graph structure to facilitate downstream tasks. However, these models can not directly applied to CF as we do not have any node features. Inspired by the dropout technique in deep learning, researchers proposed to drop edges for robust graph learning, such to avoid fake edges or adversarial attacks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27]</ref>. In contrast, we consider the scenario of sparse links in CF and how to add edges to enhance neural graph recommendation models. Mutual Information Maximization. Maximizing the mutual information between the input and output is a fundamental quantity for measuring the dependency of input and output variables. Recently, many researchers paid attention to representation learning based on mutual information estimation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref>. Due to the difficulty of measuring mutual information in high dimensional space, Belghazi et al. proposed a neural estimation approach for mutual information estimation, where a statistical network is trained to distinguish samples coming from the joint distribution of two random variables. Deep InfoMax (DIM) argued that the structure matters, and investigated representation learning by maximizing the mutual information between local representation and global representation <ref type="bibr" target="#b13">[14]</ref>. DIM significantly improved the representation learning performance via the local-global pair mutual information maximization. Inspired by DIM, Deep Graph InfoMax (DGI) is proposed for representation learning for graph structured data <ref type="bibr" target="#b31">[32]</ref>. DGI maximizes mutual information between patch representations and corresponding high level summary of graph, which are learned based on GNNs. DGI is further extended to the heterogeneous graph embedding <ref type="bibr" target="#b22">[23]</ref>, community-aware graph embedding learning <ref type="bibr" target="#b39">[40]</ref>, consideration of measuring mutual information from both node features and topological structure <ref type="bibr" target="#b24">[25]</ref>, and graph-level representation learning <ref type="bibr" target="#b28">[29]</ref>. In CF based recommendation, users and items are formed as a bipartite graph. Cao et al. proposed BiGI, a bipartite graph embedding learning method to suit recommendation scenario via mutual information maximization <ref type="bibr" target="#b2">[3]</ref>. Specifically, BiGI first generated global graph representation from the composition of user/item embedding and local representation via a subgraph-level attention mechanism. Then, it recognized global properties through local-global mutual information maximization. BiGI showed superior recommendation performance compared to neural graph based recommendation models by modeling global properties of the graph. Our work also borrows the success of mutual information modeling on graph structure data. Different from these works, we apply mutual information on the enhanced graph to model the global graph properties for the enhanced graph learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED MODEL</head><p>In this section, we would introduce the proposed Enhanced Graph Learning Network (EGLN) approach for recommendation. We first propose the overall architecture of the proposed model, followed by the two key components in our proposed model: enhanced graph learning with learned embeddings and node embedding learning with the enhanced graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Architecture</head><p>In CF based recommender systems, there are two kinds of entries: a userset 𝑈 (|𝑈 | = 𝑀) and an itemset 𝑉 (|𝑉 | = 𝑁 ). Considering the implicit feedback is more common in most recommendation scenarios, we use interaction matrix R ∈ R 𝑀×𝑁 to denote user-item interactions, where each element r 𝑎𝑖 = 1 if user 𝑎 has interacted with item i, otherwise r 𝑎𝑖 = 0. Given the interaction matrix R, most neural graph recommendation models define a user-item bipartite graph as: G = {𝑈 ∪𝑉 , A}, where the adjacency matrix is unweighted matrix defined as follows:</p><formula xml:id="formula_0">A = 0 𝑀×𝑀 R R 𝑇 0 𝑁 ×𝑁 .<label>(1)</label></formula><p>Given the above graph G = {𝑈 ∪ 𝑉 , A}, most graph based CF models take this fixed graph as input, and learn user and item embeddings based on the fixed graph structure. These models use iterative graph aggregation operations to encode the graph structure for node embedding learning. Let P ∈ R 𝐷×𝑀 and Q ∈ R 𝐷×𝑁 denote the free user embedding matrix and free item embedding matrix that need to be learned, the state-of-the-art graph based CF models learn the final user embedding matrix U ∈ R 𝐷×𝑀 and final item embedding matrix V ∈ R 𝐷×𝑁 by injecting high-order structure of the graph G for the final embedding learning. Without loss of generality, we use [P, Q, U, V] = 𝐺𝐶𝐹 (G) to represent the graph based CF approach. We use node to denote either a user or an item node without distinction.</p><p>Previous graph based recommendation models use fixed graph structure input G for user and item embedding learning. In this work, we argue that the fixed graph structure is not the optimal input for graph based CF models. The reason is that, in implicit feedback based CF, users' unobserved behaviors are mixed with negative and unknown positive preferences. Nevertheless, the fixed graph structure treats the negative and unknown preferences equally by regarding all unknown preferences as missing links on the fixed graph structure. Therefore, we argue instead of adopting a fixed user-item graph structure for embedding learning in CF, we also need to learn an enhanced user-item graph structure to better serve CF. We denote the enhanced graph structure as G 𝐸 = {𝑈 ∪ 𝑉 , A 𝐸 }, where A 𝐸 = A + A 𝑅 . Specifically, A 𝑅 ∈ R (𝑀+𝑁 )×(𝑀+𝑁 ) denotes the residual non-negative edge weight matrix that needs to be learned. We use the residual graph learning structure as all existing edges in the original user-item bipartite graph denote the positive preferences of users, and are valuable for user and item embedding learning. By using the residual graph structure, the revised graph structure is enhanced by possible unknown positive preferences that are hidden in unobserved behaviors. Our goal turns to find a better residual graph with edge weight matrix A 𝑅 , such that we could better serve user and item embedding learning for improving CF performance. Now, the graph based CF consists of two modules: residual graph learning and node embedding learning. These two modules are not isolated but closely related. On one hand, the residual graph learning module needs to rely on the currently learned user and item embeddings, in order to find possible links of A 𝐸 . As we can predict users' preferences based on the learned user and item embeddings, the possible unknown residual link matrix A 𝑅 can be formulated as: A 𝑅 = 𝐺𝐿(P, Q, U, V), where 𝐺𝐿 is the graph learning module based on the output of the graph based CF module. On the other hand, after we have obtained residual graph structure, we could use the graph based CF module to find better user and item embeddings as: [P, Q, U, V] = 𝐺𝐶𝐹 (A + A 𝑅 ) based on the enhanced graph structure. Therefore, these two modules can be performed in an iterative way, with better graph based CF module facilitates the better graph learning module, and vice versa. For better illustration, we show the overall framework of EGLN in Figure <ref type="figure" target="#fig_0">1</ref> and give the detailed structure of two modules in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Embedding Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Enhanced Graph Learning with Learned Embeddings</head><p>Given the node embeddings learned from the previous graph based CF model as:</p><formula xml:id="formula_1">[P, Q, U, V] = 𝐺𝐶𝐹 (A + A 𝑅</formula><p>), our goal is to learn a residual graph structure with residual weight matrix A 𝑅 . Since the residual graph structure is also a user-item bipartite graph, the weight matrix A 𝑅 is actually composed as:</p><formula xml:id="formula_2">A 𝑅 = 0 𝑀×𝑀 S S 𝑇 0 𝑁 ×𝑁 ,<label>(2)</label></formula><p>where S ∈ R 𝑀×𝑁 is the residual user-item preference matrix that needs to be learned. We calculate the user-item similarity matrix S as:</p><formula xml:id="formula_3">s 𝑎𝑖 = 𝜎 ( &lt; p 𝑎 × W 1 , q 𝑖 × W 2 &gt; |p 𝑎 × W 1 | |q 𝑖 × W 2 | ),<label>(3)</label></formula><p>where 𝜎 (𝑥) is a sigmoid function that transforms the computed similarity into range (0, 1), W 1 and W 2 are two trainable matrices that map user/item representations from free latent space into similarity space, and &lt;, &gt; denotes vector inner product operation. From Eq. ( <ref type="formula" target="#formula_3">3</ref>), we have the learned user-item similarity matrix. However, the learned similarity matrix S is dense and hard to be used for graph convolution. Similar to many graph construction models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39]</ref>, we sparse the learned similarity matrix for residual graph construction. Generally, there are two kinds of methods for graph sparseness: L1 normalization <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and threshold interception <ref type="bibr" target="#b38">[39]</ref>. In this paper, we use threshold interception as it can flexibly control the edges of the learned graph. In practice, for each user, we retain the edges with top-K computed similarities. The sparsified similarity matrix is calculated as follows:</p><formula xml:id="formula_4">𝑠 𝑎𝑖 = 𝑠 𝑎𝑖 , 𝑠 𝑎𝑖 ∈ 𝑡𝑜𝑝𝐾 (𝑠 𝑎 ) 0, 𝑠 𝑎𝑖 ∉ 𝑡𝑜𝑝𝐾 (𝑠 𝑎 ),<label>(4)</label></formula><p>where 𝑠 𝑎 = [𝑠 𝑎1 , ..., 𝑠 𝑎𝑁 ] is the learned similarity vector of each user based on Eq.( <ref type="formula" target="#formula_3">3</ref>).</p><p>There may exist a question that why we use the free user and item embedding matrices (i.e., P and Q) instead of the final user and item embedding matrices (i.e.,U and V ) for residual graph weight matrix learning. In practice, we also try to use the final embedding matrices for residual graph learning, but it does not perform as well as Eq.(3). We guess a possible reason is that, the final embedding matrices are injected into the node-centric highorder graph structure. By using the final node embeddings, most of the non-zero s 𝑖 𝑗 values are from the users (items) that already have links in the original graph. If we exclude the original fixed graph structure for finding the possibly unlinked user-item pairs, most of these user-item pairs are close in the graph structure and can already be modeled with graph CF models. In contrast, the free embeddings capture the original characteristics of nodes for updated graph learning, and are complementary to the final node embeddings that already modeled the current graph structure.</p><p>Please note that the edge weights A in the original graph are equal to one but vary in the enhanced graph with weight matrix A 𝐸 . The reason is that the learned residual graph has two kinds of edges: one is the old edges that already appear in the original graph and another is the newly added edges compared to the original graph. Thus, in the enhanced graph with residual graph structure, old edges are reweighted with weight value larger than 1 while new edges are weighted with value less than 1. This indicates that our enhanced graph could be able to add missing edges and reweight the existing edges simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Embedding Learning with Enhanced Graph Structure</head><p>Given the enhanced graph structure with weight matrix A 𝐸 , the embedding learning module tries to learn better user and item embedding with enhanced graph structure as:</p><formula xml:id="formula_5">[P, Q, U, V] = 𝐺𝐶𝐹 (A 𝐸 ).</formula><p>In the following, we introduce the architecture of the node embedding learning module.</p><p>GCNs are state-of-the-art techniques in representation learning, which encode local graph structure into node representation <ref type="bibr" target="#b17">[18]</ref>. Extensive works show the effectiveness of GCNs on graph based recommendations <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38]</ref>. Therefore, we also use GCN as encoder and feed the enhanced graph G 𝐸 into the encoder to generate node representations.</p><p>Generally, given the initial user/item embedding matrix P 0 = P, Q 0 = Q,the enhanced graph weight matrix A 𝐸 and a pre-defined embedding propagation depth 𝐾, graph convolution encoder outputs the final user embedding matrix U and final item embedding matrix V. At (k+1)-th layer, each user's and each item's embeddings are updated with the aggregation of neighbors' embeddings as:</p><formula xml:id="formula_6">p 𝑘+1 𝑎 = 𝐴𝐺𝐺 (p 𝑘 𝑎 , {q 𝑘 𝑗 : 𝑗 ∈ A 𝐸 𝑎 })<label>(5)</label></formula><formula xml:id="formula_7">q 𝑘+1 𝑖 = 𝐴𝐺𝐺 (q 𝑘 𝑖 , {p 𝑘 𝑏 : 𝑏 ∈ A 𝐸 (𝑀+𝑖 ) })<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">𝐴 𝐸 𝑎 = { 𝑗 |𝐴 𝐸 𝑎 𝑗 &gt; 0} ⊆ 𝑉</formula><p>is the item set that user 𝑎 links with, and 𝐴 𝐸 𝑀+𝑖 = {𝑏 |𝐴 𝐸 (𝑀+𝑖 )𝑏 &gt; 0} ⊆ 𝑈 is the user set who linked with item 𝑖. 𝐴𝐺𝐺 () denotes the aggregation operation, which can be implemented with many optional functions, such as concatenation, weighted sum and neural network based aggregation. Traditional GCN methods usually adopt non-linear activation and trainable weight transformation on feature propagation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30]</ref>. However, recent works have demonstrated that non-linear activation and feature transformation are unnecessary and bring unnecessary complexity in graph based CF <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>. Therefore, we only use the key component: neighbor aggregation in graph convolution encoder. Given these findings, we implement Eq.( <ref type="formula" target="#formula_9">7</ref>) and Eq.( <ref type="formula" target="#formula_10">8</ref>) as follows:</p><formula xml:id="formula_9">p 𝑘+1 𝑎 = p 𝑘 𝑎 + 1 𝑀+𝑁 −1 𝑗=0 A 𝐸 𝑎 𝑗 𝑗 ∈A 𝐸 𝑎 A 𝐸 𝑎 𝑗 q 𝑘 𝑗 ,<label>(7)</label></formula><formula xml:id="formula_10">q 𝑘+1 𝑖 = q 𝑘 𝑖 + 1 𝑀+𝑁 −1 𝑏=0 A 𝐸 (𝑀+𝑖)𝑏 𝑏 ∈A 𝐸 𝑀+𝑖 A 𝐸 (𝑀+𝑖)𝑏 p 𝑘 𝑏 .<label>(8)</label></formula><p>Given the pre-defined embedding propagation depth 𝐾, we could obtain the final user embedding matrix U = P 𝐾 and final item embedding matrix V = Q 𝐾 . After that, the preference of user 𝑎 to item 𝑖 can be predicted as follows:</p><formula xml:id="formula_11">rai =&lt; u 𝑎 , v 𝑖 &gt;,<label>(9)</label></formula><p>where &lt;, &gt; denotes vector inner product operation. In order to illustrate the embedding propagation process more clearly and facilitate the batch implementation, we formulate Eq.( <ref type="formula" target="#formula_9">7</ref>) and Eq.( <ref type="formula" target="#formula_10">8</ref>) a matrix form. Let matrix P 𝑘 and matrix Q 𝑘 denote the embedding matrices of users and items after k-th propagation, then the updated embedding matrices on (k+1)-th propagation are formulated as follows:</p><formula xml:id="formula_12">P 𝑘+1 Q 𝑘+1 = ( P 𝑘 Q 𝑘 + D −1 A 𝐸 × P 𝑘 Q 𝑘 ),<label>(10)</label></formula><p>where k=0,1,2,...,K-1 (K is the pre-defined propagation depth). 𝐷 is the degree matrix of the enhanced graph with weight matrix A 𝐸 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">OPTIMIZATION DESIGN AND MODEL TRAINING</head><p>In this part, we first introduce the overall optimization function design for enhanced graph learning with mutual information maximization. After that, we illustrate the model training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mutual Information Maximization for</head><p>Enhanced Graph Learning </p><formula xml:id="formula_13">Θ 𝐺 L 𝑠 = ||A − A 𝑅 || 2 𝐹 (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>where</p><formula xml:id="formula_15">Θ 𝐺 = [P, Q, W 1 , W 2 ]</formula><p>are the parameters in graph learning.</p><p>The above Euclidean distance minimization constraint encourages the learned residual graph to be close to the original graph from the edgewise level. We treat the edges of the original graph as the ground truth positive feedback. Thus, the learned residual graph should retain these edges. However, this edgewise constraint only learns the individual link correlation but fail to capture the global graph properties. In the following, we introduce the global graph properties through local-global consistency learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Constraint on Graph</head><p>Level. Given the enhanced graph G 𝐸 = {𝑈 ∪ 𝑉 , A 𝐸 }, for each edge (𝑢 𝑎 , 𝑣 𝑖 ), we summarize the sub-graph centered around the user-item pair as the local representations.</p><p>In practice, we use the final user and item embeddings output by the node embedding learning module in Section 3.3 as the local representations:</p><formula xml:id="formula_16">h 𝑎𝑖 = [𝜎 (u 𝑎 ), 𝜎 (v 𝑖 )],<label>(12)</label></formula><p>where 𝜎 is a sigmoid activation function. The combination method we adopted is concatenation. After obtaining the local representations, we aim to seek the global representations to capture the information of the entire graph. Similar to existing representation learning works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>, we obtain the global representations g by leveraging a readout function: R 𝑍 ×2𝐷 → R 2𝐷 , where 𝑍 is the number of edges on the enhanced graph G 𝐸 . The detailed process is shown as follows:</p><formula xml:id="formula_17">𝑍 = 𝑀−1 𝑎=0 𝑀+𝑁 −1 𝑖=𝑀 𝑠𝑖𝑔𝑛(A 𝐸 𝑎𝑖 ),<label>(13) g</label></formula><formula xml:id="formula_18">= [𝑎,𝑖 ] ∈ G 𝐸 h 𝑎𝑖 𝑍 ,<label>(14)</label></formula><p>where 𝑠𝑖𝑔𝑛(𝑥) is logical function, which equals to 1 if 𝑥 &gt;0, equals to 0 if 𝑥 = 0 and equals to -1 if 𝑥 &lt;0. After obtaining the local representations and global representations, we then maximize the mutual information between them to keep the local-global consistency. Similar to many existing works about mutual information maximization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref>, we employ a discriminator D to compute the score that assigned to a &lt;local,global&gt; pair with a bilinear mapping function:</p><formula xml:id="formula_19">D (h, g) = h T W d g<label>(15)</label></formula><p>where W d ∈ R 2𝐷×2𝐷 is a transformation weight matrix. Without confusion, we use vector h to denote the local representations of any edge on the enhanced graph G 𝐸 . Then, we combine the local representations h and the global representations g as positive sample [h, g] for the discriminator D.</p><p>In order to perform contrastive learning for the discriminator D, we need negative samples for discriminator optimization. Let F denote node embedding matrix of graph G 𝐸 , then G 𝐸 can be described as (A 𝐸 , F). We implement three kinds of negative samplings [ h, g] from data augment perspective. Fake Edge. Given the enhanced graph (A 𝐸 , F), we randomly sample a fake edge (𝑢 𝑎 , 𝑣 𝑗 ), where A 𝐸 𝑎 𝑗 = 0. Then, the local representations h of fake edge, and the global representations g are combined as a negative sample. Feature shuffling. It generates a corrupted graph (A 𝐸 , F) by randomly shuffling a certain percentage of features. Negative samples are achieved by pairing local representations h from (A 𝐸 , F) and global representations g from (A 𝐸 , F). Structure perturbation. It generates a corrupted graph ( Ã𝐸 , F) by randomly adding or dropping certain ratio of edges. The local representations h from ( Ã𝐸 , F) combine the global representations g from (A 𝐸 , F) as negative samples for the discriminator D.</p><p>After obtaining positive and negative samples, the positive samples are labeled by 1 and the negative samples are labeled by 0. The local-global consistency mutual information maximization tries to correctly discriminate positive and negative local-global pairs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>. Therefore, we use binary cross entropy loss as L 𝑑 : arg min</p><formula xml:id="formula_20">Θ 𝐷 L 𝑑 = − 𝑍 −1 𝑧=0 𝑙𝑜𝑔D (h, g) + (1 − 𝑙𝑜𝑔D ( h, g)) 𝑍 (<label>16</label></formula><formula xml:id="formula_21">)</formula><p>where Θ 𝐷 = W 𝑑 is the discriminator parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Training</head><p>For rating prediction, we employ the pairwise ranking based BPR loss <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>, which assumes that the observed items' prediction values should be higher than those unobserved. The objective function can be formulated as follows:</p><p>arg min</p><formula xml:id="formula_22">Θ 𝐺 L 𝑟 = 𝑀−1 𝑎=0 (𝑖,𝑗) ∈𝐷 𝑎 −ln𝜎 ( r𝑎𝑖 − r𝑎𝑗 ) + 𝜆||P|| 2 + 𝜆||Q|| 2 , (<label>17</label></formula><formula xml:id="formula_23">)</formula><p>where 𝜎 (𝑥) is a sigmoid function,</p><formula xml:id="formula_24">Θ 𝐺 = [P, Q, W 1 , W 2 ] are graph learning parameters, 𝜆 is the regularization coefficient. 𝐷 𝑎 = { (𝑖, 𝑗) |𝑖 ∈</formula><p>𝑅 𝑎 ∧ 𝑗 ∉ 𝑅 𝑎 } denotes the pairwise training data for user 𝑎. 𝑅 𝑎 represents the item set that user 𝑎 has interacted. Given rating loss L 𝑟 (Eq.( <ref type="formula" target="#formula_22">17</ref>)), the local graph learning based loss L 𝑠 (Eq.( <ref type="formula" target="#formula_13">11</ref>)) and the global graph learning based loss L 𝑑 (Eq.( <ref type="formula" target="#formula_20">16</ref>)), we combine them as the final optimization. We set two parameters 𝛼 and 𝛽 to balance these three losses, respectively. Finally, the overall objective function can be formulated as follows: </p><formula xml:id="formula_25">L = L 𝑟 + 𝛼 L 𝑠 + 𝛽 L 𝑑 ,<label>(18)</label></formula><p>where Θ = [Θ 𝐺 , Θ 𝐷 ] are all parameters in the final objective function. We implement the proposed model with TensorFlow<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we conduct extensive experiments on three realworld datasets to evaluate the effectiveness our proposed EGLN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Datasets Description.</head><p>To evaluate the effectiveness of our proposed model, we conduct experiments on three public datasets: Movielens-1M <ref type="bibr" target="#b9">[10]</ref>, Amazon-Video Games <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37]</ref>, and Pinterest <ref type="bibr" target="#b7">[8]</ref>. For Movielens-1M, we transform explicit ratings into implicit feedback, where each entry is viewed as 1 only when rating equals 5. For Amazon-Video Games, we adopt the processed dataset <ref type="bibr" target="#b36">[37]</ref> which each user have at least 5 records. For Pinterest, we also use the processed dataset <ref type="bibr" target="#b12">[13]</ref> which each user have at least 20 records. After data pre-processing, we randomly split historical interactions into training, validation, and test parts with a certain ratio(7:1:2 on Movielens-1M and 8:1:1 on Amazon-Video Games and Pinterest).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Baselines and Evaluate Metrics.</head><p>We compare our proposed model with several competing baselines which could be categorized into three groups. The first is classical matrix factorization based method BPR <ref type="bibr" target="#b25">[26]</ref>. The second is neural graph based CF models with fixed graph structure, including NGCF <ref type="bibr" target="#b32">[33]</ref>, BiGI <ref type="bibr" target="#b2">[3]</ref>, LR-GCCF <ref type="bibr" target="#b3">[4]</ref> and LightGCN <ref type="bibr" target="#b11">[12]</ref>. The last, we compare our model with three graph learning based models, including GAT <ref type="bibr" target="#b30">[31]</ref>, DropEdge <ref type="bibr" target="#b26">[27]</ref> and GLCN <ref type="bibr" target="#b15">[16]</ref>. GAT is an efficient graph neural network with edges reweighting by self-attention mechanism. DropEdge is a robust graph learning method by randomly removing edges. GLCN leverages the input features to graph reconstruction. In practice, we use the learned embeddings from the strongest baseline (LightGCN) as the input features.</p><p>As we focus on ranking and recommending Top-K items to users, we adopt two widely used ranking metrics: Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG). Specifically, HR measures the number of successfully predicted items in the top-N ranking list that the user likes in the test data. NDCG considers the hit positions of the items and will give a higher score if the hit items are in the top positions. In order to reduce the randomness caused by negative samplings, we select all unrated items as negative samples for each user, and combine them with the positive items that the user likes in the ranking process. For each model, we repeat experiments 10 times and report the average results. For all embedding based models, the free embedding dimension is fixed as 32, and we initialize the embedding matrices with a Gaussian Distribution with the mean value of 0 and the standard variance of 0.01. For those gradient descent-based methods, we use Adam as the optimizing method with a suitable initial learning rate. We stop the model learning process when the performance decreases in the validation data. Similar to many graph based CF models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33]</ref>, we set the embedding propagation depth 𝐾 in range of {0,1,2,3,4}, and analyze its impact on the experiment results. Since we have pair-wise ranking based loss, for each observed user-item interaction, we randomly select one unobserved item as candidate negative sample to compose a triple data for training. There are several other parameters in the baselines, we tune all these parameters to ensure the best performance of the baselines for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overall Performance Comparisons</head><p>The overall performance on top-k recommendation as shown in Table2, Table3 and Table4, we report HR@K and NDCG@K values of various models on three datasets. We observe that all graph based models outperform BPR since they encode user/item embeddings with high-order graph structure information, which considerably alleviate the interaction sparsity issue. Apart from comparing the performances on various graph based CF models with fixed structure, we also compare several advanced graph learning based models. For all graph learning based models, we use the same neighbors aggregation mechanism as LightGCN for fair comparisons. In practice, we find that although GAT shows  a better performance than GCN on semi-supervised node classification, it does not show superiority compared with LightGCN on recommendation task. We speculate the possible reason is that the user-item bipartite graph is too sparse, which is not suitable for attentive weight learning. DropEdge is also weaker than Light-GCN, since all observed interactions are strong positive signals for model optimization and dropping edges are not effective in recommendation scenarios. GLCN achieves the best performance among these graph learning based models, we revise the weight matrix with the node embeddings learned from LightGCN. However, this method only performs graph learning once, which can not capture the better graph structure for recommendation.</p><p>Comparing with all the baselines, we empirically find that our proposed EGLN consistently shows the best performance on all the datasets. The detailed improvement rate varies across different datasets, but the same overall trend is shared. E.g., EGLN improves 4.70% and 5.36% over the strongest baseline(LightGCN) of HR@10 and NDCG@10 on Amazon-Video Games dataset. It demonstrates the effectiveness of adaptive graph learning to learn better node embeddings to facilitate recommendation. Compared with graph based CF with fixed graph structure, EGLN captures the false negative edges on the graph and revises the input graph to better serve CF. Compared to GAT, EGLN reweights edges weights with the learned residual graph instead of self-attention. Compared to Dropedge, EGLN revises the graph structure by capturing the missing edges instead of randomly dropping edges. Compared to GLCN, EGLN achieves the revised graph structure by iteratively performing residual graph learning and node embedding learning modules. The above analysis detailed illustrates the reasons why EGLN shows a better performance. Later, we give an ablation study to investigate the role each component plays in EGLN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study of EGLN</head><p>As shown in Table <ref type="table" target="#tab_7">5</ref>, we report HR@10 and NDCG@10 of EGLN and it's simplified version. Specifically, EGLN -E denotes we implement EGLN with fixed graph (𝛼 = 𝛽 = 0). EGLN -M denotes simplified EGLN without the constraint of mutual information maximization (𝛽 = 0). Besides, EGLN -FE denotes EGLN under fake edge, EGLN -FS denotes EGLN under feature shuffling, EGLN -SP denotes EGLN under structure perturbation. Compared with EGLN -E, EGLN -M achieves obviously improvements on all three datasets, which verifies the effectiveness of the enhanced graph learning. EGLN -FE, EGLN -FS, and EGLN -SP all show better performances than EGLN -M, indicating that the local-global consistency optimization is effective to enhance graph learning. In practice, we try three implementations of EGLN , and find EGLN -FE yields the best performance on Movielens-1M and Amazon-Video Games datasets, and EGLN -FS shows the best performance on Pinterest datasets. We speculate the possible reason is that Pinterest dataset contains more interaction records, so fake edge can not provide same weight signals for graph learning as same as another two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance under Different Data Sparsity</head><p>The interaction sparsity issue usually limits the capability of CF, since precise user preference modeling needs sufficient interaction data. To this end, we conduct experiments to explore the performance of various models under different data sparsity. We divide all test users into several groups based on their observed interactions in training data. Taking the Movielens-1M dataset as the example, we split users into five groups, the interaction numbers for each user are more than 0, 8, 16, 32 and 64, respectively. Figure <ref type="figure" target="#fig_1">2</ref> illustrates NDCG@10 of different groups on Movielens-1M, Amazon-Video  Games and Pinterest datasets. We observe that all models' performances increase when the count of interactions increase, which means high quality user representation needs more interactions. In general, our proposed EGLN shows the best performance on most groups but fails in the densest group. We guess the reason is that CF models could achieve good performance with sufficient interactions, so our proposed enhanced graph learning module is not required in this scenes. Besides, we also find that EGLN achieves more improvements on sparse groups than dense groups(e.g., 17.754% and 6.674% improvements over BPR on the sparsest and the densest group on Movielens-1M dataset). This indicates EGLN is more beneficial to sparse users, because EGLN introduces weak supervised signals by adding missing edges which not appear in the input graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Detailed Model Analysis</head><p>In this part, we first analyze the effect of different embedding propagation depth 𝐾. Followed, we analyze the parameters sensitivities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.5.1</head><p>The Propagation Layer Depth. In order to investigate the effect of multiple propagation layers, we search the propagation depth 𝐾 in {0,1,2,3,4}. Please note that, when K=0, the graph convolution part disappears, our model degenerates to BPR. As shown in Table <ref type="table" target="#tab_8">6</ref>, we summarize the experimental results on different propagation layers and compare the performance improvements over BPR. When K increases from 0 to 1, the performance increases quickly on three datasets, showing that the embedding propagation part effectively alleviates the data sparsity issue. As K continues to increase, we find the performance increases at first, and then drops after a certain value. Specifically, our model reaches the best performance with K=2 in Movielens-1M, K=3 in Amazon-Video Games, and K=4 in Pinterest, respectively. The reason is that Amazon-Video Games and Pinterest datasets have more sparse interactions compared to Movielens-1M. For the sparse dataset, deeper graph convolution could help to aggregate more neighbors, which benefits the representation learning. However, for the dense dataset, too deeper propagation layer will easily lead to over-smoothing on the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Parameters Sensitivities.</head><p>Here we analyze the performances of EGLN with different hyper-parameters. Limited to the space, we only show the results on Movielens-1M dataset. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, we illustrate HR@10 and NDCG@10 values with three kinds of hyper-parameters: similarity constraint coefficient 𝛼, mutual objective coefficient 𝛽 and regularization coefficient 𝜆. We observe that EGLN achieves the best performance with 𝛼 = 0.1, 𝛽 = 0.1 and 𝜆 = 1𝑒 − 4. For the regularization coefficient 𝜆, we observe that the performance increases when 𝜆 increases from 0 to 1𝑒 − 4 and decreases quickly when 𝜆 is larger than 1𝑒 − 3. It indicates that suitable regularization could effectively prevent over-fitting issues, however too strong regularization will restrict the model optimization. Same as 𝜆, for the objective balance parameters ℎ𝑎 and 𝛽, the selections of appropriate parameters are also important for overall objective optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we argued that previous graph based CF with fixed graph structure is sub-optimal for user/item embedding learning. Therefore, we proposed an enhanced graph learning network (EGLN ) approach for better serving CF. To revise the enhanced graph structure, EGLN was designed by two folds: First, the enhanced graph learning module and the node embedding module iteratively learned from each other without any feature input. Second, we designed a local-global consistency optimization function to let the enhanced graph capture the global properties in the graph learning process. Finally, extensive experimental results on three real-world datasets clearly demonstrated the superiority and effectiveness of our proposed model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall framework of our proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Item recommendation performance under different user group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Item recommendation performance with different hyper-parameters on Movielens-1M dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Most of the</figDesc><table><row><cell>Session 1B: Recommendation 1</cell><cell>SIGIR '21, July 11-15, 2021, Virtual Event, Canada</cell></row><row><cell>above models only perform graph learning at once. As better graph</cell><cell></cell></row><row><cell>structure can help better node embedding learning, and vice versa,</cell><cell></cell></row><row><cell>an iterative deep graph learning model is proposed to let graph</cell><cell></cell></row><row><cell>learners and downstream tasks enhance each other</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The statistics of three datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Users Items Ratings Sparsity</cell></row><row><cell>Movielens-1M</cell><cell>6040</cell><cell>3952</cell><cell>226310 99.052%</cell></row><row><cell cols="4">Amazon-Video Games 31,207 33,899 300003 99.972%</cell></row><row><cell>Pinterest</cell><cell>55187</cell><cell cols="2">9916 1500809 99.726%</cell></row><row><cell>arg min Θ</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>HR@k and NDCG@k comparisons for Movielens-1M dataset.</figDesc><table><row><cell>Models</cell><cell>k=5</cell><cell>k=10</cell><cell cols="2">HR@k k=15 k=20</cell><cell>k=25</cell><cell>k=30</cell><cell>k=5</cell><cell>k=10</cell><cell cols="2">NDCG@k k=15 k=20</cell><cell>k=25</cell><cell>k=30</cell></row><row><cell>BPR</cell><cell>0.14949</cell><cell>0.20061</cell><cell>0.24543</cell><cell>0.28940</cell><cell>0.32519</cell><cell>0.35469</cell><cell>0.13634</cell><cell>0.15521</cell><cell>0.17133</cell><cell>0.18567</cell><cell>0.19667</cell><cell>0.20559</cell></row><row><cell>NGCF</cell><cell>0.15477</cell><cell>0.21055</cell><cell>0.26015</cell><cell>0.30107</cell><cell>0.33670</cell><cell>0.36503</cell><cell>0.14149</cell><cell>0.16207</cell><cell>0.17950</cell><cell>0.19293</cell><cell>0.20404</cell><cell>0.21259</cell></row><row><cell>BiGI</cell><cell>0.15831</cell><cell>0.21225</cell><cell>0.25930</cell><cell>0.29936</cell><cell>0.33500</cell><cell>0.36708</cell><cell>0.14521</cell><cell>0.16483</cell><cell>0.18124</cell><cell>0.19435</cell><cell>0.20531</cell><cell>0.21477</cell></row><row><cell>LR-GCCF</cell><cell>0.15932</cell><cell>0.21160</cell><cell>0.26004</cell><cell>0.29888</cell><cell>0.33284</cell><cell>0.36290</cell><cell>0.14696</cell><cell>0.16595</cell><cell>0.18283</cell><cell>0.19575</cell><cell>0.20654</cell><cell>0.21569</cell></row><row><cell cols="2">LightGCN 0.16597</cell><cell>0.22381</cell><cell>0.27327</cell><cell>0.31322</cell><cell>0.34957</cell><cell>0.37971</cell><cell>0.15355</cell><cell>0.17439</cell><cell>0.19191</cell><cell>0.20511</cell><cell>0.21645</cell><cell>0.22563</cell></row><row><cell>GAT</cell><cell>0.16025</cell><cell>0.21813</cell><cell>0.26682</cell><cell>0.31148</cell><cell>0.34820</cell><cell>0.37818</cell><cell>0.14613</cell><cell>0.16777</cell><cell>0.18487</cell><cell>0.19955</cell><cell>0.21099</cell><cell>0.22021</cell></row><row><cell cols="2">DropEdge 0.16010</cell><cell></cell><cell>0.26509</cell><cell>0.30735</cell><cell>0.34566</cell><cell>0.37785</cell><cell>0.14517</cell><cell>0.16609</cell><cell>0.18326</cell><cell>0.19725</cell><cell>0.20910</cell><cell>0.21887</cell></row><row><cell>GLCN</cell><cell>0.16105</cell><cell>0.21989</cell><cell>0.26871</cell><cell>0.31301</cell><cell>0.34752</cell><cell>0.38025</cell><cell>0.14898</cell><cell>0.17080</cell><cell>0.18801</cell><cell>0.20246</cell><cell>0.21338</cell><cell>0.22314</cell></row><row><cell>EGLN</cell><cell cols="12">0.16990 0.22961 0.27905 0.31680 0.35071 0.38335 0.15657 0.17842 0.19565 0.20834 0.21905 0.22857</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>HR@k and NDCG@k comparisons for Amazon-Video Games dataset.</figDesc><table><row><cell>Models</cell><cell>k=5</cell><cell>k=10</cell><cell cols="2">HR@k k=15 k=20</cell><cell>k=25</cell><cell>k=30</cell><cell>k=5</cell><cell>k=10</cell><cell cols="2">NDCG@k k=15 k=20</cell><cell>k=25</cell><cell>k=30</cell></row><row><cell>BPR</cell><cell>0.04036</cell><cell>0.06758</cell><cell>0.08729</cell><cell>0.10208</cell><cell>0.11610</cell><cell>0.12741</cell><cell>0.02985</cell><cell>0.03815</cell><cell>0.04372</cell><cell>0.04744</cell><cell>0.05069</cell><cell>0.05319</cell></row><row><cell>NGCF</cell><cell>0.05360</cell><cell>0.08268</cell><cell>0.10739</cell><cell>0.12575</cell><cell>0.14194</cell><cell>0.15680</cell><cell>0.03693</cell><cell>0.04680</cell><cell>0.05375</cell><cell>0.05839</cell><cell>0.06217</cell><cell>0.06544</cell></row><row><cell>BiGI</cell><cell>0.05587</cell><cell>0.08644</cell><cell>0.11002</cell><cell>0.12871</cell><cell>0.14662</cell><cell>0.16213</cell><cell>0.03893</cell><cell>0.04930</cell><cell>0.05591</cell><cell>0.06062</cell><cell>0.06479</cell><cell>0.06820</cell></row><row><cell>LR-GCCF</cell><cell>0.05710</cell><cell>0.08716</cell><cell>0.11064</cell><cell>0.13127</cell><cell>0.14827</cell><cell>0.16383</cell><cell>0.03995</cell><cell>0.05018</cell><cell>0.05680</cell><cell>0.06195</cell><cell>0.06591</cell><cell>0.06936</cell></row><row><cell cols="2">LightGCN 0.05957</cell><cell>0.09316</cell><cell>0.11709</cell><cell>0.13783</cell><cell>0.15489</cell><cell>0.17145</cell><cell>0.04149</cell><cell>0.05284</cell><cell>0.05955</cell><cell>0.06478</cell><cell>0.06877</cell><cell>0.07242</cell></row><row><cell>GAT</cell><cell>0.05614</cell><cell>0.08843</cell><cell>0.11134</cell><cell>0.13070</cell><cell>0.14753</cell><cell>0.16254</cell><cell>0.03918</cell><cell>0.05009</cell><cell>0.05656</cell><cell>0.06142</cell><cell>0.06536</cell><cell>0.06867</cell></row><row><cell cols="2">DropEdge 0.05365</cell><cell>0.08394</cell><cell>0.10571</cell><cell>0.12417</cell><cell>0.13969</cell><cell>0.15492</cell><cell>0.03717</cell><cell>0.04742</cell><cell>0.05359</cell><cell>0.05825</cell><cell>0.06189</cell><cell>0.06525</cell></row><row><cell>GLCN</cell><cell>0.05954</cell><cell>0.09069</cell><cell>0.11530</cell><cell>0.13546</cell><cell>0.15269</cell><cell>0.16845</cell><cell>0.04153</cell><cell>0.05211</cell><cell>0.05900</cell><cell>0.06410</cell><cell>0.06812</cell><cell>0.07157</cell></row><row><cell>EGLN</cell><cell cols="12">0.06414 0.09754 0.12189 0.14289 0.16023 0.17633 0.04433 0.05567 0.06253 0.06781 0.07189 0.07544</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>HR@k and NDCG@k comparisons for Pinterest dataset.</figDesc><table><row><cell>Models</cell><cell>k=5</cell><cell>k=10</cell><cell cols="2">HR@k k=15 k=20</cell><cell>k=25</cell><cell>k=30</cell><cell>k=5</cell><cell>k=10</cell><cell cols="2">NDCG@k k=15 k=20</cell><cell>k=25</cell><cell>k=30</cell></row><row><cell>BPR</cell><cell>0.04839</cell><cell>0.08323</cell><cell>0.11308</cell><cell>0.13879</cell><cell>0.16185</cell><cell>0.18311</cell><cell>0.04293</cell><cell>0.05874</cell><cell>0.06993</cell><cell>0.07854</cell><cell>0.08570</cell><cell>0.09191</cell></row><row><cell>NGCF</cell><cell>0.04906</cell><cell>0.08399</cell><cell>0.11290</cell><cell>0.13908</cell><cell>0.16193</cell><cell>0.18335</cell><cell>0.04380</cell><cell>0.05966</cell><cell>0.07050</cell><cell>0.07929</cell><cell>0.08638</cell><cell>0.09263</cell></row><row><cell>BiGI</cell><cell>0.05003</cell><cell>0.08471</cell><cell>0.11441</cell><cell>0.14004</cell><cell>0.16464</cell><cell>0.18563</cell><cell>0.04457</cell><cell>0.06026</cell><cell>0.07139</cell><cell>0.07997</cell><cell>0.08760</cell><cell>0.09373</cell></row><row><cell>LR-GCCF</cell><cell>0.05062</cell><cell>0.08566</cell><cell>0.11469</cell><cell>0.14085</cell><cell>0.16416</cell><cell>0.18616</cell><cell>0.04516</cell><cell>0.06104</cell><cell>0.07192</cell><cell>0.08065</cell><cell>0.08787</cell><cell>0.09430</cell></row><row><cell cols="2">LightGCN 0.05472</cell><cell>0.09160</cell><cell>0.12354</cell><cell>0.15033</cell><cell>0.17550</cell><cell>0.19775</cell><cell>0.04917</cell><cell>0.06590</cell><cell>0.07785</cell><cell>0.08680</cell><cell>0.09461</cell><cell>0.10112</cell></row><row><cell>GAT</cell><cell>0.05170</cell><cell>0.08953</cell><cell>0.12067</cell><cell>0.14807</cell><cell>0.17313</cell><cell>0.19592</cell><cell>0.04581</cell><cell>0.06287</cell><cell>0.07452</cell><cell>0.08368</cell><cell>0.09147</cell><cell>0.09813</cell></row><row><cell cols="2">DropEdge 0.05390</cell><cell>0.09203</cell><cell>0.12391</cell><cell>0.15168</cell><cell>0.17668</cell><cell>0.20027</cell><cell>0.04762</cell><cell>0.06484</cell><cell>0.07678</cell><cell>0.08607</cell><cell>0.09383</cell><cell>0.10073</cell></row><row><cell>GLCN</cell><cell>0.05409</cell><cell>0.09366</cell><cell>0.12596</cell><cell>0.15409</cell><cell>0.17979</cell><cell>0.20323</cell><cell>0.04785</cell><cell>0.06573</cell><cell>0.07780</cell><cell>0.08724</cell><cell>0.09520</cell><cell>0.10205</cell></row><row><cell>EGLN</cell><cell cols="12">0.05593 0.09468 0.12704 0.15537 0.18112 0.20393 0.05003 0.06756 0.07965 0.08914 0.09713 0.10381</cell></row><row><cell cols="2">5.1.3 Parameter Settings.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation results of our proposed model (EGLN -E denote EGLN with fixed graph, EGLN -M denote EGLN without mutual infomax, EGLN -FE denote EGLN under fake edge, EGLN -FS denote EGLN under feature shuffling, EGLN -SP denote EGLN under structure perturbation).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Models</cell><cell cols="3">Movielens-1M HR@10 NDCG@10</cell><cell></cell><cell cols="4">Amazon-Video Games HR@10 NDCG@10</cell><cell></cell><cell cols="2">HR@10</cell><cell>Pinterest NDCG@10</cell></row><row><cell></cell><cell></cell><cell cols="2">EGLN -E</cell><cell>0.21507(-)</cell><cell></cell><cell>0.16650(-)</cell><cell></cell><cell cols="2">0.09104(-)</cell><cell></cell><cell>0.05171(-)</cell><cell></cell><cell cols="2">0.09190(-)</cell><cell>0.06440(-)</cell></row><row><cell></cell><cell></cell><cell cols="2">EGLN -M</cell><cell>0.22660(+5.36%)</cell><cell></cell><cell>0.17424(+4.65%)</cell><cell cols="3">0.09392(+3.16%)</cell><cell cols="2">0.05304(+2.57%)</cell><cell cols="4">0.09338(+1.61%)</cell><cell>0.06594(+2.39%)</cell></row><row><cell></cell><cell></cell><cell cols="10">EGLN -FE 0.22961(+6.76%) 0.17842(+7.16%) 0.09754(+7.15%) 0.05567(+7.27%)</cell><cell cols="4">0.09402(+2.31%)</cell><cell>0.06711(+4.21%)</cell></row><row><cell></cell><cell></cell><cell cols="2">EGLN -FS</cell><cell>0.22924(+6.59%)</cell><cell></cell><cell>0.17606(+5.74%)</cell><cell cols="3">0.09570(+5.12%)</cell><cell cols="2">0.05381(+4.06%)</cell><cell cols="4">0.09455(+2.88%)</cell><cell>0.06730(+4.50%)</cell></row><row><cell></cell><cell></cell><cell cols="2">EGLN -SP</cell><cell>0.22520(+4.71%)</cell><cell></cell><cell>0.17435(+4.71%)</cell><cell cols="3">0.09724(+6.81%)</cell><cell cols="2">0.05514(+6.63%)</cell><cell cols="4">0.09468(+3.03%) 0.06756(+4.91%)</cell></row><row><cell></cell><cell>0.25</cell><cell>BPR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>BPR</cell><cell></cell></row><row><cell></cell><cell></cell><cell>NGCF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NGCF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NGCF</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">BiGI LR−GCCF LightGCN</cell><cell></cell><cell></cell><cell>0.08</cell><cell>BiGI LR−GCCF LightGCN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.08</cell><cell cols="2">BiGI LR−GCCF LightGCN</cell></row><row><cell>NDCG@10</cell><cell>0.15 0.20</cell><cell>GAT DropEdge GLCN EGLN</cell><cell></cell><cell></cell><cell></cell><cell>0.04 0.06</cell><cell>GAT DropEdge GLCN EGLN</cell><cell></cell><cell></cell><cell></cell><cell>NDCG@10</cell><cell>0.06</cell><cell cols="2">GAT DropEdge GLCN EGLN</cell></row><row><cell></cell><cell>0.1</cell><cell>[0,8)</cell><cell>[8,16)</cell><cell>[16,32) [32,64)</cell><cell>[64,)</cell><cell>0.02</cell><cell>[0,4)</cell><cell>[4,8)</cell><cell>[8,16)</cell><cell>[16,32)</cell><cell>[32,)</cell><cell>0.04</cell><cell>[8,16)</cell><cell cols="2">[16,24) [24,32) [32,40)</cell><cell>[40,)</cell></row><row><cell></cell><cell></cell><cell cols="4">Num. of Records for Each User (T)</cell><cell></cell><cell cols="5">Num. of Records for Each User (T)</cell><cell></cell><cell cols="3">Num. of Records for Each User (T)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Movielens-1M</cell><cell></cell><cell></cell><cell cols="4">(b) Amazon-Video Games</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) Pinterest</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Performance comparisons of different propagation depth 𝐾 on three datasets.</figDesc><table><row><cell></cell><cell cols="2">Depth K</cell><cell cols="6">Movielens-1M HR@10 NDCG@10</cell><cell></cell><cell cols="5">Amazon-Video Games HR@10 NDCG@10</cell><cell></cell><cell></cell><cell>HR@10</cell><cell>Pinterest</cell><cell>NDCG@10</cell></row><row><cell></cell><cell>K=0</cell><cell></cell><cell cols="2">0.20061(-)</cell><cell></cell><cell cols="3">0.15521(-)</cell><cell></cell><cell>0.06758(-)</cell><cell></cell><cell cols="2">0.03815(-)</cell><cell></cell><cell></cell><cell cols="2">0.08323(-)</cell><cell>0.05874(-)</cell></row><row><cell></cell><cell>K=1</cell><cell></cell><cell cols="3">0.21917(+8.98%)</cell><cell cols="4">0.16899(+8.88%)</cell><cell cols="2">0.08603(+27.30%)</cell><cell cols="3">0.04725(+23.85%)</cell><cell></cell><cell cols="2">0.08745(+5.07%)</cell><cell>0.06198(+5.52%)</cell></row><row><cell></cell><cell>K=2</cell><cell></cell><cell cols="7">0.22961(+14.46%) 0.17842(+15.25%)</cell><cell cols="2">0.09524(+40.77%)</cell><cell cols="3">0.05398(+41.49%)</cell><cell></cell><cell cols="2">0.09165(+10.12%)</cell><cell>0.06518(+10.96%)</cell></row><row><cell></cell><cell>K=3</cell><cell></cell><cell cols="3">0.22689(+13.10%)</cell><cell cols="4">0.17541(+13.01%)</cell><cell cols="5">0.09754(+44.35%) 0.05567(+45.40%)</cell><cell></cell><cell cols="2">0.09292(+11.64%)</cell><cell>0.06607(+12.48%)</cell></row><row><cell></cell><cell>K=4</cell><cell></cell><cell cols="3">0.22514(+12.23%)</cell><cell cols="4">0.17337(+11.70%)</cell><cell cols="2">0.09293(+37.51%)</cell><cell cols="3">0.05273(+38.22%)</cell><cell></cell><cell cols="2">0.09468(+13.50%) 0.06756(+13.77%)</cell></row><row><cell></cell><cell>0.23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.18</cell><cell></cell><cell></cell><cell>0.23</cell><cell></cell><cell></cell><cell></cell><cell>0.179</cell><cell></cell><cell></cell><cell>0.23</cell><cell>0.179</cell></row><row><cell>HR@10</cell><cell>0.22 0.225</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.17 0.175</cell><cell>NDCG@10</cell><cell>HR@10</cell><cell>0.22 0.225</cell><cell></cell><cell></cell><cell></cell><cell>0.173 0.176</cell><cell>NDCG@10</cell><cell>HR@10</cell><cell>0.22 0.225</cell><cell>0.173 0.176</cell><cell>NDCG@10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">HR@10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HR@10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HR@10</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">NDCG@10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NDCG@10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NDCG@10</cell></row><row><cell></cell><cell>0 0.215 0</cell><cell>0.01 0.01</cell><cell>0.05 0.05</cell><cell>0.1 0.1</cell><cell>0.5 0.5</cell><cell>1 1 0.165</cell><cell></cell><cell></cell><cell>0 0.215 0</cell><cell>0.0001 0.001 0.0001 0.001</cell><cell>0.01 0.01</cell><cell>0.1 0.1</cell><cell>1 1 0.17</cell><cell></cell><cell></cell><cell>0 0.215 0</cell><cell>1e−6 1e−6</cell><cell>1e−5 1e−5</cell><cell>1e−4 1e−4</cell><cell>1e−3 1e−3</cell><cell>1e−2 1e−2 0.17</cell></row><row><cell></cell><cell></cell><cell></cell><cell>α</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>β</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>λ</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://www.tensorflow.org</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported in part by grants from the National Natural Science Foundation of China (Grant No. 61972125, U19A2079, 61932009, 61732008, 62006066), the Fundamental Research Funds for the Central Universities, HFUT and the Young Elite Scientists Sponsorship Program by CAST and ISZS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">L2knng: Fast exact k-nearest neighbor graph construction with l2-norm pruning</title>
		<author>
			<persName><forename type="first">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Anastasiu</surname></persName>
		</author>
		<author>
			<persName><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="791" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<idno>ICML. 531-540</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bipartite Graph Embedding via Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xixun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="635" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient k-nearest neighbor graph construction for generic similarity measures</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charikar</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="577" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICML. 1972-1982</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning image and user features for recommendation in social networks</title>
		<author>
			<persName><forename type="first">Xue</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4274" to="4282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<title level="m">The movielens datasets: History and context. TIIS</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>WWW. 173-182</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with graph learning-convolutional networks</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doudou</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Luo</surname></persName>
		</author>
		<idno>CVPR. 11313-11320</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3538" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3546" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
	<note>Qinfeng Shi, and Anton Van Den Hengel</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised Attributed Multiplex Network Embedding</title>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5371" to="5378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pairwise preference regression for cold-start recommendation</title>
		<author>
			<persName><forename type="first">Seung-Taek</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Graph Representation Learning via Graphical Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<idno>WWW. 259-270</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey of collaborative filtering techniques</title>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in artificial intelligence</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01000</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">STAT</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
		<title level="m">Deep graph infomax. ICLR</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A Survey on Neural Recommendation: From Collaborative Filtering to Content and Context Enriched Recommendation</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13030</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">DiffNet++: A Neural Influence and Interest Diffusion Network for Social Recommendation</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Neural Influence Diffusion Model for Social Recommendation</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peijie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint Item Recommendation and Attribute Inference: An Adaptive Graph Convolutional Network Approach</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="679" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruohong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07123</idno>
		<title level="m">Graph-Revised Convolutional Network</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">CommDGI: Community Detection Oriented Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhu</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyong</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1843" to="1852" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
