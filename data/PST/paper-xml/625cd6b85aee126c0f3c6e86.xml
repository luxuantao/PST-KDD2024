<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LaMemo: Language Modeling with Look-Ahead Memory</title>
				<funder ref="#_bgMtNmT">
					<orgName type="full">Guoqiang Institute of Tsinghua University</orgName>
				</funder>
				<funder ref="#_wXA8wcK #_4U6AeTT">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_tacvuSN">
					<orgName type="full">National Science Foundation for Distinguished Young Scholars</orgName>
				</funder>
				<funder ref="#_UhcYscv">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rongsheng</forename><surname>Zhang</surname></persName>
							<email>zhangrongsheng@corp.netease.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhenyu</forename><surname>Yang</surname></persName>
							<email>yangzhenyu@oppo.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhipeng</forename><surname>Hu</surname></persName>
							<email>zphu@corp.netease.com</email>
						</author>
						<author>
							<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<email>aihuang@tsinghua.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">The CoAI group</orgName>
								<orgName type="department" key="dep2">DCST</orgName>
								<orgName type="department" key="dep3">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep4">State Key Lab of Intelligent Technology and Systems</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Fuxi AI Lab</orgName>
								<orgName type="institution">NetEase Inc</orgName>
								<address>
									<settlement>China</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">OPPO Mobile Telecommunications Corp</orgName>
								<address>
									<settlement>Ltd</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LaMemo: Language Modeling with Look-Ahead Memory</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>100 -90 -80 -70 -60 -50 -40 -30 -20 -10</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although Transformers with fully connected self-attentions are powerful to model longterm dependencies, they are struggling to scale to long texts with thousands of words in language modeling. One of the solutions is to equip the model with a recurrence memory. However, existing approaches directly reuse hidden states from the previous segment that encodes contexts in a uni-directional way. As a result, this prohibits the memory to dynamically interact with the current context that provides up-to-date information for token prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo) 1 that enhances the recurrence memory by incrementally attending to the right-side tokens, and interpolating with the old memory states to maintain longterm information in the history. LaMemo embraces bi-directional attention and segment recurrence with an additional computation overhead only linearly proportional to the memory length. Experiments on widely used language modeling benchmarks demonstrate its superiority over the baselines equipped with different types of memory. 2 * Corresponding author 1 We are also inspired by the French word "La M?moire", meaning "the memory".</p><p>2 Source code available at https://github.com/ thu-coai/LaMemo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language modeling is an important task that tests the ability of modeling long-term dependencies by predicting the current token based on the previous context <ref type="bibr" target="#b19">(Mikolov and Zweig, 2012;</ref><ref type="bibr" target="#b18">Merity et al., 2017)</ref>. Recently, Transformer-based language models achieved remarkable performance by enabling direct interaction between long-distance word pairs. However, as the computation overhead grows with the length of the input sequence, Transformers can only process a fixed length segment at a time. To allow long-term information flow across individual segments, existing approaches augment the model with a recurrence memory that stores hidden states computed in previous time steps <ref type="bibr" target="#b7">(Dai et al., 2019)</ref> and their compressions <ref type="bibr" target="#b22">(Rae et al., 2020;</ref><ref type="bibr" target="#b17">Martins et al., 2021)</ref> for the target tokens to attend to.</p><p>One limitation of this approach is that the recurrence memory is only aware of older contexts since they are previously computed to predict the next word from left to right. As a result, distant memory states become outdated and less activated by the current context, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. When humans read or write a document, they maintain a memory that records important information from the past and often refresh them under the current context to keep it up-to-date.</p><p>In this paper, we propose Look-Ahead Memory (LaMemo) where memory states "look ahead" to future time steps by attending to the token representations on their right side to provide up-to-date contextualization. <ref type="foot" target="#foot_0">3</ref> To maintain information from the long-term history, we propose memory interpolation to take both past and future tokens into consideration, which mimics the bi-directional attention. Note that, directly applying bi-directional attention to update the memory representations brings an additional complexity of O(M 2 ) (M is the memory length). This is expensive when the memory is very long. LaMemo incrementally attends to the right and accumulate the weighted attention sum from previous segments to simulate the full attention in only O(M ? N ) complexity (N is the target sequence length), which does not increase the attention complexity of Transformer-XL, namely O(N 2 + M ? N ). We provide an illustration of this mechanism in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Another technique proved to be effective in language modeling is the relative positional encoding <ref type="bibr" target="#b23">(Shaw et al., 2018;</ref><ref type="bibr" target="#b11">Huang et al., 2018;</ref><ref type="bibr" target="#b7">Dai et al., 2019)</ref>, which biases the pair-wise attention score purely based on the relative distance of the two tokens. However its ability to generalize to the attention of the future tokens remains unknown, since both the distance and the direction need to be taken into consideration. In preliminary experiments, we observed the unstability of directly applying the relative positional encoding of <ref type="bibr" target="#b7">Dai et al. (2019)</ref> to this setting. We propose a simple yet effective modification based on <ref type="bibr" target="#b7">Dai et al. (2019)</ref> that disentangles the bias of the relative distance and the attention direction which facilitates the training of LaMemo. We give both theoretical and empirical analysis to the unstability issue and demonstrate the effectiveness of the proposed disentangled relative positional encoding method.</p><p>To sum up, our contributions are as follows:</p><p>(1) We propose LaMemo, a memory mechanism that incrementally attends to the right-side tokens, and interpolates with the old memory, which enables bi-directional interaction with a complexity linear in memory length.</p><p>(2) We propose disentangled relative positional encoding, a simple yet effective solution that disentangles the relative distance and the attention direction that can better generalize to the attention of the future tokens.</p><p>(3) We conduct experiments on standard language modeling benchmarks and demonstrate LaMemo's superiority over various baselines equppied with different types of memory mechanisms, despite some having an access to longer contexts. Comprehensive comparisons show the benefits of learning memory representations contextualized with up-to-date information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transformer for Language Modeling</head><p>A Transformer <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> is composed of multiple layers of identical blocks, including a multi-head self-attention <ref type="bibr" target="#b3">(Bahdanau et al., 2015)</ref> that calculates pair-wise token interaction and a feed-foward layer for position-wise projection with a non-linear activation. Both two modules are followed by residual connections <ref type="bibr" target="#b9">(He et al., 2016)</ref> and layer normalization <ref type="bibr" target="#b1">(Ba et al., 2016)</ref> to facilitate optimization.</p><p>Given the input sequence representations of the current ? -th segment</p><formula xml:id="formula_0">X ? = [x ? +1 , ? ? ? , x ? +N ] ? R N ?d</formula><p>where N is the target sequence length and d is the hidden state size, they are first mapped into queries Q, keys K and values V by learned weight matrix to compute self-attention:</p><formula xml:id="formula_1">Q ? = X ? W q , K ? = X ? W k , V ? = X ? W v ,</formula><p>(1) where W q , W k , W v ? R d?d are learnable projection matrices. To perform multi-head self-attention, Q, K, V are further split into H heads. For simplicity, we only consider the case of a single head. In language modeling, the attention map is always added by a causal mask to avoid information leakage from the future when predicting the next token:</p><formula xml:id="formula_2">C ? ? = Causal-Attn(Q ? , K ? , V ? ) = softmax Q ? K ? ? d V ? ,<label>(2)</label></formula><p>where softmax (?) masks position j &gt; i for the i-th row of the input matrix with -? before taking the softmax. The resulted context representations are concatenated and then projected to the final outputs O ? ? R N ?d with a learnable projection matrix W o ? R d?d . Finally, the self-attention outputs O ? are added by the input representations X ? and fed to the following point-wise non-linear transformation, denoted as f (?):</p><formula xml:id="formula_3">f (x) = LN FFN LN(x) + LN(x) ,<label>(3)</label></formula><p>where LN(?) is the layer normalization and FFN(?) is the feed-forward layer, both of which are applied to each row vector individually. The final output of this Transformer layer is f (O ? + X ? ).</p><p>Outputs of the final layer are projected to the vocabulary to predict Pr(w t |w 1 , ? ? ? , w t-1 ). The joint probability of predicting the whole segment is the product of these conditional factors. The final objective is to maximize the following loglikelihood:</p><formula xml:id="formula_4">log Pr(w) = N t=1 log Pr(w t |w 1 , ? ? ? , w t-1 ). (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recurrence Memory Mechanism</head><p>To enable the Transformer to consider more contextual information from previous segments, <ref type="bibr" target="#b7">Dai et al. (2019)</ref> proposed to augment the Transformer with a recurrence memory which stores the hidden states of previous time steps as extended keys and values, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. Concretely, let us consider a memory length of M and memory representations</p><formula xml:id="formula_5">X ? -1 = [x ? -M +1 , ? ? ? , x ? ] ? R M ?d .</formula><p>The extended key and value matrices are obtained by prepend X ? -1 to X ? before projection:</p><formula xml:id="formula_6">Xsg ? = [sg(X ? -1 ) ? X ? ] ? R (M +N )?d ,<label>(5)</label></formula><p>where sg(?) stands for stop-gradient which disables gradient propagation to previous segments, and</p><formula xml:id="formula_7">[? ? ?]</formula><p>indicates concatenation of hidden states along the length dimension. Extended by the recurrence memory, each query vector can consider contexts even beyond the total context length of the attention M + N . As illustrated by <ref type="bibr" target="#b7">Dai et al. (2019)</ref>, the effective context length grows linearly to the number of layers and the attention context length due to layer-wise reusing.</p><p>Another technique necessary to the recurrence memory is the relative positional encodings. By considering only the relative distance between two tokens when computing the attention score, it avoids temporal confusion caused by indexing the same position across segments and injects useful relative bias. Transformer-XL uses the fixed sinusoidal encoding matrix <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> to provide relative distance bias and learns global bias terms shared across different layers, which can extrapolate to longer contexts with a great reduction of parameters compared to <ref type="bibr" target="#b23">Shaw et al. (2018)</ref>:</p><formula xml:id="formula_8">A xl i,j = X i W q W E k X j + X i W q W R k R i-j + u W E k X j + v W R k R i-j ,<label>(6)</label></formula><p>where R is the sinusoid encoding matrix, u, v are learnable weight vectors governing the global content and position bias, and W E k , W R k are separate key projection matrices for the content and position respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we describe our method in detail with our motivation to learn better representations for the memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Look-Ahead Attention</head><p>Human language is sequential with one word following another, but humans process information usually in a non-sequential way and recontextualize certain contents for several times. For example, when countering complicated contents during reading, humans usually first store them temporarily in the memory and continue to scan for relevant information if any, and revisit those old contents to refresh their meaning quite often. This dynamic memory refreshing mechanism enables us to thoroughly understand the passage under current contexts.</p><p>Existing recurrence memory however, lacks this dynamic contextualization ability. As the representations in the recurrence memory are previously computed conditioned on their past, they are not aware of the current contexts which provide more relevant information for the current token prediction.</p><p>To address this limitation, we propose a lookahead attention that allow the memory to attend to the contexts on their right. Formally, we reuse the notation</p><formula xml:id="formula_9">X ? = [x ? +1 , ? ? ? , x ? +N ] ? R N ?d for the representations of the current target sequence and X ? -1 = [x ? -M +1 , ? ? ? , x ? ] ? R M ?d</formula><p>for the representations of the memory.</p><p>Let us consider the i-th position of the memory X ? -1 , x i can attend to position x j on its right (j &gt; i) without causing information leakage as long as j ? ? + 1. Though appealing, this na?ve approach requires to calculate an M by M attention map, which would become inefficient and redundant when M is significantly greater than N . Actually, since the target segment moves forward N positions at each iteration, we devise an incremental manner of look-ahead attention computation that only requires the newest N positions on the right as key-value pairs.</p><formula xml:id="formula_10">X?-1 = [x ? -N +2 , ? ? ? , x ? +1 ] ? R N ?d . (7)</formula><p>Then the look-ahead attention results computed previously can be effectively reused and interpolated with the current ones ( ?3.2). Concretely, we formalize the look-ahead attention as follows:</p><formula xml:id="formula_11">K?-1 = X?-1 W k , ? ? -1 = X?-1 W v ,<label>(8)</label></formula><formula xml:id="formula_12">C ? ? -1 = LookAhead-Attn(Q ? -1 , K?-1 , ? ? -1 ) = softmax Q ? -1 K ? -1 ? d ? ? -1 ,<label>(9)</label></formula><p>where softmax (?) masks position j ? i for the ith row of the input matrix with -? before softmax. Q ? -1 is obtained by Eq. ( <ref type="formula">1</ref>), and the projection matrices of query, key and value are all shared with the causal attention. We illustrate this in Figure <ref type="figure" target="#fig_2">3</ref> where the look-ahead attention (yello paths) increases the attention window of each memory state to M tokens on its right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Memory Interpolation</head><p>To save computations for looking-ahead and effectively reuse the attention results of the past, we propose memory interpolation that smoothly interpolates attention results from both the future and the past to provide bi-directional contextualization.</p><p>Recall that in the previous iteration, we have calculated the causal context representations C ? ? -1 of X ? -1 using Eq. 2, where each row is a linear <ref type="table" target="#tab_8">s h a 1 _ b a s e 6 4 = " t v I e H X 8 / 8 8 A 1</ref>  <ref type="table" target="#tab_8">r 2 6 9 x 4 / 5 k y B b X 8 b C 4 t L y y u r p b X y + s b m 1 r a 5 s 9 t S I p G E N o n g Q n Z 8 r C h n E W 0 C A 0 4 7 s a Q 4 9 D l t + 6 N 6 7 r</ref>  <ref type="table" target="#tab_7">r H 8 D G 3 + C 0 H a D l S p a P z j l X 9 9 7 j J 4 J r s O 1 v a 2 l 5</ref> </p><formula xml:id="formula_13">Causal Self-Attention K, V ; Q Look-Ahead Attention Q K, V Interpolation Point-wise Non-linear Transform Concatenation &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " v B y L m X a Q 1 x z x b b g Y N 6 b l 5 Q b t a c g = " &gt; A A A C A H i c b V C 7 T s M w F H X K q 5 R X g I G B x a J C Y q F K E A j G C h b G I l F a q Y k i x 3 F a q 4 4 d 2 Q 5 S F W X h V 1 g Y Q I i V z 2 D j b 3 D a D N B y J M t H 5 9 y r e + 8 J U 0 a V d p x v q 7 a 0 v L K 6 V l 9 v b G x u b e / Y u 3 s P S m Q S k y 4 W T M h + i B R h l J O u p p q R f i o J S k J G e u H 4 p v R 7 j 0 Q q K v i 9 n q T E T 9 C Q 0 5 h i p I 0 U 2 A d e K F i k J o n 5 8 n 4 R 5 J 5 G 2 a l b B H b T a T l T w E X i V q Q J K n Q C + 8 u L B M 4 S w j V m S K m B 6 6 T a z 5 H U F D N S N L x M k R T h M R q S g a E c J U T 5 + f S A A h 4 b J Y K x k O Z x D a f q 7 4 4 c J a r c 0 V Q m S I / U v F e K / 3 m D T M d X f k 5 5 m m n C 8 W x Q n D G o B S z T g B G V B G s 2 M Q R h S c 2 u E I + Q R F i b z B o m B H f + 5 E X y c N Z y L 1 r O 3 X m z f V 3 F U Q e H 4 A i c A B d c g j a 4 B R 3 Q B R g U 4 B m 8 g j f r y X q x 3 q 2 P W W n N q n r 2 w R 9 Y n z 9 D Y J b T &lt; / l a t e x i t &gt; X ? 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " u 0 a Z s j C 2 4 2 G 2 X G 3 a t z Y 8 C a E W + N Y = " &gt; A A A B / n i c b V D N S 8 M w H E 3 n 1 5 x f V f H k J T g E T 6 M V R Y 9 D L x 4 n u A 9 Y S 0 n T d A t L k 5 K k w i g F / x U v H h T x 6 t / h z f / G d O t B N x + E P N 7 7 / c j L C 1 N G l X a c b 6 u 2 s r q 2 v l H f b G x t 7 + z u 2 f s H P S U y i U k X C y b k I E S K M M p J V 1 P N y C C V B C U h I / 1 w c l v 6 / U c i F R X 8 Q U 9 T 4 i d o x G l M M d J G C u w j L x Q s U t P E X P m g C H J P o 6 w I 7 K b T c m a A y 8 S t S B N U 6 A T 2 l x c J n C W E a 8 y Q U k P X S b W f I 6 k p Z q R o e J k i K c I T N C J D Q z l K i P L z W f w C n h o l g r G Q 5 n A N Z + r v j R w l q k x o J h O k x 2 r R K 8 X / v G G m 4 2 s / p z z N N O F 4 / l C c M a g F L L u A E Z U E a z Y 1 B G F J T V a I x 0 g i r E 1 j D V O C u / j l Z d I 7 b 7 m X L e f + o t m + q e q o g 2 N w A s 6 A C 6 5 A G 9 y B D u g C D H L w D F 7 B m / V k v V j v 1 s d 8 t G Z V O 4 f g D 6 z P H 1 k w l m E = &lt; / l a t e x i t &gt; X ? &lt; l a t e x i t</formula><formula xml:id="formula_14">x K z p T P 8 z Z x / m l s w = " &gt; A A A C D X i c b V C 7 T s M w F H V 4 l v I K M L J E F C Q W q g S B Y K z o w l g k + p C a U D m u 0 1 p 1 4 s i + A V V R f o C F X 2 F h A C F W d j b + B q f N A C 1 H s n x 0 z</formula><formula xml:id="formula_15">f v q V R M R L c w j q k X 4 k H E A k Y w a K l n H r q + 4 H 0 1 D v W X 1 r O 7 1 O U 0 A C y l e M h 6 q Q s 4 O X G y n l m x q / Y E 1 j x x C l J B B R o 9 8 8 v t C 5 K E N A L C s V J d x 4 7 B S 7 E E R j j N y m 6 i a I z J C A 9 o V 9 M I h 1 R 5 6 e S a z D r S S t 8 K h N Q v A m u i / u 5 I c a j y h X V l i G G o Z r 1 c / M / r J h B c e i m L 4 g R o R K a D g o R b I K w 8 G q v P J C X A x 5 p g I p n e 1 S J D L D E B H W B Z h + D M n j x P W q d V 5 7 x q 3 5 x V a l d F H C W 0 j w 7 Q M X L Q B a q h a 9 R A T U T Q I 3 p G r + j N e D J e j H f j Y 1 q 6 Y B Q 9 e + g P j M 8 f B t O c y A = = &lt; / l a t e x i t &gt; C ? 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 0 v q M v b Q g X j h N 1 U G p R 5 1 S q H R N k g w = " &gt; A A A C D n i c b V C 7 T s M w F H V 4 l v I q M L J E V J V Y q B I E g r G i C 2 O R 6 E N q Q u S 4 b m v V s S P 7 B l R F + Q I W f o W F A Y R Y m d n 4 G 5 y 2 A 7 Q c y f L R O f f q 3 n v C m D M N j v N t L S 2 v r K 6 t F z a K m 1 v b O 7 u l v f 2 W l o k i t E k k l 6 o T Y k 0 5 E 7 Q J D D j t x I r i K O S 0 H Y 7 q u d + + p 0 o z K W 5 h H F M / w g P B + o x g M F J Q q n i h 5 D 0 9 j s y X 1 r O 7 1 F N s M A S s l H z I g t Q D n J y 4 W V A q O 1 V n A n u R u D N S R j M 0 g t K X 1 5 M k i a g A w r H W X d e J w U + x A k Y 4 z Y p e o m m M y Q g P a N d Q g S O q / X R y T m Z X j N K z + 1 K Z J 8 C e q L 8 7 U h z p f G N T G W E Y 6 n k v F / / z u g n 0 L / 2 U i T g B K s h 0 U D / h N k g 7 z 8 b u M U U J 8 L E h m C h m d r X J E C t M w C R Y N C G 4 8 y c v k t Z p 1 T 2 v O j d n 5 d r V L I 4 C O k R H 6 B i 5 6 A L V 0 D V q o C Y i 6 B E 9 o 1 f 0 Z j 1 Z L 9 a 7 9 T E t X b J m P Q f o D 6 z P H + n t n U U = &lt; / l a t e x i t &gt; C ! ? 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " A M W y f i k y h f Q L f s N M Q n S C H v u M o 9 A = " &gt; A A A C D H i c b V C 7 T s M w F H V 4 l v I q M L J Y V E h M V Y J A M F Z 0 Y S w S f U h N q B z H a a 0 6 c W T f g K o o H 8 D C r 7 A w g B A</formula><formula xml:id="formula_16">Z X V t v b R R 3 t z a 3 t m t 7 O 2 3 t U w V Z S 0 q h V R d n 2 g m e M x a w E G w b q I Y i X z B O v 6 o U e i d e 6 Y 0 l / E t j B P m R W Q Q 8 5 B T A o b q V 6 q u L 0 W g x 5 H 5 s k Z + l 7 m K D 4 Z A l J I P e T 9 z g a S 5 c d k 1 e 1 J 4 E T g z U E W z a v Y r X 2 4 g a R q x G K g g W v c c O w E v I w o 4 F S w v u 6 l m C a E j M m A 9 A 2 M S M e 1 l k 2 N y f G y Y A I d S m R c D n r C / O z I S 6 W J f 4 4 w I D P W 8 V p D / a b 0 U w k s v 4 3 G S A o v p d F C Y C g w S F 8 n g g C t G Q Y w N I F R x s y u m Q 6 I I B Z N f 2 Y T g z J + 8 C N q n N e e 8 Z t + c V e t X s z h K 6 B A d o R P k o A t U R 9 e o i V q I o k f 0 j F 7 R m / V k v V j v 1 s f U u m T N e g 7 Q n 7 I + f w D z u Z z T &lt; / l a t e x i t &gt; C ! ? Figure 4:</formula><p>The architecture of LaMemo with look-ahead attention and memory interpolation that refresh the memory dynamically with both the current contexts and the long-term history. combination of the weighted token representations of the previous tokens. In Sec. 3.1, we describe the look-ahead attention which enables X ? -1 to attend to the contexts on their right and computes C ? ? -1 using Eq. 9. Here, we formulate the memory interpolation as the interpolation between the old representations C ? ? -1 and the new ones C ? ? -1 with a coefficient vector ? ? -1 ? R M controlling the memorization of the past activations:</p><formula xml:id="formula_17">C ? ? -1 = Mem-Interp(C ? ? -1 , C ? ? -1 , ? ? -1 ) = ? ? -1 sg(C ? ? -1 ) + (1 -? ? -1 )C ? ? -1 . (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>The resulted C ? ? -1 which attend to contexts from both directions, are further fed to the non-linear transformation defined in Eq. 3 to update representations in higher layers.</p><p>For ? ? -1 , we define it to be the sum of the normalized attention weights on the previous tokens when calculating C ? ? -1 (Eq. 2):</p><formula xml:id="formula_19">? ? -1 = sg(s ? ? -1 ) sg(s ? ? -1 ) + s ? ? -1 + ? ,<label>(11)</label></formula><p>where s ? ? -1 is the sum of the unnormalized attention score of C ? ? -1 , which is the denominator of the softmax in Eq. 2. Similarly, s ? ? -1 is the denominator of the softmax in Eq. 9. ? is a small value to prevent zero division error in practice. Then Eq. 10 can be derived into a form that resembles the bi-directional attention with the queries attending to positions on both sides<ref type="foot" target="#foot_1">4</ref> (Appendix A). Figure <ref type="figure">4</ref> shows the architecture of LaMemo.</p><p>Note that the difference between the hidden state reuse in the recurrence memory and our memory interpolation is that they simply reuse the static representations to extend the contexts for attention while we update the memory representations by aggregating weighted attention sum of the history without the need to recompute them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Disentangled Relative Positional Encodings</head><p>As the look-ahead attention allows the memory to attend to future tokens on its right, we need a relative positional encoding scheme that can generalize to this setting. We start by considering the relative positional encoding in Transformer-XL, as described by Eq. 6. When the i-th query vector attending to a position j = i + ? &gt; i, we have R i-j = R -? . As defined by <ref type="bibr" target="#b29">Vaswani et al. (2017)</ref>, R ? ? R D is composed of sine and cosine functions with different frequencies. Since the sine function is odd, sin(-??) =sin(??), we have R -? = R ? so that it can represent attention in different directions (? sign of ?) with the same relative distance (absolute value of ?).</p><p>However, this approach solely relies on the fixed sinusoid encodings to represent the relative distance and the attention direction. We argue that disentangling them is more effective in capturing these two types of temporal biases and also mitigates the numerical unstability issue. Specifically, we propose to learn two direction-aware global position biases to parameterize the sign and query R with the absolute value of the relative distance:</p><formula xml:id="formula_20">A dis i,j = X i W q W E k X j + X i W q W R k R |i-j| + u W E k X j + v i-j W R k R |i-j| ,<label>(12)</label></formula><p>where</p><formula xml:id="formula_21">v i-j = v + if i ? j else v -.</formula><p>The global positional bias now explicitly separates the contributions of sgn(ij) and |i -j|, which can better generalize to long distance in both forward and backward directions.</p><p>To illustrate the numerical unstability caused by adapting Eq. 6 to j &gt; i, we derive the variance of the dot product x T R i-j where x is a random vector. We show that the variance undergoes an oscillation and cannot be properly bounded everywhere when i shifts from i ? j to i &lt; j. Detailed analysis are presented in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate LaMemo on both word-level and character-level language modeling tasks and compare with existing Transformer baselines augmented with different types of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>For word-level language modeling task, we consider Wikitext-103 <ref type="bibr" target="#b18">(Merity et al., 2017)</ref>, which is the most widely used word-level language modeling benchmark. It contains 103 million tokens for training from 28 thousand wikipedia articles, with an average length of 3.6 thousand tokens per article and a vocabulary size around 260K. We report perplexity (ppl) on the dev and test set.</p><p>We also evaluate on two character-level language modeling benchmarks enwik8 and text8 (Mahoney, 2011). Both datasets contain 100 million Wikipedia characters. While enwik8 is unprocessed, text8 is preprocessed by case lowering and filtering to include only 26 letters from a to z and space. On both datasets, we report bit per character (bpc) on the dev and test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>To directly compare with different types of memory, we consider Transformer-XL and its variations with the same model architecture but different memory mechanism.</p><p>Transformer+RPE is the vanilla Transformer <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>  Table <ref type="table" target="#tab_1">1</ref>: Word-level language modeling results on Wikitext-103. We report ppl (perplexity) on dev and test set. We also report the number of parameters, memory size, external memory size, and the number of FLOPS (floatingpoint operations) for computing one step prediction on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>We follow the standard architecture of the Transformer-XL <ref type="bibr" target="#b7">(Dai et al., 2019)</ref> that has different configurations for different tasks. Specifically, on Wikitext-103, we use a 16-layer Transformer with 10 attention heads and head dimension 41 equipped with adaptive embeddings <ref type="bibr" target="#b2">(Baevski and Auli, 2019)</ref>. We control the target sequence length to be 150 and the memory length 150 for all models following the setting of <ref type="bibr" target="#b7">Dai et al. (2019)</ref>. For the Compressive Transformer and ?-former, we additionally use an external memory of size 150 following the setting of Martins et al. ( <ref type="formula">2021</ref>). <ref type="foot" target="#foot_2">5</ref> On the text8 and enwik8 datasets, we use a 12-layer Transformer with 8 heads and head dimension 64. The length of the target sequence and the recurrence memory are both set to 512. In the main results we use the identical evaluation setting to the training phase on all datasets and do not use a longer memory. We use the Pytorch framework <ref type="bibr" target="#b20">(Paszke et al., 2019)</ref> and Apex for mixed-precision training.</p><p>In practice, we found that calculating the exponentials ( ?3.2) may lead to numerical overflow in mixed-precision mode, so we compute the logarithm of the exponential sum using logsumexp and logaddexp operator. Further details of the dataset and the hyperparameter settings are described in the Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Results</head><p>We show the results of word-level language modeling benchmark Wikitext-103 in  compared to the compressive memory and the unbounded memory that take longer contexts into account, LaMemo still achieves lower perplexity. This indicates that the look-ahead memory allows the language model to exploit the recent contexts to gain performance, while simply increasing the context length yields marginal improvement. This is in accordance with previous findings of how language models utilize contexts <ref type="bibr" target="#b14">(Khandelwal et al., 2018;</ref><ref type="bibr" target="#b26">Sun et al., 2021)</ref>. In terms of the parameters, LaMemo has the same number of parameters as the Transformer-XL while other baselines use additional parameters in CNN to compress or smooth the hidden states. Lastly, we show the number of FLOPS necessary for computing one step prediction. ?-former has the highest number of FLOPS for resampling enough points from the continuous signal to update the memory using smoothing techniques. LaMemo also incurs additional computations to re-contextualize the memory under the current context. Note that although the Compressive Transformer has lower number of FLOPS than LaMemo, it has an external memory that consumes more GPU memory.</p><p>We also present the results of character-level language modeling on text8 and enwik8 datasets in Table <ref type="table" target="#tab_2">2</ref>. We observe similar trends as the results on the word-level benchmark, where LaMemo outperforms Transformer-XL by 0.04 on text8 and 0.02 on enwik8 with the same context length. Additionally, we observe that all models exhibit overfitting on text8, which might be caused by the extremely small vocabulary size of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We conduct ablation studies on Wikitext-103 to examine the effects of the proposed techniques, i.e., look-ahead attention, memory interpolation, and disentangled relative positional encodings.</p><p>We use the same model achitecture and the same target and memory length as the main results. We first study three configurations, including (1) using the Full model setting, (2) ablating the memory interpolation module (w/o mem interp), i.e., set the memorizing coeffecient ? ? -1 = 0, and (3) ablating the look-ahead attention (w/o look-ahead), i.e., only use the causal context representations C ? ? -1 in each layer. As shown in the First three rows in Table <ref type="table">3</ref>, both the memory interpolation and the look-ahead attention are indispensible for achieving the best performance. Additionaly, we found that cancelling out memory interpolation leads to a worse performance, which indicates that the distant past still provides additional information beyond the current context.</p><p>The second study targets at studying different encoding schemes. We substitute our encodings with the RPE of Transformer-XL <ref type="bibr" target="#b7">Dai et al. (2019)</ref> and run multiple experiments with 3 different random seeds, but all the models fail to converge. We plot the training curves using two encodings in Figure <ref type="figure" target="#fig_6">8</ref> in Appendix B, where we observe that our disentangled RPE is more stable during training and achieves lower perplexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Extrapolating to Longer Contexts</head><p>In this section, we extrapolate the models to longer contexts during inference to study the effect of dynamic contextualization to the distant past.  We fix the length of the target sequence to 64 and extrapolate the trained models to longer memory length 64 ? m during inference, where m = 1, ? ? ? , 10. We compare the perplexity of LaMemo and Transformer-XL trained on Wikitext-103 when augmented by a memory with different length. As shown in Figure <ref type="figure" target="#fig_3">5</ref>, LaMemo consistently achieves lower perplexity than Transformer-XL when extraploating to longer contexts, while the performance of both models saturate when m is over 7. Additionally, we observe that the gap of perplexity between the two models increases when taking longer contexts into account. This demonstrates the effectiveness of dynamically refreshing the distant memory representations under the current context.</p><p>In this section, we analyze the attention distribution of LaMemo to validate the effectiveness of utilizing bi-directional contexts with look-ahead attention.</p><p>We first visualize the memorizing coefficient ? which stands for the portion of the past activations in the current memory representations. As show in Figure <ref type="figure" target="#fig_4">6</ref>, we plot ? in different layers as a function of the memory index averaged on 100 text segments. <ref type="foot" target="#foot_3">6</ref> We observe that in lower layers the memory mainly attends to the past (? ? 1.0). We conjecture that long-term bi-directionality is not necessary for low-level representations such as lexical features. In higher layers, the memory substantially utilizes the future contents to refresh the high-level representations, especially for the old memory state with a small memory index.</p><p>Next, we visualize the attention weight distribution on the context tokens when predicting each target token in Figure <ref type="figure" target="#fig_0">1</ref>. For every token, we take the maximal attention weight in each interval of 5 tokens on its left and scale to a context length of 100. The result indicates that LaMemo learns better memory represetations by attending to the right-side tokens, which increases the memory utilization when predicting the target token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Case Study</head><p>We present the generated texts of LaMemo and Transformer-XL trained on Wikitext-103 in Appendix D. Both models maintain a memory size of 512, and we seed them with the same context randomly sampled from the test set and generate 256 tokens using top-p sampling <ref type="bibr" target="#b10">(Holtzman et al., 2020)</ref> with p = 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>The Transformer <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>, with its pair-wise modeling ability of the input, becomes prevailing for sequence modeling, especially long sequence processing tasks, such as long text generation <ref type="bibr" target="#b27">(Tan et al., 2021;</ref><ref type="bibr" target="#b12">Ji and Huang, 2021)</ref>, long document QA <ref type="bibr" target="#b4">(Beltagy et al., 2020;</ref><ref type="bibr" target="#b0">Ainslie et al., 2020)</ref>, language modeling <ref type="bibr" target="#b7">(Dai et al., 2019;</ref><ref type="bibr" target="#b22">Rae et al., 2020)</ref>, video processing <ref type="bibr" target="#b32">(Wu et al., 2019)</ref>, and etc. Specifically, language modeling <ref type="bibr" target="#b18">(Merity et al., 2017)</ref> which requires processing documents with thousands of tokens has become a natural testbed for benchmarking this long-term processing ability. However, due to the quadratic time and space complexity of self-attention, scaling to inputs with thousands of tokens is computationally prohibitive.</p><p>One line of work investigated the linear-time attention mechanism to mitigate the scability issue of Transformer. Linformer <ref type="bibr" target="#b30">(Wang et al., 2020)</ref> projects the inputs to lower dimension in length and approximates the full attention with a low-rank factorization. Linear Transformer <ref type="bibr" target="#b13">(Katharopoulos et al., 2020)</ref> regards the self-attention as a kernel function and uses a linear dot-product as a substitute. Choromanski et al. ( <ref type="formula">2021</ref>) and <ref type="bibr" target="#b21">Peng et al. (2021)</ref> proposed to approximate the softmax more precisely with the expectation of the dot-product of random features. Although achieving substantial improvements on benchmarks designated for long inputs <ref type="bibr" target="#b28">(Tay et al., 2021)</ref>. These methods, however, focus on approximating the full attention with low-rank factorizations or kernel functions, which compromise the expressiveness and robustness of the original softmax attention, are reported to be inferior to the simple local attentions on real world language processing tasks <ref type="bibr" target="#b34">(Xiong et al., 2021)</ref>.</p><p>Our work falls in another line, which augments the Transformer with a parametrized memory to store critical history information. Memoryaugmented networks <ref type="bibr" target="#b8">(Graves et al., 2014;</ref><ref type="bibr" target="#b31">Weston et al., 2015;</ref><ref type="bibr" target="#b25">Sukhbaatar et al., 2015)</ref> have been studied in the context of recurrent neural networks for a long time, but are mostly restricted to small and synthetic datasets. With the rapid development of Transformer, various works start to adapt memories to this architecture. <ref type="bibr" target="#b7">Dai et al. (2019)</ref> first extended Transformer with a recurrence memory that caches hidden states computed in previous steps for the target tokens to attend to. <ref type="bibr" target="#b22">Rae et al. (2020)</ref> further extended the context with an external memory that stores compressed hidden states at the temporal level. Martins et al. ( <ref type="formula">2021</ref>) used continuous space attention to attend over the old history and updated the memory with recent hidden states to enable unbounded memory capacity. <ref type="bibr" target="#b33">Wu et al. (2021)</ref> proposed to use the encoder-decoder architecture to encode the memory states with previous text segments and pass this memory to future time steps. Instead of using a fixed-size attention span for different layers, <ref type="bibr" target="#b24">Sukhbaatar et al. (2019) and</ref><ref type="bibr" target="#b6">Correia et al. (2019)</ref> proposed to learn dynamic attention spans for dif-ferent attention heads, which greatly reduced the computations. These works focused on enabling the Transformer to access contents in long distance, but did not consider to learn better memory representations by refreshing the old memory under the current context. Our work is orthogonal to learning adaptive attention spans and can be combined with this technique to reduce the complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We present LaMemo, a memory mechanism that allows the memory states to incrementally attend to the right-side tokens and interpolates with the old memory states on the left side, which enables the memory to interact with bi-directional contexts with a complexity linear in memory length. Experiments on three language modeling datasets demonstrate the superiority of LaMemo over baselines with various types of memory mechanisms. We also found that LaMemo increases the utilization of older memory states when predicting the target tokens, and yields a higher performance boost when extrapolating to longer memory length, which indicates the effectiveness of recontextualizing the memory under the current context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Derivation of Memory Interpolation</head><p>We derive Eq. 10 into the form of standard selfattention in the following:</p><formula xml:id="formula_22">C ? ? -1 = ? ? -1 sg(C ? ? -1 ) + (1 -? ? -1 )C ? ? -1 .</formula><p>We consider the i-th row of C ? ? -1 , denoted as c ? i . We omit the stop-grad operation sg(?) and substitute ? with the result from Eq. 11:</p><formula xml:id="formula_23">c ? i = ? i c ? i + (1 -? i )c ? i = s ? i s ? i + s ? i c ? i + s ? i s ? i + s ? i c ? i ,</formula><p>where s ? i , s ? i is the denominator of the softmax when computing c ? i , c ? i respectively:</p><formula xml:id="formula_24">s ? i = j?i exp q i k j ? d = j?i sim(q i , k j ), s ? i = j&gt;i exp q i k j ? d = j&gt;i sim(q i , k j ),</formula><p>where (q i , k j ) and (q i , k j ) are two sets of querykey vectors computed in the previous and this text segment respectively for the same position pair (i, j) . Then we have:</p><formula xml:id="formula_25">c ? i = j?i sim(q i , k j )</formula><p>j?i sim(q i , k j ) + j&gt;i sim(q i , k j )</p><formula xml:id="formula_26">c ? i + j&gt;i sim(q i , k j )</formula><p>j?i sim(q i , k j ) + j&gt;i sim(q i , k j )</p><formula xml:id="formula_27">c ? i = j?i sim(q i , k j )v j + j&gt;i sim(q i , k j )v j j?i sim(q i , k j ) + j&gt;i sim(q i , k j ) = j ? j ?j ,</formula><p>where j ? j = 1. Finally, we derive c ? i as the weighted sum of the value vectors ?j from both the past (j ? i) and the future (j &gt; i) of the position i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Unstability Analysis of the RPE in Transformer-XL</head><p>We conjecture that the unstability of Eq. 6 stems from the terms involving the dot-product of R i-j and another vector. So we start by considering the variance of x R i-j where x ? R d is a random vector. Without loss of generality, we assume that x has zero mean and a variance of ?:</p><formula xml:id="formula_28">E(x k ) = 0, ?k ? [1, ? ? ? , d] Var(x k ) = ? k,k , ?k ? [1, ? ? ? , d] Cov(x k , x l ) = ? k,l , ?l = k ? [1, ? ? ? , d]</formula><p>Let ij = ?. According to <ref type="bibr" target="#b29">Vaswani et al. (2017)</ref>, R ? takes the following form:</p><formula xml:id="formula_29">R ? =[sin(? 1 ?), cos(? 1 ?), ? ? ? , sin(? d/2 ?), cos(? d/2 ?)],</formula><p>where w k = 10000 -2k/d . Then the dot-product x R ? can be derived into the linear combination of sine and cosine functions:</p><formula xml:id="formula_30">x R ? = d/2 k=1 x 2k-1 sin(? k ?) + x 2k cos(? k ?),</formula><p>where we can easily derive that E(x R ? ) = 0.</p><p>According to the variance-expectation formula:</p><formula xml:id="formula_31">Var(x) = E[x 2 ] -E[x] 2</formula><p>, we can simplify the variance Var(x R ? ) in the following:</p><formula xml:id="formula_32">Var(x R ? ) = E d/2 k=1 x 2k-1 sin(? k ?) + x 2k cos(? k ?) 2 = d/2 k=1 E[x 2 2k-1 ] sin 2 (? k ?) + E[x 2 2k ] cos 2 (? k ?) + 2 d/2 k=1 d/2 l=1,l =k E[x 2k-1 x 2l ] sin(? k ?) cos(? l ?).</formula><p>We further simplify the above equation by assuming that all the elements have the same variance ? s , and all pairs of distinct elements have the same covariance ? c :</p><formula xml:id="formula_33">Var(x R ? ) = d/2 k=1 ? s [sin 2 (? k ?) + cos 2 (? k ?)] + 2 d/2 k=1 d/2 l=1,l =k ? c sin(? k ?) cos(? l ?) = d 2 ? s + 2? c g(?),</formula><p>where g(x) = d/2 k=1 d/2 l=1,l =k sin(? k x) cos(? l x) is an odd function.</p><p>We consider the value of g(x) when x ? 0. Figure <ref type="figure">7</ref>: The plot of g(x) when d = 64. We see that g(x) is symmetric with respect to the origin. The value of g(x) when x approaches zero from the left and right diverge greatly.</p><p>Since sin(? k x) ? ? k x, cos(? k x) ? 1, we have:</p><formula xml:id="formula_34">g(x) ? d/2 k=1 d/2 l=1 ? k x = d 2 d/2 k=1 w k x = xd 2 d/2 k=1 1 10000 2/d k ? d 2((10 8 ) 1/d -1) ? x = ? d ? x.</formula><p>Since a x ? 1 + x ln a when x ? 0, we derive that ? d ? d 2 2 ln 10 8 with the grow of d. This causes g(x) to have a very steep slope near 0. Since g(x) is an odd function, the value of g(?) and g(-?) will have a huge gap (? is a small positive value). To validate this, we plot the function of g(x) when d = 64 in Figure <ref type="figure">7</ref>.</p><p>Overall, the variance of x R ? is composed of two terms, the first being ? s multiplied by a constant factor d/2, and the second being ? c multiplied by g(?). Note that ? s is strictly positive, while ? c does not have this restriction. Due the asymptotic behavior of g(?) near 0, i.e., O(d 2 ?), we cannot find a proper ? c that makes Var(x R ? ) bounded by O(d? s ) for every ? that takes its value from both the positive and negative integers.</p><p>Finally, we plot the training curves of the two models using the RPE in Transformer-XL (xl-rpe) and our disentangled RPE (dis-rpe) in Figure <ref type="figure" target="#fig_6">8</ref> where we observed that the xl-rpe suffers from numerical unstability during training.   text8 dataset contains the first 100 million bytes of the clean text of Wikipedia that retains only regular articles and image captions. All the letters are converted into lower case, and only letters in the 27 character alphabet, namely letters a-z and nonconsecutive spaces, are preserved. This dataset is licensed under the CC BY-SA License.</p><p>The statistics of the three datasets is shown in Table <ref type="table" target="#tab_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Model Configurations</head><p>We follow the base model configuration of <ref type="bibr" target="#b7">Dai et al. (2019)</ref>. On Wikitext-103, we use the Transformer model with 16 layers, 10 attention heads with a head dimension of 41. The inner dimension size of the feedforward layer is 2100. We use a dropout rate of 0.1 and no attention dropout. To cope with the large vocabulary, we use the adaptive embeddings <ref type="bibr" target="#b2">(Baevski and Auli, 2019)</ref>. We set the memory length to 150 and the target sequence length to 150 as well. On text8 and enwik8 datasets, we use the Transformer model with 12 layers, 8 attention heads with a head dimension of 64. The inner dimension size of the feedforward layer is 2048. We use a dropout rate of 0.1 and no attention dropout. We set the memory length to 512 and the target length to 512. Specifically, our LaMemo uses the disentangled relative positional encodings described in Sec. 3.3. The look-ahead attention shares the query, key and value projection matrices with those in the causal attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Training Settings</head><p>We trained the models using Adam (Kingma and Ba, 2015) optimizer, with no warmup. We used a learning rate of 2.5 ? 10 -4 which decayed to 0 at the end of training with a cosine schedule. On Wikitext-103, we trained the model with 250K steps using a batch size of 64. On enwik8 and text8, we trained the model with 100K 7 steps using a batch size of 40. We conducted our experiments on 2 Tesla V100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Hyperparameters</head><p>We present the hyperparameter search space in Table 5. The number of hyperparameter search trials was 10. We adopted a manual search to select the hyperparameters, and the selection criterion was ppl/bpc on the dev set. We did not use early stopping during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Generated Examples</head><p>In this section, we present the examples generated by LaMemo and Transformer-XL trained on the Wikitext-103 dataset. Both models maintain a memory with a length of 512. We randomly select a piece of text from the test set as the context 7 We used a smaller number of training steps compared to <ref type="bibr" target="#b7">Dai et al. (2019)</ref>  and allow both models to generate 256 tokens following the context. We use top-p sampling with p = 0.95 and detokenize the context and the generated texts to facilitate reading. We present the exmples in Table <ref type="table" target="#tab_8">6</ref> and<ref type="table">7</ref>. We present our major findings below:</p><p>? Both models are able to hallucinate imaginary contents fairly relevant to the limited contexts given as prompts.</p><p>? Transformer-XL sometimes generates topicirrelevant contents without further elaboration (marked by underline), while LaMemo stays on topic more closely during the course of generation.</p><p>? Transformer-XL suffers more sever repetition issues (marked in boldface) than LaMemo both lexically and semantically.</p><p>Context: = Shackleton ( crater ) = Shackleton is an impact crater that lies at the south pole of the Moon. The peaks along the crater's rim are exposed to almost continual sunlight, while the interior is perpetually in shadow (a Crater of eternal darkness). The low-temperature interior of this crater functions as a cold trap that may capture and freeze volatiles shed during comet impacts on the Moon. Measurements by the Lunar Prospector spacecraft showed higher than normal amounts of hydrogen within the crater, which may indicate the presence of water ice. The crater is named after Antarctic explorer Ernest Shackleton. = = Description = = The rotational axis of the Moon lies within Shackleton, only a few kilometers from its center. The crater is 21 km in diameter and 4.2 km deep. From the Earth, it is viewed edge-on in a region of rough, cratered terrain. It is located within the South Pole-Aitken basin on a massif. The rim is slightly raised about the surrounding surface and it has an outer rampart that has been only lightly impacted. No significant craters intersect the rim, and it is sloped about 1.5 ?toward the direction 50 -90 ?from the Earth. The age of the crater is about 3.6 billion years and it has been in the proximity of the south lunar pole for at least the last two billion years.</p><p>Because the orbit of the Moon is tilted only 5 ?from the ecliptic, the interior of this crater lies in perpetual darkness. Estimates of the area in permanent shadow were obtained from Earth-based radar studies. Peaks along the rim of the crater are almost continually illuminated by sunlight , spending about 80 -90 % of each lunar orbit exposed to the Sun. Continuously illuminated mountains have been termed peaks of eternal light and have been predicted to exist since the 1900s.</p><p>The shadowed portion of the crater was imaged with the Terrain Camera of the Japanese SELENE spacecraft using the illumination of sunlight reflected off the rim. The interior of the crater consists of a symmetrical 30 ?slope that leads down to a 6.6 km diameter floor. The handful of craters along the interior span no more than a few LaMemo Generation: kilometres of plains. A slight rotation is observed in the crater that now bears significant elements like snow, ice, and water ice. In other areas of the crater there are indications that the nature of the alpine ice layer provides some protection to the craters by increasing their brightness. The craters seem to be cut by an ice thickness of 10 cm (4 in). Images show their prominence, varying from a visible plateau to a multi-ring topography, indicating that they are present (a major component) in the craters. Their interior has a considerable surface area of dry ice. As seen from Earth, the initial craters are marked by slow-moving metal-bearing mudstone-covered rock that has a surface of substantial amounts of coal, calcium, lead and sodium alloy, carbon monoxide, nitrogen oxides, and silicon dioxide. All are developed by processes such as heat transfer by the advancing radiation pressure. The least dense feature along the surface of the Moon is the large resurfacing event. There is no view that these rocks remain melting to about 90 ?C (150 ?F), but that their surface shine is dark and their outer layers are light, reflecting vertical oil. This late flank and early 1970s discovery was announced in 1982, as a substantial amount of ice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A parallel lava</head><p>Transformer-XL Generation: kilometres.</p><p>During the hundreds of years after its formation, the crater now has significant seismic activity in the vicinity. Most craters are dome-shaped and are usually constructed of sedimentary materials that have been deposited during alpine ice times. The upper surface of the crater is covered with higher layers of rock that are cut by an extensive thickness of debris. The debris reaches the top of a mountain and comes in contact with visible ground planes. People are often observed wearing headgear of degrading materials such as clothing and boots, their shoes or hats, or even working on the surface. Below the crater. As the crater faces the crater it has thick, thin pipes or scarps. A total of more than 200 caves have been excavated, down to some 40 m by 20 m. This exceeding the margin of the crater where it actually passes through is considered to be very high. Other geologic features by the advancing magnetic field have been reported from the crater. However, in 1992, scientists announced they would study this area again. The crater was once a common feature of the Post Lunar System. Its medieval boundaries were not fixed in the orbital plane of Mercury. An individual crater had been called " Discovery crater " and one referred to as " Bear crater ", although it is likely that an additional crater was called </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Attention weights on the context (in log-scale) in the final layer of Transformer-XL and LaMemo averaged on 15K tokens. Transformer-XL quickly loses attention to older contexts, while LaMemo maintains awareness to the history with the grow of the context length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of Transformer-XL augmenting with a recurrence memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of LaMemo with a memory length M = 2 and a target sequence length N = 1 for clarity. Solid lines stand for the attention connections computed at this iteration while dashed lines represent the previously computed attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Test perplexity of LaMemo and Transformer-XL when extrapolating to longer contexts during inference, where m is the ratio of the memory length to the target length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The memorizing coefficient ? of different layers in a 16-layer model with a same memory and target length of 150. Smaller index means older memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparison of the training dynamics using different encoding schemes: the disentangled RPE (disrpe) and the RPE of Transformer-XL (xl-rpe).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>that uses relative positional encodings from<ref type="bibr" target="#b7">Dai et al. (2019)</ref> but does not extend the context with additional memory.Transformer-XL<ref type="bibr" target="#b7">(Dai et al., 2019)</ref> is a Transformer model equipped with relative positional encodings and a recurrence memory comprised of hidden states computed in previous time steps to extend the context length of the attention.Compressive Transformer<ref type="bibr" target="#b22">(Rae et al., 2020</ref>) extends Transformer-XL with an external compressive memory that stores compressed hidden states at the temporal level using convolutional networks.?-former (Martins et al., 2021) uses continuous space attention to attend over the external memory which consists of continuous signals. They also updated the external memory with recent hidden states to enable unbounded memory capacity.</figDesc><table><row><cell>Model</cell><cell cols="6">#Params Mem size Ext mem size #FLOPS dev ppl test ppl</cell></row><row><cell>Transformer+RPE</cell><cell>151M</cell><cell>0</cell><cell>0</cell><cell>148M</cell><cell>28.11</cell><cell>29.14</cell></row><row><cell>Transformer-XL (Dai et al., 2019)</cell><cell>151M</cell><cell>150</cell><cell>0</cell><cell>157M</cell><cell>23.42</cell><cell>24.56</cell></row><row><cell>Compressive Transformer (Rae et al., 2020)</cell><cell>161M</cell><cell>150</cell><cell>150</cell><cell>169M</cell><cell>-</cell><cell>24.41</cell></row><row><cell>?-former (Martins et al., 2021)</cell><cell>160M</cell><cell>150</cell><cell>150</cell><cell>235M</cell><cell>-</cell><cell>24.22</cell></row><row><cell>LaMemo</cell><cell>151M</cell><cell>150</cell><cell>0</cell><cell>191M</cell><cell>22.98</cell><cell>23.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>. We first</cell></row><row><cell>observe that all the models extended with memo-</cell></row><row><cell>ries significantly outperforms Transformer+RPE.</cell></row><row><cell>Under the same memory length, LaMemo outper-</cell></row><row><cell>forms Transformer-XL with a clear margin, which</cell></row><row><cell>demonstrates the effectiveness of learning dynamic</cell></row><row><cell>memory representations over static ones. When</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note><p>Character-level language modeling results on text8 and enwik8. We report bpc (bits-per-character) on the dev and test set.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Statistics of the datasets used in the experiments. For Wikitext-103, we use the official split from<ref type="bibr" target="#b18">Merity et al. (2017)</ref> and present the number of tokens in each split. For enwik8 and text8, we use the split from<ref type="bibr" target="#b7">Dai et al. (2019)</ref> and report the number of characters for each split.</figDesc><table><row><cell>C Experimental Details</cell></row><row><cell>C.1 Dataset Details</cell></row><row><cell>Wikitext-103 dataset is extracted from the set of</cell></row><row><cell>verified Good and Featured articles on English</cell></row><row><cell>Wikipedia. The dataset retains the original case,</cell></row><row><cell>punctuation and numbers, and covers a broad range</cell></row><row><cell>of domains, e.g., science, culture, bibliography,</cell></row><row><cell>and etc. The dataset is available under the Creative</cell></row><row><cell>Commons Attribution-ShareAlike (CC BY-SA) Li-</cell></row><row><cell>cense.</cell></row><row><cell>enwik8 dataset is the test set data of the Large</cell></row><row><cell>Text Compression Benchmark which contains the</cell></row><row><cell>first 100 million bytes of English Wikipedia dump</cell></row><row><cell>on Mar. 3, 2006. All characters are encoded in</cell></row><row><cell>UTF-8. This dataset is licensed under the CC BY-</cell></row><row><cell>SA License.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>, since it would take too long to train one model.</figDesc><table><row><cell>Hyper-parameter</cell><cell>Search Space</cell></row><row><cell>Learning Rate</cell><cell>choice[1e-4, 2.5e-4, 5e-4]</cell></row><row><cell>Learning Rate Schedule</cell><cell>choise[linear, cosine]</cell></row><row><cell>Warmup Steps</cell><cell>choice[0, 1000, 2000]</cell></row><row><cell cols="2">Maximum Gradient Norm choice[0.25, 0.5, 1.0]</cell></row><row><cell>Epsilon (Sec. 3.2)</cell><cell>choice[1e-6, 1e-5, 1e-4]</cell></row><row><cell>Optimizer</cell><cell>Adam</cell></row><row><cell>Epsilon (for Adam)</cell><cell>1e-8</cell></row><row><cell>Momentum (for Adam)</cell><cell>?1 = 0.9, ?2 = 0.999</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameter search space. choice indicates that the listed numbers will be chosen with the same probability. Best-found hyperparameters are in boldface.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Example 1 generated by LaMemo and Transformer-XL given a context prompt from the test set of Wikitext-103. Original Wikipedia page: https://en.wikipedia.org/wiki/Shackleton_ (crater).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Note that the look-ahead attention does not exceed the current step of the autoregressive model to prevent information leakage.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Note that the query vectors for the past and the future are under different contextualization in higher layers of the model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>The external memory consists of 150 compressed vectors for Compressive Transformer, and 150 radial basis functions for ?-former respectively.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>Due to the space limit, we only sample 8 layers from all the 16 layers.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">National Science Foundation for Distinguished Young Scholars</rs> (with No. <rs type="grantNumber">62125604</rs>) and the <rs type="funder">NSFC</rs> projects (<rs type="projectName">Key</rs> project with No. <rs type="grantNumber">61936010</rs> and regular project with No. <rs type="grantNumber">61876096</rs>). This work was also supported by the <rs type="funder">Guoqiang Institute of Tsinghua University</rs>, with Grant No. <rs type="grantNumber">2019GQG1</rs> and <rs type="grantNumber">2020GQG0005</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tacvuSN">
					<idno type="grant-number">62125604</idno>
				</org>
				<org type="funded-project" xml:id="_wXA8wcK">
					<idno type="grant-number">61936010</idno>
					<orgName type="project" subtype="full">Key</orgName>
				</org>
				<org type="funding" xml:id="_4U6AeTT">
					<idno type="grant-number">61876096</idno>
				</org>
				<org type="funding" xml:id="_bgMtNmT">
					<idno type="grant-number">2019GQG1</idno>
				</org>
				<org type="funding" xml:id="_UhcYscv">
					<idno type="grant-number">2020GQG0005</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context:</head><p>Nero was not expected to become Emperor because his maternal uncle, Caligula, had begun his reign at the age of 24 with enough time to produce his own heir. Nero 's mother, Agrippina, lost favour with Caligula and was exiled in 39 after her husband 's death. Caligula seized Nero 's inheritance and sent him to be brought up by his less wealthy aunt, Domitia &lt;unk&gt;, who was the mother of Valeria &lt;unk&gt;, Claudius 's third wife. Caligula, his wife &lt;unk&gt; and their infant daughter Julia Drusilla were murdered on 24 January 41. These events led Claudius, Caligula 's uncle, to become emperor. Claudius allowed Agrippina to return from exile.</p><p>Claudius had married twice before marrying Valeria &lt;unk&gt;. His previous marriages produced three children including a son, Drusus, who died at a young age. He had two children with &lt;unk&gt; -Claudia Octavia (born 40) and Britannicus (born 41). &lt;unk&gt; was executed by Claudius in the year 48.</p><p>In 49 AD , Claudius married a fourth time, to Nero 's mother Agrippina, despite her being his niece. To aid Claudius politically, young Nero was adopted in 50 and took the name Nero Claudius Caesar Drusus Germanicus (see adoption in Rome). Nero was older than his stepbrother Britannicus, and thus became heir to the throne. Nero was proclaimed an adult in 51 at the age of 14. He was appointed proconsul, entered and first addressed the Senate, made joint public appearances with Claudius, and was featured in coinage. In 53, he married his stepsister Claudia Octavia. = = Emperor (54 -68 AD) = = = = = Early rule = = = Claudius died in 54 and Nero, taking the name Nero Claudius Caesar Augustus Germanicus, was established as Emperor. Though accounts vary, many ancient historians state Agrippina poisoned Claudius. According to Pliny the Elder, she used poison mushrooms. It is not known how much Nero knew or if he was even involved in the death of Claudius.</p><p>Suetonius wrote " ... for even if he was not the instigator of the emperor 's death , he was at least privy to it, as he openly admitted; for he used afterwards to laud mushrooms, the vehicle in which the poison was administered to Claudius, as ' the food of the gods , ' as the Greek proverb has it. At any rate, after Claudius' death he vented on him every kind of insult, in act and word, charging him now with folly and now with cruelty; for it was a favourite joke of his to say that Claudius had ceased ' to play the fool ' among mortals, lengthening the first syllable of the word &lt;unk&gt;, and he disregarded many of his decrees and acts as the work of a madman and a &lt;unk&gt;. Finally, he neglected to enclose the place where his body was burned except with a low and mean wall. "</p><p>According to Suetonius; Nero became Emperor at the age of 17 when the news of Claudius' death was made known , making him the youngest emperor at that time Although, what Suetonius may have meant is that he was LaMemo Generation: killed as a result of the subsequent anti-Caligula propaganda. At some time after 47 AD, Nero retired to his villa at Baiae. The combination of a curse that led to the death of Agrippa, and a duel between Claudius and his own son Drusus, may have encouraged the young Emperor to fire his own stela. = = = = = Germanicus = = = = = After becoming emperor, Nero received letters from Suetonius, which finally followed him on 28 May 44 AD with a letter to Claudius, completing the Emperor 's plot. The letters were printed in Latin. Claudius' plot was offered to Caligula by Agrippina the Elder, a woman from Kent, but she declined. Further letters from Gaius and Nero are known from the reigns of Tiberius and Caligul , though none of them mention Caligula.</p><p>Germanicus' conspiracy had developed into a protracted affair. His wife Antonia, his sister Agrippina 's sister and grandmother, had refused to accept him because she believed they were connected for communication, but the Via Flavia was left to her husband and Agrippina the Elder, whom she had defeated in her husband 's palace. Both had been sent through Gibraltar to compete for power. Gaius and Nero were outraged that Agrippina was supposed to be ready to support Caligula; Suetonius says that Nero insisted on making Transformer-XL Generation: killed as a result of poison and was deemed a good comedian. At now and then he had adopted Nero as his heir.</p><p>Although Nero 's rule was as close as he was to his father to his daughter Agrippina, he was not to physically exhibit any ill degree of higher kingship but to deny it. The Augustan History describes him as having been strongly willed, possessing an excellent hand and often claiming the title " Augustus Caesar ", and often referred to him as Caesar 's " paternal heir ". The Augustan History, however, asserts that he was no longer in China, and therefore he was raised as a Roman Hercules rather than a Roman citizen. Claudius Claudius was added as a junior emperor in 53 AD; he was crowned emperor in 61 AD.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ETC: encoding long and structured inputs in transformers</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Onta??n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaclav</forename><surname>Cvicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.19</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11-16">2020. 2020. November 16-20, 2020</date>
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<idno>CoRR, abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno>CoRR, abs/2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Krzysztof Marcin Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tam?s</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sarl?s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Benjamin</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">J</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptively sparse transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gon?alo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Correia</surname></persName>
		</author>
		<author>
			<persName><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Andr?</surname></persName>
		</author>
		<author>
			<persName><surname>Martins</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1223</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. 2019. November 3-7, 2019</date>
			<biblScope unit="page" from="2174" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Viet Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1285</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno>CoRR, abs/1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas</title>
		<meeting><address><addrLine>NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27">2016. June 27-30, 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An improved relative self-attention mechanism for transformer with application to music generation</title>
		<author>
			<persName><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<idno>CoRR, abs/1809.04281</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discodvt: Generating long text with discourse-aware discrete variational transformer</title>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana<address><addrLine>Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="4208" to="4224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<title level="s">Nikolaos Pappas, and Fran?ois Fleuret</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020-07-18">2020. 18 July 2020</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sharp nearby, fuzzy far away: How neural language models use context</title>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1027</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-07-15">2018. July 15-20, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="284" to="294" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<title level="m">Large text compression benchmark</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">?-former: Infinite memory transformer</title>
		<author>
			<persName><forename type="first">Henrique</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zita</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><surname>Marinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Andr?</surname></persName>
		</author>
		<author>
			<persName><surname>Martins</surname></persName>
		</author>
		<idno>CoRR, abs/2109.00301</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. Open-Review</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<idno type="DOI">10.1109/SLT.2012.6424228</idno>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Spoken Language Technology Workshop (SLT)</title>
		<meeting><address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012-12-02">2012. December 2-5, 2012</date>
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-2074</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1032</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="331" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. 2015. December 7-12, 2015</date>
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Do long-range language models actually use long-range context?</title>
		<author>
			<persName><forename type="first">Simeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mattarella-Micke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="807" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Progressive generation of long text with pretrained language models</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maruan</forename><surname>Al-Shedivat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.341</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06-06">2021. June 6-11, 2021</date>
			<biblScope unit="page" from="4313" to="4324" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long range arena : A benchmark for efficient transformers</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria</title>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno>CoRR, abs/2006.04768</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<title level="m">Memory networks. In 3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long-term feature banks for detailed video understanding</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00037</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16">2019. 2019. June 16-20, 2019</date>
			<biblScope unit="page" from="284" to="293" />
		</imprint>
		<respStmt>
			<orgName>Computer Vision Foundation / IEEE</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Memformer: The memory-augmented transformer</title>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Simple local attentions remain competitive for long-context tasks</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Liskovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<idno>CoRR, abs/2112.07210</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
