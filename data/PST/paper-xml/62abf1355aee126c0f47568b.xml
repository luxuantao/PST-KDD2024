<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Condensing Graphs via One-Step Gradient Matching</title>
				<funder ref="#_gNmcme7">
					<orgName type="full">Army Research Office</orgName>
					<orgName type="abbreviated">ARO</orgName>
				</funder>
				<funder>
					<orgName type="full">Amazon.com, Inc.</orgName>
				</funder>
				<funder ref="#_jREAxEQ #_cMHSp7r #_GRh4GHW #_khYfMMv #_bsNVq6Q #_6d4wBfx #_xqbaVbb #_xEVTD4k">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-15">15 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
							<email>jinwei2@msu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
							<email>jhaoming@amazon.com</email>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Danqing</forename><surname>Zhang</surname></persName>
							<email>danqinz@amazon.com</email>
						</author>
						<author>
							<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Michigan State University ?</orgName>
								<address>
									<country>Amazon ?</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Condensing Graphs via One-Step Gradient Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-15">15 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539429</idno>
					<idno type="arXiv">arXiv:2206.07746v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As training deep learning models on large dataset takes a lot of time and resources, it is desired to construct a small synthetic dataset with which we can train deep learning models sufficiently. There are recent works that have explored solutions on condensing image datasets through complex bi-level optimization. For instance, dataset condensation (DC) matches network gradients w.r.t. largereal data and small-synthetic data, where the network weights are optimized for multiple steps at each outer iteration. However, existing approaches have their inherent limitations: (1) they are not directly applicable to graphs where the data is discrete; and (2) the condensation process is computationally expensive due to the involved nested optimization. To bridge the gap, we investigate efficient dataset condensation tailored for graph datasets where we model the discrete graph structure as a probabilistic model. We further propose a one-step gradient matching scheme, which performs gradient matching for only one single step without training the network weights. Our theoretical analysis shows this strategy can generate synthetic graphs that lead to lower classification loss on real graphs. Extensive experiments on various graph datasets demonstrate the effectiveness and efficiency of the proposed method. In particular, we are able to reduce the dataset size by 90% while approximating up to 98% of the original performance and our method is significantly faster than multi-step gradient matching (e.g. 15? in CIFAR10 for synthesizing 500 graphs) 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph-structured data plays a key role in various real-world applications. For example, by exploiting graph structural information, we can predict the chemical property of a given molecular graph <ref type="bibr" target="#b45">[46]</ref>, detect fraud activities in a financial transaction graph <ref type="bibr" target="#b34">[35]</ref>, or recommend new friends to users in a social network <ref type="bibr" target="#b7">[8]</ref>. Due to its prevalence, graph neural networks (GNNs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42]</ref> have been developed to effectively extract meaningful patterns from graph data and thus tremendously facilitate computational tasks on graphs. Despite their effectiveness, GNNs are notoriously datahungry like traditional deep neural networks: they usually require massive datasets to learn powerful representations. Thus, training GNNs is often computationally expensive. Such cost even becomes prohibitive when we need to repeatedly train GNNs, e.g., in neural architecture search <ref type="bibr" target="#b22">[23]</ref> and continual learning <ref type="bibr" target="#b21">[22]</ref>.</p><p>One potential solution to alleviate the aforementioned issue is dataset condensation or dataset distillation. It targets at constructing a small-synthetic training set that can provide sufficient information to train neural networks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52]</ref>. In particular, one of the representative methods, DC <ref type="bibr" target="#b51">[52]</ref>, formulates the condensation goal as matching the gradients of the network parameters between small-synthetic and large-real training data. It has been demonstrated that such a solution can greatly reduce the training set size of image datasets without significantly sacrificing model performance. For example, using 100 images generated by DC can achieve 97.4% test accuracy on MNIST compared with 99.6% on the original dataset (60, 000 images). These condensed samples can significantly save space for storing datasets and speed up retraining neural networks in many critical applications, e.g., continual learning and neural architecture search. In spite of the recent advances in dataset distillation/condensation for images, limited attention has been paid on domains involving graph structures.</p><p>To bridge this gap, we investigate the problem of condensing graphs such that GNNs trained on condensed graphs can achieve comparable performance to those trained on the original dataset. However, directly applying existing solutions <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52]</ref> for dataset condensation to graph domain faces some challenges. First, existing solutions have been designed for images where the data is continuous and they cannot output binary values to form the discrete graph structure. Thus, we need to develop a strategy that can handle the discrete nature of graphs. Second, they usually involve a complex bi-level problem that is computationally expensive to optimize: they require multiple iterations (inner iterations) of updating neural network parameters before updating the synthetic data for multiple iterations (outer iterations). It can be catastrophically inefficient for learning pairwise relations for nodes, of which the complexity is quadratic to the number of nodes.</p><p>To address the aforementioned challenges, we propose an efficient condensation method for graphs, where we follow DC <ref type="bibr" target="#b51">[52]</ref> to match the gradients of GNNs between synthetic graphs and real graphs. In order to produce discrete values, we model the graph structure as a probabilistic graph model and optimize the discrete structures in a differentiable manner. Based on this formulation, we further propose a one-step gradient matching strategy which only performs gradient matching for one single step. Consequently, the advantages of the proposed strategy are twofold. First, it significantly speeds up the condensation process while providing reasonable guidance for synthesizing condensed graphs. Second, it removes the burden of tuning hyper-parameters such as the number of outer/inner iterations of the bi-level optimization as required by DC. Furthermore, we demonstrate the effectiveness of the proposed one-step gradient matching strategy both theoretically and empirically. Our contributions can be summarized as follows:</p><p>1. We study a novel problem of learning discrete synthetic graphs for condensing graph datasets, where the discrete structure is captured via a graph probabilistic model that can be learned in a differentiable manner. 2. We propose a one-step gradient matching scheme that significantly accelerates the vanilla gradient matching process. 3. Theoretical analysis is provided to understand the rationality of the proposed one-step gradient matching. We show that learning with one-step matching produces synthetic graphs that lead to a smaller classification loss on real graphs. 4. Extensive experiments have demonstrated the effectiveness and efficiency of the proposed method. Particularly, we are able to reduce the dataset size by 90% while approximating up to 98% of the original performance and our method is significantly faster than multi-step gradient matching (e.g. 15? in CIFAR10 for synthesizing 500 graphs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE PROPOSED FRAMEWORK</head><p>Before detailing the framework, we first introduce the main notations used in this paper. We majorly focus on the graph classification task where the goal is to predict the labels of given graphs. Specifically, we denote a graph dataset as T = {? 1 , . . . , ? ? } with ground-truth label set Y. Each graph in T is associated with a discrete adjacency matrix and a node feature matrix. Let A (?) , X (?) represent the adjacency matrix and the feature matrix of ?-th real graph, respectively. Similarly, we use S = {? ? 1 , . . . , ? ? ? ? } and Y ? to indicate the synthetic graphs and their labels, respectively. Note that the number of synthetic graphs ? ? is essentially much smaller than that of real graphs ? . We use ? and ? to denote the number of feature dimensions the number of nodes in each synthetic graph, respectively. 2 . Let ? denote the number of classes and ? denote the cross entropy loss. The goal of our work is to learn a set of synthetic graphs S such that a GNN trained on S can achieve comparable performance to the one trained on the much larger dataset T .</p><p>In the following subsections, we first introduce how to apply the vanilla gradient matching to condensing graphs for graph classification (Section 2.1). However, it cannot generate discrete graph structure and is highly inefficient. To correspondingly address these two limitations, we discuss the approach to handling the discrete nature of graphs (Section 2.2) and propose an efficient solution, one-step gradient matching, which significantly accelerates the condensation process (Section 2.3). 2 We set ? to the average number of nodes in original dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gradient Matching as the Condensation Objective</head><p>Since we aim at learning synthetic graphs that are highly informative, one solution is to allow GNNs trained on synthetic graphs to imitate the training trajectory on the original large dataset. Dataset condensation <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b51">52]</ref> introduces a gradient matching scheme to achieve this goal. Concretely, it tries to reduce the difference of model gradients w.r. </p><formula xml:id="formula_0">min S ? -1 ?? ? =0 ? (? ? ? (? ? ? (S), Y ? ), ? ? ? (? ? ? (T ), Y)), s.t. ? ? +1 = opt ? (? ? , S),<label>(1)</label></formula><p>where ? (?, ?) is a distance function, ? is the number of steps of the whole training trajectory and opt ? (?) is the optimization operator for updating parameter ? . Note that Eq. ( <ref type="formula" target="#formula_0">1</ref>) is a bi-level problem where we need to learn the synthetic graphs S at the outer optimization and update model parameters ? ? at the inner optimization.</p><p>To learn synthetic graphs that generalize to a distribution of model parameters ? ? 0 , we sample ? 0 ? ? ? 0 and rewrite Eq. ( <ref type="formula" target="#formula_0">1</ref>) as:</p><formula xml:id="formula_1">min S E ? 0 ?? ? 0 ? -1 ?? ? =0 ? ? ? ? ? ? ? (S), Y ? , ? ? ? ? ? ? (T ), Y , s.t. ? ? +1 = opt ? (? ? , S).<label>(2)</label></formula><p>Discussion. The aforementioned strategy has demonstrated promising performance on condensing image datasets <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b51">52]</ref>. However, it is not clear how to model the discrete graph structure. Moreover, the inherent bi-level optimization inevitably hinders its scalability.</p><p>To tackle these shortcomings, we propose DosCond that models the structure as a probabilistic graph model and is optimized through one-step gradient matching. In the following subsections, we introduce the details of DosCond.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning Discrete Graph Structure</head><p>For graph classification, each graph in the dataset is composed of an adjacency matrix and a feature matrix. For simplicity, we use X ? ? R ? ? ???? to denote the node features in all synthetic graphs S and A ? ? {0, 1} ? ? ???? to indicate the graph structure information in S. Note that ? ? ? can be instantiated as any graph neural network and it takes both graph structure and node features as input. Then we rewrite the objective in Eq. (2) as follows:</p><formula xml:id="formula_2">min A ? ,X ? E ? 0 ?? ? 0 ? -1 ?? ? =0 ? ? ? ? ? ? ? (A ? , X ? ), Y ? , ? ? ? ? ? ? (T ), Y , s.t. ? ? +1 = opt ? (? ? , S),<label>(3)</label></formula><p>where we aim to learn both graph structure A ? and node features X ? . However, Eq. ( <ref type="formula" target="#formula_2">3</ref>) is challenging to optimize as it requires a function that outputs binary values. To address this issue, we propose to model the graph structure as a probabilistic graph model with Bernoulli distribution. Note that in the following, we reshape A ? from ? ? ? ? ? ? to ? ? ? ? 2 for the purpose of demonstration only. Specifically, for each entry A ? ? ? ? {0, 1} in the adjacency matrix A ? , it follows a Bernoulli distribution:</p><formula xml:id="formula_3">? ? ? ? (A ? ? ? ) = A ? ? ? ? (? ? ? ) + (1 -A ? ? ? )? (-? ? ? ),<label>(4)</label></formula><p>where ? (?) is the sigmoid function; ? ? ? ? R is the success probability of the Bernoulli distribution and also the parameter to be learned. Since A ? ? ? is independent of all other entries, the distribution of A ? can be modeled as:</p><formula xml:id="formula_4">? ? (A ? ) = ? ? ?=1 ? 2 ?=1 ? ? ? ? A ? ? ? .<label>(5)</label></formula><p>Then, the objective in Eq. ( <ref type="formula" target="#formula_1">2</ref>) needs to be modified to</p><formula xml:id="formula_5">min A ? ,X ? E ? 0 ?? ? 0 E A ? ?? ? ? (A ? (?), X ? , ? 0 ) .<label>(6)</label></formula><p>With the new parameterization, we obtain a function that outputs discrete values but it is not differentiable due to the involved sampling process. Thus, we employ the reparameterization method <ref type="bibr" target="#b25">[26]</ref>, binary concrete distribution, to refactor the discrete random variable into a differentiable function of its parameters and a random variable with fixed distribution. Specifically, we first sample ? ? Uniform(0, 1), and edge weight A ? ? ? ? [0, 1] is calculated by:</p><formula xml:id="formula_6">A ? ? ? = ? log ? -log(1 -?) + ? ? ? /? ,<label>(7)</label></formula><p>where ? ? (0, ?) is the temperature parameter that controls the continuous relaxation. As ? ? 0, the random variable A ? ? ? smoothly approaches the Bernoulli distribution. In other words, we have lim ??0 ? A ? ? ? = 1 = ? (? ? ? ). While small ? is necessary for obtaining discrete samples, large ? is useful in getting large gradients as suggested by <ref type="bibr" target="#b25">[26]</ref>. In practice, we employ an annealing schedule <ref type="bibr" target="#b0">[1]</ref> to gradually decrease the value of ? in training. With the reparameterization trick, the objective function becomes differentiable w.r.t. ? ? ? with well-defined gradients. Then we rewrite our objective as:</p><formula xml:id="formula_7">min ?,X ? E ? 0 ?? ? 0 E ? ?Uniform(0,1) ? (A ? (?), X ? , ? 0 ) =<label>(8)</label></formula><formula xml:id="formula_8">E ? 0 E ? ? -1 ?? ? =0 ? ? ? ? ? ?? (A ? (?), X ? ), Y ? , ? ? ? ? ?? ( T), Y , s.t. ? ? +1 = opt ? (? ? , S).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">One-Step Gradient Matching</head><p>The vanilla gradient matching scheme in Eq. ( <ref type="formula" target="#formula_1">2</ref>) presents a bi-level optimization problem. To solve this problem, we need to update the synthetic graphs S at the outer loop and then optimize the network parameters ? ? at the inner loop. The nested loops heavily impede the scalability of the condensation method, which motivates us to design a new strategy for efficient condensation. In this work, we propose a one-step gradient matching scheme where we only match the network gradients for the model initializations ? 0 while discarding the training trajectory of ? ? . Essentially, this strategy approximates the overall gradient matching loss for ? ? with the initial matching loss at the first epoch, which we term as one-step matching loss. The intuition is: the one-step matching loss informs us about the direction to update the synthetic data, in which, we have empirically observed a strong decrease in the cross-entropy loss (on real samples) obtained from the model trained on synthetic data. Hence, we can drop the summation symbol ? -1 ? =0 in Eq. ( <ref type="formula" target="#formula_7">8</ref>) and simplify Eq. ( <ref type="formula" target="#formula_7">8</ref>) as follows:</p><formula xml:id="formula_9">min ?,X ? E ? 0 E ? ? ? ? ? ? ? 0 (A ? (?), X ? ), Y ? , ? ? ? ? ? 0 (T ), Y ,<label>(9)</label></formula><p>where we sample ? 0 ? ? ? 0 and ? ? Uniform(0, 1). Compared with Eq. ( <ref type="formula" target="#formula_7">8</ref>), one-step gradient matching avoids the expensive nestedloop optimization and directly updates the synthetic graph S. It greatly simplifies the condensation process. In practice, as shown in Section 3.3, we find this strategy yields comparable performance to its bi-level counterpart while enabling much more efficient condensation. Next, we provide theoretical analysis to understand the rationality of the proposed one-step gradient matching scheme.</p><p>Theoretical Understanding. We denote the cross entropy loss on the real graphs as ? T (? ) = ? ? ? (A (?) , X (?) , ? ) and that on synthetic graphs as ? S (? ) = ? S (A ? (?) , X ? (?) , ? ). Let ? * denote the optimal parameter and ? ? be the parameter trained on S at the ?-th epoch by optimizing ? S (? ). For notation simplicity, we assume that A and A ? are already normalized. When not specified, the matrix norm ? ? ? is the Frobenius norm. We focus on the GNN of Simple Graph Convolutions (SGC) <ref type="bibr" target="#b40">[41]</ref> to study our problem since SGC has a simpler architecture but shares a similar filtering pattern as GCN.</p><p>Theorem 1. When we use a ?-layer SGC as the GNN used in condensation, i.e., ? ? (A (?) ,</p><formula xml:id="formula_10">X (?) ) = Pool(A ? (?) X (?) W 1 )W 2 with ? = [W 1 ; W 2 ]</formula><p>and assume that all network parameters satisfy ?? ? 2 ? ? 2 (? &gt; 0), we have</p><formula xml:id="formula_11">min ? =0,1,...,? -1 ? T (? ? ) -? T ? * ? ? -1 ?? ? =0 ? 2? ? ?? ? ? T (? ? ) -? ? ? S (? ? ) ? + 3? 2 ? ? ? ? -1 ?? ? ?? ?? ? ? ? ?1 ? A ?? (?) X ? (?) ? 2<label>(10)</label></formula><p>where ? ? = 1 if we use sum pooling in ? ? ; ? ? = 1 ? ? if we use mean pooling, with ? ? as the number of nodes in the ?-th synthetic graph.</p><p>We provide the proof of Theorem 1 in Appendix B.1. Theorem 1 suggests that the smallest gap between the resulted loss (by training on synthetic graphs) and the optimal loss has an upper bound. This upper bound depends on two terms: (1) the difference of gradients w.r.t. real data and synthetic data and (2) the norm of input matrices. Thus, the theorem justifies that reducing the gradient difference w.r.t real and synthetic graphs can help learn desirable synthetic data that preserves sufficient information to train GNNs well. Based on Theorem 1, we have the following proposition. Proposition 1. Assume the largest gradient gap happens at 0-th</p><formula xml:id="formula_12">epoch, i.e., ?? ? ? T (? 0 )-? ? ? ? (? 0 ) ? = max ? ?? ? ? T (? ? )-? ? ? ? (? ? ) ? with ? = 0, 1, . . . ,? -1, we have min ? =0,1,...,? -1 ? T (? ? ) -? T ? * ? ? 2? ?? ? ? T (? 0 ) -? ? ? ? (? 0 ) ? + 3? 2 ? ? ? ? -1 ?? ? ?? ?? ? ? ? ?1 ? A ?? (?) X ? (?) ? 2 . (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>We omit the proof for the proposition since it is straightforward. The above proposition suggests that the smallest gap between the ? T (? ? ) and ? T (? * ) is bounded by the one-step matching loss and the norm ?1 ? A ?? (?) X ? (?) ? 2 . As we will show in Section 3.3.4, when using mean pooling, the second term tend to have a smaller scale than the first one and can be neglected; the second term matters more when we use max pooling. Hence, we solely optimize the one-step gradient matching loss for GNNs with mean pooling and additionally include the second term (the norm of input matrices) as a regularization for GNNs with sum pooling. As such, when we consider the optimal loss ? T (? * ) as a constant, reducing the one-step matching loss indeed learns synthetic graphs that lead to a smaller loss on real graphs. This demonstrates the rationality of one-step gradient matching from a theoretical perspective.</p><p>Remark 1. Note that the spectral analysis from <ref type="bibr" target="#b40">[41]</ref> demonstrated that both GCN and SGC share similar graph filtering behaviors. Thus practically, we extend the one-step gradient matching loss from ?-layer SGC to ?-layer GCN and observe that the proposed framework works well under the non-linear scenario.</p><p>Remark 2. While we focus on the graph classification task, it is straightforward to extend our framework to node classification and we obtain similar conclusions for node classification as shown in Theorem 2 in Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Final Objective and Training Algorithm</head><p>In this subsection, we describe the final objective function and the detailed training algorithm. We note that the objective in Eq. ( <ref type="formula" target="#formula_7">8</ref>) involves two nested expectations, we adopt Monte Carlo to approximately optimize the objective function. Together with one-step gradient matching, we have</p><formula xml:id="formula_14">min ?,X ? E ? 0 ?? ? 0 E ??Uniform(0,1) ? (A ? (?), X ? , ? 0 )<label>(12)</label></formula><formula xml:id="formula_15">? ? 1 ?? ? 1 =1 ? 2 ?? ? 2 =1 ? ? ? ? ? ? 0 (A ? (?), X ? ), Y ? , ? ? ? ? ? 0 (T ), Y</formula><p>where ? 1 is the number of sampled model initializations and ? 2 is the number of sampled graphs. We find that ? 2 = 1 is able to yield good performance in our experiments.</p><p>Regularization. In addition to the one-step gradient matching loss, we note that the proposed DosCond can be easily integrated with various priors as regularization terms. In this work, we focus on exerting sparsity regularization on the adjacency matrix, since a denser adjacency matrix will lead to higher cost for training graph neural networks. Specifically, we penalize the difference of the sparsity between ? (?) and a given sparsity ?:</p><formula xml:id="formula_16">? reg = max( 1 |?| ?? ?,? ? (? ? ? ) -?, 0).<label>(13)</label></formula><p>We initialize ? (?) and X ? as randomly sampled training graphs <ref type="foot" target="#foot_0">3</ref>and set ? to the average sparsity of initialized ? (?) so as to maintain a low sparsity. On top of that, as we discussed earlier in Section 2.3, we include the following regularization for GNNs with sum pooling:</p><formula xml:id="formula_17">? reg2 = 3 2 ? 2? ? ? -1 ?? ? ?? ?? ? ?1 ? A ?? (?) X ? (?) ? 2<label>(14)</label></formula><p>Algorithm 1: DosCond for Condensing Graphs Sample ? ? Uniform(0, 1)</p><formula xml:id="formula_18">7: Compute A ? = ? ( (log ? -log(1 -?) + ?) /?) 8:</formula><p>for ? = 0, . . . , ? -1 do</p><formula xml:id="formula_19">9: Sample (A ? , X ? , Y ? ) ? T and (A ? ? , X ? ? , Y ? ? ) ? S 10: Compute ? ? = ? ? ? 0 (A ? , X ? ), Y ? 11: Compute ? ? = ? ? ? 0 (A ? ? , X ? ? ), Y ? ? 12:</formula><p>Compute ? reg = max( ?,? ? (? ? ? ) -?, 0)</p><formula xml:id="formula_20">13: Update ? ? ? -? 1 ? ? (? ( ? ? 0 ? ? , ? ? 0 ? ? ) + ?? reg ) 14: Update X ? ? X ? -? 2 ? X ? (? ( ? ? 0 ? ? , ? ? 0 ? ? ) + ?? reg ) 15:</formula><p>end for 16: end for 17: Return:</p><formula xml:id="formula_21">(?, X ? , Y ? )</formula><p>Training Algorithm. We provide the details of our proposed framework in Algorithm 1. Specifically, we sample ? 1 model initializations ? 0 to perform one-step gradient matching. Following the convention in DC <ref type="bibr" target="#b51">[52]</ref>, we match gradients and update synthetic graphs for each class separately in order to make matching easier. For class ?, we first retrieve the synthetic graphs of that class, denoted as (A ? ? , X ? ? , Y ? ? ) ? S, and sample a batch of real graphs (A ? , X ? , Y ? ). We then forward them to the graph neural network and calculate the one-step gradient matching loss together with the regularization term. Afterwards, ? and X ? are updated via gradient descent. It is worth noting that the training process for each class can be run in parallel since the graph updates for one class is independent of another class.</p><p>Comparison with DC. Recall that the gradient matching scheme in DC involves a complex bi-level optimization. If we denote the number of inner-iterations as ? ? and that of outer-iterations as ? ? , its computational complexity can be ? ? ? ? ? of our method. Thus DC is significantly slower than DosCond. In addition to speeding up condensation, DosCond removes the burden of tuning some hyper-parameters, i.e., the number of iterations for outer/inner optimization and learning rate for updating ? ? , which can potentially save us enormous training time when learning larger synthetic sets.</p><p>Comparison with Coreset Methods. Coreset methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b39">40]</ref> select representative data samples based on some heuristics calculated on the pre-trained embedding. Thus, it requires training the model first. Given the cheap cost on calculating and ranking heuristics, the major computational bottleneck for coreset method is on pre-training the neural network for a certain number of iterations. Likewise, our proposed DosCond has comparable complexity because it also needs to forward and backward the neural network for multiple iterations. Thus, their efficiency difference majorly depends on how many epochs we run for learning synthetic graphs in DosCond and for pre-training the model embedding in coreset methods. In practice, we find that DosCond even requires less training cost than the coreset methods as shown in Section 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENT</head><p>In this section, we conduct experiments to evaluate DosCond. Particularly, we aim to answer the following questions: (a) how well can we condense a graph dataset and (b) how efficient is DosCond.</p><p>Our code can be found in the supplementary files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental settings</head><p>Datasets. To evaluate the performance of our method, we use multiple molecular datasets from Open Graph Benchmark (OGB) <ref type="bibr" target="#b11">[12]</ref> and TU Datasets (DD, MUTAG and NCI1) <ref type="bibr" target="#b26">[27]</ref> for graph-level property classification, and one superpixel dataset CIFAR10 <ref type="bibr" target="#b6">[7]</ref>. We also introduce a real-world e-commerce dataset. In particular, we randomly sample 1,109 sub-graphs from a large, anonymized internal knowledge graph. Each sub-graph is created from the ego network of a random selected product on the e-commerce website. We form a binary classification problem aiming at predicting the product category of the central product node in each sub-graph. We use the public splits for OGB datasets and CIFAR10. For TU Datasets and the e-commerce dataset, we randomly split the graphs into 80%/10%/10% for training/validation/test. Detailed dataset statistics are shown in Appendix A.1.</p><p>Baselines. We compare our proposed methods with four baselines that produce discrete structures: three coreset methods (Random, Herding <ref type="bibr" target="#b39">[40]</ref> and K-Center <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref>), and a dataset condensation method DCG <ref type="bibr" target="#b51">[52]</ref>: (a) Random: it randomly picks graphs from the training dataset. (b) Herding: it selects samples that are closest to the cluster center. Herding is often used in replay-based methods for continual learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>. (c) K-Center: it selects the center samples to minimize the largest distance between a sample and its nearest center. (d) DCG: As vanilla DC <ref type="bibr" target="#b51">[52]</ref> cannot generate discrete structure, we randomly select graphs from training and apply DC to learn the features for them, which we term as DCG. We use the implementations provided by Zhao et al. <ref type="bibr" target="#b51">[52]</ref> for Herding, K-Center and DCG. Note that coreset methods only select existing samples from training while DCG learns the node features.</p><p>Evaluation Protocol. To evaluate the effectiveness of the proposed method, we test the classification performance of GNNs trained with condensed graphs on the aforementioned graph datasets. Concretely, it involves three stages: (1) learning synthetic graphs, (2) training a GCN on the synthetic graphs and (3) test the performance of GCN. We first generate the condensed graphs following the procedure in Algorithm 1. Then we train a GCN classifier with the condensed graphs. Finally we evaluate its classification performance on the real graphs from test set. For baseline methods, we first get the selected/condensed graphs and then follow the same procedure. We repeat the generation process of condensed graphs 5 times with different random seeds and train GCN on these graphs with 10 different random seeds. In all experiments, we report the mean and standard deviation of these results.</p><p>Parameter Settings. When learning the synthetic graphs, we adopt 3-layer GCN with 128 hidden units as the model for gradient matching. The learning rates for structure and feature parameters are set to 1.0 (0.01 for ogbg-molbace and CIFAR10) and 0.01, respectively. We set ? 1 to 1000 and ? to 0.1. Additionally, we use mean pooling to obtain graph representation for all datasets except ogbg-molhiv. We use sum pooling for ogbg-molhiv as it achieves better classification performance on the real dataset. During the test stage, we use GCN with the same architecture and we train the model for 500 epochs (100 epochs for ogbg-molhiv) with an initial learning rate of 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance with Condensed Graphs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Classification Performance Comparison.</head><p>To validate the effectiveness of the proposed framework, we measure the classification performance of GCN trained on condensed graphs. Specifically, we vary the number of learned synthetic graphs per class in the range of {1, 10, 50} ({1, 10, 20} for MUTAG and E-commerce) and train a GCN on these graphs. Then we evaluate the classification performance of the trained GCN on the original test graphs. Following the convention in OGB <ref type="bibr" target="#b11">[12]</ref>, we report the ROC-AUC metric for ogbg-molbace, ogbg-molbbbp and ogbg-molhiv; for other datasets we report the classification accuracy (%). The results are summarized in Table <ref type="table" target="#tab_2">1</ref>. Note that the Ratio column presents the ratio of synthetic graphs to original graphs and we name it as condensation ratio; the Whole Dataset column shows the GCN performance achieved by training on the original dataset. From the table, we make the following observations: (a) The proposed DosCond consistently achieves better performance than the baseline methods under different condensation ratios and different datasets. Notably, when generating only 2 graphs on ogbg-molbace dataset (0.2%), we achieve an ROC-AUC of 0.657 while the performance on full training set is 0.714, which means we approximate 92% of the original performance with only 0.2% data. Likewise, we are able to approximate 96.5% of the original performance on ogbg-molhiv with 0.3% data. By contrast, baselines underperform our method by a large margin. Similar observations can be made on other datasets, which demonstrates the effectiveness of learned synthetic graphs in preserving the information of the original dataset. (b) Increasing the number of synthetic graphs can improve the classification performance. For example, we can approximate the original performance by 89%/93%/98% with 0.2%/2.1%/10.6% data. More synthetic samples indicate more learnable parameters that can preserve the information residing in the original dataset and present more diverse patterns that can help train GNNs better. This observation is in line with our experimental results in Section 3.3.1.</p><p>(c) The performance on CIFAR10 is less promising due to the limit number of synthetic graphs. We posit that the dataset has more complex topology and feature information and thus requires more parameters to preserve sufficient information. However, we note that our method still outperforms the baseline methods especially when producing only 1 sample per class, which suggests that our method is much more data-efficient. Moreoever, we are able to promote the performance on CIFAR10 by learning a larger synthetic set as shown in Section 3.3.1. (d) Learning both synthetic graph structure and node features is necessary for preserving the information in original graph datasets. By checking the performance DCG, which only learns node features based on randomly selected graph structure, we see that DCG underperforms DosCond by a large margin in most cases. This indicates that learning node features solely is sub-optimal for condensing graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Efficiency</head><p>Comparison. Since one of our goals is to enable scalable dataset condensation, we now evaluate the efficiency of DosCond. We compare DosCond with the coreset method Herding, as it is less time-consuming than DCG and generally achieves better performance than other baselines. We adopt the same setting as in Table <ref type="table" target="#tab_2">1</ref>: 1000 iterations for DosCond, i.e., ? 1 = 1000, and 500 epochs (100 epochs for ogbg-molhiv) for pre-training the graph convolutional network as required by Herding. We also note that pre-training the neural network need to go over the whole dataset at every epoch while DosCond only processes a batch of graphs.</p><p>In Table <ref type="table" target="#tab_3">2</ref>, we report the running time on an NVIDIA V100 GPU for CIFAR10, ogbg-molhiv and DD. From the table, we make two observations: (a) DosCond can be faster than Herding. In fact, DosCond requires less training time in all the cases except in DD with 50 graphs per class. Herding needs to fully training the model on the whole dataset to obtain good-quality embedding, which can be quite time-consuming. On the contrary, DosCond only requires matching gradients for ? 1 initializations and does not need to fully train the model on the large real dataset. (b) The running time of DosCond increases with the increase of the number of synthetic graphs ? ? . It is because DosCond processes the condensed graphs at each iteration, of which the time complexity is ? (? ? ?(? 2 ? + ?? 2 )) for an ?-layer GCN. Thus, the additional complexity depends on ? ? . By contrast, the increase of ? ? has little impact on Herding since the process of selecting samples based on pre-defined heuristic is very fast. (c) The average nodes in synthetic graph ? also impacts the training cost of DosCond. For instance, the training cost on ogbg-molhiv (?=26) is much lower than that on DD (?=285), and the gap of cost between the two methods on ogbg-molhiv and DD is very different. As mentioned earlier, it is because the complexity of the forward process in GCN is ? (? ? ?(? 2 ? + ?? 2 )) for ? ? condensed graphs with node size of ?. To summarize, the efficiency difference of Herding and DosCond depends on the number of condensed/selected samples and the training iterations adopted in practice and we empirically found that DosCond consumes less training cost.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Further Investigation</head><p>In this subsection, we perform further investigations to provide a better understanding of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Increasing the Number of Synthetic</head><p>Graphs. We study whether the classification performance can be further boosted when using larger synthetic size. Concretely, we vary the size of the learned graphs from 1 to 300 and report the results of absolute and relative accuracy w.r.t. whole dataset training accuracy for CIFAR10 in Figure <ref type="figure" target="#fig_3">1a</ref>. It is clear to see that both Random and DosCond achieve better performance when we increase the number of samples used for training. Moreover, our method outperforms the random baseline under different condensed dataset sizes. It is worth noting that the performance gap between the two methods diminishes with the increase of the number of samples. This is because the random baseline will finally approach the whole dataset training if we continue to enlarge the size of the condensed set, in which the performance can be considered as the upper bound of DosCond.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Ablation Study.</head><p>To examine how different model components affect the model performance, we perform ablation study on the proposed one-step gradient matching and regularization terms.</p><p>We create an ablation of our method, namely DosCond-Bi, which adopts the vanilla gradient matching scheme that involves a bi-level optimization. Without loss of generality, we compare the training time and classification accuracy of DosCond and DosCond-Bi in the setting of learning 50 graphs/class synthetic graphs on CIFAR10</p><p>dataset. The results are summarized in Figure <ref type="figure">2c</ref> and we can see that DosCond needs approximately 5 minutes to reach the performance of DosCond-Bi trained for 75 minutes, which indicates that DosCond only requires 6.7% training cost. It further demonstrates the efficiency of the proposed one-step gradient matching strategy.</p><p>Next we study the effect of sparsity regularization on DosCond. Specifically, we vary the sparsity coefficient ? in the range of {0, 0.001, 0.01, 0.1, 1, 10} and report the classification accuracy and graph sparsity on DD and NCI datasets in Figure <ref type="figure">2d</ref>. Note that the graph sparsity is defined as the ratio of the number of edges to the square of the number of nodes. As shown in the figure, when ? gets larger, we exert a stronger regularization on the learned graphs and the graphs become more sparse. Furthermore, the increased sparsity does not affect the classification performance. This is a desired property since sparse graphs can save much space for storage and reduce training cost for GNNs. We also remove the regularization of Eq. ( <ref type="formula" target="#formula_17">14</ref>) for ogbg-molhiv, we obtain the performance of 0.724/0.727/0.731 for 1/10/50 graphs per class, which is slightly worse than the one with this regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Visualization.</head><p>We further investigate whether GCN can learn discriminative representations from the synthetic graphs learned by DosCond. Specifically, we use t-SNE <ref type="bibr" target="#b32">[33]</ref> to visualize the learned graph representation from GCN trained on different condensed graphs. We train a GCN on graphs produced by different methods and use it to extract the latent representation for real graphs from test set. Without loss of generality, we provide the t-SNE plots on DD dataset with 50 graphs per class in Figure <ref type="figure">2</ref>. It is observed that the graph representations learned with randomly selected graphs are mixed for different classes. This suggests that using randomly selected graphs cannot help GCN learn discriminative features. Similarly, DCG graphs also resulted in poorly trained GCN that outputs indistinguishable graph representations. By contrast, the representations are well separated for different classes when learned with  (b) ogbg-molhiv Figure <ref type="figure">3</ref>: Scale of the two terms in Eq. ( <ref type="formula" target="#formula_12">11</ref>). DosCond graphs (Figure <ref type="figure">2c</ref>) and they are as discriminative as those learned on the whole training dataset (Figure <ref type="figure">2d</ref>). This demonstrates that the graphs learned by DosCond preserve sufficient information of the original dataset so as to recover the original performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.4</head><p>Scale of the two terms in Eq. <ref type="bibr" target="#b10">(11)</ref>. As mentioned earlier in Section 2.3, the scale of the first term is essentially larger than the second term in Eq. <ref type="bibr" target="#b10">(11)</ref>. We now perform empirical study to verify this statement. Since both terms contain the factor ?, we simply drop it and focus on studying</p><formula xml:id="formula_22">? 1 = ? 2?? ? ? T (? 0 ) -? ? ? ? (? 0 ) ? and ? 2 = 3 2 ? ? ? ?-1 ?? ? ?? ? ? ? ?1 ? A ?? (?) X ? (?) ? 2 .</formula><p>Specifically, we set ? to 500 and ? ? to 50, and plot the changes of these two terms during the training process of DosCond. The results on DD (with mean pooling) and ogbg-molhiv (with sum pooling) are shown in Figure <ref type="figure">3</ref>. We can observe that the scale of ? 1 is much larger than ? 2 at the first few epochs when using mean pooling as shown in Figure <ref type="figure">3a</ref>. By contrast, ? 2 is not negligible when using sum pooling as shown in Figure <ref type="figure">3b</ref> and it is desired to include it as a regularization term in this case. These observations provide support for ours discussion of theoretical analysis in Section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Node Classification</head><p>Next, we investigate whether the proposed method works well in node classification so as to support our analysis in Theorem 2 in Appendix B.2. Specifically, following GCond <ref type="bibr" target="#b14">[15]</ref>, a condensation method for node classification, we use 5 node classification datasets: Cora, Citeseer, Pubmed <ref type="bibr" target="#b17">[18]</ref>, ogbn-arxiv <ref type="bibr" target="#b11">[12]</ref> and Flickr <ref type="bibr" target="#b48">[49]</ref>. The dataset statistics are shown in 5. We follow the settings in GCond to generate one condensed graph for each dataset, train a GCN on the condensed graph, and evaluate its classification performance on the original test nodes. To adopt DosCond into node classification, we replace the bi-level gradient matching scheme in GCond with our proposed one-step gradient matching. The results of classification accuracy and running time per epoch are summarized in Table <ref type="table" target="#tab_4">3</ref>. From the table, we make the following observations: (a) The proposed DosCond achieves similar performance as GCond and the performance is also comparable to the original dataset.</p><p>For example, we are able to approximate the original training performance by 99% with only 2.6% data on Cora. It demonstrates the effectiveness of DosCond in the node classification case and justifies Theorem 2 from an empirical perspective. (b) The training cost of DosCond is essentially lower than GCond as DosCond avoids the expensive bi-level optimization. By examining their running time, we can see that DosCond is up to 40 times faster than GCond. We further note that GCond produces weighted graphs which require storing the edge weights in float formats, while DosCond outputs discrete graph structure which can be stored as binary values. Hence, the graphs learned by DosCond are more memory-efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>Graph Neural Networks. As the generalization of deep neural network to graph data, graph neural networks (GNNs) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> have revolutionized the field of graph representation learning through effectively exploiting graph structural information. GNNs have achieved remarkable performances in basic graphrelated tasks such as graph classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">43]</ref>, link prediction <ref type="bibr" target="#b7">[8]</ref> and node classification <ref type="bibr" target="#b17">[18]</ref>. Recent years have also witnessed their great success achieved in many real-world applications such as recommender systems <ref type="bibr" target="#b7">[8]</ref>, computer vision <ref type="bibr" target="#b20">[21]</ref>, drug discovery <ref type="bibr" target="#b5">[6]</ref> and etc. GNNs take both adjacency matrix and node feature matrix as input and output node-level representations or graph-level representations. Essentially, they follow a message-passing scheme <ref type="bibr" target="#b9">[10]</ref> where each node first aggregates the information from its neighborhood and then transforms the aggregated information to update its representation. Furthermore, there is significant progress in developing deeper GNNs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>, self-supervised GNNs <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref> and graph data augmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>. Dataset Distillation &amp; Dataset Condensation. It is widely received that training neural networks on large datasets can be prohibitively costly. To alleviate this issue, dataset distillation (DD) <ref type="bibr" target="#b35">[36]</ref> aims to distill knowledge of a large training dataset into a small number of synthetic samples. DD formulates the distillation process as a learning-to-learning problem and solves it through bilevel optimization. To improve the efficiency of DD, dataset condensation (DC) <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b51">52]</ref> is proposed to learn the small synthetic dataset by matching the gradients of the network parameters w.r.t. large-real and small-synthetic training data. It has been demonstrated that these condensed samples can facilitate critical applications such as continual learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref>, neural architecture search <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44]</ref> and privacy-preserving scenarios <ref type="bibr" target="#b4">[5]</ref> Recently, following the gradient matching scheme in DC, Jin et al. <ref type="bibr" target="#b14">[15]</ref> propose a condensation method to condense a large-scale graph to a small graph for node classification. Different from <ref type="bibr" target="#b14">[15]</ref> which learns weighted graph structure, we aim to solve the challenge of learning discrete structure and we majorly target at graph classification. Moreover, our method avoids the costly bi-level optimization and is much more efficient than the previous work. A detailed comparison is included in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>Training graph neural networks on a large-scale graph dataset consumes high computational cost. One solution to alleviate this issue is to condense the large graph dataset into a small synthetic dataset. In this work, we propose a novel framework DosCond that adopts a one-step gradient matching strategy to efficiently condenses real graphs into a small number of informative graphs with discrete structures. We further justify the proposed method from both theoretical and empirical perspectives. Notably, our experiments show that we are able to reduce the dataset size by 90% while approximating up to 98% of the original performance. In the future, we plan to investigate interpretable condensation methods and diverse applications of the condensed graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENTAL SETUP A.1 Dataset Statistics and Code</head><p>Dataset statistics are shown in Table <ref type="table" target="#tab_5">4</ref> and<ref type="table" target="#tab_6">5</ref>. We provide our code in the supplementary file for the purpose of reproducibility. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOFS B.1 Proof of Theorem 1</head><p>Let A (?) , X (?) denote the adjacency matrix and the feature matrix of ?-th real graph, respectively. We denote the cross entropy loss on the real samples as ? T (? ) = ? ? ? (A (?) , X (?) , ? ) and denote that on synthetic samples as ? ? (? ) = ? ? (A ? (?) , X ? (?) , ? ). Let ? * denote the optimal parameter and let ? ? be the parameter trained on condensed data at ?-th epoch by optimizing ? ? (? ). For simplicity of notations, we assume A and A ? are already normalized. Part of the proof is inspired from the work <ref type="bibr" target="#b15">[16]</ref>.</p><p>Theorem 1. When we use a linearized ?-layer SGC as the GNN used in condensation, i.e., ? ? (A (?) , X (?) ) = Pool(A ? (?) X (?) W 1 )W 2 with ? = [W 1 ; W 2 ] and assume that all network parameters satisfy ?? ? 2 ? ? 2 (? &gt; 0), we have</p><formula xml:id="formula_23">min ? =0,1,...,? -1 ? T (? ? ) -? T ? * ? ? -1 ?? ? =0 ? 2? ? ?? ? ? T (? ? ) -? ? ? ? (? ? ) ? + 3? 2 ? ? ? ? -1 ?? ? ?? ?? ? ? ? ?1 ? A ?? (?) X ? (?) ? 2<label>(15)</label></formula><p>where ? ? = 1 if we use sum pooling in ? ? ; ? ? = 1 ? ? if we use mean pooling, with ? ? being the number of nodes in ?-th synthetic graph.</p><p>Proof. We start by proving that ? T (? ) is convex and ? ? (? ) is lipschitz continuous when we use ? ? (A (?) , X (?) ) = Pool(A ? (?) X (?) W 1 )W 2 as the mapping function. Before proving these two properties, we first rewrite ? ? (A (?) , X (?) ) as:</p><formula xml:id="formula_24">? ? (A (?) , X (?) ) = 1 ? A ? (?) X (?) W 1 W 2 if use sum pooling, 1 ? ? 1 ? A ? (?) X (?) W 1 W 2 if use mean pooling, (<label>16</label></formula><formula xml:id="formula_25">)</formula><p>where ? is the number of nodes in A (?) and 1 is an ? ? ? 1 matrix filled with constant one. From the above equation we can see that ? ? with different pooling methods only differ in a multiplication factor 1  ? ? . Thus, in the following we focus on ? ? with sum pooling to derive the major proof. I. For ? ? with sum pooling: Substitute W for W 1 W 2 and we have ? ? (A (?) , X (?) ) = 1 ? A ? (?) X (?) W for the case with sum pooling. Next we show that ? T (? ) is convex and ? ? (? ) is lipschitz continuous when we use ? ? (A (?) , X (?) ) = 1 ? A ? (?) X (?) W with ? = W. (a) Convexity of ? T (? ). From chapter 4 of the book <ref type="bibr" target="#b38">[39]</ref>, we know that softmax classification ? (W) = XW with cross entropy loss is convex w.r.t. the parameters W. In our case, the mapping function</p><formula xml:id="formula_26">? ? (A (?) , X (?) ) = 1 ? A ? (?) X (?)</formula><p>W applies an affine function on XW. Given that applying affine function does not change the convexity, we know that ? T (? ) is convex. (b) Lipschitz continuity of ? ? (? ). In <ref type="bibr" target="#b44">[45]</ref>, it shows that the lipschitz constant of softmax regression with cross entropy loss is ?-1 ?? ?X?, where X is the input feature matrix, ? is the number of classes and ? is the number of samples. Since ? ? (? ) is cross entropy loss and ? ? is linear, we know that the ? ? is lipschitz continuous and it satisfies:</p><formula xml:id="formula_27">? ? ? ? (? ) ? ? -1 ?? ? ?? ?? ? ?1 ? A ?? (?) X ? (?) ? 2<label>(17)</label></formula><p>With (a) and (b), we are able to proceed our proof. First, from the convexity of ? T (? ) we have</p><formula xml:id="formula_28">? T (? ? ) -? T ? * ? ? ? ? T (? ? ) ? ? ? -? *<label>(18)</label></formula><p>We can rewrite ? ? ? T (? ? ) ? (? ? -? * ) as follows:</p><formula xml:id="formula_29">? ? ? T (? ? ) ? ? ? -? * = ( ? ? ? T (? ? ) ? -? ? ? ? (? ? ) ? + ? ? ? ? (? ? ) ? ) ? ? -? * = ( ? ? ? T (? ? ) ? -? ? ? ? (? ? ) ? ) ? ? -? * + ? ? ? ? (? ? ) ? ? ? -? *<label>(19)</label></formula><p>Given that we use gradient descent to update network parameters, we have</p><formula xml:id="formula_30">? ? ? ? (? ? ) = 1 ? (? ? -? ? +1 )</formula><p>where ? is the learning rate. Then we have,</p><formula xml:id="formula_31">? ? ? ? (? ? ) ? ? ? -? * = 1 ? (? ? -? ? +1 ) ? ? ? -? * = 1 2? ?? ? -? ? +1 ? 2 + ? ? -? * 2 -? ? +1 -? * 2 = 1 2? ??? ? ? ? (? ? )? 2 + ? ? -? * 2 -? ? +1 -? * 2<label>(20)</label></formula><p>Combining Eq. ( <ref type="formula" target="#formula_28">18</ref>) and Eq. ( <ref type="formula" target="#formula_31">20</ref> </p><p>As we assume that ?? ? 2 ? ? 2 , we have ?? -? * ? 2 ? 2?? ? 2 = 2? 2 . Then Eq. ( <ref type="formula" target="#formula_32">23</ref>) can be rewritten as, </p><p>where ? ? is the number of nodes in ?-th synthetic graph. ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Theorem for Node Classification Case</head><p>We adopt similar notations for representing the data in node classification but note that there is only one graph for node classification task. Let A ? {0, 1} ? ?? , A ? ? {0, 1} ? ? ?? ? denote the adjacency matrix for real graph and synthetic graph, respectively. Let X ? R ? ?? , X ? ? R ? ? ?? denote the feature matrix for real graph and synthetic graph, respectively. We denote the cross entropy loss on the real samples as ? T (? ) and denote that on synthetic samples as ? ? (? ).</p><p>Theorem 2. When we use a ?-layer SGC as the model used in condensation, i.e., ? ? (A, X, ? ) = A ? XW with ? = W and assume that all network parameters satisfy ?? ? 2 ? ? 2 (? &gt; 0), we have min ? =0,1,...,? -1</p><formula xml:id="formula_34">? T (? ? ) -? T ? * ? ? -1 ?? ? =0 ? 2? ? ?? ? ? T (? ? ) -? ? ? ? (? ? ) ? + 3? 2 ? ? ? ? -1 ?? ? ?A ?? X ? ?<label>(28)</label></formula><p>Proof. We start by proving that ? T (? ) is convex and ? ? (? ) is lipschitz continuous when ? ? (A, X, ? ) = A ? XW. (a) Convexity of ? T (? ): Similar to the graph classification case, the Hessian matrix of ? T (? ) in node classification is positive semidefinite and thus ? T (? ) is convex. (b) Lipschitz continuity of ? ? (? ): As shown in <ref type="bibr" target="#b44">[45]</ref>, the lipschitz constant of softmax regression with cross entropy loss is ?-1 ?? ?X? with ? being the number of classes and ? being the number of samples. Thus, we know that the lipschitz constant of ? ? (? ) is ?-1 ?? ? ?A ?? X ? ?, which indicates ? ? ? ? (? ) ? ?-1 ?? ? ?A ?? X ? ?. From the convexity of ? T (? ), we still have the following inequality (see Eq. ( <ref type="formula">24</ref>)). Then recall that ? ? (? ) is lipschitz continuous and ? ? ? ? (? ) ? ?-1 ?? ? ?A ?? X ? ?, and combine min </p><formula xml:id="formula_35">?? ? =0 ? 2? ? ?? ? ? T (? ? ) -? ? ? ? (? ? ) ? + 3? 2 ? ? ? ? -1 ?? ? ?A ?? X ? ?<label>(30)</label></formula><p>?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Parameter analysis w.r.t. the sparsity regularization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>????? 2 ? ( 24 ) 1 ? 2 ?? ( 25 ) 1 ?</head><label>22412251</label><figDesc>? (? ? )? ? ? * ? 2? ?? ? ? ? (? ? ) -? ? ? ? (? ? ) ? ? ? (? ? )? 2 + ?Recall that ? ? (? ) is lipschitz continuous as shown in Eq.<ref type="bibr" target="#b16">(17)</ref>, and combine min ? =0,1,...,? -1(? T (? ? )? T (? * )) ? ? -=0 ? T (? ? )-? T (? * ) ? : min ? =0,1,...,? -1 ? T (? ? ) -? T ? * ? ?? ? ? T (? ? ) -? ? ? ? (? ? ) ? + ? (? -1) 2 2? 2 ? ?2 ?? ? ?1 ? A ?? (?) X ? (?) ? 2 + ? Then we choose ? = ? ? ? ?? ? ?1 ? A ?? (? ) X ? (? ) ? 2and we can get:min ? =0,1,...,? -1 ? T (? ? ) -? T ? * ? ?? ? ? T (? ? ) -? ? ? ? (? ? )? ? with mean pooling: Following similar derivation as in the case of sum pooling, we havemin ? =0,1,...,? -1 ? T (? ? ) -? T ? * ? ?? ? ? T (? ? ) -? ? ? ? (? ? ) ? ?1 ? A ?? (?) X ? (?) ? 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>? 1 ?</head><label>1</label><figDesc>(? T (? ? )? T (? * )) ? ? -=0 ? T (? ? )-? T (? * ) ? : min ? =0,1,...,? -1 ? T (? ? )? T ? * ? ? -1 ?? ? =0 ? 2? ? ?? ? ? T (? ? ) -? ? ? ? (? ? ) ? + ? (? -1) 2 2? 2 ? ?2 ?A ?? X ? ? 2 + ? 2 ? ?(29)Then we choose? = ? ? ? ?A ?? X ? ?and we can get:min ? =0,1,...,? -1 ? T (? ? )? T ? * ? ? -1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>t. large-real data and small-synthetic data for model parameters at every training epoch. Hence, the model parameters trained on synthetic data will be close to these trained on real data at every training epoch. Let ? ? denote the network parameters at the ?-th epoch and ? ? ? indicate the neural network parameterized by ? ? . The condensation objective is expressed as:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Pre-defined condensed labels Y ? , graph neural network ? ? , temperature ?, desired sparsity ?, regularization coefficient ?, learning rates ? 1 , ? 2 , number of epochs ? 1 . 3: Initialize ?, X ? 4: for ? = 0, . . . , ? 1 -1 do</figDesc><table><row><cell>5:</cell><cell>Sample ? 0 ? ? ? 0</cell></row><row><cell>6:</cell><cell></cell></row></table><note><p>1: Input: Training data T = (A, X, Y) 2: Required:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The classification performance comparison to baselines. We report the ROC-AUC for the first three datasets and accuracies (%) for others. Whole Dataset indicates the performance with original dataset.</figDesc><table><row><cell></cell><cell cols="2">Graphs/Cls. Ratio</cell><cell>Random</cell><cell>Herding</cell><cell>K-Center</cell><cell>DCG</cell><cell>DosCond</cell><cell>Whole Dataset</cell></row><row><cell>ogbg-molbace (ROC-AUC)</cell><cell>1 10 50</cell><cell cols="6">0.2% 0.580?0.067 0.548?0.034 0.548?0.034 0.623?0.046 0.657?0.034 1.7% 0.598?0.073 0.639?0.039 0.591?0.056 0.655?0.033 0.674?0.035 8.3% 0.632?0.047 0.683?0.022 0.589?0.025 0.652?0.013 0.688?0.012</cell><cell>0.714?0.005</cell></row><row><cell>ogbg-molbbbp (ROC-AUC)</cell><cell>1 10 50</cell><cell cols="6">0.1% 0.519?0.016 0.546?0.019 0.546?0.019 0.559?0.044 0.581?0.005 1.2% 0.586?0.040 0.605?0.019 0.530?0.039 0.568?0.032 0.605?0.008 6.1% 0.606?0.020 0.617?0.003 0.576?0.019 0.579?0.032 0.620?0.007</cell><cell>0.646?0.004</cell></row><row><cell>ogbg-molhiv (ROC-AUC)</cell><cell>1 10 50</cell><cell cols="6">0.01% 0.719?0.009 0.721?0.002 0.721?0.002 0.718?0.013 0.726?0.003 0.06% 0.720?0.011 0.725?0.006 0.713?0.009 0.728?0.002 0.728?0.005 0.3% 0.721?0.014 0.725?0.003 0.725?0.006 0.726?0.010 0.731?0.004</cell><cell>0.757?0.007</cell></row><row><cell>DD (Accuracy)</cell><cell>1 10 50</cell><cell cols="2">0.2% 2.1% 10.6% 67.29?1.53 57.69?4.92 64.69?2.55</cell><cell>61.97?1.32 69.79?2.30 73.95?1.70</cell><cell>61.97?1.32 63.46?2.38 67.41?0.92</cell><cell>58.81?2.90 61.84?1.44 61.27?1.01</cell><cell>70.42?2.21 73.53?1.13 77.04?1.86</cell><cell>78.92?0.64</cell></row><row><cell>MUTAG (Accuracy)</cell><cell>1 10 20</cell><cell cols="2">1.3% 13.3% 77.89?7.55 67.47?9.74 26.7% 78.21?5.13</cell><cell>70.84?7.71 80.42?1.89 80.00?1.10</cell><cell>70.84?7.71 81.00?2.51 82.97?4.91</cell><cell>75.00?8.16 82.66?0.68 82.89?1.03</cell><cell>82.21?1.61 82.76?2.31 83.26?2.34</cell><cell>88.63?1.44</cell></row><row><cell>NCI1 (Accuracy)</cell><cell>1 10 50</cell><cell>0.1% 0.6% 3.0%</cell><cell>51.27?1.22 54.33?3.14 58.51?1.73</cell><cell>53.98?0.67 57.11?0.56 58.94?0.83</cell><cell>53.98?0.67 53.21?1.44 56.58?3.08</cell><cell>51.14?1.08 51.86?0.81 52.17?1.90</cell><cell>56.58?0.48 58.02?1.05 60.07?1.58</cell><cell>71.70?0.20</cell></row><row><cell>CIFAR10 (Accuracy)</cell><cell>1 10 50</cell><cell cols="2">0.06% 15.61?0.52 0.2% 23.07?0.76 1.1% 30.56?0.81</cell><cell>22.38?0.49 28.81?0.35 33.94?0.37</cell><cell>22.37?0.50 20.93?0.62 24.17?0.51</cell><cell>21.60?0.42 29.27?0.77 34.47?0.52</cell><cell>24.70?0.70 30.70?0.23 35.34?0.14</cell><cell>50.75?0.14</cell></row><row><cell>E-commerce (Accuracy)</cell><cell>1 10 20</cell><cell>0.2% 0.9% 3.6%</cell><cell>51.31+-2.89 54.99+-2.74 57.80+-3.58</cell><cell>52.18+-0.25 56.83+-0.87 62.56+-0.71</cell><cell cols="3">52.36+-0.38 57.14+-1.72 60.82+-1.23 56.49+-0.36 61.03+-1.32 64.73+-1.34 62.76+-0.45 64.92+-1.35 67.71+-1.22</cell><cell>69.25?0.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of running time (minutes).</figDesc><table><row><cell></cell><cell cols="2">CIFAR10</cell><cell cols="2">ogbg-molhiv</cell><cell>DD</cell><cell></cell></row><row><cell cols="7">G./Cls. Herding DosCond Herding DosCond Herding DosCond</cell></row><row><cell>1</cell><cell>44.5m</cell><cell>4.7m</cell><cell>4.3m</cell><cell>0.66m</cell><cell>1.6m</cell><cell>1.5m</cell></row><row><cell>10</cell><cell>44.5m</cell><cell>4.9m</cell><cell>4.3m</cell><cell>0.67m</cell><cell>1.6m</cell><cell>1.5m</cell></row><row><cell>50</cell><cell>44.5m</cell><cell>5.7m</cell><cell>4.3m</cell><cell>0.68m</cell><cell>1.6m</cell><cell>2.0m</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Node classification accuracy (%) comparison. The numbers in parentheses indicate the running time for 100 epochs and ? indicates the ratio of number of nodes in the condensed graph to that in the original graph. , ? =2.6% Citeseer, ? =1.8% Pubmed, ? =0.3% Arxiv, ? =0.25% Flickr, ? =0.1%</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CoraGCond 80.1 (75.9s)</cell><cell>70.6 (71.8s)</cell><cell>77.9 (51.7s)</cell><cell>59.2 (494.3s)</cell><cell>46.5 (51.9s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DosCond</cell><cell>80.0 (3.5s)</cell><cell>71.0 (2.8s)</cell><cell>76.0 (1.3s)</cell><cell>59.0 (32.9s)</cell><cell>46.1 (14.3s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Whole Dataset</cell><cell>81.5</cell><cell>71.7</cell><cell>79.3</cell><cell>71.4</cell><cell>47.2</cell></row><row><cell></cell><cell>0.12 0.10</cell><cell></cell><cell></cell><cell></cell><cell>1 2</cell></row><row><cell>Loss Value</cell><cell>0.04 0.06 0.08</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.02</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.00</cell><cell>0</cell><cell>200</cell><cell>400 Epoch 600</cell><cell>800 1000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Graph classification dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell>Type</cell><cell cols="4">#Clases #Graphs Avg. Nodes Avg. Edges</cell></row><row><cell>CIFAR10</cell><cell>Superpixel</cell><cell>10</cell><cell>60,000</cell><cell>117.6</cell><cell>941.07</cell></row><row><cell cols="2">ogbg-molhiv Molecule</cell><cell>2</cell><cell>41,127</cell><cell>25.5</cell><cell>54.9</cell></row><row><cell cols="2">ogbg-molbace Molecule</cell><cell>2</cell><cell>1,513</cell><cell>34.1</cell><cell>36.9</cell></row><row><cell cols="2">ogbg-molbbbp Molecule</cell><cell>2</cell><cell>2,039</cell><cell>24.1</cell><cell>26.0</cell></row><row><cell>MUTAG</cell><cell>Molecule</cell><cell>2</cell><cell>188</cell><cell>17.93</cell><cell>19.79</cell></row><row><cell>NCI1</cell><cell>Molecule</cell><cell>2</cell><cell>4,110</cell><cell>29.87</cell><cell>32.30</cell></row><row><cell>DD</cell><cell>Molecule</cell><cell>2</cell><cell>1,178</cell><cell>284.32</cell><cell>715.66</cell></row><row><cell cols="2">E-commerce Transaction</cell><cell>2</cell><cell>1,109</cell><cell>33.7</cell><cell>56.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Node classification dataset statistics.</figDesc><table><row><cell cols="2">Dataset #Nodes</cell><cell>#Edges</cell><cell cols="2">#Classes #Features</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell></row><row><cell cols="2">Pubmed 19,717</cell><cell>44,338</cell><cell>3</cell><cell>500</cell></row><row><cell>Arxiv</cell><cell cols="2">169,343 1,166,243</cell><cell>40</cell><cell>128</cell></row><row><cell>Flickr</cell><cell>89,250</cell><cell>899,756</cell><cell>7</cell><cell>500</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>) we have,? T (? ? ) -? T ? * ? (? ? ? T (? ? ) ? -? ? ? ? (? ? ) ? ) ? ? -? * + 1 2? ??? ? ? ? (? ? )? 2 + ? ? -? * 2 -? ? +1 -? * 2We sum up the two sides of the above inequality for different values of ? ? [0,? -1]:T (? ? )? T ? * ? ? ? T (? ? ) ? -? ? ? ? (? ? ) ? ) ? ? -? * ?? ? -? * ? 2 ? 0, we have ? ? T (? ? ) ? -? ? ? ? (? ? ) ? ) ? ? -? * ??? ? ? ? (? ? )? 2 + 1 2? ? 0 -? * 2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(21)</cell></row><row><cell>? -1 ?? ? =0</cell><cell>? ? -1 ?? ? =0 (? + 1 2? ? -1 ?? ? =0 ??? ? ? ? (? ? )? 2 +</cell><cell>1 2?</cell><cell>? 0 -?  *  2 -</cell><cell>1 2?</cell><cell>? ? -?  *  2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(22)</cell></row><row><cell cols="2">Since 1 2? ? -1 ?? ? =0 ? ? -1 ?? ? =0 1 ? -1 ?? (? + 2? ? =0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>T (? ? ) -? T ? * ?</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>If an entry in the real adjacency matrix is 1, the corresponding value in ? is initialized as a large value, e.g.,5.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p><rs type="person">Wei Jin</rs> and <rs type="person">Jiliang Tang</rs> are supported by the <rs type="funder">National Science Foundation (NSF)</rs> under grant numbers <rs type="grantNumber">IIS1714741</rs>, <rs type="grantNumber">CNS1815636</rs>, <rs type="grantNumber">IIS1845081</rs>, <rs type="grantNumber">IIS1907704</rs>, <rs type="grantNumber">IIS1928278</rs>, <rs type="grantNumber">IIS1955285</rs>, <rs type="grantNumber">IOS2107215</rs>, and <rs type="grantNumber">IOS2035472</rs>, the <rs type="funder">Army Research Office (ARO)</rs> under grant number <rs type="grantNumber">W911NF-21-1-0198</rs>, and <rs type="funder">Amazon.com, Inc.</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jREAxEQ">
					<idno type="grant-number">IIS1714741</idno>
				</org>
				<org type="funding" xml:id="_cMHSp7r">
					<idno type="grant-number">CNS1815636</idno>
				</org>
				<org type="funding" xml:id="_GRh4GHW">
					<idno type="grant-number">IIS1845081</idno>
				</org>
				<org type="funding" xml:id="_khYfMMv">
					<idno type="grant-number">IIS1907704</idno>
				</org>
				<org type="funding" xml:id="_bsNVq6Q">
					<idno type="grant-number">IIS1928278</idno>
				</org>
				<org type="funding" xml:id="_6d4wBfx">
					<idno type="grant-number">IIS1955285</idno>
				</org>
				<org type="funding" xml:id="_xqbaVbb">
					<idno type="grant-number">IOS2107215</idno>
				</org>
				<org type="funding" xml:id="_xEVTD4k">
					<idno type="grant-number">IOS2035472</idno>
				</org>
				<org type="funding" xml:id="_gNmcme7">
					<idno type="grant-number">W911NF-21-1-0198</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Concrete autoencoders for differentiable feature selection and reconstruction</title>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Fatih Balin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09346</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Francisco M Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicol?s</forename><surname>Mar?n-Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Data augmentation for deep graph learning: A survey</title>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08235</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Privacy for Free: How does Dataset Condensation Help Privacy?</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjuan</forename><surname>Lyu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In ICML</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Graph Neural Networks for Social Recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Yihong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Facility location: concepts, models, algorithms and case studies</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Zanjirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farahani</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Hekmatfar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Few-shot graph learning for molecular property prediction</title>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Herr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Wiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2559" to="2567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Feature Overcorrelation in Deep Graph Neural Networks: A New Perspective</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph Structure Learning for Robust Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Xianfeng Tang, Suhang Wang, and Jiliang Tang</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Graph Condensation for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<idno>ICLR 2022</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GRAD-MATCH: Gradient Matching based Data Subset Selection for Efficient Deep Model Training</title>
		<author>
			<persName><forename type="first">Krishnateja</forename><surname>Killamsetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Durga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abir</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Iyer</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonhyun</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14959</idno>
		<title level="m">Dataset Condensation via Efficient Synthetic-Data Parameterization</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dataset Condensation with Contrastive Signals</title>
		<author>
			<persName><forename type="first">Saehyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangwon</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep-GCNs: Can GCNs Go As Deep As CNNs?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">DARTS: Differentiable Architecture Search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards deeper graph neural networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In KDD</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generating 3D Molecules for Target Protein Binding</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzhi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanji</forename><surname>Uchino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koji</forename><surname>Maruhashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franka</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dataset Meta-Learning from Kernel Ridge-Regression</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dataset distillation with infinitely wide convolutional networks</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lechao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><surname>Sylvestre-Alvise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><surname>Lampert</surname></persName>
		</author>
		<title level="m">iCaRL: Incremental Classifier and Representation Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Active Learning for Convolutional Neural Networks: A Core-Set Approach</title>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transferring robustness for graph neural network against poisoning attacks</title>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Semi-Supervised Graph Attentive Network for Financial Fraud Detection</title>
		<author>
			<persName><forename type="first">Daixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanhui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Dataset distillation. ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph Neural Networks: Self-supervised Learning</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Neural Networks: Foundations, Frontiers, and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="391" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving Fairness in Graph Neural Networks via Mitigating Sensitive Attribute Leakage</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Watt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Borhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<title level="m">Machine learning refined: Foundations, algorithms, and applications</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Herding dynamical weights to learn</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Dataset Pruning: Reducing Training Data by Examining Generalization Influence</title>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeke</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09329</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">LipschitzLR: Using theoretically computed adaptive learning rates for fast convergence</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Yedida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snehanshu</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tejas</forename><surname>Prashanth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1460" to="1478" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning Automated</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">When Does Self-Supervision Help Graph Convolutional Networks?</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph Sampling Based Inductive Learning Method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dataset Condensation with Differentiable Siamese Augmentation</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (Proceedings of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04181</idno>
		<title level="m">Dataset Condensation with Distribution Matching</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Dataset Condensation with Gradient Matching</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konda</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08871</idno>
		<title level="m">Graph Data Augmentation for Graph Machine Learning: A Survey</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In AAAI</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
