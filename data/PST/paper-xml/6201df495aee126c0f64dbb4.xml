<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HANDLING DISTRIBUTION SHIFTS ON GRAPHS: AN INVARIANCE PERSPECTIVE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-02-05">5 Feb 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjunchi@sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
							<email>daviwipf@amazon.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering Shanghai</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering Shanghai</orgName>
								<orgName type="institution">Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">AWS Shanghai AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HANDLING DISTRIBUTION SHIFTS ON GRAPHS: AN INVARIANCE PERSPECTIVE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-02-05">5 Feb 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2202.02466v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is increasing evidence suggesting neural networks' sensitivity to distribution shifts, so that research on out-of-distribution (OOD) generalization comes into the spotlight. Nonetheless, current endeavors mostly focus on Euclidean data, and its formulation for graph-structured data is not clear and remains under-explored, given the two-fold fundamental challenges: 1) the inter-connection among nodes in one graph, which induces non-IID generation of data points even under the same environment, and 2) the structural information in the input graph, which is also informative for prediction. In this paper, we formulate the OOD problem for node-level prediction on graphs and develop a new domain-invariant learning approach, named Explore-to-Extrapolate Risk Minimization, that facilitates GNNs to leverage invariant graph features for prediction. The key difference to existing invariant models is that we design multiple context explorers (specified as graph editers in our case) that are adversarially trained to maximize the variance of risks from multiple virtual environments. Such a design enables the model to extrapolate from a single observed environment which is the common case for node-level prediction. We prove the validity of our method by theoretically showing its guarantee of a valid OOD solution and further demonstrate its power on various real-world datasets for handling distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As the demand for handling in-the-wild unseen instances draws increasing concerns, out-ofdistribution (OOD) generalization <ref type="bibr" target="#b39">(Mansour et al., 2009;</ref><ref type="bibr" target="#b9">Blanchard et al., 2011;</ref><ref type="bibr" target="#b40">Muandet et al., 2013;</ref><ref type="bibr" target="#b22">Gong et al., 2016)</ref> occupies a central role in the ML community. Yet, recent evidence suggests that deep neural networks can be sensitive to distribution shifts, exhibiting unsatisfactory performance within new environments, e.g., <ref type="bibr" target="#b6">Beery et al. (2018)</ref>; <ref type="bibr" target="#b51">Su et al. (2019)</ref>; <ref type="bibr" target="#b43">Recht et al. (2019)</ref>; <ref type="bibr" target="#b38">Mancini et al. (2020)</ref>. A more concerning example is that a model for COVID-19 detection exploits undesired 'shortcuts' from data sources (e.g., hospitals) to boost training accuracy <ref type="bibr" target="#b18">(DeGrave et al., 2020)</ref>.</p><p>Recent studies of the OOD generalization problem like <ref type="bibr" target="#b44">Rojas-Carulla et al. (2018)</ref>; <ref type="bibr" target="#b10">Bühlmann (2018)</ref>; <ref type="bibr" target="#b22">Gong et al. (2016)</ref>; <ref type="bibr" target="#b3">Arjovsky et al. (2019)</ref> treat the cause of distribution shifts between training and testing data as a potential unknown environmental variable e. Assuming that the goal is to predict target label y given associated input x, the environmental variable would impact the underlying data generating distribution p(x, y|e) = p(x|e)p <ref type="bibr">(y|x, e)</ref>. With E as the support of environments, f (•) as a prediction model and l(•, •) as a loss function, the OOD problem could be formally represented as min f max e∈E E (x,y)∼p(x,y|e=e) [l(f (x), y)|e].</p><p>(1)</p><p>Such a problem is hard to solve since the observations in training data cannot cover all the environments in practice. Namely, the actual demand is to generalize a model trained with data from p(x, y|e = e 1 ) to new data from p(x, y|e = e 2 ). Recent research opens a new possibility via learning domain-invariant models <ref type="bibr" target="#b3">(Arjovsky et al., 2019)</ref> under a cornerstone data-generating assumption: there exists a portion of information in x that is invariant for prediction on y across different environments. Based on this, the key idea is to learn a equipredictive representation model h that gives rise to equal conditional distribution p(y|h(x), e = e) for ∀e ∈ E. The implication is that such a representation h(x) will bring up equally (optimal) performance for a downstream classifier under arbitrary environments. The model p(y|x) with such a property is called as invariant model/predictor. Several up-to-date studies develop new objective designs and algorithms for learning invariant models, showing promising power for tackling OOD generalization <ref type="bibr" target="#b11">(Chang et al., 2020;</ref><ref type="bibr" target="#b1">Ahuja et al., 2020;</ref><ref type="bibr">Krueger et al., 2021;</ref><ref type="bibr" target="#b36">Liu et al., 2021;</ref><ref type="bibr" target="#b16">Creager et al., 2021;</ref><ref type="bibr" target="#b33">Koyama &amp; Yamaguchi, 2021)</ref>.</p><p>While the OOD problem is well-established in certain settings (where the dataset can be obviously modeled as a set of i.i.d. generated pairs (x, y) from p(x, y|e)), its formulation for graph-structured data, especially for node-level tasks on graphs (where we note that each node in the graph corresponds to an instance), is not clear and remains as an open problem. Compared with classic data format (e.g. vision or texts), graph-structured data have two fundamental differences: 1) the non-independent and non-identically distributed nature exists in data generation even within the same environment, embodied with the inter-connection among data points in one graph; 2) the structural information also plays a role for prediction beside the node features. These differences bring up unique technical challenges for handling distribution shifts of node-level tasks on graphs.</p><p>Distribution shifts indeed widely exist in real-world graphs. For instance, in citation networks, the distributions for paper citations (the input) and subject areas/topics (the label) would go through significant change as time goes by <ref type="bibr" target="#b28">(Hu et al., 2020b)</ref>. In social networks, the distributions for users' friendships (the input) and their activity (the label) would highly depend on when/where the networks are collected <ref type="bibr" target="#b19">(Fakhraei et al., 2015)</ref>. In financial networks <ref type="bibr" target="#b41">(Pareja et al., 2020)</ref>, the payment flows between transactions (the input) and the appearance of illicit transactions (the label) would have strong correlation with some external contextual factors (like time and market). In these cases, neural models built on graph-structured data, particularly, Graph Neural Networks (GNNs) which are the common choice, need to effectively deal with OOD data during test time. Moreover, as GNNs have become popular and easy-to-implement tools for modeling relational structures in broad AI areas <ref type="bibr">(vision, texts, audio, etc.)</ref>, enhancing its robustness to distribution shifts is a pain point for building general AI systems, especially applied to high-stake applications like autonomous driving <ref type="bibr" target="#b17">(Dai &amp; Gool, 2018</ref><ref type="bibr">), medical diagnosis (AlBadawy et al., 2018)</ref>, criminal justice <ref type="bibr" target="#b7">(Berk et al., 2018)</ref>, etc.</p><p>In this paper, we endeavor to 1) formulate the OOD problem for node-level tasks on graphs, 2) develop a new learning approach based on an invariance principle, 3) provide theoretical results to dissect its rationale, and 4) design comprehensive experiments to show its practical efficacy. Concretely:</p><p>1. To accommodate the non-IID nature of nodes in a graph, we fragment a graph into a set of egographs for centered nodes and decompose the data-generating process into: 1) sampling a whole input graph and 2) sampling each node's label conditioned the ego-graph. Based on this, we can inherit the spirit of Eq. 1 to formulate the OOD problem for node-level tasks over graphs (see Section 2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>To take into account structural information that is useful for prediction, we first re-formulate the invariant assumption used in prior arts with recursive computation on the induced BFS trees of ego-graphs, inspired by the Weisfeiler-Lehman test <ref type="bibr" target="#b55">(Weisfeiler &amp; Lehman, 1968)</ref>. Then, to endow GNNs with enough ability for handling distribution shifts, we devise a new learning approach, entitled Explore-to-Extrapolate Risk Minimization, that aims GNNs at minimizing the mean and variance of risks from multiple environments that are simulated by adversarial context generators (instantiated as graph editers), as shown in Fig. <ref type="figure" target="#fig_0">1</ref>(a) (see Section 2.2 and 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>To shed more insights on the rationales of the proposed approach and its relationship with the formulated OOD problem, we prove that our objective can guarantee a valid solution for the  formulated OOD problem given some mild conditions and furthermore, an upper bound on the OOD error can be effectively controlled when minimizing the training error (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>To evaluate the approach, we design a comprehensive set of experiments on diverse real-world nodelevel prediction datasets that entail distribution shifts from artificial spurious features, cross-domain transfers and dynamic graph evolution. We also apply our approach to distinct GNN backbones (GCN, GAT, GraphSAGE, GCNII and GPRGNN), and the results show that it consistently outperforms standard empirical risk minimization with promising improvements on OOD data (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM FORMULATION</head><p>In this section, we present our problem formulation for the OOD node-level prediction problem on graphs and then introduce a cornerstone invariance assumption for data generation. All the random variables are denoted as bold letters while the corresponding realizations are denoted as thin letters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">OUT-OF-DISTRIBUTION PROBLEM FOR GRAPH-STRUCTURED DATA</head><p>An input graph G = (A, X) contains two-fold information<ref type="foot" target="#foot_0">1</ref> : an adjacency matrix A = {a vu |v, u ∈ V } and node features X = {x v |v ∈ V } where V denotes node set. Apart from these, each node in the graph has a label, which can be represented as a vector Y = {y v |v ∈ V }. We define G as a random variable of input graphs and Y as a random variable of node label vectors. Such a definition takes a global view and treat the input graph as a whole. Based on this, one can adapt the definition of general OOD problem Eq. 1 via instantiating the input as G and the target as Y, and then the data generation can be characterized as p(G, Y|e) = p(G|e)p(Y|G, e) where e is a random variable of environments that is a latent variable and impacts data distribution.</p><p>However, the above definition makes little sense in node-level problems where in most cases there is a single input graph that contains a massive number of nodes. To make the problem-solving reasonable, we instead take a local view and investigate each node's ego-graph that has influence on the centered node. Assume v as a random variable of nodes. We define node v's L-hop neighbors as N v (where L is an arbitrary integer) and the nodes in N v form an ego-graph G v which consists of a (local) node feature matrix</p><formula xml:id="formula_0">X v = {x u |u ∈ N v } and a (local) adjacency matrix A v = {a uw |u, w ∈ N v }.</formula><p>Use G v as a random variable of ego-graphs<ref type="foot" target="#foot_1">2</ref> whose realization is</p><formula xml:id="formula_1">G v = (A v , X v ).</formula><p>Besides, we define y as a random variable of node labels. In this way, we can fragment a whole graph as a set of instances {(G v , y v )} v∈V where G v denotes an input and y v is a target. Notice that the ego-graph can be seen as a Markov blanket for the centered node, so the conditional distribution p(Y|G, e) can be decomposed as a product of |V | independent and identical marginal distributions p(y|G v , e).</p><p>Therefore, the data generation of {(G v , y v )} v∈V from a distribution p(G, Y|e) can be considered as a two-step procedure: 1) the entire input graph is generated via G ∼ p(G|e) which can then be fragmented into a set of ego-graphs {G v } v∈V ; 2) each node's label is generated via y ∼ p(y|G v = G v , e). Then the OOD node-level prediction problem can be formulated as: given training data {G v , y v } v∈V from p(G, Y|e = e), the model needs to handle testing data {G v , y v } v∈V from a new distribution p(G, Y|e = e ). Denote E as the support of environments, f as a predictor model with ŷ = f (G v ) and l(•, •) as a loss function. More formally, the OOD problem can be written as:</p><formula xml:id="formula_2">min f max e∈E E G∼p(G|e=e) 1 |V | v∈V E y∼p(y|Gv=Gv,e=e) [l(f (G v ), y)] .</formula><p>(2)</p><p>We remark that the first-step sampling G ∼ p(G|e = e) can be ignored since in most cases one only has a single input graph in the context of node-level prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">INVARIANT FEATURES FOR NODE-LEVEL PREDICTION ON GRAPHS</head><p>To solve the OOD problem Eq. 2 is impossible without any prior domain knowledge or structural assumptions since one only has access to data from limited environments in the training set. Recent studies <ref type="bibr" target="#b44">(Rojas-Carulla et al., 2018;</ref><ref type="bibr" target="#b3">Arjovsky et al., 2019)</ref> propose to learn invariant predictor models which resorts to an assumption for data-generating process: the input instance contains a portion of features (i.e., invariant features) that 1) contributes to sufficient predictive information for the target and 2) gives rise to equally (optimal) performance of the downstream classifier across environments.</p><p>With our definition in Section 2.1, for node-level prediction on graphs, each input instance is an ego-graph G v with target label y v . It seems not straightforward for how to define invariant features on graphs given two observations: 1) the ego-graph possesses a hierarchical structure for associated nodes (i.e., G v induces a BFS tree rooted at v where the l-th layer contains the l-order neighbored nodes N (l) v ) and 2) the nodes in each layer are permutation-invariant and variable-length. Inspired by Weisfeiler-Lehman test, we can adapt the definition of invariance assumption in prior arts <ref type="bibr" target="#b44">(Rojas-Carulla et al., 2018;</ref><ref type="bibr" target="#b22">Gong et al., 2016;</ref><ref type="bibr" target="#b3">Arjovsky et al., 2019;</ref><ref type="bibr" target="#b33">Koyama &amp; Yamaguchi, 2021;</ref><ref type="bibr" target="#b36">Liu et al., 2021)</ref> to accommodate structural information in graph data: Assumption 1. (Invariance Property of Node-Level Prediction) Assume input feature dimension as d 0 . There exists a sequence of (non-linear) functions {h * l } L l=0 where h * l : R d0 → R d and a permutation-invariant function Γ : R d m → R d , which gives a node-level readout r v = r (L) v that is calculated in a recursive way: r</p><formula xml:id="formula_3">(l) u = Γ{r (l−1) w |w ∈ N (1) u ∪{u}} for l = 1, • • • , L and r (0) u = h * l (x u ) if u ∈ N (l)</formula><p>v . Denote r as a random variable of r v and it satisfies that 1) (Invariance condition): p(y|r, e) = p(y|r), and 2) (Sufficiency condition): y = c * (r) + n, where c * is a non-linear function and n is an independent noise.</p><p>A more intuitive illustration for the above computation is presented in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. The node-level readout r v aggregates the information from neighbored nodes recursively along the structures of BFS tree given by G v . Essentially, the above definition assumes that in each layer the neighbored nodes contain a portion of causal features that contribute to stable prediction for y across different e. Such a definition possesses two merits: 1) the (non-linear) transformation h * l can be different across layers, and 2) for arbitrary node u in the original graph G, its causal effect on distinct centered nodes v could be different dependent on its relative position in the ego-graph G v . Therefore, this formulation gives rise to enough flexibility and capacity for modeling on graph data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We next present our solution for the challenging OOD problem. Before going into the formal method, we first introduce a motivating example based on Assumption 1 to provide some high-level intuition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MOTIVATING EXAMPLE</head><p>We consider a linear toy example and assume 1-layer graph convolution for illustration. Namely, the ego-graph G v (and N v ) only contains the centered node and its 1-hop neighbors. We simplify the h * and c * in Assumption 1 as identity mappings and instantiate Γ as a mean pooling function. Then we assume 2-dim node features</p><formula xml:id="formula_4">x v = [x 1 v , x 2 v ]</formula><p>and</p><formula xml:id="formula_5">y v = 1 |N v | u∈Nv x 1 u + n 1 v , x 2 v = 1 |N v | u∈Nv y u + n 2 v + ,<label>(3)</label></formula><p>where n 1 v and n 2 v are independent standard normal noise and is a random variable with zero mean and non-zero variance dependent on environment e. In Fig. <ref type="figure" target="#fig_0">1(c</ref>) we show the dependency among these random variables in a graphical representation and instantiate them in an example of citation networks, where a paper's published avenue is an invariant feature for predicting the paper's sub-area while its citation index (a spurious feature) is affected by both the label and the environment.</p><p>Based on this, we consider a vanilla GCN as the predictor model ŷv = 1 |Nv| u∈Nv θ 1 x 1 u + θ 2 x 2 u . Then the ideal solution for the predictor model is [θ 1 , θ 2 ] = <ref type="bibr">[1,</ref><ref type="bibr">0]</ref>. This indicates that the GCN identifies the invariant feature, i.e., x 1 v insensitive to environment changes. However, here we show a negative result when using standard empirical risk minimization. Proposition 1. Let the risk under environment e be R(e) = 1</p><formula xml:id="formula_6">|V | v∈V E y|Gv=Gv [ ŷv − y v 2 2 ]. The unique optimal solution for objective min θ E e [R(e)] would be [θ 1 , θ 2 ] = [ 1+σ 2 e 2+σ 2 e , 1 2+σ 2 e ]</formula><p>where σ e &gt; 0 denotes the standard deviation of across environments.</p><p>This indicates that directly minimizing the expectation of risks across environments would inevitably lead the model to rely on spurious correlation (x 2 v depends on environments). Also, such a reliance would be strengthened with smaller σ e , i.e., when there is less uncertainty for the effect from environments. To mitigate the issue, fortunately, we can prove another result that implies a new objective as a sufficient condition for the ideal solution. Proposition 2. The objective min θ V e [R(e)] reaches the optimum if and only if</p><formula xml:id="formula_7">[θ 1 , θ 2 ] = [1, 0].</formula><p>The new objective tackles the variance across environments and guarantees the desirable solution. The enlightenment is that if the model yields equal performance on different e's, it would manage to leverage the invariant features, which motivates us to devise a new objective for solving Eq. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">STABLE LEARNING WITH EXPLORE-TO-EXTRAPOLATE RISK MINIMIZATION</head><p>We now return to the general case where we have {(G v , y v )} for training and leverage a GNN model as the predictor: ŷv = f θ (G v ). The intuition in Section 3.1 implies a new learning objective:</p><formula xml:id="formula_8">min θ V e [L(G e , Y e ; θ)] + βE e [L(G e , Y e ; θ)],<label>(4)</label></formula><p>where L(G e , Y e ; θ) = 1 |Ve| v∈Ve l(f θ (G e v ), y e v ) and β is a trading hyper-parameter. If we have training graphs from a sufficient number of environments E tr = {e} and the correspondence of each graph to a specific e, i.e., {G e , Y e } e∈Etr which induces {{G e v , y e v } v∈Ve : e ∈ E tr }, we can use the empirical estimation with risks from different environments to handle Eq. 4 in practice, as is done by the Risk Extrapolation (REX) approach <ref type="bibr">(Krueger et al., 2021)</ref>. Unfortunately, as mentioned before, for node-level tasks on graphs, the training data is often a single graph (without any correspondence of nodes to environments), and hence, one only has training data from a single environment. Exceptions are some multi-graph scenarios where one can assume each graph is from an environment, but there are still a very limited number of training graphs (e.g., less than five). The objective Eq. 4 would require data from diverse environments to enable the model for desirable extrapolation. To detour such a dilemma, we introduce K auxiliary context generators</p><formula xml:id="formula_9">g w k (G) (k = 1, • • • , K) that aim to generate K-fold graph data {G k } K k=1 (which induces {{G k v } v∈V : 1 ≤ k ≤ K})</formula><p>based on the input one G and mimics training data from different environments. The generators are trained to maximize the variance loss so as to explore the environments and facilitate stable learning of the GNN:</p><formula xml:id="formula_10">min θ Var({L(g w * k (G), Y ; θ) : 1 ≤ k ≤ K}) + β K K k=1 L(g w * k (G), Y ; θ), s. t. [w * 1 , • • • , w * K ] = arg max w1,••• ,w K Var({L(g w k (G), Y ; θ) : 1 ≤ k ≤ K}),<label>(5)</label></formula><p>where</p><formula xml:id="formula_11">L(g w k (G), Y ; θ) = L(G k , Y ; θ) = 1 |V | v∈V l(f θ (G k v ), y v ).</formula><p>One remaining problem is how to specify g w k (G). Following recent advances in adversarial robustness on graphs <ref type="bibr" target="#b60">(Xu et al., 2019;</ref><ref type="bibr" target="#b29">Jin et al., 2020)</ref>, we consider editing graph structures by adding/deleting edges. Assume a Boolean matrix</p><formula xml:id="formula_12">B k = {0, 1} N ×N (k = 1, • • • , K</formula><p>) and denote the supplement graph of A as A = 11 − I − A, where I is an identity matrix. Then the modified graph</p><formula xml:id="formula_13">for view k is A k = A + B k • (A − A)</formula><p>where • denotes element-wise product. The optimization for B k is difficult due to its non-differentiability and one also needs to constrain the modification within a threshold. To handle this, we use policy gradient method REINFORCE, treating graph generation as a decision process and edge editing as actions (see details in Appendix A). We call our approach in Eq. 5 Explore-to-Extrapolate Risk Minimization (EERM) and present our training algorithm in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICAL DISCUSSIONS</head><p>We next present theoretical analysis to shed insights on the objective and its relationship with our formulated OOD problem in Section 2.1. To begin with, we introduce some building blocks.</p><p>The GNN model f can be decomposed into an encoder h for representation and a classifier c for prediction, i.e., f = c • h and we have Another tricky point is that in computation of the KL divergence and mutual information, we require the samples from the joint distribution p e (G, Y), which also results in difficulty for handling data generation of interconnected nodes. Therefore, we again adopt our perspective in Section 2.1 and consider a two-step sampling procedure. Concretely, for any probability function f 1 , f 2 associated with ego-graphs G v and node labels y, we define computation for KL divergence as</p><formula xml:id="formula_14">z v = h(G v ), ŷv = c(z v ). Besides,</formula><formula xml:id="formula_15">DKL(f1(Gv, y) f2(Gv, y)) := E G∼p(G) 1 |V | v∈V E yv ∼p(y|Gv =Gv ) log f1(Gv = Gv, y = yv) f2(Gv = Gv, y = yv) .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RELATIONSHIP BETWEEN INVARIANCE PRINCIPLE AND OOD PROBLEM</head><p>We will show that the objective Eq. 4 can guarantee a valid solution for OOD problem Eq. 2. To this end, we rely on another assumption for data-generating distribution. Assumption 2. (Environment Heterogeneity): For (G v , r) that satisfies Assumption 1, there exists a random variable r such that G v = m(r, r) according to the functional representation lemma. We assume that p(y|r, e = e) would arbitrarily change across environments e ∈ E.</p><p>Assumptions 1 and 2 essentially distill two portions of features in input data: one is domain-invariant for prediction and the other contributes to sensitive prediction that depends on environments. The GNN model f = c • h induces two model distributions q(z|G v ) (by the encoder) and q(y|z) (by the classifier). Based on this, we can dissect the effects of Eq. 4 which indeed forces the representation z to satisfy the invariance and sufficiency conditions illustrated in Assumption 1. Theorem 1. If q(y|z) is treated as a variational distribution, then 1) minimizing the expectation term in Eq. 4 contributes to max q(z|Gv) I(y; z), i.e., enforcing the sufficiency condition on z for prediction, and 2) minimizing the variance term in Eq. 4 would play a role for min q(z|Gv) I(y; e|z), i.e., enforcing the invariance condition p(y|z, e) = p(y|z).</p><p>Based on these results, we can bridge the gap between the invariance principle and OOD problem. Theorem 2. Under Assumption 1 and 2, if the GNN encoder q(z|G v ) satisfies that 1) I(y; e|z) = 0 (invariance condition) and 2) I(y; z) is maximized (sufficiency condition), then the model f * given by E y [y|z] is the solution to OOD problem in Eq. 2.</p><p>The proof for Theorem 2 follows a similar line of <ref type="bibr" target="#b36">Liu et al. (2021)</ref>. The above theorems indicate that the objective Eq. 4 can guarantee a valid solution for the formulated node-level OOD problem on graph-structured data, which serves as a theoretical justification for our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">INFORMATION-THEORETIC ERROR FOR OOD GENERALIZATION</head><p>We proceed to analyze the OOD generalization error given by our learning approach. Recall that we assume training data from p(G, Y|e = e) and testing data from p(G, Y|e = e ). In fact, the training error and OOD generalization error can be respectively measured by the discrepancy terms: D KL (p e (y|G v ) q(y|G v )) and D KL (p e (y|G v ) q(y|G v )) which can be calculated based on our definition in Eq. 6. This allows us to generalize the information-theoretic framework <ref type="bibr" target="#b20">(Federici et al., 2021)</ref> for analysis on graph data. Based on Theorem 1, we can arrive at the following theorem which reveals the effect of Eq. 4 that contributes to tightening the bound for the OOD error.</p><p>Table <ref type="table">1</ref>: Summary of the experimental datasets that entail diverse distribution shifts ("Artificial Transformation" means that we add synthetic spurious features, "Cross-Domain Transfers" means that each graph in the dataset corresponds to distinct domains, "Temporal Evolution" means that the dataset is a dynamic one with evolving nature), different train/val/test splits ("Graph-Level" means splitting by graphs and "Time-Aware" means splitting by time) and the evaluation metrics. In Appendix E we provide more detailed information and discussions on the evaluation protocols.  The condition can be satisfied once z is a sufficient representation across environments. Therefore, we have proven that the new objective could help to reduce the generalization error on out-of-distribution data and indeed enhance GNN model's power for in-the-wild extrapolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we aim to verify the effectiveness and robustness of our approach in a wide variety of tasks reflecting real situations, using different GNN backbones. Table <ref type="table">1</ref> summarizes the information of experimental datasets and evaluation protocols, and we provide more dataset information in Appendix E. We compare our approach EERM with standard empirical risk minimization (ERM). Implementation details are presented in Appendix F. In the following subsections, we will investigate three scenarios that require the model to handle distribution shifts stemming from different causes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">HANDLING DISTRIBUTION SHIFTS WITH ARTIFICIAL TRANSFORMATION</head><p>We first consider artificial distribution shifts based on two public node classification benchmarks Cora and Amazon-Photo. For each dataset, we adopt two randomly initialized GNNs to 1) generate node labels based on the original node features and 2) generate spurious features based on the node labels and environment id, respectively (See Appendix E.1 for details). We generate 10-fold graph data with distinct environment id's and use 1/1/8 of them for training/validation/testing. We use a 2-layer vanilla GCN <ref type="bibr" target="#b31">(Kipf &amp; Welling, 2017)</ref> as the GNN model. We report results on 8 test graphs (T1∼ T8) of two datasets in Fig. <ref type="figure">2</ref>(a) and 3(a), respectively, where we also adopt 2-layer GCNs for data generation. The results show that our approach consistently outperforms ERM. In Cora/Photo, we manage to achieve 9.1%/2.6% improvement on average, which suggests the effectiveness of our approach for handling distribution shifts. We also observe that in Photo, the performance variances within one graph and across different test graphs are both much lower  compared with those in Cora. We conjecture the reasons are two-fold. First, there is evidence that in Cora the (original) features from adjacent nodes are indeed informative for prediction while in Photo this information contributes to negligible gain over merely using centered node's features.</p><p>Based on this, once the node features are mixed up with invariant and spurious ones, it would be harder for distinguishing them in the former case that relies more on graph convolution.</p><p>In Fig. <ref type="figure">2</ref>(b) and Fig. <ref type="figure">3</ref>(b), we compare the averaged training accuracy (achieved by the epoch with the highest validation accuracy) given by two approaches when using all the input features and removing the spurious ones for inference (we still use all the features for training in the latter case). As we can see, the performance of ERM drops much more significantly than EERM when we remove the spurious input features, which indicates that the GCN trained with standard ERM indeed exploits spurious features to increase training accuracy while our approach can help to alleviate such an issue and guide the model to focus on invariant features. Furthermore, in Fig. <ref type="figure">2</ref>(c) and Fig. <ref type="figure">3</ref>(c), we compare the test accuracy averaged on eight graphs when using different GNNs e.g. GCN, SGC <ref type="bibr" target="#b57">(Wu et al., 2019)</ref> and GAT <ref type="bibr" target="#b53">(Velickovic et al., 2018)</ref>, for data generation (See Appendix G for more results).</p><p>The results verify that our approach achieves consistently superior performance in different cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">GENERALIZING TO UNSEEN DOMAINS</head><p>We proceed to consider another scenario where there are multiple observed graphs in one dataset and a model trained with one graph or a limited number of graphs is expected to generalize to new unseen graphs. The graphs of a dataset share the input feature space and output space and may have different sizes and data distributions since they are collected from different domains. We adopt two public social network datasets Twitch-Explicit and Facebook-100 collected by <ref type="bibr" target="#b35">Lim et al. (2021)</ref>.</p><p>Training with a Single Graph. In Twitch, we adopt a single graph DE for training, ENGB for validation and the remaining five networks (ES, FR, PTBR, RU, TW) for testing. We follow <ref type="bibr" target="#b35">Lim et al. (2021)</ref> and use test ROC-AUC for evaluation. We specify the GNN model as GCN, GAT and a recently proposed model GCNII <ref type="bibr" target="#b12">(Chen et al., 2020a</ref>) that can address the over-smoothing of GCN and enable stacking of deep layers. The layer numbers are set as 2 for GCN and GAT and 10 for GCNII. Fig. <ref type="figure" target="#fig_3">4</ref> compares the results on five test graphs, where EERM significantly outperforms ERM in most cases with up to 4.8% improvement on ROC-AUC. The results verify the effectiveness of EERM for generalizing to new graphs from unseen domains.</p><p>Training with Multiple Graphs. In FB-100, we adopt three graphs for training, two graphs for validation and the remaining three for testing. We also follow <ref type="bibr" target="#b35">Lim et al. (2021)</ref> and use test accuracy for evaluation. We use GCN as the backbone and compare using different configurations of training graphs, as shown in Table <ref type="table" target="#tab_2">2</ref>. We can see that EERM outperforms ERM on average on all the test graphs with up to 12.2% improvement. Furthermore, EERM maintains the superiority with different training graphs, which also verifies the robustness of our approach w.r.t. training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">EXTRAPOLATING OVER DYNAMIC DATA</head><p>We consider the third scenario where the input data are temporal dynamic graphs and the model is trained with dataset collected at one time and needs to handle newly arrived data in the future. Here are also two sub-cases that correspond to distinct real-world scenarios, as discussed below.  Handling Dynamic Graph Snapshots. We adopt a dynamic financial network dataset Elliptic <ref type="bibr" target="#b41">(Pareja et al., 2020)</ref> that contains dozens of graph snapshots where each node is a Bitcoin transaction and the goal is to detect illicit transactions. We use 5/5/33 snapshots for training/validation/testing. Following <ref type="bibr" target="#b41">Pareja et al. (2020)</ref> we use F1 score for evaluation. We consider two GNN architectures as the backbone: GraphSAGE <ref type="bibr" target="#b24">(Hamilton et al., 2017)</ref> and a recently proposed model GPRGNN <ref type="bibr">(Chien et al., 2021</ref>) that can adaptively combine information from node features and graph topology. The results are shown in Fig. <ref type="figure" target="#fig_4">5</ref> where we group the test graph snapshots into 9 folds in chronological order. Our approach yields much better F1 scores than ERM in most cases with 2.5% ∼ 28.1% improvements, which again verifies its superiority. Furthermore, there is an interesting phenomenon that both methods suffer a performance drop after T7. The reason is that this is the time when the dark market shutdown occurred <ref type="bibr" target="#b41">(Pareja et al., 2020)</ref>. Such an emerging event causes considerable variation to data distributions that leads to performance degrade for both methods, with ERM suffering more. In fact, the emerging event acts as an external factor which is unpredictable given the limited training data. The results also suggest that how neural models generalize to OOD data depends on the learning approach but its performance limit is dominated by observed data. Nonetheless, our approach contributes to better F1 scores than ERM even in such an extreme case.  <ref type="table" target="#tab_3">3</ref> presents the test accuracy and shows that EERM outperforms ERM in five cases out of six. Notably, when using GPRGNN as the backbone, EERM manages to achieve up to 7.8% relative improvement, which shows that EERM is capable of improving GNN model's learning for extrapolating to future data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Handling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">COMPARISON WITH EXISTING WORKS</head><p>We compare with some related works, highlight our differences and discuss the potential impacts on broad areas. Due to the space limit, we defer more discussions to Appendix B.</p><p>Generalization on Graph Data. Recent endeavors <ref type="bibr" target="#b49">(Scarselli et al., 2018;</ref><ref type="bibr" target="#b21">Garg et al., 2020;</ref><ref type="bibr" target="#b54">Verma &amp; Zhang, 2019)</ref> derive generalization error bounds for GNNs on node-level tasks. Yet, they focus on in-distribution generalization and put little emphasis on distribution shifts, which are the main focus of our work. Furthermore, some up-to-date works explore GNN's extrapolation ability for OOD data, e.g. unseen features/structures <ref type="bibr">(Xu et al., 2021)</ref> and varying graph sizes <ref type="bibr">(Yehudai et al., 2021;</ref><ref type="bibr" target="#b8">Bevilacqua et al., 2021)</ref>. However, they concentrate on graph-level tasks (e.g., graph classification), where each input instance is a graph (usually with less than 100 nodes) and one dataset contains massive graphs for training and testing. By contrast, in node-level tasks, i.e., what this paper studies, each input is a node in one graph (usually with ∼1K to ∼1M nodes) and a dataset usually contains only a single graph. The graph-level problems have straightforward relationship to the general setting (in Eq. 1) since one can treat input graphs as x and graph labels as y and then the data from one environment becomes a set of i.i.d. generated pairs (x, y). Differently, node-level problems cannot be tackled in this way due to the inter-connection among data points that results in non-IID nature in data generation within one environment. To resolve this case, our work introduces a new perspective for problem formulation, backed up with a concrete approach for problem solving. Also, EERM can be adapted to the general setting especially for generalization from a single observed environment.  <ref type="bibr">et al., 2021)</ref> summarizes the ways that prior works resort to for introducing distribution shifts to datasets, including 1) artificial transformations, 2) synthetic-to-real transfers, 3) constrained splits and 4) cross-dataset transfers. As far as we know, the majority of them focus on classic data format (vision, texts, tabular data, etc.), and there are few studies designed for graph-structured data, e.g. the OGB-MolPCBA <ref type="bibr" target="#b28">(Hu et al., 2020b)</ref> for graph-level classification/regression. As a by-product, our experiment designs (including datasets, splits and evaluation protocols) for three distinct scenarios (artificial transformations, cross-graph transfers and (time)-constrained splits) could help to enrich the OOD benchmarking zoo, particularly for node-level tasks on graphs, which remains unexplored in the literature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmarking OOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A OPTIMIZATION AND ALGORITHM</head><p>We illustrate the details of using policy gradient for optimizing the graph editers in Eq. 5. Concretely, for view k, we consider a parameterized matrix P k = {π k nm }. For the n-th node, the probability for editing the edge between it and the m-th node would be p(a k nm ) = exp(π k nm ) m exp(π k nm ) . We then sample s actions {b k nmt } s t=1 from a multinomial distribution M(p(a k n1 ), • • • , p(a k nn )), which give the nonzero entries in the n-th row of B k . The reward function R(G k ) can be defined as the inverse loss. We can use REINFORCE algorithm to optimize the generator with the gradient</p><formula xml:id="formula_16">∇ w k log p w k (A k )R(G k ) where w k = P k and p w k (A k ) = Π n Π s t=1 p(b k nmt ).</formula><p>We present the training algorithm in Alg. 1.</p><p>Algorithm 1: Stable Learning for OOD Generalization in Node-Level Prediction on Graphs.</p><p>1 INPUT: training graph data G = (A, X) and Y , initialized parameters of GNN θ, initialized parameters of graph editers w = {w k }, learning rates α g , α f . 2 while not converged or maximum epochs not reached do </p><formula xml:id="formula_17">3 for t = 1, • • • , T do 4 Obtain modified graphs G k = (A k , X) from graph editer g w k , k = 1, • • • , K; 5 Compute loss J 1 (w) = Var({L(G k , Y ; θ) : 1 ≤ k ≤ K}) ; 6 Update w k ← w k + α g ∇ w k log p w k (A k )J 1 (w), k = 1, • • • , K ; 7 if t == T then 8 Compute loss J 2 (θ) = Var({L(G k , Y ; θ) : 1 ≤ k ≤ K}) + β K K k=1 L(G k , Y ; θ) ; 9 Update θ ← θ − α f ∇ θ J 2 (θ) ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B FURTHER RELATED WORKS B.1 OUT-OF-DISTRIBUTION GENERALIZATION AND INVARIANT MODELS</head><p>Out-of-distribution generalization has drawn extensive attention in the machine learning community. To endow the learning systems with the ability for handling unseen data from new environments, it is natural to learn invariant features under the setting of the causal factorization of physical mechanisms <ref type="bibr" target="#b50">(Schölkopf et al., 2012;</ref><ref type="bibr" target="#b42">Peters et al., 2016)</ref>. A recent work <ref type="bibr" target="#b3">(Arjovsky et al., 2019)</ref> proposes Invariant Risk Minimization (IRM) as a practical solution for OOD problem via invariance principle. Based on this, follow-up works make solid progress in this direction, e.g., with group distributional robust optimization <ref type="bibr" target="#b47">(Sagawa et al., 2019)</ref>, causal attribution <ref type="bibr" target="#b11">(Chang et al., 2020)</ref>, game theory <ref type="bibr" target="#b1">(Ahuja et al., 2020)</ref>, lottery ticket hypothesis <ref type="bibr">(Zhang et al., 2021)</ref>, etc. Several works attempt to resolve extended settings. For instance, <ref type="bibr">Ahmed et al. (2021)</ref> proposes to match the output distribution spaces from different domains via some divergence, while a recent work <ref type="bibr" target="#b37">(Mahajan et al., 2021</ref>) also leverages a matching-based algorithm that resorts to shared representations of cross-domain inputs from the same object. Also, <ref type="bibr" target="#b16">Creager et al. (2021)</ref> and <ref type="bibr" target="#b36">Liu et al. (2021)</ref> point out that in most real situations, one has no access to the correspondence of each data point in the dataset with a specific environment, based on which they propose to estimate the environments as a latent variable. <ref type="bibr">Krueger et al. (2021)</ref> devises Risk Extrapolation (REX) which aims at minimizing the weighted combination of the variance and the mean of risks from multiple environments. <ref type="bibr" target="#b59">Xie et al. (2020)</ref> contributes to a similar objective from different theoretical perspective. Also, <ref type="bibr" target="#b33">Koyama &amp; Yamaguchi (2021)</ref> extends the spirit of MAML algorithm and arrives at a similar objective form. In our model, we also consider minimization of the combination of variance and mean terms (in Eq. 4) and on top of that we further propose to optimize through a bilevel framework in Eq. 5. Compared to existing works, the differences of EERM are two-folds. First, we do not assume input data from multiple environments and the correspondence between each data point and a specific environment. Instead, our formulation enables learning and extrapolation from a single observed environment. Second, on methodology side, we introduce multiple context generators that aim to generate data of virtual environments in an adversarial manner. Besides, our formulation in this paper focus on node-level prediction on graphs, where the essential difference, as mentioned before, lies in the inter-connection of data points in one graph (that corresponds to an environment), which hinders trivial adaption from existing works in the general setting.</p><p>There are also some recent studies that discuss the pitfalls of IRM in some cases by analyzing its performance on concrete examples <ref type="bibr">(Rosenfeld et al., 2021;</ref><ref type="bibr">Nagarajan et al., 2021;</ref><ref type="bibr" target="#b30">Kamath et al., 2021)</ref>. A recent work <ref type="bibr" target="#b20">(Federici et al., 2021)</ref> harnesses an information-theoretic perspective to unify existing invariant models and provide insightful reflections on current endeavors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 GRAPH NEURAL NETWORKS AND GENERALIZATION</head><p>Another line of research related to us attempts to enhance the generalization ability of graph neural networks via modifying the graph structures. One category of recent works is to learn new graph structures based on the input graph and node features. To improve the generalization power, a common practice is to enforce a certain regularization for the learned graph structures. For example, <ref type="bibr" target="#b29">Jin et al. (2020)</ref> proposes to constrain the sparsity and smoothness of graphs via matrix norms and further adopts proximal gradient methods for handling the non-differentiable issue. <ref type="bibr" target="#b14">Chen et al. (2020b)</ref>; Zhang et al. ( <ref type="formula">2019</ref>) also aim to regularize the sparsity and smoothness but differently harness energy function to enforce the constraints. From a different perspective, <ref type="bibr" target="#b60">Xu et al. (2019)</ref> attempts to attack the graph topology for improving the model's robustness and proposes to leverage projected gradient descent to make it tractable for the optimization of discrete graph structures.</p><p>Alternatively, several works focus on pruning the graph networks <ref type="bibr" target="#b13">(Chen et al., 2021)</ref> or adaptively sparsifying the structures <ref type="bibr">(Zheng et al., 2020;</ref><ref type="bibr" target="#b25">Hasanzadeh et al., 2020)</ref>. While in our model, the introduced context generators are specified as graph editers which also attempt to optimize new graph structures, our big picture motivation and specific method are quite different from the abovementioned works. On problem setting side, we focus on out-of-distribution generalization and target handling distribution shifts over graphs, which is more difficult than the setting of previous methods that concentrate on in-distribution generalization. On methodology side, our context generators aim to generate data of multiple virtual environments and are learned from maximizing the variance of risks of multiple (virtual) environments. In other words, these context generators work adversarially among each other and collaboratively with the GNN backbone to enable the model for out-of-distribution generalization from a single observed environment. This is a new learning method. Also, one can specify our context generators with other existing graph generation/editing/attacking frameworks as mentioned above, which we leave as future works.</p><p>There are a few recent studies that focus on out-of-graph problems. For example, <ref type="bibr" target="#b27">Hu et al. (2020a)</ref> considers transferable active learning over multiple graphs and treats the labeling process for nodes in an input graph as a decision process for optimization. Furthermore, <ref type="bibr" target="#b4">Baek et al. (2020)</ref> deals with link prediction in knowledge graphs and proposes a meta-learning approach for extrapolating to unseen nodes out of the input graph. While these studies concentrate on transferring a model to new data, their main bodies including the formulations, approaches and experiment designs are not aimed at OOD generalization where there exist distribution shifts between training and testing data.</p><p>There is a very recent work <ref type="bibr" target="#b5">(Baranwal et al., 2021)</ref> that endeavors to understand the out-of-distribution generalization ability of GNNs for semi-supervised node classification. It introduces contextual stochastic block model that is a mix-up of standard stochastic block model and a Gaussian mixture model with each node class corresponding to a component, based on which the authors show some cases where linear separability can be achieved. By contrast, our work possesses the following distinct technical contributions. First, we formulate OOD problem for node-level tasks in a more general setting without any assumption on specific distribution forms or the way for generation of graph structures<ref type="foot" target="#foot_2">3</ref> Second, our proposed formulation, model and algorithm are rooted on invariant models, providing a new perspective and methodology for node-level prediction on graphs. Third, compared with <ref type="bibr" target="#b5">Baranwal et al. (2021)</ref> that focus on synthetic datasets and simulated distribution shifts with artificially adding inter-class edges, we design and conduct comprehensive experiments on diverse real-world datasets that can reflect in-the-wild nature in real situations (e.g., cross-graph transfers and dynamic evolution) and demonstrate the power of our approach in three scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C PROOFS FOR SECTION 3.1 C.1 PROOF FOR PROPOSITION 1</head><p>We define the aggregated node feature a v = 1 |Nv| u∈Nv x u . According to the definition and setup in Section 3.1, we derive the risk under a specific environment R(e):</p><formula xml:id="formula_18">1 |V | v∈V E y|Gv=Gv ŷv − y v 2 2 = 1 |V | v∈V E n 1 ,n 2 θ 1 a 1 v + θ 2 a 2 v − y v 2 2 = 1 |V | v∈V E n 1 ,n 2 (θ 1 + θ 2 − 1)a 1 v + θ 2 (n 1 v + n 2 v + ) − n 1 v 2 2 . (<label>7</label></formula><formula xml:id="formula_19">)</formula><p>Denote the objective for empirical risk minimization as L 1 = E e [R(e)] and we have its first-order derivative w.r.t. θ 1 as</p><formula xml:id="formula_20">∂L 1 ∂θ 1 = E e 1 |V | v∈V E n 1 ,n 2 2[(θ 1 + θ 2 − 1)a 1 v + θ 2 (n 1 v + n 2 v + ) − n 1 v ] • a 1 v = E e 1 |V | v∈V E n 1 ,n 2 2[(θ 1 + θ 2 − 1) • a 1 v • a 1 v ,<label>(8)</label></formula><p>where the second step is given by independence among a 1 v , n 1 v , n 2 v and . Let ∂L1 ∂θ1 = 0, and we will obtain</p><formula xml:id="formula_21">θ 1 + θ 2 = 1.</formula><p>Also, the first-order derivative w.r.t. θ 2 is</p><formula xml:id="formula_22">∂L 1 ∂θ 2 = E e 1 |V | v∈V E n 1 ,n 2 2[(θ 1 + θ 2 − 1)a 1 v + θ 2 (n 1 v + n 2 v + ) − n 1 v ] • (a 1 v + n 1 v + n 2 v + ) = E e 1 |V | v∈V E n 1 ,n 2 2[(θ 1 + θ 2 − 1) • a 1 v • a 1 v + θ 2 (n 1 v • n 1 v + n 2 v • n 2 v + • ) − n 1 v • n 1 v = E e 1 |V | v∈V E n 1 ,n 2 2[(θ 1 + θ 2 − 1) • a 1 v • a 1 v + θ 2 (1 + 1 + σ 2 e ) − 1 ,<label>(9)</label></formula><p>where the last step is according to</p><formula xml:id="formula_23">E x [x 2 ] = E 2 x [x] + V x [x].</formula><p>We further let ∂L1 ∂θ2 = 0, and will get the unique solution</p><formula xml:id="formula_24">θ 1 = 1 + σ 2 e 2 + σ 2 e , θ 2 = 1 2 + σ 2 e . (<label>10</label></formula><formula xml:id="formula_25">) C.2 PROOF FOR PROPOSITION 2 Let L 2 = V e [R(e)] = E e [R 2 (e)] − E 2 e [R(e)] and l(e) = (θ 1 + θ 2 − 1)a 1 v + θ 2 (n 1 v + n 2 v + ) − n 1 v .</formula><p>We derive the first-order derivation of L 2 w.r.t. θ 1 and θ 2 . Firstly,</p><formula xml:id="formula_26">∂L 2 ∂θ 1 = E e 1 |V | v∈V E n 1 ,n 2 4l 3 (e)a 1 v + E 2 e 1 |V | v∈V E n 1 ,n 2 2l(e)a 1 v , (<label>11</label></formula><formula xml:id="formula_27">)</formula><formula xml:id="formula_28">∂L 2 ∂θ 2 = E e 1 |V | v∈V E n 1 ,n 2 4l 3 (e) • (a 1 v + n 1 v + n 2 v + ) + E 2 e 1 |V | v∈V E n 1 ,n 2 2l(e) • (a 1 v + n 1 v + n 2 v + ) . (<label>12</label></formula><formula xml:id="formula_29">)</formula><p>By letting ∂L2 ∂θ1 = 0, we obtain the equation</p><formula xml:id="formula_30">θ 1 + θ 2 = 1. Plugging it into ∂L2 ∂θ2 we have ∂L 2 ∂θ 2 = E e 1 |V | v∈V E n 1 ,n 2 4(θ 2 (n 1 v + n 2 v + ) − n 1 v ) 3 • (a 1 v + n 1 v + n 2 v + ) + E 2 e 1 |V | v∈V E n 1 ,n 2 2(θ 2 (n 1 v + n 2 v + ) − n 1 v ) • (a 1 v + n 1 v + n 2 v + ) ,<label>(13)</label></formula><p>which is a function of e unless [θ 1 , θ 2 ] = [1, 0] that gives rise to ∂L2 ∂θ2 = 0 for arbitrary distributions of environments. We thus conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D PROOFS FOR SECTION 4 D.1 PROOF FOR THEOREM 1</head><p>We first present a useful lemma that interprets the invariance and sufficiency conditions with the terminology of information theory. Lemma 1. The two conditions in Assumption 1 can be equivalently expressed as 1) (Invariance): I(y; e|r) = 0 and 2) (Sufficiency): I(y; r) is maximized.</p><p>Proof. For the invariance, we can easily arrive at the equivalence given the fact I(y; e|r) = D KL (p(y|e, r) p(y|r)). ( <ref type="formula">14</ref>)</p><p>For the sufficiency, we first prove that for (G v , r, y) satisfying that y = c * (r) + n would also satisfy that r = arg max r I(y; r). We prove it by contradiction. Suppose that r = arg max r I(y; r) and r = arg max r I(y; r) where r = r. Then there exists a random variable r such that r = m(r, r) where m is a mapping function. We thus have I(y; r ) = I(y; r, r) = I(c * (r); r, r) = I(c * (r); r) = I(y; r) which leads to contradiction.</p><p>We next prove that for (G v , r, y) satisfying that r = arg max r I(y; r) would also satisfy that y = c * (r) + n. Suppose that y = c * (r) + n and y = c * (r ) + n where r = r. We then have the relationship I(c * (r ); r) ≤ I(c * (r ); r ) which yields that r = arg max r I(y; r) and leads to contradiction.</p><p>Given the dependency relationship z ← G v → y, we have the fact that max q(z|Gv) I(y, z) is equivalent to min q(z|Gv) I(y, G v |z). Also, we have (treating q(y|z) as a variational distribution)</p><formula xml:id="formula_31">I(y, G v |z) = D KL (p(y|G v , e) p(y|z, e)) = D KL (p(y|G v , e) q(y|z)) − D KL (p(y|z, e) q(y|z)) ≤ D KL (p(y|G v , e) q(y|z)).<label>(15)</label></formula><p>Based on this, we have the inequality</p><formula xml:id="formula_32">I(y, G v |z) ≤ min q(y|z) D KL (p(y|G v , e) q(y|z)).<label>(16)</label></formula><p>Also, we have (according to our definition in Eq. 6) D KL (p(y|G v , e) q(y|z))</p><formula xml:id="formula_33">=E e E G∼pe(G) 1 V v∈V E yv∼pe(y|Gv=Gv) E zv∼q(z|Gv=Gv) log p e (y = y v |G v = G v ) q(y = y v |z = z v ) ≤E e E G∼pe(G) 1 V v∈V E yv∼pe(y|Gv=Gv) log p e (y = y v |G v = G v ) E zv∼q(z|Gv=Gv) [q(y = y v |z = z v )] ,<label>(17)</label></formula><p>where the second step is according to Jensen Inequality and the equality holds if q(z|G v ) is a delta distribution (induced by the GNN encoder h).</p><p>Then the problem min q(z|Gv),q(y|z) D KL (p(y|G v , e) q(y|z)) can be equivalently converted into</p><formula xml:id="formula_34">min f E e 1 |V e | v∈Ve l(f (G e v ), y e v ) = min f E e [L(G e , Y e ; f )].<label>(18)</label></formula><p>We thus have proven that minimizing the expectation term in Eq. 4 is to minimize the upper bound of I(y, G v |z) and contributes to max q(z|Gv) I(y, z).</p><p>Second, we have</p><formula xml:id="formula_35">I(y; e|z) =D KL (p(y|z, e) p(y|z)) =D KL (p(y|z, e) E e [p(y|z, e)]) =D KL (q(y|z) E e [q(y|z)]) − D KL (q(y|z) p(y|z, e)) − D KL (E e [p(y|z, e)] E e [q(y|z)]) ≤D KL (q(y|z) E e [q(y|z)]).<label>(19)</label></formula><p>Besides, we have (according to the definition in Eq. 6)</p><formula xml:id="formula_36">D KL (q(y|z) E e [q(y|z)]) =E e E G∼pe(G) 1 |V | v∈V E yv∼pe(y|Gv=Gv) E zv∼q(z|Gv=Gv) log q(y = y v |z = z v ) E e [q(y = y v |z = z v )] .<label>(20)</label></formula><p>Using Jensen Inequality, we will obtain that D KL (q(y|z) E e [q(y|z)]) is upper bounded by</p><formula xml:id="formula_37">E e [|L(G e , Y e ; f ) − E e [L(G e , Y e ; f )]|] = V e [L(G e , Y e ; f )].<label>(21)</label></formula><p>Hence we have proven that minimizing the variance term in Eq. 4 plays a role for solving min q(z|Gv) I(y; e|z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 PROOFS FOR THEOREM 2</head><p>With Lemma 1, we know that 1) the representation z (given by GNN encoder z = h(G v )) satisfies the invariant condition, i.e., p(y|z) = p(y|z, e) if and only if I(y; e|z) = 0 and 2) the representation z satisfies the sufficiency condition, i.e., y = c * (z) + n if and only if z = arg max z I(y; z).</p><p>We denote the GNN encoder that satisfies the invariance and sufficiency conditions as h * and the corresponding predictor model</p><formula xml:id="formula_38">f * (G v ) = E y [y|h * (G v )] with f * = c * • h * .</formula><p>Since we assume the GNN encoder q(z|G v ) satisfies the conditions in Assumption 1, then according to Assumption 2, we know that there exists random variable z such that G v = m(z, z) and p(y|z, e) would change arbitrarily across environments. Based on this, for any environment e that gives the distribution p e (y, z, z), we can construct environment e with the distribution p e (y, z, z) that satisfies p e (y, z, z) = p e (y, z)p e (z).</p><p>Then we reproduce the proof of Theorem 2.1 in <ref type="bibr" target="#b36">Liu et al. (2021)</ref> to prove the result by showing that for arbitrary function f = c • h and environment e, there exists an environment e such that</p><formula xml:id="formula_40">E G∼p e (G) 1 |V | v∈V E yv∼p e (y|Gv=Gv) [l(f (G v ), y v )] ≥ E G∼pe(G) 1 |V | v∈V E y∼pe(y|Gv=Gv) [l(f * (G v ), y v )] .<label>(23)</label></formula><p>Concretely we have</p><formula xml:id="formula_41">E G∼p e (G) 1 |V | v∈V E (yv,zv,zv)∼p e (y,z,z|Gv=Gv) [l(c(z v , z v ), y v )] =E G ∼p e (G) 1 |V | v ∈V E z v ∼p e (z|Gv=G v ) E G∼pe(G) 1 |V | v∈V E (yv,zv)∼pe(y,z|Gv=Gv) [l(c(z v , z v ), y v )] ≥E G ∼p e (G) 1 |V | v ∈V E z v ∼p e (z|Gv=G v ) E G∼pe(G) 1 |V | v∈V E (yv,zv)∼pe(y,z|Gv=Gv) [l(c * (z v , z v ), y v )] =E G ∼p e (G) 1 |V | v ∈V E z v ∼p e (z|Gv=G v ) E G∼pe(G) 1 |V | v∈V E (yv,zv)∼pe(y,z|Gv=Gv) [l(c * (z v ), y v )] =E G∼pe(G) 1 |V | v∈V E (yv,zv)∼pe(y,z|Gv=Gv) [l(c * (z v ), y v )] =E G∼pe(G) 1 |V | v∈V E (yv,zv,zv)∼pe(y,z,z|Gv=Gv) [l(c * (z v ), y v )] ,<label>(24)</label></formula><p>where the first equality is given by Eq. 22 and the second/third steps are due to the sufficiency condition of h * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 PROOF FOR THEOREM 3</head><p>Recall that according to our definition in Eq. 6, the KL divergence D KL (p e (y|G v ) q(y|G v )) would be</p><formula xml:id="formula_42">D KL (p e (y|G v ) q(y|G v )) := E G∼pe(G) 1 |V | v∈V E yv∼pe(y|Gv=Gv) log p e (y = y v |G v = G v ) q(y = y v |G v = G v ) .</formula><p>(25) This newly defined KL divergence allows us to apply the information-theoretic framework <ref type="bibr" target="#b20">(Federici et al., 2021)</ref> for our analysis on graph data. First, we can decompose the training error (resp. OOD error) into a representation error and a latent predictive error. Lemma 2. For any GNN encoder q(z|G v ) and classifier q(y|z), we have</p><formula xml:id="formula_43">D KL (p e (y|G v ) q(y|G v )) ≤ I e (G v ; y|z) + D KL (p e (y|z) q(y|z)),<label>(26)</label></formula><formula xml:id="formula_44">D KL (p e (y|G v ) q(y|G v )) ≤ I e (G v ; y|z) + D KL (p e (y|z) q(y|z)).<label>(27)</label></formula><p>Proof. Firstly, we have</p><formula xml:id="formula_45">D KL (p e (y|G v ) q(y|G v )) =E G∼pe(G) 1 |V | v∈V E yv∼pe(y|Gv=Gv) log p e (y = y v |G v = G v ) q(y = y v |G v = G v ) =E G∼pe(G) 1 |V | v∈V E yv∼pe(y|Gv=Gv) log p e (y = y v |G v = G v ) E zv∼q(z|Gv=Gv) q(y = y v |z = z v ) ≤E G∼pe(G) 1 |V | v∈V E yv∼pe(y|Gv=Gv) E zv∼q(z|Gv=Gv) log p e (y = y v |G v = G v ) q(y = y v |z = z v ) =D KL (p e (y|G v ) q(y|z)),<label>(28)</label></formula><p>where the third step is again due to Jensen Inequality and the equality holds once q(z|G v ) is a delta distribution.</p><p>Besides, we have</p><formula xml:id="formula_46">D KL (p e (y|G v ) q(y|z)) =E G∼pe(G) 1 |V | v∈V E yv∼pe(y|Gv=Gv) E zv∼q(z|Gv=Gv) log p e (y = y v |G v = G v ) q(y = y v |z = z v ) =E G∼pe(G) 1 |V | v∈V E yv∼pe(y|Gv=Gv) E zv∼q(z|Gv=Gv) log p e (y = y v |G v = G v )p e (y = y v |z = z v ) p e (y = y v |z = z v )q(y = y v |z = z v ) =I(G v ; y|z) + D KL (p e (y|z) q(y|z)).<label>(29)</label></formula><p>The result for D KL (p e (y|G v ) q(y|G v )) can be obtained in a similar way.</p><p>Lemma 3. For any q(z|G v ) and q(y|z), the following inequality holds for any z satisfying p(z = z|e = e) &gt; 0, ∀e ∈ E.</p><formula xml:id="formula_47">D JSD (p e (y|z) q(y|z)) ≤ 1 2α I(y; e|z) + 1 2 D KL (p e (y|z) q(y|z) 2 . (<label>30</label></formula><formula xml:id="formula_48">)</formula><p>Proof. The proof can be adapted by from the Proposition 3 in <ref type="bibr" target="#b20">Federici et al. (2021)</ref> by replacing e in our case with t.</p><p>The results of Lemma 2 and 3 indicate that if we aim to reduce the OOD error measured by D KL (p e (y|G v ) q(y|G v ), one need to control three terms: 1) I e (G v ; y|z), 2) D KL (p e (y|z) q(y|z) and 3) I(y; e|z). The next lemma unifies minimization for the first two terms. Lemma 4. For any q(z|G v ) and q(y|z), we have min q(z|Gv),q(y|z) D KL (p e (y|G v ) q(y|z)) ⇔ min q(z|Gv)</p><p>I e (G v ; y|z) + min q(y|z) D KL (p e (y|z) q(y|z)).</p><p>(31)</p><p>Proof. Recall that q(y|z) is a variational distribution. We have</p><formula xml:id="formula_49">I e (G v ; y|z) = D KL (p e (y|G v ) p e (y|z))) = D KL (p e (y|G v ) q(y|z)) − D KL (p e (y|z) q(y|z)) ≤ D KL (p e (y|G v ) q(y|z)).<label>(32)</label></formula><p>Therefore, we can see that I e (G v ; y|z) is upper bounded by D KL (p e (y|G v q(y|z)) and the equality holds if and only if D KL (p e (y|z) q(y|z)) = 0. We thus conclude the proof.</p><p>Recall that according to Lemma 1 we have the fact that our objective in Eq. 4 essentially has the similar effect as min q(z|Gv),q(y|z) D KL (p e (y|G v ) q(y|z)) + I(y; e|z).</p><p>Based on the Lemma 2, 3 and 4, we know that optimization for the objective Eq. 4 can reduce the upper bound of OOD error given by D KL (p e (y|G v ) q(y|G v ) on condition that I e (G v ; y|z) = I e (G v ; y|z). We conclude our proof for Theorem 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E DATASETS AND EVALUATION PROTOCOLS</head><p>In this section, we introduce the detailed information for experimental datasets and also provide the details for our evaluation protocols including data preprocessing, dataset splits and the ways for calculating evaluation metrics. In the following subsections, we present the information for the three scenarios, respectively.  We consider three GNN backbones trained with ERM. The "OOD" means that we train the model on one graph DE and report the metric on another graph ENGB. The "IID" means that we train the model on 90% nodes of DE and report the metric on the remaining nodes. The results clearly show that the model performance suffers a significantly drop from the case "IID" to the case "OOD". This indicates that the graph-level splitting for training/validation/testing splits used in Section 5.2 indeed introduces distribution shifts and would require the model to deal with out-of-distribution data during test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 ARTIFICIAL DISTRIBUTION SHIFTS ON CO R A AND AM A Z O N-PH O T O</head><p>Cora and Amazon-Photo are two commonly used node classification benchmarks and widely adopted for evaluating the performance of GNN designs. These datasets are of medium size with thousands of nodes. See Table <ref type="table">1</ref> for more statistic information. Cora is a citation network where nodes represent papers and edges represent their citation relationship. Amazon-Photo is a copurchasing network where nodes represent goods and edges indicate that two goods are frequently bought together. In the original dataset, the available node features have strong correlation with node labels. To evaluate model's ability for out-of-distribution generalization, we need to introduce distribution shifts into the training and testing data.</p><p>For each dataset, we use the provided node features to construct node labels and spurious environmentsensitive features. Specifically, assume the provided node features as X 1 . Then we adopt a randomly initialized GNN (with input of X 1 and adjacency matrix) to generate node labels Y (via taking an argmax in the output layer to obtain one-hot vectors), and another randomly initialized GNN (with input of the concatenation of Y and an environment id) to generate spurious node features X 2 . After that, we concatenate two portions of features X = [X 1 , X 2 ] as input node features for training and evaluation. In this way, we construct ten graphs with different environment id's for each dataset. We use one graph for training, one for validation and report the classification accuracy on the remaining graphs. One may realize that this data generation is a generalized version of our motivating example in Section 3.1 and we replace the linear aggregation as a randomly initialized graph neural network to introduce non-linearity.</p><p>In fact, with our data generation, the original node features X 1 can be seen as domain-invariant features that are sufficiently predictive for node labels and insensitive to different environments, while the generated features X 2 are domain-variant features that are conditioned on environments. Therefore, in principle, the ideal case for the model is to identify and leverage the invariant features for prediction. In practice, there exist multiple factors that may affect model's learning, including the local optimum and noise in data. Therefore, one may not expect the model to exactly achieve the ideal case since there also exists useful predictive information in X 2 that may help the model to increase the training accuracy. Yet, through our experiments in Fig. <ref type="figure">2</ref>(b) and 3(b), we show that the reliance of EERM on spurious features is much less than ERM, which we believe could serve as concrete evidence that our approach is capable for guiding the GNN model to alleviate reliance on domain-variant features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 CROSS-DOMAIN TRANSFERS ON MULTI-GRAPH DATA</head><p>A typical scenario for distribution shifts on graphs is the problem of cross-domain transfers. There are quite a few real-world situations where one has access to multiple observed graphs each of which is from a specific domain. For example, in social networks, the domains can be instantiated as where or when the networks are collected. In protein networks, there may exist observed graph data (protein-protein interactions) from distinct species which can be seen as distinct domains. In short, since most of graph data records the relational structures among a specific group of entities and the interactions/relationships among entities from different groups often have distinct characteristics, the data-generating distributions would vary across groups, which bring up domain shifts.</p><p>Yet, to enable transfer learning across graphs, the graphs in one dataset need to share the same input feature space and output space. We adopt two public datasets Twitch-Explicit and Facebook-100 that satisfy this requirement.</p><p>Twitch-Explicit contains seven networks where nodes represent Twitch users and edges represent their mutual friendships. Each network is collected from a particular region, including DE, ENGB, ES, FR, PTBR, RU and TW. These seven networks have similar sizes and different densities and maximum node degrees, as shown in Table <ref type="table" target="#tab_5">4</ref>. Also, in Fig. <ref type="figure" target="#fig_8">6</ref>, we compare the ROC-AUC results on different leave-out data. We consider GCN, GAT and GCNII as the GNN backbones and train the model with standard empirical risk minimization (ERM). We further consider two ways for data splits. In the first case, which we call "OOD", we train the model on the nodes of one graph DE and report the highest ROC-AUC on the nodes of another graph ENGB. In the second case, which we call "IID", we train the model on 90% nodes of DE and evaluate the performance on the leave-out 10% data. The results in Fig. <ref type="figure" target="#fig_8">6</ref> show that the model performance exhibits a clear drop from "IID" to "OOD", which indicates that there indeed exist distribution shifts among different input graphs. This also serves as a justification for our evaluation protocol in Section 5.2 where we adopt the graph-level splitting to construct training/validation/testing sets.</p><p>Another dataset is Facebook-100 which consists of 100 Facebook friendship network snapshots from the year 2005, and each network contains nodes as Facebook users from a specific American university. We adopt fourteen networks in our experiments: John Hopkins, Caltech, Amherst,  Recall that in Section 5.2 we use Penn, Brown and Texas for testing, Cornell and Yale for validation, and use three different combinations from the remaining graphs for training. These graphs have significantly diverse sizes, densities and degree distributions. In Fig. <ref type="figure">7</ref> we present a comparison which indicates that the distributions of graph structures among these graphs are different. Concretely, the testing graphs Penn and Texas are much larger (with 41554 and 31560 nodes, respectively) than training/validation graphs (most with thousands of nodes). Also, the training graphs Caltech and Amherst are much denser than other graphs in the dataset, while some graphs like Penn have nodes with very large degrees. These statistics suggest that our evaluation protocol requires the model to handle different graph structures from training/validation to testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 TEMPORAL EVOLUTION ON DYNAMIC GRAPH DATA</head><p>Another common scenario is for temporal graphs that dynamically evolve as time goes by. The types of evolution can be generally divided into two categories. In the first case, there are multiple graph snapshots and each snapshot is taken at one time. As time goes by, there exists a sequence of graph snapshots which may contain different node sets and data distributions. Typical examples include financial networks that record the payment flows among transactions within different time intervals. In the second case, there is one graph that evolves with node/edge adding or deleting. Typical examples include some large-scale real-world graphs like social networks and citation networks where the distribution for node features, edges and labels would have strong correlation with time (in different scales). We adopt two public real-world datasets Elliptic and OGB-Arxiv for node classification experiments.</p><p>Elliptic contains a sequence of 49 graph snapshots. Each graph snapshot is a network of Bitcoin transactions where each node represents one transaction and each edge indicates a payment flow. Approximately 20% of the transactions are marked with licit or illicit ones and the goal is to identify illicit transaction in the future observed network. Since in the original dataset, the first six snapshots have extremely imbalanced classes (where the illicit transactions are less than 10 among thousands of nodes), we remove them and use the 7th-11th/12th-17th/17th-49th snapshots for training/validation/testing. Also, due to the fact that each graph snapshot has very low positive label rate, we group the 33 testing graph snapshots into 9 test sets according to the chronological order. In Fig. <ref type="figure" target="#fig_9">8</ref> we present the label rate and positive label rate for training/validation/testing sets. As we can see, the positive label rates are quite different in different data sets. Indeed, the model needs to handle distinct label distributions from training to testing data.</p><p>OGB-Arxiv is composed of 169,343 Arxiv CS papers from 40 subject areas and their citation relationship. The goal is to predict a paper's subject area. In <ref type="bibr" target="#b28">(Hu et al., 2020b)</ref>, the papers published before 2017, on 2018 and since 2019 are used for training/validation/testing. Also, the authors adopt the transductive learning setting, i.e., the nodes in validation and test sets also exist in the graph for training. In our case, we instead adopt inductive learning setting where the nodes in validation and test sets are unseen during training, which is more akin to the real-world situation. Besides, for better evaluation on generalization, especially extrapolating to new data, we consider dataset splits with a larger year gap: we use papers published before 2011 for training, from 2011 to 2014 for validation, and after 2014 for test. Such a dataset splitting way would introduce distribution shift between  training and testing data, since several latent influential factors (e.g., the popularity of research topics) for data generation would change over time. In Fig. <ref type="figure" target="#fig_11">9</ref>, we visualize the T-SNE embeddings of the nodes and mark the training/validation/testing nodes with different colors. From Fig. <ref type="figure" target="#fig_11">9</ref>(a) to Fig. <ref type="figure" target="#fig_11">9</ref>(c), we can see that testing nodes non-overlapped with the training/validation ones exhibit an increase, which suggests that the distribution shifts enlarge as time difference goes large. This phenomenon echoes the results we achieve in Table <ref type="table" target="#tab_3">3</ref> where we observe that as the time difference between testing and training data goes larger, model performance suffers a clear drop, with ERM suffering more than EERM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F IMPLEMENTATION DETAILS</head><p>In this section, we present the details for our implementation in Section 5 including the model architectures, hyper-parameter settings and training details in order for reproducibility. Most of our experiments are run on GeForce RTX 2080Ti with 11GB except some experiments requiring large GPU memory for which we adopt RTX 8000 with 48GB. The configurations of our environments and packages are listed below: F.2.2 SETTINGS FOR SECTION 5.2 For GCN, we set the layer number L as 2. For GAT, we set L = 2 and H = 4. For GCNII, we set the layer number as 10. We use hidden size 32 and weight decay with coefficient set as 1e-3.</p><p>For Twitch-Explicit, other hyper-parameters are set as follows:</p><p>• GCN: α g = 0.001, α f = 0.01, β = 3.0, s = 5, T = 1.</p><p>• GAT: α g = 0.005, α f = 0.01, β = 1.0, s = 5, T = 1.</p><p>• GCNII: α g = 0.01, α f = 0.001, β = 1.0, s = 5, T = 1.</p><p>For Facebook-100, other hyper-parameters are set as: α g = 0.005, α f = 0.01, β = 1.0, s = 5, T = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.3 SETTINGS FOR SECTION 5.3</head><p>For GraphSAGE and GPRGNN, we set the layer number as 5 and hidden size as 32.</p><p>For Elliptic, other hyper-parameters are set as follows:</p><p>• GraphSAGE: α g = 0.0001, α f = 0.0002, β = 1.0, s = 5, T = 1.</p><p>• GPRGNN: α g = 0.005, α f = 0.01, β = 1.0, s = 5, T = 1.</p><p>For OGB-Arxiv, other hyper-parameters are set as follows:</p><p>• GraphSAGE: α g = 0.01, α f = 0.005, β = 0.5, s = 1, T = 5.</p><p>• GPRGNN: α g = 0.001, α f = 0.01, β = 1.0, s = 1, T = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 TRAINING DETAILS</head><p>For each method, we train the model with a fixed number of epochs and report the test result achieved at the epoch when the model provides the best performance on validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G MORE EXPERIMENT RESULTS</head><p>We provide additional experiment results in this section. In Fig. <ref type="figure" target="#fig_12">10</ref> and 11 we present the distribution of test accuracy on Cora when using SGC and GAT, respectively, as the GNNs for data generation. In Fig. <ref type="figure" target="#fig_14">12</ref> and 13 we further compare with the training accuracy using all the features and removing the spurious ones for inference. These results are consistent with those presented in Section 5.1, which again verifies the effectiveness of our approach. Besides, the corresponding extra results on Photo are shown in Fig. <ref type="figure" target="#fig_17">14</ref>, 15, 16 and 17, which also back up our discussions in Section 5.1.          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: (a) The proposed approach Explore-to-Extrapolate Risk Minimization which entails K context generators that generate graph data of different (virtual) environments based on input data from a single (real) environment. The GNN model is updated via gradient descent to minimize a weighted combination of mean and variance of risks from different environments, while the context generators are updated via REINFORCE to maximize the variance loss. (b) Illustration for our Assumption 1 where the neighbored nodes in each layer contributes to a portion of causal features for prediction. (c) The dependence among variables in the motivating example in Section 3.1 and a concrete example that instantiates these variables in the context of a citation network scenario.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>we assume I(x; y) stands for the mutual information between x and y and I(x; y|z) denotes the conditional mutual information given z. To keep notations simple, we define p e (•) = p(•|e = e) and I e (•) = I(•|e = e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Results on Cora with artificial distribution shifts. We run each experiment with 20 trials. (a) The (distribution of) test accuracy of vanilla GCN using our approach for training and using ERM. (b) The (averaged) accuracy on the training set (achieved by the epoch where the highest validation accuracy is achieved) when using all the input node features and removing the spurious ones for inference. (c) The (averaged) test accuracy with different GNNs for data generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Test ROC-AUC on Twitch where we compare different GNN backbones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Test F1 score on Elliptic where we group graph snapshots into 9 test sets (T1∼ T9).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>New Nodes in Temporally Augmented Graph. Citation networks often go through temporal augmentation with new papers published. We adopt OGB-Arxiv<ref type="bibr" target="#b28">(Hu et al., 2020b)</ref> for experiments and enlarge the time difference between training and testing data to introduce distribution shifts: we select papers published before 2011 for training, in-between 2011 and 2014 for validation, and within 2014-2016/2016-2018/2018-2020 for testing. Also different from the original (transductive) setting in<ref type="bibr" target="#b28">Hu et al. (2020b)</ref>, we use the inductive learning setting, i.e., test nodes are strictly unseen during training, which is more akin to practical situations. Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Problems. A surge of recent works release and study OOD benchmarks, e.g., Worrall et al. (2017); Hendrycks et al. (2019); Gulrajani &amp; Lopez-Paz (2020); Xiao et al. (2020); Santurkar et al. (2020); Ye et al. (2021). A very recent literature (Koh</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>10 OUTPUT: trained parameters of GNN θ * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Comparison of different leave-out data on Twithc-Explicit. We consider three GNN backbones trained with ERM. The "OOD" means that we train the model on one graph DE and report the metric on another graph ENGB. The "IID" means that we train the model on 90% nodes of DE and report the metric on the remaining nodes. The results clearly show that the model performance suffers a significantly drop from the case "IID" to the case "OOD". This indicates that the graph-level splitting for training/validation/testing splits used in Section 5.2 indeed introduces distribution shifts and would require the model to deal with out-of-distribution data during test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The label rates and positive label rates of training/validation/testing data splits of Elliptic. The positive class (illicit transaction) and negative class (licit transaction) are very imbalanced. Also, in different splits, the distributions for labels exhibit clear differences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) Test nodes within 2014 -2016 (colored yellow) (b) Test nodes within 2016 -2018 (colored yellow) (c) Test nodes within 2018 -2020 (colored yellow)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: T-SNE visualization of training/validation/testing nodes in OGB-Arxiv. We mark training nodes (within 1950-2011) and validation nodes (within 2011-2014) as red and blue, respectively.In (a)-(c), the test nodes within different time intervals are visualized as yellow points. We can see that as the time difference of testing data and training/validation data goes large from (a) to (c), the testing nodes non-overlapped with training/validation ones become more, which suggests that the distribution shifts become more significant and require the model to extrapolate to more difficult future data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Distribution of test accuracy results on Cora with artificial distribution shifts generated by SGC as the GNN generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Distribution of test accuracy results on Cora with artificial distribution shifts generated by GAT as the GNN generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Comparison of training accuracy using all the features v.s. removing the spurious features for inference on Cora with artificial distribution shifts generated by SGC as the GNN generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Comparison of training accuracy using all the features v.s. removing the spurious features for inference on Cora with artificial distribution shifts generated by GAT as the GNN generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Distribution of test accuracy results on Photo with artificial distribution shifts generated by SGC as the GNN generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Distribution of test accuracy results on Photo with artificial distribution shifts generated by GAT as the GNN generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Comparison of training accuracy using all the features v.s. removing the spurious features for inference on Photo with artificial distribution shifts generated by SGC as the GNN generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Comparison of training accuracy using all the features v.s. removing the spurious features for inference on Photo with artificial distribution shifts generated by GAT as the GNN generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy on FB-100 where we compare different configurations of training graphs.</figDesc><table><row><cell>Training graph combination</cell><cell cols="2">Penn</cell><cell cols="2">Brown</cell><cell cols="2">Texas</cell></row><row><cell></cell><cell>ERM</cell><cell>EERM</cell><cell>ERM</cell><cell>EERM</cell><cell>ERM</cell><cell>EERM</cell></row><row><cell cols="2">John Hopkins + Caltech + Amherst 49.92 ± 0.90</cell><cell cols="2">50.69 ± 0.30 55.23 ± 2.01</cell><cell cols="2">56.59 ± 0.19 50.85 ± 3.07</cell><cell>54.86 ± 1.39</cell></row><row><cell>Bingham + Duke + Princeton</cell><cell>50.18 ± 0.97</cell><cell cols="2">51.07 ± 1.01 50.04 ± 2.05</cell><cell cols="2">52.16 ± 2.97 50.10 ± 2.96</cell><cell>56.22 ± 0.14</cell></row><row><cell>WashU + Brandeis+ Carnegie</cell><cell>50.55 ± 0.52</cell><cell cols="2">51.97 ± 0.82 54.17 ± 2.97</cell><cell cols="2">55.08 ± 2.61 56.10 ± 0.30</cell><cell>56.21 ± 0.40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy on OGB-Arxiv with papers in different time intervals for evaluation.</figDesc><table><row><cell>Method</cell><cell>2014-2016</cell><cell>2016-2018</cell><cell>2018-2020</cell></row><row><cell>ERM-SAGE</cell><cell>41.31 ± 1.91</cell><cell>39.33 ± 2.86</cell><cell>36.42 ± 2.52</cell></row><row><cell cols="2">EERM-SAGE 41.28 ± 0.91  *</cell><cell>39.94 ± 1.58</cell><cell>38.46 ± 2.28</cell></row><row><cell>ERM-GPR</cell><cell>47.26 ± 0.43</cell><cell>45.10 ± 0.82</cell><cell>41.66 ± 1.00</cell></row><row><cell>EERM-GPR</cell><cell>49.82 ± 0.47</cell><cell>48.57 ± 0.51</cell><cell>44.91 ± 0.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Gilad Yehudai, Ethan Fetaya, Eli A. Meirom, Gal Chechik, and Haggai Maron. From local structures to size generalization in graph neural networks. In International Conference on Machine Learning (ICML), pp. 11975-11986, 2021.</figDesc><table><row><cell>Dinghuai Zhang, Kartik Ahuja, Yilun Xu, Yisen Wang, and Aaron C. Courville. Can subnetwork</cell></row><row><cell>structure be the key to out-of-distribution generalization? In International Conference on Machine</cell></row><row><cell>Learning (ICML), pp. 12356-12367, 2021.</cell></row><row><cell>Yingxue Zhang, Soumyasundar Pal, Mark Coates, and Deniz Üstebay. Bayesian graph convolutional</cell></row><row><cell>neural networks for semi-supervised classification. In AAAI Conference on Artificial Intelligence</cell></row><row><cell>(AAAI), pp. 5829-5836, 2019.</cell></row><row><cell>Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen,</cell></row><row><cell>and Wei Wang. Robust graph representation learning via neural sparsification. In International</cell></row><row><cell>Conference on Machine Learning (ICML), pp. 11458-11468, 2020.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Statistic information for Twitch-Explicit datasets.</figDesc><table><row><cell cols="6">Dataset #Nodes #Edges #Density Avg Degree Max Degree</cell></row><row><cell>DE</cell><cell>9498</cell><cell>153138</cell><cell>0.0033</cell><cell>16</cell><cell>3475</cell></row><row><cell>ENGB</cell><cell>7126</cell><cell>35324</cell><cell>0.0013</cell><cell>4</cell><cell>465</cell></row><row><cell>ES</cell><cell>4648</cell><cell>59382</cell><cell>0.0054</cell><cell>12</cell><cell>809</cell></row><row><cell>FR</cell><cell>6549</cell><cell>112666</cell><cell>0.0052</cell><cell>17</cell><cell>1517</cell></row><row><cell>PTBR</cell><cell>1912</cell><cell>31299</cell><cell>0.0171</cell><cell>16</cell><cell>455</cell></row><row><cell>RU</cell><cell>4385</cell><cell>37304</cell><cell>0.0038</cell><cell>8</cell><cell>575</cell></row><row><cell>TW</cell><cell>2772</cell><cell>63462</cell><cell>0.0165</cell><cell>22</cell><cell>1171</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Comparison of node numbers, densities and maximum node degrees of fourteen graphs used in our experiments on Facebook-100. The index 0-13 stand for John Hopkins, Caltech, Amherst, Bingham, Duke, Princeton, WashU, Brandeis, Carnegie, Cornell, Yale, Penn, Brown and Texas, respectively. As we can see, these graphs have very distinct statistics, which indicates that there exist distribution shifts w.r.t. graph structures.</figDesc><table><row><cell>40000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6 (a) Node Number 7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell></row><row><cell>0.100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.075</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.050</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.025</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.000</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6 (b) Density 7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell></row><row><cell>4000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6 (c) Max Degree 7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell></row><row><cell>Figure 7:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Our formulation and method can be trivially extended to cover edge features which we omit here for brevity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We use a subscript v here to remind that it is an ego-graph from the view of a target node.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Our Assumption 1 and 2 mainly focus on the existence of causal features and spurious features in data generation across different environments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/chennnM/GCNII</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/jianhao2016/GPRGNN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This work targets out-of-distribution generalization for graph-structured data with the focus on node-level problems where the inter-connection of data points hinders trivial extension from existing formulation and methods. To this end, we take a fresh perspective to formulate the problem in a principled way and further develop a new approach for extrapolation from a single environment, backed up with theoretical guarantees. We also design comprehensive experiments to show the practical power of our approach on various real-world datasets with diverse distribution shifts.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>GCN. We use the GCNConv available in Pytorch Geometric for implementation. The detailed architecture description is as below:</p><p>• A sequence of L-layer GCNConv.</p><p>• Add self-loop and use batch normalization for graph convolution in each layer.</p><p>• Use ReLU as the activation.</p><p>GAT. We use the GATConv available in Pytorch Geometric for implementation. The detailed architecture description is as below:</p><p>• A sequence of L-layer GATConv with head number H.</p><p>• Add self-loop and use batch normalization for graph convolution in each layer.</p><p>• Use ELU as the activation.</p><p>GraphSAGE. We use the SAGEConv available in Pytorch Geometric for implementation. The detailed architecture description is as below:</p><p>• A sequence of L-layer SAGEConv.</p><p>• Add self-loop and use batch normalization for graph convolution in each layer.</p><p>• Use ReLU as the activation.</p><p>GCNII. We use the implementation 4 provided by the original paper <ref type="bibr" target="#b12">(Chen et al., 2020a)</ref>. The associated hyper-parameters in GCNII model are set as: α GCN II = 0.1 and λ GCN II = 1.0.</p><p>GPRGNN. We use the implementation 5 provided by <ref type="bibr">Chien et al. (2021)</ref>. We adopt the PPR initialization and GPRprop as the propagation unit. The associated hyper-parameters in GPRGNN model are set as: α GP RGN N = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 HYPER PARAMETER SETTINGS</head><p>The hyper-parameters for model architectures are set as default values in different cases. Other hyperparameters are searched with grid search on validation dataset. The searching space are as follows: learning rate for GNN backbone α f ∈ {0.0001, 0.0002, 0.001, 0.005, 0.01}, learning rate for graph editers α g ∈ {0.0001, 0.001, 0.005, 0.01}, weight for combination β ∈ {0.2, 0.5, 1.0, 2.0, 3.0}, number of edge editing for each node s ∈ {1, 5, 10}, number of iterations for inner update before one-step outer update T ∈ {1, 5}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2.1 SETTINGS FOR SECTION 5.1</head><p>We consider 2-layer GCN with hidden size 32. We use weight decay with coefficient set as 1e-3. Besides, we set α g = 0.005, α f = 0.01, β = 2.0, s = 5, T = 1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Systematic generalisation with group invariant predictions</title>
		<author>
			<persName><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harm</forename><surname>Van Seijen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Invariant risk minimization games</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><surname>Dhurandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="145" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for segmentation of brain tumors: Impact of cross-institutional training and testing</title>
		<author>
			<persName><forename type="first">Ashirbani</forename><surname>Ehab A Albadawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><forename type="middle">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical physics</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Invariant risk minimization</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno>CoRR, abs/1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to extrapolate knowledge: Transductive few-shot out-of-graph link prediction</title>
		<author>
			<persName><forename type="first">Jinheon</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Bok Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph convolution for semisupervised classification: Improved linear separability and out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Aseem</forename><surname>Baranwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimon</forename><surname>Fountoulakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aukosh</forename><surname>Jagannath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="684" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="472" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fairness in criminal justice risk assessments: The state of the art</title>
		<author>
			<persName><forename type="first">R</forename><surname>Berk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jabbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sociological Methods &amp; Research</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Size-invariant graph representations for graph classification extrapolations</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangze</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="837" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalizing from several related classification tasks to a new unlabeled sample</title>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyemin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2178" to="2186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Invariance, causality and robustness</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
		<idno>CoRR, abs/1812.08233</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Invariant rationalization</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1448" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A unified lottery ticket hypothesis for graph neural networks</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1695" to="1706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Iterative deep graph learning for graph neural networks: Better and robust node embeddings</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Environment inference for invariant learning</title>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2189" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dark model adaptation: Semantic image segmentation from daytime to nighttime</title>
		<author>
			<persName><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Transportation Systems (ITSC)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3819" to="3824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">COVID-19 detection through transfer learning using multimodal imaging data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Janizek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Collective spammer detection in evolving multi-relational social networks</title>
		<author>
			<persName><forename type="first">Shobeir</forename><surname>Fakhraei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">R</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Madhusudana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Shashanka</surname></persName>
		</author>
		<author>
			<persName><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1769" to="1778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An information-theoretic approach to distribution shifts</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Federici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Forré</surname></persName>
		</author>
		<idno>CoRR, abs/2106.03783</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Vikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3419" to="3430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain adaptation with conditional transferable components</title>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2839" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01434</idno>
		<title level="m">search of lost domain generalization</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian graph neural networks with adaptive connection sampling</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahin</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4094" to="4104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Scaling out-of-distribution detection for real-world settings</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11132</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph policy network for transferable active learning on graphs</title>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc-Alexandre</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Does invariant risk minimization capture invariance</title>
		<author>
			<persName><forename type="first">Pritish</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akilesh</forename><surname>Tangella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danica</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4069" to="4077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">WILDS: A benchmark of in-the-wild distribution shifts</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Lanas</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irena</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Stavness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berton</forename><surname>Earnshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><forename type="middle">M</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5637" to="5664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">When is invariance useful in an out-of-distribution generalization problem ? CoRR, abs</title>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoichiro</forename><surname>Yamaguchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1883">2008.01883, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">New benchmarks for learning on nonhomophilous graphs</title>
		<author>
			<persName><forename type="first">Derek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hohne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW) workshop</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Heterogeneous risk minimization</title>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6804" to="6814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Domain generalization using causal matching</title>
		<author>
			<persName><forename type="first">Divyat</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Tople</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7313" to="7324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards recognizing unseen categories in unseen domains</title>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="466" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory (COLT)</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding the failure modes of out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations (ICLR)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Evolvegcn: Evolving graph convolutional networks for dynamic graphs</title>
		<author>
			<persName><forename type="first">Aldo</forename><surname>Pareja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Kanezashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Schardl</surname></persName>
		</author>
		<author>
			<persName><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5363" to="5370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Causal inference by using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of the Royal Statistical Society. Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="page" from="947" to="1012" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5389" to="5400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Invariant models for causal transfer learning</title>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The risks of invariant risk minimization</title>
		<author>
			<persName><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Kumar Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization</title>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>CoRR, abs/1911.08731</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Breeds: Benchmarks for subpopulation shift</title>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.04859</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The vapnik-chervonenkis dimension of graph and recursive neural networks</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="248" to="259" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Günnemann. Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Sgouritsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<idno>CoRR, abs/1811.05868</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2012">2012. 2018</date>
		</imprint>
	</monogr>
	<note>On causal and anticausal learning</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><forename type="middle">Vasconcellos</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kouichi</forename><surname>Sakurai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="828" to="841" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Social structure of facebook networks</title>
		<author>
			<persName><forename type="first">Amanda</forename><forename type="middle">L</forename><surname>Traud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mason</forename><forename type="middle">A</forename><surname>Porter</surname></persName>
		</author>
		<idno>CoRR, abs/1102.2166</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Stability and generalization of graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1539" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nauchno-Technicheskaya Informatsia</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniyar</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7168" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Noise or signal: The role of image backgrounds in object recognition</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09994</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Risk variance penalization: From distributional robustness to causality</title>
		<author>
			<persName><forename type="first">Chuanlong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/2006.07544</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Topology attack and defense for graph neural networks: An optimization perspective</title>
		<author>
			<persName><forename type="first">Kaidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsui-Wei</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3961" to="3967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">How neural networks extrapolate: From feedforward to graph neural networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Ood-bench: Benchmarking and understanding out-of-distribution generalization datasets and algorithms</title>
		<author>
			<persName><forename type="first">Nanyang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanqing</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyue</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/2106.03721</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
