<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Fast Dense Spectral-Spatial Convolution Network Framework for Hyperspectral Images Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-07-05">5 July 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenju</forename><surname>Wang</surname></persName>
							<email>wangwenju666@163.com</email>
							<idno type="ORCID">0000-0002-8549-4710</idno>
							<affiliation key="aff0">
								<orgName type="department">College of Communication and Art Design</orgName>
								<orgName type="institution">University of Shanghai for Science and Technology</orgName>
								<address>
									<postCode>021</postCode>
									<settlement>Shanghai</settlement>
									<region>SH</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuguang</forename><surname>Dou</surname></persName>
							<email>doushuguang52@163.com</email>
							<idno type="ORCID">0000-0003-3231-8817</idno>
							<affiliation key="aff0">
								<orgName type="department">College of Communication and Art Design</orgName>
								<orgName type="institution">University of Shanghai for Science and Technology</orgName>
								<address>
									<postCode>021</postCode>
									<settlement>Shanghai</settlement>
									<region>SH</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhongmin</forename><surname>Jiang</surname></persName>
							<idno type="ORCID">0000-0003-3231-8817</idno>
							<affiliation key="aff0">
								<orgName type="department">College of Communication and Art Design</orgName>
								<orgName type="institution">University of Shanghai for Science and Technology</orgName>
								<address>
									<postCode>021</postCode>
									<settlement>Shanghai</settlement>
									<region>SH</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liujie</forename><surname>Sun</surname></persName>
							<email>liujiesunx@163.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Communication and Art Design</orgName>
								<orgName type="institution">University of Shanghai for Science and Technology</orgName>
								<address>
									<postCode>021</postCode>
									<settlement>Shanghai</settlement>
									<region>SH</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Fast Dense Spectral-Spatial Convolution Network Framework for Hyperspectral Images Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-07-05">5 July 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">20182373D17D5C160245D72FBB441664</idno>
					<idno type="DOI">10.3390/rs10071068</idno>
					<note type="submission">Received: 1 June 2018; Accepted: 3 July 2018;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>hyperspectral image classification</term>
					<term>densely connected convolutional neural network</term>
					<term>deep learning</term>
					<term>parametric rectified linear unit</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent research shows that deep-learning-derived methods based on a deep convolutional neural network have high accuracy when applied to hyperspectral image (HSI) classification, but long training times. To reduce the training time and improve accuracy, in this paper we propose an end-to-end fast dense spectral-spatial convolution (FDSSC) framework for HSI classification. The FDSSC framework uses different convolutional kernel sizes to extract spectral and spatial features separately, and the "valid" convolution method to reduce the high dimensions. Densely-connected structures-the input of each convolution consisting of the output of all previous convolution layers-was used for deep learning of features, leading to extremely accurate classification. To increase speed and prevent overfitting, the FDSSC framework uses a dynamic learning rate, parametric rectified linear units, batch normalization, and dropout layers. These attributes enable the FDSSC framework to achieve accuracy within as few as 80 epochs. The experimental results show that with the Indian Pines, Kennedy Space Center, and University of Pavia datasets, the proposed FDSSC framework achieved state-of-the-art performance compared with existing deep-learning-based methods while significantly reducing the training time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Hyperspectral images (HSIs), which include hundreds of bands, contain a great deal of information. Among the many typical applications of HSIs are civil and biological threat detection <ref type="bibr" target="#b0">[1]</ref>, atmospheric environmental research <ref type="bibr" target="#b1">[2]</ref>, and ocean research <ref type="bibr" target="#b2">[3]</ref>, among others. The most commonly used technology in these applications is the classification of pixels in the HSI, referred to as HSI classification. However, HSI classification presents numerous difficulties, particularly in processing high-dimensional data and images with high spatial resolution.</p><p>Machine learning and other feature-extraction methods have been applied to HSI classification to cope with these difficulties. The relative performances of support vector learning machines (SVM), a radial basis function (RBF) neural network, and k-neighbor classifiers demonstrate that the SVM method could effectively replace the traditional method, which combines feature reduction algorithms with classification <ref type="bibr" target="#b3">[4]</ref>. Li <ref type="bibr" target="#b4">[5]</ref>, however, proposed a framework that uses local binary patterns (LBPs) to extract image features and a high-efficiency extreme learning machine (ELM) as a classifier to show that the ELM classifier is more efficient than SVM methods. However, when compared with the LBP feature extraction method, the complex spectral and spatial information of HSIs requires more sophisticated feature selection methods. Deng et al. <ref type="bibr" target="#b5">[6]</ref> proposed a HSI classification framework based on HSI micro-texture. The framework extends local response patterns to texture enhancement to represent HSIs and uses discriminant locality-preserving projections to reduce the dimensionality of the HSI data. However, the framework does not make use of the spectral information within the HSIs and requires performance improvement.</p><p>The above-mentioned traditional machine learning methods for HSI classification all have the same disadvantage-the classification accuracy needs improvement. Since these traditional methods are based on hand-crafted features, hyperspectral data need an algorithm that can learn the representative and discriminative features <ref type="bibr" target="#b6">[7]</ref>. Recently, deep learning, an alternative to the traditional machine learning algorithms discussed above, has been introduced into HSI classification, and is able to extract deep spatial and spectral features from hyperspectral data. Much of the pioneering work on deep learning applied to hyperspectral data classification has shown that identification of deep features leads to higher classification accuracies for hyperspectral data classification <ref type="bibr" target="#b7">[8]</ref>.</p><p>In 2014, Chen et al. <ref type="bibr" target="#b7">[8]</ref> first proposed a deep learning framework to merge spatial and spectral features. The deep learning framework combined principal component analysis (PCA) with deep learning architecture, and to obtain classification results it used stacked autoencoders to obtain high-level features and logistic regression; this framework was abbreviated to SAE-LR. Although SAE-LR has a disadvantage in terms of its training time, it showed that deep learning methods had a large potential for HSI classification. The following year, Makantasis et al. <ref type="bibr" target="#b8">[9]</ref> exploited a deep supervised method for HSI classification through a convolutional neural network (CNN). The approach used randomized PCA (R-PCA) to reduce the dimensions of raw input data, a CNN to construct high-level features, and a multi-layer perceptron (MLP) for classification. In 2016, Zhao and Du <ref type="bibr" target="#b9">[10]</ref> proposed a spectral-spatial feature-based classification (SSFC) framework. Their SSFC framework used a balanced local discriminant embedding (BLDE) algorithm to extract spectral features, a CNN to find high-level spatial features, and a multiple-feature-based classifier for training. Chen et al. <ref type="bibr" target="#b10">[11]</ref>, also in 2016, proposed a deep feature extraction (FE) method based on a CNN and built a deep FE model based on a three-dimensional (3D) CNN to extract the spectral-spatial characteristics of HSIs. This paper established a direction for the application of a CNN and its extended network in the field of HSI classification.</p><p>In the last two years, Li et al. <ref type="bibr" target="#b11">[12]</ref> have proposed a 3D CNN framework for accurate HSI classification and used original 3D high-level data directly as an input without actively extracting the features of the HSI. This framework does not rely on any pre-or post-processing, but effectively extracts spectral-spatial features and does not distinguish between these two kinds of features. Distinguished from the above-mentioned deep-learning-based methods, Zhong et al. <ref type="bibr" target="#b12">[13]</ref> proposed a spectral-spatial residual network (SSRN) that uses spectral and spatial residual blocks to learn the deep distinguishing features from the rich spectral features and spatial backgrounds of HSIs.</p><p>Among deep-learning-based methods, the SSRN achieves the best performance compared to other methods for three main reasons. First, the SSRN learns spectral and spatial features separately, meaning that more discriminative features can be extracted. Second, SSRN depend on CNN to extract high-level features. Third, SSRN has a deeper CNN structure than other deep learning methods. Early work showed that the deeper a CNN is, the higher the accuracy. The disadvantage of the SSRN, however, is an overly long training time.</p><p>Recently, other methods have been devised for which it is claimed that use of additional features can improve classification accuracy. Zhou et al. <ref type="bibr" target="#b13">[14]</ref> incorporated the group knowledge of the hyperspectral features for deep-learning-based-method spatial-spectral classification. Ma et al. <ref type="bibr" target="#b14">[15]</ref> took a local decision based on weighted neighborhood information. Maltezos et al. <ref type="bibr" target="#b15">[16]</ref> introduced a set of features for improving overall classification accuracy. However, only for HSI classification might these methods lead to sub-optimal results, because the SSRN achieved its optimal HSI classification accuracy by learning deep spectral and spatial representations separately.</p><p>Inspired by the SSRN and to alleviate its problems, we aimed at building a deeper convolution network that can learn deeper spectral and spatial features separately, but much faster. In 2017, Gao et al. proposed a new deep network structure, DenseNet <ref type="bibr" target="#b16">[17]</ref>, based on Google Inception <ref type="bibr" target="#b17">[18]</ref> and Residual Net <ref type="bibr" target="#b18">[19]</ref> (ResNet). As the depth of the network increases, DenseNet can reduce the problem of gradients becoming zero, and the structure can more effectively utilize features and enhance feature transfer between convolution layers. Despite its advantages, DenseNet has a long training time. To reduce the training time and prevent overfitting, we use the parametric rectified linear unit (PReLU), a dynamic learning rate, and other technical improvements (see Section 2.3).</p><p>We propose an end-to-end fast and dense spectral-spatial convolution (FDSSC) network framework for HSI classification. The FDSSC framework has the following three characteristics distinguishing it from the above-mentioned deep learning based methods:</p><p>(1) It is an end-to-end spectral-spatial convolution network without feature engineering as compared with SAE-LR, Makantasis's method, and the SSFC framework. Without relying on PCA, R-PCA, or BLDE to reduce the dimension, our framework completely utilizes the CNN to reduce the high dimensionality and to automatically learn spatial and spectral features separately at a very high level, which is more effective and robust. Moreover, the FDSSC framework uses a smaller training sample size than SAE-LR and Makantasis's method, while achieving higher accuracy. (2) It has a deeper structure than the SSFC framework, and the methods in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>  The rest of this paper is structured as follows: In Section 2, we present our proposed FDSSC framework. In Section 3, we introduce the HSI dataset and set up our proposed method. In Section 4, we present the HSI classification results of the proposed framework and discuss the performance and training time compared with other classification methods. Section 5 provides a summary, as well as suggestions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Framework</head><p>In this section, we explain the FDSSC framework in detail, elaborate on how to extract spectral and spatial features separately from HSI, how to go deeper with a densely-connected structure, and how it manages fast training and prevents overfitting. At the end, we summarize all of the steps in a graphical flowchart to explain the FDSSC network and describe the framework to clarify the advantages of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Extracting Spectral and Spatial Features Separately from HSI</head><p>A 1D CNN extracts spectral features, whereas a 2D CNN extracts local spatial features of pixels. However, HSIs contain both abundant spatial and spectral information. For HSI classification, this means to the use of a 3D CNN, which can extract both types of information. As shown in Figure <ref type="figure" target="#fig_1">1a</ref>, the 2D convolution sends a channel of the input image to another feature map after a convolution kernel operation. For an input image with three channels of spectral information (Band1, Band2, and Band3), a 3D convolution processes the data from three channels using two convolution kernels to obtain two characteristic maps, as shown in Figure <ref type="figure" target="#fig_1">1b</ref>. Within the neural network, the value V xyz i,j at position (x, y, z) on the jth feature cube in the ith layer can be formulated as follows [12]:</p><formula xml:id="formula_0">V xyz i,j = g b i,j + ∑ m P i -1 ∑ p=0 Q i -1 ∑ q=0 R i -1 ∑ r=0 W p,q,r i,j,m V (x+p)(y+q)(z+r) i-1,m<label>(1)</label></formula><p>where the feature map attached to the current feature map in the (i -1)th layer is denoted m, the length and width of the convolution kernel in space are denoted by P i and Q i , respectively, the size of the When a 3D CNN is applied to HSI classification, the target pixel is at the center of an 𝑟 × 𝑟 × 𝐿size block taken from the original pixels of the HSI as the input of the network, where 𝑟 × 𝑟 is the size of the image block in the spatial domain and 𝐿 is the spectral dimension of the HSI. After the convolution and pooling step, the results are converted into 1D feature vectors. Finally, the feature vectors are input into a classifier to obtain the classification results.</p><p>The above operations describe the general process of a 3D CNN-based deep learning method. The key to these operations is the size of the convolution kernel, because features determine accuracy. Taking <ref type="bibr" target="#b11">[12]</ref> as an example, a convolution kernel of 3 × 3 × 7 or similar size is used to learn the spectral and spatial features at the same time. Distinguished from obtaining the spectral and spatial features together, the proposed framework uses the CNN to learn the spectral and spatial features separately to extract more discriminative features. Next, we explain how to use different-sized kernels to achieve this.</p><p>A kernel of size 1 × 1 × 𝑑 (𝑑 &gt; 1) learns the spectral features from a HSI. Local spatial features exist in the HSI space, and the 2D convolution process aims to extract the local spatial features. However, the convolution with a kernel size of 1 × 1 × 𝑑 does not extract any spatial features because it does not consider the relationship between pixels and their neighbors in the spatial field. Nevertheless, the convolution of a kernel size of 1 × 1 can make linear combinations or integrate spatial information for each pixel in a spatial field. Therefore, for 3D hyperspectral data a kernel of size of 1 × 1 × 𝑑 extracts spectral features and perfectly retains the spatial features.</p><p>The spectral information is encoded within bands of hyperspectral data, which is the reason for the high dimensionality of hyperspectral data. However, after spectral features are learned by a kernel of size 1 × 1 × 𝑑, the high dimensions of the hyperspectral data can be reduced by a 3D CNN and a reshaping operation. The key to reducing high dimensionality lies in the method of padding the 3D convolution layer. "Same" and "valid" are two frequently used ways of padding. "Same" denotes convolution results at the reserved boundary, which usually cause the output shape to be When a 3D CNN is applied to HSI classification, the target pixel is at the center of an r × r × L-size block taken from the original pixels of the HSI as the input of the network, where r × r is the size of the image block in the spatial domain and L is the spectral dimension of the HSI. After the convolution and pooling step, the results are converted into 1D feature vectors. Finally, the feature vectors are input into a classifier to obtain the classification results.</p><p>The above operations describe the general process of a 3D CNN-based deep learning method. The key to these operations is the size of the convolution kernel, because features determine accuracy. Taking <ref type="bibr" target="#b11">[12]</ref> as an example, a convolution kernel of 3 × 3 × 7 or similar size is used to learn the spectral and spatial features at the same time. Distinguished from obtaining the spectral and spatial features together, the proposed framework uses the CNN to learn the spectral and spatial features separately to extract more discriminative features. Next, we explain how to use different-sized kernels to achieve this.</p><p>A kernel of size 1 × 1 × d (d &gt; 1) learns the spectral features from a HSI. Local spatial features exist in the HSI space, and the 2D convolution process aims to extract the local spatial features. However, the convolution with a kernel size of 1 × 1 × d does not extract any spatial features because it does not consider the relationship between pixels and their neighbors in the spatial field. Nevertheless, the convolution of a kernel size of 1 × 1 can make linear combinations or integrate spatial information for each pixel in a spatial field. Therefore, for 3D hyperspectral data a kernel of size of 1 × 1 × d extracts spectral features and perfectly retains the spatial features.</p><p>The spectral information is encoded within bands of hyperspectral data, which is the reason for the high dimensionality of hyperspectral data. However, after spectral features are learned by a kernel of size 1 × 1 × d, the high dimensions of the hyperspectral data can be reduced by a 3D CNN and a reshaping operation. The key to reducing high dimensionality lies in the method of padding the 3D convolution layer. "Same" and "valid" are two frequently used ways of padding. "Same" denotes convolution results at the reserved boundary, which usually cause the output shape to be the same as the input shape. "Valid" represents only effective convolution; that is, the boundary data are not processed. The valid convolution is used to reduce dimensions and retain extracted spectral features and raw spatial information.</p><p>A kernel of size of a × a × 1 (a &gt; 1) learns the spatial features from a HSI after the spectral features have been learned. By reducing the high dimension, a kernel of size a × a × 1 can learn the spatial features from the reserved spatial information of the previous step.</p><p>In short, our framework uses a 3D convolution layer of a kernel of size 1 × 1 × d (d &gt; 1) to learn the spectral features. Next, the high dimension of the feature maps is reduced, and then a 3D convolution layer of a kernel of size a × a × 1 (a &gt; 1) learns the spatial features. Finally, the classification result is obtained by average pooling, flattening, and a fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Going Deeper with Densely-Connected Structures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Densely-Connected Structure</head><p>Assume that the CNN has l convolution layers, X l is the output of the lth layer and H l ( * ) represents the complex nonlinear transformation operations in the lth convolution layer. The connected structure of the traditional CNN is such that the output of the (l -1)th layer is the input of the lth layer:</p><formula xml:id="formula_1">X l = H l (X l-1 ), l ∈ N +<label>(2)</label></formula><p>As shown in Figure <ref type="figure" target="#fig_2">2</ref>, DenseNet <ref type="bibr" target="#b16">[17]</ref> uses an extremely densely-connected structure, with the feature map of the output of the zeroth to the (l -1)th layers acting as the input to the lth layer. The connected structure is formulated as</p><formula xml:id="formula_2">X l = H l ([X 0 , X 1 , . . . . . . , X l-1 ]), l ∈ N +<label>(3)</label></formula><p>Remote Sens. 2018, 10, x FOR PEER REVIEW 5 of 18</p><p>are not processed. The valid convolution is used to reduce dimensions and retain extracted spectral features and raw spatial information.</p><p>A kernel of size of 𝑎 × 𝑎 × 1 (𝑎 &gt; 1) learns the spatial features from a HSI after the spectral features have been learned. By reducing the high dimension, a kernel of size 𝑎 × 𝑎 × 1 can learn the spatial features from the reserved spatial information of the previous step.</p><p>In short, our framework uses a 3D convolution layer of a kernel of size 1 × 1 × 𝑑 (𝑑 &gt; 1) to learn the spectral features. Next, the high dimension of the feature maps is reduced, and then a 3D convolution layer of a kernel of size 𝑎 × 𝑎 × 1 (𝑎 &gt; 1) learns the spatial features. Finally, the classification result is obtained by average pooling, flattening, and a fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Going Deeper with Densely-Connected Structures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Densely-Connected Structure</head><p>Assume that the CNN has 𝑙 convolution layers, 𝑋 𝑙 is the output of the 𝑙th layer and 𝐻 𝑙 ( * ) represents the complex nonlinear transformation operations in the 𝑙th convolution layer. The connected structure of the traditional CNN is such that the output of the (𝑙 -1)th layer is the input of the 𝑙th layer:</p><formula xml:id="formula_3">  1 l l l X H X l N     ,<label>(2)</label></formula><p>As shown in Figure <ref type="figure" target="#fig_2">2</ref>, DenseNet <ref type="bibr" target="#b16">[17]</ref> uses an extremely densely-connected structure, with the feature map of the output of the zeroth to the (𝑙 -1)th layers acting as the input to the 𝑙th layer. The connected structure is formulated as</p><formula xml:id="formula_4">  0 1 1 [ , ] l l l X H X X X l N      ,, ,<label>(3)</label></formula><p>DenseNet combines the number of channels and leaves the value of the feature maps unchanged. To promote the down-sampling of the framework, DenseNet is divided into multiple denselyconnected blocks called Dense Blocks, with a transition layer connecting each one. Each layer of DenseNet directly connects to the input and the prior layer, resulting in a hidden deep supervision. This connected structure reduces the phenomenon of gradient disappearance and thus constructs a deeper network. In addition, DenseNet has a regularizing effect that inhibits overfitting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Separately Learning Deeper Spectral and Spatial Features</head><p>The densely-connected structure is used to learn deeper spectral and spatial features from HSIs. The small cube block 𝑟 × 𝑟 × 𝐿 is the input of our model. To improve down-sampling and separately learn the deeper spatial and spectral features of HSIs, we divided the model into two denselyconnected blocks called dense spectral and spatial blocks.</p><p>Dense spectral blocks identify the deeper spectral features between multiple channels of a HSI. The first 3D convolution layer processes the original pixel data of size 𝑟 × 𝑟 × 𝐿 to produce 𝑛 feature maps with size 𝑟 × 𝑟 × 𝑏. The maps are the input to a dense spectral block denoted 𝑥 1 0 , where the DenseNet combines the number of channels and leaves the value of the feature maps unchanged. To promote the down-sampling of the framework, DenseNet is divided into multiple densely-connected blocks called Dense Blocks, with a transition layer connecting each one. Each layer of DenseNet directly connects to the input and the prior layer, resulting in a hidden deep supervision. This connected structure reduces the phenomenon of gradient disappearance and thus constructs a deeper network. In addition, DenseNet has a regularizing effect that inhibits overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Separately Learning Deeper Spectral and Spatial Features</head><p>The densely-connected structure is used to learn deeper spectral and spatial features from HSIs. The small cube block r × r × L is the input of our model. To improve down-sampling and separately learn the deeper spatial and spectral features of HSIs, we divided the model into two densely-connected blocks called dense spectral and spatial blocks.</p><p>Dense spectral blocks identify the deeper spectral features between multiple channels of a HSI. The first 3D convolution layer processes the original pixel data of size r × r × L to produce n feature maps with size r × r × b. The maps are the input to a dense spectral block denoted x 0 1 , where the subscript 1 represents the data in the dense spectral block of the model and the superscript 0 represents the data in the starting position of the dense spectral block. The 3D convolution layers (including the first 3D convolution layer) use k kernels of size 1 × 1 × d to learn deeper spectral features. The convolution layer in the dense spectral block is recorded as D 1 (•). Since the model is densely connected, the input of the lth layer is</p><formula xml:id="formula_5">x l 1 = D 1 x 0 1 , x 1 1 , . . . . . . , x l-1 1 l ∈ N + (4)</formula><p>As shown in Figure <ref type="figure" target="#fig_4">3</ref>, the size of the input and output feature maps of each composite convolution layer is the constant value r × r × b and the number of output feature maps is also a constant, k. However, the number of input feature maps increases linearly with the number of composite convolution layers. The number of the input feature maps can be formulated as follows:</p><formula xml:id="formula_6">k l = n + (l -1) × k (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where n is the index of the initial feature map. Through the dense spectral block, the channel feature maps merge to become k m , and successfully learn deeper spectral features and keep the spatial information. </p><formula xml:id="formula_8">  0 1 1 1 1 1 1 1</formula><p>, , ,</p><formula xml:id="formula_9">l l x D x x x l N      (4)</formula><p>As shown in Figure <ref type="figure" target="#fig_4">3</ref>, the size of the input and output feature maps of each composite convolution layer is the constant value 𝑟 × 𝑟 × 𝑏 and the number of output feature maps is also a constant, k. However, the number of input feature maps increases linearly with the number of composite convolution layers. The number of the input feature maps can be formulated as follows:</p><formula xml:id="formula_10">  1 l k n l k     (5)</formula><p>where 𝑛 is the index of the initial feature map. Through the dense spectral block, the channel feature maps merge to become 𝑘 𝑚 , and successfully learn deeper spectral features and keep the spatial information. The reducing dimensional layer connects the dense spectral block and dense spatial block. The aim is to compress the model and reduce the high dimensionality of feature maps. Inside the dense spectral and spatial blocks, the method of padding a 3D convolution layer is "same", which is the reason the output sizes are constant (𝑟 × 𝑟 × 𝑏). However, in reducing the dimensional layer, the method used for padding the 3D convolution layer is "valid" to change the size of feature maps.</p><p>As shown in Figure <ref type="figure" target="#fig_7">4</ref>, 𝑘 𝑚 feature maps with a size of 𝑟 × 𝑟 × 𝑏 proceed through the 3D convolution layer, which has a kernel size of 1 × 1 × 𝑏 and a kernel number 𝑝 (𝑝 &gt; 𝑏). Due to the 3D convolution layer with "valid" padding, the results is p feature maps of size 𝑟 × 𝑟 × 1. Through a reshaping operation, 𝑝 channels of 𝑟 × 𝑟 × 1 feature maps become one channel of size 𝑟 × 𝑟 × 𝑝. Then, a 3D convolution layer that has a kernel size of 𝑎 × 𝑎 × 𝑝 and a kernel number n transforms the feature map to an 𝑠 × 𝑠 × 1 with 𝑛 channels.</p><p>In summary, through two 3D convolution layers with "valid" padding and reshaping, the size The reducing dimensional layer connects the dense spectral block and dense spatial block. The aim is to compress the model and reduce the high dimensionality of feature maps. Inside the dense spectral and spatial blocks, the method of padding a 3D convolution layer is "same", which is the reason the output sizes are constant (r × r × b). However, in reducing the dimensional layer, the method used for padding the 3D convolution layer is "valid" to change the size of feature maps.</p><p>As shown in Figure <ref type="figure" target="#fig_7">4</ref>, k m feature maps with a size of r × r × b proceed through the 3D convolution layer, which has a kernel size of 1 × 1 × b and a kernel number p (p &gt; b). Due to the 3D convolution layer with "valid" padding, the results is p feature maps of size r × r × 1. Through a reshaping operation, p channels of r × r × 1 feature maps become one channel of size r × r × p. Then, a 3D convolution layer that has a kernel size of a × a × p and a kernel number n transforms the feature map to an s × s × 1 with n channels.  The dense spatial block learns the deeper spatial features of the HSI. For the convolution layer in the dense spatial block, the kernel size is 𝑎 × 𝑎 × 1, and the number of kernels is also 𝑘. The convolution layer in the dense spatial block is termed 𝐷 2 (•). The output of the convolution layer of the 𝑙th layer in the dense spatial block is given by</p><formula xml:id="formula_11">  0 1 1 2 2 2 2 2</formula><p>, , ,</p><formula xml:id="formula_12">l l x D x x x l N     <label>(6)</label></formula><p>As shown in Figure <ref type="figure" target="#fig_8">5</ref>, the size of the input and output feature maps of each convolution layer are of a constant size (𝑠 × 𝑠 × 1) and the number of output feature maps is also constant with value k. The number of input feature maps is the same as in Equation (5). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Going Faster and Preventing Overfitting</head><p>There are a large number of training parameters in our framework, which means long training times and a tendency to overfit the training sets. Here, we explain how our framework is able to be faster and prevent overfitting.</p><p>We selected PReLU as the activation function <ref type="bibr" target="#b19">[20]</ref>. It introduces a very small number of parameters on the basis of the ReLU <ref type="bibr" target="#b20">[21]</ref>. Its formula is</p><formula xml:id="formula_13">PReLU(𝑥 𝑖 ) = { 𝑥 𝑖 , 𝑖𝑓 𝑥 𝑖 &gt; 0 𝑎 𝑖 𝑥 𝑖 , 𝑖𝑓 𝑥 𝑖 ≤ 0 (7)</formula><p>where 𝑥 𝑖 is the input of the nonlinear activation on the 𝑖th channel and 𝑎 𝑖 is a learnable parameter that determines the slope of the negative part. PReLU adopts the momentum method when updating In summary, through two 3D convolution layers with "valid" padding and reshaping, the size of the feature maps becomes s × s × 1, which reduces the space size, the large number of channels, and the high dimensionality of the data blocks. This process facilitates the extraction of new features from the dense spatial block.</p><p>The dense spatial block learns the deeper spatial features of the HSI. For the convolution layer in the dense spatial block, the kernel size is a × a × 1, and the number of kernels is also k. The convolution layer in the dense spatial block is termed D 2 (•). The output of the convolution layer of the lth layer in the dense spatial block is given by</p><formula xml:id="formula_14">x l 2 = D 2 x 0 2 , x 1 2 , . . . . . . , x l-1 2 l ∈ N +<label>(6)</label></formula><p>As shown in Figure <ref type="figure" target="#fig_8">5</ref>, the size of the input and output feature maps of each convolution layer are of a constant size (s × s × 1) and the number of output feature maps is also constant with value k. The number of input feature maps is the same as in Equation ( <ref type="formula" target="#formula_6">5</ref>).  The dense spatial block learns the deeper spatial features of the HSI. For the convolution layer in the dense spatial block, the kernel size is 𝑎 × 𝑎 × 1, and the number of kernels is also 𝑘. The convolution layer in the dense spatial block is termed 𝐷 2 (•). The output of the convolution layer of the 𝑙th layer in the dense spatial block is given by</p><formula xml:id="formula_15">  0 1 1 2 2 2 2 2</formula><p>, , ,</p><formula xml:id="formula_16">l l x D x x x l N     <label>(6)</label></formula><p>As shown in Figure <ref type="figure" target="#fig_8">5</ref>, the size of the input and output feature maps of each convolution layer are of a constant size (𝑠 × 𝑠 × 1) and the number of output feature maps is also constant with value k. The number of input feature maps is the same as in Equation (5). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Going Faster and Preventing Overfitting</head><p>There are a large number of training parameters in our framework, which means long training times and a tendency to overfit the training sets. Here, we explain how our framework is able to be faster and prevent overfitting.</p><p>We selected PReLU as the activation function <ref type="bibr" target="#b19">[20]</ref>. It introduces a very small number of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Going Faster and Preventing Overfitting</head><p>There are a large number of training parameters in our framework, which means long training times and a tendency to overfit the training sets. Here, we explain how our framework is able to be faster and prevent overfitting.</p><p>We selected PReLU as the activation function <ref type="bibr" target="#b19">[20]</ref>. It introduces a very small number of parameters on the basis of the ReLU <ref type="bibr" target="#b20">[21]</ref>. Its formula is</p><formula xml:id="formula_17">PReLU(x i ) = x i , i f x i &gt; 0 a i x i , i f x i ≤ 0 (7)</formula><p>where x i is the input of the nonlinear activation on the ith channel and a i is a learnable parameter that determines the slope of the negative part. PReLU adopts the momentum method when updating a i :</p><formula xml:id="formula_18">∆a i := µ∆a i + lr ∂ε ∂a i<label>(8)</label></formula><p>For the updating formula, µ is the momentum and lr is the learning rate. When updating a i , the weight decay should not be used because a i may tend to zero. a i = 0.25 is used as the initial value. Although ReLU is a useful nonlinear function, it hinders counter-propagation, whereas PReLU makes the model converge more quickly. Batch normalization (BN) <ref type="bibr" target="#b21">[22]</ref> adds standardized processing to the input data of each layer in the training process of a neural network and means that the gradients converge faster, saving time and resources during model training. For the proposed framework, BN and PReLU are added before the 3D convolution layer, except for the first 3D convolution layer.</p><p>Early stopping and the dynamic learning rate are also used when training a model. Stopping early means that, after a certain number of epochs (such as 50 in this paper), if the loss is no longer decreasing, the training process will be stopped early. This reduces the training time as well as preventing overfitting. We adopted a variable learning rate because the step size should decrease as the result approaches an optimal value. With a better initial learning rate, the learning rate is halved when the precision does not increase after a certain number of epochs (such as 10 epochs in this paper). If precision no longer increases after a certain number of epochs, the learning rate will be reduced by half again and will loop until it is less than the set minimum learning rate. In this paper, the minimum learning rate was set to 0; that is, the learning rate looped until the maximum number of epochs was reached.</p><p>Since the network of the proposed model is deeper, we used a dropout layer <ref type="bibr" target="#b22">[23]</ref> before the full connection layer to reduce the possibility of overfitting. We set the dropout rate to 50% because at this point the network structure randomly generated by the dropout layer is the greatest and produced the best results. Cross-validation prevents overfitting for complex models, so we divided our datasets into a training, validation, and test datasets for cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Fast Dense Spectral-Spatial Convolution Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">Objective Function</head><p>HSI classification presents a typical classification problem. For such problems, a cross-entropy loss function is commonly used to measure the difference between predicted value and real value to optimize the parameters of the model. In this paper, the predicted value of the FDSSC framework is a vector, y, where y ∈ R 1×C , and is formulated as follows:</p><formula xml:id="formula_19">y = FDSSC x r×r×L , δ<label>(9)</label></formula><p>where δ is the parameter of the FDSSC model to be optimized and C is the number of categories to be classified. Since HSI classification requires multiple classification discriminations, we performed a softmax regression, with the loss function</p><formula xml:id="formula_20">L s = - m ∑ i=1 log e W T y i x i +b y i ∑ n j=1 e W T j x i +b j<label>(10)</label></formula><p>where m denotes the size of the mini-batch, n the number of categories to be classified, x i the ith deep feature belonging to the y i th class, W j the jth column of the weights W in the last fully connected layer, and b the bias term. Therefore, the objective function of the FDSSC framework, F FDSSC , is</p><formula xml:id="formula_21">F FDSSC = min δ (- m ∑ i=1 log e W T y i x i +b y i ∑ n j=1 e W T j x i +b j )<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">Fast Dense Spectral-Spatial Convolution Network for Classification of Labeled Pixels</head><p>For a hyperspectral image with L channels and H × W size, r was selected as 9; that is, a target pixel served as the center of a small cube with size 9 × 9 × L selected from the original pixel data as the input of the neural network. In this paper, the convolution kernel number of dense blocks k was 12 and the number of convolution layers of dense blocks l was 3. The FDSSC network is shown in Figure <ref type="figure" target="#fig_15">6</ref>. For a hyperspectral image with 𝐿 channels and 𝐻 × 𝑊 size, r was selected as 9; that is, a target pixel served as the center of a small cube with size 9 × 9 × 𝐿 selected from the original pixel data as the input of the neural network. In this paper, the convolution kernel number of dense blocks 𝑘 was 12 and the number of convolution layers of dense blocks 𝑙 was 3. The FDSSC network is shown in Figure <ref type="figure" target="#fig_15">6</ref>. For the following detailed explanation, BN and PReLU are added before all convolution and average pooling layers, except the first 3D convolution layer. As shown in Figure <ref type="figure" target="#fig_15">6</ref>, the 9 × 9 × 𝐿 original data pass through the first 3D convolution layer generated 𝑛 = 24 feature maps of size 9 × 9 × 𝑏 because the stride of the first convolution layer is (1,1,2) and the method of padding is "valid." For the convolution layers of the dense spectral block, the kernel size is (1,1,7), the kernel number is 12, the method of padding is "same", and the stride is (1,1,1), so the output of each convolution layer is 12 9 × 9 × 𝑏 feature maps containing the learned spectral features. Merging all output and the initial input, the size of feature maps is unchanged and the number of channels is 24 + 12 × 3 = 60.</p><p>In reducing the dimensional layer, 60 feature maps with a size of 9 × 9 × 𝑏 proceed through the 3D convolution layer, which has a kernel size of 1 × 1 × 𝑏 and a kernel number of 200. Since the 3D convolution layer has "valid" padding, there are 200 9 × 9 × 1 feature maps. Through a reshaping operation, the 200 channels of 9 × 9 × 1 feature maps become one feature map with a size of 9 × 9 × 200. Next, a 3D convolution layer with 𝑛 = 24 and a size of 3 × 3 × 200 transformed the feature map into a 7 × 7 × 1 with 24 channels. For the following detailed explanation, BN and PReLU are added before all convolution and average pooling layers, except the first 3D convolution layer. As shown in Figure <ref type="figure" target="#fig_15">6</ref>, the 9 × 9 × L original data pass through the first 3D convolution layer generated n = 24 feature maps of size 9 × 9 × b because the stride of the first convolution layer is (1, 1, 2) and the method of padding is "valid." For the convolution layers of the dense spectral block, the kernel size is (1, 1, 7), the kernel number is 12, the method of padding is "same", and the stride is (1, 1, 1), so the output of each convolution layer is 12 9 × 9 × b feature maps containing the learned spectral features. Merging all output and the initial input, the size of feature maps is unchanged and the number of channels is 24 + 12 × 3 = 60. In reducing the dimensional layer, 60 feature maps with a size of 9 × 9 × b proceed through the 3D convolution layer, which has a kernel size of 1 × 1 × b and a kernel number of 200. Since the 3D convolution layer has "valid" padding, there are 200 9 × 9 × 1 feature maps. Through a reshaping operation, the 200 channels of 9 × 9 × 1 feature maps become one feature map with a size of 9 × 9 × 200. Next, a 3D convolution layer with n = 24 and a size of 3 × 3 × 200 transformed the feature map into a 7 × 7 × 1 with 24 channels.</p><p>For the convolution layers of the dense spatial block, the kernel size is (3, 3, 1), the kernel number is 12, and the padding is "same," so each output of the convolution layers is 12 7 × 7 × 1 feature maps to learn the deeper spatial features. Similar to the dense spectral block, 60 feature maps with a size of 7 × 7 × 1 are produced.</p><p>Finally, the 3D average pooling layer with a 7 × 7 × 1 pooling size changes the size of the feature maps to 1 × 1 × 1. Through the flattening operation, dropout layer, and fully-connected layers, a prediction vector 1 × 1 × C is produced, where C is the number of categories to be classified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3.">Fast Dense Spectral-Spatial Framework</head><p>Summarizing the above steps, we have proposed a framework that learns deeper spectral and spatial features while reducing the training time compared to deep learning CNN-based methods. The FDSSC framework is shown in Figure <ref type="figure" target="#fig_19">7</ref>.  The partition of HSI data and labels is the first step of the proposed framework. For crossvalidation, we randomly divided the labeled pixels and corresponding labels into training, validation, and testing datasets selected with size 9 × 9 × 𝐿 from the original 𝐻 × 𝑊 × 𝐿 HSIs, denoting these datasets 𝑋 train , 𝑋 val , and 𝑋 test , respectively.</p><p>Taking the cross-entropy as the objective function, 𝑋 train and 𝑋 val were utilized to train the FDSSC network to obtain the best FDSSC model under the control of the dynamic learning rate and early stopping. The FDSSC framework was only used 𝑋 train to optimize the parameters of the model through back-propagation, and tested the initial trained models using 𝑋 val . Through crossvalidation, the FDSSC could obtain the best-trained model. Finally, FDSSC made use of the besttrained model to obtain three evaluation indices of performance by 𝑋 test and classified all datasets.</p><p>Combining Figures <ref type="figure" target="#fig_16">6</ref> and<ref type="figure" target="#fig_19">7</ref> clarifies the technical advantages of the proposed method. First, the FDSSC network only uses a convolution layer and an average pooling layer to learn features and the fully-connected layer as a classifier, so it is an end-to-end framework and reduces high dimensions without complicated feature engineering. Second, because of the densely-connected method, the FDSSC network has a deeper structure resulting in extremely efficient performance. Finally, the convergence rate of the model is very fast because of the BN and PReLU applied to the FDSSC network and dynamic learning rate, and early stopping. Therefore, the training time of the proposed framework is shorter, and although the FDSSC model has a high quantity of parameters, it lacks overfitting on account of the dropout layer in the FDSSC network, early stopping, and crossvalidation. The partition of HSI data and labels is the first step of the proposed framework. For cross-validation, we randomly divided the labeled pixels and corresponding labels into training, validation, and testing datasets selected with size 9 × 9 × L from the original H × W × L HSIs, denoting these datasets X train , X val , and X test , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets and Experimental Setup</head><p>Taking the cross-entropy as the objective function, X train and X val were utilized to train the FDSSC network to obtain the best FDSSC model under the control of the dynamic learning rate and early stopping. The FDSSC framework was only used X train to optimize the parameters of the model through back-propagation, and tested the initial trained models using X val . Through cross-validation, the FDSSC could obtain the best-trained model. Finally, FDSSC made use of the best-trained model to obtain three evaluation indices of performance by X test and classified all datasets.</p><p>Combining Figures <ref type="figure" target="#fig_16">6</ref> and<ref type="figure" target="#fig_19">7</ref> clarifies the technical advantages of the proposed method. First, the FDSSC network only uses a convolution layer and an average pooling layer to learn features and the fully-connected layer as a classifier, so it is an end-to-end framework and reduces high dimensions without complicated feature engineering. Second, because of the densely-connected method, the FDSSC network has a deeper structure resulting in extremely efficient performance. Finally, the convergence rate of the model is very fast because of the BN and PReLU applied to the FDSSC network and dynamic learning rate, and early stopping. Therefore, the training time of the proposed framework is shorter, and although the FDSSC model has a high quantity of parameters, it lacks overfitting on account of the dropout layer in the FDSSC network, early stopping, and cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets and Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>In our experiments, we used the Indiana Pines (IN), the University of Pavia (UP, Pavia, Italy), and the Kennedy Space Center (KSC, Merritt Island, FL, USA) datasets (Supplementary Materials). The KSC data were obtained by the AVIRIS spectrometer in Florida in 1996. The size of the original data is 512 × 614 × 176 and it contains 13 kinds of ground cover. Table <ref type="table" target="#tab_2">1</ref> summarizes the categories and image counts for each. The University of Pavia data are from flights of the ROSIS sensor over Pavia in Northern Italy in 2003. The original data has size 610 × 340 × 103 with spatial resolution 1.3 m. Table <ref type="table" target="#tab_3">2</ref> shows the nine types of ground cover and the image counts for each. The IN data were obtained by the AVIRIS spectrometer in Northwestern Indiana in 1996. The original data size is 145 × 145 × 220, with 16 kinds of ground cover. Table <ref type="table" target="#tab_4">3</ref> provides detailed category information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experimental Setup for Classification of Labeled Pixels</head><p>We configured our FDSSC framework for the classification of labeled pixels as follows: The kernel number of the dense blocks was set to k = 12 and the number of convolution layers in each dense block was set to 3. From the possible batch sizes of <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr">32,</ref><ref type="bibr">64)</ref>, 32 was selected in view of the performance of our graphics processing unit (GPU) and test accuracy. The time limit of the initial training epochs was 400, but on the basis of our prior experimental results the best precision was reached within 80 epochs, so the number of epochs used during training was only 80.</p><p>Taking the IN data as an example, the RMSprop <ref type="bibr" target="#b23">[24]</ref>, Adam <ref type="bibr" target="#b24">[25]</ref>, and Nadam <ref type="bibr" target="#b25">[26]</ref> optimizers yielded final precision results of 99.777%, 99.665%, and 99.567%, respectively. Thus, we chose the RMSprop optimizer. The initial learning rate was 0.0003. We used the He normal distribution initialization method <ref type="bibr" target="#b19">[20]</ref> as the initialization method for all convolution layers in the neural network model. We used the Xavier normal distribution initialization method <ref type="bibr" target="#b26">[27]</ref>, also known as the Glorot normal distribution initialization method, for the fully-connected layer.</p><p>For the 3D CNN used for HSI classification, the spatial size of the input sample is an important factor affecting the HSI classification. Using the three datasets, we set r to 5, 7, 9, 11, and 13; that is, spatial sizes of 5 × 5, 7 × 7, 9 × 9, 11 × 11, and 13 × 13 for the input sample. We measured the overall accuracy (OA), the average accuracy (AA), and kappa coefficient (K) for each dataset. Figure <ref type="figure">8a</ref> shows that with an increase in the size of the input space the classification accuracy on the IP dataset began to fall after 7 × 7. For the KSC dataset, Figure <ref type="figure">8b</ref> shows that the classification accuracy increased with increasing size of the input space, but after 9 × 9, the accuracy reached 99.9% and then increased by less than 0.01%. The UP dataset showed very little increase in accuracy after 9 × 9, as shown in Figure <ref type="figure">8c</ref>. Therefore, we chose 9 × 9 as the size for testing the performance of the FDSSC framework.</p><p>Deep learning algorithms rely greatly on the training samples. The more data used in training, the higher the test accuracy. We tried different training sample sizes on the three datasets, using 10%, 15%, 20%, 25%, and 30% on the IP and KSC datasets. For the UP dataset, the large number of data samples meant that the accuracy of the three training samples reached 99.97% with a training sample size of only 15%. Thus, we chose to test with sample sizes of 5%, 7.5%, 10%, 12.5%, and 15%. Figure <ref type="figure">9a</ref> shows that, as the training sample size increased on the IN dataset, the OA and AA also increased, but the kappa coefficient decreased after 20%. As shown in Figure <ref type="figure">9b</ref>, after the 20% training sample size with the KSC dataset, the OA, AA, and kappa coefficient reached 99.9%, and when the training sample size increased to 30%, all three reached 100%. As shown in Figure <ref type="figure">9c</ref>, for the UP dataset, the OA, AA, and kappa coefficent reached 99.9% at a 10% training sample size. At a 15% training sample size, all three datasets achieved 0.01-0.02% higher accuracy than at 10%. Therefore, for the FDSSC framework, although larger training sample sizes improve accuracy to a degree, accuracy was over 99.9% for the KSC dataset with only a 20% training sample size, with the same accuracy for the UP dataset at only 10%. Even substantial increases in training time brought only very limited increases in accuracy. Therefore, we chose a training sample size of 20% for the IN and KSC datasets and 10% for the UP dataset; for all of the HSI datasets the validation dataset size was half the training dataset size and the remainder comprised the test dataset. </p><note type="other">1</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results and Discussion</head><p>In our experiment, we compared the proposed FDSSC framework to other deep-learning-based methods, that is, SAE-LR <ref type="bibr" target="#b7">[8]</ref>, CNN <ref type="bibr" target="#b8">[9]</ref>, 3D-CNN-LR <ref type="bibr" target="#b10">[11]</ref>, and the state-of-art SSRN method <ref type="bibr" target="#b12">[13]</ref> (only for labeled pixels). SAE-LR was implemented with Theano <ref type="bibr" target="#b27">[28]</ref>. CNN, SSRN, and the proposed FDSSC were implemented with Keras <ref type="bibr" target="#b28">[29]</ref> using TensorFlow <ref type="bibr" target="#b29">[30]</ref> as a backend. 3D-CNN-LR was obtained from the literature <ref type="bibr" target="#b10">[11]</ref>. In the following, the detailed classification accuracy and training times are shown and discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Results</head><p>In our experiment, we randomly selected 10 groups of training samples from the KSC, UP, and IN datasets. Experimental results are given in the form "mean ± variance." The training and testing time results were obtained using the same computer, which was configured with 32 GB of memory and a NVIDIA GeForce GTX 1080Ti GPU. The OA, AA, and kappa coefficient were used to determine the accuracy of the classification results. Work from <ref type="bibr" target="#b8">[9]</ref> was denoted CNN. The input spatial size is important for the 3D convolution method. Therefore, to ensure a fair comparison, an appropriate spatial size was chosen for each method. For the SSRN method, the classification accuracy increases with the spatial size, so we used an input spatial size of 9 × 9, which was the same as that of the FDSSC framework. Figures <ref type="figure" target="#fig_22">10</ref><ref type="figure" target="#fig_24">11</ref><ref type="figure" target="#fig_27">12</ref>show classification maps for each of the methods. and a NVIDIA GeForce GTX 1080Ti GPU. The OA, AA, and kappa coefficient were used to determine the accuracy of the classification results. Work from <ref type="bibr" target="#b8">[9]</ref> was denoted CNN. The input spatial size is important for the 3D convolution method. Therefore, to ensure a fair comparison, an appropriate spatial size was chosen for each method. For the SSRN method, the classification accuracy increases with the spatial size, so we used an input spatial size of 9 × 9, which was the same as that of the FDSSC framework. Figures <ref type="figure" target="#fig_22">10</ref><ref type="figure" target="#fig_24">11</ref><ref type="figure" target="#fig_27">12</ref>show classification maps for each of the methods.       The comparison of deep-learning-based methods relies heavily on training data. Sometimes a method may appear better, but in reality it simply has more training data. Therefore, the same amount of training data should be used for all of the methods compared. However, for some methods, such as the CNN for the IN dataset, when we used the same proportion to train the model the overall accuracy would fall by approximately 2%. Therefore, in order to obtain the best accuracy for each model, in these cases we use the same training proportion as previously used in the literature. For SAE-LR, the split ratio of training, validation, and testing data was 6:2:2; for a CNN, the split ratio was 8:1:1; for the SSRN method, the split ratio was 2:1:7 for the IN and KSC datasets, and 1:1:8 for the UP dataset; for the proposed FDSSC framework, we used 20% of the labeled pixels as the training set, and 10% and 70% for validation and testing datasets, respectively, for the IN and KSC datasets, and 10%, 5%, and 85% for the UP dataset. Table <ref type="table" target="#tab_5">4</ref> shows the OA, AA, and kappa coefficient for the different methods for the KSC, IN, and UP datasets. From Table <ref type="table" target="#tab_5">4</ref>, it can be clearly seen that the proposed FDSSC framework is superior to SAE-LR, CNN, and 3D-CNN-LR methods. For the state-of-the-art SSRN method, we note slight improvements of 0.02%, 0.37%, and 0.04% for the KSC, IN, and UP datasets, respectively. There is no more obvious improvement because the OAs of two of the methods are higher than 99%.</p><p>Table <ref type="table" target="#tab_6">5</ref> summarizes the average training times, training epochs, and testing times of 10 runs of the SAE-LR, CNN, SSRN, and FDSSC methods. SAE-LR was trained with 3300 epochs of pre-training and 400,000 epochs of fine-tuning <ref type="bibr" target="#b7">[8]</ref>; the training time for fine-tuning was only 61.7 min. For a CNN <ref type="bibr" target="#b8">[9]</ref>, the training process of this model converged in almost 40 epochs, but in our experiments the model trained with 120 epochs achieved the best accuracy. The SSRN method needed 200 training epochs <ref type="bibr" target="#b12">[13]</ref> and the FDSSC framework only needed 80 training epochs to achieve the best accuracy. Therefore, the FDSSC training time was less than that of other deep-learning-based methods. The hyperspectral data became larger from KSC to UP, and the time difference of the FDSSC framework increased compared with other deep-learning-based methods. Therefore, the larger the hyperspectral data, the more time was reduced by the FDSSC framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Discussion</head><p>In terms of accuracy of the HSI classification methods, for deep-learning-based methods, our experiments show that the deeper the framework is, the higher the classification accuracy. The proposed FDSSC framework is obviously superior to the SAE-LR, CNN, and 3D-CNN-LR methods. Compared with the SSRN method, which has state-of-the-art accuracy, the FDSSC method improves OA and AA by 0.40% and 11.23%, respectively, for the IN dataset. It is precisely because of the greater depth of the FDSSC network that the spectral and spatial features of HSIs are more effectively utilized, with better feature transfer between the convolution layers. Although the training size of the SAE-LR and CNN methods are greater than that of the FDSSC framework, the FDSSC framework has higher accuracy than the SAE-LR and CNN methods. In addition, compared with the 24 kernel numbers of the SSRN method, the FDSSC framework uses only 12 kernel numbers, and the model is narrower.</p><p>In terms of the training time, the FDSSC framework takes less time and has the characteristics of fast convergence. Many deep learning methods, such as the SSRN, use ReLU, but the FDSSC framework uses PReLU. Problems with ReLU include neuronal death and the offset phenomenon. The former occurs because when x &lt; 0, ReLU will be in the hard-saturation area. As training advances, part of the input will fall into the hard-saturation area, meaning that the corresponding weight cannot be updated. The latter occurs because the mean of the activations is always greater than zero. Neuronal death and offset phenomena jointly influence the convergence of a network. Compared with ReLU, PReLU converges faster because the output of PReLU is closer to zero. BN and dynamic learning rate are the other reasons for fast convergence. Thus, compared with the 400,000 epochs required by the SAE-LR method, 200 epochs required by the SSRN method, and 120 epochs by the CNN, the FDSSC framework needs only 80 epochs to obtain the best accuracy, which leads to a shorter training time than other deep-learning-based methods.</p><p>Therefore, taking both accuracy and running time into account, we conclude that the FDSSC framework has state-of-the-art accuracy with less required training time than methods achieving similar accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this paper, we propose an end-to-end, fast, and dense spectral-spatial convolution framework for HSI classification. The most significant features of the proposed FDSSC framework are depth and speed. Furthermore, the FDSSC framework has no complicated mechanism for reducing the dimensionality, and instead uses original 3D pixel data directly as input. The proposed framework uses two different dense blocks to extract abundant spatial features and spectral features in HSIs automatically. The densely-connected arrangement of dense blocks deepens the network, reducing the problem of gradient disappearance. The result is that the classification precision of the FDSSC framework reaches a very high level. We introduced BN, dropout layers, and dynamic learning rates, and adopted PReLU as the activation function of the neural network to initialize fully-connected layers. These improvements led the FDSSC framework to converge faster and prevented overfitting, such that only 80 epochs were needed to achieve the best classification accuracy.</p><p>The future direction of our work is hyperspectral data segmentation, aimed at segmenting hyperspectral data based on our classification work. We plan to study an end-to-end, pixel-to-pixel deep-learning-based method for hyperspectral data segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) 2D convolution and (b) 3D convolution operations per Equation (1).</figDesc><graphic coords="4,172.61,146.22,250.08,327.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) 2D convolution and (b) 3D convolution operations per Equation (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example of a DenseNet with four composite layers (l = 4).</figDesc><graphic coords="5,98.10,525.99,400.27,133.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example of a DenseNet with four composite layers (l = 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Structure of dense spectral block with three convolution layers (l = 3).</figDesc><graphic coords="6,93.44,490.03,414.66,169.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Structure of dense spectral block with three convolution layers (l = 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Remote Sens. 2018, 10, x FOR PEER REVIEW 7 of 18</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Structure of reducing dimensional layer. "Filter" is the number of convolution kernels and "padding" is the strategy of supplementing zero.</figDesc><graphic coords="7,94.00,178.53,412.26,139.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Structure of dense spatial block with three convolution layers (l = 3).</figDesc><graphic coords="7,90.13,495.77,420.03,171.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Structure of reducing dimensional layer. "Filter" is the number of convolution kernels and "padding" is the strategy of supplementing zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Remote Sens. 2018, 10, x FOR PEER REVIEW 7 of 18</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Structure of reducing dimensional layer. "Filter" is the number of convolution kernels and "padding" is the strategy of supplementing zero.</figDesc><graphic coords="7,91.51,254.87,412.26,139.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Structure of dense spatial block with three convolution layers (l = 3).</figDesc><graphic coords="7,87.64,572.11,420.03,171.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Structure of dense spatial block with three convolution layers (l = 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Remote Sens. 2018, 10, x FOR PEER REVIEW 9 of 18 2.4.2. Fast Dense Spectral-Spatial Convolution Network for Classification of Labeled Pixels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The fast dense spectral-spatial convolution (FDSSC) network for hyperspectral image (HSI) classification of labeled pixels with a 9 × 9 × 𝐿 input. L is the number of bands of HSI. C is the number of categories to be classified.</figDesc><graphic coords="9,94.90,382.74,409.43,209.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The fast dense spectral-spatial convolution (FDSSC) network for hyperspectral image (HSI) classification of labeled pixels with a 9 × 9 × L input. L is the number of bands of HSI. C is the number of categories to be classified.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>Remote Sens. 2018, 10, x FOR PEER REVIEW 10 of 18 2.4.3. Fast Dense Spectral-Spatial Convolution Framework Summarizing the above steps, we have proposed a framework that learns deeper spectral and spatial features while reducing the training time compared to deep learning CNN-based methods. The FDSSC framework is shown in Figure 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. FDSSC Framework for HSI classification of labeled pixels. The details of the FDSSC network are shown in Figure 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. FDSSC Framework for HSI classification of labeled pixels. The details of the FDSSC network are shown in Figure 6.</figDesc><graphic coords="10,122.41,350.81,354.05,150.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 8 . 1 Figure 9 .</head><label>819</label><figDesc>Figure 8. Accuracy with different spatial size inputs: (a) Indian Pines; (b) Kennedy Space Center; and (c) University of Pavia scenes.</figDesc><graphic coords="13,103.04,199.80,403.79,110.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Classification maps for KSC dataset: (a) real image of one band in KSC dataset; (b) groundtruth map; (c) SAE-LR, OA = 92.99%; (d) CNN, OA = 99.31%; (e) SSRN, OA = 99.94%; and (f) FDSSC, OA = 99.96%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Classification maps for KSC dataset: (a) real image of one band in KSC dataset; (b) ground-truth map; (c) SAE-LR, OA = 92.99%; (d) CNN, OA = 99.31%; (e) SSRN, OA = 99.94%; and (f) FDSSC, OA = 99.96%. Remote Sens. 2018, 10, x FOR PEER REVIEW 14 of 18</figDesc><graphic coords="14,103.95,92.98,393.15,192.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Classification maps for UP dataset: (a) real image of one band in UP dataset; (b) groundtruth map; (c) SAE-LR, OA = 98.46%; (d) CNN, OA = 99.38%; (e) SSRN, OA = 99.93%; and (f) FDSSC, OA = 99.96%.</figDesc><graphic coords="14,107.77,349.11,385.23,329.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Classification maps for UP dataset: (a) real image of one band in UP dataset; (b) ground-truth map; (c) SAE-LR, OA = 98.46%; (d) CNN, OA = 99.38%; (e) SSRN, OA = 99.93%; and (f) FDSSC, OA = 99.96%.</figDesc><graphic coords="14,103.61,733.01,393.51,212.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Classification maps for UP dataset: (a) real image of one band in UP dataset; (b) groundtruth map; (c) SAE-LR, OA = 98.46%; (d) CNN, OA = 99.38%; (e) SSRN, OA = 99.93%; and (f) FDSSC, OA = 99.96%.</figDesc><graphic coords="15,107.38,-292.83,385.23,329.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Classification maps for IN dataset: (a) real image of one band in the IN dataset; (b) groundtruth map; (c) SAE-LR, OA = 93.98%; (d) CNN, OA = 95.96%; (e) SSRN, OA = 99.35%; and (f) FDSSC, OA = 99.72%. The comparison of deep-learning-based methods relies heavily on training data. Sometimes a method may appear better, but in reality it simply has more training data. Therefore, the same</figDesc><graphic coords="15,103.22,91.06,393.51,212.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Classification maps for IN dataset: (a) real image of one band in the IN dataset; (b) ground-truth map; (c) SAE-LR, OA = 93.98%; (d) CNN, OA = 95.96%; (e) SSRN, OA = 99.35%; and (f) FDSSC, OA = 99.72%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>which are CNN-based deep learning methods. A deeper structured CNN can learn more useful deep spatial and spectral features, which leads to extremely high accuracy, and thus the FDSSC framework has better performance than previous methods. (3) It reduces the training time while achieving state-of-the-art performance. Although it has a deeper structure, the FDSSC framework is easier to train than other deep-learning-based methods.</figDesc><table /><note><p>Specifically, it achieves the best accuracy in only 80 epochs, compared with 600,000 epochs for SAE-LR and 200 epochs for the SSRN framework.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>the data in the dense spectral block of the model and the superscript 0 represents the data in the starting position of the dense spectral block. The 3D convolution layers (including the first 3D convolution layer) use 𝑘 kernels of size 1 × 1 × 𝑑 to learn deeper spectral features. The convolution layer in the dense spectral block is recorded as 𝐷 1 (•). Since the model is densely connected, the input of the 𝑙th layer is</figDesc><table><row><cell>Remote Sens. 2018, 10, x FOR PEER REVIEW</cell><cell>6 of 18</cell></row><row><cell>subscript 1 represents</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Category information for the Kennedy Space Center dataset.</figDesc><table><row><cell>Order Number</cell><cell>Classification</cell><cell>Number of Samples</cell></row><row><cell>1</cell><cell>Scrub</cell><cell>347</cell></row><row><cell>2</cell><cell>Willow swamp</cell><cell>243</cell></row><row><cell>3</cell><cell>CP hammock</cell><cell>256</cell></row><row><cell>4</cell><cell>Slash pine</cell><cell>252</cell></row><row><cell>5</cell><cell>Oak/broadleaf</cell><cell>161</cell></row><row><cell>6</cell><cell>Hardwood</cell><cell>229</cell></row><row><cell>7</cell><cell>Swamp</cell><cell>105</cell></row><row><cell>8</cell><cell>Graminoid marsh</cell><cell>390</cell></row><row><cell>9</cell><cell>Spartina marsh</cell><cell>520</cell></row><row><cell>10</cell><cell>Cattail marsh</cell><cell>404</cell></row><row><cell>11</cell><cell>Salt marsh</cell><cell>419</cell></row><row><cell>12</cell><cell>Mud flats</cell><cell>503</cell></row><row><cell>13</cell><cell>Water</cell><cell>927</cell></row><row><cell></cell><cell>Total</cell><cell>5211</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Category information for the University of Pavia dataset.</figDesc><table><row><cell>Order Number</cell><cell>Classification</cell><cell>Number of Samples</cell></row><row><cell>1</cell><cell>Asphalt</cell><cell>6631</cell></row><row><cell>2</cell><cell>Meadows</cell><cell>18,649</cell></row><row><cell>3</cell><cell>Gravel</cell><cell>2099</cell></row><row><cell>4</cell><cell>Trees</cell><cell>3064</cell></row><row><cell>5</cell><cell>Painted metal sheets</cell><cell>1345</cell></row><row><cell>6</cell><cell>Bare soil</cell><cell>5029</cell></row><row><cell>7</cell><cell>Bitumen</cell><cell>1330</cell></row><row><cell>8</cell><cell>Self-blocking bricks</cell><cell>3682</cell></row><row><cell>9</cell><cell>Shadows</cell><cell>947</cell></row><row><cell></cell><cell>Total</cell><cell>42,776</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Category information for the Indiana Pines dataset.</figDesc><table><row><cell>Order Number</cell><cell>Classification</cell><cell>Number of Samples</cell></row><row><cell>1</cell><cell>Alfafa</cell><cell>46</cell></row><row><cell>2</cell><cell>Corn-notill</cell><cell>1428</cell></row><row><cell>3</cell><cell>Corn-mintill</cell><cell>830</cell></row><row><cell>4</cell><cell>Corn</cell><cell>237</cell></row><row><cell>5</cell><cell>Grass-pasture</cell><cell>483</cell></row><row><cell>6</cell><cell>Grass-trees</cell><cell>730</cell></row><row><cell>7</cell><cell>Grass-pasture</cell><cell>28</cell></row><row><cell>8</cell><cell>Hay-windrowed</cell><cell>478</cell></row><row><cell>9</cell><cell>Oats</cell><cell>20</cell></row><row><cell>10</cell><cell>Soybean-notill</cell><cell>972</cell></row><row><cell>11</cell><cell>Soybean-mintill</cell><cell>2455</cell></row><row><cell>12</cell><cell>Soybean-clean</cell><cell>593</cell></row><row><cell>13</cell><cell>Wheat</cell><cell>205</cell></row><row><cell>14</cell><cell>Woods</cell><cell>1265</cell></row><row><cell>15</cell><cell>Building-grass-trees-drives</cell><cell>386</cell></row><row><cell>16</cell><cell>Stone-steal-towers</cell><cell>93</cell></row><row><cell></cell><cell>Total</cell><cell>10,249</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Classification results of different methods for labeled pixels of the KSC, IN, and UP datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>KSC</cell><cell></cell><cell></cell><cell>IN</cell><cell></cell><cell></cell><cell>UP</cell><cell></cell></row><row><cell></cell><cell>OA%</cell><cell>AA%</cell><cell>K × 100</cell><cell>OA%</cell><cell>AA%</cell><cell>K × 100</cell><cell>OA%</cell><cell>AA%</cell><cell>K × 100</cell></row><row><cell>SAE-LR [8]</cell><cell>92.99 ± 0.82</cell><cell>89.76 ± 1.25</cell><cell>92.18 ± 0.91</cell><cell>96.53 ± 0.08</cell><cell>96.03 ± 0.49</cell><cell>96.05 ± 0.11</cell><cell>98.46 ± 0.02</cell><cell>97.67 ± 0.04</cell><cell>97.78 ± 0.03</cell></row><row><cell>CNN [9]</cell><cell>99.31 ± 0.04</cell><cell>98.92 ± 0.15</cell><cell>99.23 ± 0.07</cell><cell>95.96 ± 0.44</cell><cell>97.75 ± 0.05</cell><cell>95.42 ± 0.57</cell><cell>99.38 ± 0.01</cell><cell>99.23 ± 0.01</cell><cell>99.17 ± 0.02</cell></row><row><cell>3D-CNN-LR [11]</cell><cell>96.31 ± 1.25</cell><cell>94.68 ± 1.97</cell><cell>95.90 ± 1.39</cell><cell>97.56 ± 0.43</cell><cell>99.23 ± 0.19</cell><cell>97.02 ± 0.52</cell><cell>99.54 ± 0.11</cell><cell>99.66 ± 0.11</cell><cell>99.41 ± 0.15</cell></row><row><cell>SSRN [13]</cell><cell>99.94 ±0.07</cell><cell>99.93 ± 0.08</cell><cell>99.94 ± 0.08</cell><cell>99.35 ± 0.19</cell><cell>88.44 ± 2.35</cell><cell>99.26 ± 0.22</cell><cell>99.93 ± 0.03</cell><cell>99.91 ± 0.05</cell><cell>99.91 ± 0.04</cell></row><row><cell>FDSSC</cell><cell>99.96 ± 0.06</cell><cell>99.94 ± 0.10</cell><cell>99.95 ± 0.07</cell><cell>99.75 ± 0.13</cell><cell>99.67 ± 0.15</cell><cell>99.72 ± 0.15</cell><cell>99.97 ± 0.02</cell><cell>99.96 ± 0.03</cell><cell>99.96 ± 0.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Training and testing time of different methods for three datasets.</figDesc><table><row><cell>Dataset</cell><cell>Time</cell><cell>SAE-LR</cell><cell>SSRN</cell><cell>CNN</cell><cell>FDSSC</cell></row><row><cell>KSC</cell><cell>Training Testing</cell><cell>61.7 m 0.12 s</cell><cell>329.3 s 1.79 s</cell><cell>313.6 s 1.83 s</cell><cell>201.2 s 2.57 s</cell></row><row><cell>IN</cell><cell>Training Testing</cell><cell>142.2 m 0.05 s</cell><cell>720.5 s 4.14 s</cell><cell>441.2 s 2.19 s</cell><cell>423.7 s 5.56 s</cell></row><row><cell>UP</cell><cell>Training Testing</cell><cell>115.2 m 0.08 s</cell><cell>1189.8 s 12.56 s</cell><cell>1799.2 s 2.22 s</cell><cell>498.2 s 15.51 s</cell></row><row><cell cols="2">Training epochs</cell><cell>400,000</cell><cell>200</cell><cell>120</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments:</head><p>The authors are grateful to the editor and reviewers for their constructive comments, which have significantly improved this work.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Funding: This study was funded by the Laboratory of Green Platemaking and Standardization for Flexography Printing (grant no. ZBKT201710).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials:</head><p>The Kennedy Space Center, Indiana Pines, and University of Pavia datasets are available online at http://www.ehu.eus/ccwintco/index.php?title=Hyperspectral_Remote_Sensing_Scenes. They are also available online at http://www.mdpi.com/2072-4292/10/7/1068/s1.</p><p>Author Contributions: All the authors made significant contributions to this work. W.W. and S.D. conceived and designed the experiments; S.D. performed the experiments; W.W. and Z.J. analyzed the data; and L.S. contributed analysis tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflicts of interest. The founding sponsors had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; nor in the decision to publish the results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">High Performance Computing for Hyperspectral Remote Sensing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>King</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2010.2095495</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="528" to="544" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Hyperspectral Method for Remotely Sensing the Grain Size of Snow</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Nolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dozier</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0034-4257(00)00111-5</idno>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="207" to="216" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identification of Coral Reef Feature Using Hyperspectral Remote Sensing</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panditrao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Mahendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SPIE-The International Society for Optical Engineering</title>
		<meeting>the SPIE-The International Society for Optical Engineering<address><addrLine>New Delhi, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-04">April 2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classification of Hyperspectral Remote Sensing Images with Support Vector Machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2004.831865</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1778" to="1790" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Local Binary Patterns and Extreme Learning Machine for Hyperspectral Imagery Classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2014.2381602</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="3681" to="3693" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Hyperspectral Image Classification Framework and Its Application</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2014.12.025</idno>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">299</biblScope>
			<biblScope unit="page" from="379" to="393" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Learning for Remote Sensing Data: A Technical Tutorial on the State of the Art</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1109/MGRS.2016.2540798</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Learning-Based Classification of Hyperspectral Data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTARS.2014.2329330</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Supervised Learning for Hyperspectral Data Classification through Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Makantasis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Doulamis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Geoscience and Remote Sensing Symposium</title>
		<meeting>the 2015 IEEE International Geoscience and Remote Sensing Symposium<address><addrLine>Milan, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="4959" to="4962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spectral-Spatial Feature Extraction for Hyperspectral Image Classification: A Dimension Reduction and Deep Learning Approach</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2016.2543748</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="4544" to="4554" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Feature Extraction and Classification of Hyperspectral Images Based on Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2016.2584107</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="6232" to="6251" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Spectral-Spatial Classification of Hyperspectral Imagery with 3D Convolutional Neural Network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs9010067</idno>
		<imprint/>
	</monogr>
	<note>Remote Sens. 2017, 9, 67. [CrossRef</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spectral-Spatial Residual Network for Hyperspectral Image Classification: A 3-D Deep Learning Framework</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chapman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TGRS.2017.2755542</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Learning with Grouped Features for Spatial Spectral Classification of Hyperspectral Images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/LGRS.2016.2630045</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="97" to="101" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semisupervised Classification for Hyperspectral Image Based on Multi-Decision Labeling and Deep Feature Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2016.09.001</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="99" to="107" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Convolutional Neural Networks for Building Extraction from Orthoimages and Dense Image Matching Point Clouds</title>
		<author>
			<persName><forename type="first">E</forename><surname>Maltezos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ioannidis</surname></persName>
		</author>
		<idno type="DOI">10.1117/1.JRS.11.042620</idno>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Conference on Pattern Recognition and Computer Vision (CVPR)</title>
		<meeting>the 2017 IEEE Conference on Pattern Recognition and Computer Vision (CVPR)<address><addrLine>College Park, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">2015. June 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on Imagenet Classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting>the 15th IEEE International Conference on Computer Vision, ICCV 2015<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-18">18 December 2015</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Neural Information Processing Systems</title>
		<meeting>the 26th Annual Conference on Neural Information Processing Systems<address><addrLine>Lake Tahoe, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-12">2012. December 2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lile, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07">July 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lecture 6.5-Rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tijmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">COURSERA Neural Netw. Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations 2015</title>
		<meeting>the International Conference on Learning Representations 2015<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the Importance of Initialization and Momentum in Deep Learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning, ICML 2013</title>
		<meeting>the 30th International Conference on Machine Learning, ICML 2013<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">June 2013</date>
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding the Difficulty of Training Deep Feedforward Neural Networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 13th International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05">2010. May 2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Belopolsky</surname></persName>
		</author>
		<title level="m">Theano: A Python Framework for Fast Computation of Mathematical Expressions</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Keras-Team. GitHub Repository</title>
		<author>
			<persName><forename type="first">C</forename><surname>François</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015-06">2015. June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<title level="m">Large-Scale Machine Learning on Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
