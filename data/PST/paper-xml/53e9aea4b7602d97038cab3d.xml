<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fusion of Multichannel Local and Global Structural Cues for Photo Aesthetics Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luming</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yue</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE, Qi Tian, Senior Member, IEEE</roleName><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
							<email>rogerz@comp.nus.edu.sg</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
							<email>xuelong_li@opt.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><surname>Zimmermann</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<postCode>119613</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100086</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Texas at San Antonio</orgName>
								<address>
									<addrLine>San Antonio</addrLine>
									<postCode>78249</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Center for OPTical IMagery Analysis and Learning</orgName>
								<orgName type="department" key="dep2">Xi&apos;an Institute of Optics and Precision Mechanics</orgName>
								<orgName type="laboratory">State Key Laboratory of Transient Optics and Photonics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>710119</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fusion of Multichannel Local and Global Structural Cues for Photo Aesthetics Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5F630A9ADA5DFC1E7186661AC09F3764</idno>
					<idno type="DOI">10.1109/TIP.2014.2303650</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-channel</term>
					<term>structural cues</term>
					<term>aesthetic evaluation</term>
					<term>probabilistic model in image retrieval [32]</term>
					<term>[34]</term>
					<term>graphic design [29]</term>
					<term>[30]</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Photo aesthetic quality evaluation is a fundamental yet under addressed task in computer vision and image processing fields. Conventional approaches are frustrated by the following two drawbacks. First, both the local and global spatial arrangements of image regions play an important role in photo aesthetics. However, existing rules, e.g., visual balance, heuristically define which spatial distribution among the salient regions of a photo is aesthetically pleasing. Second, it is difficult to adjust visual cues from multiple channels automatically in photo aesthetics assessment. To solve these problems, we propose a new photo aesthetics evaluation framework, focusing on learning the image descriptors that characterize local and global structural aesthetics from multiple visual channels. In particular, to describe the spatial structure of the image local regions, we construct graphlets small-sized connected graphs by connecting spatially adjacent atomic regions. Since spatially adjacent graphlets distribute closely in their feature space, we project them onto a manifold and subsequently propose an embedding algorithm. The embedding algorithm encodes the photo global spatial layout into graphlets. Simultaneously, the importance of graphlets from multiple visual channels are dynamically adjusted. Finally, these post-embedding graphlets are integrated for photo aesthetics evaluation using a probabilistic model. Experimental results show that: 1) the visualized graphlets explicitly capture the aesthetically arranged atomic regions; 2) the proposed approach generalizes and improves four prominent aesthetic rules; and 3) our approach significantly outperforms state-of-the-art algorithms in photo aesthetics prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fig. <ref type="figure">1</ref>. An example of the local aesthetics extracted from a photo. and etc. For example, a successful photo management system should rank photos based on the human perception of photo aesthetics, so that users can conveniently select their favorite pictures into albums. Moreover, an effective photo aesthetics evaluation algorithm can help photographers to crop an aesthetically pleasing sub-region from an original poorly framed photo. However, photo aesthetics evaluation is still a challenging task due to the following two problems.</p><p>• Both the spatial layout of locally and globally distributed regions in a scene play important roles in determining photo aesthetics. As seen from Fig. <ref type="figure">1</ref>, the spatial interaction of the four linearly arranged sailboats captures the regional aesthetics; while the relative displacement of the sailboats, the water, and the sky reflects the global aesthetics. Existing rules can only heuristically define what spatial distribution among the salient image regions is aesthetically pleasing. For example, rule of the thirds <ref type="bibr" target="#b5">[6]</ref> favors salient regions locating near the evenly 3 × 3 intersections of a photo. Although these aesthetic rules are convenient to use, they cannot reflect the specific spatial structure in photo aesthetics assessment, e.g., the linearity and the triangularity among salient regions. • Multi-channel visual cues collaboratively describe photo local aesthetics. That is to say, only visually salient regions with a particular color and texture distribution can arouse viewers' aesthetic perception. Yet it is difficult to determine the importance of each visual cue.</p><p>1057-7149 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. To address the aforementioned problems, we present a new photo aesthetics evaluation framework. To represent the structure of locally distributed regions of a photo (as shown in Fig. <ref type="figure" target="#fig_0">2</ref>), we extract graphlets that can effectively capture the interaction of spatially neighboring atomic regions. Because both atomic regions and their spatial arrangements are essential for describing photo local structures, we represent each graphlet by a matrix that can encode both the properties. Based on the matrix representation, graphlets can be deemed as points on the Grassmann manifold <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref>. To preserve the global spatial layout, we propose a manifold embedding algorithm to preserve all the distances between pairwise graphlets of each photo. At the same time, visual cues from multiple channels are dynamically tuned. More specifically, the weights of the three channel visual features (i.e., color, texture, and visual saliency) are adjusted by optimizing the objective function as formulated in <ref type="bibr" target="#b7">(8)</ref>. After the embedding, graphlets are transformed into equal-lengthed feature vectors. Then, we integrate them into a probabilistic model for evaluating the aesthetic quality of a test photo. The probabilistic model quantifies the amount of aesthetic features (post-embedding graphlets) that are shared between the training photos (aesthetically pleasing) and the test one.</p><p>The rest of the paper is organized as follows: Section II briefly reviews the previous photo quality models. From Section III to Section V, we exploit the local and global structures from multiple channels to represent photo aesthetics, and a probabilistic photo aesthetic model is developed correspondingly. Experimental results in Section VI thoroughly demonstrate the effectiveness of our proposed model and Section VII concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Recently many photo aesthetics evaluation methods have been proposed. Roughly, these methods can be divided into two groups: global feature-based approaches and local patch integration-based approaches.</p><p>Global feature-based approaches <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> design global low-level and high-level visual features to represent photo aesthetics in an implicit manner. Ke et al. <ref type="bibr" target="#b10">[11]</ref> designed a group of high-level visual features, such as the image simplicity based on the spatial distribution of edges, to imitate human perception of photo aesthetic quality. Datta et al. <ref type="bibr" target="#b2">[3]</ref> proposed 58 low-level visual features, e.g., shape convexity, to capture photo aesthetics. Dhar et al. <ref type="bibr" target="#b38">[39]</ref> proposed a set of high-level attribute-based predictors to evaluate photo aesthetics. In <ref type="bibr" target="#b39">[40]</ref>, Luo et al. proposed using the GMM (Gaussian mixture model)-based hue distribution and the prominent lines extraction-based texture distribution to represent the global composition of a photo. To describe local composition of a photo, three regional features respectively describing human faces, region clarity, and region complexity were developed. In <ref type="bibr" target="#b16">[17]</ref>, Marchesotti et al. proposed using generic descriptors, i.e., the bag of visual words and the Fisher vector, to access photo aesthetics. Experimental results demonstrated that the two generic descriptors outperform many specifically designed photo aesthetic descriptors. Ji et al. <ref type="bibr" target="#b30">[31]</ref> proposed a multi-channel coding based approach for mobile location recognition, in which different channel cues, which can largely ensure the search robustness to achieve the state-of-the-art search accuracy. For the aforementioned global aesthetic features, there is no strong indication that they can effectively capture photo aesthetics, such as the linearly arranged sailors in Fig. <ref type="figure">1</ref>. This implies that they may perform unsatisfactorily on some photos. In particular, it is worth noting the limitations/shortcomings of the above global feature-based approaches: 1) Luo et al. <ref type="bibr" target="#b39">[40]</ref>'s approach adopts a category-dependent regional feature extraction, which has the prerequisite that photos can be 100% accurately classified into one of the seven categories. This prerequisite, however, is infeasible in real applications. 2) The attributes proposed in Dhar et al. <ref type="bibr" target="#b38">[39]</ref>'s approach are designed manually and are data set dependent, and thus prove difficult to generalize to different data sets. Third, all these global low-level and highlevel visual features are designed heuristically. They model the statistics of visual descriptors within the whole image. There is no strong indication that they can accurately capture the photo local and global compositions. For example, the co-occurrence of the four sailboats and their linear spatial arrangements as shown in Fig. <ref type="figure">1</ref>.</p><p>Local patch integration-based approaches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b36">[37]</ref> extract local patches within a photo and then integrate them to measure photo aesthetic quality. In <ref type="bibr" target="#b1">[2]</ref>, Cheng et al. proposed the omni-range context, i.e., the spatial distribution of arbitrary pairwise image patches, to model photo composition. The learned omni-range context priors are combined with the other cues, such as the patch number, to form a posterior probability to measure the aesthetics of a photo. One limitation of Cheng et al.'s work is that only the binary correlation between image patches is considered. To describe high-order spatial interactions of image patches, Zhang et al. <ref type="bibr" target="#b28">[29]</ref> introduced graphlets. And a probabilistic model is proposed to quantify photo aesthetics as the amount of graphlets that can be transferred them from the training photos into the cropped one. However, graphlets cannot reflect photo global spatial configurations, which are essential cues for determining photo aesthetics. Besides, the color and texture channel visual features are assigned with the same weight in the graphlet transferring phase, which is not consistent with human perception of photo aesthetics. In <ref type="bibr" target="#b36">[37]</ref>, Nishiyama et al. first detected multiple subject regions in a photo, where each subject region is a bounding rectangle containing the salient part of an object. Then, an SVM classifier is trained for each subject region. Finally, the aesthetics of each candidate cropped photo is computed by combining the scores of the SVM classifier corresponding to a photo's internal subject regions. One limitation of Nishiyama et al.'s approach is that it cannot model the spatial interaction of multiple image regions explicitly. Thus, this method cannot discriminate linearly or triangularly distributed sailboat as shown in Fig. <ref type="figure">1</ref>. In <ref type="bibr" target="#b20">[21]</ref>, Nishiyama et al. proposed a color harmony-based photo aesthetic evaluation method. A color harmony model is first applied to the patches within a photo to describe their color distribution. The patch-level color distribution is then integrated into a bag-of-patches histogram. The histogram is further classified by an SVM to identify whether a photo is highly or low aesthetic. Note that, Nishiyama et al. <ref type="bibr" target="#b20">[21]</ref> evaluates photo aesthetics by utilizing visual features in color channel only. Features capturing aesthetics in other channels, such as texture, are neglected. Bhattacharya et al. <ref type="bibr" target="#b0">[1]</ref> proposed the spatial recomposition to allow users interactively select a foreground object. The system then presents recommendations to indicate an optimal location of the foreground object, which is detected by combining multiple aesthetic cues, such as the relative foreground position and the visual weight ratio. The major shortcoming of Bhattacharya et al.'s method is the necessity of human interaction, limiting its application to large scale photo aesthetics evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GRAPHLET-BASED LOCAL STRUCTURE DESCRIPTOR</head><p>There are usually tens to hundreds of components within a photo, such as the sailboats and sailors in Fig. <ref type="figure">1</ref>. Among these components, a few spatially neighboring ones and their interactions capture photo local aesthetics. Since graph is a powerful tool to describe the relationships between objects, we use graph to model the spatial interactions between image components. Our technique is to segment a photo into a set of atomic regions using unsupervised fuzzy clustering (UFC) <ref type="bibr" target="#b25">[26]</ref>, where each atomic region denotes the segmented image patch. Based on this, we extract graphlets to characterize the local aesthetics of a photo. Graphlet is a small-sized connected graph defined as:</p><formula xml:id="formula_0">G = (V, E), (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where V is a set of vertices representing locally distributed atomic regions (as the example in Fig. <ref type="figure" target="#fig_0">2</ref>); and E is a set of edges, each of which connects pairwise spatially adjacent atomic regions. <ref type="foot" target="#foot_0">1</ref> We call a graphlet with t vertices a t-sized graphlet. Because the number of graphlets within a photo exponentially increases with graphlet size, 2 only small-sized graphlets are employed. As a purely data-driven segmentation algorithm, UFC produces numerous imperfectly segmented regions. 3 To maximally preserve optimally segmented regions, we generate a large number of atomic regions and then remove those are imperfect. Five times segmentation under UFC tolerance bounds {0.1, 0.2, 0.3, 0.4, 0.5} is applied firstly. Then tiny-sized regions (less than 50 pixels) are removed. Finally, segmented regions with low correlation to photo categories (corr (R) &lt; 1/13) are abandoned since they reflect little semantics. The category correlation of segmented region is calculated by an LDA <ref type="bibr" target="#b13">[14]</ref>-like measure. It shows that a higher discriminative segmented region is more correlated with the photo semantics:</p><formula xml:id="formula_2">corr (R) = c(R i )=c(R) ||F(R i ) -F(R)|| l 2 c(R i ) =c(R) ||F(R i ) -F(R)|| l 2 , (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where F(R) is a vector that combines the HOG <ref type="bibr" target="#b4">[5]</ref> (128-dimensional) and the color moment <ref type="bibr" target="#b21">[22]</ref> (9-dimensional) from segmented region R. c(R i ) indicates the category of photo from which segmented region R i is extracted.</p><p>It is computed from the 13-class SVM trained from Feifei et al. <ref type="bibr" target="#b14">[15]</ref>'s scene data set.</p><p>The graphlet extraction can be illustrated as a probabilistic walking process. As shown in Fig. <ref type="figure" target="#fig_0">2</ref>, we first choose a starting vertex with a probability of p( A) A , where p(A) is the probability of A atomic regions existing in photo I . The spatially adjacent atomic regions are then visited one-by-one. The probability of visiting a spatially adjacent vertex is decided by the degree (e.g., the superpixel marked as red in Fig. <ref type="figure" target="#fig_1">3</ref> is with degree 6.) of the current vertex, i.e.,</p><formula xml:id="formula_4">1 d p d (R l )d(R l )</formula><p>, where d(R l ) denotes the degree of the current atomic region R l and p d (R l ) is the corresponding probability. The visiting process stops when the maximum graphlet size is reached. Based on the above description, the probability of extracting a t-sized graphlet G from photo I is calculated by:</p><formula xml:id="formula_5">p(G|I ) ∝ p(A) A t -1 l 1 d p d (R l )d(R l ) , (<label>3</label></formula><formula xml:id="formula_6">)</formula><p>where p(A)/A denotes the probability of choosing a starting vertex. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, the probability of A vertices existing in a photo is p(A), and the probability of selecting a vertex from these A atomic regions is 1/A. Thus, the probability of choosing a starting vertex in a photo is p(A)/A.</p><formula xml:id="formula_7">1 d p d (R l )d(R l )</formula><p>reflects the probability of choosing a vertex in the l-th step of random walking. It is noticeable that d p d (R l )d(R l ) denotes the expectation degree of superpixel R l . Due to the number of graphlets is exponentially increasing with graphlet size, it is computationally intractable to adopt all the t-sized graphlets into the proposed aesthetic model. Therefore, we sample 500 graphlets from each photo.</p><p>Because visual features from multiple visual channels collaboratively contribute to photo aesthetics, a 9-dimensional color moment <ref type="bibr" target="#b21">[22]</ref>, a 128-dimensional histogram of gradient (HOG) <ref type="bibr" target="#b4">[5]</ref>, and a 64-dimensional quantized visual saliency histogram, are used to describe each atomic region. Visual features from the three channels are used here because color and texture are generally complementary in describing the appearance of an atomic region, and the saliency channel indicates which atomic region is visually attractive. In this work, the visual saliency descriptor is implemented as the graph-based visual saliency (GBVS) <ref type="bibr" target="#b8">[9]</ref>. We choose GBVS because: 1) compared with high-level saliency models that are manually designed and are data dependent, GBVS relies completely on the low-level visual features, making it more adaptable to real-world applications; 2) GBVS is among the top performers of the purely low-level visual feature-based saliency models. It is worth emphasizing that, for each atomic region, GBVS only outputs its pixel-level saliency map. In our approach, a K-means-based quantization is adopted for fixedlength vector representation.</p><p>The above three visual features result in three matrices M C R ,M T R , and M S R , describing the atomic regions of a graphlet in color, texture, and visual saliency channels respectively. Given a t-sized graphlet, each row of matrix M C R ∈ R t ×9 represents a 9-dimensional color moment of an atomic region (M T R and M S R are defined similarly). To capture the spatial interactions of atomic regions, we adopt a t ×t-sized adjacency matrix as:</p><formula xml:id="formula_8">M S (i, j ) = θ(i, j ) if R i and R j are spatially adjacent 0 o t h e r w i s e , (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>where θ(i, j ) is the horizontal angle of the vector from the center of atomic region R i to that of atomic region R j . Based on</p><formula xml:id="formula_10">{M C R , M T R , M S R } and M S , three matrices M C = [M C R , M S ], M T = [M T R , M S ],</formula><p>and M S = [M S R , M S ] are constructed, which describe a graphlet in color, texture, and visual saliency channels respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PURSUING GLOBAL SPATIAL LAYOUT ON MANIFOLD</head><p>The above matrix-form graphlets are descriptive, but they are still not ready for evaluating photo aesthetic quality. First, although different-sized graphlets have comparable aesthetic properties, e.g., four and five linearly arranged skaters are aesthetically similar, their distance cannot be directly calculated as their corresponding matrices are with different sizes. Second, global composition plays an important role in photo aesthetics. However, as the number of graphlets is exponentially increasing with their size, only small-sized graphlets are employed. In this case, the small graphlet size limits the descriptive ability of graphlets to photo global spatial layout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Manifold Embedding to Preserve Global Layout</head><p>It can be observed that, spatially neighboring graphlets in a photo are partially overlapping. This indicates that it is beneficial to exploit the local structure <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b22">[23]</ref> among graphlets. Therefore, we project the matrix-form graphlets onto a manifold, thereby the Golub-Werman distance <ref type="bibr" target="#b23">[24]</ref> between identical-sized matrices is:</p><formula xml:id="formula_11">d GW (M, M ) = ||M O -M O || 2 , (<label>5</label></formula><formula xml:id="formula_12">)</formula><p>where M O and M O denote the orthonormal basis of M and M respectively; and || • || F denotes the Frobenius norm.</p><p>Inspired by the patch alignment framework <ref type="bibr" target="#b27">[28]</ref>, we propose a graphlet embedding algorithm to 1) transform different-sized graphlets from multiple visual channels into equal-lengthed vectors, and 2) encode the global spatial layout of each photo into its constituent graphlets (i.e. graphlets extracted from a photo). The graphlet embedding algorithm contains two parts. As shown on the right of ( <ref type="formula" target="#formula_11">5</ref>), the first part incorporates the global spatial layout of each photo. That is, it minimizes the discrepancy between the distances between graphlets on the manifold and those in the Euclidean space. For graphlets in color/texture/saliency channel, the objective function is:</p><formula xml:id="formula_13">arg min Y h i j [d GW (M h i , M h j ) -d E (y h i , y h j )] 2 , (<label>6</label></formula><formula xml:id="formula_14">)</formula><p>where M h i and M h j are t × (F + t)-sized matrices (F denotes the feature dimension in color/texture/saliency channel) to the i -th and the j -th identical-sized graphlets, from the h-th photo; y h i and y h j are their d-dimensional vectors; d GW (•, •) and d E (•, •) are the Golub-Werman distance <ref type="bibr" target="#b23">[24]</ref> and Euclidean distance between identical-sized matrices respectively. ( <ref type="formula" target="#formula_13">6</ref>) is an objective function that minimizes the Golub-Werman distance between graphlets and the Euclidean distance between postembedding graphlets. The Golub-Werman distance between graphlets is d GW (M h i , M h j ). The Euclidean distance between graphlets is d E (y h i , y h j ). The global spatial layout of a photo can be considered as the relative position of all pairwise graphlets in a photo. If we preserve all these relative distances in the graphlet embedding, the global spatial layout can be preserved.</p><p>Based on the derivation in the Appendix, the above objective function can be reorganized as: <ref type="bibr" target="#b6">(7)</ref> where</p><formula xml:id="formula_15">arg min Y h i j [d GW (M h i , M h j ) -d E (y h i , y h j )] 2 = arg max Y h tr(Y h Z h (Y h ) T ),</formula><formula xml:id="formula_16">Y h = [y h 1 , y h 2 , . . . , y h N ] ∈ R d×N h denotes the matrix containing all the post-embedding graphlets from the h-th photo, Z h = -R N h S h GW R N h /2</formula><p>. By summing the graphlet embedding from all the photos in color, texture, and saliency channel, the second part embedding is given as:</p><formula xml:id="formula_17">arg max Y,α H h=1 3 k=1 α r k tr(YS h Z h k (S h ) T Y T ) = arg max Y,α<label>3</label></formula><formula xml:id="formula_18">k=1 α r k tr(YZ k Y T ) s.t. YY T = I d , k α k = 1,<label>(8)</label></formula><p>where Y = [y 1 , y 2 , . . . , y N ] ∈ R d×N is a matrix containing all the post-embedding graphlets; the constraint YY T = I d uniquely determines the embedding Y; and r &gt; 1 determines the complementary property of the multiple visual channels, as detailed in the appendix.</p><formula xml:id="formula_19">Z k = H h=1 S h Z h k (S h ) T ;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. A PROBABILISTIC AESTHETICS MEASURE</head><p>The post-embedding graphlets capture both the local and global spatial layouts from multiple visual channels in a photo. To effectively leverage them for photo aesthetics evaluation, a probabilistic model is proposed.</p><p>Given a set of training photos {I 1 , . . . , I H } and a test one I * , they are highly correlated through their respective graphlets G and G * . Thus, a probabilistic graphical model is utilized to describe this correlation. As shown in Fig. <ref type="figure" target="#fig_2">4</ref>, the graphical model contains two types of nodes: observable nodes (blue rectangle) and hidden nodes (orange rectangle). The probabilistic graphical model can be divided into four layers. The first layer represents all training photos, the second layer denotes the post-embedding graphlets from the training photos, the third layer represents all the post-embedding graphlets from the test photo, and the last layer denotes the test photo. The correlation between the first and second layers is p(G|I 1 , . . . , I H ), the correlation between the second and third layers is p(G * |G), and the correlation between the third and fourth layers is p(I * |G * ). 4  According to the formulation above, photo aesthetics can be quantified as the similarity between post-embedding graphlets from the test photo and those from the training aesthetically pleasing photos. This similarity is interpreted as the amount of graphlets that can be transferred from the training photos into the test one. That is, the aesthetic quality γ (I * ) of a test photo I * is measured as:</p><formula xml:id="formula_20">γ (I * ) = p(I * |I 1 , . . . , I H ) = p(I * |G * ) * p(G * |G) * p(G|I 1 , . . . I H ),<label>(9)</label></formula><p>The probabilities p(I * |G * ), p(G * |G), and p(G|I 1 , I 2 , . . . , I N ) in ( <ref type="formula" target="#formula_21">10</ref>) are computed respectively as:</p><formula xml:id="formula_21">p(I * |G * ) = p(I * |G * 1 , . . . , G * T ) = p(G * 1 , . . . , G * T |I * ) p(I * ) p(G * 1 , . . . , G * T ) = T t =1 N t * j =1 p(G * t ( j )|I * ), (<label>10</label></formula><formula xml:id="formula_22">)</formula><formula xml:id="formula_23">p(G * |G) = p(G * 1 , . . . , G * T |G 1 , . . . , G T ) = T t =1 N t j =1 p(G * t ( j )|G 1 , . . . , G T ), (<label>11</label></formula><formula xml:id="formula_24">)</formula><p>4 To reduce time consumption, our probabilistic model allows for employing a small proportion of training and test graphlets, where p(G|I 1 , . . . , I H ) and p(I * |G * ) are defined in ( <ref type="formula">24</ref>) and ( <ref type="formula">22</ref>) respectively. If all the graphlets are used, then p(G|I 1 , . . . , I H ) = 1 and p(I * |G * ) = 1.</p><p>Algorithm 1 Probabilistic Photo Aesthetic Quality Evaluation</p><formula xml:id="formula_25">p(G|I 1 , . . , I H ) 1 , . . . , G T |I 1 , . . . , I H ) = T t =1 N t j =1</formula><p>p(G t ( j )|I 1 , . . . , I H ), <ref type="bibr" target="#b11">(12)</ref> where </p><formula xml:id="formula_26">G</formula><formula xml:id="formula_27">(3). Second, p(G * t ( j )|G 1 , . . . , G T ) is the probability of graphlet G * t ( j ) existing in G 1 , . . . , G T .</formula><p>Inspired by many previous works such as <ref type="bibr" target="#b24">[25]</ref>, this probability can be defined as a Gaussian kernel:</p><formula xml:id="formula_28">p(G * t ( j )|G 1 , . . . , G T ) = exp - G∈G 1 ,...,G T ||G * t ( j ) -G|| |G 1 , . . . , G T | ,<label>(13)</label></formula><p>Third, p(G t ( j )|I 1 , I 2 , . . . , I H ) is the probability of graphlet G t ( j ) coming from all the training photos {I 1 , I 2 , . . . , I H }, which is computed as follows:</p><formula xml:id="formula_29">p(G t ( j )|I 1 , . . . , I H ) = 1 - H h=1 1 -p(G t ( j )|I h ) , (<label>14</label></formula><formula xml:id="formula_30">)</formula><p>This equation is explained as follows:</p><formula xml:id="formula_31">1 -p(G i ( j )|I h ) is the probability of graphlet G i ( j ) not coming from photo I h . Straightforwardly, H h=1 (1 -p(G i ( j )|I h )) is the probabil- ity of G i ( j ) not coming from any of {I 1 , . . . , I H }. Thus, 1-H h=1 (1-p(G i ( j )|I h )) is the probability of G i ( j ) coming from {I 1 , . . . , I H }.</formula><p>By summarizing the discussion from Section III to Section V, the pipeline of our probabilistic graphlet-guided photo aesthetics evaluation is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS AND ANALYSIS</head><p>This section evaluates the effectiveness of the proposed method, which can be divided into four parts. The first part compares our approach with well-known photo aesthetics evaluation methods. The second part step-by-step evaluates each component of the proposed approach. In the third part, we discuss the influence of the two free parameters. Lastly, we illustrate the relationships between the proposed method and the four prominent aesthetic rules.</p><p>As far as we know, there are three off-the-shelf data sets for evaluating photo aesthetics: the CUHK <ref type="bibr" target="#b10">[11]</ref>, the Photo.net <ref type="bibr" target="#b2">[3]</ref>, and the AVA <ref type="bibr" target="#b18">[19]</ref>. A rough description of the three data sets are given as follows:</p><p>• The CUHK contains 12,000 photos collected from DPChallenge.com. These photos have been labeled by ten independent viewers. Each photo is classified as highly aesthetic if more than eight viewers agree on the assessment. For this data set, we use the standard split of training/test image sets.  <ref type="bibr" target="#b16">[17]</ref> and Nishiyama et al. <ref type="bibr" target="#b36">[37]</ref>, both the highly-and low-aesthetic training photos are adopted to learn the model. Particularly, the highly-aesthetic photos function as the positive samples while the low-aesthetic ones as the negative samples. For those models that are based on transferring aesthetic features, such as Cheng et al. <ref type="bibr" target="#b1">[2]</ref>'s model, they employ those "good" aesthetic features to evaluate a test photo. Thus, it is necessary to assign a weight for each graphlet that denotes its aesthetics, that is, a larger weight reflects a higher aesthetic level. And the weight is determined by the aesthetics of the photo from which the graphlet is extracted. For the three data sets, different experimental settings are used to assign the weight of each photo. For the CUHK, we use the probabilistic output from Yan et al.'s work <ref type="bibr" target="#b10">[11]</ref> to rank the aesthetics of each photo. For the PNE, we manually selected 674 highly-aesthetic photos and leave the rest as the low-aesthetic ones. Then, we extracted the aesthetic features based on <ref type="bibr" target="#b10">[11]</ref>, and further used a probabilistic SVM output to score the aesthetics of each photo. For the AVA, each training photo is rated according to their aesthetics on a scale of {0.1, 0.2, . . . , 1}. We average the rating scores as the aesthetics of each photo. The aesthetics of these additionally crawled photos are manually labeled by 23 students from the department of computer science at Zhejiang University. Most of them are experienced with photography.</p><p>All the experiments were carried out on a personal computer with an Intel E8500 processor and 4GB RAM. The algorithm was implemented on the Matlab 2011 platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison With the Existing Aesthetic Evaluation Models</head><p>In this subsection, we compare our approach with five photo aesthetics evaluation methods: 1) three global features-based  <ref type="bibr" target="#b39">[40]</ref>, and Marchesotti et al. <ref type="bibr" target="#b16">[17]</ref>; and 2) two local patch integration-based methods proposed by Cheng et al. <ref type="bibr" target="#b1">[2]</ref> and Nishiyama et al. <ref type="bibr" target="#b36">[37]</ref> respectively.</p><p>In the comparative study, we notice that the source codes of the above five compared methods are not provided and some experimental details are not mentioned, therefore it is difficult to strictly implement them. Toward a convincing comparative study, in our implementation, we tend to strengthen some components of the compared methods. Based on this, we adopt the following implementation settings: For Dhar's approach, we use the public code from Li et al. <ref type="bibr" target="#b11">[12]</ref> to extract the attributes from each photo. These attributes are combined with the low-level features proposed by Yeh et al. <ref type="bibr" target="#b37">[38]</ref> to train the aesthetics classifier. For Luo et al.'s approach, not only the low-level and high-level features in their publication are implemented, but also the six global features from Getlter et al. <ref type="bibr" target="#b7">[8]</ref> are used to strengthen the aesthetic prediction ability. For Marchesotti et al.'s approach, similar to the implementation of Luo et al.'s method, the six additional features are also adopted. For Cheng et al.'s approach, we implemented it as a simplified version of our approach, i.e., only 2-sized graphlets are employed for aesthetics measure. Noticeably, for the three probabilistic model-based aesthetic evaluation methods respectively proposed by Cheng et al., Nishiyama et al., and us, given a test photo, if the aesthetics probability calculated by ( <ref type="formula" target="#formula_20">9</ref>) is larger than 0.5, then this photo is deemed as highly aesthetic, and vice versa. We choose 0.5 as the threshold because for each of the three data sets, half of the photos are highly aesthetic.</p><p>We present the aesthetics prediction accuracy on the CUHK, the PNE, and the AVA in Table <ref type="table" target="#tab_2">I</ref>. On the three data sets, our approach outperforms Marchesotti et al.'s approach by nearly 2%, and exceeds the rest of the compared methods by more than 5%, which demonstrates the effectiveness our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discriminative Ability Evaluation</head><p>Each image can be represented by a set of graphlets. The extracted graphlets are planar visual features in R 2 . Unfortunately, conventional classifier, such as SVM, can only handle 1-D vector form features. Moreover, both the number and the size of the extracted graphlets are different from one image to another. Thus, it would be impractical for a conventional classifier such as SVM to carry out classification directly based on the extracted graphlets. To tackle this problem, a quantization scheme is developed to transform the extracted graphlets into 1-D vectors. Particularly, the quantization is inspired by graph kernel <ref type="bibr" target="#b9">[10]</ref>, where each element of the vector A = [a 1 , a 2 , . . . , a N ] is calculated as:</p><formula xml:id="formula_32">a i ∝ exp - 1 N • N i y∈I,y ∈I i d(y, y ) , (<label>15</label></formula><formula xml:id="formula_33">)</formula><p>where N and N i respectively denote the number of graphlets in photo I and I i ; y and y i are the post-embedding graphlets from photo I and I i respectively. On the basis of the feature vector obtained above, a multiclass SVM is trained. That is, for the training images from the p-th and the q-th classes, we construct the following binary SVM classifier:</p><formula xml:id="formula_34">max α∈R N pq W (α) = N pq i=1 α i - 1 2 N pq i=1 α i α j l i l j k(A i , A j ) s.t. 0 ≤ α i ≤ C, N pq i=1 α i l i = 0,<label>(16)</label></formula><p>where A i ∈ R N is the quantized feature vector from the i -th training image; N is the number of training images; l i is the class label (+1 for the p-th class and -1 for the q-th class) to the i -th training image; α determines the hyper-plane to separate images in the p-th class from those in the q-th class; C &gt; 0 trades the complexity of the machine off the number of nonseperable images; and N pq is the number of training images from both the p-th and the q-th classes. Given a quantized feature vector A ∈ R N obtained from a test image, its label ( p or q) is classified by: sgn(</p><formula xml:id="formula_35">N pq i=1 l i α i k(A i , A) + b), (<label>17</label></formula><formula xml:id="formula_36">)</formula><p>where the bias b = 1-N pq i=1 l i α i k(A i , A s ) and A s is a support vector with class label +1. During testing, classification is conducted C(C -1)/2 times and the voting rule is utilized to get the final decision. Each binary classification can be deemed to be a voting process wherein votes can be cast for A, and A is assigned to a class with the maximum number of votes.</p><p>Based on the above kernel SVM, a multi-class SVM is trained for image categorization. We experiment on the PASCAL VOC 2009 <ref type="bibr" target="#b17">[18]</ref>, and the training/validation/test splits are set as defaults. We compare the proposed kernel with FV-Color-SP in Marchesotti et al's work, FV-SIFT-SP as illustrated in Chatfield et al.'s work, and SC-SIFT-SP proposed by Yang et al. <ref type="bibr" target="#b44">[45]</ref>. For our approach, both a single segmentation and multiple segmentations are adopted to decompose each image into numerous atomic regions. As shown in Table <ref type="table" target="#tab_3">II</ref>, our approach significantly outperforms Marchesotti et al's method, which is in line with the aesthetics prediction performance in Table <ref type="table" target="#tab_2">I</ref>. Besides, our approach is less effective than the SIFT pyramid. This is because there are a huge number of graphlets within an image, some of which contribute slightly or even negatively to the categorization performance. Toward a better categorization performance, a graphlet selection can be adopted in the future.</p><p>Lastly, we compare the categorization performance of our approach on the MIT 67 <ref type="bibr" target="#b35">[36]</ref> indoor scenes data set. In addition to the above compared methods, we incorporate a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Step-by-Step Model Justification</head><p>This experiment justifies the effectiveness of the three main components in our graphlet-based photo aesthetics model: graphlet-based local compositional descriptor extraction, multi-channel graphlet embedding, and probabilistic aesthetics measure.</p><p>• To evaluate the effectiveness of the first component, two experimental settings are adopted: 1) reducing graphlet to a single atomic region that captures no contextual cues (Graphlet→single atomic region); and 2) removing the adjacent matrix term from graphlets (Remove adj. mat from graphlet), which abandons the spatial cues of graphlets. • To testify the effectiveness of the second component, two experimental settings are applied: 1) reducing the multi-channel graphlet embedding to single channel one (Mani.Grap. emb.→Single-ch. emb.), where only the color channel is used. Color channel because as shown by many photo aesthetics methods <ref type="bibr" target="#b20">[21]</ref>, it is the most important channel for representing photo aesthetics; and 2) replacing the manifold graphlet embedding by kernel PCA (Mani.Grap. emb.→kernel PCA), where the kernel is computed as:</p><formula xml:id="formula_37">k(G, G ) ∝ exp(-d GW (M, M )), M = [M C R , M T R , M S R , M S ] and d GW (•, •)</formula><p>is the Golub-Werman <ref type="bibr" target="#b23">[24]</ref> distance between identical-sized matrices.</p><p>• To demonstrate the effectiveness of the third component, we replace the probabilistic aesthetics measure by a kernel SVM-based one (Prob. mea. → clasf. Mea.), wherein the kernel is computed based on (2). • Finally, to demonstrate the importance of the three visual cues: color, texture, and visual saliency, we report the aesthetic prediction accuracy by abandoning each of the three cues. As shown in Table <ref type="table" target="#tab_4">IV</ref>, when the color channel visual cue is removed, we observe the highest performance decrease of the aesthetic evaluation. This clearly confirms the importance of color in aesthetics prediction, which is consistent with the results reported in Marchesotti et al. <ref type="bibr" target="#b16">[17]</ref>'s work. As shown in Table <ref type="table" target="#tab_4">IV</ref>, when replacing one component of the proposed approach with an existing one, aesthetics prediction accuracy reduces dramatically. This implies that each component of the proposed approach is indispensable and inseparable. In addition, the performance decrement reflects the importance of each component. As can be seen, manifold graphlet embedding, the key contribution of the proposed approach, plays the most important role in the proposed aesthetics model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Parameter Analysis</head><p>This experiment evaluates the influence of the graphlet size T and the dimensionality of post-embedding graphlets d, on the performance of the proposed approach.</p><p>To analyze the effects of the maximum graphlet size T on evaluating photo aesthetics, we set up an experiment by varying T continuously. In Fig. <ref type="figure" target="#fig_3">5</ref>(a), we present the aesthetics prediction accuracy (APA) when the maximum size of graphlet is tuned from 1 to 10. As can be seen, prediction accuracy increases moderately when T ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref> but remains stable when T ∈ <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>. This observation implies that 6-sized graphlets are sufficient for capturing the local composition of images from the CUHK. Also in Fig. <ref type="figure" target="#fig_3">5</ref>(b), we present the performance of our model when the dimensionality of post-embedding graphlets is tuned from 12 to 120 with a step size of 12. As can be seen, the prediction accuracy increases steadily when d ∈ <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36]</ref> but remains stable when d ∈ <ref type="bibr" target="#b35">[36,</ref><ref type="bibr">120]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Photo Ranking Results</head><p>This subsection presents the photos of the three data sets that are ranked by our probabilistic photo aesthetics measure. As can be seen from Fig. <ref type="figure" target="#fig_4">6</ref>, we made the following three observations.</p><p>• As shown in the photos ranked between 0.8 and 1, highly aesthetic photos with multiple interacting objects are ranked with very high scores, demonstrating that the post-embedding graphlets effectively capture both local and global aesthetics of a photo. • As seen from the photos ranked between 0.5 and 0.8, highly aesthetic photos with a single object are also appreciated by the proposed aesthetic model. This is because graphlets are naturally local composition descriptors, and they influence the proposed photo aesthetics based on the proposed probabilistic model. • Objects from the photos ranked between 0 and 0.5 are either spatially disharmonically distributed or blurred. Thus, these photos are considered as aesthetically low by our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>By discovering both the local and the global spatial structure among image regions, this paper presents a probabilistic model for photo aesthetics evaluation. In particular, we first extract graphlets which represent photo local composition. Then, these graphlets are projected onto the Grassmann manifold, based on which a manifold embedding algorithm encodes global layout and multi-channel visual features into graphlets. Finally, these post-embedding graphlets are integrated to form a probabilistic measure for evaluating photo aesthetics. Experimental results demonstrate the proposed approach outperforms its competitors. The visualized cropping results confirm photo aesthetics are appropriately captured.</p><p>In the future, we plan to develop a more general and comprehensive photo aesthetics evaluation model that includes not only the spatial interaction of image regions, but also other important photography elements such as exposure, contrast, etc. In addition, we want to propose a weakly supervised learning paradigm to transfer the image-level semantics into graphlets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>A. Derivation from ( <ref type="formula" target="#formula_13">6</ref>) to <ref type="bibr" target="#b6">(7)</ref> Denote D h GW = [d GW (M h i , M h j )] as a matrix whose i j-th element is the Golub-Werman distance between the i -th and the j -th graphlets from the h-th photo. Then, the inner product matrix is obtained by Z h = -R N h S h GW R ( N h ) T , wherein (S h GW ) i j = (D h GW ) 2 i j , R N h = I N he N h S h GW e T N h /N is the centralization matrix.</p><p>Based on the above formulation, (6) can be reorganized into: arg min</p><formula xml:id="formula_38">Y h ||Z h -(Y h ) T Y h || 2 = arg min Y h tr(Z h (Z h ) T -2Y h Z h (Y h ) T + (Y h ) T Y h (Y h ) T Y h ). (<label>18</label></formula><formula xml:id="formula_39">)</formula><p>By assuming that (Y h ) T Y h is a constant matrix, (29) can be rewritten as:</p><formula xml:id="formula_40">arg max Y h tr(Y h Z h (Y h ) T ). (<label>19</label></formula><formula xml:id="formula_41">)</formula><p>B. Illustration of the Parameter r in <ref type="bibr" target="#b8">(9)</ref> If we ignore the parameter r (or set r = 1), then the solution to α in ( <ref type="formula" target="#formula_20">9</ref>) is α k = 1 when maximizing tr(Y h k Z h k (Y h k ) T ), or α k = 0 otherwise. That means only one channel visual features is finally selected when r = 1. Obviously, this solution does not meet our objective on exploring the complementary properties of visual features from multiple channels. We adopted the trick used in <ref type="bibr" target="#b40">[41]</ref> to avoid this phenomenon, i.e., we set α i ← α r i with r &gt; 1. In this way, k α r k = 1 achieves its maximum value when α i = 1/3. Straightforwardly, to maximize k α r k tr(YZ k Y T ) = 1, α i of different views will be obtained by setting r &gt; 1, which means that each view has a particular contribution to the final low-dimensional embedding Y. Also, we found that r determines the complementary property of different channels: rich complementation implies a larger r . In our experiment, we fix the value of r to 2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Graphlet extraction procedure (the yellow marked atomic regions are locally distributed because they are within the circle.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An illustration of the degree of a superpixel as well as the random walking process.</figDesc><graphic coords="3,353.75,58.37,166.82,120.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The probabilistic model for aesthetic evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Performance of photo aesthetics evaluation under different parameters. (a) Performance under different maximum graphlet sizes. (b) Performance under different dimensionalities of post-embedding graphlets d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Ranking results on the CUHK (top left), the PNE (top right), and the AVA (bottom left). Photos with the green rectangles indicate highly aesthetic test photos, and those with red rectangles are deficiently aesthetic test photos. The three pie charts denote the statistics of images from three data sets, according to the proposed model.</figDesc><graphic coords="9,323.39,179.69,148.58,108.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>t denotes all the training t-sized graphlets; G t ( j ) is the j -th training t-sized graphlets; G *</figDesc><table><row><cell>t denotes the t-sized test t ( j ) is the j -th test t-sized graphlet; N t is the graphlets; G  *  number of training t-sized graphlets and N t  *  the number of</cell></row><row><cell>test t-sized graphlets.</cell></row><row><cell>To calculate (11), (12) and (13), three probabilities</cell></row><row><cell>p(G  *</cell></row></table><note><p>t ( j )|I * ), p(G * t ( j )|G 1 , . . . , G T ) and p(G t ( j )|I 1 , . . . , I H ) are required. First, p(G * t ( j )|I * ) is the probability of extracting graphlet G * t ( j ) from test photo I * , which is computed based on</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>• The Photo.net consists of 3581 images. Only URLs of the original photos are provided. Approximately half images were removed from the websites, leaving only nearly 1,700 images available. Thus, we extend this data set by online crawling 4,000 photos and naming the extended Photo.net data set PNE. The aesthetics of these additionally crawled photos are manually labeled and are randomly split into equal partitions, one for training and the rest for testing. • The AVA [19] contains 25,000 highly-and low-aesthetic photos in total, each of which is associated with two semantic tags. The selection criteria is based on the aesthetic quality of each photo, which is scored by 78 to 549 amateur/professional photographers. The training and test photos of the AVA data set are pre-specified. In our experiment, for the classifier-based photo aesthetic models, such as those proposed by Marchesotti et al.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF AESTHETICS PREDICTION ACCURACIES approaches respectively proposed by Dhar et al. [39], Luo et al.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF CATEGORIZATION PERFORMANCE ON THE PASCAL VOC 2009 TABLE III COMPARISON OF CATEGORIZATION PERFORMANCE ON THE MIT INDOOR 67</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV AESTHETICS</head><label>IV</label><figDesc>PREDICTION ACCURACY DECREMENT well-known part-based model proposed by Juneja et al.<ref type="bibr" target="#b34">[35]</ref>. As shown in TableIII, the categorization performance is consistent with that on the PASCAL VOC 2009.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Based on the definition of graphlets, as shown in Fig.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2, each vertex denotes an atomic region. Thus, we use "vertex" and "an atomic region" indiscriminately in this paper.<ref type="bibr" target="#b1">2</ref> The number is A * K t-1 /t! where K is the average degree of atomic regions; A counts atomic regions in an image; and t is the graphlet size.<ref type="bibr" target="#b2">3</ref> Imperfection means some segmented regions partially cover one or multiple semantic components.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the Singapore National Research Foundation through the International Research Centre at Singapore Funding Initiative, in part by the IDM Programme Office, in part by the National Natural Science Foundation of China under Grant 61125106, and in part by the Shaanxi Key Innovation Team of Science and Technology under Grant 2012KCT-04. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Bulent Sankur.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Yue Gao (M'13) received the B.S. degree from the Harbin Institute of Technology, Harbin, China, and the M.E. and Ph.D. degrees from Tsinghua University, Beijing, China. His research interests include large scale multimedia retrieval and live social media analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Roger Zimmermann</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for photoquality assessment and enhancement based on visual aesthetics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimedia, 2010</title>
		<meeting>Int. Conf. Multimedia, 2010</meeting>
		<imprint>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to photograph</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimedia, 2010</title>
		<meeting>Int. Conf. Multimedia, 2010</meeting>
		<imprint>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Studying aesthetics in photographic images using a computational approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Photographic Cmposition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scanlon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Amphoto Books</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Manifold regularized discriminative nonnegative matrix factorization with fast gradient descent</title>
		<author>
			<persName><forename type="first">N</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2030" to="2048" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On feature combination for multiclass object classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th ICCV</title>
		<meeting>12th ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="221" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image classification with segmentation graph kernels</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The design of high-level features for photo quality assessment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object bank: A highlevel image representation for scene classification and semantic feature sparsification</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1378" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Content-based photo quality assessment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2206" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kernel discriminant analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liddell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Program. Lang. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="745" to="770" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="524" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Manifold regularized multitask learning for semi-supervised multilabel image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="523" to="536" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes (VOC) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Veringham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">88</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge</title>
		<imprint>
			<date type="published" when="2009-10">2009. Oct. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">AVA: A largescale database for aesthetic visual analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2408" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sensation-based photo cropping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>ACM Multimedia</publisher>
			<biblScope unit="page" from="669" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Aesthetic quality classification of photographs based on color harmony</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe1</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Similarity of color images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orengo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Storage Retr. Image Video Databases</title>
		<meeting>Storage Retr. Image Video Databases</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="381" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Biologically inspired feature manifold for scene classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="174" to="184" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Similarity and affine invariant distances between 2D point sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="810" to="814" />
			<date type="published" when="1995-08">Aug. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatial latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1577" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards an unsupervised optimal fuzzy clustering algorithm for image database organization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th ICPR</title>
		<meeting>15th ICPR</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="897" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Personalized photograph ranking and selection system</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Barsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Patch alignment for dimensionality reduction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1299" to="1313" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic graphlet transfer for photo cropping</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2887" to="2897" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Probabilistic graphlet cut, exploiting spatial structure cue for weakly supervied image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1908" to="1915" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards low bit rate mobile visual search with multiple channel coding</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th ACM Int. Conf. Multimedia</title>
		<meeting>19th ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Less is more: Efficient 3-D object retrieval with query view selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1071" to="1018" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual-textual joint relevance learning for tag-based social image search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Discovering discriminative graphlets for aerial image categories recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5071" to="5084" />
			<date type="published" when="2013-12">Dec. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Blocks that shout: Distinctive parts for scene classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Juneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="923" to="930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sensation-based photo cropping</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th ACM Int. Conf. Multimedia</title>
		<meeting>17th ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="669" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Personalized photograph ranking and selection system</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Barsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimedia</title>
		<meeting>Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="211" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Content-based photo quality assessment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2206" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiview spectral embedding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern., B, Cybern</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1438" to="1446" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Task dependent visual codebook compression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2282" to="2293" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to distribute vocabulary indexing for scalable visual search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="166" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The devil is in the details: An evaluation of recent feature encoding methods</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m">Luming Zhang is a Post-Doctoral Research Fellow with the School of Computing, National University of Singapore. His research interests include multimedia analysis, image enhancement, and pattern recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
