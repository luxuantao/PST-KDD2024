<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">StereoScan: Dense 3d Reconstruction in Real-time</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
							<email>geiger@kit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Measurement and Control</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julius</forename><surname>Ziegler</surname></persName>
							<email>ziegler@kit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Measurement and Control</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
							<email>stiller@kit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Measurement and Control</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">StereoScan: Dense 3d Reconstruction in Real-time</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1137FFF1683A968BB6184CC0AEEAFC1E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate 3d perception from video sequences is a core subject in computer vision and robotics, since it forms the basis of subsequent scene analysis. In practice however, online requirements often severely limit the utilizable camera resolution and hence also reconstruction accuracy. Furthermore, real-time systems often rely on heavy parallelism which can prevent applications in mobile devices or driver assistance systems, especially in cases where FPGAs cannot be employed.</p><p>This paper proposes a novel approach to build 3d maps from high-resolution stereo sequences in real-time. Inspired by recent progress in stereo matching, we propose a sparse feature matcher in conjunction with an efficient and robust visual odometry algorithm. Our reconstruction pipeline combines both techniques with efficient stereo matching and a multi-view linking scheme for generating consistent 3d point clouds. In our experiments we show that the proposed odometry method achieves state-of-the-art accuracy. Including feature matching, the visual odometry part of our algorithm runs at 25 frames per second, while -at the same time -we obtain new depth maps at 3-4 fps, sufficient for online 3d reconstructions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Today, laser scanners are still widely used in robotics and autonomous vehicles, mainly because they directly provide 3d measurements in real-time. However, compared to traditional camera systems, 3d laser scanners are often more expensive and more difficult to seamlessly integrate into existing hardware designs (e.g., cars or trains). Moreover, they easily interfere with other sensors of the same type as they are based active sensing principles. Also, their vertical resolution is limited (e.g., 64 laser beams in the Velodyne HDL-64E). Classical computer vision techniques such as appearance-based object detection and tracking are hindered by the large amount of noise in the reflectance measurements.</p><p>Motivated by those facts and the emergent availability of high-resolution video sensors, this paper proposes a novel system enabling accurate 3d reconstructions of static scenes, solely from stereo sequences 1 . To the best of our knowledge, ours is the first system which is able to process images of approximately one Megapixel resolution online on a single CPU. Our contributions are threefold: First, we demonstrate real-time scene flow computation with several thousand feature matches. Second, a simple but robust visual odometry algorithm is proposed, which reaches significant speed-ups compared to current state-of-the-art. Finally, using the obtained ego-motion, we integrate dense stereo measurements from LIBELAS <ref type="bibr" target="#b11">[12]</ref> at a lower frame rate and 1 Source code available from: www.cvlibs.net ⇓ Fig. <ref type="figure">1</ref>. Real-time 3d reconstruction based on stereo sequences: Our system takes stereo images (top) as input and outputs an accurate 3d model (bottom) in real-time. All processing is done on a single CPU.</p><p>solve the associated correspondence problem in a greedy fashion, thereby increasing accuracy while still maintaining efficiency. Fig. <ref type="figure">1</ref> illustrates the input to our system and resulting live 3d reconstructions on a toy example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>As video-based 3d reconstruction is a core topic in computer vision and robotics, there exists a large body of related work, sketched briefly in the following:</p><p>Simultaneous Localisation and Mapping (SLAM) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b4">[5]</ref> is the process by which a mobile robot incrementally builds a consistent map of its environment and at the same time uses this map to compute its own location. However, for computational reasons, most of the proposed approaches are only able to handle very sparse sets of landmarks in real-time, while here we are interested in a dense mapping solution.</p><p>With the seminal work by Hoiem et al. <ref type="bibr" target="#b13">[14]</ref> learningbased approaches to geometry estimation from monocular images have seen a revival <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Those methods typically segment images into superpixels and, based on local appearance as well as global constraints, infer the most likely 3d configuration of each segment. Even though impressive results have been demonstrated recently <ref type="bibr" target="#b12">[13]</ref>, those methods are still too inaccurate and erroneous to directly support applications like mobile navigation or autonomous driving.</p><p>3d reconstruction from uncalibrated image collections has been shown by <ref type="bibr">Koch [17]</ref>, Pollefeys <ref type="bibr" target="#b21">[22]</ref>, Seitz <ref type="bibr" target="#b23">[24]</ref> et al. using classical Structure-from-Motion (SfM) techniques. Extensions to urban reconstruction have been demonstrated in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. More recently, the availability of photo sharing platforms like Flickr led to efforts of modeling cities as large as Rome <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>. However, in order to obtain accurate semi-dense reconstructions, powerful multi-view stereo schemes are employed, which, even on small image collections, easily take up to several hours while making extensive use of parallel processing devices. Further, most of the proposed methods require several redundant viewpoints, while our application target is a continuously moving mobile platform, where objects can be observed only over short periods of time.</p><p>In <ref type="bibr" target="#b2">[3]</ref> Badino et al. introduces Stixel World as mediumlevel representation to reduce the amount of incoming sensor information. They observe that free space in front of a vehicle is usually limited by objects with vertical surfaces, and represent those by adjacent rectangular sticks of fixed width, which are tracked over time <ref type="bibr" target="#b20">[21]</ref>. Another kind of frequently employed mid-level representations are occupancy grids <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, which discretize the 3d world into binary 2d cells. Though useful in many applications, those types of abstractions are not detailed enough to represent curbstones or overhanging objects such as trees, signs or traffic lights. Alternatively, 3d voxel grids can be employed. However, without waiving resolution, computational complexity increases dramatically. In this paper instead, we are interested in representing the perceived information as detailed as possible, but without losing real-time performance.</p><formula xml:id="formula_0">+8 +1 +1 +1 +1 +1 +1 +1 +1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 (a)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. 3D RECONSTRUCTION PIPELINE</head><p>Our 3d reconstruction pipeline consists of four stages: Sparse feature matching, egomotion estimation, dense stereo matching and 3d reconstruction. We assume two cores of a CPU available, such that two threads can carry out work in parallel: As illustrated in Fig. <ref type="figure" target="#fig_0">2</ref> the first worker thread performs feature matching and egomotion estimation at 25 fps, while the second thread performs dense stereo matching and 3d reconstruction at 3 to 4 fps. As we show in our experiments, this is sufficient for online 3d reconstruction of static scenes. In the following we will assume a calibrated stereo setup and rectified input images, as this represents the standard case and simplifies computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Matching</head><p>The input to our visual odometry algorithm are features matched between four images, namely the left and right images of two consecutive frames. In order to find stable feature locations, we first filter the input images with 5 × 5 blob and corner masks, as given in Fig. <ref type="figure">3</ref>. Next, we employ non-maximum-and non-minimum-suppression <ref type="bibr" target="#b19">[20]</ref> on the filtered images, resulting in feature candidates which belong to one of four classes (i.e., blob max, blob min, corner max, corner min). To reduce computational efforts, we only match features within those classes.</p><p>In contrast to methods concerned with reconstructions from unordered image collections, here we assume a smooth camera trajectory, superseding computationally intense rotation and scale invariant feature descriptors like SURF <ref type="bibr" target="#b3">[4]</ref>. Given two feature points, we simply compare 11 × 11 block windows of horizontal and vertical Sobel filter responses to each other by using the sum of absolute differences (SAD) error metric. To speed-up matching, we quantize the Sobel responses to 8 bits and sum the differences over a sparse set of 16 locations (see Fig. <ref type="figure">3(c</ref>)) instead of summing over the whole block window. Since the SAD of 16 bytes can be computed efficiently using a single SSE instruction we only need two calls (for horizontal + vertical Sobel responses) in order to evaluate this error metric.</p><p>Our egomotion estimation mechanism expects features to be matched between the left and right images and two consecutive frames. This is achieved by matching features in a 'circle': Starting from all feature candidates in the current left image, we find the best match in the previous left image within a M × M search window, next in the previous right image, the current right image and last in the current left image again. A 'circle match' gets accepted, if the last feature coincides with the first feature. When matching between the left and right images, we additionally make use of the epipolar constraint using an error tolerance of 1 pixel. Sporadic outliers are removed by establishing neighborhood relations as edges of a 2d Delaunay triangulation <ref type="bibr" target="#b24">[25]</ref> on the feature locations in the current left image. We only retain matches which are supported by at least two neighboring matches, where a match is supporting another match, if its disparity and flow differences fall within some threshold τ disp or τ f respectively. If required, sub-pixel refinement via parabolic fitting can be employed to further improve feature localization.</p><p>Even though our implementation is very efficient, establishing several thousands to ten thousands of correspondences still takes time in the order of seconds, hence making it too slow for online applications. By transferring ideas already employed in previous works on stereo matching <ref type="bibr" target="#b11">[12]</ref>, further significant speed-ups are possible: In a first pass, we match only a subset of all features, found by non-maximasuppression (NMS) using a larger NMS neighborhood size (factor 3). Since this subset is much smaller than the full feature set, matching is very fast. Next, we assign each feature in the current left image to a 50 × 50 pixel bin of an equally spaced grid. Given all sparse feature matches, we compute the minimum and maximum displacements for each bin. Those statistics are used to locally narrow down the final search space, leading to faster matching and a higher number of matches at the same time, as evidenced in the experimental section. Fig. <ref type="figure">4</ref> illustrates feature matching and tracking results using our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Egomotion Estimation</head><p>Given all 'circular' feature matches from the previous section, we compute the camera motion by minimizing the sum of reprojection errors and refining the obtained velocity estimates by means of a Kalman filter.</p><p>First, bucketing is used to reduce the number of features (in practice we retain between 200 and 500 features) and spread them uniformly over the image domain. Next, we project feature points from the previous frame into 3d via triangulation using the calibration parameters of the stereo camera rig. Assuming squared pixels and zero skew, the reprojection into the current image is given by</p><formula xml:id="formula_1">  u v 1   =   f 0 c u 0 f c v 0 0 1       R(r) t     x y z 1     -   s 0 0       (1) with • homogeneous image coordinates (u v 1) T • focal length f • principal point (c u , c v ) • rotation matrix R(r) = R x (r x )R y (r y )R z (r z ) • translation vector t = (t x t y t z ) T • 3d point coordinates X = (x y z) T</formula><p>• and shift s = 0 (left image), s = baseline (right image). Let now π (l) (X; r, t) : R 3 → R 2 denote the projection implied by Eq. 1, which takes a 3d point X and maps it to a pixel x (l) i ∈ R 2 on the left image plane. Similarly, let π (r) (X; r, t) be the projection onto the right image plane. Using Gauss-Newton optimization, we iteratively minimize</p><formula xml:id="formula_2">N i=1 x (l) i -π (l) (X i ; r, t) 2 + x (r) i -π (r) (X i ; r, t) 2 (2)</formula><p>with respect to the transformation parameters (r, t). Here x (l) i and x (r) i denote the feature locations in the current left and right images respectively. The required Jacobians J π (l) and J π (r) are readily derived from Eq. 1. In practice we note that even if we initialize r and t to 0, a couple of iterations (e.g., 4-8) are sufficient for convergence. To be robust against outliers, we wrap our estimation approach into a RANSAC scheme, by first estimating (r, t) for 50 times independently using 3 randomly drawn correspondences. All inliers of the winning iteration are then used for refining the parameters, yielding the final transformation (r, t).</p><p>On top of this simple, but efficient estimation procedure we place a standard Kalman filter, assuming constant acceleration. To this end, we first obtain the velocity vector v = (r t) T /∆ t as the transformation parameters divided by the time between frames ∆ t . The state equation is given by v a</p><formula xml:id="formula_3">(t) = I ∆ t I 0 I v a (t-1) +<label>(3)</label></formula><p>and the output equation reduces to since we directly observe v. Here, a denotes acceleration, I is the 6 × 6 identity matrix and , ν represent Gaussian process and measurement noise, respectively.</p><formula xml:id="formula_4">1 ∆ t r t (t) = I 0 v a (t) + ν<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stereo Matching</head><p>For obtaining dense disparity maps, we use a method called ELAS <ref type="bibr" target="#b11">[12]</ref>, which is freely available. ELAS is a novel approach to binocular stereo for fast matching of highresolution imagery. It builds a prior on the disparities by forming a triangulation on a set of support points which can be robustly matched, reducing matching ambiguities of the remaining points. This allows for efficient exploitation of the disparity search space, yielding accurate and dense reconstructions without global optimization. Also, the method automatically determines the required disparity search range, which is important for outdoor scenarios. ELAS achieves state-of-the-art performance on the large-scale Middlebury benchmark while being significantly faster than existing methods: For the outdoor sequences used in our experiments (≈ 0.5 Megapixel resolution), we obtain 3 -4 frames per second on a single i7 CPU core running at 3.0 GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. 3d Reconstruction</head><p>The last step in our reconstruction pipeline creates consistent point-based models from the large amount of incoming data (i.e., ≈ 500.000 points, 3 -4 times every second). The simplest method to point-based 3d reconstructions maps all valid pixels to 3d and projects them into a common coordinate system according to the estimated camera motion. However, without solving the association problem, storage requirements will grow rapidly. Further, redundant information can not be used for improving reconstruction accuracy. On the other hand, traditional multi-view optimizations such as bundle adjustment are computationally infeasible for dense real-time systems as the proposed one.</p><p>Instead, here we propose a greedy approach which solves the association problem by reprojecting reconstructed 3d points of the previous frame into the image plane of the current frame. In case a point falls onto a valid disparity, we fuse both 3d points by computing their 3d mean. This doesn not only dramatically reduce the number of points which have to be stored, but also leads to improved accuracy by averaging out measurement noise over several frames. Fig. <ref type="figure">5</ref> illustrates our approach for two frames: Parts of the points tables show timings for individual parts of our algorithm when parameterized to the online settings (2 scales, NMS neighborhood 3 corresponding to approximately 500 to 2000 feature matches and 50 RANSAC iterations). For timings of the stereo matching stage we refer the reader to <ref type="bibr" target="#b11">[12]</ref>. captured in frame one (blue) get fused with points captured in frame two (orange), after the camera underwent a forward movement. Our method only involves projections and pointer book keeping and hence can be implemented very efficiently: Appending a single disparity map to the 3d model typically takes less than 50 ms, hence only adding minor computations to the stereo matching and reconstruction thread.</p><p>Since our reconstructed sequences are relatively short, we do not consider the 'soft reset problem' in this paper. However, a simple solution would be to remove all 3d points associated with depth maps of 'outdated' poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS</head><p>In this section we compare our results to <ref type="bibr" target="#b15">[16]</ref>, a freely available visual odometry library. All experiments were performed on image sequences of the Karlsruhe dataset (www.cvlibs.net), which provides ground truth GPS+IMU data as well as stereo sequences at a resolution of 1344×391 pixels and 10 fps. Our real-time parameterization uses the standard settings for LIBELAS stereo matching <ref type="bibr" target="#b11">[12]</ref>, and sparse feature matching at 2 scales with 50 RANSAC iterations, an inlier threshold of 1.5 pixels and τ disp = τ f low = 5 pixels. For egomotion estimation, we empirically set the measurement noise parameters of the Kalman filter to ν ∼ N (0, 10 -2 ×I), 1..6 ∼ N (0, 10 -8 ×I) and 7..12 ∼ N (0, I).</p><p>A. Feature matching Fig. <ref type="figure" target="#fig_3">6</ref>(a) illustrates sparse feature matching running times over the number of matched features. We compare the proposed method to a version evaluated at half-size resolution and refined at full resolution, and to the baseline by Kitt et al. <ref type="bibr" target="#b15">[16]</ref>. We observe that our multi-stage matching approach helps in reducing running times significantly, while at the same time increasing the number of feature matches. This beneficial behaviour is mainly due to reduced ambiguities in the second matching stage. Also, note that a two scale matching approach, which computes features at half resolution and refines them at full resolution, further reduces running time, while preserving a reasonable amount of feature matches for visual odometry (500-2000): Using this setting we are able to achieve feature matching over 25 fps on a single CPU core, as shown in the table, which lists running times of individual parts ouf our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visual Odometry</head><p>As evidenced by Fig. <ref type="figure" target="#fig_3">6</ref>(b), we are also able to cut visual odometry running times significantly with respect to the CVMLIB-based version of the algorithm presented in <ref type="bibr" target="#b15">[16]</ref>. While Kitt et al. requires about one second to process 200 feature matches, 4.3 milliseconds are sufficient for our method, leading to speed ups of more than factor 200. This is mainly due to the relatively complex nature of the observation model employed in <ref type="bibr" target="#b15">[16]</ref>, which is based on trifocal tensors and requires inverting matrices growing linearly with the number of matched features, while our matrix inversions are constant in this number. In Fig. <ref type="figure" target="#fig_4">7</ref> we further compare our visual odometry trajectories to Kitt et al. and 'ground truth' output of a OXTS RT 3003 GPS/IMU system on the Karlsruhe dataset. Even though running much faster, our method achieves localization accuracy comparable to <ref type="bibr" target="#b15">[16]</ref>. Please note that the GPS/IMU system can only be considered as 'weak' ground truth, because localization errors of up to two meters may occur in inner-city scenarios due to limited satellite availability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 3d Reconstruction</head><p>We also qualitatively evaluate our complete reconstruction pipeline. Fig. <ref type="figure">8</ref> illustrates 3d reconstructions obtained by our system for three different sequences (rows) and from four different viewpoints (columns).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper we have demonstrated a system to generate accurate dense 3d reconstructions from stereo sequences.</p><p>Compared to existing methods, we were able to reduce running times of feature matching and visual odometry by more than one or two orders of magnitude, respectively, allowing real-time 3d reconstructions from large-scale imagery on the CPU. We believe our system will be valuable for other researchers working on higher-level reasoning for robots and intelligent vehicles. In the future we intend to combine our visual odometry system with GPS/INS systems to reduce localization errors in narrow urban scenarios with restricted satellite reception. We also plan on handling dynamic objects and on using our maps for 3d scene understanding at road intersections <ref type="bibr" target="#b10">[11]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2.System overview: We use two worker threads in order to obtain egomotion estimates and disparity maps in parallel: While the stereo matching and 3d reconstruction part runs at 3-4 fps (Sec. III C+D), our sparse feature matching and visual odometry system (Sec. III A+B) achieves 25 fps. Taken together, this is sufficient for online 3d reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3.Blob/corner detector and feature descriptor: Our feature detections are minima and maxima of blob and corner filter responses. The descriptor concatenates Sobel filter responses using the layout given in (c).</figDesc><graphic coords="2,316.69,186.76,232.56,67.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 Fig. 5 .</head><label>15</label><figDesc>Fig. 5. Multi-view reconstruction: In order to fuse 3d points we greedily associate them by reprojection into the image plane of the current frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Sparse feature matching and visual odometry running times.tables show timings for individual parts of our algorithm when parameterized to the online settings (2 scales, NMS neighborhood 3 corresponding to approximately 500 to 2000 feature matches and 50 RANSAC iterations). For timings of the stereo matching stage we refer the reader to<ref type="bibr" target="#b11">[12]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Visual odometry results on the Karlsruhe data set. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Inner-city stereo scans. Four rendered views (columns) are shown for three sequences (rows). Also see videos at: www.cvlibs.net</figDesc><graphic coords="6,54.00,361.14,68.88,129.53" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGEMENTS</head><p>We thank the reviewers for their feedback and the Karlsruhe School of Optics and Photonics for financial support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building rome in a day</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards urban 3d reconstruction from video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akbarzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Clipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Engels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Merrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Talton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>in 3DPVT, 2006</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The stixel world -a compact medium level representation of the 3d-world</title>
		<author>
			<persName><forename type="first">H</forename><surname>Badino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pfeiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Monoslam: Real-time single camera slam</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Molton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stasse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1052" to="1067" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A solution to the simultaneous localization and map building (slam) problem</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W M G</forename><surname>Dissanayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Durrantwhyte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Csorba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="229" to="241" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simultaneous localisation and mapping</title>
		<author>
			<persName><forename type="first">H</forename><surname>Durrant-Whyte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Magazine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2006</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building rome on a cloudless day</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fite-Georgel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raguram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Jen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Clipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="368" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast robust large-scale mapping from video and internet photo collections</title>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Clipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raguram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Piecewise planar and nonplanar stereo for urban scene reconstruction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1418" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A generative model for 3d urban scene understanding from movable platforms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Colorado Springs, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient large-scale stereo matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Blocks world revisited: Image understanding using qualitative geometry and mechanics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric context from a single image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="654" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stereo vision based mapping and navigation for mobile robots</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1694" to="1699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Visual odometry based on stereo image sequences with ransac-based outlier rejection scheme</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lategahn</surname></persName>
		</author>
		<editor>IV</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi viewpoint stereo from uncalibrated video sequences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="55" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Outdoor mapping and navigation using stereo vision</title>
		<author>
			<persName><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gerkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Experimental Robotics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fastslam: A factored solution to the simultaneous localization and mapping problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Montemerlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wegbreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">National Conference on Artificial Intelligence. AAAI</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="593" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient non-maximum suppression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Neubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR 2006</title>
		<imprint>
			<date type="published" when="2006-08">August 2006</date>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient representation of traffic scenes by means of dynamic stixels</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<editor>IV</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual modeling with a hand-held camera</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vergauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Verbiest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tops</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="207" to="232" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning 3-d scene structure from a single still image</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comparison and evaluation of multi-view stereo reconstruction algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Shewchuk</surname></persName>
		</author>
		<title level="m">Applied Computational Geometry: Towards Geometric Engineering</title>
		<imprint>
			<date type="published" when="1996-05">May 1996</date>
			<biblScope unit="volume">1148</biblScope>
			<biblScope unit="page" from="203" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Real-time monocular slam with straight lines</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
