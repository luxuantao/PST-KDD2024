<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bring Your Own View: Graph Neural Networks for Link Prediction with Personalized Subgraph Selection</title>
				<funder ref="#_TmmbjQ2">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-12-23">23 Dec 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
							<email>qytan@tamu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
							<email>xin12.zhang@connect.polyu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
							<email>ninghao.liu@uga.edu</email>
						</author>
						<author>
							<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
							<email>daochen.zha@rice.edu</email>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Li</surname></persName>
							<email>li.li1@samsung.com</email>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
							<email>rui.chen1@samsung.com</email>
						</author>
						<author>
							<persName><forename type="first">Soo-Hyun</forename><surname>Choi</surname></persName>
							<email>sh9.choi@samsung.com</email>
						</author>
						<author>
							<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
							<email>xia.hu@rice.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The Hong Kong Polytechnic University Hong Kong SAR</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Georgia</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Rice University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Samsung Electronics America</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Samsung Electronics America</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Samsung Electronics</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Rice University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bring Your Own View: Graph Neural Networks for Link Prediction with Personalized Subgraph Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-12-23">23 Dec 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3539597.3570407</idno>
					<idno type="arXiv">arXiv:2212.12488v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computer systems organization ? Embedded systems</term>
					<term>Redundancy</term>
					<term>Robotics</term>
					<term>? Networks ? Network reliability Graph neural networks, personalized subgraph selection, link prediction, bi-level optimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have received remarkable success in link prediction (GNNLP) tasks. Existing efforts first predefine the subgraph for the whole dataset and then apply GNNs to encode edge representations by leveraging the neighborhood structure induced by the fixed subgraph. The prominence of GNNLP methods significantly relies on the adhoc subgraph. Since node connectivity in real-world graphs is complex, one shared subgraph is limited for all edges. Thus, the choices of subgraphs should be personalized to different edges. However, performing personalized subgraph selection is nontrivial since the potential selection space grows exponentially to the scale of edges. Besides, the inference edges are not available during training in link prediction scenarios, so the selection process needs to be inductive. To bridge the gap, we introduce a Personalized Subgraph Selector (PS2) as a plug-and-play framework to automatically, personally, and inductively identify optimal subgraphs for different edges when performing GNNLP. PS2 is instantiated as a bi-level optimization problem that can be efficiently solved differently. Coupling GNNLP models with PS2, we suggest a brand-new angle towards GNNLP training: by first identifying the optimal subgraphs for edges; and then focusing on training the inference model by using the sampled subgraphs. Comprehensive experiments endorse the effectiveness of our proposed method across various GNNLP backbones (GCN, GraphSage, NGCF, LightGCN, and SEAL) and diverse benchmarks (Planetoid, OGB,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph is a ubiquitous and powerful data structure to present different types of relational data, such as social networks and biological molecules. Given that real-world graphs are often only partially observed, link prediction <ref type="bibr" target="#b0">[1]</ref>, which aims to predict missing links in a graph, is a central problem across many scientific domains. For example, link prediction has applications in predicting protein interactions <ref type="bibr" target="#b1">[2]</ref>, drug responses <ref type="bibr" target="#b2">[3]</ref>, and completing the knowledge graph <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Besides, it is also the backbone for various recommendation systems, e.g., friend suggestion in social networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> or product recommendation in online market-places <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>.</p><p>Recently, considerable efforts have been made to develop advanced link prediction techniques <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. Among them, graph neural networks (GNNs) based link prediction models (GNNLP) have achieved impressive results <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>, owing to the expressive encoding capacity of GNNs. The essential idea behind GNNLP is to generate edge representation based on the subgraph around the anchor edge via a GNN encoder and then estimate its likelihood with a prediction function. According to the difference in utilizing subgraph for edge embedding, they can be divided into two categories: node2link <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref> and subgraph2link <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>. The node2link approaches (i.e., GAE <ref type="bibr" target="#b13">[14]</ref>, GraphSage <ref type="bibr" target="#b14">[15]</ref>, and Light-GCN <ref type="bibr" target="#b16">[17]</ref>) aim to first learn node representations for the head and tail nodes of the anchor edge independently, and then combine the representations of end nodes for edge embedding. In contrast, the subgraph2link approaches (e.g., SEAL <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref>) target to learn edge representation by pooling over the subgraph of the given edge, casting it as a graph representation learning task.</p><p>While effective, both of them have largely overlooked the diversity of subgraphs when embedding different edges. For example, GAE and SEAL assume that the best subgraph structure for all edges is the same and adopt the neighbors within ?-hops for edge embedding, where ? ? {1, 2, ? ? ? , ? } is a hyperparameter. Although such collective selection can significantly reduce the tedious tuning efforts to identify the best ? value from ? options, the shared subgraph structure assumption is rather limited. Different edges may favor different subgraph structures for link prediction. This hypothesis is reasonable because node connectivity patterns in real-world graphs are complex <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. For instance, in social networks, the social connectivity of users is created by different factors <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. In the recommendation system, a user's purchase behavior could be motivated by either his/her like-minded customers or conceptually similar products in the historical records <ref type="bibr" target="#b25">[26]</ref>.</p><p>Motivated by this, we conduct a preliminary experiment on the Cora dataset to test how different subgraph structures impact the link prediction results in Figure <ref type="figure" target="#fig_5">1</ref>. We observe that GAE can accurately infer different missing edges by training over various predefined subgraph structures. For instance, the first missing edge can be well recovered by using the neighbors within 3 hops, while the second missing edge can be effectively reconstructed by using the 2-hops and 3-hops of neighbors of its head and tail nodes, respectively. In terms of their input neighborhood subgraphs, this personalized phenomenon of edges, has never been explored in link prediction scenarios. To bridge the gap, in this paper, we propose to develop an effective subgraph selector to automatically identify the most informative subgraphs for different edges.</p><p>However, it is a nontrivial and challenging task mainly because of three roadblocks. First, given a graph data, the latent subgraph selection space is exponential to the size of edges, which is millions or even billions in practice. It is impossible to identify the optimal subgraph configurations for all edges via the brute-force search. Second, in link prediction applications, the edges to be predicted are not available during the training. Thus, the subgraph selection process must be inductive, enabling infer subgraph structures for unseen edges. Third, how to make the edge-wise subgraph selection adaptive to the well-established base models such as methods under the node2link approach (e.g., GAE <ref type="bibr" target="#b13">[14]</ref>, GraphSage <ref type="bibr" target="#b14">[15]</ref>, NGCF <ref type="bibr" target="#b15">[16]</ref> and LightGCN <ref type="bibr" target="#b16">[17]</ref>) or subgraph2link category (e.g., SEAL <ref type="bibr" target="#b17">[18]</ref>).</p><p>To address these challenges, we propose a novel personalized subgraph selector, dubbed PS2, as a plug-and-play framework. It aims to develop an automatic and inductive subgraph selection module for GNNLP methods, such that the most informative subgraph structures can be explicitly identified and exploited for different edges. Specifically, we aim to explore two important research questions. (i) How to automatically sample the optimal subgraph structure for each edge efficiently, and make the selection process inductive? (ii) How to effectively equip well-established GNNLP methods with the proposed personalized selector, so as to offer orthogonal gains across a variety of graph domains and GNNLP backbones? We summarize our major contributions as follows.</p><p>? We focus on subgraph selection for GNNs based link prediction (GNNLP) problem, and propose an effective personalized subgraph selector (PS2). PS2 is the first to automate subgraph selection in an edge-wise fashion when performing GNLLP. It can be easily adopted to boost the well-studied GNNLP methods. ? PS2 can be formulated under bi-level optimization and solved using the alternating gradient-descent algorithm. It is inspired by the differentiable architecture search <ref type="bibr" target="#b26">[27]</ref>, but we extend it from a transductive search model to an inductive subgraph selector, focusing on the edge-wise subgraph structure selection rather than the model architecture search. ? We conduct extensive experiments to evaluate PS2 on multiple graph benchmarks of diverse types and scales, over a variety of GNNLP backbones. Empirical results show that with PS2, the performance of state-of-the-art GNNLP competitors can be advanced with a wide margin.  </p><formula xml:id="formula_0">G $ &amp; G " ' h $ &amp; h " ' Shared z $," &amp;,'</formula><p>(i, j) of thumb or validation, where the same ? is applied to the whole graph G. However, as shown in Figure <ref type="figure" target="#fig_5">1</ref>, the optimal ? varies for predicting different edges. Therefore, we propose a personalized subgraph selection to identify the most informative subgraph for different edges, where the problem is formally defined below.</p><formula xml:id="formula_1">+ z $,"</formula><p>Definition 1. Personalized subgraph selection. Given a graph G = (V, E), the subgraph space for node ? ? V is defined as Compared with existing GNNLP methods, our personalized setting implies two essential properties as below.</p><formula xml:id="formula_2">S ? = {G 1 ? , G 2 ? , ? ? ? , G ? ? , ? ? ? , G ? ? }.</formula><p>? Edge subgraph is personal. In our setting, the local subgraphs are personalized to different edges. For example, the subgraph can be</p><formula xml:id="formula_3">G 1,2 = (G 1 1 , G<label>3</label></formula><p>2 ) in predicting edge ? 1,2 , but it also can be G 3,4 = (G 2 3 , G 1 4 ) for edge ? 3,4 . However, in existing GNNLP efforts, the subgraph order is restricted to be the same for all edges, i.e., <ref type="bibr" target="#b3">4</ref> ). ? Node subgraph is polysemous. Node ? can use different subgraphs in predicting different edges. For example, the optimal subgraph for ?</p><formula xml:id="formula_4">G 1,2 = (G 2 1 , G 2 2 ) and G 3,4 = (G 2 3 , G<label>2</label></formula><formula xml:id="formula_5">1,2 is G 1,2 = (G 1 1 , G 3 2 ), while G 1,4 = (G 3 1 , G<label>2 4</label></formula><p>) for edge ? 1,4 . In existing GNNLP methods, the neighbor range of node subgraph is fixed, e.g., subgraphs for node 1, 2, and 4 are G 2  1 , G 2 2 , and G 2 4 , respectively. GNNs for Node Embedding. Given a node ? ? V, GNNs models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> are widely adapted to mapping nodes into hidden representations, i.e., ? ? : V ? R ? . GNNs target to update the node presentation by aggregating representations of itself and its neighbors. Formally, at the ?-th layer, we have</p><formula xml:id="formula_6">h (?) ? = UPDATE(h (?-1) ? , AGGREGATE({h (?-1) ? : ? ? N ? })). (1)</formula><p>h ? ? ? R ? is the hidden representation of node ? at the ?-th layer, while N ? is the set of nodes adjacent to ?. We often initialize h 0 ? as x ? . The AGGREGATE function aims to receive messages from neighbors and the UPDATE function focuses on updating ?'s representation based on the representation from the previous GNN layer and the information from neighbors. By stacking ? GNN layers, each node ? has ? hidden representations {h 1  ? , ? ? ? , h ? ? }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED METHOD</head><p>In this section, we present the details of the proposed PS2 shown in Figure <ref type="figure" target="#fig_1">2</ref>. We first discuss the exponential subgraph selection space of our problem. Then, we elaborate on a tailored inductive subgraph selector to effectively sample subgraphs for seen and unseen edges in this space. Finally, we show how to formulate our training objectives into bi-level optimization and solve it via alternating gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Subgraph Selection Space</head><p>Given a graph G = (V, E) and the maximum number of hops ? considered, there are ? latent subgraphs for each node ? ? V, denoted by S ? = {G ? ? } ? ?=1 . Each subgraph G ? ? is spanned by the anchor node ? and its neighbors within ? hops, whose shortest path distance from ? is less than ?. In previous studies, ? is a dataset-level hyperparameter, which is fixed for all nodes/edges in the graph G. In this work, we denote the subgraph of edge ? ?,? as G ? ?,? = (G ? ? , G ? ? ). In practice, the best ? value is usually selected through validation and is applied to the whole graph. This collective selection strategy has been widely adopted as the default protocol in prior GNNLPs.</p><p>However, as discussed before, applying a constant ? value in selecting subgraphs leads to suboptimal results for predicting some edges. Thus, we propose to adaptively choose the subgraph selection space for different edges. Specifically, we define the subgraph of edge ? ?,? to be composed of its end nodes' subgraphs. That is,</p><formula xml:id="formula_7">G ?,? = (G ? ? , G ? ? |1 ? ?, ? ? ?).</formula><p>Then, the potential subgraph pool size for each edge is ? 2 , and the total subgraph selection space equals to ? 2| E | for the whole graph, where |E | is the number of edges in G. Although ? is empirically small (e.g., ? = 2, 3) in link prediction scenarios, the selection space in our personalized setting is still huge and intractable as the complexity grows exponentially with the edge size. For example, when ? = 2 and |E | = 50, we have nearly 10 30 selection candidates. The situation is more difficult in real-world graphs, where |E | is millions or even billions.</p><p>In summary, by personalizing edge subgraphs, the subgraph selection space for link prediction increases from ? to ? 2| E | . Therefore, existing strategies based on the rule of thumb or grid search are no longer appropriate. Also, a tailored subgraph space selector is needed to tackle our personalized subgraph selection problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Personalized Subgraph Selector</head><p>To assign different subgraph orders to different edges for link prediction, the intuitive solution is random selection. For example, given an edge ? ?,? , we can randomly select its subgraph (e.g.,</p><formula xml:id="formula_8">G ?,? = (G 1 ? , G 3 ? ))</formula><p>from the candidate pool. Despite the simplicity, the random selection approach fails to control the quality of the resulting subgraphs. Coupling existing GNNLP methods with random subgraph selection could incur significant performance degradation, especially when the graph is challenging, e.g., on OGB datasets (see Table <ref type="table" target="#tab_7">3</ref>).</p><p>To address this issue, we focus on data-driven selection by making the subgraph selection process learnable. The core idea is to parameterize the subgraph selector with a deep neural network, which takes a query edge ? ?,? as input and outputs its optimal subgraph for downstream link prediction. The main challenges to achieving this goal are two-fold. (i) Given the exponential complexity (i.e., ? | E | ) of the selection space, how can we make the personalized subgraph selection scale to real-world graphs with millions or even billions of edges? (ii) Since the edges to be inferred are not available in training under link prediction scenarios, how to make the subgraph selector inductive to unseen edges? We introduce our solutions below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Learnable Subgraph Selector.</head><p>To tackle the first challenge, we propose to make the selection process learnable. Given an edge ? ?,? and its ? 2 subgraph candidates, i.e., {(G ? ? , G ? ? )} ? ?,?=1 , our subgraph selector aims to find the most informative subgraph from the candidate set. This selection problem is well-known to be discrete and non-differentiable. While enormous efforts based on evolution <ref type="bibr" target="#b29">[30]</ref> or reinforcement learning <ref type="bibr" target="#b30">[31]</ref> have been dedicated to addressing the discrete selection problem, they are inefficient for training. To tackle this problem, we make the selection process learnable by relaxing the discrete selection space to be continuous. The core idea is to relax the selection of a single subgraph to a softmax over all possible subgraph candidates. Formally, let ? ?,? ?,? denote the contribution of subgraph (G ? ? , G ? ? ) in predicting edge ? ?,? , the learnable selection process is defined as:</p><formula xml:id="formula_9">G ?,? = ?? 1??,? ?? exp(? ?,? ?,? /?) ? ? ? ,? ? =1 exp(? ? ? ,? ? ?,? /?) * G ?,? ?,? ,<label>(2)</label></formula><p>we use * to denote the multiplication alike operation on the subgraph in theory.</p><formula xml:id="formula_10">? ?,? = [? 1,1 ?,? , ? ? ? , ? ?,? ?,? ] ? R ? 2</formula><p>is the subgraph related weight vector for edge ? ?,? , which is initialized as part of model parameters. ? is a small temperature parameter, which helps approximate the categorical selection distribution. By Eq. ( <ref type="formula" target="#formula_4">2</ref>), the subgraph selection process reduces to learning a set of continuous variables {? ?,? : ? ?,? ? E}. After the subgraph selector is welltrained, a discrete subgraph selection can be acquired by replacing the mixed selection with the most likely subgraph, i.e., G ?,? = G </p><formula xml:id="formula_11">G ?,? = ?? 1??,? ?? exp(? ? (G ?,? ?,? )/?) ? ? ? ,? ? =1 exp(? ? (G ? ? ,? ? ?,? )/?) * G ?,? ?,? .<label>(3)</label></formula><p>The above equation provides a principled solution to our personalized subgraph problem. ). COM(?, ?) is a combination function, and the default setting is the element-wise multiplication. This approximation is reasonable in GNNs since h ? ? is obtained by aggregating messages from ?'s neighbors within ? hops. By doing this, we don't need to extract ? 2 subgraphs and apply GNN on these graphs separately. Instead, we can directly obtain ? 2 subgraph embeddings upon the hidden representations of one GNN forward. So, the computational complexity of plugging in our personalized selector is close to standard GNNLPs (See Section 5.6 for efficiency analysis).</p><p>After obtaining ? 2 edge representations, we feed them into a MLP layer with ReLU activation function to predict their importance scores, i.e., ? ?,? ?,? = ? ? (z ?,? ?,? ). Through relaxing the hard selection operation, we can rewrite the mixed selection process in Eq. ( <ref type="formula" target="#formula_3">3</ref>) in embedding space as:</p><formula xml:id="formula_12">z ?,? = ?? 1??,? ?? exp(? ? (z ?,? ?,? )/?) ? ? ? ,? ? =1 exp(? ? (z ? ? ,? ? ?,? )/?) z ?,? ?,? .<label>(4)</label></formula><p>z ?,? ? R ? is the final representation of edge ? ?,? . It is a mixed representation obtained by summing over the representations of various subgraph forms. Note that the mixed operation is only applied for the search phase. In the application phase, we output one subgraph for each edge via max selection (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Training</head><p>After generating the edge representation z ?,? , we adopt an edgewise loss function to estimate the reconstruction errors, expressed as:</p><formula xml:id="formula_13">L = - ?? (?,?) ? E exp(? ?,? ) ? ? ?V exp(? ?,? ? ) ,<label>(5)</label></formula><p>where ? ?,? = ? ? (z ?,? ) is the predicted score for edge ? ?,? , and ? ? is another multilayer perceptron with ReLU activation. As the sum operation in the denominator of Eq. ( <ref type="formula" target="#formula_13">5</ref>) is computationally expensive, we adopt negative selection techniques <ref type="bibr" target="#b14">[15]</ref> to accelerate the optimization in experiments.</p><p>In the search phase, our goal is to jointly learn the subgraph selector ? ? and the model weights within the mixed selection, including GNN encoder ? ? and link predictor ? ? . Following <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>, we employ the validation set performance as a reward to optimize the subgraph selector, and train the GNN encoder and predictor by fitting the training set. Specifically, we optimize our model via the following bi-level optimization framework:</p><formula xml:id="formula_14">min ? ? L ????? (? * , ? ) s.t. ? * = arg min ? L ????? (? * , ?).<label>(6)</label></formula><p>We use ? to wrap up the parameters of GNN encoder ? ? and link predictor ? ? for simplicity. L ????? and L ????? denote the loss function in Eq. ( <ref type="formula" target="#formula_13">5</ref>) computed based on the training and validation sets, respectively. The upper-level objective L (? * , ? ) aims to find ? that minimizes the validation rewards given the optimal ? * , and the lower-level objective L (? * , ?) targets to optimize ? by minimizing the training loss with ? fixed.</p><p>It is worth noting that Eq. ( <ref type="formula" target="#formula_14">6</ref>) only exploits the cheap signals from observed edges, without accessing downstream labeled data for evaluation. Therefore, the validation set used to train the selector can be easily constructed. Since a closed-form solution cannot be computed, we optimize Eq. ( <ref type="formula" target="#formula_14">6</ref>) via alternating between the lowerlevel and the upper-level objectives as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Lower-level optimization.</head><p>With ? fixed, the lower-level optimization w.r.t. ? follows the conventional gradient descent procedure, represented as:</p><formula xml:id="formula_15">? ? = ? -?? ? L ????? (?, ? * ),<label>(7)</label></formula><p>where ? ? R &gt;0 is the learning rate. The converged solution is denoted as ? * (? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Upper-level optimization.</head><p>With ? fixed, the upper-level optimization updates ? according to the validation performance as:</p><formula xml:id="formula_16">? ? = ? -?? ? L ????? (? * (? ), ? ).<label>(8)</label></formula><p>However, evaluating the gradient w.r.t. ? exactly is computationally prohibitive, since it requires solving for the optimal ? * (? ) whenever ? gets updated. To approximate the optimal solution ? * (? ), we propose to take one step of gradient descent update for ?, without solving the lower-level optimization completely by training until convergence. The full derivation is delegated to Appendix C. Here, we directly present the final result:</p><formula xml:id="formula_17">? ? L ????? (? * (? ), ? ) ? ? ? L ????? (? ? , ? ) -? ? ? L ????? (? + , ? ) -? ? L ????? (? -, ? ) 2? ,<label>(9)</label></formula><p>where ? ? = ? ? ?? ? ? L ????? (? ? (? ), ? ), and ? is a small scalar for finite difference approximation. By alternating between the two update rules in Eq. ( <ref type="formula" target="#formula_15">7</ref>) and Eq. ( <ref type="formula" target="#formula_16">8</ref>), we can learn an effective personalized subgraph selector that generalizes well for unseen edges. Although an optimizer with the theoretical guarantee of convergence for the bi-level optimization problem in Eq. ( <ref type="formula" target="#formula_14">6</ref>) remains an open challenge, alternating gradient descent algorithm has been widely adopted to solve similar objectives in Bayesian optimization <ref type="bibr" target="#b31">[32]</ref>, automatic differentiation <ref type="bibr" target="#b32">[33]</ref>, and adversarial training <ref type="bibr" target="#b33">[34]</ref>. Algorithm (1) in Appendix depicts the optimization procedure of our model. It shows some level of empirical convergence as seen in Figure <ref type="figure">8</ref> of Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APPLICATION PHASE</head><p>After the search phase, we can apply the selected subgraphs of different edges to various GNNLP models. In this section, we elaborate on two scenarios as examples. First, we illustrate how to train node2link-based models based on the selected edge subgraphs. Second, we show how to train subgraph2link-based methods given the sampled subgraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Personalized node2link Based Models</head><p>Typical examples under the node2link approach include GAE, Graph-SAGE, LightGCN, and NGCF, to name a few. Given the subgraph G ?,? = (G ? ? , G ? ? ) of edge ? ?,? , the models continue to learn node representations of end node subgraphs using GNNs, and then combine the representations of end nodes as the edge embedding towards prediction. In traditional settings, ? = ? = ?, the edge representation can be easily generated by concatenating embeddings of end nodes in the last GNN layer, i.e., z  We can observe that the three GNN layers will be updated inconsistently in mini-batch training, since the number of edges being encoded across three layers is different. To eliminate this issue, inspired by the success of the pre-training models in texts <ref type="bibr" target="#b34">[35]</ref> and images <ref type="bibr" target="#b35">[36]</ref>, we adopt the pre-train&amp;finetune fashion to train node2link methods in personalized subgraph setting. By initializing the GNN encoder of the application model with the pre-trained one in the search phase, the application model can be well-tuned with limited training samples (a.k.a. limited training edges in the third layer). Note that the subgraph imbalance issue is different from class imbalance problem <ref type="bibr" target="#b36">[37]</ref> in standard machine learning, since the training subgraphs in deeper layers are dependent on previous layers, which makes up-sampling or down-sampling techniques not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Personalized subgraph2link Based Models</head><p>Different from node2link-based methods, subgraph2link approaches naturally take the subgraphs of anchor edges as input, since they treat edge embedding as a graph-level representation learning task. Therefore, the personalized edge subgraphs generated by our model can be directly fed to them as input without additional effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We try to answer five research questions through experiments. Q1: Is considering personalized edge subgraphs beneficial for GNNLP models when evaluated on different applications? Q2: How effective is the proposed personalized subgraph selector in identifying edge subgraphs across various datasets? Q3: How will our personalized subgraph selector react to the changes in different optimization strategies? Q4: What are the impacts of hyperparameters on PS2, such as the maximum hop number ? and the embedding dimension ? of the score function? Q5: What is the running complexity of our personalized subgraph selector compared with standard GNNLPs?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Experiment Settings</head><p>Datasets. For a comprehensive comparison, we use nine datasets of diverse nature with both homogeneous and heterogeneous graphs.</p><p>For homogeneous graphs, we consider six popular datasets including three benchmark Planteoid datasets (Cora, CiteSeer, and PubMed <ref type="bibr" target="#b37">[38]</ref>) and three large-scale benchmark datasets, ogblddi, ogbl-collab, and ogbl-ppa from Open Graph Benchmark (OGB) <ref type="bibr" target="#b38">[39]</ref>. We summarize their statistics in Table <ref type="table" target="#tab_10">4</ref> of Appendix.</p><p>For heterogeneous graphs, we include three benchmark datasets including Gowalla <ref type="bibr" target="#b39">[40]</ref>, Yelp2018 <ref type="bibr" target="#b15">[16]</ref>, and Amazon-book <ref type="bibr" target="#b40">[41]</ref>. We summarize their statistics in Table <ref type="table" target="#tab_9">5</ref> of Appendix.</p><p>Learning protocols. We aim to provide a rigorous and fair comparison between different models across various graph domains by following the standard dataset splits and training procedure. For homogeneous graphs, we follow <ref type="bibr" target="#b13">[14]</ref> to randomly split three graphs in Planetoid datasets into three sets, i.e., the training set (85%), the validation set (5%), and the test set (10%), and measure model performance based on AUC and Average Precision (AP) scores. For OGB datasets (ogbl-ddi, ogbl-collab, and ogbl-ppa), we follow <ref type="bibr" target="#b41">[42]</ref> to split the datasets into three sets according to the split ratio summarized in Table <ref type="table" target="#tab_10">4</ref>, and evaluate the performance using Hit rate (Hit@? ), where ? is the number of nodes recalled. For heterogeneous graphs, we follow <ref type="bibr" target="#b16">[17]</ref> to generate the training, validation, and testing sets with split ratios in Table <ref type="table" target="#tab_9">5</ref>. Since it is too time-consuming to rank all items for every user during evaluation, we follow the common strategy <ref type="bibr" target="#b42">[43]</ref> that randomly samples 100 items that are not interacted with by the user, ranking the test item among the sampled items.</p><p>The performance of the ranked list is judged by two widely-used evaluation metrics: hit@? and ndcg@? . Baselines. To demonstrate the effectiveness, we compare our model with state-of-the-art link prediction methods of two domains. For homogeneous graphs, we include two popular node2link based methods (GAE <ref type="bibr" target="#b13">[14]</ref> and GraphSage <ref type="bibr" target="#b14">[15]</ref>) and one subgraph2link based method (SEAL <ref type="bibr" target="#b17">[18]</ref>). For heterogeneous graphs, we consider two recently proposed benchmark methods (NGCF <ref type="bibr" target="#b15">[16]</ref> and Light-GCN <ref type="bibr" target="#b16">[17]</ref>). Besides, we include one variant of our model based on the random search, named "RS". For all baseline methods, we use their open-source implementations with the best configurations on datasets that are tested in original papers. For datasets not originally tested, we tune their hyperparameters according to the range suggested in original papers. Implementation details. Our model is built upon the Pytorch platform. We train our model for 100 epochs with Adam optimizer and early stopping with patience of 20 epochs. Following common practice in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42]</ref>, we employ a three-layer GNN encoder with dimension 32, 256, and 64 for the Planetoid, OGB, and heterogeneous datasets, respectively. When applying our personalized subgraph selector PS2 to node2link-based baselines (GAE, GraphSage, NGCF, and LightGCN), we use the same GNN architectures as the vanilla counterparts in the search phase. For subgraph2link-based baseline (SEAL), we employ GCN <ref type="bibr" target="#b27">[28]</ref> as the backbone in the search phase, since it is memory and time expensive to generate subgraph embeddings by pooling over the whole subgraph as SEAL does. Our model has two hyper-parameters, i.e., the maximum hop number ? and the hidden dimension ? of score function ? ? . We set ? = 3 by default and search ? within the set {64, 128, 256, 512, 1024}. The best options for three Planetoid and other datasets are 256 and 512, respectively. We provide more details in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with the Baselines</head><p>To answer the question Q1, we compare the performance of the proposed personalized subgraph selector with state-of-the-art baselines across homogeneous and heterogeneous domains. Table <ref type="table" target="#tab_5">1</ref>, Table <ref type="table" target="#tab_7">3</ref> and Table <ref type="table" target="#tab_6">2</ref> report the results over Planetoid, OGB, and  three recommendation datasets, respectively. From the tables, we have the following Observations.</p><p>Obs. 1. Through edge subgraph personalization, PS2 boosts the performance of link prediction across different domains. By comparing classical GNNLP methods (GAE, GraphSage, SEAL, NGCF, and LightGCN) with our personalized subgraph selector (GAE-PS2, GraphSage-PS2, SEAL-PS2, NGCF-PS2, and LightGCN-PS2), our model consistently outperforms the vanilla counterparts on both homogeneous and heterogeneous graphs (in Table <ref type="table" target="#tab_5">1</ref> and Table <ref type="table" target="#tab_6">2</ref>). Specifically, on homogeneous graphs (Table <ref type="table" target="#tab_5">1</ref>), GAE-PS2, GraphSage-PS2, and SEAL-PS2 achieve better results than GAE, GraphSage, and SEAL across two evaluation metrics. Our model has different impacts concerning various backbones. For example, GraphSage-PS2 significantly outperforms GraphSage with up to 9.6% improvements. In heterogeneous scenarios, NGCF-PS2 and LightGCN-PS2 generally perform better than NGCF and LightGCN on three datasets. In particular, the performance gap between our model and two baselines increases on top-10 based metrics. This result verifies the effectiveness of our model in accurately recalling related items in the top-ranking list.</p><p>Obs. 2. Across various datasets, the proposed personalized subgraph selector consistently outperforms random searchbased variants. For different datasets and scenarios (in Table <ref type="table" target="#tab_5">1</ref> and<ref type="table" target="#tab_6">Table 2</ref>), our model consistently outperforms the random search based variants with a large margin. Specifically, random searchbased variants are not robust across various datasets. For example, GraphSage-RS could generally achieve better or comparable results with their counterparts on Cora, CiteSeer, and PubMed datasets. But it loses to their counterparts on recommendation datasets in all cases (see Table <ref type="table" target="#tab_6">2</ref>). This comparison validates our motivation to design an automated subgraph selector in a data-driven fashion. Obs. 3. The proposed PS2 scales up well on large-scale datasets. On three challenging OGB datasets, our model PS2 can continuously boost the performance of vanilla GNNLP methods, as shown in Table <ref type="table" target="#tab_7">3</ref>. Specifically, GAE-PS2 improves 60.5%, 14.5%, and 9.8% over GAE on ogbl-ddi, ogbl-collab, and ogbl-ppa datasets, respectively. In contrast, the random search-based variants lose to their counterparts on these three datasets. This observation further demonstrates the effectiveness of considering learnable subgraph selection on large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Subgraph Distribution Analysis</head><p>We visualize the learned subgraph distributions of GAE-PS2 on all datasets in Figure <ref type="figure">4</ref> and Figure <ref type="figure">7</ref> in Appendix to study (Q2). By comparing the distributions across different benchmarks, we have the following observation.</p><p>Obs. <ref type="bibr" target="#b3">4</ref>. By learning from the data, PS2 can effectively learn different subgraph distributions for various datasets, and even  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Optimization Analysis</head><p>To examine the influence of optimization strategies on PS2 (Q3), we compare the default search &amp; apply paradigm with the joint learning schema on node2link based models. Here, jointly learning means we directly train PS2 with a downstream inference model, i.e., GAE, end-to-end. In this setting, the PS2 training still uses the mixed selection, while the GAE optimization exploits the most likely subgraph via maximum discrete selection. Figure <ref type="figure" target="#fig_3">5</ref> shows the results of two settings on GAE-PS2 over all datasets. We can observe that although joint learning strategy achieves comparable results with search &amp; apply schema on Cora, CiteSeer, and PubMed datasets, the later schema performs better on the other six largescale datasets. The possible reason is that joint learning is hard to optimize since the personalized selector and the downstream model are entangled. This comparison validates our choice to adopt the search &amp; apply fashion similar to the AutoML <ref type="bibr" target="#b26">[27]</ref> domain.</p><p>Besides, we also explore the effectiveness of finetune strategy to avoid the subgraph imbalance issue when applying PS2 for node2link-based methods. Table <ref type="table">6</ref> in Appendix reports the results on Planetoid datasets. Similar observations could be made on other datasets. From Table <ref type="table">6</ref>, we observe that finetune strategy outperforms training from scratch on GAE and GraphSage backbones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Hyperparameter Analysis</head><p>To answer Q4, we study the impact of ? and hidden dimension ? of the score function on PubMed. Figure <ref type="figure" target="#fig_4">6</ref> shows the results on GAE-PS2 and GraphSage-PS2. From the two subfigures, we can observe that our model performs relatively stable over a wide range of combinations of ? and ?. Specifically, the best results in two cases are achieved when ? and ? are around 3 and 256, respectively. Similar observations are obtained on other datasets. In experiments, we fix ? = 3 and set ? = 256 and ? = 512 for Planetoid and other datasets (OGB and recommendation), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Efficiency Analysis</head><p>To study Q5, we analyze the training costs of two representatives GNNLPs (GAE and SEAL) after plugging in our personalized selector. For SEAL, we exclude the sampling cost since it is far more than its forward pass running costs. From Table <ref type="table">7</ref> of Appendix, we observe that the additional costs to activate specific subgraph using our personalized selector is marginal, i.e., usually less than 20% running consumption. This is because our selector is simple MLPs, and we can directly generate subgraph embeddings based on GNN output, thanks to the embedding approximation trick in Section 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>In this paper, we mainly focus on graph neural networks (GNNs) based link prediction (GNNLP) techniques. For methods beyond GNN, please refer to <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b43">44]</ref> for a comprehensive review. For illustration purposes, the existing methods can be mainly divided into two categories: node2link <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b44">45]</ref> and subgraph2link <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref>. node2link is the classical approach to perform link prediction based on GNNs. Given a query edge, it works by first generating representations for two end entities based on their local subgraphs via the GNNs encoder, and then combining the two representations to estimate the edge existence probability. Some efforts have been made to predict missing edges for homogeneous graphs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, while several methods propose to tackle link prediction on heterogeneous graphs, such as recommendation systems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref> and knowledge graph completion <ref type="bibr" target="#b3">[4]</ref>. subgraph2link is a recently proposed new link prediction paradigm. The key idea is to represent each edge with a subgraph around it, and then apply GNNs to learn representation for the whole subgraph. The pioneering work of <ref type="bibr" target="#b17">[18]</ref> adopts node labeling to first create structure-aware features for nodes in the subgraph, and then pool over the node representations obtained by GNNs to get the final edge representation. A follow-up work <ref type="bibr" target="#b18">[19]</ref> analyzes the impacts of different labeling techniques. Another recent work <ref type="bibr" target="#b20">[21]</ref> proposes to replace the pooling operation with a more advanced yet complicated random-walk-based pooling strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we explore a new perspective to train link prediction models by considering edge personalization in terms of neighborhood subgraphs. Specifically, we propose an effective personalized subgraph selector (PS2) as a plug-and-play framework for the graph neural network based link prediction (GNNLP) community. PS2 can automatically and inductively identify optimal subgraph orders for different edges when performing GNNLP. Extensive experiments on multiple datasets with various domains and scales demonstrate the superiority of PS2 against diverse GNNLP backbones. In the future, we will extend PS2 to perform subgraph order selection and critical neighbor sampling within the selected subgraph jointly.  A DATASET DETAILS</p><p>In this section, we introduce the details of applied datasets as below.</p><p>? Cora, CiteSeer, and PubMed: They are the most popular benchmark citation networks used in the graph domain.</p><p>Nodes correspond to documents and edges correspond to citations. Each node has a bag-of-words feature vector according to the paper abstract. Labels are defined as academic topics.</p><p>? ogbl-ddi: This is a drug-drug interaction network. Each node represents an FDA-approved or experimental drug. Edges represent interactions between drugs. Node features are not available, in experiments, following <ref type="bibr" target="#b38">[39]</ref>, we randomly initialize a 256-dimensional embedding vector for each node. ? ogbl-collab: This is a challenging author collaboration network from KDD Cup 2021. Each node is an author and edges indicate the collaboration between authors. All nodes come with 128-dimensional features, obtained by averaging the word embeddings of papers that are published by the authors. ? ogbl-ppa: This is a protein-protein association network.</p><p>Nodes represent proteins from 58 different species, and edges indicate biologically meaningful associations between proteins. In experiments, we use the 58-dimensional one-hot vectors as node features. In addition to the aforementioned six homogeneous graphs, we also consider three popular recommendation datasets.</p><p>? Gowalla: This is the check-in dataset obtained from Gowalla, where users share their locations by checking-in. To ensure the qualify of the dataset, following <ref type="bibr" target="#b15">[16]</ref>, we use the 10core setting, i.e., retaining users and items with at least ten interactions. ? Yelp2018: This dataset is adopted from the 2018 edition of the Yelp challenge. It describes the relationships between customers and items like restaurants and bars. We use the same 10-core setting in order to ensure data quality. ? Amazon-book 1 : This is one of the widely used datasets for product recommendation. Similarly, we use the 10-core setting to ensure that each user and item have at least ten interactions.</p><p>We split all datasets above into the training/validation/testing sets according to common practice <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b38">39]</ref> and the specific splitting ratios are summarized in Table <ref type="table" target="#tab_10">4</ref> and<ref type="table" target="#tab_9">5.</ref> 1 https://jmcauley.ucsd.edu/data/amazon/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B MODEL DETAILS</head><p>In this section, we provide more details of the proposed PS2 methods from the neural architecture, hyper-parameter, and hardware perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Details of the Neural Architecture</head><p>Recall that our model consists of a personalized subgraph selector ? ? , the GNN encoder ? ? , and the link predictor ? ? . The personalized subgraph selector is parameterized by a two-layer MLP with hidden dimension ? and output dimension 1. The GNN encoder is a ?-layer GCN <ref type="bibr" target="#b27">[28]</ref> module, which varies from different downstream models. For example, when combining our PS2 with GAE and GraphSage, the default GNN module is GCN <ref type="bibr" target="#b27">[28]</ref> and SAGE <ref type="bibr" target="#b14">[15]</ref>. The link predictor is initialized as another three-layer MLPs. The hidden activation function in all neural networks is ReLU.</p><p>Algorithm 1: Alternating Gradient Descent for Eq. ( <ref type="formula" target="#formula_14">6</ref>)</p><p>Input: Initial subgraph selector parameters ? and initial weight parameters ?. 1 while not converge do  2. Lower-level optimization: Fix ? , update weights parameters ? by descending ? ? L ????? (?, ? ).</p><p>4 Return Derive the optimal subgraphs for different edges based on the learned ? and ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Hyperparameter Configuration</head><p>To provide a fair comparison with state-of-the-art link prediction methods, we generally follow the same parameter settings across different baselines in terms of two different applications. In general, our model is optimized based on minibatch training. Following common practice for link prediction training, in each step, we sample a minibatch of positive edges from the training loader and then randomly generate one negative sample for each positive edge to construct the minibatch training set. Notice that we don't conduct subgraph sampling for node representation as done in <ref type="bibr" target="#b14">[15]</ref>. We feed the whole adjacency matrix into the model for graph convolution. Specifically, for Planetoid datasets (Cora, CiteSeer, and PubMed), we adopt a three-layer GNN module with dimension 32. We set the batch size to 1024 and fixed the learning rate to 0.01. For OGB datasets, we adopt a three-layer GNN with the hidden dimension 256. The learning rate and batch size are fixed at 0.001 and 10 * 1024 as suggested in <ref type="bibr" target="#b38">[39]</ref> <ref type="foot" target="#foot_0">2</ref> . For recommendation datasets, we adopt a three-layer GNN with the hidden dimension 64 according to <ref type="bibr" target="#b16">[17]</ref>. The batch size and learning rate are fixed as 1024 and 0.001, respectively. For different datasets, we search the hidden dimension ? of</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 1 , 1 )Figure 1 :</head><label>111</label><figDesc>Figure 1: The effect of different edge subgraphs towards 6 randomly sampled test edges on the Cora dataset. The results are obtained by training GAE<ref type="bibr" target="#b13">[14]</ref> nine times with different subgraph ranges. For example, (2, 3) means the subgraph of an edge is composed of the 2-hops and 3-hops of neighbors of its head and tail nodes, respectively. The X-axis denotes different subgraphs selection strategies, and the Yaxis is the id of the sampled edges. The color from light to dark represents the predicted probability for edge existence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The training pipeline (i.e., search phase) of the proposed PS2. The subgraph selector generates an importance score for each subgraph candidate. The upper-level optimization updates the parameters of the selector ? ? . The lower-level optimization updates the parameters of the GNN ? ? and predictor ? ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example illustration of the subgraph imbalance issue. The frequency of updating three GNN layers differs due to the imbalanced edge distribution across layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The performance of GAE-PS2 under different optimization paradigms. The evaluation metrics for (Cora, Cite-Seer, and Pubmed) and other datasets are AUC and Hit ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Hyperparameter study of PS2 on BlogCatalog with different base models: GAE (left) and GraphSage (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 1 .</head><label>1</label><figDesc>Upper-level optimization: Fix ?, update subgraph selector ? by descending ? ? L ??? (? -?? ? L ????? (?, ? ), ? ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3</head><label>3</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>? = arg max ? ? ,? ? ? ? ? ,? ? ?,? . Although Eq. (2) makes our personalized subgraph selection learnable, it still cannot resolve the second challenge. This is because it only learns weight vectors {? ?,? : ? ?,? ? E} for observed edges, yet cannot generate weight variables for unseen ones. As a result, it cannot be applied to infer missing edges.3.2.2 Inductive SubgraphSelector. To address the second challenge, we propose to make Eq. (2) inductive by computing the weight vector ? ? with a deep neural network. Specifically, we estimate the contribution score ? ?,? ?,? of edge ? ?,? w.r.t. the subgraph G Here ? ? takes edge subgraphs as input and outputs their selection scores. Following this principle, we rewrite Eq. 2 into an inductive version:</figDesc><table><row><cell>?,?</cell></row><row><cell>?,?</cell></row><row><cell>?,?</cell></row><row><cell>if ? ?,?,?</cell></row><row><cell>?,? via a</cell></row><row><cell>score function ?</cell></row></table><note><p>? : G ?,? -? R.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>? } of an edge ? ?,? as input, an intuitive strategy is applying GNNs to encode the ? 2 subgraphs independently. However, this schema is time-consuming in training because it requires roughly ? 2 GNNs forward passes to iterate over all edges once. Efficient ? 2 subgraphs embedding. To avoid running GNNs forward pass repeatedly, we propose to get the representations of ? 2 edge subgraphs simultaneously, by directly combining the hidden representations of end nodes from ? GNN layers. Specifically, given the ? hidden representations of node ? {h ? ? : 1 ? ? ? ? } and ? {h ? ? : 1 ? ? ? ? }, we generate the representation of edge ? ?,? in terms of subgraph (G ? ? , G</figDesc><table><row><cell>? ? ) as z ?,? ?,? = COM(h ? ? , h</cell></row></table><note><p>On the one hand, it allows efficient subgraph selection for different edges based on the simple forward pass of a neural network. On the other hand, it enables the selection of the most informative subgraphs for unseen edges based on their subgraph characteristics. We now illustrate how to implement the score function ? ? . Since ? ? takes ? 2 subgraphs {G ?,? ?,? : 1 ? ?, ? ? ? ?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Link prediction results on Planetoid data. "+"/"-" in the bracket indicates relative improvement with baselines.</figDesc><table><row><cell></cell><cell>Cora</cell><cell></cell><cell>CiteSeer</cell><cell></cell><cell>PubMed</cell><cell></cell></row><row><cell></cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell></row><row><cell>GAE</cell><cell>91.08 ? 0.01</cell><cell>92.03 ? 0.03</cell><cell>89.52 ? 0.04</cell><cell>89.95 ? 0.05</cell><cell>96.40 ? 0.01</cell><cell>96.50 ? 0.02</cell></row><row><cell>GAE-RS</cell><cell cols="6">89.38 ? 0.23(-1.8%) 91.66 ? 0.45(-0.4%) 89.81 ? 0.29(0.3%) 91.39 ? 0.22(1.6%) 95.34 ? 0.23(-1.1%) 96.13 ? 0.18(-0.3%)</cell></row><row><cell>GAE-PS2</cell><cell cols="6">92.25 ? 0.71(+1.4%) 93.60 ? 0.25(+1.7%) 92.16 ? 0.19(+3.0%) 93.07 ? 0.06(+3.6%) 98.27 ? 0.10(+2.0%) 98.10 ? 0.16(+1.7%)</cell></row><row><cell>GraphSage</cell><cell>86.36 ? 1.06</cell><cell>88.22 ? 0.87</cell><cell>85.24 ? 2.56</cell><cell>86.60 ? 2.54</cell><cell>87.61 ? 0.87</cell><cell>89.41 ? 0.82</cell></row><row><cell cols="7">GraphSage-RS 89.27 ? 0.70(+3.3%) 89.67 ? 0.58(+1.6%) 88.30 ? 1.08(+3.5%) 89.23 ? 0.91(+3.0%) 88.72 ? 0.77(+1.3%) 90.58 ? 0.75(+1.3%)</cell></row><row><cell cols="7">GraphSage-PS2 93.86 ? 0.01(8.7%) 93.35 ? 0.25(5.8%) 93.46 ? 1.20(9.6%) 93.62 ? 1.10(8.1%) 93.47 ? 0.22(6.7%) 93.52 ? 0.25(4.6%)</cell></row><row><cell>SEAL</cell><cell>90.82 ? 1.97</cell><cell>92.18 ? 0.82</cell><cell>88.49 ? 1.22</cell><cell>90.64 ? 1.46</cell><cell>97.57 ? 0.05</cell><cell>97.20 ? 0.03</cell></row><row><cell>SEAL-RS</cell><cell cols="6">88.55 ? 0.88(-2.5%) 90.37 ? 0.36(-1.9%) 86.70 ? 1.34(-2.0%) 88.61 ? 1.62(-2.2%) 94.39 ? 0.35(-3.2%) 94.08 ? 0.27(-3.2%)</cell></row><row><cell>SEAL-PS2</cell><cell cols="6">92.31 ? 0.31(+1.6%) 93.53 ? 0.22(+1.4%) 90.29 ? 0.36(+2.0%) 92.42 ? 0.20(+1.9%) 97.60 ? 0.10(+0.0%) 97.44 ? 0.05(+0.2%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Link prediction results on recommendation datasets. RS 85.85 ? 0.39(-1.3%) 66.69 ? 0.20(-2.1%) 81.72 ? 0.57(-0.9%) 57.88 ? 0.35(-1.8%) 73.75 ? 0.38(-0.8%) 53.51 ? 0.18(-2.3%) LightGCN-PS2 89.79 ? 0.25(+3.3%) 69.75 ? 0.20(+2.4%) 88.44 ? 0.32(+7.3%) 59.99 ? 0.19(+1.8%) 81.26 ? 0.26(+9.3%) 59.29 ? 0.33(+8.2%)</figDesc><table><row><cell></cell><cell>Gowalla</cell><cell></cell><cell>Yelp</cell><cell></cell><cell cols="2">Amazon-book</cell></row><row><cell></cell><cell>Hit@10</cell><cell>ndgc@50</cell><cell>Hit@10</cell><cell>ndgc@50</cell><cell>Hit@10</cell><cell>ndgc@50</cell></row><row><cell>NGCF</cell><cell>85.16 ? 0.58</cell><cell>64.20 ? 0.34</cell><cell>80.17 ?</cell><cell>55.12 ? 0.44</cell><cell>70.15 ? 0.59</cell><cell>50.63 ? 0.64</cell></row><row><cell>NGCF-RS</cell><cell cols="6">83.55 ? 0.47(-1.8%) 62.74 ? 0.40(-2.2%) 78.07 ? 0.53(-2.6%) 52.90 ? 0.32(-4.0%) 67.82 ? 0.65(-3.3%) 48.02 ? 0.81(-5.1%)</cell></row><row><cell>NGCF-PS2</cell><cell cols="6">88.21 ? 0.24(+3.6%) 65.59 ? 0.60(+1.4%) 87.72 ? 0.35(+9.4%) 56.61 ? 0.27(+2.7%) 78.66 ? 0.30(+12.1%) 51.43 ? 0.41(+1.6%)</cell></row><row><cell>LightGCN</cell><cell>86.96 ? 0.46</cell><cell>68.11 ? 0.18</cell><cell>82.44 ? 0.37</cell><cell>58.94 ? 0.28</cell><cell>74.32 ? 0.36</cell><cell>54.79 ? 0.24</cell></row><row><cell>LightGCN-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Link prediction performance on OGB datasets.</figDesc><table><row><cell></cell><cell cols="3">ogbl-ddi ogbl-collab cogbl-ppa</cell></row><row><cell></cell><cell>Hit@20</cell><cell>Hit@50</cell><cell>Hit@100</cell></row><row><cell>GAE</cell><cell cols="3">37.07 ? 5.07 44.75 ? 1.07 18.67 ? 1.32</cell></row><row><cell>GAE-RS</cell><cell cols="3">30.72 ? 6.86 34.51 ? 0.68 12.65 ? 1.22</cell></row><row><cell>GAE-PS2</cell><cell cols="3">49.53 ? 5.99 50.26 ? 0.32 20.50 ? 0.79</cell></row><row><cell>GraphSage</cell><cell cols="3">53.90 ? 4.74 54.63 ? 1.12 16.55 ? 2.40</cell></row><row><cell cols="4">GraphSage-RS 27.17 ? 5.74 37.54 ? 0.37 9.89 ? 3.46</cell></row><row><cell cols="4">GraphSage-PS2 56.90 ? 5.32 55.71 ? 0.93 17.88 ? 1.33</cell></row><row><cell>SEAL</cell><cell cols="3">30.56 ? 3.86 63.64 ? 0.71 48.80 ? 3.16</cell></row><row><cell>SEAL-RS</cell><cell cols="3">24.58 ? 4.65 43.56 ? 1.30 35.68 ? 5.21</cell></row><row><cell>SEAL-PS2</cell><cell cols="3">32.77 ? 2.50 64.83 ? 0.54 50.25 ? 2.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Subgraph distribution of GAE-PS2 on the Plantetoid. skip some suboptimal subgraphs. Our model PS2 can identify different subgraphs for different edges, and allow different datasets to have their own subgraph distributions (see Figure4). Specifically, the subgraph distribution on OGB datasets is more sparse than the other two types of datasets, while recommendation datasets generally tend to have smoother distribution. One promising property of PS2 is that it can skip some subgraphs if they are not optimal for any edges. For example, no edges are assigned to the subgraph (2, 2) on ogbl-ddi and ogbl-ppa datasets.</figDesc><table><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cora</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CiteSeer</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PubMed</cell></row><row><cell>Frequency</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(1,1)</cell><cell cols="2">(1,2)</cell><cell cols="2">(1,3)</cell><cell>(2,1)</cell><cell cols="2">(2,2)</cell><cell>(2,3)</cell><cell>(3,1)</cell><cell>(3,2)</cell><cell>(3,3)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Subgraph</cell></row><row><cell cols="3">0 20 40 60 80 100 Figure 4: co ra Performance (%)</cell><cell cols="2">ci te .</cell><cell cols="2">pu bm .</cell><cell>dd i</cell><cell cols="2">co lla b joint learning pp a search &amp; apply go w al la</cell><cell>ye lp</cell><cell>am az on</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Datasets</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Statistics of heterogeneous graph datasets.</figDesc><table><row><cell>Data</cell><cell># User # Item</cell><cell># Edges</cell><cell>Split ratio</cell></row><row><cell>Gowalla</cell><cell cols="3">29, 858 40, 981 1, 027, 370 70/10/20</cell></row><row><cell>Yelp2018</cell><cell cols="3">31, 668 38, 048 1, 561, 406 70/10/20</cell></row><row><cell cols="4">Amazon-Book 52, 643 91, 599 2, 984, 108 70/10/20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Statistics of homogeneous graph datasets.</figDesc><table><row><cell>Data</cell><cell># Nodes</cell><cell># Edges</cell><cell cols="2"># Features Split ratio</cell></row><row><cell>Cora</cell><cell>2, 708</cell><cell>5, 429</cell><cell>1, 433</cell><cell>85/5/15</cell></row><row><cell>CiteSeer</cell><cell>3, 312</cell><cell>4, 660</cell><cell>3, 703</cell><cell>85/5/15</cell></row><row><cell>PubMed</cell><cell>19, 717</cell><cell>44, 338</cell><cell>500</cell><cell>85/5/15</cell></row><row><cell>ogbl-ddi</cell><cell>4, 267</cell><cell>1, 334, 889</cell><cell>-</cell><cell>80/10/10</cell></row><row><cell cols="2">ogbl-collab 235, 868</cell><cell>1, 285, 465</cell><cell>128</cell><cell>92/4/4</cell></row><row><cell>ogbl-ppa</cell><cell cols="2">576, 289 30, 326, 273</cell><cell>58</cell><cell>70/20/10</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/snap-stanford/ogb/tree/master/examples/linkproppred</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank the anomalous reviewers for the feedback. The work is, in part, supported by <rs type="funder">NSF</rs> (<rs type="grantNumber">IIS-1849085</rs>, IIS-1750074, IIS-2006844). The views and conclusions in this paper are those of the authors and should not be interpreted as representing any funding agencies.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TmmbjQ2">
					<idno type="grant-number">IIS-1849085</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Hardware</head><p>We conduct all the experiments on a server with 48 Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz processors, 188 GB memory, and four NVIDIA GeForce RTX 3090 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C GRADIENT APPROXIMATION FOR UPPER-LEVEL OPTIMIZATION</head><p>With ? fixed, the upper-level optimization updates ? according to the validation performance as:</p><p>However, evaluating the gradient w.r.t. ? exactly is computationally prohibitive, since it requires solving for the optimal ? * (? ) whenever ? gets updated. To approximate the optimal solution ? * (? ), we propose to take one step of gradient descent update for ?, without solving the lower-level optimization completely by training until convergence. Applying the chain rule, the approximated gradient yields: <ref type="bibr" target="#b10">(11)</ref> where ? ? = ? -?? ? L ????? (?, ? ) is the weight for one-step forward model. The second term in Eq. ( <ref type="formula">11</ref>) contains an expensive matrixvector product, which requires ? (|? ||? |) complexity. To further accelerate the optimization, we approximate the second term using the finite difference approximation, defined as:</p><p>Based on this approximation, we only need two forward passes for ? and two backward passes for ? , therefore, the complexity is reduced from ? </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Progresses and challenges in link prediction</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11472</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of different biological data and computational classification methods for use in protein interaction prediction</title>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziv</forename><surname>Bar-Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Klein-Seetharaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="490" to="500" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Drug response prediction as a link prediction problem</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Stanfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Co?kun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><surname>Koyut?rk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A survey on graph neural networks for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12374</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Active ensemble learning for knowledge graph error detection</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Junnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Friends and neighbors on the web</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eytan</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="230" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep representation learning for social network analysis</title>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic memory based attention network for sequential recommendation</title>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4384" to="4392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwei-Herng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhargav</forename><surname>Bhushanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Kejariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Dreamshard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02023</idno>
		<title level="m">Generalizable embedding table placement for recommender systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph neural networks for recommender systems</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingrong</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghua</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12843</idno>
	</analytic>
	<monogr>
		<title level="m">Challenges, methods, and directions</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Line graph neural networks for link prediction</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Renchi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokui</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourav</forename><forename type="middle">S</forename><surname>Bhowmick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06826</idno>
		<title level="m">Homogeneous network embedding for massive graphs via reweighted personalized pagerank</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5165" to="5175" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Labeling trick: A theory of using graph neural networks for multi-node representation learning</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to hash with graph neural networks for recommender systems</title>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="1988">1988-1998, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Neural link prediction with walk pooling</title>
		<author>
			<persName><forename type="first">Liming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Dokmani?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04375</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Link prediction in complex networks: A survey</title>
		<author>
			<persName><forename type="first">Linyuan</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: statistical mechanics and its applications</title>
		<imprint>
			<biblScope unit="volume">390</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1150" to="1170" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Graph contrastive learning with personalized augmentation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.06560</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Link creation and information spreading over social and communication ties in an interest-based online social network</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Aiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ciro</forename><surname>Barrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rossano</forename><surname>Cattuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giancarlo</forename><surname>Schifanella</surname></persName>
		</author>
		<author>
			<persName><surname>Ruffo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EPJ Data Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Is a single vector enough? exploring node polysemy for network embedding</title>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="932" to="940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sparse-interest network for sequential recommendation</title>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="598" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Darts</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4780" to="4789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards automated imbalanced learning with deep hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwei-Herng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><forename type="middle">Ben</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2476" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Towards a unified min-max framework for adversarial exploration and robustness</title>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makan</forename><surname>Fardad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03563</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pretraining of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Class imbalance problem in data mining review</title>
		<author>
			<persName><forename type="first">Rushi</forename><surname>Longadge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snehalata</forename><surname>Dongre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1305.1707</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Ogb-lsc: A large-scale challenge for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maho</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09430</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling user exposure in recommendation</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Mcinerney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="951" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural collaborative filtering. In WWW</title>
		<imprint>
			<biblScope unit="page" from="173" to="182" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Link prediction techniques, applications, and performance: A survey</title>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheshar</forename><surname>Shashank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">553</biblScope>
			<biblScope unit="page">124289</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">S2gae: Self-supervised graph autoencoders are gen-eralizable learners with graph masking</title>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soo-Hyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="xx" to="xx" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04407</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Structure enhanced graph neural networks for link prediction</title>
		<author>
			<persName><forename type="first">Baole</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Zhou Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.05293</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Graph neural networks in recommender systems: a survey</title>
		<author>
			<persName><forename type="first">Shiwen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02260</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal augmented graph neural networks for session-based recommendations</title>
		<author>
			<persName><forename type="first">Huachi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoling</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1798" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
