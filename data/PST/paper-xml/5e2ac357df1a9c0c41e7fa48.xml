<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Building attention and edge message passing neural networks for bioactivity and physical-chemical property prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">M</forename><surname>Withnall</surname></persName>
							<idno type="ORCID">0000-0002-9706-8698</idno>
						</author>
						<author role="corresp">
							<persName><forename type="first">E</forename><surname>Lindel?f</surname></persName>
							<email>edvardlindelof@gmail.com</email>
							<idno type="ORCID">0000-0002-9706-8698</idno>
						</author>
						<author>
							<persName><forename type="first">O</forename><surname>Engkvist</surname></persName>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation>
								<orgName>1 Hit Discovery, Discovery Sciences, </orgName>
								<address><addrLine>R&amp;D, AstraZeneca, Gothenburg, Sweden R&amp;D, AstraZeneca, Gothenburg, Sweden. 190 Kai Yuan Avenue, Science Park, Guangzhou, China.</addrLine></address>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 1 Hit Discovery, Discovery Sciences, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 2 Centre of Chemistry and Chemical Biology, Guangzhou Regenerative Medicine and Health-Guangdong Laboratory,</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Building attention and edge message passing neural networks for bioactivity and physical-chemical property prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1186/s13321-019-0407-y</idno>
					<note type="submission">Received: 17 September 2019 Accepted: 25 December 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Message passing neural network</term>
					<term>Graph convolution</term>
					<term>Virtual screening</term>
					<term>Machine learning</term>
					<term>Deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Message Passing for graphs is a promising and relatively recent approach for applying Machine Learning to networked data. As molecules can be described intrinsically as a molecular graph, it makes sense to apply these techniques to improve molecular property prediction in the field of cheminformatics. We introduce Attention and Edge Memory schemes to the existing message passing neural network framework, and benchmark our approaches against eight different physical-chemical and bioactivity datasets from the literature. We remove the need to introduce a priori knowledge of the task and chemical descriptor calculation by using only fundamental graph-derived properties. Our results consistently perform on-par with other state-of-the-art machine learning approaches, and set a new standard on sparse multi-task virtual screening targets. We also investigate model performance as a function of dataset preprocessing, and make some suggestions regarding hyperparameter selection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>QSAR (Quantitative Structure Activity Relationships) have been applied for decades in the development of relationships between physicochemical properties of chemical substances and their biological activities to obtain a reliable mathematical and statistical model for prediction of the activities of new chemical entities. The major aim of QSAR study is to reduce the number of compounds synthesized during the drug development, a notoriously long and costly process, hence the desire to improve its efficiency from a drug discovery perspective. After Hansch proposed the QSAR concept <ref type="bibr" target="#b3">[1]</ref>, engineering molecular descriptors to build accurate models for the prediction of various properties has become the standard approach to QSAR modelling. Researchers <ref type="bibr" target="#b4">[2]</ref><ref type="bibr" target="#b5">[3]</ref><ref type="bibr" target="#b6">[4]</ref><ref type="bibr" target="#b7">[5]</ref><ref type="bibr" target="#b8">[6]</ref> have proposed numerous descriptors to represent molecular 2D and 3D structures, aiming to correlate these descriptors with predicted endpoints. Approaches to generating representations using the graph representation of a molecule include graph kernels <ref type="bibr" target="#b9">[7]</ref>, and perhaps most importantly in the present context, ECFP (Extended Connectivity Circular Fingerprints) <ref type="bibr" target="#b10">[8]</ref>. Once a descriptor set has been defined, various modelling methods, including linear mapping methods like linear regression, partial least square and non-linear methods like support vector machine, random forest etc., are applied to building models. Recently, deep neural network methods have become the latest weapon in a Cheminformatician's arsenal for doing QSAR.</p><p>Over the past decade, deep learning has become a staple in the machine learning toolbox of many fields and research areas <ref type="bibr" target="#b11">[9,</ref><ref type="bibr" target="#b12">10]</ref>. Notably in the pharmaceutical area, in recent years AI has shown incredible growth, and is being used now not just for bioactivity and physical-chemical property prediction, but also for de novo design, image analysis, and synthesis prediction, to name a few. This rapid growth is due in part to the substantial increase in available biochemical data thanks to the rise of techniques such as High Throughput Screening (HTS) and parallel synthesis, and also to the recent surge in parallel computational power that can be feasibly attained by harnessing General Purpose computing on Graphics Processing Units (GPGPU).</p><p>Efforts have also been taken to enable neural networks to do representation learning, i.e. the neural network is able to learn descriptors itself instead of relying on predefined molecular descriptors. Among these, the graph convolution network (GCN) is gaining popularity and various architectures have been proposed in data science community. The first Graph Neural Networks (GNNs) was put forward by <ref type="bibr">Gori et al. in 2005 [11]</ref>, presenting an architecture for learning node representations using recurrent neural networks capable of acting on directed, undirected, labelled, and cyclic graphs. This work was later expanded upon by Micheli <ref type="bibr" target="#b14">[12]</ref> and Scarselli et al. <ref type="bibr" target="#b15">[13]</ref> In 2013, the Graph Convolutional Network (GCN) was presented by Bruna et al. <ref type="bibr" target="#b16">[14]</ref> using the principles of spectral graph theory. Many other forms of GNN have been presented since then, including, but not limited to, Graph Attention Networks <ref type="bibr" target="#b17">[15]</ref>, Graph Autoencoders <ref type="bibr" target="#b18">[16]</ref><ref type="bibr" target="#b19">[17]</ref><ref type="bibr" target="#b20">[18]</ref><ref type="bibr" target="#b21">[19]</ref>, and Graph Spatial-Temporal Networks <ref type="bibr" target="#b22">[20]</ref><ref type="bibr" target="#b23">[21]</ref><ref type="bibr" target="#b24">[22]</ref><ref type="bibr" target="#b25">[23]</ref>.</p><p>In GCNs and some other forms of GNNs, information is propagated through a graph in a manner similar to how conventional convolutional neural networks (CNNs) treat grid data (e.g. image data). However, whilst graphbased deep learning shares some connection with CNNs with respect to local connectivity of the component data, CNNs exploit the properties of regular connectivity, shift-invariance, and compositionality to achieve their noteworthy performance. In order to cope with the irregularity of graph data, alternative approaches must be designed, most notably to circumvent the issue of irregular non-Euclidean data, and to be invariant to the graph representation.</p><p>Whilst many implementations are designed for use on a single large graph, such as social networks or citation graphs, approaches designed for use on multiple smaller graphs such as graphs of small molecule are also desired for their potential use in, amongst other things, drug design. Duvenaud <ref type="bibr" target="#b26">[24]</ref> proposed the neural fingerprint method, describing it as an analogue of ECFP, as one of the first efforts in applying graph convolution model on chemistry related problems. The notable advancement embodied in the neural fingerprint approach with regards to predecessing concepts such as graph kernels and ECFP, is that the generation of descriptors is adaptedlearned-during training. Other molecular graph convolution methods were reported by Kearnes et al. <ref type="bibr" target="#b27">[25]</ref> and Coley <ref type="bibr" target="#b28">[26]</ref> as extensions to Duvenaud's method. Recently researchers from Google <ref type="bibr" target="#b29">[27]</ref> put forward an new NN architecture called as message passing neural networks (MPNNs) and used the MPNNs to predict quantum chemical properties. The MPNN framework contains three common steps: (1) message passing step, where, for each atom, features (atom or bond features) from its neighbours are propagated, based on the graph structure, into a so called a message vector; (2) update step, where embedded atom features are updated by the message vector; (3) aggregation step, where the atomic features in the molecule are aggregated into the molecule feature vector. These molecule feature vector can then be used in a dense layer to correlate with the endpoint property. It has been shown that the MPNN framework has a high generalizability such that several popular graph neural network algorithms <ref type="bibr">[24-26, 28, 29]</ref> can be translated into the MPNN framework. Several research groups have made various extensions to the MPNN framework to augment it for work on cheminformatic problems <ref type="bibr" target="#b32">[30]</ref>.</p><p>Like GCN methods, MPNN model learns task specific molecule features from the graph structure and avoid feature engineering in the pre-processing stage. This type of method also presents an approach for the secure sharing of chemical data, i.e. it is possible to disseminate trained models for activity predictions without the risk of reverse-engineering IP-sensitive structural information <ref type="bibr" target="#b33">[31]</ref><ref type="bibr" target="#b34">[32]</ref><ref type="bibr" target="#b35">[33]</ref>.</p><p>We introduce a selection of augmentations to known MPNN architectures, which we refer to as Attention MPNN (AMPNN) and Edge Memory Neural Network (EMNN) <ref type="bibr" target="#b36">[34]</ref>, and evaluate them against published benchmark results with a range of metrics. The EMNN network shares architectural similarities to the D-MPNN model published by Yang et al. <ref type="bibr" target="#b37">[35]</ref> that was developed concurrently to this work <ref type="bibr" target="#b38">[36]</ref>, but the D-MPNN includes additional chemical descriptor information. We applied these two types of neural network to eight datasets from the MoleculeNet <ref type="bibr" target="#b32">[30]</ref> benchmark and analyse the performances and offer chemical justification for these results with respect to both architecture and parameter selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concepts of graphs</head><p>A graph G = (V, E) is a set V of nodes and a set E of edges, which are pairs of elements of V . If the members of E are ordered pairs, the graph is said to be directed. In the graph representation of a molecule, atoms are viewed as nodes and (v, w) ? E indicates there is a bond between atoms v and w . This representation is an undirected graph: we do not consider a bond to have a direction, so we do not distinguish between (v, w) and (w, v).</p><p>In the given context, a graph comes together with a feature vector x v corresponding to each node v and an edge feature vector e vw corresponding to each edge (v, w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Message passing neural network</head><p>The Message Passing Neural Network <ref type="bibr" target="#b29">[27]</ref> is a deep learning architecture designed for implementation in chemical, pharmaceutical and material science contexts. They were introduced as a framework to generalise several proposed techniques <ref type="bibr" target="#b16">[14,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b27">25,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b31">29,</ref><ref type="bibr" target="#b39">37,</ref><ref type="bibr" target="#b40">38]</ref>, and have demonstrated state-of-the-art results on multiple related benchmarks. For the specific MPNN implementations used for experiments in this paper, the most important predecessor is the Gated Graph Sequence Neural Network (GGNN) <ref type="bibr" target="#b30">[28]</ref>.</p><p>In simplistic terms, MPNNs operate by the following mechanism: An initial set of states is constructed, one for each node in the graph. Then, each node is allowed to exchange information, to "message", with its neighbours. After one such step, each node state will contain an awareness of its immediate neighbourhood. Repeating the step makes each node aware of its second order neighbourhood, and so forth. After a chosen number of "messaging rounds", all these context-aware node states are collected and converted to a summary representing the whole graph. All the transformations in the steps above are carried out with neural networks, yielding a model that can be trained with known techniques to optimise the summary representation for the task at hand.</p><p>More formally, MPNNs contain three major operations: message passing, node update, and readout. Using a message passing neural network entails iteratively updating a hidden state h v ? R D of each node v . This is done according to the following formulas:</p><p>(1)</p><formula xml:id="formula_0">m (t) v = w?N (v) M t h (t) v , h (t) w , e vw (2) h (t+1) v = U t h (t) v , m (t)</formula><p>v where M t is the message function, U t is the node update function, N (v) is the set of neighbours of node v in graph G , h</p><p>v is the hidden state of node v at time t , and m</p><formula xml:id="formula_2">(t)</formula><p>v is a corresponding message vector. For each atom v , mes- sages will be passed from its neighbours and aggregated as the message vector m (t) v from its surrounding environment. Then the atom hidden state h v is updated by the message vector.</p><p>The formula for the readout function is shown in formula 3:</p><p>where ? is a resulting fixed-length feature vector gen- erated for the graph, and R is a readout function that is invariant to node ordering, an important feature that allows the MPNN framework to be invariant to graph isomorphism. The graph feature vector ? then is passed to a fully connected layer to give prediction. All functions M t , U t and R are neural networks and their weights are learned during training. While details are given in the following sections, we provide summary differences between our presented architectures in Tables 1, 2, 3 and 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SELU message passing neural network (SELU-MPNN)</head><p>Our first architecture involved the basic MPNN framework, but with the use of the SELU activation function <ref type="bibr" target="#b41">[39]</ref> instead of more traditional batch or layer norm functions. The SELU activation function is parameterised to converge towards a zero mean and unit variance, and removed the need to experiment with different normalisation approaches (batch, layer, tensor, etc.) explicitly. All other architectures we propose also use SELU as their activation functions. Whilst many of the graph neural network approaches presented by MolNet can be cast into the MPNN framework, we chose to use SELU-MPNN as our baseline for our implementation of the framework due to the increased convergence speed that SELU offers <ref type="bibr" target="#b42">[40]</ref>. This affords us consistent results within our framework for a less biased comparison to more basic methods.</p><p>( </p><formula xml:id="formula_3">) ? = R h (K ) v |v ? G<label>3</label></formula><formula xml:id="formula_4">(t) v N(v) m (t) v = w?N(v) M t h (t) v , h<label>(t)</label></formula><p>w , e vw AMPNN h</p><formula xml:id="formula_5">(t) v N(v) m (t) v = A t h (t) v , S<label>(t)</label></formula><p>v , where</p><formula xml:id="formula_6">S (t) v = h (t) w , e vw |w ? N(v) EMNN h (t) vw {(k, v)|k ? N(v), k ? = w} m (t) vw = A t e vw , S<label>(t)</label></formula><p>vw , where</p><formula xml:id="formula_7">S (t) vw = {h kv |k ? N(v), k ? = w}</formula><p>Apart from the different choice of activation function and hidden layers in the message function, the model we in our experiments denote SELU-MPNN shares great similarity with the original GGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention message passing neural network (AMPNN)</head><p>Here we propose a further augmentation to the MPNN architecture by considering a more general form of the MPNN message summation step (Eq. 1). Using simple summation to convert an unknown cardinality set of vectors into a single vector is hypothetically an expressive bottleneck. Potential better ways to implement such aggregation functions are currently being researched <ref type="bibr" target="#b43">[41]</ref><ref type="bibr" target="#b44">[42]</ref><ref type="bibr" target="#b45">[43]</ref><ref type="bibr" target="#b46">[44]</ref>. In the current study we extend previous MPNN models for graph-level prediction by employing a straight forward aggregation function with an attention mechanism. The attention mechanism has been proposed on image recognition and language translation problems amongst others <ref type="bibr" target="#b43">[41,</ref><ref type="bibr" target="#b47">45,</ref><ref type="bibr" target="#b48">46]</ref> and have achieved better performance compared with normal deep neural network algorithms. We denote our specific implementation of the extended framework an Attention Message Passing Neural Network (AMPNN). Its most important predecessor is, as for our SELU-MPNN, the GGNN <ref type="bibr" target="#b30">[28]</ref>.</p><p>As mentioned earlier, the non-weighted summation in message passing function (Eq. 1) of the original MPNN constitutes a potential limitation. In the AMPNN framework, a computationally heavier but potentially more expressive attention layer is proposed in the message passing stage to aggregate messages (Eq. 4). Equation 1 is replaced by the more general formula:</p><p>where A t is an aggregate function invariant to the order- ing of set members at step t. Just as for the original MPNN, the message to node v is computed based on its neighbours {w|w ? N (v)} , but the method of aggrega- tion is not restricted to being a simple summation. The A t here chosen to be able to investigate the architecture is that of the SELU-MPNN augmented with an attention mechanism. This is mainly inspired by <ref type="bibr" target="#b43">[41]</ref> and essentially eliminates the cardinality dimension of the set of neighbours by taking weighted sums. Formally, our layer is Two feed forward neural network (FFNN) f  </p><formula xml:id="formula_8">m (t) v = A t h (t) v , h (t) w , e vw |w ? N (v)<label>(5)</label></formula><formula xml:id="formula_9">A t h (t) v , h (t) w , e vw = w?N (v) f (e vw ) NN h (t) w ? exp g (e vw ) NN h (t) w w ? ?N (v) exp g (e vw ? ) NN h (t) w ?</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2 Aggregation function special cases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Hidden states Aggregation form</head><p>MPNN h</p><formula xml:id="formula_10">(t) v M t h (t) v , h<label>(t)</label></formula><p>w , e vw = f</p><formula xml:id="formula_11">(evw ) NN h (t) w AMPNN h (t) v A t h (t) v , h<label>(t)</label></formula><p>w , e vw = </p><formula xml:id="formula_12">w?N(v) f (evw ) NN h (t) w ? exp g (evw ) NN h (t) w w ? ?N(v) exp g (e vw ? ) NN h (t) w ? EMNN h (t) vw A t e ? vw , S (t) vw = x?S ? (t) vw f NN (x) ? exp(g NN (x)) x ? ?S ? (t) vw exp(g NN (x ? )) S ?(t) vw = S (t) vw e vw ?</formula><formula xml:id="formula_13">MPNN AMPNN NA h (t+1) v = GRU m (t) v , h (t) v NA EMNN e vw ? = f emb NN e vw , h<label>(0)</label></formula><p>v , h</p><formula xml:id="formula_14">(0) w h (t+1) vw = GRU m (t) vw , h (t) vw h (K ) v = w?N(v) h (K ) vw</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 4 Model readout function and post-readout function</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Readout function Post-readout</head><formula xml:id="formula_15">All R h (K ) v , h<label>(0)</label></formula><formula xml:id="formula_16">v = v?G p NN h (K ) v ? ? q NN h (K ) v , h (0) v FFNN</formula><p>bar represent Hadamard multiplication and Hadamard division, respectively. Note that because of the output dimensionality of g</p><formula xml:id="formula_17">(e vw )</formula><p>NN , the softmax-like operation embodied in the fraction of Eq. 5 uses a multitude of weightings rather than just one.</p><p>The f</p><formula xml:id="formula_18">(e vw )</formula><p>NN network turns the hidden state of atom into an embedding vector, while the g</p><formula xml:id="formula_19">(e vw )</formula><p>NN network embeds the atom hidden states into weight vectors which are turned into weight coefficients after the softmax operation. Notably, the softmax operation is done along the cardinality dimension of the set of weight vectors. Thus, the contribution of one element in the embedding vector depends on equivalent element of weight vectors in the set.</p><p>In the node update stage, similar to the GGNN, the node hidden states are updated via a gated recurrent unit, where the m (t) v is treated as the input and the current node hidden state h (t) v is used as the hidden state of the GRU At the initial state (t = 0), h (0) v is the predefined atom feature vector. After the message passing and node updating steps are iterated for K steps, a readout function is applied to aggregate the hidden state of all the nodes in the graph into a graph level feature vector using two FFNNs. More precisely we use the GGNN readout function, where p NN and q NN are FFNNs, the ? denotes Hadamard multiplication, ? is the sigmoid function and the (,) of the right hand side denotes concatenation. The generated <ref type="bibr" target="#b8">(6)</ref> </p><formula xml:id="formula_20">h (t+1) v = GRU h (t) v , m (t) v . (<label>7</label></formula><formula xml:id="formula_21">) R h (K ) v , h (0) v = v?G p NN h (K ) v ? ? q NN h (K ) v , h (0)</formula><p>v graph feature vector is then passed into the final FFNN layer to make prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Edge Memory Neural Network (EMNN)</head><p>The message passing concept in the MPNN framework computes the message to a centre atom by aggregating information from its neighbourhood atoms in a symmetric fashion. Another MPNN-inspired model in our study has a hidden state in each directed edge (every bond has two directed edges in the directed graph) instead of in the nodes. In the directed graph, each bond (node-node connection) has two directed edges, thus two hidden states.</p><p>The hidden state of a directed edge is updated based on hidden states of edges whose heads coincide with its tail (Fig. <ref type="figure" target="#fig_2">1</ref>). We call this model an Edge Memory Neural Network (EMNN). In the resulting message passing step, the update of a hidden state has a corresponding direction. This model shares underlying principles with the D-MPNN architecture proposed by Yang et al. <ref type="bibr" target="#b37">[35]</ref> which also uses directed edges to improve MPNN performance. Their proposed model also injects additional chemical descriptor information alongside the FFNN after the message passing stage. Another notable difference between these architectures is our implementation of the afore-mentioned attention mechanism in the aggregation function. We include the D-MPNN model in our result and discussion to compare implementations and contrast the performance benefits of additional descriptor information, as has been explored in other literature <ref type="bibr" target="#b49">[47]</ref>. We refer to their manuscript for further details on their implementation and architecture.</p><p>One hypothetical advantage compared to MPNN is explained in the following. Consider a small graph of three nodes A, B and C connected as A-B-C, as illustrated on the right-hand side of Fig. <ref type="figure" target="#fig_2">1</ref>. If information passage from A to C is relevant to the task, two message passes are necessary with conventional MPNN. In the first pass, information is passed from A to B, as desired. However, information is also passed from C to B, so that part of B's memory is being occupied with information that C already has. This back-and-forth passing of information happening in an MPNN hypothetically dilutes the useful information content in the hidden state of node B. When hidden states instead reside in the directed edges as per EMNN, this cannot happen. The closest thing corresponding to a hidden state in B is the hidden states in the edges -? AB and -? CB . The update of -? BC uses information from -? AB , but not from -? CB. As shown in Fig. <ref type="figure" target="#fig_2">1</ref>, the flow of messages in each edge is directional where the message flows from a node (tail node) to another node (head node). Formally, the set of </p><p>v are the raw bond feature vector and atom feature vector respectively and (,) refers to the concatenation operation.</p><p>The edge hidden state h</p><formula xml:id="formula_23">(t)</formula><p>vw of (v, w) at time t is updated according to Eqs. 8-10:</p><p>Note that each directed edge has both a static edge feature e vw ? and the time-mutated edge state h</p><formula xml:id="formula_24">(t) vw contribut- ing. h (0)</formula><p>vw is instantiated as a vector of zeros. One choice of aggregation function A t is</p><formula xml:id="formula_25">m (t)</formula><p>vw is the message for edge (v, w) at iteration t . A e t is an attention based aggregation function similar to the one used in the AMPNN. S ? (t) vw means all the edges involving node v including the edge (v, w) itself. Equation 10 is the update of edge (v, w) using a GRU unit.</p><p>After K message passing iterations, a node hidden state for each node is taken as the sum of the edge hidden state of edges that the node is end to, This is done to be able to utilize the same readout functions as seen effective for the MPNNs. The readout function for EMNN is the same as in AMPNN (Eq. 7).</p><formula xml:id="formula_26">S (t) vw = h kv |k ? N (v), k ? = w . e ? vw = f emb NN e vw , h (0) v , h (0) w (<label>8</label></formula><formula xml:id="formula_27">) ? ? ? ? ? ? ? m (t) vw = A t ? e vw ? , S (t) vw ? h (t+1) vw = U t ? h (t) vw , m (t) vw ? .<label>(9)</label></formula><p>A e t e ? vw ,</p><formula xml:id="formula_28">S (t) vw = x?S? (t) vw f NN (x) ? exp g NN (x) x??S? (t) vw exp g NN (x?)</formula><p>where</p><formula xml:id="formula_29">S ? (t) vw = S (t) vw ? e ? vw (<label>10</label></formula><formula xml:id="formula_30">) h (t+1) vw = GRU h (t) vw , m (t) vw h (K ) v = w?N (v) h (K ) vw</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of architectural differences</head><p>All models we present are available from our git repository as abstract classes, and have been designed from the ground-up in the Pytorch <ref type="bibr" target="#b50">[48]</ref> framework to allow modification at all points, and have been tested using CUDA libraries for GPU acceleration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian optimisation</head><p>Bayesian Optimisation is a method for returning the next best expected value of an N-dimensional surface by utilising all available information, in contrast to local gradient or Hessian approximation techniques. Gaussian processes are fit around datapoints as they become available, and by using suitable evaluator types, estimates of the next datapoints to be evaluated can be obtained, and a balance between surface exploration and locality  optimisation can be struck. We used Expected Improvement as the acquisition function, and Local Penalisation <ref type="bibr" target="#b51">[49]</ref> as the evaluator type in order to make batch predictions and hence explore our hyperparameter surface in parallel. The hyperparameters used in the NN were tuned using the Bayesian optimization package GPyOpt <ref type="bibr" target="#b52">[50]</ref>.</p><p>The hyperparameters searched in Bayesian optimization and their constrained ranges are listed in Table <ref type="table" target="#tab_2">5</ref>. Due to architectural differences and an increased number of parameters, the optimisation range for the EMNN was slightly tightened.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We used a selection of 8 datasets presented in the Mol-eculeNet (MolNet) <ref type="bibr" target="#b32">[30]</ref> benchmarking paper to evaluate the networks. Datasets (shown in Table <ref type="table" target="#tab_3">6</ref>) were split according to the methods described in the MolNet paper. Datasets were split either randomly, or by Bemis-Murcko scaffold <ref type="bibr" target="#b53">[51]</ref>. In the case of randomly split sets, three sets were produced, split by fixed random seeds. Each dataset was split into train/test/validation sets in the ratio 80/10/10 as per the MolNet procedure. Optimal hyperparameters were determined based on their performance on the validation set of the primary split. Once optimal hyperparameters were selected three models were trained, one for each split, and the test scores for the best validation set epoch were averaged and the standard deviation calculated. In the case of scaffold splitting, test runs were still performed three times, and variation in the runs is the result of randomly initiated weights and biases. Each task in each dataset was normalised prior to training, and the results were transformed back after being passed through the model. Normalisation was done the same way as MolNet, with the notable exception of QM8. <ref type="foot" target="#foot_0">1</ref> The node features generated from the datasets were: Atom Type, Atom Degree, Implicit Valence, Formal Charge, Number of Radical Electrons, Hybridization (SP, SP2, SP3, SP3D, SP3D2), Aromaticity, and Total Number of Hydrogens. These features were generated as per the MolNet Deepchem functions. For edge features, the bond types were limited to single bonds, double bonds, triple bonds and aromatic bonds.</p><p>The QM8 dataset <ref type="bibr" target="#b54">[52]</ref> contains electronic spectra calculated from coupled-cluster (CC2) and TD-DFT data on synthetically feasible small organic molecules. The ESOL <ref type="bibr" target="#b55">[53]</ref> dataset comprises aqueous solubility values for small molecules, "medium" pesticide molecules, and large proprietary compounds from in-house Syngenta measurements. The LIPO dataset comprises lipophilicity data. The MUV dataset <ref type="bibr" target="#b56">[54]</ref> contains PubChem bioactivity data specially selected and arranged by refined nearest-neighbour analysis for benchmarking virtual screening approaches. The HIV dataset [55] comprises classification data for compound anti-HIV activity. The BBBP dataset <ref type="bibr" target="#b57">[56]</ref> contains data regarding compound ability to penetrate the blood-brain barrier. The Tox21 dataset <ref type="bibr" target="#b58">[57]</ref> was released as a data analysis challenge to predict compound toxicity against 12 biochemical pathways. The SIDER set <ref type="bibr" target="#b59">[58]</ref> is a collection of drugs and corresponding potential adverse reactions grouped following MedDRA classifications <ref type="bibr" target="#b60">[59]</ref> according to previous usage <ref type="bibr" target="#b61">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprocessing</head><p>Datasets were used both directly as provided from the MolNet repository without any preprocessing, and with some preprocessing procedure. Dataset preprocessing constituted transformation of the given SMILES string to that of the standardised charge-parent molecule, and reintroduction of 'missing value' labels where appropriate in multitask sets, which we refer to as SMD (Standardised Missing Data) preprocessing (Fig. <ref type="figure">2</ref>). Charge-parent fragmentation was performed using the MolVS standardizer <ref type="bibr" target="#b62">[61]</ref>, which returned the uncharged version of the largest organic covalent unit in the molecule or complex. In the original datasets, these values were imputed as inactive as per previous literature. The reintroduction of 'missing value' labels allows the use of a masking loss function that operates over the set [Active, Inactive, Missing] and does not include missing data in the loss calculation. This prevents backpropagation of molecule-target information in multitask datasets when it is not available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We present our results as a comparison against the Mol-eculeNet paper <ref type="bibr" target="#b32">[30]</ref>, showing test set performances and relative test set errors to the best reported graph-based MoleculeNet architecture, as well as other classical machine learning models. We show our architectures (SELU-MPNN, AMPNN and EMNN models) for both the unaltered and for the SMD preprocessed data, compared against the literature values for the original datasets to allow for fair benchmarking comparison for both the methods and for the preprocessing approaches. Complete tables are available in Additional file 1, alongside model performance information and statistical tests. The results from the literature for other machine learning methods were also reported to have hyperparameters optimised by the authors, using Bayesian Optimisation where applicable, so should present a fair comparison. Some techniques are missing for some larger datasets; this is because they were not reported in the original publications, presumably due to computational limits. Our runs were performed only for the models we present, and these are compared against values taken from literature benchmark studies for other models.</p><p>Performance in terms of AUC in classification on the original dataset was on par with state of the art for the majority of models, with the exception of the MUV set (Fig. <ref type="figure">3</ref>), where a modest increase in performance was observed relative to MolNet. However, this increase was not significant compared to Support-Vector Machines, which had the highest performance by a large margin. The AMPNN architecture was the best of our presented approaches, with the third highest overall performance Fig. <ref type="figure">2</ref> Examples of ionic complexes found in the datasets, and their charge-parent standardized counterparts, as used in the SMD datasets Fig. <ref type="figure">3</ref> Predictive performances of machine-learning approaches relative to the best MolNet graph model. With the exception of MUV, the metric used is ROC-AUC. The higher the y-axis is, the better the model performs on the MUV dataset. The D-MPNN showed a mild performance increase over our architectures for sets other than MUV.</p><p>In terms of regression on the original datasets (Fig. <ref type="figure">4</ref>), the AMPNN was also one of the best performing architectures we present, achieving the lowest error with smallest variance on two of the three sets, covering single and multi-task problems. Performance on the QM8 and ESOL datasets over our three presented architectures was more-or-less on par with MolNet, performing better than Random Forest and XGBoost models, and being beaten by the D-MPNN consistently. However, on the lipophilicity set, all our presented architectures achieved a lower error than all other presented approaches excepting the D-MPNN, which was rivalled by the AMPNN implementation. The Random Forest and XGBoost results are to be expected, as these approaches are much more suited to classification than regression.</p><p>Performance in classification on the SMD preprocessed dataset was also on par with state of the art for the majority of models, again with the exception of the MUV set (Fig. <ref type="figure">5</ref>). Little change was observed between the preprocessing techniques for the rest of the datasets, with minor improvement observed in the Tox21 models, a couple of the SIDER and HIV models, and one BBBP model. However, the MUV performance was considerably increased, with two of our architectures (SELU-MPNN and AMPNN) performing as well as SVM model, at three times the predictive power of the presented MolNet architecture. The EMNN network was the best performing architecture, beating SVM models and presenting a predictive power on average over four times higher than MoleculeNet original performance, with only a slightly higher variance.</p><p>Regression on the SMD datasets (Fig. <ref type="figure" target="#fig_3">6</ref>) also showed a little improvement overall versus the original datasets. The AMPNN was again one of the best performing architectures we present, achieving the lowest error with the smallest variance of the SMD models on the same two of the three sets as before, and showing a marked improvement on the ESOL dataset with this preprocessing approach. The lipophilicity set also showed lower overall error with these approaches, though the improvement is minor compared to the improved performance in classification.</p><p>Overall, we have demonstrated increased predictive power for some of our architectures dependent on task modelled. We have also demonstrated an improved dataset preprocessing technique that can increase the Fig. <ref type="figure">4</ref> Regression errors of machine-learning approaches relative to the best MolNet graph model. Metrics are specified for each dataset. The lower the y-axis is, the better the model performs modelling capabilities of our networks under certain circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets Classification</head><p>The reintroduction of missing data labels is likely the cause of the increased MUV performance over other methods. As shown in Table <ref type="table" target="#tab_4">7</ref> and Fig. <ref type="figure">7</ref>, approximately 84% of the data points in the MUV multitask set are unlabelled. In the original datasets, these points are imputed as inactives, which may introduce a large erroneous class imbalance to the dataset and affect performance.</p><p>When treating missing data as inactive in the original datasets, actives represent only 0.03% of the dataset, whereas ignoring missing data as with SMD sets the actives represent approximately 0.2% of the dataset, nearly an order of magnitude more. Heavily unbalanced datasets are notoriously tricky to train models on, and a reduction of this bias may explain the performance improvements of SMD processed data over the original MUV dataset.</p><p>As the SMD MUV dataset greatly outperformed other deep-learning approaches, we present a deeper analysis on this set. Per-task results (Fig. <ref type="figure">8</ref>) ranged between minimal knowledge and well-learned knowledge when averaged across the three runs, and were on the whole very consistent between architectures. Tasks 548 and 644, and tasks 832, 846 and 852 are of particular note: These correspond to Kinase Inhibitors and Protease Inhibitors respectively, and are our highest-performing tasks with the exception of task 712.</p><p>An analysis of these tasks gave a greater insight into one reason for the performance boost. As shown in Fig. <ref type="figure" target="#fig_6">9</ref>, these tasks had a much greater activity correlation than others, i.e. ligands observed to be active or inactive for these tasks were likely to share similar activity with the others. This allows the network to much more Fig. <ref type="figure">5</ref> Predictive performances of our machine-learning approaches on the SMD sets relative to MolNet and the respective original models. With the exception of MUV, the metric used is ROC-AUC. The higher the y-axis is, the better the model performs effectively pick up on common structural features and learn them as reported in other studies <ref type="bibr" target="#b63">[62,</ref><ref type="bibr" target="#b64">63]</ref>. However, in the case where missing data is imputed as inactive, these correlations become more difficult to learn, as negative counterexamples examples are artificially introduced. Other tasks, such as the PPIc or GPCR tasks, are    The other tasks display generally poor activity, or occasional performance peaks. Due to the extremely limited number of active compounds per task in the test-set, these performance peaks are expected to be sporadic and not true signal. Indeed, for task MUV-733, there were no active compounds in the test set for two of the three splits<ref type="foot" target="#foot_1">2</ref> as split by MolNet procedure. As a method for improving performance, for future work we suggest encoding structural features of the target alongside the ligand may be one approach that could be used when correlated target information is not available.</p><p>The imputation of missing data as inactives in smaller sets with fewer missing labels has a much smaller impact. Tox21, with only approximately 17% missing data, has a barely perceptible change in active/inactive ratios when missing data is ignored-changing from 6.1% active to 7.4% (Additional file 1). The performance increase here is therefore more likely to be due to false imputation of inactives in the dataset disrupting the learning process and making learning molecular features harder, than it is to be from a confusion of transfer learning examples.</p><p>The SIDER (no missing labels) performance demonstrates our algorithms are remarkably resilient to multiple unbalanced sets in a multitask setting, performing on par with most other contemporary machine learning algorithms (Additional file 1). They maintain an advantage even against algorithms that must be trained as multiple single-task models instead of a singular multitask algorithm. The performance increase between the Original and SMD datasets was found to be negligible.</p><p>The networks perform on-par with other approaches for single-task classification-the HIV and BBBP classification sets. During the dataset analysis we observed that some compounds exist in counterionic forms in some datasets, which may not be optimal for ADMETox modelling: the charge-parent aspect of the SMD preprocessing was introduced to convert molecules to more pharmacologically-relevant forms as they may exist in the body. This was na?vely done by removing complexes from the datasets, notably ionic complexes such as those shown in Fig. <ref type="figure">2</ref>, under the assumption that the largest fragment contributes the effect, and to ensure the consistency of charge representation. Further, there was an initial concern that, as ionic bonds are not modelled in the models' edge types, information would not be able to propagate between the disjoint components of the complex, and smaller components such as the sodium ions would act as artefacts in the graph and introduce noise. However, the lack of performance difference between the two suggests that the readout function bridged these gaps successfully, and the network can be robust against multiple fragments. As well as HIV and BBBP, this is supported by the negligible performance difference between the SIDER models of the two sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Regression</head><p>The models performed in general on-par with existing models in regression modelling, with a significant reduction in error when working on the LIPO dataset. The models seem robust against various distributions of values, with ESOL and LIPO datasets resembling skewed normal distributions and QM8 resembling a much more atypical distribution, with most values centred in a singular narrow range close to zero (Fig. <ref type="figure" target="#fig_2">10</ref>).</p><p>It is not known whether improvement can be further gained in some of these modelled tasks. The ESOL solubility models, for example, are close to the estimated experimental error of the original data. The estimated experimental error of drug-like compound solubility is usually cited as an RMSE around 0.6 logS units <ref type="bibr" target="#b65">[64]</ref>.</p><p>Simpler molecules nevertheless can be modelled with a much lower error around 0.3-0.4 log units <ref type="bibr" target="#b66">[65]</ref>-this same study further suggests that the limit of ca. 0.6 log units for drug-like compounds may not be due to experimental or data curation issues, but a limit of QSPR modelling as applied to these databases. The creation of large datasets suitable for training complex models with lower experimental error is a nontrivial task, as solubility is a difficult property to measure correctly in a high throughput scenario: The 'gold-standard' measure for solubilitythe shake-flask method, is a comparatively costly and time-consuming approach.</p><p>In contrast to the estimation of error for experimental physical chemical properties, other datasets can be difficult to give a lower bound of error, for example the QM8 dataset. DFT is in theory exact, however in practice a small but important energy component must be approximated. Though modern approximations provide useful Fig. <ref type="figure" target="#fig_2">10</ref> Distribution of property values from the ESOL, LIPO and QM8 regression datasets after normalisation by mean and standard deviation accuracy for practical purposes, errors are not strictly variational, so systematic improvement is problematic. Compounding this, practical implementations introduce other errors (from e.g. choice of basis set, grid resolution), and as such quantifying the limit of how well neural networks can model these properties is difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters</head><p>Due to the extensive hyperparameter optimisation that was performed during the training process, we analysed the distributions of hyperparameters to see if there were any tendencies towards optimal configurations for future work. Of the optimised hyperparameters (Table <ref type="table" target="#tab_2">5</ref>) we found that the shrinkage rate of the output fully-connected layer, the learning rate, the number of message passing iterations, and the output layer dropout rate were of note (Fig. <ref type="figure" target="#fig_2">11</ref>). Other hyperparameters did not display any notable trends. We found that generally a higher output layer shrinkage rate and a higher learning rate was more optimal for network performance. The learning rate was often hitting the maximum allowed value of the specified optimisation domain, which may indicate that performance could be further improved if this limit was expanded, pushing the distribution towards a more uniform coverage.</p><p>Conversely, dropout was observed to be generally lower in optimal hyperparameters across model training. Whilst this may generally be undesirable as it can lead to model overfitting, the evaluation of the model in a train/test/validation splitting approach should penalise any tendencies to overfit. This would imply that other aspects of the MPNN architecture act as feature regularisation and prevent this, though this cannot be stated conclusively. Figures supplied in the ESI suggest that no notable overfitting was observed during training, which may give the approach inherent advantages over machine learning methods that are traditionally more prone to overfitting. The number of message Fig. <ref type="figure" target="#fig_2">11</ref> Aggregate distributions of hyperparameters observed over all tasks and architectures on the SMD datasets after optimisation passes did not show any clear trend, and can be assumed to be heavily dependent on task and other hyperparameters. Some tasks such as ESOL and Tox21 however showed a small bias towards fewer message passing iterations, which makes sense as features such as hydrogen bond donors/ acceptors, toxicophores etc. can be very localised and large contributing factors to these properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We have introduced two augmentations to the MPNN framework that have shown performance on-par or greater than existing benchmarking models. One is the Attention MPNN, and the other the Edge Memory NN, both of which performed competitively with state of the art machine learning techniques of both traditional and deep learning varieties. The introduction of the attention scheme to our baseline MPNN framework added minimal model overhead, and offers no disadvantages for its use compared to the baseline model, in situations where it is effective. The EMNN had computational cost disadvantages, however, its use may be justified in situations where it offers significant performance increases: We demonstrate that our algorithms can outperform stateof-the-art models in virtual screening settings, notably demonstrated on sparse multi-task datasets, even without the inclusion of target structural information. Further, the inclusion of an attention mechanism may aid in model interpretability, as explored in other literature <ref type="bibr">[66]</ref>. We were fairly consistently outperformed by the analogous D-MPNN architecture on other tasks, however we noted generally comparable performance without the inclusion of additional chemical descriptor information, using only low-level chemical graph data. We have analysed different approaches to multitask modelling and dataset preprocessing that have demonstrated increased performance under specific conditions, most notably presenting that the graceful handling of missing data can contribute significantly to model performance in highly sparse datasets. Further, we have performed an extensive hyperparameter optimisation over many model parameters and provided a summary analysis of some more common hyperparameters, indicating potential starting values for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(e vw ) NN and g (e vw ) NN are used for each edge type e vw and give output vectors with the same length. The ? and the fraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1</head><label>1</label><figDesc>Fig.1The message passing from directed neighbouring edges to another edge in EMNN. Blue and green dots represent each directed hidden state for edges. Each coloured arrow is used to represent a respective message pass within the graph-purple represents the transition from one arbitrary direction to the other when the graph branches</figDesc><graphic url="image-6.png" coords="5,66.42,95.08,215.40,84.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6</head><label>6</label><figDesc>Fig.<ref type="bibr" target="#b8">6</ref> Regression errors of our machine-learning approaches for the SMD sets relative to MolNet and the respective original models. Metrics are specified for each dataset. The lower the y-axis is, the better the model performs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 Fig. 8</head><label>78</label><figDesc>Fig. 7 Ratio of actives, inactives, and missing data for each task in the MUV dataset. Actives represent such a small proportion that they are not visible in this diagram</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>more challenging to learn; by the nature of the target, the structural diversity of the actives compounded with the sparsity of the data, the class imbalances and the lack of transfer learning examples, results in very low performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9</head><label>9</label><figDesc>Fig. 9 Correlation heatmaps between tasks for the training and test sets. These have been averaged across all splits. White indicates no data available for correlation (at least one missing datapoint for all pairs)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-7.png" coords="8,113.88,95.08,368.52,189.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Core differences between model architectures Model Hidden states Denotion of neighbourhood Message aggregation scheme MPNN h</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 Other model architecture differences</head><label>3</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Pre-message passing</cell><cell>Update</cell><cell>Pre-readout</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 A list of hyperparameters optimised for each architecture type, and the domains over which they were optimised</head><label>5</label><figDesc></figDesc><table><row><cell>Hyperparameter</cell><cell>SELU-MPNN AMPNN</cell><cell>EMNN</cell></row><row><cell>Learn-rate</cell><cell></cell><cell></cell></row><row><cell cols="2">Square brackets indicate discrete domains</cell><cell></cell></row><row><cell>NA not applicable</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 The selection of datasets on which models were trained, and details pertaining to these sets</head><label>6</label><figDesc></figDesc><table><row><cell cols="3">Dataset Tasks Type</cell><cell cols="2">Compounds Split</cell><cell>Metric</cell></row><row><cell>MUV</cell><cell>17</cell><cell cols="2">Classification 93,127</cell><cell cols="2">Random PRC-AUC</cell></row><row><cell>HIV</cell><cell>1</cell><cell cols="2">Classification 41,913</cell><cell cols="2">Scaffold ROC-AUC</cell></row><row><cell>BBBP</cell><cell>1</cell><cell>Classification</cell><cell>2053</cell><cell cols="2">Scaffold ROC-AUC</cell></row><row><cell>Tox21</cell><cell>12</cell><cell>Classification</cell><cell>8014</cell><cell cols="2">Random ROC-AUC</cell></row><row><cell>SIDER</cell><cell>27</cell><cell>Classification</cell><cell>1427</cell><cell cols="2">Random ROC-AUC</cell></row><row><cell>QM8</cell><cell>12</cell><cell>Regression</cell><cell>21,786</cell><cell cols="2">Random MAE</cell></row><row><cell>ESOL</cell><cell>1</cell><cell>Regression</cell><cell>1128</cell><cell cols="2">Random RMSE</cell></row><row><cell>LIPO</cell><cell>1</cell><cell>Regression</cell><cell>4200</cell><cell cols="2">Random RMSE</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 Number of Actives, inactives, and missing datapoints in the classification sets used in the study Classification set Number of actives Missing datapoints Number of inactives</head><label>7</label><figDesc></figDesc><table><row><cell>MUV</cell><cell>398</cell><cell>1,066,216</cell><cell>199,359</cell></row><row><cell>HIV</cell><cell>1232</cell><cell>0</cell><cell>31,669</cell></row><row><cell>BBBP</cell><cell>1341</cell><cell>0</cell><cell>290</cell></row><row><cell>Tox21</cell><cell>4617</cell><cell>12,821</cell><cell>57,730</cell></row><row><cell>SIDER</cell><cell>17,440</cell><cell>0</cell><cell>13,367</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 Task Information for the MUV dataset</head><label>8</label><figDesc></figDesc><table><row><cell>Task label</cell><cell>Target</cell><cell>Mode of interaction</cell><cell>Target class</cell><cell>Assay type</cell></row><row><cell>MUV-466</cell><cell>S1P1 rec.</cell><cell>Agonists</cell><cell>GPCR</cell><cell>Reporter Gene</cell></row><row><cell>MUV-548</cell><cell>PKA</cell><cell>Inhibitors</cell><cell>Kinase</cell><cell>Enzyme</cell></row><row><cell>MUV-600</cell><cell>SF1</cell><cell>Inhibitors</cell><cell>Nuclear Receptor</cell><cell>Reporter Gene</cell></row><row><cell>MUV-644</cell><cell>Rho-Kinase2</cell><cell>Inhibitors</cell><cell>Kinase</cell><cell>Enzyme</cell></row><row><cell>MUV-652</cell><cell>HIV RT-RNase</cell><cell>Inhibitors</cell><cell>RNase</cell><cell>Enzyme</cell></row><row><cell>MUV-689</cell><cell>Eph rec. A4</cell><cell>Inhibitors</cell><cell>Rec. Tyr. Kinase</cell><cell>Enzyme</cell></row><row><cell>MUV-692</cell><cell>SF1</cell><cell>Agonists</cell><cell>Nuclear Receptor</cell><cell>Reporter Gene</cell></row><row><cell>MUV-712</cell><cell>HSP 90</cell><cell>Inhibitors</cell><cell>Chaperone</cell><cell>Enzyme</cell></row><row><cell>MUV-713</cell><cell>ER-a-coact. bind.</cell><cell>Inhibitors</cell><cell>PPIc</cell><cell>Enzyme</cell></row><row><cell>MUV-733</cell><cell>ER-?-coact. bind.</cell><cell>Inhibitors</cell><cell>PPIc</cell><cell>Enzyme</cell></row><row><cell>MUV-737</cell><cell>ER-a-coact. bind.</cell><cell>Potentiators</cell><cell>PPIc</cell><cell>Enzyme</cell></row><row><cell>MUV-810</cell><cell>FAK</cell><cell>Inhibitors</cell><cell>Kinase</cell><cell>Enzyme</cell></row><row><cell>MUV-832</cell><cell>Cathepsin G</cell><cell>Inhibitors</cell><cell>Protease</cell><cell>Enzyme</cell></row><row><cell>MUV-846</cell><cell>FXIa</cell><cell>Inhibitors</cell><cell>Protease</cell><cell>Enzyme</cell></row><row><cell>MUV-852</cell><cell>FXIIa</cell><cell>Inhibitors</cell><cell>Protease</cell><cell>Enzyme</cell></row><row><cell>MUV-858</cell><cell>D1 rec.</cell><cell>Allosteric modulators</cell><cell>GPCR</cell><cell>Reporter Gene</cell></row><row><cell>MUV-859</cell><cell>M1 rec.</cell><cell>Allosteric inhibitors</cell><cell>GPCR</cell><cell>Reporter Gene</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In other regression datasets, the dataset was normalised, and then split into train/test/validation splits, whereas QM8 was split and then each split normalised. We chose to normalise QM8 in the same manner as the other regression sets, splitting after normalising over the whole set.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In future work, to mitigate these issues in highly sparse, highly unbalanced datasets, we encourage the use of alternative splitting approaches such as stratified sampling. Alternatively, if random sampling is preferring, repeat selection of seeds until at least one active is available for each task is recommended.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">Zhenqin Wu</rs> for assistance in reproducing the MolNet datasets for use in this publication, and <rs type="person">Dr. Igor Tetko</rs> for comments that improved the manuscript.</p></div>
			</div>
			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and materials</head><p>The code we used in this paper is published and available at https ://githu b.com/edvar dlind elof/graph -neura l-netwo rks-for-drug-disco very.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary information</head><p>Supplementary information accompanies this paper at https ://doi. org/10.1186/s1332 1-019-0407-y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional file 1. Additional tables and figures.</head><p>Authors' contributions MW assisted with code development, experiment design and analysis, and wrote the manuscript. EL assisted heavily with the development of the codebase and the initial augmentations, and co-authored the manuscript. HC helped with project analysis and cosupervised the project with OE. All authors read and approved the final manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>The project leading to this article received funding from the European Union's Horizon 2020 research and innovation program under the Marie Sk?odowska-Curie Grant Agreement No. 676434, "Big Data in Chemistry" ("BIGCHEM", http://bigch em.eu). The article reflects only the authors' view, and neither the European Commission nor the Research Executive Agency are responsible for any use that may be made of the information it contains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors declare that they have no competing interests.</p><p>? fast, convenient online submission ? thorough peer review by experienced researchers in your field</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? rapid publication on acceptance</head><p>? support for research data, including large and complex data types ? gold Open Access which fosters wider collaboration and increased citations maximum visibility for your research: over 100M website views per year </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">} Message-size</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="10" to="16" />
		</imprint>
	</monogr>
	<note>10,16,25,40] NA</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">Message-passes</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Msg-hidden-dim</note>
	<note>1-10. 1-10. 1-8. 30,45,70,100</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Edge-emb-hidden-dim NA NA [60,105,180] Edge-embedding-size NA NA</title>
		<idno>Att-hidden-dim NA [50,85,150</idno>
		<imprint>
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note>30,50,80] References</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Substituent constants for correlation analysis in chemistry and biology</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Flynn</surname></persName>
		</author>
		<idno type="DOI">10.1002/jps.2600690938</idno>
		<ptr target="https://doi.org/10.1002/jps.2600690938" />
	</analytic>
	<monogr>
		<title level="j">J Pharm Sci</title>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Counts of all walks as atomic and molecular descriptors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ruecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Chem Inf Comput Sci</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="683" to="695" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dragon software: an easy approach to molecular descriptor calculations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mauri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Consonni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Todeschini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Match</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="248" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flexible 3D pharmacophores as descriptors of dynamic biological space</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Nettles</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jmgm.2007.02.005</idno>
		<ptr target="https://doi.org/10.1016/j.jmgm.2007.02.005" />
	</analytic>
	<monogr>
		<title level="j">J Mol Graph Model</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="622" to="633" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Handbook of molecular descriptors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Todeschini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Consonni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Wiley VCH</publisher>
			<pubPlace>Weinheim</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">New molecular descriptors for 2D and 3D structures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Todeschini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lasagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marengo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory. J Chemom</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Survey on Graph Kernels</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cs Stat</title>
		<imprint>
			<date type="published" when="2019">2019. 190311835</date>
		</imprint>
	</monogr>
	<note>ArXiv</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extended-connectivity fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hahn</surname></persName>
		</author>
		<idno type="DOI">10.1021/ci100050t</idno>
		<ptr target="https://doi.org/10.1021/ci100050t" />
	</analytic>
	<monogr>
		<title level="j">J Chem Inf Model</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="742" to="754" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The rise of deep learning in drug discovery</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.drudis.2018.01.039</idno>
		<ptr target="https://doi.org/10.1016/j.drudis.2018.01" />
	</analytic>
	<monogr>
		<title level="j">Drug Discov Today</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A renaissance of neural networks in drug discovery</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Baskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Tetko</surname></persName>
		</author>
		<idno type="DOI">10.1080/17460441.2016.1201262</idno>
		<ptr target="https://doi.org/" />
	</analytic>
	<monogr>
		<title level="j">Expert Opin Drug Discov</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="785" to="795" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<idno type="DOI">10.1109/ijcnn.2005.1555942</idno>
		<ptr target="https://doi.org/10.1109/ijcnn" />
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE international joint conference on neural networks</title>
		<meeting>2005 IEEE international joint conference on neural networks</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005. 15559</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural network for graphs: a contextual constructive approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNN.2008.2010350</idno>
		<ptr target="https://doi.org/10.1109/TNN" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">50</biblScope>
			<date type="published" when="2008">2009. 2008. 20103</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNN.2008.2005605</idno>
		<ptr target="https://doi.org/10.1109/TNN.2008.2005605" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>ArXiv13126203 Cs</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Attention models in graphs: a survey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<idno>ArXiv180707984 Cs</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939753</idno>
		<ptr target="https://doi.org/10.1145/2939672" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22Nd ACM SIGKDD international conference on knowledge discovery and data mining<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="29397" to="29353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adversarially regularized graph Autoencoder for graph embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ArXiv180204407 Cs Stat</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Deep Network Representations with Adversarially Regularized Autoencoders</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3220000</idno>
		<idno>19.32200 00</idno>
		<ptr target="https://doi.org/10.1145/32198" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2663" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Diffusion convolutional recurrent neural network: data-driven traffic forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv170701926 Cs Stat</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Structural-RNN: deep learning on spatio-temporal graphs presented at the</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolutional networks: a deep learning framework for traffic forecasting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/505</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2018/505" />
	</analytic>
	<monogr>
		<title level="m">Proc. Twenty-Seventh Int Jt Conf Artif Intell</title>
		<meeting>Twenty-Seventh Int Jt Conf Artif Intell</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3634" to="3640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10822-016-9938-8</idno>
		<ptr target="https://doi.org/10.1007/s10822-016-9938-8" />
	</analytic>
	<monogr>
		<title level="j">J Comput Aided Mol Des</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional embedding of attributed molecular graphs for physical property prediction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Jensen</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jcim.6b00601</idno>
		<ptr target="https://doi.org/10.1021/acs.jcim.6b006" />
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno>ArXiv170401212 Cs</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno>ArXiv151105493 Cs Stat</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>ArXiv160902907 Cs Stat</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MoleculeNet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1039/C7SC02664A</idno>
		<ptr target="https://doi.org/10.1039/C7SC02664A" />
	</analytic>
	<monogr>
		<title level="j">Chem Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Descriptor collision and confusion: toward the design of descriptors to mask chemical structures</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bologa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Allu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Oprea</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10822-005-9020-4</idno>
		<ptr target="https://doi.org/" />
	</analytic>
	<monogr>
		<title level="j">J Comput Aided Mol Des</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9-10</biblScope>
			<biblScope unit="page" from="625" to="635" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Why relevant chemical information cannot be exchanged without disclosing structures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Filimonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Poroikov</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10822-005-9014-2</idno>
		<ptr target="https://doi.org/10.1007/s10822-005-9014-2" />
	</analytic>
	<monogr>
		<title level="j">J Comput Aided Mol Des</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9-10</biblScope>
			<biblScope unit="page" from="705" to="713" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Surrogate data-a secure way to share corporate data</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Tetko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abagyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Oprea</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10822-005-9013-3</idno>
		<ptr target="https://doi.org/10.1007/s10822-005-9013-3" />
	</analytic>
	<monogr>
		<title level="j">J Comput Aided Mol Des</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9-10</biblScope>
			<biblScope unit="page" from="749" to="764" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention and edge memory convolution for bioactivity prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Withnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lindel?f</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-30493-5_69</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-30493-5_69" />
	</analytic>
	<monogr>
		<title level="m">Artificial neural networks and machine learning-ICANN 2019: Workshop and Special Sessions</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="752" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Are learned molecular representations ready for prime time?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ArXiv190401561 Cs Stat</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep Learning for Drug Discovery, Property Prediction with Neural Networks on Raw Molecular Graphs</title>
		<author>
			<persName><surname>Lindel?f</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Chalmers</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Masters Thesis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<editor>Lee DD, Sugiyama M, Luxburg UV, Guyon I, Garnett R</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Curran Associates Inc</publisher>
			<biblScope unit="page" from="3844" to="3852" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<idno type="DOI">10.1038/ncomms13890</idno>
		<ptr target="https://doi.org/10.1038/ncomm" />
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">0</biblScope>
			<date type="published" when="1389">2017. 1389</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deepchem/contrib/mpnn at master deepchem/deepchem GitHub</title>
		<ptr target="https://github.com/deepchem/deepchem/tree/master/contrib/mpnn" />
		<imprint>
			<date type="published" when="2019-08-12">12 Aug 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ArXiv171010903 Cs Stat</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep Perm-Set Net: Learn to predict sets with unknown permutation and cardinality using deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<idno>ArXiv180500613 Cs</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<idno>ArXiv170306114 Cs Stat</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Deep Sets.</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>ArXiv180204712 Cs Stat</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning natural language inference using bidirectional LSTM model and inner-attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv160509090 Cs</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Look closer to see better: recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Synergy effect between convolutional neural networks and the multiplicity of SMILES for improvement of molecular prediction</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Kimber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Engelke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Tetko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Godin</surname></persName>
		</author>
		<idno>ArXiv181204439 Cs Stat</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Batch Bayesian optimization via local penalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hennig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<idno>ArXiv150508052 Stat</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Gpyopt: A bayesian optimization framework in python</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gonz?lez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The properties of known drugs. 1. Molecular frameworks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Bemis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Murcko</surname></persName>
		</author>
		<idno type="DOI">10.1021/jm9602928</idno>
		<ptr target="https://doi.org/10.1021/jm" />
	</analytic>
	<monogr>
		<title level="j">J Med Chem</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">2928</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Electronic spectra from TDDFT and machine learning in chemical space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tapavicza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename></persName>
		</author>
		<idno type="DOI">10.1063/1.4928757</idno>
		<ptr target="https://doi.org/10.1063/1.4928757" />
	</analytic>
	<monogr>
		<title level="j">J Chem Phys</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">84111</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">ESOL: estimating aqueous solubility directly from molecular structure</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Delaney</surname></persName>
		</author>
		<idno type="DOI">10.1021/ci034243x</idno>
		<ptr target="https://doi.org/10.1021/ci034243x" />
	</analytic>
	<monogr>
		<title level="j">J Chem Inf Comput Sci</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1000" to="1005" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Maximum unbiased validation (MUV) data sets for virtual screening based on PubChem bioactivity data</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Rohrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Baumann</surname></persName>
		</author>
		<idno type="DOI">10.1021/ci8002649</idno>
		<ptr target="https://doi.org/10.1021/ci8002649" />
	</analytic>
	<monogr>
		<title level="j">J Chem Inf Model</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="184" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A Bayesian approach to in silico blood-brain barrier penetration modeling</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Falcao</surname></persName>
		</author>
		<idno type="DOI">10.1021/ci300124c</idno>
		<ptr target="https://doi.org/10.1021/ci300124c" />
	</analytic>
	<monogr>
		<title level="j">J Chem Inf Model</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1686" to="1697" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Tox21</title>
		<ptr target="https://tripod.nih.gov/tox21/challenge/index.jsp" />
		<imprint>
			<date type="published" when="2019-07-10">10 July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The SIDER database of drugs and side effects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Letunic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bork</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkv1075</idno>
		<ptr target="https://doi.org/10.1093/nar/gkv1075" />
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1075" to="D1079" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">MedDRA |</title>
		<ptr target="https://www.meddra.org/" />
		<imprint>
			<date type="published" when="2019-07-10">10 July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Low data drug discovery with one-shot learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Altae-Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<idno type="DOI">10.1021/acscentsci.6b00367</idno>
		<ptr target="https://doi.org/10.1021/acscentsci.6b003" />
	</analytic>
	<monogr>
		<title level="j">ACS Cent Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">MolVS: molecule validation and standardization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Swain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Human platelet-specific antigen, Siba, is associated with the molecular weight polymorphism of glycoprotein Ib alpha</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Maruya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Furihata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Blood</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1722" to="1729" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A survey of multi-task learning methods in chemoinformatics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sosnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vashurina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Withnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Karpov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Tetko</surname></persName>
		</author>
		<idno type="DOI">10.1002/minf.201800108</idno>
		<ptr target="https://doi.org/10.1002/minf.201800108" />
	</analytic>
	<monogr>
		<title level="j">Mol Inform</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1800108</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Prediction of drug solubility from structure</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Jorgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Duffy</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0169-409X(02)00008-X</idno>
		<ptr target="https://doi.org/10.1016/S0169-409X" />
	</analytic>
	<monogr>
		<title level="j">Adv Drug Deliv Rev</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Is experimental data quality the limiting factor in predicting the aqueous solubility of druglike molecules?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jbo</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.1021/mp500103r</idno>
		<ptr target="https://doi.org/10.1021/mp500103r" />
	</analytic>
	<monogr>
		<title level="j">Mol Pharm</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2962" to="2972" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
