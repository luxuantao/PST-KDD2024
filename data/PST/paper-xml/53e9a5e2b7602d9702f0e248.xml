<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Levels of Detail for Crowds and Groups</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">C</forename><surname>O'sullivan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Trinity College</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Trinity College</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><surname>Vilhj√°lmsson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Trinity College</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Dingliana</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Trinity College</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>Dobbyn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Trinity College</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">B</forename><surname>Mcnamee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Trinity College</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">C</forename><surname>Peters</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Trinity College</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">T</forename><surname>Giang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Trinity College</orgName>
								<address>
									<settlement>Dublin</settlement>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Levels of Detail for Crowds and Groups</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">89435B46052F466D06BDBE5C10B99990</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>crowd animation</term>
					<term>group animation</term>
					<term>real-time animation</term>
					<term>human simulation</term>
					<term>collision handling ACM CSS: I.3.7 Three-Dimensional Graphics and Realism-Animation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Work on levels of detail for human simulation has occurred mainly on a geometrical level, either by reducing the numbers of polygons representing a virtual human, or replacing them with a two-dimensional imposter.</head><p>Approaches that reduce the complexity of motions generated have also been proposed. In this paper, we describe ongoing development of a framework for Adaptive Level Of Detail for Human Animation (ALOHA), which incorporates levels of detail for not only geometry and motion, but also includes a complexity gradient for natural behaviour, both conversational and social.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Crowd and group simulations are becoming increasingly important in the entertainment industry. In movies, they can be used to simulate the presence of real humans. For example, in the movie Titanic, extensive use was made of virtual people, whose movements were generated from a library of pre-captured motions. Such technology can be used in situations where it is dangerous for real people to perform the actions, such as falling over 50 feet off a ship, or to reduce the complexity and expense of handling large numbers of human extras. In animated movies, crowd simulation really comes into its own, such as in the stampede scene in Disney's The Lion King and the colonies of ants in Dreamworks' Antz. Crowd and group simulations are becoming more popular in simulated online communities and interactive applications such as games and Virtual Reality.</p><p>Recent research into crowd simulation has to a large extent been inspired by the flocking work of Craig Reynolds <ref type="bibr" target="#b0">[1]</ref>. He revolutionised the animation of flocks of animals, in his case birds (or "Boids"), by adapting some ideas from particle systems, where each individual bird is a particle. More recently, groups of creatures travelling in close proximity have been simulated, whose movements are controlled by dynamical laws <ref type="bibr" target="#b1">[2]</ref>. A key element of this type of animation is collision avoidance. Significant research has been conducted into the area of crowd simulation, an example being the work of Musse et al. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. In their ViCrowd system, they can create a virtual crowd, where the individuals have variable levels of autonomy i.e. scripted, rule-based, or guided interactively by the user. They have demonstrated the emergent behaviour of a crowd attending, for example, a political demonstration or a football match. Nevertheless, the realistic simulation of crowds and groups of humans remains a challenge, due to the ability of humans to detect even slightly unnatural human behaviour. In particular, realistic gesture and interactions between the individuals in the crowd are important cues for realism. Individuals in the crowd should look as if they are conversing or communicating with each other in both verbal and nonverbal ways (see Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>The crowd effects in the movie Antz were impressive. An extensive library of motion-tracked movements was used to assign random movement to the individuals in the crowd. Such a method looks good from a distance, but up closer it is obvious that the characters are behaving in a cyclical manner and not interacting naturally with each other. In the movie, the main characters were animated manually by human animators, which would just not be possible in any kind of real-time, interactive application. Such random behaviour may be retained as a lower level of detail, and increasingly realistic (and hence computationally expensive) techniques may be used for more important characters, e.g. those closer to the viewer, or characters that are more significant to the plot. Some work has been done by Aubel et al. <ref type="bibr" target="#b4">[5]</ref> and Tecchia et al. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> on methods to reduce the rendering requirements of crowd simulations, but the approach taken there was to replace a full geometrical model with a lower resolution one, such as a two-dimensional imposter. Giang et al. proposed a framework for Adaptive Level Of detail for Human Animation (ALOHA) <ref type="bibr" target="#b7">[8]</ref>, incorporating levels of detail for geometry, motion, and behaviour. In this paper, we describe the ongoing development of this framework, to which we have added a new behaviour controller (see Figure <ref type="figure" target="#fig_1">2</ref>).</p><p>In Section 2 we describe the technique that we use for varying the geometrical levels of detail of our humans for rendering purposes. In Section 3 we describe how we are adding simulation levels of detail to the framework, both for generating the motions of the humans and for collision handling. Before concluding with plans for future work, Section 4 describes the behavioural levels of detail in the system, introducing a complexity gradient for both conversational and more general behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Geometry</head><p>The requirement in interactive systems for real-time frame rates means that a limited number of polygons can be displayed by the graphics engine in each frame of a simulation. Therefore, meshes with a high polygon count often have to be simplified in order to achieve acceptable display rates. This can be achieved in several ways: a representation of the object at several levels of detail with a fixed polygon count can be generated, although switching between such levels of detail can cause a perceivable pop in an animation, or multi-resolution or progressive meshes that can be refined at run-time can be used. Alternatively, image-based imposters can be used by replacing the entire geometry of the character with images that have been taken from multiple angles <ref type="bibr" target="#b5">[6]</ref>. While the latter technique is excellent for visualising large crowds in real-time, the representations are not detailed enough for closer viewing, being simply flat textures. In addition, severe popping effects occur when switching from the imposter to a full geometrical model.</p><p>Subdivision surfaces offer a good solution for increasing and reducing the detail of characters in games, as demonstrated recently by Leeson <ref type="bibr" target="#b8">[9]</ref>. High end rendering packages offer good support for these surfaces, and companies like Pixar have used them to produce very effective results <ref type="bibr" target="#b9">[10]</ref>. Many of the problems associated with other curved surface representations such as NURBS can be avoided by using subdivision schemes and they behave in a way similar to polygonal meshes. Character skins can be used almost directly with some subdivision schemes while others require modification in order to get a good representation of the original mesh. We are using subdivision surfaces as a means of improving the appearance of our virtual humans, along with some acceleration methods based on culling and preprocessing. We have found that one of the advantages of subdivision surfaces is that only a low resolution mesh is necessary as the starting point -see Figure <ref type="figure" target="#fig_3">3</ref>.    absent. Other masks are also used to generate creases in a surface, making them capable of having sharp features. The masks are applied to each vertex in the mesh to produce a new mesh (and can also be applied to the texture coordinates). After successive applications of the mask the mesh converges to a surface. Figure <ref type="figure" target="#fig_4">4</ref> shows examples of some subdivision schemes that can be used: Linear, Loop <ref type="bibr" target="#b10">[11]</ref> and Butterfly <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. In this way, detailed meshes can be generated from a small number of preliminary vertices -see Figure <ref type="figure" target="#fig_5">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motion</head><p>At the lowest level of detail in ALOHA, key-framed animations, created in Character Studio or motion-captured, are exported from 3D Studio Max and used to animate the Virtual Humans. When the characters are far from the viewer, these animations are chosen at random and changed at different intervals, in order to give an impression of varied activity to the crowd. When the viewer focusses on these characters (e.g. by moving closer or starting an interaction), actions that are more meaningful are then chosen. This means applying more sophisticated behaviour, as described in Section 4, but also generating more realistic motions that do not cycle and become more accurate as the character increases in importance.</p><p>One approach we are taking to making motion look more realistic is by using a real-time reaching and grasping system. The virtual human is endowed with a memory model and has the ability to sense their environment using a virtual vision sensor. Virtual humans are not automatically aware of the exact characteristics of all objects in their environment or visual field, and must pay attention to them in order to be able to sample their attributes at a lower granularity. Given a command (e.g. "pick up the bottle"), the virtual human will become attentive towards the object and will not only generate a goal-directed arm motion towards it, but will also remember the position and other perceived attributes of the object. Future requests regarding the object can then be dealt with using the memorised state of the object; they will be able to make realistic reaching movements towards the remembered object location without having to look at it. The generation of these arm motions is based on results from neurophysiology. See Peters and O'Sullivan <ref type="bibr" target="#b13">[14]</ref> for further details.</p><p>Another reason why human crowd simulations tend to lack realism is because of the flocking approach often adopted. Most crowd simulations implement only collision avoidance and use coarse approximations, such as bounding boxes or spheres, to detect possible future collisions. Such approaches tend to focus on the macroscopic motions of individuals and the crowd as a whole and are suitable models for simulating distant views of crowds when they are in motion. However when a closer, more detailed view is required, this can lead to simulations where the individual elements in the crowd are too sparse and therefore unconvincing. People in a large crowd, perhaps attending a concert or sporting event, may appear to move in a flocking manner at times. However, when they are observed more closely or all group together into an enclosed area such as a bar, more accurate collision handling is needed. Real people bump up against each other, or are squashed together in an enclosed area. Even if they are not too tightly packed, people who know each other will be walking along chatting, holding hands, or occasionally touching. To model such interactions, we need to incorporate proper collision detection and appropriate responses, based on behavioural rules, in order to achieve a more chaotic, less military look to the crowds.</p><p>At the other extreme of the spectrum, approaches for highly detailed collision detection with virtual humans have been implemented, performing polygon level checks between humans and objects in the scene <ref type="bibr" target="#b14">[15]</ref>. Although this might be appropriate for offline simulations and for smaller, less populated scenes requiring high detail collision information e.g. for cloth simulation, this obviously becomes computationally infeasible for a large scene with many virtual humans interacting and being simulated accurately at the same time. A real-time adaptive approach is most suitable for this situation, and techniques for adaptive collision detection and contact modelling, as described by Dingliana and O'Sullivan <ref type="bibr" target="#b15">[16]</ref>, can be adapted to deal with this situation.</p><p>A hierarchy of rigid bounding volumes is used for collision detection in our system (see Figure <ref type="figure" target="#fig_7">6</ref>). If the characters are modelled based on hierarchical transforms then these transforms can easily be used to update the positions of nodes in the collision detection hierarchy. A hierarchy of sphere swept volumes (SSV's) <ref type="bibr" target="#b16">[17]</ref> is ideal for hierarchically modelled virtual humans as the volume nodes are a good fit for most of the nodes in the transformation hierarchy. Line swept spheres (LSS's) are the volumes generated by sweeping a sphere across a line segment. A hierarchy of SSV's is an efficient way to model characters for collision detection as a combination of simple spheres (referred to as point swept spheres or PSS) and LSS nodes provides a good balance between computational efficiency and tight-boundedness in the case of virtual humans. Such bounding volume hierarchies are normally associated with rigid body collision detection where the full collision volume can be generated at the pre-processing stage. Pre-processing still occurs in order to generate the individual nodes of the volume hierarchy and these are used as individual rigid bodies in an articulated transformation hierarchy.</p><p>An adaptive approach switches between different levels of resolution of the character's volume hierarchy as required, thus facilitating the necessary level of collision detail. As in the approach by Dingliana and O'Sullivan, the volume hierarchy also makes possible the rapid elimination of nodes from the collision detection phase when their parent nodes are found not to be colliding. Due to the simple nature of the nodes in the volume representation, it is possible to incorporate procedures for quickly detecting collisions between the LSS nodes and more general volume representations such as boxes, planes and polygons, which can be used to model the rest of the environment. Thus, it is possible to model not only collisions and self-collisions between characters but also between the characters and a more generic virtual environment.</p><p>Once collisions with the humans have been detected, a realistic response is necessary. An optimal solution framework is currently being developed for the inclusion of physically correct responsive objects within the virtual space. This work is primarily concerned with the provision of a feasible real time level of detail hybrid impulse/constraint based solution (see Figure <ref type="figure" target="#fig_8">7</ref>). To evaluate our collision handling   techniques, psychophysical experiments similar to those described by O'Sullivan and Dingliana <ref type="bibr" target="#b17">[18]</ref> are being used. We vary the ways in which collision processing can be speeded up and examine the effect of the resultant degradation upon the perception of the viewer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Behaviour</head><p>In our system, we allow for several levels of detail with respect to behaviour. Firstly, the concept of conversational levels of detail has been used, along with an implementation of Level of detail AI (LODAI) for modelling more general behaviour and motivations at multiple levels of detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Conversational LODS</head><p>When people come together to socialize and engage in conversations, they become players in an elaborate game governed by many complex rules, both explicit and implicit. These rules are needed to effectively establish and maintain channels of communication among multiple participants. The conversational conduct is carefully coordinated so that, for example, speakers get heard, listener feedback gets seen and topic changes can be negotiated. A large portion of the coordination is carried out by nonverbal behaviours. For example, those who wish to get the floor often bring their hands up from resting position to indicate their intent,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 8: BEAT generates behaviours based on rules that operate on linguistic and social parameters</head><p>and in turn receive a gaze cue from the current speaker as confirmation that the floor will indeed be turned over to them. While speaking, the speaker only needs to look at a listener and quickly raise their eyebrows to elicit immediate listener feedback such as a head nod <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>As participants in this game, we rarely have to stop and think about what we are doing, our subconscious social competence keeps us playing along. However, when rules get broken, we quickly spot behaviours that do not fit into the natural flow. Anomalies like that can cause breakdowns in communication or at least have a jarring effect on participants, even for those that are only observing the event <ref type="bibr" target="#b20">[21]</ref>. This has important implications for computer animated social activity, meant to mimic actual human interactions. The animated behaviours cannot break fundamental social rules, or they risk standing out and sending unintended signals of confused activity.</p><p>A computational framework has been developed that allows interactive animated characters to have the same properties as humans in face-to-face conversation, including the ability to produce and to respond to both verbal and nonverbal behaviour <ref type="bibr" target="#b21">[22]</ref>. Such characters, termed embodied conversational agents (ECAs), have to deal with both multimodal generation and interpretation of conversational content (e.g. what is being said) and coordination (e.g. taking and giving the floor). Within this framework, a nonverbal behaviour generation toolkit called BEAT has been created by Cassell et al. <ref type="bibr" target="#b22">[23]</ref>.</p><p>BEAT takes as input the text that is to be spoken by an animated human character, and produces as output appropriate nonverbal behaviours closely synchronized with either synthesized or recorded speech (see Figure <ref type="figure" target="#fig_10">9</ref>). The input text could be from a prepared movie script, it could be automatically generated by a natural language planning module or it could be extracted from an ongoing conversation among human users of a graphical chat system.</p><p>The nonverbal behaviours are assigned on the basis of actual linguistic and contextual analysis of the text, relying on rules derived from extensive research into human conversational behaviour (see Figure <ref type="figure">8</ref>). For example, one rule states that emphasis strokes with a hand or a head nod have a high likelihood of directly preceding or coinciding with a pitch accent, which in turn is commonly associated with new or contrasting spoken material. Such a principled procedural approach guarantees consistency across modalities that is hard to achieve through general stochastic methods and practically impossible to achieve through a purely random behaviour assignment.</p><p>The modular nature of BEAT makes it easy to add new rules to generate behaviours as well as rules to filter out behaviours that are conflicting or that meet certain characteristics. For the purpose of animating groups of people having a conversation, BEAT has been extended to include rules for floor management, speaker addressing, feedback elicitation and corresponding listener feedback.</p><p>With regard to level of detail (LOD), BEAT lends itself well to controlled generation of behaviours based on visual and functional properties. At the generation level, it is easy to choose which behaviour generation rules are active when processing a given conversation. Filter rules can also be used to remove generated behaviours based on LOD criteria. At the animation level, since each behaviour generator annotates its generated behaviour with a visual salience parameter, the LOD framework can selectively drop behaviours as the animated character moves further away or out of the focus of attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Level of Detail AI</head><p>We are also working on integrating an intelligent agent based role-passing technique into the ALOHA framework. When intelligent agents are used in virtual environments it is often required that they behave believably in a range of different situations. For example, it might be required   that within the same simulation an agent is found at work in an office, and then later on enjoying a drink in a bar. See Figure <ref type="figure" target="#fig_11">10</ref> for a typical scenario. The kind of behaviour required of the agent, and the motivations that should drive this behaviour, are quite different in each of these situations. Inspired by Horswill and Zubek <ref type="bibr" target="#b23">[24]</ref>, the technique of rolepassing allows intelligent agents to take on different roles depending on the situation in which they are found.</p><p>Role-passing operates by using a schedule to layer appropriate roles on top of a very basic agent at appropriate times within a simulation. This basic agent has a number of attributes which define a character's personality traits, and is capable of simple behaviours such as moving through a virtual world, using objects and interacting with other agents.</p><p>When a particular role is layered upon this basic agent, it instructs the agent on how to behave in a certain situation. The first key component of a role is a set of motivations which drive the agent. Activation levels for these motivations are extrapolated from the attributes defining the personality traits of the basic agent. Activation of a motivation results in the agent performing a particular task, such as using a particular object. Secondly, a role contains rules which dictate the agent's interaction with other agents. For example, a role should specify when it is appropriate for an agent to stop and interact with another character. A role also contains a set of bindings to objects important to the current role. Finally, a role also defines animations required to render the character in that role.</p><p>The main advantage of role-passing is the simplicity it lends to populating a virtual world with agents. Placing agents within a novel situation involves simply defining a new role. This eases some of the complications involved in attempting to design very general agents capable of behaving realistically in many situations, and avoids having to write completely separate agents for different roles within a single scene. Another advantage of the role-passing technique is that it moves some way towards creating agents capable of being transferred between different applications. Through role-passing, the same basic agent is able to behave believably in very different situations. This is a major research area in intelligent agent technology <ref type="bibr" target="#b24">[25]</ref>.</p><p>Finally, role passing allows the use of level of detail artificial intelligence (LODAI). By assuming and discarding roles as required, motivations unrelated to the current situation encountered by an agent are never considered. The result of this is that motivation levels surplus to the current situation need not be stored and decisions not related to the current situation are never even considered. Additionally, LODAI techniques can be further utilised when defining roles by specifying an agent's behaviour at varying levels of detail. For example, if the agent is not in the current view frame, behaviours required only to make the agent appear believable are omitted and only those behaviours crucial to the simulation need to be included. This avoids updating complex behaviours for less important characters, while still maintaining some basic functionality as they may become more salient later. See MacNamee et al. <ref type="bibr" target="#b25">[26]</ref> for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>This paper has presented ongoing work on a framework for achieving crowd and group simulation in real-time. This allows refinement of the animation both at a macro level, whereby full components of the system may be activated or deactivated based on importance heuristics, and within the individual components themselves. At the geometrical level, subdivision techniques can be used to achieve smooth rendering LOD changes, while other objects can be preemptively simplified. At the motion level, the movements themselves can be simulated at adaptive levels of detail. For behaviour, LODAI can be employed to reduce the computational costs of updating the behaviour of characters that are less important. Purely random playback can be used for characters that are not important to the scene, while motions that are synchronised with speech, using the BEAT framework, can be played for more salient beings. Furthermore, the knowledge embedded in the system can be used to allow the gesture generation engine to make informed decisions about which features are most important to retain based on the salience of an agent or group of agents, thus allowing graceful degradation of the gesture repertoire.</p><p>Although significant gains can be achieved by using level of detail techniques for all tasks involved in the simulation of a complex graphical environment, in-built redundancies could be introduced if the system fails to share the prioritisation information generated by one process with the others. We want to make the LOD resolver more generic, so that truly polymorphic LOD control can be achieved. This will involve developing a seamless interface to the knowledge base that can be used to schedule the processing of each of the constituent components of the system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Natural conversation in a group.</figDesc><graphic coords="1,313.96,413.97,215.40,161.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Extended ALOHA framework with LODs for geometry, motion and behaviour.</figDesc><graphic coords="2,319.34,73.00,204.23,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Subdivision schemes use a mask to define a set of vertices and corresponding weights, which are used to create new vertices or modify existing ones. Different masks are used for vertices on a boundary than the rest of the surface, because on a boundary edge some neighbouring vertices are c The Eurographics Association and Blackwell Publishers Ltd 2002</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Subdivision schemes begin with a low-resolution model.</figDesc><graphic coords="3,82.39,73.81,200.08,259.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: From left to right: two iterations of the linear, butterfly and loop subdivision techniques.</figDesc><graphic coords="3,82.39,383.60,200.03,267.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: High-resolution models created by (from top to bottom) the linear, butterfly and loop subdivision schemes.</figDesc><graphic coords="3,322.57,72.86,197.98,353.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>c</head><label></label><figDesc>The Eurographics Association and Blackwell Publishers Ltd 2002</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: LSS are used to efficiently detect collisions with the virtual humans.</figDesc><graphic coords="5,86.21,73.09,431.14,184.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Dynamic constraint-based solution applied to a hierarchical object -(a) figure is resting under gravity; (b) and (c) show external interaction with the object -object is being dragged around by the right hand.</figDesc><graphic coords="5,131.64,297.21,340.46,184.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>c</head><label></label><figDesc>The Eurographics Association and Blackwell Publishers Ltd 2002</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The BEAT gesture toolkit is used to generate speech synchronized output for the speaker.</figDesc><graphic coords="7,163.44,72.87,277.12,273.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: A group of agents drinking in a bar -suitable roles must be chosen for each individual.</figDesc><graphic coords="7,163.44,385.75,277.12,207.84" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>c The Eurographics Association and Blackwell Publishers Ltd 2002. Published by Blackwell Publishers, 108 Cowley Road, Oxford OX4 1JF, UK and 350 Main Street, Malden, MA 02148, USA.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>c The Eurographics Association and Blackwell Publishers Ltd 2002</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has been supported by Enterprise Ireland and the Higher Education Authority of Ireland.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Flocks, herds, and schools: a distributed behavioral model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings SIGGRAPH 1987</title>
		<meeting>SIGGRAPH 1987</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="25" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Group behaviors for systems with significant dynamics</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Brogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Guiding and interacting with virtual crowds in real-time</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Musse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Garat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Eurographics Workshop on Computer Animation and Simulation (CAS &apos;99)</title>
		<meeting>eeding of the Eurographics Workshop on Computer Animation and Simulation (CAS &apos;99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="23" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A behavioral model for real time simulation of virtual human crowds</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Musse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="152" to="164" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real-time display of virtual humans: level of details and impostors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology, Special Issue on 3D Video Technology</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time rendering of densely populated urban environments</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tecchia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chrysanthou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rendering Techniques&apos;00 (Proceedings of the 10th Eurographics Workshop on Rendering)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="45" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Real time rendering of populated urban environments</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tecchia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Loscos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chrysanthou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Siggraph&apos;01 technical sketch</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ALOHA: adaptive level of detail for human animation: towards a new framework</title>
		<author>
			<persName><forename type="first">T</forename><surname>Giang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O'sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics 2000 short paper proceedings</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="71" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Subdivision surfaces for character animation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Leeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Games Programming Gems III</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="372" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Subdivision surfaces in character animation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Derose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Truong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings SIG-GRAPH 1998</title>
		<meeting>SIG-GRAPH 1998</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Smooth subdivision surfaces based on triangles, Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Loop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
		<respStmt>
			<orgName>University of Utah, Department of Mathematics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A butterfly subdivision scheme for surface interpolation with tension control</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="160" to="190" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interpolating subdivision for meshes with arbitrary topology</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schr√∂der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sweldens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings SIGGRAPH 1996</title>
		<meeting>SIGGRAPH 1996</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="189" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vision-based reaching for autonomous virtual humans</title>
		<author>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O'sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AISB&apos;02 symposium: Animating Expressive Characters for Social Interactions (short paper)</title>
		<meeting>the AISB&apos;02 symposium: Animating Expressive Characters for Social Interactions (short paper)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="69" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large steps in cloth simulation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baraff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH 98 Conference Proceedings</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graceful degradation of collision handling in physically based animation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dingliana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O'sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurographics 2000)</title>
		<meeting>Eurographics 2000)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="239" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fast proximity queries with swept sphere volumes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gottschalk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<idno>TR99-018</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>UNC Chapel Hill</publisher>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Collisions and perception</title>
		<author>
			<persName><forename type="first">C</forename><surname>O'sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dingliana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="151" to="168" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Goffmann</surname></persName>
		</author>
		<title level="m">Behavior in public places; notes on the social organization of gatherings</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Free Press of Glencoe</publisher>
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Conversational Organization: Interaction between speakers and hearers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goodwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Conducting Interaction: Patterns of behavior in focused encounters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kendon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Requirements for an architecture for embodied conversational characters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vilhj√°lmsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bickmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Animation and Simulation &apos;99 (Eurographics Series)</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Thalmann</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="109" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BEAT: behavior expression animation toolkit</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vilhj√°lmsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bickmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="477" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robot architectures for believable game agents</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Horswill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 AAAI Spring Symposium on Artificial Intelligence and Computer Games, AAAI Technical ReportSS-99-02</title>
		<meeting>the 1999 AAAI Spring Symposium on Artificial Intelligence and Computer Games, AAAI Technical ReportSS-99-02</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Can models of agents be transferred between different areas?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Aylett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Knowledge Engineering Review</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="203" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Men behaving appropriately: integrating the role passing technique into the ALOHA system</title>
		<author>
			<persName><forename type="first">B</forename><surname>Macnamee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dobbyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O'sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AISB&apos;02 symposium: Animating Expressive Characters for Social Interactions</title>
		<meeting>the AISB&apos;02 symposium: Animating Expressive Characters for Social Interactions</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="59" to="62" />
		</imprint>
	</monogr>
	<note>short paper</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
