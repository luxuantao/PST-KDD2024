<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">M 3 : Multimodal Memory Modelling for Video Captioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Junbo</forename><surname>Wang</surname></persName>
							<email>junbo.wang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
							<email>wangwei@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Huang</surname></persName>
							<email>yhuang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<email>wangliang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Excellence in Brain Science and Intelligence Technology (CEBSIT)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center for Research on Intelligent Perception and Computing (CRIPAC)</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition (NLPR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Excellence in Brain Science and Intelligence Technology (CEBSIT)</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences (CASIA)</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences (UCAS)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">M 3 : Multimodal Memory Modelling for Video Captioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video captioning which automatically translates video clips into natural language sentences is a very important task in computer vision. By virtue of recent deep learning technologies, video captioning has made great progress.</head><p>However, learning an effective mapping from the visual sequence space to the language space is still a challenging problem due to the long-term multimodal dependency modelling and semantic misalignment. Inspired by the facts that memory modelling poses potential advantages to longterm sequential problems <ref type="bibr" target="#b34">[35]</ref> and working memory is the key factor of visual attention <ref type="bibr" target="#b32">[33]</ref>, we propose a Multimodal Memory Model (M 3 ) to describe videos, which builds a visual and textual shared memory to model the longterm visual-textual dependency and further guide visual attention on described visual targets to solve visual-textual alignments. Specifically, similar to <ref type="bibr" target="#b9">[10]</ref>, the proposed M 3 attaches an external memory to store and retrieve both visual and textual contents by interacting with video and sentence with multiple read and write operations. To evaluate the proposed model, we perform experiments on two public datasets: MSVD and MSR-VTT. The experimental results demonstrate that our method outperforms most of the stateof-the-art methods in terms of BLEU and METEOR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Describing videos with natural sentences automatically also called video captioning is very important for bridging vision and language, which is also a very challenging problem in computer vision. It has plenty of practical applications, e.g., human-robot interaction, video indexing and describing videos for the visually impaired.</p><p>Video captioning involves in understanding both vision * Corresponding Author: Wei Wang and language, and then builds the mapping from visual contents to words. As we know, video as image sequence contains rich information about actor, object, action, scene and their interactions. It is very difficult for the existing methods to use a single visual representation <ref type="bibr" target="#b30">[31]</ref> to capture all these information over a long period. Yao et al. <ref type="bibr" target="#b36">[37]</ref> at-tempt to dynamically select multiple visual representations based on temporal attention mechanism which is driven by the hidden representations from a Long Short-Term Memory (LSTM) text decoder. The LSTM text decoder, which integrates the information from both words and selected visual contents, models the sentence generation and guides visual selection. Recently, neural memory models have been proposed and successfully applied to question answering <ref type="bibr" target="#b34">[35]</ref>, which show greater advantages than LST-M to model long-term dependency in sequential problems. Furthermore, working memory is one of the key factors to guide eye movement in visual attention for efficient visual search, which has been computationally modelled in <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b32">[33]</ref>. Explicitly modelling memory for video and sentence in visual captioning can not only model the long-term visual-textual dependency, but also guide visual attention to solve multimodal semantic misalignment. As we know, few memory models have been proposed for multimodal sequences.</p><p>In this paper, we propose a Multimodal Memory Model (M 3 ) to describe videos, which builds a visual and textual shared memory to guide visual attention on described targets and enhance the long-term visual-textual dependency modelling. Inspired by Neural Turing Machines <ref type="bibr" target="#b9">[10]</ref>, the proposed M 3 attaches an external memory to store and retrieve both visual and textual information by interacting with video and sentence with multiple read and write operations. Fig. <ref type="figure" target="#fig_0">1</ref> shows the overall framework of multimodal memory modelling for video captioning, which consists of three key components: convolutional neural networks (CN-N) based video encoder, multimodal memory and LSTMbased text decoder. (1) CNN-based video encoder first extracts video frame/clip features using pretrained 2D/3D C-NNs which are often used for image/video classification. The extracted features {v i } n i=1 form the original video representation. Similar to <ref type="bibr" target="#b36">[37]</ref>, temporal soft-attention Attend is used to select visual information most related to each word. But very different from <ref type="bibr" target="#b36">[37]</ref> using the hidden states from a LSTM decoder, we guide the soft-attention based on the content from a multimodal memory (read att in Fig. <ref type="figure" target="#fig_0">1</ref> denotes the content read from memory for attention). Then the selected visual information will be written into the memory (write att denotes the content written to memory from selective attention). (2) LSTM-based text decoder models the sentence generation with a LSTM-RNN architecture, which predicts the {t + 1} th word conditioned on not only previous hidden representation LST M t but also the content read from the multimodal memory (read dec denotes the content read from memory for decoder). Besides word prediction, the text decoder also writes the updated representation to the memory (write dec denotes the content written to memory from the decoder). (3) Multimodal memory contains a memory matrix M em to interact with video and sentence, e.g., write hidden representation from the LSTM decoder to memory write dec , and read memory contents for the decoder read dec . Each write operation will update the multimodal memory, e.g., from M em t to M em t+1 . In Fig. <ref type="figure" target="#fig_0">1</ref>, we illustrate the procedure of memory-video/sentence interactions: 1 write hidden states to update memory, 2 read the updated memory content to perform soft-attention, 3 write selected visual information to update memory again, 4 read the updated memory content for next word prediction. The main contributions of our work are summarized as follows:</p><p>• To our knowledge, we are the first to model multimodal data by selective reading/writing both visual contents and sentences with a shared memory structure, and apply it to video captioning.</p><p>• The proposed model performs better than most of the state-of-the-art methods on two public datasets: MSVD and MSR-VTT, which demonstrates its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly introduce some existing work that closely related to our proposed model.</p><p>Video Captioning Video captioning has been investigated for a long period due to its importance in bridging vision and language. Various methods have been proposed to solve this problem, which can be mainly categorized into two classes. The first class <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref> detect the attributes of given videos and derive the sentence structure with predefined sentence templates. Then probabilistic graphical models are used to align the phases to the attributes. Similar to image captioning, these methods always generate grammatically correct sentences, but lose the novelty and flexibility of the sentence. The second class of methods inspired by Neural Machine Translation (NMT) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref> map video sequence to sentence by virtue of deep neural networks, e.g., CNNs and RNNs. Venugopalan et al. <ref type="bibr" target="#b30">[31]</ref> apply average pooling to extract the features of multiple video frames and use a two-layer LSTM network on these features to generate descriptions. In order to enhance video representation, Ballas et al. <ref type="bibr" target="#b0">[1]</ref> exploit the intermediate visual representation extracted from pre-trained image classification models, and Pan et al. <ref type="bibr" target="#b19">[20]</ref> propose a hierarchical recurrent neural encoder to explore the temporal transitions with different granularities. In order to generate more sentences for each video, Yu et al. <ref type="bibr" target="#b37">[38]</ref> exploit a hierarchical recurrent neural network decoder which contains a sentence generator and a paragraph generator. To emphasize the mapping from video to sentence, Yao et al. <ref type="bibr" target="#b36">[37]</ref> propose a temporal attention model to align the most relevant visual segments to the generated captions, and Pan et al. <ref type="bibr" target="#b20">[21]</ref> propose a long short-term memory with a visual-semantic embedding model. Recently, the second class of deep learning based methods have made much progress in video captioning. We augment the existing deep learning based models with an external memory to guide visual attention and enhance the long-term visual-textual dependency modelling in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Modelling</head><p>To extend the memory ability of traditional neural networks, Graves et al. <ref type="bibr" target="#b9">[10]</ref> propose a Neural Turing Machine (NTM) which holds an external memory to interact with the internal state of neural networks by attention mechanism. NTM has shown the potential of storage and access of information over long time periods which has always been problematic for RNNs, e.g., copying, sorting and associative recall. Besides memory matrix in NTM, memory is also modelled as continuous and differentiable doubly-linked lists and stacks <ref type="bibr" target="#b14">[15]</ref>, queues and deques <ref type="bibr" target="#b10">[11]</ref>. Different from exploring various forms of dynamic storages, Weston et al. <ref type="bibr" target="#b33">[34]</ref> model large long-term static memory. The internal information stored in the static memory is not modified by external controllers, which is specially used for reading comprehension. These memory networks have been successfully applied to the tasks which need dynamic reasoning, e.g., textual question answering <ref type="bibr" target="#b2">[3]</ref> and visual question answering <ref type="bibr" target="#b34">[35]</ref>. As we know, few memory models have been proposed for video captioning. In this paper, we will propose an external multimodal memory to interact with video and sentence simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Model</head><p>In this section, we will first introduce three key components of our model including: 1) convolutional neural networks (CNN) based video encoder, 2) Long Short-Term Memory (LSTM) based text decoder, and 3) multimodal memory. Then we will explain the procedure of model training and inference in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">CNN-Based Video Encoder</head><p>Convolutional neural networks (CNNs) have achieved great success in many computer vision tasks recently, e.g., image classification <ref type="bibr" target="#b17">[18]</ref> and object detection <ref type="bibr" target="#b8">[9]</ref>. Due to the power of representation learning, CNNs pre-trained by these tasks can be directly transferred to other computer vision tasks as generic feature extractors. To make better video representations, we consider using pre-trained 2D CNNs to extract appearance features of videos, and pretrained 3D CNNs to obtain motion features of videos since the temporal dynamics is very important for video understanding. In particular for an input video, we first sample it with fixed number of frames/clips n, and then exploit the pre-trained 2D CNNs/3D CNNs to extract features of each frame/clip. We denote the obtained video representation as</p><formula xml:id="formula_0">V = {v 1 , v 2 , v 3 , . . . , v n },</formula><p>where n is the number of sampled frames/clips. In the following, we define the proposed temporal attention model. Given the visual representations of the video, and the content r read from the multimodal memory. By virtue of a single layer neural network followed by a softmax function, the attention weights over all locations of the input video can be formulated as follows:</p><formula xml:id="formula_1">α t i = sof tmax w T tanh (W r r vr t + U α v i + b α )<label>(1)</label></formula><p>where W r , U α , b α , and w are the parameters to be learned.</p><p>Different from <ref type="bibr" target="#b36">[37]</ref>, here we incorporate the content read from multimodal memory instead of the previous hidden state from LSTM network. We argue that the hidden state from LSTM network can not fully represent all the information of previous words, while our multimodal memory can well keep them. Based on the attention weights, the final representation of input video can be gained by:</p><formula xml:id="formula_2">V t = n i=1 α t i v i<label>(2)</label></formula><p>To simplify the following description, the above procedure can be abbreviated as follows:</p><formula xml:id="formula_3">V t = β (V, r)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LSTM-Based Text Decoder</head><p>Different from the commonly used unimodal LSTM <ref type="bibr" target="#b38">[39]</ref>, we incorporate the fused multimodal information r t as another input, which is read from our multimodal memory during caption generation as demonstrated in next section. For given sentences, we use one-hot vector encoding to represent each word. By denoting the input word sequence as {y t |t = 0, 1, 2, • • • , T }, and the corresponding embedding vector of word y t as E t , the hidden activations h t at time t can be computed as follows.</p><formula xml:id="formula_4">i t = σ (W i E t−1 + U i h t−1 + M i r t + b i )<label>(4)</label></formula><formula xml:id="formula_5">f t = σ (W f E t−1 + U f h t−1 + M f r t + b f )<label>(5)</label></formula><formula xml:id="formula_6">o t = σ (W o E t−1 + U o h t−1 + M o r t + b o ) (6) ct = φ (W c E t−1 + U c h t−1 + M c r t + b c )<label>(7)</label></formula><formula xml:id="formula_7">c t = i t ⊙ ct + f t ⊙ c t−1 (<label>8</label></formula><formula xml:id="formula_8">)</formula><formula xml:id="formula_9">h t = o t ⊙ φ (c t )<label>(9)</label></formula><p>where the default operation between matrices is matrix multiplication, ⊙ denotes an element-wise multiplication, W , U , and M denote the shared weight matrices to be learned, and b denotes the bias term. ct is the input to the memory cell c t , which is gated by the input gate i t . σ denotes the element-wise logistic sigmoid function, and φ denotes hyperbolic tangent function tanh. For clear illustration, the process of language modelling mentioned above can be abbreviated as follows.</p><formula xml:id="formula_10">h t = ψ (h t−1 , c t−1 , y t−1 , r t )<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multimodal Memory</head><p>Our multimodal memory at time t is a N × M matrix M t , where N denotes the number of memory locations and M denotes the vector length of each location. The memory interacts with the LSTM-based language model and CNNbased visual model via selective read and write operations. Since there exists bimodal information, i.e., video and language, we employ two independent read/write operations to guide the information interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Memory Interaction</head><p>The interaction of visual information and textual elements is performed in the following order.</p><p>Writing hidden representations to update memory Before predicting the next word during the process of caption generation, our LSTM-based language model will write previous hidden representations into the multimodal memory, to summarize the previous textual information. We denote the current textual weighting vector, textual erase vector and textual add vector as w sw t , e sw t and a sw t , respectively, all of which are emitted by the LSTM-based language model. The elements of textual erase vector e sw t lie in the range of (0,1). The lengths of textual erase vector e sw t and textual add vector a sw t are both M . Since both the textual erase vector and textual add vector have M independent elements, the elements in every memory location can be erased or added in a fine-grained way. Then the textual information can be written into the memory as follows.</p><formula xml:id="formula_11">M t (i) = M t−1 (i) [1 − w sw t (i) e sw t ] + w sw t (i) a sw t (<label>11</label></formula><formula xml:id="formula_12">)</formula><p>where i ∈ [1, N ] denotes i-th memory location.</p><p>Reading the updated memory for temporal attention After writing textual information into the memory, the updated memory content is read out to guide a visual attention model to select prediction-related visual information. Assuming that the current visual weighting vector over the N locations at time t is w vr t , which needs to be normalized as follows.</p><formula xml:id="formula_13">N i=1 w vr t (i) = 1, 0 ≤ w vr t (i) ≤ 1, ∀i ∈ [1, N ]<label>(12)</label></formula><p>Then the visual read vector r vr t returned by the visual attention model is computed as a linear weighting of the rowvectors M t (i):</p><formula xml:id="formula_14">r vr t = N i=1 w vr t (i)M t (i)<label>(13)</label></formula><p>Temporal attention selection for video representation After reading the updated memory content, we apply the proposed temporal attention model to select most relevant video representations by increasing corresponding weights, which is very effective when there exist explicit visualsemantic mappings.</p><formula xml:id="formula_15">c t = β (V, r vr t )<label>(14)</label></formula><p>Writing selected visual information to update memory</p><p>After selecting visual information via the attention model above, the information will be written into the memory for updating. Similar to the operation of writing hidden representations into the memory, the current visual weighting vector w vw t , visual erase vector e vw t and visual add vector a vw t are all emitted by the visual attention model. The elements of visual erase vector e vw t lie in the range of (0,1). The lengths of visual erase vector e vw t and visual add vector a vw t are both M . Then the visual information can be written into the memory as follows.</p><formula xml:id="formula_16">M t (i) = M t (i) [1 − w vw t (i) e vw t ] + w vw t (i) a vw t (<label>15</label></formula><formula xml:id="formula_17">)</formula><p>Reading the updated memory for LSTM-based language model When finishing the above writing operation, the updated memory is read out for language modelling. Similarly, assuming that the textual weighting vector over the N locations at the current time is w sr t , which also has to be normalized as follows.</p><formula xml:id="formula_18">N i=1 w sr t (i) = 1, 0 ≤ w sr t (i) ≤ 1, ∀i ∈ [1, N ]<label>(16)</label></formula><p>Then the textual read vector r sr t returned by the LSTMbased language model is computed as a linear weighting of the row-vectors M t (i):</p><formula xml:id="formula_19">r sr t = N i=1 w sr t (i)M t (i)<label>(17)</label></formula><p>Computing of RNN-based language model After getting the reading information from the updated memory, we can compute the current hidden state of LSTM-based language model by calling the following function.</p><formula xml:id="formula_20">h t = ψ (h t−1 , c t−1 , y t−1 , r sr t )<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Memory Addressing Mechanisms</head><p>As stated in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b9">10]</ref>, the objective function is hard to converge when using a location-based addressing strategy. Therefore, we use a content-based addressing strategy to update the above read/write weighting vector. During the process of content-based addressing, each read/write head (e.g., the LSTM-based text decoder) first produces a key vector k t and a sharpening factor β t . The key vector k t is mainly used for comparing with each memory vector M t (i) by a similarity measure function K, and the sharpening factor β t is employed for regulating the precision of the focus. Then all of them can be computed as follows.</p><formula xml:id="formula_21">K (x, y) = x • y x • y + ε (19) d t (i) = β t K (k t , M t (i))<label>(20)</label></formula><formula xml:id="formula_22">w t (i) = sof tmax (d t (i))<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training and Inference</head><p>Assuming that there are totally L training videodescription pairs x i , y i in the entire training dataset, where the description y i has a length of t i . The overall objective function used in our model is the averaged loglikelihood over the whole training dataset plus a regularization term.</p><formula xml:id="formula_23">L (θ) = 1 L L i=1 ti j=1 log ρ y i j |y i 1:j−1 , x i , θ + λ θ 2 2<label>(22)</label></formula><p>where y i j is a one-hot vector used to denote the input word, θ is all parameters to be optimized in the model, and λ denotes the regularization coefficient. As all components in our model including multimodal memory components are differential, we can use Stochastic Gradient Descent (SGD) to learn the parameters.</p><p>Similar to most LSTM language models, we use a softmax layer to model the next word's probability distribution over the whole vocabulary.</p><formula xml:id="formula_24">z t = tanh (W v V t + W h h t + W e y t−1 + b h )<label>(23)</label></formula><formula xml:id="formula_25">ρ t = sof tmax (U ρ z t + b ρ )<label>(24)</label></formula><p>where W v ,W h ,W e ,b h ,U ρ , and b ρ are the parameters to be estimated. Based on the probability distribution ρ t , we can recursively sample y t until obtaining the end of symbol in the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To validate the effectiveness of the proposed model, we perform extensive experiments on two public video captioning datasets. The one is Microsoft Video Description Dataset (MSVD) <ref type="bibr" target="#b3">[4]</ref> which has been used by most of the state-of-the-art methods. The other is recently released Microsoft Research-Video to Text (MSR-VTT) <ref type="bibr" target="#b35">[36]</ref> which is the largest dataset in terms of number of sentence and vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets Microsoft Video Description Dataset</head><p>Microsoft Video Description Dataset (MSVD) <ref type="bibr" target="#b3">[4]</ref> consists of 1970 videos which range from 10 seconds to 25 seconds. Each video has multi-lingual descriptions which are labelled by the Amazon's Mechanical Turk workers. For each video, the descriptions depict a single activity scene with about 40 sentences. So there are about 80,000 video-description pairs. Following the standard split <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b20">21]</ref>, we divide the original dataset into a training set of 1200 videos, a validation set of 100 videos, and a test set of 670 videos, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Microsoft Research-Video to Text Dataset</head><p>Microsoft Research-Video to Text Dataset (MSR-VTT) is the recently released largest dataset in terms of number of sentence and vocabulary, which consists of 10,000 video clips and 200,000 sentences. Each video clip is labelled with about 20 sentences. Similar to MSVD, the sentences are annotated by Amazon's Mechanical Turk workers. With the split in <ref type="bibr" target="#b35">[36]</ref>, we divide the original dataset into a training set of 6513 videos, a validation set of 497 videos and a testing set of 2990 videos, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data Preprocessing Video Preprocessing</head><p>Instead of extracting features for each video frame, we uniformly sample K frames from original video for feature extraction. When the video length is less than K, we pad zero frames at the end of original frames. Empirically, we set K to 28 for 98 frames per video in MSVD, and set K to 40 for 149 frames per video in MSR-VTT. For the extensive comparisons, we extract features from both pretrained 2D CNN networks, e.g., GoogleNet <ref type="bibr" target="#b25">[26]</ref>, VGG-19 <ref type="bibr" target="#b22">[23]</ref>, Inception-V3 <ref type="bibr" target="#b26">[27]</ref>, ResNet-50 <ref type="bibr" target="#b12">[13]</ref>, and 3D CNN networks, e.g., C3D <ref type="bibr" target="#b28">[29]</ref>. Specifically, we extract the features of the pool5/7x7 s1 layer in GoogleNet, the fc7 layer in VGG-19, the pool3 layer in Inception-V3, the pool5 layer in ResNet-50 and the fc6 layer in C3D. Description Preprocessing The descriptions in MSVD and MSR-VTT are all converted into lower case. To reduce unrelated symbols, we tokenize all sentences by NLTK toolbox<ref type="foot" target="#foot_0">1</ref> and remove punctuations. The vocabulary in MSVD is about 13,000 while the vocabulary in MSR-VTT is about 29,000. For convenience, we set the vocabulary size to 20,000 for both datasets. So the rare words in MSR-VTT are eliminated to further reduce the vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Metrics</head><p>In this paper, we adopt two standard evaluation metrics: BLEU <ref type="bibr" target="#b21">[22]</ref> and METEOR <ref type="bibr" target="#b6">[7]</ref>, which are widely used in machine translation and image/video captioning. The BLEU metric measures the n-grams precision between generated sentence and original description, which correlates highly with human evaluation results. The METEOR metric measures the word correspondences between generated sentences and reference sentences by producing an alignment <ref type="bibr" target="#b4">[5]</ref>. METEOR is often used as a supplement to BLEU. To guarantee a fair comparison with previous methods, we utilize the Microsoft COCO Caption Evaluation tool <ref type="bibr" target="#b4">[5]</ref> to gain all experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experimental Settings</head><p>During model training, we add a start tag and an end tag to the sentence in order to deal with variable-length sentences. We also add masks to both sentences and visual features for the convenience of batch training. Similar to <ref type="bibr" target="#b36">[37]</ref>, the sentences with length larger than 30 in MSVD and the sentences with length larger than 50 in MSR-VTT are removed. For the unseen words in the vocabulary, we set them to unknown flags. Several other parameters, e.g., word embedding dimension (468), beam size <ref type="bibr" target="#b4">(5)</ref> and the size of multimodal memory matrix (128,512), are set using the validation set. To reduce the overfitting during training, we apply dropout <ref type="bibr" target="#b23">[24]</ref> with a rate of 0.5 on the output of fully connected layers and the output of LSTMs but not on the recurrent transitions. To further prevent gradient explosion, we clip the gradients to <ref type="bibr">[-10,10]</ref>. The optimization algorithm is ADADELTA <ref type="bibr" target="#b39">[40]</ref> which we find fast in convergence. Table <ref type="table">2</ref>. The performance comparison with the other five state-ofthe-art methods using multiple visual feature fusion on MSVD. Here V, C, I and G denote VGG-19 <ref type="bibr" target="#b22">[23]</ref>, C3D <ref type="bibr" target="#b28">[29]</ref>, Inception-V3 <ref type="bibr" target="#b26">[27]</ref> and GoogleNet <ref type="bibr" target="#b25">[26]</ref>, respectively. Table <ref type="table">3</ref>. The performance comparison with SA <ref type="bibr" target="#b36">[37]</ref> using different visual features on MSR-VTT. Here V and C denote VGG-19 <ref type="bibr" target="#b22">[23]</ref> and C3D <ref type="bibr" target="#b28">[29]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Quantitative Analysis</head><formula xml:id="formula_26">Method B@1 B@2 B@3 B@4 METEOR FGM - - -<label>13</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Experimental Results on MSVD</head><p>For comprehensive experiments, we evaluate and compare with the state-of-the-art methods using single visual feature and multiple visual feature fusion, respectively. Before the comparisons to these methods, we refer to ten state-ofthe-art approaches( <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b18">[19]</ref>) as these abbreviations (FGM, LSTM-YT, SA, S2VT, LSTM-E, p-RNN, HRNE, BGRCN, MAA, RMA). When using single visual feature, we evaluate and compare our model with the above ten state-of-the-art approaches. The experimental results in terms of BLEU (ngram) and METEOR are shown in Table <ref type="table" target="#tab_0">1</ref>. Here we give the best single feature results of the compared ten methods, and show the results of the proposed M 3 together with five single features, e.g., VGG-19 <ref type="bibr" target="#b22">[23]</ref>, C3D <ref type="bibr" target="#b28">[29]</ref>, Inception-V3 <ref type="bibr" target="#b26">[27]</ref>, ResNet-50 <ref type="bibr" target="#b12">[13]</ref> and GoogleNet <ref type="bibr" target="#b25">[26]</ref>. Among these compared methods, SA <ref type="bibr" target="#b36">[37]</ref> is the most similar method to ours, which also has an attention-driven video encoder and LSTM-based text decoder but no external memory. When both models use the same GoogleNet feature, our M 3 -google can make a great improvement over SA by 51.17 = 8.5% in the METEOR score, respectively. It can be concluded that the better performance of our model benefits from multimodal memory modelling. In addition, our five M 3 models outperform all the other methods except HRNE <ref type="bibr" target="#b19">[20]</ref> in terms of METEOR. It is because HRNE <ref type="bibr" target="#b19">[20]</ref> specially focuses on building a hierarchical video encoder for captioning. To be noted, both MAA <ref type="bibr" target="#b7">[8]</ref> and RMA <ref type="bibr" target="#b18">[19]</ref> apply a different memory modelling for video captioning, but our model apparently performs much better than them by a large margin in many evaluation metrics, which proves the superiority of our model in the video captioning. To further compare the results of the five M 3 models using different visual features, we can see that M 3 -inv3 achieves the best performance, following by M 3 -res, M 3 -google and M 3 -vgg19. The performance rank is very similar to that of these methods' image classification accuracy on ImageNet <ref type="bibr" target="#b17">[18]</ref>, which proves that visual feature is very important for video captioning. Actually, the same conclusion has been drawn in image captioning where GoogleNet features ob-Generated Sentence: SA: someone is playing 3 :a man is drawing on a piece of paper Reference Sentence: 1. a person is drawing a picture 2. a person is drawing a cartoon 3. the man is drawing a cartoon Generated Sentence: SA: a man is playing a guitar . Descriptions generated by SA-google, our M 3 -google and human-annotated ground truth on the test set of MSVD. We can see that, M 3 -google generates more relevant object terms than SA-google ("basketball" vs. "soccer ball"), and M 3 -google places more focus on the described targets than SA-google ("dog" vs. "guitar"). In particular, M 3 -google can generate longer sentences to describe more visual contents, e.g., "mixing ingredients in a bowl", "slicing a carrot with a knifes". tain better results than VGG-19 features <ref type="bibr" target="#b31">[32]</ref>.</p><p>When using multiple visual feature fusion, we compare our model with the other five state-of-the-art approaches( <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b1">[2]</ref>). The comparison results are shown in Table <ref type="table">2</ref>. SA-G-3C <ref type="bibr" target="#b36">[37]</ref> uses the combination of GoogleNet feature and 3D-CNN feature. S2VT-rgb-flow <ref type="bibr" target="#b29">[30]</ref> uses the two-stream features consisting of RGB feature extracted from VGG-16 networks and optical flow feature extracted from AlexNet <ref type="bibr" target="#b17">[18]</ref>. Both LSTM-E-VC <ref type="bibr" target="#b20">[21]</ref> and p-RNN-VC <ref type="bibr" target="#b37">[38]</ref> combine VGG-19 feature and C3D feature. We propose M 3 -VC and M 3 -IC for comparison. M 3 -VC also uses VGG-19 feature and C3D feature while M 3 -IC uses Inception-V3 feature and C3D feature. They all perform better than the other methods in terms of the two metrics, which proves the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Experimental Results on MSR-VTT</head><p>MSR-VTT is a recently released benchmark dataset <ref type="bibr" target="#b35">[36]</ref> which has the largest number of video-sentence pairs. Considering that there are few methods tested on this dataset, we compare our model with SA <ref type="bibr" target="#b36">[37]</ref> which is the most similar work to ours. Similarly, we perform experiments with these two methods using single visual feature and multiple visual feature fusion simultaneously. The comparison results are reported in Table <ref type="table">3</ref>. SA-V and SA-C use the VGG-19 feature and C3D feature, respectively. SA-VC fuses these two kinds of features. Our M 3 -V, M 3 -C and M 3 -VC use the same features with the corresponding SA methods. It can be seen that our methods consistently outperform the corresponding SAs. The improved performance proves the importance of multimodal memory in our M 3 again. In addition, from either M 3 or SA, we can see that the results from C3D feature are generally better than those using VGG-19 feature. It may be that the motion information is very critical for the video representation in this dataset, because C3D feature encodes both visual appearance and motion information in video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Analysis</head><p>We evaluate our model with BLEU (n-gram) and ME-TEOR above, which quantitatively reveal the relevance between generated sentence and human-annotated sentence. In this section, we will qualitatively analyze our model through visualizing the generated sentences and the corresponding attention shift across visual frames. Fig. <ref type="figure">2</ref> illustrates several descriptions generated by our M 3 -google, SA-google <ref type="bibr" target="#b36">[37]</ref> and human-annotated ground truth on the test set of MSVD. We can see that our M 3 -google generates more relevant object terms than SA-google ("basketball" vs. "soccer ball" in the third video), and M 3 -google places more focus on the described targets than SA-google ("dog" vs. "guitar" in the second video). Particularly, M 3google can generate longer sentences to describe more visual contents, e.g., "mixing ingredients in a bowl" and "slicing a carrot with a knifes" in the final two videos. All these results demonstrate the effectiveness of our method. Fig. <ref type="figure">3</ref> shows the attention shift of our M 3 -google and SA-google <ref type="bibr" target="#b36">[37]</ref> across multiple frames when generating the sentence. There are 14 frames sampled from a testing video in MSVD, our M 3 -google generates "a shark is swimming in the water" while SA-google generates "a dolphin is swimming". The attention weights of several generated key words corresponidng to the 14 frames are shown as bar charts. We can see that the two methods show very different attention distributions for each word. It can be seen that the generated sentence is very relevant to the semantic object and action of the video, which further demonstrate the correctness that the proposed M 3 can guide the attention by multimodal memory modelling. Compared with SA, our model not only can identify the object ('shark' vs 'dolphin'), but also can attend to specifical frames relevant to the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>This paper has proposed a Multimodal Memory Model to describe videos, which builds a visual and textual shared memory to model the long-term visual-textual dependency and further guide visual attention. The extensive experimental results on two publicly available benchmark datasets demonstrate that our method outperforms the state-of-theart methods in terms of BLEU and METEOR metrics.</p><p>As we can see from the experimental results, video representation is very important for the performance of video captioning. In the future, we will consider to improve video representation learning algorithm, and integrate video feature extraction networks with multimodal memory networks to form an end-to-end deep learning system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The overall framework of M 3 for video captioning. It contains a CNN-based video encoder, a multimodal memory and a LSTM-based text decoder which are denoted by dashed box in different colors. The multimodal memory M em stores and retrieves both visual and textual information by interacting with video and sentence with multiple read and write operations. The proposed M 3 with explicit memory modelling can not only model the longterm visual-textual dependency, but also guide visual attention for effective video representation. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>−40. 3 40. 3 = 26 .</head><label>3326</label><figDesc>9% in the BLEU@4 score and by 31.47−29.0 29.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 :Figure 2</head><label>32</label><figDesc>Figure2. Descriptions generated by SA-google, our M 3 -google and human-annotated ground truth on the test set of MSVD. We can see that, M 3 -google generates more relevant object terms than SA-google ("basketball" vs. "soccer ball"), and M 3 -google places more focus on the described targets than SA-google ("dog" vs. "guitar"). In particular, M 3 -google can generate longer sentences to describe more visual contents, e.g., "mixing ingredients in a bowl", "slicing a carrot with a knifes".</figDesc><graphic url="image-30.png" coords="7,99.03,394.48,216.00,80.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 :Figure 3 .</head><label>33</label><figDesc>Figure 3. The attention shift of our M 3 -google and SA-google [37] across 14 sampled frames when generating the sentence. The attention weights of several generated key words corresponidng to the 14 frames are shown as bar charts.</figDesc><graphic url="image-32.png" coords="7,98.61,524.38,216.00,77.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The performance comparison with the other ten state-ofthe-art methods using single visual feature on MSVD. The results of the proposed M 3 with five single features are shown at the bottom of the table. We compare the best single feature results of the other ten methods at the top of the table.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>.68</cell><cell>23.90</cell></row><row><cell>LSTM-YT</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>33.29</cell><cell>29.07</cell></row><row><cell>SA</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>40.28</cell><cell>29.00</cell></row><row><cell>S2VT</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.2</cell></row><row><cell>LSTM-E</cell><cell>74.9</cell><cell>60.9</cell><cell>50.6</cell><cell>40.2</cell><cell>29.5</cell></row><row><cell>p-RNN</cell><cell>77.3</cell><cell>64.5</cell><cell>54.6</cell><cell>44.3</cell><cell>31.1</cell></row><row><cell>HRNE</cell><cell>79.2</cell><cell>66.3</cell><cell>55.1</cell><cell>43.8</cell><cell>33.1</cell></row><row><cell>BGRCN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>48.42</cell><cell>31.70</cell></row><row><cell>MAA</cell><cell cols="4">79.40 67.10 56.80 46.10</cell><cell>31.80</cell></row><row><cell>RMA</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>45.7</cell><cell>31.9</cell></row><row><cell>M 3 -c3d</cell><cell cols="4">77.30 68.20 56.30 45.50</cell><cell>29.91</cell></row><row><cell>M 3 -vgg19</cell><cell cols="4">77.70 67.50 58.90 49.60</cell><cell>30.09</cell></row><row><cell>M 3 -google</cell><cell cols="4">79.05 68.74 60.00 51.17</cell><cell>31.47</cell></row><row><cell>M 3 -res</cell><cell cols="4">80.80 69.90 60.40 49.32</cell><cell>31.10</cell></row><row><cell>M 3 -inv3</cell><cell cols="4">81.56 71.39 62.34 52.02</cell><cell>32.18</cell></row><row><cell>Method</cell><cell>B@1</cell><cell>B@2</cell><cell>B@3</cell><cell>B@4</cell><cell>METEOR</cell></row><row><cell>SA-G-3C</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>41.92</cell><cell>29.60</cell></row><row><cell>S2VT-rgb-flow</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>29.8</cell></row><row><cell>LSTM-E-VC</cell><cell>78.8</cell><cell>66.0</cell><cell>55.4</cell><cell>45.3</cell><cell>31.0</cell></row><row><cell>p-RNN-VC</cell><cell>81.5</cell><cell>70.4</cell><cell>60.4</cell><cell>49.9</cell><cell>32.6</cell></row><row><cell>HBA</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>42.5</cell><cell>32.4</cell></row><row><cell>M 3 -VC</cell><cell cols="4">81.90 71.26 62.08 51.78</cell><cell>32.49</cell></row><row><cell>M 3 -IC</cell><cell cols="4">82.45 72.43 62.78 52.82</cell><cell>33.31</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://www.nltk.org/index.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>This work is jointly supported by National Key Research and Development Program of China (2016YFB1001000), National Natural Science Foundation of China (61525306, 61633021, 61721004, 61572504). In addition, this work is also supported by grants from NVIDIA and the NVIDIA DGX-1 AI Supercomputer.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06432</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical boundary-aware neural encoder for video captioning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Largescale simple question answering with memory networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<title level="m">Microsoft coco captions: Data collection and evaluation server</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Memory-augmented attention modelling for videos</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02261</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to transduce with unbounded memory</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Y-outube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>IC- CV</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using fast weights to attend to the recent past</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jimmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lonescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inferring algorithmic patterns with stack-augmented recurrent nets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating natural-language video descriptions using text-mined knowledge</title>
		<author>
			<persName><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent memory addressing for describing videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hierarchical recurrent neural encoder for video representation with application to captioning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03476</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.01861</idno>
		<title level="m">Jointly modeling embedding and translation to bridge video and language</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>iv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Integrating language and vision to generate natural language descriptions of videos in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sequence to sequence-video to text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno>arX- iv:1412.4729</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Show and tell: Lessons learned from the 2015 mscoco image captioning challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simulating human saccadic scanpaths on natural images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Memory networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno>iv:1603.01417</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Video paragraph captioning using hierarchical recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.07712</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
