<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explanation-Based Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="1992-05">May 1992</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
							<email>etzioni@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>FR-35, 98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explanation-Based Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="1992-05">May 1992</date>
						</imprint>
					</monogr>
					<idno type="MD5">FA4F11430934CF5B0C7A8CE9BB20475F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The impact of Explanation-Based Learning (EBL) on problem-solving eciency varies greatly from one problem space to another. In fact, seemingly minute modications to problem space encoding can drastically alter EBL's impact. For example, while prodigy/ebl (a state-of-the-art EBL system) signicantly speeds up the prodigy problem solver in the Blocksworld, prodigy/ebl actually slows prodigy down in a representational variant of the Blocksworld constructed by adding a single, carefully chosen, macro-operator to the Blocksworld operator set. Although EBL has been tested experimentally, no theory has been put forth that accounts for such phenomena. This paper presents such a theory.</p><p>The theory exhibits a correspondence between a graph representation of problem spaces and the proofs used by EBL systems to generate search-control knowledge. The theory relies on this correspondence to account for the variations in EBL's impact. This account is validated by static, a program that extracts EBL-style control knowledge directly from the graph representation, without using training examples. When tested on prodigy/ebl's benchmark tasks, static was up to three times as eective as prodigy/ebl in speeding up prodigy.</p><p>referred to as \prodigy+ebl" (see, for example, Figure <ref type="figure">1</ref>).</p><p>The Structural Thesis: there is a correspondence between the PSG representation and the proofs used by EBL systems to generate search-control knowledge. As a result, automatic PSG analysis can be used to compute the weakest preconditions of the proofs and to discriminate between recursive and nonrecursive proofs.</p><p>The thesis is validated by static, a novel computational procedure that yields eective search-control knowledge based on PSG analysis. Given a problem-space denition, static generates and traverses the appropriate PSGs, systematically searching for PSG subgraphs that correspond to nonrecursive EBL proofs. static outputs the control knowledge EBL would have acquired from these proofs. static's performance is compared with that of prodigy/ebl in Section 6.2; its design is described in detail in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Asymptotic Analysis EBL's impact on problem solving is a complex phenomenon that depends on a large number of factors including the problem space denition, the problem distribution, the problem solver's search method, the problem solver's matcher, EBL's target concepts, EBL's training examples, and more. The analysis in this paper abstracts away from many of these intricate details, and attempts to identify key problem space characteristics that strongly inuence EBL's performance.</p><p>The analysis contrasts recursive and nonrecursive proofs and shows that, as state size increases, the size of nonrecursive proof trees is bounded whereas the size of recursive proof trees is not. It follows that the cost of matching the weakest preconditions of recursive proofs increases exponentially with state size, but only polynomially in the case of nonrecursive proofs. The analysis also shows that, unless the depth of the problem solver's recursions is bounded, no nite set of recursive proofs can achieve a polynomial-node search in the limit. However, when learning from nonrecursive proofs, EBL can achieve polynomial-time</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Controlling search is a central concern for AI. Explanation-Based Learning (EBL) is a widelyused technique for acquiring search-control knowledge <ref type="bibr" target="#b33">[34]</ref>. The credibility of EBL was bolstered by Minton's experiments <ref type="bibr" target="#b29">[30]</ref>. Minton tested EBL's impact on the prodigy problem solver in three benchmark problem spaces. In each case, prodigy's explanation-based learning module, prodigy/ebl, was able to signicantly speed up prodigy by acquiring search-control rules. <ref type="foot" target="#foot_0">1</ref> To demonstrate the power of EBL, Minton ran prodigy, with and without prodigy/ebl's control rules, on one hundred randomly generated problems in each problem space. prodigy ran considerably faster when guided by prodigy/ebl's control rules. Figure <ref type="figure" target="#fig_1">1</ref> shows the results of applying prodigy/ebl to the Blocksworld. A bound is placed on the CPU-time allotted to each problem. The total time to solve all the problems (Y-axis) is graphed against the time bound (X-axis). Thus, the Y-coordinate of a point on a curve represents the amount of time a system spent to solve all the problems, given the time bound in the X-coordinate. The graph is designed to show how the relative performance of the systems scales as the time bound is increased. See Section 6.2 for additional discussion of this graphical format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation</head><p>Unfortunately, EBL's impact on problem-solving eciency varies widely from one problem space to another. In fact, seemingly minute modications to problem space encoding can drastically alter EBL's impact on problem-solving time. The Augmented Blocksworld problem space (ABworld for short) illustrates this point. The ABworld, created by adding a single, carefully chosen, macro-operator to prodigy's Blocksworld operator set, foiled prodigy/ebl. The macro-operator, displayed in Figure <ref type="figure" target="#fig_3">2</ref>, enables prodigy to grasp a block that is second-from-the-top of a tower. Running prodigy/ebl on the ABworld (following Minton's training procedure for the Blocksworld) produced a rule set that actually slowed prodigy down on Minton's test problems (Figure <ref type="figure" target="#fig_5">3</ref>). A detailed discussion of this experiment appears in Section 6.3.  Note that controlling search is easy in the ABworld. Merely ignoring the added macrooperator yields Minton's original Blocksworld. Indeed, the rules learned by prodigy/ebl in Minton's Blocksworld lead to adequate performance even when they are used to guide prodigy in the ABworld (Figure <ref type="figure" target="#fig_5">3</ref>). The added macro-operator makes learning to control search more dicult. A theory is needed to explain this surprising phenomenon. Previous work (e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b51">52]</ref>) has emphasized the role of the problem distribution encountered by the problem solver in accounting for EBL's performance. However, problem INTRODUCTION 3 distributions cannot explain why prodigy/ebl was foiled in the ABworld since the same problems were used to train and test prodigy/ebl in both the Blocksworld and the ABworld.</p><p>The only change was the addition of a single macro-operator to the Blocksworld's operator set. Only a theory that analyzes the eectiveness of EBL in terms of problem-space characteristics can account for prodigy/ebl's performance in the ABworld experiment. This paper presents such a theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Overview</head><p>The theory relies on two basic constructs: the dichotomy between recursive and nonrecursive proofs and the Problem Space Graph (PSG) representation. The dichotomy is important because, as argued below, nonrecursive proofs tend to generate eective search-control knowledge whereas recursive proofs often fail to do so. The PSG is an and/or graph representing all backward-chaining paths through a problem space. PSGs facilitate answering questions such as \in what ways do dierent subgoals interact?" and \which subgoals give rise to recursive plans?" PSGs enable us to anticipate the proofs EBL can generate, given a particular problem space encoding.</p><p>These constructs enables us to state the fundamental tenet of the structural theory. Heuristics Based on the asymptotic analysis summarized above, I derive two simple heuristics for EBL systems:</p><p>Learn from nonrecursive proofs. Do not learn from recursive proofs.</p><p>Since these heuristics do not take problem distribution into account, it is easy to manufacture cases in which they are inappropriate (see <ref type="bibr">Section 5)</ref>. Nevertheless, the experiments in Section 6 demonstrate that both heuristics are useful in practice. Note that by learning from target concepts such as failure, EBL can learn from nonrecursive proofs even in highly recursive problem spaces.</p><p>The heuristics are attractive because they constitute an a priori basis for generating eective control knowledge. A number of researchers have developed post hoc mechanisms for evaluating control knowledge by measuring its eectiveness on a sample of problems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54]</ref>. However, as argued in Section 5, these post hoc mechanisms are heuristic as well. In addition, generating the large samples typically required by these mechanisms is very costly.</p><p>Experimental Validation When applied to prodigy, the structural theory helps explain prodigy/ebl's success in Minton's experiments as well as several experimental results reported in <ref type="bibr" target="#b9">[10]</ref>:</p><p>1. static outperforms prodigy/ebl when compared using prodigy/ebl's benchmark tasks.</p><p>2. prodigy/ebl's impact degrades signicantly in the ABworld relative to the Blocksworld.</p><p>3. prodigy/ebl signicantly reduces prodigy's Blocksworld problem-solving time over a wide range of problem distributions. <ref type="bibr" target="#b3">4</ref>. prodigy/ebl's impact degrades sharply when learning only from success.</p><p>Organization The paper is organized as follows. Sections 2 and 3 present a complexity analysis of EBL problem solvers. Section 4 introduces the structural thesis, and Section 5 argues for the heuristics mentioned above, concluding the presentation of the theory. Section 6 describes a host of experiments performed to test the theory, and Section 7 contrasts the theory with related work analyzing EBL. Finally, Section 8 presents a critique of the structural theory and points to directions for future work. This section describes an abstract model of meta-level problem solvers that is the basis for the analysis that follows. The model is an idealization of problem solvers such as <ref type="bibr" target="#b6">[7]</ref>,</p><p>mrs <ref type="bibr" target="#b17">[18]</ref>, Soar <ref type="bibr" target="#b24">[25]</ref>, prodigy <ref type="bibr" target="#b33">[34]</ref>, theo <ref type="bibr" target="#b36">[37]</ref>, and many others. The distinguishing feature of meta-level problem solvers is their ability to use domain-specic meta-level rules (called control rules) to guide their problem solving. Each control rule consists of applicability conditions and a recommendation. At every node in its problem-solving search, a meta-level problem solver matches the applicability conditions of its rules against its current state.</p><p>When the applicability conditions of a rule are met, the meta-level problem solver abides by its recommendation. The problem solver does not subgoal on the applicability conditions of control rules. The prodigy problem solver, described below, is an example of a meta-level problem solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The PRODIGY Problem Solver</head><p>Detailed descriptions of prodigy appear in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. The bare essentials follow. prodigy is a domain-independent problem solver. Given an initial state and a goal expression, prodigy searches for a sequence of operators that will transform the initial state into a state that matches the goal expression. A sample prodigy operator appears in Table 1. prodigy's sole problem-solving method is a form of means-ends analysis <ref type="bibr" target="#b39">[40]</ref>. Like strips <ref type="bibr" target="#b15">[16]</ref>, prodigy employs operator preconditions as its dierences. However, prodigy's operator description language is considerably more expressive, allowing universal quantication and conditional eects.</p><p>(UNSTACK (preconditions (and</p><formula xml:id="formula_0">(object Block-X) (object Block-Y) (on Block-X Block-Y) (clear Block-X) (arm-empty))) (effects ((del (on Block-X Block-Y)) (del (clear Block-X)) (del (arm-empty)) (add (holding Block-X)) (add (clear Block-Y)))))</formula><p>Table <ref type="table">1</ref>: The Blocksworld operator unstack. Variable names are capitalized. prodigy's default search strategy is depth-rst search. The search is carried out by repeating the following decision cycle <ref type="bibr" target="#b30">[31]</ref>:</p><p>1. Choose a node in the search tree. A node consists of a set of goals and a world state.</p><p>2. Choose one of the goals at that node.</p><p>3. Choose an operator that can potentially achieve the goal.</p><p>4. Choose bindings for the variables in the operator. If the instantiated operator's preconditions match the state then apply the operator and update the state, otherwise subgoal on the operator's unmatched preconditions. In either case, a new node is created.</p><p>Search-control knowledge in prodigy is encoded via control rules, which override prodigy's default behavior by specifying that particular candidates (nodes, goals, operators, or bindings) should be selected, rejected, or preferred over other candidates <ref type="bibr" target="#b30">[31]</ref>. Alternatives that are selected are the only ones tried; alternatives that are rejected are removed from the selected set. Finally, all other things being equal, preferred alternatives are tried before other ones. prodigy matches control rules against its current state. If the antecedent of a control rule matches, prodigy abides by the recommendation in the consequent. For example, the control rule in Table <ref type="table" target="#tab_1">2</ref> tells prodigy to reject the Blocksworld operator unstack when the block to be held is not on any other block.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Formal Denition</head><p>I specify the meta-level problem solvers model more precisely below. See <ref type="bibr" target="#b9">[10]</ref> for a complete specication. A problem space is dened by a set of operators that add and delete ground literals from states. <ref type="foot" target="#foot_1">2</ref> Given a problem-space denition, a problem solver takes as input an initial state and a ground goal expression, and searches for an operator sequence that will map the initial state to one that matches the goal. The size of the problem solver's state is denoted by s. In the absence of control knowledge, the number of nodes expanded during search is assumed to scale exponentially with s in the worst case.</p><p>Control rules have the potential to reduce the number of nodes expanded to a polynomial or even linear function of s. Since matching each control rule has a cost, however, control rules typically reduce the number of nodes searched but increase the cost of expanding each node, measured by the total number of elementary matching operations <ref type="bibr" target="#b56">[57]</ref>. Consequently, control rules do not necessarily reduce problem-solving time.</p><p>When does a set of control rules reduce problem-solving time on an individual problem? This question can be answered simply using the following notation:</p><p>: the number of nodes expanded during unguided problem solving.</p><p>: the number of nodes expanded when problem solving is guided by . : the xed cost of expanding a node without matching any control rules.</p><p>: the average cost, per node, of matching .</p><p>The cost of solving a problem without control rules is . The cost of solving the problem using the rule set is ( + ) . Clearly, reduces problem-solving time if and only if:</p><formula xml:id="formula_1">( + ) &lt; (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Utility Problem</head><p>The above inequality applies to individual problems. However, control knowledge is usually provided for an entire problem space. When is control knowledge eective over the entire space? The most widely-used notion of eectiveness in the literature is average speedup.</p><p>A set of control rules is said to be eective if it speeds up problem solving, on average, on a population of problems (e.g. <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52]</ref>). Average speedup is relatively easy to test experimentally; measuring problem-solving time with and without a set of control rules, on a large, randomly generated sample of problems, indicates whether the set achieves average speedup or not <ref type="bibr" target="#b12">[13]</ref>. Unfortunately, average speedup is distribution-specic|a rule set may be eective on one problem distribution and ineective on another. Furthermore, average speedup is a weak notion. Problem solving may remain intractable, due to its exponential nature, despite a sizable average speedup. We can replace \average speedup" with \achieving polynomial-time problem solving" as the criterion for the eectiveness of a rule set. Polynomial-time problem solving is distribution-free, and achieving polynomial-time problem solving is generally taken to guarantee tractability (\polynomial" may be replaced with \low-order polynomial" if necessary). Moreover, any rule set that achieves polynomial-time problem solving will also achieve average speedup on suciently dicult problems because, for any pair of polynomial and exponential functions, there is some point after which the exponential function is always larger. Thus, the exponential cost of default problem solving is invariably greater than the cost of polynomial-time problem solving for suciently dicult problems.</p><p>Note that the terms \polynomial" and \exponential" in the above paragraph refer to the computational complexity of the problem solver's running time, not to the inherent complexity class of the problem (e.g., NP, PSPACE, etc.). The complexity class of any problem is xed given the problem denition and a particular computational model. However, the complexity of dierent methods or algorithms for solving the problem varies. The goal of speedup learning, as formulated above, is to automatically transform the problem solver's default search behavior into a polynomial-time algorithm for a given problem. Clearly, this can only be done for problems in the complexity class P (i.e., problems solvable in polynomial time). Still, automatically generating polynomial-time algorithms, for a wide range of problems in P, has distinct advantages over requiring a human programmer to derive the algorithms by hand.</p><p>Although the polynomial-time criterion is stronger and easier to analyze (cf. <ref type="bibr" target="#b38">[39]</ref>), it is worst-case and asymptotic. Furthermore, average speedup may be attainable in cases where polynomial-time problem solving is not. Consequently, this paper will consider both criteria explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">The Cost of Matching Control Rules</head><p>This section shows that the cost of matching a conjunctive logical expression against a problem solver's state, using standard match algorithms, is exponential in the expression's length <ref type="bibr" target="#b56">[57]</ref>. <ref type="foot" target="#foot_2">3</ref> This observation is invoked repeatedly in the analysis that follows.</p><p>Consider the tree generated by the matching process. A node in the tree represents an unbound variable in the expression being matched, and a branch in the tree represents a potential binding for the variable. A path from the root of the tree to a leaf represents a variable substitution under which the expression is matched (Figure <ref type="figure" target="#fig_6">4</ref>). Thus, the tree's depth is the number of unbound variables, and the tree's branching factor is the number of potential bindings for each variable. When an expression containing v unbound variables is matched against a state of size s, the number of nodes in the tree (and hence the work done by the matcher) is O(s v+1 ). Since the number of unbound variables in an expression is bounded by the expression's length, match time is said to be exponential in the expression's length. The exponential cost of matching cannot be overcome by merely improving matching algorithms, because the problem of matching arbitrary conjunctive expressions is intrinsically dicult. To see this note that the problem of subgraph isomorphism, which is known to be NP-hard <ref type="bibr">[17, page 202]</ref>, can be reduced to the problem of matching a conjunctive expression <ref type="bibr">[36, page 184]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X=a Y=b Y=c</head><note type="other">Z=c Z=d</note><p>This observation is important because, although the cost of matching a rule against a state of size s is exponential in the rule's length, match cost is polynomial in s for any rule of bounded length. If the rule's length is bounded by k, its match cost is O(s k+1 ) which is polynomial in s. This observation holds for any nite set of control rules. Consequently, we have the following:</p><p>Proposition 1 Any nite set of control rules, which reduces the number of nodes expanded by a problem solver from an exponential in s to a polynomial in s, will achieve polynomialtime problem solving (and average speedup for suciently large values of s).</p><p>Note that this observation holds only when the problem solver is able to reach a point where no further learning is necessary. Thus, the number and size of the control rules does not scale with s. As argued in Section 3.3.3, this favorable situation can occur when the problem solver is learning control rules based on nonrecursive proofs. If, on the other hand, the problem solver is forced to continue learning additional control rules as the state size s increases, then it will not converge to a nite set of control rules in the limit. As explained in Section 3.3.1, this situation occurs when the problem solver is forced to learn from unbounded recursive proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EBL Problem Solvers</head><p>This section considers EBL problem solvers, a restricted class of meta-level problem solvers that acquires control rules via EBL. The section presents the EBG framework, which denes standard EBL terminology, and describes the relationship between EBL's proofs and the length of EBL's control rules, a prerequisite to the analysis in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The EBG Framework</head><p>Mitchell, Keller and Kedar-Cabelli <ref type="bibr" target="#b37">[38]</ref> describe a model of EBL, called Explanation-Based Generalization (EBG) that articulates many of the aspects common to various EBL systems (see also <ref type="bibr" target="#b7">[8]</ref>). Two of the major contributions of the EBG model are the identication of explanations with proofs, giving a precise meaning to the term \explanation," and the clear specication of the inputs and output of EBL shown in Table <ref type="table" target="#tab_2">3</ref>. The inputs consist of a target concept, a theory for constructing explanations, a training example, and an operationality criterion. The output is a sucient condition for recognizing the target concept. The operationality criterion is intended to ensure that the sucient condition can be used to recognize instances of the concept eciently.</p><p>According to the model, EBL systems prove that the training example is an instance of the target concept and output the weakest precondition of the proof. The weakest precondition of a proof is the (weakest) sucient condition under which the proof succeeds. Section 3.2, below, denes this notion more precisely (cf. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29]</ref>).</p><p>Given:</p><p>Target Concept Denition: A concept denition describing the concept to be learned.</p><p>(It is assumed that this concept denition fails to satisfy the Operationality Criterion.) Training Example: An example of the target concept. Domain Theory: A set of rules and facts to be used in explaining how the training example is an example of the target concept.</p><p>Operationality Criterion: A predicate over concept denitions, specifying the form in which the learned concept denition must be expressed.</p><p>Determine:</p><p>A generalization of the training example that is a sucient concept description for the target concept and that satises the operationality criterion. We can use EBG terminology to characterize the control rules acquired by EBL problem solvers. Consider the target concept \success." A training example is a portion of the problem solver's trace exemplifying the success of a particular candidate (e.g. an operator). Based on the training example, EBL proves that the operator invariably succeeds under certain conditions. The applicability conditions of the resulting control rule are the weakest precondition of that proof. The rule's recommendation is a function of what is being proved. In prodigy/ebl, for example, proving that an operator succeeds yields an operator preference rule; proving that a binding fails yields binding rejection rule, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Proof Trees</head><p>This section denes the notion of a weakest precondition (WP), and shows how WP length is determined by proof-tree size. The section then analyzes the relationship between proof-tree size and recursive proofs and shows that, given any nite rule set, the size of a nonrecursive proof tree is bounded whereas the size of a recursive proof tree is not.</p><p>For simplicity, I restrict the discussion to Horn Clauses. The analysis is easily extended to more expressive languages. I dene a proof to be the instantiation of a proof tree. A proof tree is an and-tree rooted in the literal derived by the proof. Each internal node in the tree is derived by some Horn Clause whose consequent or \head" unies with the literal at the node. The variable substitutions from the unication are propagated to the rule's antecedent conditions or \body," which is denoted by a collection of nodes connected by an and-arc.</p><p>Note that a proof tree, as dened here, is only partially instantiated. An illustrative proof tree appears in Figure <ref type="figure" target="#fig_9">5</ref>. The conjunction of the tree's leaves constitutes the proof tree's WP. WP length is the number of literals in a proof's WP. WP length can be characterized in terms of the corresponding proof tree: it is the number of leaf nodes in the tree. It follows that, in the worst case, WP length is: polynomial in the proof-tree's branching factor. exponential in the proof-tree's depth.</p><p>These observations are of interest because, as shown in Section 2.4, the cost of matching a logical expression depends on its length. Since EBL problem solvers match the WP of EBL's proofs, the cost of matching EBL's control rules depends on WP length. The following sections relate WP length to the recursive nature of the proof.</p><p>A proof is said to be recursive when it is derived from a recursive proof schema. A proof schema is said to be recursive when it yields proofs that derive one literal from a second literal that unies with, but is not identical to, the rst. For example, the inference rule: above(X,Y) :-on(X,V1), above(V1,Y). can be used to dene a recursive proof schema in which above(X,Y) is derived from above(V1,Y), above(V1,Y) can be derived from another application of the inference rule and so on. Of course, recursive derivations need not be immediate in general. The recursive depth of a proof is the maximal depth to which any recursion (there may be several) is expanded or \unfolded" in the proof. The recursive depth of the proof in Figure <ref type="figure" target="#fig_9">5</ref> is two. A recursive proof is said to be nontrivial when the body of its recursive inference rule contains more than one literal. For example, the inference rule p(X) :-p(Y) is trivial, but the above inference rule is not.</p><p>When each nontrivial recursive call replaces a proof tree leaf by two leaves, WP length increases linearly with recursive depth. In Figure <ref type="figure" target="#fig_9">5</ref>, for example, above(X,Y) is replaced by on(X,V1), above(V1,Y). When the recursion is carried a step further, above(V1,Y) is replaced by on(V1,V2), above(V2,Y) and so on. When the number of recursive calls increases with each recursive expansion, as in the inference rule: above(X,Y) :-above(X,Z), above(Z,Y).</p><p>WP length increases exponentially with recursive depth. In general, we have the following: <ref type="bibr">Proposition 2</ref> The WP length of a nontrivial recursive proof increases (at least) linearly with the proof's recursive depth.</p><p>This proposition suggests that recursive proofs expanded to nontrivial depths will result in lengthy WPs. This suggestion is corroborated by the empirical results in Section 6.</p><p>A proof is said to be nonrecursive when it is not generated by a recursive proof schema.</p><p>Given a nite set of inference rules, the depth of an arbitrary nonrecursive proof tree is bounded by the number of literals in the rule set that do not unify with each other. Since the branching factor of a proof tree is bounded by the maximal number of antecedent conditions to an inference rule in the theory, it follows that the size and WP length of nonrecursive proof trees are bounded as well. Thus, we have the following: Proposition 3 Given a nite rule set, the WP length of a nonrecursive proof tree is bounded. Note that this result applies to nonrecursive proof trees, where a tree is dened to be rooted in a single literal. Naturally, if the number of literals at the root is unbounded so is the size of the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A Complexity Analysis of EBL Problem Solvers</head><p>The previous section described EBL's role in meta-level problem solvers and showed that whereas the WP length of arbitrary nonrecursive proof trees is bounded, in any nite rule set, the WP length of recursive proofs is not. This section considers the implications of this observation for the eectiveness of EBL. Note that although learning from recursive proofs is shown to be problematic in many cases, learning in recursive problem spaces is not. 4 Utilizing a variety of target concepts (e.g. failure) facilitates learning from nonrecursive proofs even in recursive problem spaces. The following discussion does not suggest, therefore, that EBL is ineective when confronted with recursive problem spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Recursive Proofs</head><p>Since the cost of matching a conjunctive expression is exponential in its length (Section 2.4), and since WP length increases (at least) linearly with recursive depth (Proposition 2), it follows that the cost of matching the WP of a recursive proof tree is exponential (or worse) in the tree's recursive depth. In many cases, recursive depth increases with state size. Consider, 4 A problem space is said to be recursive when there are problems in the space whose solution necessitates recursive plans. A recursive plan is a plan generated by a recursive plan schema. A plan schema is said to be recursive when it generates plans where, in order to achieve one literal, the plan achieves a second literal that unies with, but is not identical to, the rst.</p><p>for example, a Blocksworld state that consists entirely of a tower of blocks. Proving that the bottom block of the tower can be cleared is recursive and the depth of the recursion is equal to the height of the tower or, equivalently, to the state size. In general, we have the following:</p><p>Proposition 4 When the recursive depth of the proof trees generated by EBL increases linearly with the state size s, the cost of matching the weakest preconditions of EBL's proofs increases exponentially (or worse) with s.</p><p>Thus, as the problem solver encounters larger and larger problems, and correspondingly deeper recursions, it is forced to acquire control rules with successively longer antecedents, and the match cost of the antecedents increases exponentially. Clearly, this result refers to worst-case match cost. If the predicates in a proof tree are unique attributes, for example, the cost of matching the proof tree's WP can be linear in its recursive depth. 5  Even in this case, however, rules learned from unique-attribute recursive proofs can result in exponential match cost due to insucient reduction in the number of nodes expanded. To see this consider Equation 1 (Section 2.2). The equation demonstrates that the total overhead due to learning is the overhead of matching control rules, at a given node, multiplied by the number of nodes expanded. If the number of nodes expanded after learning, , remains exponential in s, then the total overhead of matching is exponential in s, even when the per-node match cost of is constant, let alone linear in rule length. Thus, we have the following: <ref type="bibr">Proposition 5</ref> When is exponential in s, the total overhead of matching is exponential in s. 6   Unique-attribute rules are particularly susceptible to the problem of insucient pruning because recoding a domain theory into unique attributes, to reduce the per-node match cost of learned rules, can lead EBL to produce highly-specic rules <ref type="bibr" target="#b55">[56]</ref>).</p><p>Another problem with recursive proofs is recursion-depth-specicity, the WP of a recursive proof only matches states that support the very same recursive expansion the WP was learned from. As a result, when EBL's proofs are recursive, distinct rules are learned at every recursive depth to which the proofs are expanded <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref>. For example, prodigy/ebl learns distinct Blocksworld control rules for clearing the bottom block of a two-block tower, a three-block tower, and so on. Thus, after learning from examples of two, three, and four block towers, prodigy/ebl will fail to generalize to ve-block towers; in fact, any nite set of control rules learned from recursive proofs will fail to cover all possible Blocksworld problems.</p><p>As a result, when learning from recursive proofs, EBL is particularly sensitive to both problem distribution and choice of training examples. To make this observation more precise 5 Roughly, unique attributes (also known as determinate literals) are predicates whose arguments have unique bindings thereby restricting the branching factor in their match tree to one <ref type="bibr" target="#b55">[56]</ref>. The cost of matching a unique-attribute rule scales only linearly with rule length. 6 Paul Rosenbloom notes that even though the total overhead of is exponential in s, the ratio ( + ) = is constant, so long as is constant. See <ref type="bibr" target="#b11">[12]</ref> for additional discussion of this issue.</p><p>I dene the recursive depth of a problem, relative to a given EBL problem solver, to be the maximum of the recursive depths of the proofs EBL has to learn from in order to eliminate backtracking on that problem. Suppose EBL learns from recursive proofs whose maximal depth is , and subsequently encounters a problem whose recursive depth is 0 , where 0 &gt; . The problem solver's search will be unguided until it reduces its problem into subproblems whose recursive depth is . If we assume that the number of nodes expanded by the problem solver is exponential in 0 0 in the worst case then, by Proposition 5, we have that the total match cost overhead of EBL's control rules is exponential in 0 0 . It follows that, unless 0 0 is bounded, EBL cannot achieve polynomial-time problem solving by learning from recursive proofs. This observation applies to unique-attribute proofs as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Generalization-to-N</head><p>In some cases, the recursion-depth-specicity of EBL's control rules can be overcome using a technique known as generalization-to-N <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52]</ref>. Generalization-to-N algorithms analyze the recursive expansion in their training example, induce a general representation of the recursion's structure (e.g. a nite automaton or a context-free grammar), and construct a new recursion based on this representation. This recursion is expanded \at run-time," when the rule produced by generalization-to-N is matched. 7  Thinking of a recursive proof as a string where each inference rule is a symbol helps to understand how generalization-to-N constructs new recursions. In essence, generalization-to-N heuristically detects repeated substrings in the proof and generates rules that allow these substrings to repeat an arbitrary number of times. For example, the technique can generalize from aaab to a 3 b or from abcabc to (abc) 3 . However, the formalism used to represent recursion imposes limitations on the process. If the formalism is nite automata, for example, then generalization-to-N cannot generalize from aabb to the expression a n b n , which cannot be represented as a nite automaton. Furthermore, many strings are ambiguous. The string abaaba is plausibly generalized to (aba) 3 or to aba 3 ba. In eect, generalization-to-N imposes an inductive bias on the generalization of recursions, based on its heuristics for detecting and generalizing repeated substrings, which can lead it to overgeneralize (cf. <ref type="bibr" target="#b2">[3]</ref>).</p><p>Thus, although generalization-to-N addresses the recursion-depth-specicity of EBL's control knowledge, it will not always generalize a recursion appropriately or adequately. Furthermore, the technique yields rules that expand recursions at run time. The match cost of this expansion is still exponential in its depth in the worst case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Nonrecursive Proofs</head><p>Nonrecursive proofs avoid both the unbounded WP length and the recursion-depth-specicity associated with recursive proofs. As Proposition 3 shows, the size of nonrecursive proofs is 7 For this reason, generalization-to-N is outside the scope of the EBL problem solvers model employed here. Indeed, EBL problem solvers such as Soar and prodigy do not use generalization-to-N. Nevertheless, the above discussion shows that generalization-to-N does not solve all of the problems associated with recursive proofs.</p><p>bounded in any nite rule set. Unlike recursive proof trees, the WP length of nonrecursive proofs does not scale with the state size s. By the complexity argument in Section 2.4, it follows that the match cost of a nonrecursive control rule (i.e., a control rule derived from a nonrecursive proof) is polynomial in s.</p><p>Proposition 6 Given a nite rule set, the cost of matching the weakest precondition of an arbitrary nonrecursive proof tree against a state of size s is polynomial in s.</p><p>Since the WP length of nonrecursive proof trees is bounded, given any nite rule set, it follows that the number of distinct nonrecursive proof trees is also bounded. Furthermore, by Proposition 6, the cost of matching the WPs of an arbitrary set of nonrecursive control rules is polynomial in s. Thus, if EBL acquires a set of nonrecursive control rules that reduces the number of nodes expanded by the problem solver to a polynomial in s then, by Proposition 1, EBL has achieved polynomial-time problem solving. Note that, unless the depth of the problem solver's recursions is bounded, EBL cannot achieve polynomial-time problem solving by learning from recursive proofs.</p><p>The depth of nonrecursive proof trees is not only bounded but, often, quite small. Consider, for example, rule sets that contain no constants (e.g. the Blocksworld problem space denition). The depth of a nonrecursive proof tree, in such a rule set, is bounded by the number of predicates in the set. Even this bound is tight only when every predicate in the set participates in some nonrecursive inference chain. In practice, the depth of nonrecursive proof trees is even smaller, resulting in highly compact WPs. Thus, often, the per-node cost of matching these WPs is not merely an arbitrary polynomial, as guaranteed by Proposition 6, but a low-order one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Conclusion</head><p>This section presented an idealized model of EBL problem solvers, and used a complexity analysis to draw a dichotomy between recursive and nonrecursive proofs. The weakest preconditions (WPs) of nonrecursive proofs have bounded length and polynomial per-node match cost in any nite rule set. The number of distinct literals in a rule set is a natural bound on the maximal WP length for nonrecursive proofs in that rule set. In contrast, the WPs of recursive proofs have unbounded length and exponential match cost. Furthermore, recursive proofs are recursion-depth-specic whereas nonrecursive proofs are not. It follows that, unless recursive depth is bounded, EBL can only achieve polynomial-time problem solving when learning from nonrecursive proofs. <ref type="bibr" target="#b3">4</ref> The Structural Thesis</p><p>The previous section drew a dichotomy between recursive and nonrecursive proofs. This section demonstrates that we can discriminate between recursive and nonrecursive proofs using Problem Space Graphs (PSGs), a graph representation of problem spaces. The basic tenet of this section is the following.</p><p>The Structural Thesis: there is a correspondence between PSGs and the proofs used by EBL systems to generate search-control knowledge. As a result, automatic PSG analysis can be used to compute the weakest preconditions of the proofs and to discriminate between recursive and nonrecursive proofs.</p><p>The section is organized as follows. Section 4.1 denes Problem Space Graphs (PSGs).</p><p>Section 4.2 illustrates how a prodigy/ebl proof can be translated into a PSG subgraph, explains how to discriminate between recursive and nonrecursive proofs, and how to compute the weakest precondition of a proof based on its PSG correlate. Finally, Section 4.3 considers the scope of the thesis, and its limitations. The motivation for focusing on prodigy/ebl is two-fold. First, prodigy/ebl is a state-of-the-art EBL system that is interesting in and of itself. Second, since prodigy and its EBL module are well-documented and publicly available, experiments using prodigy (see Section 6) can be veried and replicated by other researchers. For the sake of generality, the analysis abstracts away from prodigy/ebl's singular features such as compression analysis, empirical utility evaluation, example selection heuristics, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Space Graphs (PSGs)</head><p>The PSG represents all possible paths in a backward-chaining search through a problem space. The PSG is derived from the problem space denition via partial evaluation or symbolic execution. <ref type="foot" target="#foot_3">8</ref> The PSG is precisely specied below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Specifying the PSG</head><p>A problem space can be represented as a set of disjoint graphs (or PSGs), each rooted in a distinct achievable literal. A depiction of the PSG rooted in the Blocksworld literal (holding V) appears in Figure <ref type="figure" target="#fig_10">6</ref>. The PSG is a directed, acyclic and/or graph. The root literal is connected, via or-links, to the operators that match it. The operators are partially instantiated via the variable substitution from the match. Each operator is connected, via and-links, to its partially instantiated preconditions. Each precondition is connected to the operators that match it, and so on. Thus, the PSG's nodes are an alternating sequence of (sub)goals and operators, and the PSG's edges are an alternating sequence of and-links and or-links.</p><p>When two operators that have a common goal share a precondition, a single node designates that precondition in the PSG. Both operators are linked to the node via and-links, so the graph is not a tree in general. In Figure <ref type="figure" target="#fig_10">6</ref>, for example, both pick-up(V) and unstack(V,V2) have (clear V) as a precondition. Consequently, both operators have andlinks to that node. As described thus far, recursion would result in innite PSGs. In fact, the PSG has welldened termination conditions under which PSGs are provably nite <ref type="bibr" target="#b10">[11]</ref>. To state these </p><formula xml:id="formula_2">(holding V) UNSTACK (on V V2) STACK (holding V) (clear V) UNSTACK (on V1 V) (clear V1) (arm-empty) STACK (holding V) PUT-DOWN (holding V) (arm-empty) PICK-UP (on-table V) PUT-DOWN (holding V)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PSG expansion is terminated at n if and only if:</head><p>No operators can achieve the literal at n. The literal is identical to one of its ancestor literals (I refer to this condition as a goal cycle).</p><p>The literal unies with, but is not identical to, one of its ancestors (I say that the literal recurs).</p><p>The literal is necessarily satised, given that its ancestors are subgoals waiting to be achieved. In the holding PSG, for example, (on V1 V) is labeled holds because, to reach it, prodigy subgoals on (holding V) and (clear V) and, if a block is neither clear nor held, then there must be some block on it. 9   An algorithm that derives PSGs from problem space denitions appears in <ref type="bibr" target="#b10">[11]</ref>.</p><p>The semantics of PSG nodes can be dened relative to prodigy's problem solving. A literal node appears in the PSG if and only if there is a world state in which the problem 9 The (arm-empty) nodes are labeled holds to keep the PSG's depiction compact. The label is justied by noting that put-down can invariably achieve (arm-empty).</p><p>solver subgoals on an instance of that literal in trying to achieve its current goal. An operator node appears in the PSG if and only if there is a world state in which prodigy will backchain on an instance of the operator in trying to achieve its current goal. A literal node has an or-link to an operator node if and only if the operator matches the literal. An operator has an and-link to a literal if and only if the literal is a precondition to the operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Correspondence of PSGs to EBL's Proofs</head><p>The notion of representing programs (or problem spaces) as graphs is well-known in computer science (see, for example, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b57">58]</ref>). It is interesting to note, however, that prodigy/ebl's proofs correspond to PSG subgraphs. <ref type="foot" target="#foot_4">10</ref>prodigy/ebl's failure proofs, for example, have the following avor: an operator cannot be executed because one of its preconditions cannot be achieved. A precondition cannot be achieved because all of the operators that could potentially achieve it cannot be executed and so on. Each proof \explains" a failure by an alternating series of existentially and universally quantied statements about the relevant preconditions and operators. Success proofs are analogous, but the quantiers are reversed. These alternating sequences correspond to the alternating sequences of operator and precondition nodes in the PSG.</p><p>For example, unstack fails because one of its preconditions, (on V V2), cannot be achieved in the context of (holding V). On cannot be achieved because all of the relevant operators (stack is the only one) cannot be executed. stack cannot be executed because trying to achieve one of its preconditions, (holding V), results in a goal cycle. That is, the problem solver subgoals on (holding V) in the process of trying to achieve (holding V), which leads to an innite loop. The PSG subgraph in Figure <ref type="figure" target="#fig_11">7</ref> is a representation of this proof. The weakest precondition of the proof can be determined by analyzing the subgraph.</p><p>In this case, the goal cycle occurs when the goal is (holding V) and the precondition (on V V2) is not true in the current state, leading prodigy to back-chain on the operator stack in order to satisfy the precondition. This proof yields the rejection rule that appears in Table <ref type="table" target="#tab_1">2</ref>, Section 2.1. The example illustrates the general correspondence between PSG subgraphs and prodigy/ebl's proofs. This relationship is not surprising if we consider that, by construction, the PSG explicitly represents the failure and success of dierent problem-solving paths which is precisely the subject matter of EBL's proofs. See <ref type="bibr" target="#b9">[10]</ref> for a more detailed description of the mapping between PSGs and EBL's proofs.</p><formula xml:id="formula_3">(holding V) UNSTACK (on V V2) STACK (holding V) failure failure failure goal-cycle</formula><p>The correspondence described above facilitates traversing the PSG searching for subgraphs corresponding to nonrecursive proofs (i.e. subgraphs none of whose leaves are labeled recurs) and deriving control rules based on these subgraphs. That is precisely what the static program does. A complete description of static's algorithms appears in <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scope of the Structural Thesis</head><p>Although the structural thesis applies to many existing EBL problem solvers (e.g., <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b51">52]</ref>) its scope is limited for a number of reasons. First, although most commonly-used target concepts (success, failure, etc.) map naturally to PSG subgraphs, other target concepts (e.g., state cycles) do not. Second, PSGs have only been developed for backward-chaining problem solvers. Developing a PSG representation for forward-chaining problem solvers is a topic for future research. Finally, consider a domain theory that augments and transcends the problem space denition. For example, consider a domain theory that contains inference rules which can assess the outcome of a complex problem space recursion using a simple reasoning process. 11 When the domain theory and the problem space are divorced in this manner, the structural thesis is false. However, a sibling thesis may be formulated linking a graph representation of the domain theory to EBL's proofs. Ultimately, determining the precise scope of the thesis is an empirical enterprise. Further analysis of a variety of EBL problem solvers, domain theories, and target concepts is required to determine whether the thesis applies or not. <ref type="bibr" target="#b4">5</ref> Choosing What Proofs to Learn From</p><p>An EBL system can have access to a wide variety of target concepts, and there are many dierent ways to prove that a given training example is subsumed by a particular target concept. Each such proof yields a dierent control rule, some of which are considerably more useful than others. When analyzing a training example, EBL merely computes the weakest precondition of a particular proof. It is by no means guaranteed to nd \the best" control rule, or even an eective one <ref type="bibr" target="#b13">[14]</ref>. Indeed, EBL frequently derives ineective control knowledge in practice.</p><p>Based on the complexity analysis in Section 3, this section presents heuristics that help EBL to overcome this problem by indicating which proofs EBL should learn from. The complexity analysis does not prove that the heuristics are appropriate in every case. In fact, it is easy to manufacture cases in which the heuristics are inappropriate; I discuss such cases in Sections 5.2 and 5.1. Nevertheless, the experiments in Section 6 show that the heuristics are valuable in practice.</p><p>The heuristics are attractive because they constitute an a priori basis for generating eective control knowledge. A number of researchers have developed post hoc mechanisms for 11 Imagine a Blocksworld domain theory that specied the Cartesian coordinates of each block. Determining whether one block is above another, while recursive in the problem space, merely requires comparing Xcoordinates in the domain theory. evaluating control knowledge by measuring its eectiveness on a sample of problems <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54]</ref>. The advantage of post hoc approaches is that they \consider" problem distribution, whereas a priori approaches do not. Sample-based approaches have several disadvantages, though. First, the complex interactions between dierent control rules mean that individual control rules cannot be tested in isolation. As a result, the post hoc mechanisms rely on hill climbing to incrementally (and heuristically) evaluate sets of control rules. Second, the mechanisms' performance is sensitive to the composition and ordering of their sample. With some probability, a randomly-generated sample will not accurately represent the population from which it is drawn. When this is the case, the performance of a sample-based mechanism is arbitrarily bad. Large sample sizes ensure that the probability of this event is low but, to provide such guarantees, the mechanisms proposed in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b53">54]</ref> typically require their problem solver to solve literally thousands of problems. Even the ad hoc rule-evaluation mechanism used by prodigy/ebl <ref type="bibr" target="#b31">[32]</ref> accounts for a signicant fraction of prodigy/ebl's learning time <ref type="bibr">[10, chapter 8]</ref>. Finally, sample-based rule selection is distribution-dependent. If the problem distribution changes, the rule selection algorithm has to be invoked again, on a fresh sample, to ensure that an appropriate rule set is chosen. Thus, as recognized by <ref type="bibr" target="#b18">[19]</ref>, a priori heuristics for generating eective control rules have distinct advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Nonrecursive Heuristic</head><p>The nonrecursive heuristic can be stated as follows:</p><p>Learn from nonrecursive proofs.</p><p>As pointed out earlier, control knowledge represents a tradeo between reduced problemspace search and increased match cost. Proposition 6 suggests that this tradeo may be favorable when learning from nonrecursive proofs. In general, learning from nonrecursive proofs is not guaranteed to be eective for several reasons. The analysis in Section 3.3 presupposes an exponential search. If unguided search is polynomial in s, for example, then the nonrecursive heuristic can easily result in a signicant average slowdown. In addition, although the per-node match cost of nonrecursive control rules increases only polynomially with state size s, their total match cost overhead can still be exponential when the number of nodes expanded by the problem solver, after learning, is exponential (Proposition 5).</p><p>Finally, due to the a priori nature of the nonrecursive heuristic, it is easy to dene problem distributions where nonrecursive control rules are rarely applicable and thus ineective. The experiments reported in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42]</ref> demonstrate this point.</p><p>Despite the caveats enumerated above, the experiments described in Section 6 demonstrate the value of the nonrecursive heuristic in practice. Additional work is required to precisely identify the classes of problems in which it is eective. Augmenting the heuristic with a variety of example-based and sample-based approaches as suggested in <ref type="bibr" target="#b9">[10]</ref> and investigated in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42]</ref> is a worthwhile direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Bounded-Depth Recursion</head><p>Bounded-depth recursive proofs are distinct from nonrecursive proofs. When the size of a recursive proof is bounded, the analysis used to derive Proposition 6 applies, and the cost of matching the proof's WP is polynomial in the state size s. One might be tempted to conclude that a nonrecursive proof is merely \a recursive proof unfolded to depth zero." This is not the case. Consider, for example, the PSG displayed in Section 4.1.1. The subgraph containing the root, (holding V), the operator unstack, and unstack's preconditions, corresponds to a depth-zero recursive proof that unstack can achieve (holding V). In contrast to the WP of a nonrecursive proof, this proof's WP is recursion-depth-specic; it does not apply when V is covered by one or more blocks. Each recursive invocation of unstack, aimed at achieving (clear V), results in a deeper recursive proof with a longer recursion-depth-specic WP.</p><p>In contrast, the PSG subgraph displayed in Figure <ref type="figure" target="#fig_11">7</ref> corresponds to a \true" nonrecursive proof. The proof is not derived from a recursive proof schema; the same proof can be used to explain the failure of unstack regardless of the number of blocks in the problem, the recursive depth of the problem solver's plan, etc. See Figure <ref type="figure">8</ref> (Section 6.1.3) for another example of a nonrecursive proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Fortuitous Recursion</head><p>The eectiveness of the nonrecursive heuristic is enhanced by a structural feature of problem spaces I refer to as fortuitous recursion. I introduce this idea using a Blocksworld example.</p><p>Consider a plan to achieve (holding block-1) when block-1 is at the bottom of a threeblock tower: block-3, block-2, block-1. In order to grasp block-1, the problem solver has to achieve the subgoal (clear block-1). To achieve (clear block-1), it has to achieve (clear block-2) and so on. The plan is generated by expanding a recursive plan schema. The depth of the expansion is determined by the height of the block tower.</p><p>Proving that a recursive plan succeeds gives rise to a recursive proof. However, in some cases, we can prove that a recursive plan fails using a nonrecursive proof. Consider achieving (holding block-1) using the unstack operator which is shown in Table <ref type="table">1</ref> in Section 2.1.</p><p>A nonrecursive proof shows that the plan fails, for our three-block tower, because using unstack requires subgoaling on its (on block-1 block-Y) precondition, which leads to a goal cycle. The proof is described in Section 4.2, and its PSG representation appears in Figure <ref type="figure" target="#fig_11">7</ref> in that section.</p><p>The proof yields a sucient condition for the failure of unstack that is the basis of the operator rejection rule in Table <ref type="table" target="#tab_1">2</ref>, Section 2.1. The rule rejects unstack, when its antecedent is matched, but does not necessarily guarantee that unstack will achieve its goal when the rule's antecedent is not matched. When the antecedent is not matched (i.e. (on block-1 block-Y) holds in the current state), the success of unstack depends on whether its other preconditions, (clear block-1) and (arm-empty), can be achieved.</p><p>In fact, it turns out that (arm-empty) is easily achieved by putting down any block that is being held and that, invariably, there is some recursive plan that will succeed in achieving (clear block-1). The Blocksworld turns out to have a fortuitous property: whenever (on block-X block-Y) is matched, unstack will achieve (holding block-X). Thus, the rule for rejecting unstack denes a condition that turns out to be both necessary and sucient for failure; if the applicability conditions of the rejection rule in Table <ref type="table" target="#tab_1">2</ref> are not matched, plans to achieve holding via unstack are guaranteed to succeed. This property is not guaranteed by EBL. It is a structural feature of the Blocksworld.</p><p>The above example shows how a nonrecursive proof of failure can yield a necessary and sucient condition for the failure (or, equivalently, success) of an operator. In this example, EBL need not form a recursive proof in order to eliminate backtracking. The essential feature of the example is that the success of a nonrecursive portion of a recursive plan (namely, (on block-X block-Y)) guarantees the success of the entire plan. When this is the case for all plans in which the recursion appears, the recursion is said to be \fortuitous." In the Blocksworld, the recursion required to clear a block is fortuitous.</p><p>Clearly, fortuitous recursion enhances the eectiveness of nonrecursive proofs of failure. When all problem-space recursions are fortuitous, and EBL is trained appropriately, EBL can eliminate backtracking by learning solely from nonrecursive proofs. Given a nite rule set, the number of distinct nonrecursive proofs is nite. Thus, by Proposition 6 in Section 3.3.3, it follows that EBL is guaranteed to achieve polynomial-time problem solving and average speedup in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The Recursive Heuristic</head><p>The recursive heuristic is a negative heuristic that complements the nonrecursive one: Do not learn from recursive proofs.</p><p>Proposition 4 shows that, in the limit, learning from recursive proofs trades an exponential search for an exponential match. Furthermore, as argued in Section 3.3.1, for any xed set of recursive control rules there are problem distributions on which the set results in exponential overhead and average slowdown of the problem solver.</p><p>Again, it is easy to manufacture cases in which the recursive heuristic is inappropriate, and learning from recursive proofs is actually benecial. Essentially, when the depth of a certain recursion is bounded in a given problem distribution, then the recursive depth of proofs that unfold this recursion (and no others) is bounded, and the asymptotic analysis of nonrecursive proofs in Section 3.3.3 applies to these \bounded-depth" recursive proofs. In general, it is impossible to anticipate a priori that a recursion will have bounded depth in the absence of precise a priori knowledge of the problem distribution. <ref type="foot" target="#foot_5">12</ref> Furthermore, bounds on the depth of recursive unfolding can change with the problem distribution whereas the bound on nonrecursive unfolding is xed in any given problem space. Thus, the recursive heuristic remains the best a priori heuristic we can formulate.</p><p>The recursive heuristic is particularly important because of the practice of training EBL on relatively simple problems that exhibit shallow recursions. As pointed out in <ref type="bibr" target="#b13">[14]</ref>, this practice often helps EBL nd simpler and more compact explanations of its problem solver's behavior. However, in the case of recursive proofs, the practice can lead to a disparity between the shallow recursions in training examples and deep recursions on test cases. As Section 3.3.1 indicates, such disparity leads to exponential overhead due to EBL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">The Recursive Heuristic In Practice</head><p>Experimental inquiries have shown that control knowledge learned from recursive proofs can be eective when a tight bound is imposed on recursive depth in domains such as the Eightpuzzle <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b54">55]</ref>, or on highly skewed problem distributions <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b55">56]</ref>, but is ineective in many other cases <ref type="bibr" target="#b9">[10]</ref>, even in the presence of generalization-to-N <ref type="bibr" target="#b51">[52]</ref>. Determining exactly how skewed the distribution of problems has to be for a problem solver to benet from recursive EBL proofs depends on the specic search space, problem solver, and matcher involved. <ref type="bibr" target="#b51">[52]</ref> estimated this parameter empirically using recursive macro rules learned by BAGGER2 <ref type="bibr" target="#b46">[47]</ref> in a circuit synthesis domain. They report that \unless the percentage of problems in the future queries that are solvable by the macro rules alone exceeds 60 percent for input expressions of arity 3..and 80 percent for arity 5, BAGGER2 style macro rules are not useful."</p><p>The above discussion suggests that, due to insucient pruning, EBL will fail to achieve average speedup when learning from recursive proofs on suciently uniform problem distributions. Additional work is required to precisely characterize \suciently uniform" distributions and to formally analyze the relationship between distributional skew and the eectiveness of EBL when learning from recursive proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Complementary Target Concepts</head><p>Learning from the success of a recursive plan yields a recursive proof. Although I have argued against learning from recursive proofs, I have also argued that EBL can be eective in recursive problem spaces. Indeed, most of the plans in prodigy/ebl's benchmark problem spaces are recursive, yet prodigy/ebl is quite eective in these problem spaces <ref type="bibr" target="#b29">[30]</ref>. This paradox is resolved by observing that prodigy/ebl learns from failure and goal interaction in addition to learning from success. As the Blocksworld example in Section 5.1.2 illustrates, proving that a plan will fail can be nonrecursive even when the plan itself is recursive. In fact, ve failure proofs yield all the control rules necessary to make appropriate operator and bindings choices on any Blocksworld problem. The failure proofs are nonrecursive, yielding rules that are compact, general, and cheap-to-match.</p><p>I refer, informally, to target concepts (e.g. failure) whose proofs are not direct translations of the problem solver's plans as complementary target concepts. When recursive plans abound, complementary target concepts are EBL's primary means of nding nonrecursive proofs. It follows that such target concepts will enhance EBL's eectiveness in many cases.</p><p>Practical considerations suggest another argument for learning from complementary target concepts. Even after analyzing a large number of training examples, EBL's coverage is often incomplete in practice|it is unable to guide the problem solver on every control decision in every problem. As a result, the problem solver frequently departs from the path to a solution. When backtracking chronologically (as in prodigy or Prolog), the problem solver exhausts all the available alternatives at a node before backtracking from the node. Thus, in the absence of control rules that curtail backtracking, departing from a solution path often results in extensive backtracking. Control rules learned from success (\success rules") attempt to keep the problem solver on a solution path. Unfortunately, success rules will never re at a node that is not on any solution path relative to the current goal, because the state at such a node can never match the weakest preconditions of a successful plan. Thus, once the problem solver has diverged from a solution path, the only control rules that can curtail backtracking are control rules learned from complementary target concepts. It follows that complementary target concepts are particularly useful.</p><p>Most existing EBL systems (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52]</ref>) and partial evaluators (e.g., <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b26">27]</ref>) do not use complementary target concepts. Thus, as Minton <ref type="bibr" target="#b29">[30]</ref> argues, prodigy/ebl's use of multiple (and, in particular, complementary) target concepts is an important extension of the EBL method. The structural theory supports and elaborates Minton's contention, yielding an operational recommendation to the designers of EBL problem solvers: use complementary target concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Conclusion</head><p>This section described two a priori heuristics for choosing what proofs to learn from in EBL.</p><p>The heuristics yield a simple criterion for proof choice: learn only from nonrecursive proofs.</p><p>To increase the number of nonrecursive proofs available to EBL, I recommend developing a host of complementary target concepts, concepts (e.g. failure in prodigy/ebl) whose proofs do not mirror the problem solver's plans. I describe a number of cases in which this criterion is imperfect, and others have appeared in the literature <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b41">42]</ref>. The following section argues that, though imperfect, the heuristics formulated above are valuable in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Validation</head><p>This section reports on an array of experimental results that are explained, informally, using the structural theory. In particular, the theory explains prodigy/ebl's success in Minton's experiments <ref type="bibr" target="#b29">[30]</ref> as well as the experimental results listed in Section 1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Explaining PRODIGY/EBL's Success</head><p>\It is rare that one sees an AI system evaluated carefully by anyone other than its creator." <ref type="bibr" target="#b1">[2]</ref> To demonstrate the explanatory power of the structural theory, this section utilizes it to explain prodigy/ebl's success in Minton's experiments. Each of prodigy/ebl's benchmark problem spaces exhibits several structural features that facilitate prodigy/ebl's success. The section discusses these features abstractly, and explains how the features account for prodigy/ebl's success in each space. Table <ref type="table">4</ref>, at the end of the section, summarizes the explanation using the terms dened below.</p><p>Problem Distribution Minton's problem distributions play an important role in explaining his results. First, the problem distributions determine the recursive depths of prodigy/ebl's proofs. Second, the distributions determine the aspects of each problem space that prodigy is exposed to. Each of the problem spaces contains some recursive structure that prodigy/ebl is unable to learn about eectively (see Section 6.1.1 for an example). prodigy/ebl's success in Minton's experiments is due, in part, to the paucity of problems that exercise this recursive structure.</p><p>Short Nonrecursive Proofs of Failure. Failure, in prodigy/ebl's problem spaces, is often explainable by nonrecursive proofs. As it turns out, the depth and the branching factor of prodigy/ebl's nonrecursive failure proofs are small, yielding compact control rules whose applicability conditions contain no more than two or three literals in many cases. As a result, control rules learned from failure are both general and cheap-to-match making the tradeo between increased match cost and search reduction favorable even for relatively small problems; prodigy/ebl relies heavily on learning from failure.</p><p>Fortuitous Recursions Many of the recursions in prodigy/ebl's problem spaces are fortuitous (see Section 5.1.2) which means that plans containing the recursions will succeed if the nonrecursive portions of the plans succeed. This facet of the problem spaces makes learning from nonrecursive failure proofs particularly potent.</p><p>Nonrecursively Serializable Subgoals A set of subgoals is said to be serializable if there exists an ordering of the subgoals such that, if the subgoals are achieved in that order, once a subgoal is satised it need never be violated in order to achieve the remaining subgoals <ref type="bibr">[23, page 39]</ref>. A problem is said to be serializable when the subgoals that comprise the problem's goal conjunction are serializable. Since the appropriate goal ordering may depend on the initial state, serializing dierent problems may require distinct goal orderings. prodigy's goal ordering control rules enable it to serialize its goals on a variety of dierent problems.</p><p>A problem is said to be nonrecursively serializable if control rules that serialize its goals can be learned from nonrecursive proofs of goal interactions. Most of the problems in Minton's experiments are nonrecursively serializable, which enhances the coverage and reduces the match cost of prodigy/ebl's goal-ordering rules.</p><p>Using the terms introduced above, I now consider each of prodigy/ebl's benchmark problem spaces in turn.</p><p>6.1.1 The Blocksworld prodigy's Blocksworld is a standard encoding of the Blocksworld problem space based on the encoding in <ref type="bibr" target="#b40">[41]</ref>. The Blocksworld exhibits short nonrecursive failure proofs, and fortuitous recursions for each subgoal in the space. As a result, prodigy/ebl is able to form low-cost operator and bindings choice rules that obviate backtracking in achieving individual subgoals. Thus, after prodigy/ebl learns from the appropriate examples, prodigy can achieve any individual subgoal without search on every possible problem. Search is still required to nd the appropriate subgoal ordering. There are ve Blocksworld goal predicates. Most Blocksworld problems can be solved without backtracking if the subgoals are achieved in the following order:</p><p>1. Achieve any on-table subgoals.</p><p>2. Achieve any clear subgoals.</p><p>3. Build any desired towers from the bottom up (on goals). 4. Achieve holding or arm-empty, if necessary. prodigy/ebl is able to approximate this goal ordering strategy by learning goal-ordering control rules from nonrecursive proofs of goal clobbering. As Minton points out, however, prodigy/ebl is unable to learn the rule \build towers from the bottom up" in its full generality, because the rule requires knowing which blocks are constrained to be below each other in the goal, and prodigy/ebl can only determine this using recursive proofs whose generality is limited <ref type="bibr">[32, page 385</ref>]. Thus, not all Blocksworld problems are nonrecursively serializable.</p><p>As a result, prodigy/ebl's ability to serialize prodigy's goals in the Blocksworld depends on prodigy's problem distribution. prodigy/ebl will be eective when the problems encountered by prodigy are covered by a small number of tower-height-specic rules, or when the problems are nonrecursively serializable. In fact, most of the problems in Minton's experiment were nonrecursively serializable. Thus, prodigy/ebl's ability to avoid goal clobbering in the experiment was due both to the Blocksworld's structure and to Minton's problem distribution.</p><p>The features of the Blocksworld discussed above enabled prodigy+ebl 13 to solve Minton's Blocksworld test problems with little to no backtracking. Without backtracking prodigy expands 2L + 2 nodes to produce a solution of length L in each problem. The total length of prodigy+ebl 's solutions is 650 steps, and the total number of problems is 100. Thus, the minimal number of nodes prodigy+ebl could expand to produce its solutions is (2 3 650 + 2 3 100) which is 1500. In fact, prodigy+ebl expanded 1689 nodes which is only 1.13 times the minimal node count. In contrast, prodigy's minimal node count is 1400 (since it found shorter solutions to some problems) but it expanded 217,948 which is 155.67 times its minimal node count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">The Stripsworld</head><p>prodigy's Stripsworld is an extended version of the strips robot-planning problem space <ref type="bibr" target="#b15">[16]</ref>. The Stripsworld also exhibits nonrecursive failure and goal-clobbering proofs. 13 Recall that prodigy+ebl denotes prodigy guided by prodigy/ebl's rules. Thus, prodigy's primary challenge in the Stripsworld is nding its way through each problem's room conguration. Since prodigy has no \sense of direction," it nds room congurations dicult to navigate. Furthermore, since the doors to dierent rooms may be locked, necessitating the retrieval of keys from rooms whose doors may themselves be locked and so on, navigation can be quite dicult in general.</p><p>prodigy's plans to move from one room to another invariably contain recursive calls to the operator go-thru-dr. As a result, its proofs of both successful and failed paths are recursive and thus path-length-specic. Potentially, therefore, the Stripsworld could present a severe challenge for prodigy/ebl. In Minton's experiments, prodigy/ebl is aided considerably by the fact that each of the problems takes place in one of three simple room congurations (See <ref type="bibr">[30, page 182]</ref>). As a result, the rules prodigy/ebl learns from recursive proofs, which enable it to look ahead along certain paths, turn out to be reasonably compact and frequently applicable. Although expensive to match, the rules are useful since prodigy only encounters one of three room congurations.</p><p>6.1.3 The Schedworld prodigy's Schedworld is a simplied process planning and machine-shop scheduling problem space. prodigy/ebl speeds up prodigy in the Schedworld in two ways: by quickly detecting unsolvable problems and by ordering top-level goals to avoid goal interactions.</p><p>While prodigy+ebl is almost four times faster than prodigy on Minton's test-problem set, it is only 1.6 times faster when the top-level node rejection rules (which detect unsolvable problems) and goal ordering rules are removed from the prodigy/ebl's rule set. The dierence in the number of nodes expanded is more striking. prodigy+ebl expands only 5,401 nodes compared with prodigy's 181,938 nodes in the Schedworld. With the node rejection and goal ordering rules removed prodigy+ebl expands 110,611 nodes, demonstrating the value of the removed rules.</p><p>Virtually all of the removed rules can be derived via nonrecursive proofs of failure or goal interaction. Consider, for example, the goal of painting an object. Only two operators are available for painting an object: spray-paint and immersion-paint. An object cannot be painted if prodigy does not have-paint-for-immersion and the required paint is not sprayable. This observation suces to prove that any problem that matches this description is unsolvable. A graphical depiction of this nonrecursive proof of failure appears in Figure <ref type="figure">8</ref>.</p><formula xml:id="formula_4">(PAINTED V10 V11) IMMERSION-PAINT (HAVE-PAINT-FOR-IMMERSION V11) SPRAY-PAINT (SPRAYABLE V11) UNACH UNACH Figure 8:</formula><p>The PSG subgraph corresponding to the proof that an object cannot be painted, using a certain paint, when neither sprayable nor have-paint-for-immersion hold for that paint.</p><p>Remarkably, scheduling conicts rarely arise in Minton's problems. Thus, learning about such conicts, which would require analyzing recursive interactions, was not required for success in Minton's experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blocksworld Stripsworld Schedworld</head><p>Nonrecursive failures:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All subgoals Most All</head><p>Fortuitous recursions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All operators Few Most</head><p>Additional recursions:</p><p>Goal interactions Room congs. Scheduling conicts Nonrec. Serializable:</p><p>Most goals Some Almost all Table <ref type="table">4</ref>: A summary of the structural explanation of prodigy/ebl's success in Minton's experiments. Note that in all problem spaces the \additional recursions," which are potentially intractable for prodigy/ebl, appeared very infrequently due to the problem distributions used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The Power of Static PSG Analysis</head><p>The static program is a natural product of the structural theory. static learns by analyzing PSG subgraphs that correspond to nonrecursive proofs; it does not construct logical proofs, analyze training examples, or even utilize an explicit domain theory. Instead, static symbolically back-chains on prodigy's operator schemas to construct the PSG representation of a problem space. static traverses its PSGs, annotating each node with a label indicating which operators and subgoals will succeed or fail, and a logical expression indicating under what conditions the label holds. Finally, static derives control rules based on this annotation. See <ref type="bibr" target="#b10">[11]</ref> for a complete description of static's algorithms. The previous section suggested that, on prodigy/ebl's benchmark tasks, eective control knowledge can be derived from nonrecursive proofs. The experiment described below supports this claim by comparing the impact of static, prodigy/ebl, and control rules written by human experts on prodigy/ebl's benchmark tasks.</p><p>Experimental Methodology In each problem space prodigy was run on one hundred randomly generated problems. prodigy was run under three experimental conditions: guided by static's rules, guided by prodigy/ebl's rules, and guided by control rules written by human experts. prodigy/ebl's rules, the \human control rules," and the randomly generated problem sets, were taken from Minton's experiments. <ref type="foot" target="#foot_6">14</ref>The experiments were run in Allegro Common Lisp on a SUN-4 workstation. prodigy's problem-solving time, on each problem was bounded by 150 CPU seconds. A time bound is necessary for the experiments to complete in reasonable time. This time bound is less restrictive than Minton's, and the machine used is faster. Thus, relative to Minton's experiments, more problems were solvable within the time bound, more nodes were expanded within the time bound, the average time per node was smaller, and the problem-solving time per problem was smaller. This change does not inuence the experimental results because all the comparisons made below are between data sets obtained on the same machine, under the same time bound.</p><p>Data Presentation Figure <ref type="figure">9</ref> shows the total time (in CPU seconds) spent tackling Minton's test problem set (Y-axis) graphed against a bound on CPU time (X-axis). The Y-coordinate of a point on a curve in Figure <ref type="figure">9</ref> represents the amount of time spent by a system summed over all the problems, given the time bound in the X-coordinate. When all the problems are solved, the total problem-solving time (Y-axis) does not change as the time bound increases (X-axis). Thus, when all the problems are solved the curve is horizontal and its slope is zero.</p><p>The departure from Minton's cumulative graphs format is motivated by Segre et al.'s argument that, when some problems are unsolved within the CPU time bound, changing the bound can inuence the graphical relationship between systems being compared using the cumulative graphs format <ref type="bibr" target="#b44">[45]</ref>. The graphs used here are specically designed to address this problem, showing how the relative performance of the systems scales on larger and larger CPU time bounds. Segre et al. also argue that the experimenter's choice of time bound can bias the results of the experiment. To address this problem, Etzioni and Etzioni <ref type="bibr" target="#b12">[13]</ref> develop statistical hypothesis tests designed to analyze speedup learning data that is \truncated" due to the use of time bounds. The tests show that the dierences between prodigy+static and prodigy+ebl are statistically signicant. The graphs in Figure <ref type="figure">9</ref> show that prodigy+static is faster than prodigy+ebl in each of prodigy/ebl's problem spaces. Problem-solving time is a function of the match cost and the number of nodes pruned by the learned control rules. How does the number of nodes expanded by prodigy+ebl compare with the number of expanded by prodigy+static? Figure <ref type="figure" target="#fig_1">10</ref> graphs the total number of nodes expanded on all problems (Y-axis) against an increasing bound on the number of nodes expanded in each problem (X-axis). 15 The nodebound graphs in gure 10 show that static's ability to curtail search, measured by the number of nodes pruned, rivals prodigy/ebl's in each of prodigy/ebl's problem spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">PRODIGY/EBL is Foiled in the ABworld</head><p>Section 1.1 showed how a seemingly minute modication to the Blocksworld, creating the ABworld, foiled prodigy/ebl. This section explains this observation using the structural theory. Before reporting in detail on the experiment, I explain how the ABworld was constructed. The presence and location of recursion is sensitive to problem space encoding. A problem space can be \robbed" of nonrecursive proofs of failure by inserting operators. Since all operators have to fail if a given subgoal is to fail, inserting an operator means that 15 The maximal node bound chosen was 120 nodes per problem. an and-node is necessarily added to any proof that the subgoal fails. When the inserted operator results in a recursion, its addition transforms a nonrecursive proof into a recursive one.</p><p>In the Blocksworld, the failure of any operator can be explained by a nonrecursive proof. When explaining the success of an operator is recursive, explaining the failure of its sibling operators is not. In the ABworld, by way of contrast, recursion occurs both on the route to failure and to success. Consider, for example, choosing unstack to achieve the goal (holding V) when the block is on the table. This path is doomed to failure in both problem spaces. In the Blocksworld, the failure can be explained nonrecursively since trying to achieve on leads to a goal cycle immediately (Figure <ref type="figure" target="#fig_11">7</ref>, Section 4.2).</p><p>In the ABworld, in contrast, on can be achieved by the grasp-second-block macrooperator. The macro-operator allows prodigy to grasp the block that's second-from-the-top of a tower. The top block lands on the block that is third-from-the-top as a side eect (see Figure <ref type="figure" target="#fig_3">2</ref>, Section 1.1). Thus, prodigy has another means of achieving on which does not immediately cause a goal cycle. Since prodigy explores the recursive path that begins with the macro-operator before failing, prodigy/ebl is forced to analyze that path in order to explain prodigy's failure. Consequently, explaining the failure of unstack in the ABworld is recursive. The PSG representation of (a fragment of) this proof appears in Figure <ref type="figure" target="#fig_13">11</ref>. Contrasting the fragment with Figure <ref type="figure" target="#fig_11">7</ref> shows how adding the grasp-secondblock macro-operator to the Blocksworld makes explaining the failure unstack recursive. The above discussion explains how adding a macro-operator to the Blocksworld led to prodigy/ebl's lackluster performance in the ABworld. In essence, the ABworld robs prodigy/ebl of key nonrecursive proofs, forcing it to analyze a complex and unwieldy recursion. Much of the work on recursive proofs (e.g., the work on generalization-to-N discussed in Section 3.3.2) has focused on simple tail recursions of the sort found when clearing a block in the Blocksworld. The ABworld recursions are far more complex. For example, in the ABworld prodigy expands over eight-thousand nodes in the process of trying to grasp the block at the bottom of a three-block tower. In order to learn, prodigy/ebl is forced to analyze the resulting problem-solving trace with dire consequences. Several additional aspects of the experiment are worth noting. First, not all ABworld proofs are recursive as evidenced by the fact that static was able to learn several control rules in this space. Second, the problem distribution used is highly skewed in that no problems with more than ve blocks or goal conjuncts were used. Third, all of the Blocksworld predicates are unique attributes so that the cost of matching prodigy/ebl's control rules scales linearly with the state size. Finally, prodigy/ebl's impact was signicantly improved by its use of utility evaluation. <ref type="foot" target="#foot_7">16</ref> Despite all these factors (which aid EBL) prodigy/ebl still failed to be eective in the ABworld demonstrating the diculty of analyzing complex recursions, even in relatively simple Blocksworld problems.</p><p>Experimental Methodology In the ABworld prodigy was tested on the rst 30 problems of Minton's Blocksworld test-problem set. The massive searches required to solve the larger problems in Minton's test set exhausted the machine's memory. Figure <ref type="figure" target="#fig_5">3</ref>, in Section 1.1, shows that prodigy/ebl was foiled in this problem space. Comparing Figure <ref type="figure" target="#fig_5">3</ref> with Figure <ref type="figure" target="#fig_1">1</ref>, which shows prodigy/ebl's impact in the Blocksworld, demonstrates that prodigy/ebl's impact in the ABworld did indeed degrade signicantly relative to the Blocksworld. prodigy+ebl problem-solving time with prodigy's on the randomly generated problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Modied Problem Distributions</head><p>The number of blocks in Minton's problem set ranged from three to twelve, and the number of goal conjuncts ranged from one to ten. In the rst experiment, the number of blocks was increased to twenty and number of goal conjuncts was uniformly ten. The results of the experiment on this problem distribution appear in Figure <ref type="figure" target="#fig_3">12</ref>. Prodigy, running with no control rules, was unable to solve any of the large Blocksworld problems within the time bound (150 CPU seconds). Hence, the curve representing prodigy's problem-solving time appears linear, and the dierence between prodigy+ebl's problem-solving time and prodigy's is understated. prodigy+ebl was able to solve all of the large Blocksworld problems within the time bound. Minton reports on three parameters used to determine the nature of his Blocksworld problem set: the probability that a block was being held in the initial state, that a block was on the table (as opposed to on another block) in the initial state, and the probability that a goal conjunct was in fact satised in the initial state. To modify the problem distribution in the second experiment, the three parameters were changed from 1/3 to 2/3, from 1/3 to 1/2, and from 1/3 to 1/6. The intended impact of the change was to make towers in the initial state shorter and to increase problem diculty by requiring more goal conjuncts to be actively achieved. The results of the experiment on this problem distribution appear in Figure <ref type="figure" target="#fig_5">13</ref>. In both cases, as in Minton's experiments, prodigy+ebl remained signicantly faster than prodigy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Complementary Target Concepts are Essential</head><p>We might hypothesize that, although prodigy/ebl relies strongly on the failure and goal interaction target concepts in Minton's experiments, it might be able to compensate for their absence by relying more heavily on learning from success. This hypothesis is important because many EBL (and partial evaluation) systems rely exclusively on learning from success. If the hypothesis were true, these systems would not need to be extended to cover a wider range of target concepts. The structural theory suggests that this hypothesis is false due to the arguments in Section 5.3. Briey, learning only from success in recursive problem spaces will yield recursive proofs which, by the recursive heuristic, are unlikely to be useful. In addition, once the problem solver has diverged from a solution path, the only control rules that can curtail backtracking are control rules learned from complementary target concepts such as failure and goal interaction.</p><p>To test the hypothesis prodigy/ebl was run, using the training procedure in Minton's thesis, with its failure and goal interaction target concepts \turned o." prodigy/ebl only learned from success. I refer to prodigy guided only by learning from success as prodigy+success. Figure <ref type="figure" target="#fig_8">14</ref> contrasts prodigy+success with prodigy+ebl and with prodigy in prodigy/ebl's problem spaces. In all cases, prodigy+success was close to prodigy, and much slower than prodigy+ebl. This observation invalidates the hypothesis suggested above; the experimental results support the structural theory which argues that complementary target concepts enhance the eectiveness of EBL. This section relied on the structural theory to account for prodigy/ebl's success in Minton's experiments and to informally explain several additional experimental results originally reported in <ref type="bibr" target="#b9">[10]</ref>. The experiments, and their analysis, provide a degree of conrmation for the structural theory. only briey below. One analysis <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> independently reaches a conclusion that is closely related to the recursive heuristic, and is discussed at greater length. No analysis of EBL has derived conclusions akin to the nonrecursive heuristic or the structural thesis. Mahadevan, Natarajan, and Tadepalli <ref type="bibr" target="#b27">[28]</ref> present a formal model of learning as improving problem-solving performance based on the framework in <ref type="bibr" target="#b38">[39]</ref>. Based on the model, Tadepalli <ref type="bibr" target="#b54">[55]</ref> argues that EBL relies on structural constraints (e.g. serial decomposability) and that, like inductive learning, EBL can be analyzed using Valiant's PAC framework <ref type="bibr" target="#b58">[59]</ref>. Although Tadepalli's model is dierent from my own, the spirit of the two approaches is similar: we both seek to identify classes of problem spaces in which EBL results in tractable problem solving. Cohen <ref type="bibr" target="#b5">[6]</ref> describes a solution-path caching mechanism that operates in polynomial time and provably improves performance (see also <ref type="bibr" target="#b4">[5]</ref>). Both Cohen and Tadepalli make useful connections between Valiant's framework and the learning of search control knowledge.</p><p>Minton's thesis contains a thorough analysis of prodigy/ebl. Minton's focus, however, is on the impact of prodigy/ebl's components on its performance. His thesis does consider how problem space characteristics inuence the ecacy of learning, but the discussion is, by and large, qualitative and informal. For example, Minton points out that a problem space's \solution density and distribution" inuences the utility of dierent learning strategies. Minton himself concedes that \we have not adequately characterized the types of domains for which the learning method produces good results." <ref type="bibr" target="#b31">[32]</ref>.</p><p>Greiner and Likuski <ref type="bibr" target="#b20">[21]</ref> model EBL as adding redundant rules to a set of inference rules. They report that, in general, nding the optimal inference strategy in a redundant search space is NP-hard. However, if a single EBL rule is added to a nonconjunctive, nonrecursive, and irredundant rule set, then a simple extension to Smith's algorithm <ref type="bibr" target="#b50">[51]</ref> can still yield an optimal inference strategy in linear time. Most of the issues considered in this paper (e.g. recursion and complementary target concepts) do not arise in Greiner and Likuski's model.</p><p>Subramanian and Feldman <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref> extend Greiner and Likuski's model to cover recursive and conjunctive rule sets. Adopting Minton's utility criterion <ref type="bibr" target="#b32">[33]</ref> they show that, relative to an arbitrary probability distribution and theory, adding a redundant rule m can be justied exactly when p m &gt; C mf =(C mf + C d 0 C ms ) where p m is the probability that m applies, C mf is the average cost of determining that m does not apply, C d is the average cost of answering queries using the theory, and C ms is the average cost of using m to answer a query. Subramanian and Feldman argue that estimating the various costs and probabilities required to apply their inequality can be done by sampling (in a manner similar to prodigy/ebl's utility evaluation module, perhaps). They go on to show that \unless we make very strong assumptions about the nature of the distribution of future problems, it is not protable to form recursive macro-rules via EBL."</p><p>Thus, based on an entirely dierent model, Subramanian and Feldman reach a conclusion similar to the recursive heuristic. Although a simple complexity analysis suces to argue against learning from recursive proofs via EBL (the overhead of utilizing rules learned from such proofs is exponential in the depth of the recursions encountered and the rules are recursion-depth-specic), they rely on a more elaborate cost model of horn-clause theorem proving. This detailed model is motivated by their broader project which seeks to \quantitatively estimate" the cost of inference. The formal analysis in their paper is supported by experiments using Shavlik's <ref type="bibr" target="#b46">[47]</ref> circuit synthesis domain theory, providing further conrmation of the value of the recursive heuristic. Subramanian and Feldman did not reach conclusions akin to the nonrecursive heuristic, or the structural thesis.</p><note type="other">8</note><p>Concluding Remarks I conclude with a critique of the structural theory and a discussion of directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Critique of the Structural Theory</head><p>Although the structural theory has been tested extensively on several problem spaces using the prodigy system, additional experiments using dierent problem solvers, problem spaces, and target concepts are necessary to further test and rene the theory. Several directions for investigation are particularly intriguing:</p><p>Can we substitute \short" for \nonrecursive" and \long" for \recursive" in the structural theory? <ref type="foot" target="#foot_8">17</ref> Although short rules are guaranteed to have low-match cost per node, Proposition 5 (Section 2.3) shows that short rules will not necessarily yield average speedup. In fact, static's nonrecursive proofs resulted in control rules of widelyvarying lengths demonstrating that the terms \short" and \nonrecursive" are not coextensive. Furthermore, as explained in Section 5.1.1, the \short heuristic" would lead EBL to learn from bounded-depth recursive proofs forbidden by the recursive heuristic. While the long/short dichotomy is clearly distinct from the recursive/nonrecursive one, it is worth investigating the performance of an EBL system restricted to acquiring only short rules. To study this issue, the terms \long" and \short" would have to be dened precisely. Does a \short" proof have the same length in all problem spaces? Or is \short" a function of problem-space parameters such as the number of operators and predicates?</p><p>Recall that complementary target concepts are concepts, such as failure in prodigy/ebl, whose proofs are not direct translations of the problem solver's plans. The structural theory advocates learning from complementary target concepts in order to obtain nonrecursive explanations of the problem solver's behavior. In prodigy/ebl's benchmark problem spaces, all but one of static's rules were learned from complementary target concepts. Is it possible that learning from complementary target concepts, rather than learning from nonrecursive proofs, is actually the key to the success of EBL? To test this conjecture we need to identify tasks where complementary target concepts yield recursive proofs, and compare the eectiveness of EBL, learning from complementary concepts, to its eectiveness learning only from nonrecursive proofs.</p><p>Problem space Graphs (PSGs) are dened for backward-chaining problem solvers. It Can the recursive heuristic be rened to exclude cases in which EBL will succeed when learning from recursive proofs? When will EBL be eective on problem spaces with bounded recursive depths such as the Eightpuzzle or the three-disk Tower of Hanoi? How will EBL fare when generalization-to-N methods (seeking broad coverage for EBL's knowledge) are applied to unique-attribute problem space (ensuring low local match cost for EBL's weakest preconditions)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Task Encoding and EBL</head><p>In some cases EBL is eective and in others it is not. The structural theory explains this observation in two steps. First, the theory exhibits a correspondence between EBL's proofs and the PSG, a graph representation of problem spaces. Second, the theory shows how certain subgraphs (corresponding to nonrecursive proofs) aid EBL whereas other subgraphs (corresponding to recursive proofs) hinder EBL. Thus, structure of the PSG is a major factor inuencing the eectiveness of EBL.</p><p>The PSG representation changes with the problem space encoding. In practice, problem space encoding is often modied and tweaked repeatedly until EBL generates eective control knowledge. As Letovsky <ref type="bibr" target="#b26">[27]</ref> put it: \It is not the case that EBG systems can take any, or even most, inecient encodings of a task and turn out ecient versions; rather it is sometimes the case that there exists some encoding of a task for which an EBG system can do something reasonable." Today, successfully applying EBL is something of a black art. If EBL is to be more broadly useful, this art needs to mature into a well-understood body of knowledge. Given a task, the ideal theory would recommend an appropriate problem space encoding, problem solving method, and \brand" of EBL (Which target concepts? What operationality criterion? Macros versus control rules? etc.). The structural theory of EBL is only a rst step in this direction. The theory does, however, provide an account of EBL's performance that is based on problem-space characteristics, providing a problem-space designer with a principled basis for making representational choices.</p><p>Whether a problem space yields recursive or nonrecursive proofs depends, in part, on the problem space's PSGs. Since minor modications to the problem space's representation can result in major changes to its PSG, the problem-space designer has to understand the impact of dierent representational choices on the PSG. For example, reordering operator preconditions has no appreciable impact on the PSG, suggesting that this transformation will not aid EBL. Changing problem-space predicates, in contrast, can rid the problem space of troublesome recursions. In the Blocksworld, for example, deciding whether one block is above another block requires a recursive computation. If the Cartesian coordinates of each block are specied, however, determining whether one block is above another merely requires comparing the X-coordinates of the blocks, ridding the PSG of a recursion. Thus, the theory suggests a general methodology for understanding the impact of problem space encoding on EBL: catalog the impact of representational transformations on the location and presence of recursion in PSGs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: prodigy/ebl speeds up prodigy in the Blocksworld. A bound is placed on the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The grasp-second-block macro-operator used to create the ABworld. Running prodigy/ebl on the ABworld (following Minton's training procedure for the Blocksworld) produced a rule set that actually slowed prodigy down on Minton's test</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: prodigy/ebl is foiled in the ABworld.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>INTRODUCTION 4 problem</head><label>4</label><figDesc>solving in certain cases (see Section 5.1.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A match tree showing how the expression p(X,Y),r(Y,Z) might be matched against the state p(a,c),p(a,b),r(c,d),r(b,c) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A recursive proof tree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The holding PSG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The PSG subgraph corresponding to the proof that unstack fails to achieve holding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: Total problem-solving time in prodigy/ebl's problem spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: A fragment of the PSG subgraph corresponding to the proof that unstack fails to achieve holding in the ABworld. For brevity, the recursive subgraph below the node (clear V) is not shown. The subgraph leads the node (clear V) to be labeled recurs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Figure12: prodigy/ebl's impact on large Blocksworld problems. Due to the size of the problems prodigy was unable to solve any of the problems within the time bound.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>is not entirely clear what the corresponding notion is for forward-chaining problem solvers. Consequently, determining the precise scope of the structural thesis remains an open question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>A control rule from prodigy's Blocksworld.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Mitchell et al.'s specication of EBG.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Section 6.1 argues that prodigy/ebl is eective in the Blocksworld due to the nonrecursive proofs found by prodigy/ebl. An important advantage of learning from nonrecursive proofs is the ability to acquire control rules that are eective across changes in problem distribution. I illustrate this point by showing how the rule set learned by prodigy/ebl in the Blocksworld remains eective across two problem distributions that are very dierent from Minton's original distribution.Experimental Methodology A Blocksworld problem consists of an initial state and a goal conjunction. A distribution of Blocksworld problems is dened by a problem generator which determines the initial state and the goal. Blocksworld states can be characterized by the number of blocks, the height and number of towers, and whether the robot is holding a block. Blocksworld goal conjunctions can be characterized by the length of the conjunctions and their composition. I drastically modied the distribution dened by Minton's problem generator, randomly generating fty problems using the new distribution, and comparing</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The paper uses the term \prodigy/ebl" to refer specically to prodigy's EBL module, and \EBL" to refer to the general paradigm. The prodigy problem solver, guided by prodigy/ebl's control rules, is</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A literal is a possibly negated atomic formula. A ground literal is one that contains no variables.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This match cannot be performed via unication (which is linear-time) because unication is not dened for conjunctive expressions. See<ref type="bibr" target="#b21">[22]</ref> for a precise denition of unication.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3"><p>The PSG should not be confused with a state space graph. The state space graph's nodes are states, and its edges are instantiated operators, whereas PSG nodes are literals or operators, and PSG edges are and/or links.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_4"><p>The recursive expansions in prodigy/ebl's recursive proofs are represented as leaf nodes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_5"><p>See<ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b45">46]</ref> for exceptions in certain special cases.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_6"><p>The prodigy system, control rules, benchmark problem spaces, and problem sets are publicly available by sending mail to prodigy@cs.cmu.edu.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_7"><p>The results reported in this experiment and in all others are for prodigy/ebl using utility evaluation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_8"><p>This insightful question was posed by Paul Rosenbloom and Prasad Tadepalli.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This paper is based on my Ph.D. dissertation at Carnegie Mellon University, which was supported by an AT&amp;T Bell Labs Ph.D. Fellowship. Thanks are due to my advisor, Tom Mitchell, to the members of my committee, Jaime Carbonell, Paul Rosenbloom, and Kurt Vanlehn, and to Allen Newell for their impact on the dissertation. Special thanks are due to Steve Minton|whose thesis work made studying prodigy/ebl possible. Steve Hanks, Craig Knoblock, Phil Laird, Neal Lesh, Alicia P erez, Prasad Tadepalli, Dan Weld and the anonymous reviewers provided helpful comments on earlier drafts.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive search by explanation-based learning of heuristic censors</title>
		<author>
			<persName><forename type="first">Neeraj</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Mostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth National Conference on Articial Intelligence</title>
		<meeting>the Eighth National Conference on Articial Intelligence</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Retrospectives: A note from the editor</title>
		<author>
			<persName><forename type="first">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Bobrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Articial Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generalizing the order of goals as an approach to generalizing number</title>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Bostrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Machine Learning</title>
		<meeting>the Seventh International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The FERMI system: Inducing iterative macro-operators from experience</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth National Conference on Articial Intelligence</title>
		<meeting>the Fifth National Conference on Articial Intelligence</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="490" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Concept Learning Using Explanation Based Generalization as an Abstraction Mechanism</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>Rutgers University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using distribution-free learning theory to analyze chunking</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Biennial Conference of the Canadian Society for Computational Studies of Intelligence</title>
		<meeting>the Eighth Biennial Conference of the Canadian Society for Computational Studies of Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meta-rules: reasoning about control</title>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="179" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Explanation-based learning: An alternative view</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Dejong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A Discipline of Programming</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Dijkstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Englewood Clis, N.J.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Structural Theory of Explanation-Based Learning</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno>CMU-CS-90-185</idno>
	</analytic>
	<monogr>
		<title level="m">Available as technical report</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Acquiring search-control knowledge via static analysis</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno>92-04-01</idno>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An asymptotic analysis of speedup learning</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Machine Learning</title>
		<meeting>the Ninth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Statistical methods for analyzing speedup learning experiments</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Etzioni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>Submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Why EBL produces overly-specic knowledge: A critique of the PRODIGY approaches</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Minton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Machine Learning</title>
		<meeting>the Ninth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comparative analysis of chunking and decisionanalytic control</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Soar Papers: Research on Integrated Intelligence</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>To Appear</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning and executing generalized robot plans</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Articial Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Computers And Intractability A guide to the Theory of NP-Completeness</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<pubPlace>Freeman, New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An overview of meta-level architecture</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Genesereth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI<address><addrLine>Washington, D.C.</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="1983-08">August 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Composer: A probabilistic solution to the utility problem in speed-up learning</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerald</forename><surname>Dejong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI-92</title>
		<meeting>AAAI-92</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A solution to the ebl utility problem</title>
		<author>
			<persName><forename type="first">Russell</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI-92</title>
		<meeting>AAAI-92</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Incorporating redundant learned rules: A preliminary formal analysis of EBL</title>
		<author>
			<persName><forename type="first">Russell</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Likuski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Joint Conference on Articial Intelligence</title>
		<meeting>the Eleventh International Joint Conference on Articial Intelligence</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unication: A multidisciplinary survey</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1989-03">March 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Macro-operators: A weak method for learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Korf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Articial Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="35" to="77" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A proof procedure using connection graphs</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Kowalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="572" to="595" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Soar: An architecture for general intelligence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Rosenbloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Articial Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">64</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Chunking in Soar: The anatomy of a general learning mechanism</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Rosenbloom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="46" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Operationality criteria for recursive predicates</title>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Letovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth National Conference on Articial Intelligence</title>
		<meeting>the Eighth National Conference on Articial Intelligence</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A framework for learning as improving problem-solving performance</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Sridhar Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Spring Symposium on Explanation-Based Learning</title>
		<meeting>the AAAI Spring Symposium on Explanation-Based Learning</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">EBL and weakest preconditions</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Minton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Spring Symposium on Explanation-Based Learning</title>
		<meeting>the AAAI Spring Symposium on Explanation-Based Learning</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="210" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Eective Search Control Knolwedge: An Explanation-Based Approach</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Minton</surname></persName>
		</author>
		<idno>CMU-CS-88-133</idno>
	</analytic>
	<monogr>
		<title level="m">Available as technical report</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quantitative results concerning the utility of explanation-based learning</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Minton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh National Conference on Articial Intelligence</title>
		<meeting>the Seventh National Conference on Articial Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Quantitative results concerning the utility of explanation-based learning</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Minton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Articial Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<date type="published" when="1990-03">March 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Acquiring eective search control rules: Explanation-based learning in the Prodigy system</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Minton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Kuokka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Workshop on Machine Learning</title>
		<meeting>the Fourth International Workshop on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Explanation-based learning: A problem-solving perspective</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Minton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Kuokka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yolanda</forename><surname>Gil</surname></persName>
		</author>
		<idno>CMU-CS-89-103</idno>
	</analytic>
	<monogr>
		<title level="m">Available as technical report</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Prodigy 2.0: The manual and tutorial</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Minton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Kuokka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yolanda</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<idno>CMU-CS-89-146</idno>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Version Spaces: An Approach to Concept Learning</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<idno>HPP- 79-2</idno>
		<imprint>
			<date type="published" when="1978-12">December 1978</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Stanford CS report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Theo: A framework for self-improving systems</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Chalasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Ringuette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerey</forename><forename type="middle">C</forename><surname>Schlimmer</surname></persName>
		</author>
		<editor>K. VanLehn</editor>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Architectures for Intelligence. Erlbaum</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Explanation-based generalization: A unifying view</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smadar</forename><surname>Kedar-Cabelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two new frameworks for learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Machine Learning</title>
		<meeting>the Fifth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Human Problem Solving</title>
		<author>
			<persName><forename type="first">Allen</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herbert</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Principles of Articial Intelligence</title>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Tioga Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DYNAMIC: a new role for training problems in EBL</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alicia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno>CMU-CS-92-124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Machine Learning</title>
		<meeting>the Ninth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>An expanded version available as technical report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discovery of algorithms from weak methods</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Prieditis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international meeting on advances in learning</title>
		<meeting>the international meeting on advances in learning</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Generalizing on multiple grounds: Performance learning in model-based troubleshooting. Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Resnick</surname></persName>
		</author>
		<idno>1052</idno>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
		<respStmt>
			<orgName>AI Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A critical look at experimental evaluations of EBL</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Segre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, 1991. forthcoming methodological note</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Automatic call unfolding in a partial evaluator</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sestfot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Partial Evaluation and Mixed Computation</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Bjorner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Ershov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Jones</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier Science Publishers</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>Workshop Proceedings</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Acquiring recursive concepts and iterative concepts with explanationbased learning</title>
		<author>
			<persName><forename type="first">Jude</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Building a computer model of classical mechanics</title>
		<author>
			<persName><forename type="first">Jude</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Dejong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Annual Conference of the Cognitive Science Society</title>
		<meeting>the Seventh Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="351" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards a general framework for composing disjunctive and iterative macro-operators</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Joint Conference on Articial Intelligence</title>
		<meeting>the Eleventh International Joint Conference on Articial Intelligence</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A graph-dynamic model of the power law of practice and the problem-solving fan-eect</title>
		<author>
			<persName><forename type="first">Je</forename><surname>Shrager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tad</forename><surname>Hogg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">242</biblScope>
			<biblScope unit="page" from="414" to="416" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<idno>STAN-CS-86-1107</idno>
		<title level="m">Available as technical report</title>
		<meeting><address><addrLine>Stanford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>Controlling Inference</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The utility of EBL in recursive domain theories</title>
		<author>
			<persName><forename type="first">Devika</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth National Conference on Articial Intelligence</title>
		<meeting>the Eighth National Conference on Articial Intelligence</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The utility of EBL in recursive domain theories</title>
		<author>
			<persName><forename type="first">Devika</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Feldman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
	<note>An extended version of the AAAI 1990 paper.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Measuring utility and the design of provably good ebl algorithms</title>
		<author>
			<persName><forename type="first">Devika</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Hunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of the AAAI Spring Symposium on Knowledge Assimilation</title>
		<meeting><address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A formalization of explanation-based macro-operator learning</title>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelveth International Joint Conference on Articial Intelligence</title>
		<meeting>the Twelveth International Joint Conference on Articial Intelligence</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The problem of expensive chunks and its solution by restricting expressiveness</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tambe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rosenbloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning Journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="348" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Eliminating expensive chunks by restricting expressiveness</title>
		<author>
			<persName><forename type="first">Milind</forename><surname>Tambe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Rosenbloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Joint Conference on Articial Intelligence</title>
		<meeting>the Eleventh International Joint Conference on Articial Intelligence</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Database and Knowledge-base systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jerey</surname></persName>
		</author>
		<author>
			<persName><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Computer Science Press</publisher>
			<biblScope unit="volume">II</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A theory of the learnable</title>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Explanation-based generalisation = partial evaluation</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Van Harmelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Bundy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Articial Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>Research Note</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
