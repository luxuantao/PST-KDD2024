<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Powerful are Spectral Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiyuan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">How Powerful are Spectral Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based on graph signal filters, and some models able to learn arbitrary spectral filters have emerged recently. However, few works analyze the expressive power of spectral GNNs. This paper studies spectral GNNs' expressive power theoretically. We first prove that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2) no missing frequency components in node features. We also establish a connection between the expressive power of spectral GNNs and Graph Isomorphism (GI) testing which is often used to characterize spatial GNNs' expressive power. Moreover, we study the difference in empirical performance among different spectral GNNs with the same expressive power from an optimization perspective, and motivate the use of an orthogonal basis whose weight function corresponds to the graph signal density in the spectrum. Inspired by the analysis, we propose JacobiConv, which uses Jacobi polynomial basis due to their orthogonality and flexibility to adapt to a wide range of weight functions. JacobiConv deserts nonlinearity while outperforming all baselines on both synthetic and real-world datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph Neural Networks (GNNs) have achieved state-of-theart performance on almost all tasks among various graph representation learning methods <ref type="bibr" target="#b37">(Yao et al., 2019;</ref><ref type="bibr" target="#b11">Fout et al., 2017;</ref><ref type="bibr" target="#b3">Chen et al., 2018)</ref>. Spectral GNNs are a kind of GNNs that design graph signal filters in the spectral domain.</p><p>Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).</p><p>Though various models have emerged, spectral GNNs' expressive power is still under-researched. Moreover, these models differ mainly in the basis choices of the spectral filters. However, to our best knowledge, no study has systematically explained these differences and studied the advantages and disadvantages of different bases.</p><p>Existing spectral GNNs can be summarized into a general form: first transforming the spatial signal X through an MLP, then applying spectral filters parameterized by a polynomial of the normalized Laplacian L, and finally applying another MLP to the filtered signal. By designing/learning the polynomial coefficients, spectral GNNs can simulate a wide range of filters (low-pass, band-pass, high-pass) in the spectral domain, enabling GNNs to work on not only homophilic but also heterophilic graphs <ref type="bibr" target="#b8">(Chien et al., 2021)</ref>.</p><p>However, a natural question to ask is, whether the MLPs or nonlinearity are useful at all, or spectral filters themselves are enough? Therefore, we remove nonlinearity from spectral GNNs, and explore the expressive power of such linear spectral GNNs whose power relies only on the spectral filters. We prove that linear GNNs are universal under some mild conditions, i.e., they are powerful enough to produce arbitrary predictions without relying on MLPs. Our results show that nonlinearity is not necessary for spectral GNNs to reach a high expressive power, which is also verified in our experiments. Moreover, we analyze spectral GNNs' universality conditions from a Graph Isomorphism (GI) testing perspective. The latter is often used to characterize spatial GNNs' expressive power <ref type="bibr" target="#b34">(Xu et al., 2019)</ref>. Our results for the first time build a bridge between the expressivity analyses of spectral GNNs and spatial GNNs.</p><p>Next, we notice that spectral GNNs with different filter bases have the same expressive power but different empirical performance. To study these differences, we analyze the optimization of such models. By checking the Hessian matrix of linear GNNs near the global minimum, we find that using an orthogonal basis with the density of graph signal as the weight function can maximize the convergence speed. We further check the orthogonality of existing bases.</p><p>Inspired by these discussions, we propose a novel expressive spectral GNN, JacobiConv. JacobiConv deserts nonlinearity, approximates filter functions with Jacobi polynomial bases, and can adapt to a wide range of weight functions. We also design a novel technique named Polynomial Coefficient Decomposition (PCD) to improve the filter coefficient optimization. In numerical experiments, we first test the expressive power of JacobiConv to approximate filter functions on synthetic datasets. JacobiConv achieves the lowest loss on learning the filter functions compared to state-of-the-art spectral GNNs. We also show that JacobiConv outperforms all baselines on ten real-world datasets by up to 12%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>For any matrix M ? R a?b , M i is the i th row of M , M :i is the i th column of M , M AB is the submatrix of M corresponding to row index set A and column index set B. Let ? ij denote Kronecker delta: 1 if i = j, and 0 otherwise. We define condition number of a matrix M as ?(M ) = |?max| |?min| , where ? min , ? max are the minimum and maximum eigenvalues of M , respectively. If M is singular, ?(M ) = +?.</p><p>Let G = (V, E, X) denote an undirected graph with a finite node set V = {1, 2, ..., n}, an edge set E ? V ? V and a node feature matrix X ? R n?d , whose i th row X i is the feature vector of node i. N (i) refers to the set of nodes adjacent to node i. Let A be the adjacency matrix of G and D be the diagonal matrix whose diagonal element D ii is the degree of node i. The normalized adjacency matrix is ? = D -1 2 AD -1 2 . Let I denote the identity matrix. The normalized Laplacian matrix L = I -?. Let L = U ?U T denote the eigendecomposition of L, where U is the matrix of eigenvectors and ? is the diagonal matrix of eigenvalues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph Isomorphism</head><p>A permutation ? is a bijective mapping from {1, 2, ..., n} to {1, 2, ..., n}, where n</p><formula xml:id="formula_0">? N + . For node set V, ?(V) = {?(i)|i ? V}. For node attribute matrix X, ?(X) ?(i) = X i . An automorphism of a graph G = (V, E) is a permutation ? such that ?(V) = V, ?(E) = E. An automorphism of a graph with node features G = (V, E, X) is a permutation ? such that ?(V) = V, ?(E) = E, ?(X) = X. The order of an automorphism is min k ? k = e, k = 1, 2, ...</formula><p>, where e is the identity mapping. Two nodes i, j are isomorphic if ?(i) = j under some automorphism ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Graph Signal Filter and Spectral GNNs</head><p>The graph Fourier transform of a signal X ? R n is defined as X = U T X ? R n and the inverse transform is X = U X <ref type="bibr" target="#b30">(Shuman et al., 2013)</ref>. The i th column of U , U :i , is a frequency component corresponding to the eigenvalue ? i .</p><p>Let X? = U T :? X, where U :? is the eigenvector corresponding to ?, be the frequency component of X at ? frequency. If X? = 0, we say X contains the ? frequency component. Otherwise, the ? frequency component is missing from X.</p><p>We can use g : [0, 2] ? R to filter each frequency component by multiplying g(?). A spectral filter g on signal X can be defined as g( L)X = U g(?)U T X.</p><p>(1)</p><p>By setting the filter function g to a polynomial, the filter g( L) can be expressed as a polynomial of L:</p><formula xml:id="formula_1">g(?) = K-1 k=0 ? k ? k . g( L) = K-1 k=0 ? k U ? k U T = K-1 k=0 ? k Lk .</formula><p>(2)</p><p>We show the forms of some popular Spectral GNNs in Appendix A. In general, existing spectral-based GNNs can be formulated as follows:</p><formula xml:id="formula_2">Z = ?(g( L)?(X)),<label>(3)</label></formula><p>where Z is the prediction, ?, ? are functions like multi-layer perceptrons (MLPs) and g is a polynomial. If a spectralbased GNN can express any polynomial filter function g, we call it Polynomial-Filter-Most-Expressive (PFME) GNNs.</p><p>We also define Filter-Most-Expressive (FME) GNNs as the GNNs able to express arbitrary real-valued filter functions.</p><p>This study is mainly interested in the case when ? and ? are linear functions, so we define linear GNN. Definition 2.1. A linear GNN can be formulated as Z = g( L)XW , where Z ? R n?d is the prediction matrix, g is a learnable real-valued polynomial, and W ? R d?d is a learnable matrix.</p><p>Linear GNN keeps the spectral filter form of spectral GNNs despite its simplicity. And the expressive power of linear GNNs is a lower bound for that of spectral GNNs in (3). Proposition 2.2. Linear GNN is PFME. If ? and ? can express all linear functions, then spectral GNNs can differentiate any pair of nodes for which linear GNNs can produce different outputs.</p><p>We assume all our models work on a fixed graph and only perform node property prediction tasks. Suppose there is an arbitrary real-valued filter function to approximate. Though PFME GNNs can only express polynomial filter functions, as the eigenvalue ? is a discrete variable in a fixed graph, an interpolation polynomial always exists for the arbitrary filter and can produce the same output. Therefore, PFME GNNs are FME in our problem setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spectral GNNs</head><p>Spectral GNNs are GNNs based on spectral graph filters <ref type="bibr" target="#b33">(Wu et al., 2021)</ref>. <ref type="bibr" target="#b13">He et al. (2021)</ref> categorize spectral GNNs by the filter operation adopted. One class is spectral GNNs with fixed filters: APPNP <ref type="bibr">(Klicpera et al., 2019a)</ref> utilizes Personalized PageRank (PPR) <ref type="bibr" target="#b24">(Page et al., 1999)</ref> to build filter functions. GNN-LF/HF <ref type="bibr" target="#b40">(Zhu et al., 2021)</ref> designs filter weights from the perspective of graph optimization functions. The other class is spectral GNNs with learnable filters: ChebyNet <ref type="bibr" target="#b9">(Defferrard et al., 2016)</ref> approximates the filters with Chebyshev polynomials. GPRGNN <ref type="bibr" target="#b8">(Chien et al., 2021)</ref> learns a polynomial filter by directly performing gradient descent on the polynomial coefficients. ARMA <ref type="bibr" target="#b2">(Bianchi et al., 2021)</ref> uses rational filters. Bern-Net <ref type="bibr" target="#b13">(He et al., 2021)</ref> expresses the filtering operation with Bernstein polynomials. All these methods use some form of polynomial filter despite the different bases they use. GPRGNN is one of the most expressive models. It can express all polynomial filters. So does ChebyNet, as the Chebyshev polynomial is also a complete set of basis in the polynomial space. They are all PFME. BernNet is less expressive as it forces the coefficients of the Bernstein polynomial bases to be positive and can only express positive filter functions. However, such constraints are introduced for regularization, so we ignore them when analyzing the expressive power. The filter forms of these models are summarized in Table <ref type="table" target="#tab_4">5</ref>. Though ARMA <ref type="bibr" target="#b2">(Bianchi et al., 2021)</ref> and GNN-LF/HF <ref type="bibr" target="#b40">(Zhu et al., 2021)</ref> use rational functions, they approximate the rational functions with polynomials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Removing Nonlinearity from GNNs</head><p>Various GNNs removing nonlinearity have been proposed. <ref type="bibr" target="#b32">Wu et al. (2019)</ref> precompute ?k X and perform logistic regression on the preprocessed features. Some works leverage personalized PageRank (PPR) <ref type="bibr" target="#b24">(Page et al., 1999)</ref> and random walk on the graph. <ref type="bibr">Klicpera et al. (2019b)</ref> use generalized graph diffusion, like the heat kernel and PPR, to reconstruct the graph. APPNP <ref type="bibr">(Klicpera et al., 2019a)</ref> replaces normalized adjacency matrix with approximate PPR matrix to capture multi-hop neighborhood information. Some models with more complex acceleration techniques to computer PPR are introduced, like GBP <ref type="bibr">(Chen et al., 2020a)</ref>.</p><p>Existing linear models are mainly motivated by improving the scalability, and have restricted filters. In contrast, we analyze the expressive power and optimization property of linear GNNs with arbitrary polynomial filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Expressive Power of GNNs</head><p>The Weisfeiler-Lehman (WL) test of graph isomorphism <ref type="bibr" target="#b31">(Weisfeiler &amp; Leman, 1968</ref>) is an algorithm that can distinguish almost all non-isomorphic graphs. Its 1dimensional form (1-WL) iteratively aggregates neighborhood labels and maps the aggregated labels into a new label for each node, which is similar to GNNs based on neighborhood node feature aggregation. The label of nodes assigned by 1-WL test can also be used to check if two nodes are isomorphic <ref type="bibr" target="#b34">(Xu et al., 2019)</ref>, as two isomorphic nodes always have the same label while two non-isomorphic ones mostly have different labels. <ref type="bibr" target="#b34">Xu et al. (2019)</ref> show that the 1-WL test bounds the expressive power of GNNs to distinguish non-isomorphic graphs. Since then, various works attempt to analyze GNNs with the WL tests and graph isomorphism testing <ref type="bibr" target="#b22">(Morris et al., 2019;</ref><ref type="bibr">Maron et al., 2019a;</ref><ref type="bibr" target="#b6">Chen et al., 2019;</ref><ref type="bibr" target="#b38">You et al., 2019;</ref><ref type="bibr" target="#b17">Li et al., 2020;</ref><ref type="bibr" target="#b27">Sato et al., 2019;</ref><ref type="bibr" target="#b39">Zhang &amp; Chen, 2018;</ref><ref type="bibr" target="#b28">Sato et al., 2021)</ref>. Other than the WL test and graph isomorphism, some works describe the expressive power in different ways, such as expressing universal invariant functions <ref type="bibr">(Maron et al., 2019b;</ref><ref type="bibr" target="#b14">Keriven &amp; Peyr?, 2019)</ref>, counting substructures <ref type="bibr">(Chen et al., 2020b)</ref>, simulating Turing Machine <ref type="bibr" target="#b18">(Loukas, 2020)</ref>, computing graph properties <ref type="bibr" target="#b12">(Garg et al., 2020)</ref>, and differentiating rooted graphs <ref type="bibr" target="#b4">(Chen et al., 2021)</ref>. Some works <ref type="bibr" target="#b1">(Balcilar et al., 2021)</ref> analyze expressive power from a spectral perspective but the discussion is constrained to concluding former models to spectral forms. This study provides conditions for spectral GNNs to approximate any functions and discuss these conditions in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Expressive Power of Linear GNNs</head><p>In this section, we prove that linear GNNs are universal under some conditions, and discuss these conditions for universal approximation to characterize the expressive power of linear GNNs. All proofs are in the appendix.</p><p>There are two components in a linear GNN Z = g( L)XW :</p><p>Linear Transformation W . Since XW = U ( XW ), a linear transformation in the spatial domain is also a linear transformation in the frequency domain, which produces a linear combination of signals in different channels.</p><p>Filter g( L). Since g( L)X = U (g(?) X), the filtering operation scales the frequency component corresponding to ? of X by g(?) fold in the frequency domain.</p><p>Now we give the universality theorem of linear GNNs.</p><p>Theorem 4.1. Linear GNNs can produce any onedimensional prediction if L has no multiple eigenvalues and the node features X contain all frequency components.</p><p>There are three conditions for linear GNNs to be universal: 1) one-dimensional prediction, 2) no multiple eigenvalues, and 3) no missing frequency component. These are thus three bottlenecks for linear GNNs' expressive power. In the following, we discuss each of the three conditions in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multidimensional Prediction</head><p>Though linear GNNs are powerful enough when the output has only one dimension, each dimension may need a different polynomial filter when the prediction has multiple channels. Take the toy graph in Figure <ref type="figure">1</ref> as an example. W , in this case, is just a scalar for each output dimension, as the node feature is one-dimensional. Therefore, W can be merged into g by Z = g( L)XW = (W g( L))X = g ( L)X, and cannot help with the multidimensional prediction case.</p><p>We also formally describe this property in Proposition 4.2.</p><p>Proposition 4.2. Assuming that the node feature matrix X is not a full-row-rank matrix, even if X contains all the frequency components, and there are no multiple eigenvalues of L, then for all k &gt; 1, there exists a k-dimensional prediction linear GNNs cannot produce.</p><p>We can use individual polynomial coefficients for each output channel to solve this problem.</p><formula xml:id="formula_3">2 0 2 0 4 -6 ?? ? ?? ?? 0 2 0.5 1.5 ?? ? ?? ?? 0 2 0.5 1.5 ?? ? ?? ?? 0 2 0.5 1.5 0 -1 0 -1 1 -3 2 1 2 1 3 -1 0.5 ? ?? ?? ??(??) 0 2 0.5 1.5 ?? -0.5 ? ?? ?? ??(??) 0 2 0.5 1.5 Figure 1</formula><p>. Individual filter function is needed for each prediction dimension. We illustrate each graph with both its spatial representation (where numbers on nodes represent one-dimensional node features) and its spectrum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multiple Eigenvalue</head><p>If two frequency components have the same eigenvalue ?, they will be scaled by the same number g(?). Therefore, the coefficients of these frequency components in prediction will keep the same ratio as in input XW . This issue is related to graph topology. More discussion is in Theorem 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Missing Frequency Components</head><p>The filter operation can only scale a frequency component.</p><p>If this frequency component is missing from the node feature, the prediction cannot contain it either. Take the toy graph in Figure <ref type="figure">2</ref> as an example. The node feature only contains components corresponding to frequency ? = 0, so a linear GNN cannot produce output with frequency ? = 2 component. This problem is rooted in both the topology of graph G and node features X and is also difficult to solve.</p><formula xml:id="formula_4">? ?? ?? ? ?? ?? 1 1 1 1 1 1 -1 1 -1 1 1 -1 ?? 0 2 0.5 1.5 ?? 0 2 0.5 1.5</formula><p>Figure 2. Node features with missing frequency components cannot produce some outputs.</p><p>Nevertheless, multiple eigenvalues and missing frequency components are both rare in real-world graphs with node features. See Appendix E for the ratio of multiple eigenvalues and number of missing frequency components in each of the 10 real-world benchmark datasets. In all the datasets, no frequency component is missing, and on average less than 1% of eigenvalues are multiple. Therefore, the universality conditions can be largely satisfied in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Connection to Graph Isomorphism</head><p>Traditional expressivity analyses for spatial GNNs often leverage Graph Isomorphism testing. In this section, we explore the connections between our universality conditions and GI. We first build a connection between the expressive power of linear GNNs using a K-degree polynomial filter function and that of (K + 1)-iteration WL test. Proposition 4.3. Given a linear GNN whose filter function is a K-degree polynomial, define the function LG K (i) as the prediction of node i produced by the linear GNN. Let WL k (i) denote the label of node i produced by k-iteration WL test whose initial label of node i is the node feature</p><formula xml:id="formula_5">X i . ?i, j ? V, LG K (i) = LG K (j) if WL K+1 (i) = WL K+1 (j).</formula><p>Proposition 4.3 means that linear GNNs' expressive power is also bounded by the 1-WL test: if 1-WL cannot differentiate two nodes, linear GNNs will also fail. However, this result seems to contradict with the universal approximation property of linear GNNs. We know that: 1) 1-WL provably cannot discriminate some non-isomorphic nodes (such as nodes in a non-attributed regular graph), and 2) 1-WL always gives isomorphic nodes the same label. However, a universal linear GNN should be able to give any two nodes different predictions, no matter they are isomorphic or not.</p><p>To close this gap, we study the connections between the universality conditions of linear GNNs and the GI problem.</p><p>Our results show that the no-multiple-eigenvalue and nomissing-frequency conditions enable 1-WL to discriminate all non-isomorphic nodes, and also constrain the graph to contain no isomorphic nodes, therefore closing the gap.</p><p>We first show that under the two conditions, 1-WL can discriminate all non-isomorphic nodes. Corollary 4.4. If a graph has no missing frequency component and its normalized Laplacian has no multiple eigenvalues, then 1-WL can differentiate all non-isomorphic nodes.</p><p>The other part of the gap is that 1-WL cannot produce different labels for isomorphic nodes, while linear GNNs with universal approximation property can. Therefore, we analyze how our no-multiple-eigenvalue and no-missing-frequency conditions constrain the graph in Theorems 4.5 and 4.6. Theorem 4.5. For a graph whose normalized Laplacian has no multiple eigenvalues, the order of its automorphism is less than three.</p><p>More intuitively, an eigenvector can be permuted to produce another linearly independent eigenvector with the same eigenvalue for graphs with high symmetry, leading to multiple eigenvalues. Take the toy graph in Figure <ref type="figure">3</ref> as an example. The triangle (b) has a three-order automorphism and an eigenvector can be permuted to produce another linearly independent eigenvector, while for the two-node graph (a) with only two-order automorphism, permuting its eigenvector results in the same eigenvector. Since realworld graphs are often highly irregular, Theorem 4.5 partly explains why multiple eigenvalues are rare in practice.</p><formula xml:id="formula_6">1 0 -1 1 -1 -1 1 (a) (b) 1 0 -1</formula><p>Figure <ref type="figure">3</ref>. Graphs with high-order automorphisms (symmetries) have multiple Laplacian eigenvalues.</p><p>When considering node features containing all frequency components, all pairs of nodes are non-isomorphic and thus closing the gap between 1-WL and linear GNN. Theorem 4.6. Suppose a graph with node features does not have multiple eigenvalues in its normalized Laplacian, and no frequency component is missing from the node features.</p><p>There will be no automorphism for the graph other than the identical mapping.</p><p>Therefore, the conditions of Theorem 4.1 constrain the graph topology and node features so that 1-WL still bounds the expressive power of linear GNNs. On the other hand, our results indicate that 1-WL can be quite powerful given expressive node features and irregular graph structures. Our results build a bridge between the expressive power of spectral GNNs (in terms of universality under some conditions) and that of spatial GNNs (in terms of 1-WL test). As an analysis example, we also discuss how random features can boost the expressive power and why models with random features have poor empirical performance in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Role of Nonlinearity</head><p>Though linear GNNs have strong theoretical expressive power and remarkable empirical performance, various existing state-of-the-art GNNs utilize nonlinear activation functions. In this section, we analyze the role of nonlinearity.</p><p>In linear GNNs, the ? frequency component of the prediction Z? is just a function of g(?), X? and W . Yet for nonlinear GNNs, different frequency components may mix. Consider an element-wise activation function ? over the spatial signal X. We investigate its equivalent effect ? over the spectral signal X. Its function on a spectral signal is ? ( X) = U T ?(U X), meaning that different frequency components are first mixed via U , then nonlinearly transformed via ? element-wisely, and finally distributed back to each frequency via U T . Thus, ? is a column-wise nonlinear function over all frequency components. The mixing of different frequency components may alleviate the issues from multiple eigenvalues and missing frequency components. Figure <ref type="figure">4</ref> is an example. However, such a mix is not expressive enough to solve all the problems, as 1-WL still bounds the expressive power of GNNs. Furthermore, since the universality conditions are easily satisfied by real-world graphs to a large degree (Appendix E), we desert nonlinearity in our experiments.</p><formula xml:id="formula_7">ReLU ?? ? ?? ?? 0 2 0.5 1.5 2 1 0 0 1 0 ?? ? ?? ?? 0 2 0.5 1.5 2 1 -1 -2 1 -1</formula><p>Figure <ref type="figure">4</ref>. Nonlinear functions can mix different frequency components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Choice of Basis for Polynomial Filters</head><p>Assume the polynomial basis is g k (?), k = 1, 2, ... In this section, we discuss linear GNNs with individual filter parameters for each output dimension, which is formulated as</p><formula xml:id="formula_8">Z :l = K-1 k=0 ? kl g k ( L)XW :l ,<label>(4)</label></formula><p>where ? kl is the coefficient of polynomial filter basis g k ( L) and XW :l is the linear combination of node features for the l th output dimension Z :l .</p><p>All complete polynomial bases can build PFME models. However, models with different bases show different empirical performance. This section analyzes the effect of polynomial basis from an optimization perspective, which motivates the use of Jacobi Polynomials in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Hessian Matrix and Polynomial Basis</head><p>Following the setting in <ref type="bibr" target="#b35">(Xu et al., 2021)</ref>, we study linear GNNs trained with squared loss</p><formula xml:id="formula_9">R = 1 2 ||Z -Y || 2 F</formula><p>, where Y is. We first prove that linear GNN converges to the global minimum under mild conditions in Appendix H. Therefore, linear GNN has a good convergence guarantee and we can study its convergence speed near the global minimum. When considering the optimization of linear GNN, both ? and W are learnable parameters. However, the gradient of loss over W is a function of the learnable filter function g :l := k ? kl g k ( L) as a whole:</p><formula xml:id="formula_10">?R ?W jl = g :l ( L)XW :l -Y :l T g :l ( L)X :j ,<label>(5)</label></formula><p>The learned filter function is approximately the same for different bases as they have the same expressive power and can all converge to the global minimum. So the optimization of W is roughly irrelevant to the choice of basis near the global minimum. However, the optimization of ? is heavily dependent on the basis choice. So here we only analyze the optimization of ? by merging W into X.</p><p>Consider the optimization w.r.t. ?. The loss is a convex function, and the convergence rate of steepest descent is dependent on the condition number of the Hessian matrix <ref type="bibr" target="#b23">(Nocedal &amp; Wright, 2008)</ref>. Therefore, we analyze the Hessian matrix of linear GNNs near the global minimum.</p><p>Since the total loss is summed over different output dimensions, and each output dimension adopts a different set of polynomial coefficients ? kl , we can analyze the Hessian w.r.t. each dimension independently. Ignoring l, the (k 1 , k 2 ) element of the Hessian matrix H can be written as</p><formula xml:id="formula_11">?R ?? k1 ?? k2 = X T g k2 ( L)g k1 ( L)X = n i=1 g k2 (? i )g k1 (? i ) X2 ?i .<label>(6)</label></formula><p>It can be equivalently expressed as a Riemann sum:</p><formula xml:id="formula_12">n i=1 g k2 (? i )g k1 (? i ) F (? i ) -F (? i-1 ) ? i -? i-1 (? i -? i-1 ),<label>(7)</label></formula><p>where F (?) := ?i?? X2 ?i is the accumulated amplitude of signal with frequency lower than ?. Define f (?) = ?F (?) ?? , which is the density of signal at frequency ?. In the limit when n ? ?, we have:</p><formula xml:id="formula_13">H k1k2 = 2 ?=0 g k1 (?)g k2 (?)f (?)d?. (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>The condition number ?(H) reaches minimum if H is an identity matrix, which is equivalent to that g k 's form an orthonormal basis in the polynomial space whose inner product is defined by h, g = 2 0 h(?)g(?)f (?)d? with f (?) being the weight function.</p><p>Our results show that although all complete polynomial bases have the same expressive power, using orthonormal basis g k whose weight function corresponds to the graph signal density can enable linear GNNs to achieve the highest convergence rate. As the normalization of bases is straightforward, we only consider orthogonality in the analysis.</p><p>Given the weight function f (?), we can construct an orthonormal basis using the Gram-Schmidt process. However, the exact form of the weight function f depends on the eigendecomposition of L and cannot be calculated efficiently and accurately for large graphs. Therefore, we choose a general form of orthogonal polynomials with flexible enough weight functions to adapt to different graph signal distributions f (?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Jacobi Polynomial Bases</head><p>Among orthogonal polynomials, the Jacobi basis has a very general form, where the Chebyshev basis is a special case of it. The Jacobi basis P a,b k has the following form.</p><formula xml:id="formula_15">P a,b 0 (z) = 1, P a,b 1 (z) = a -b 2 + a + b + 2 2 z.<label>(9)</label></formula><p>For k ? 2.</p><formula xml:id="formula_16">P a,b k (z) = (? k z + ? k )P a,b k-1 (z) -? k P a,b k-2 (z),<label>(10)</label></formula><p>where</p><formula xml:id="formula_17">? k = (2k + a + b)(2k + a + b -1) 2k(k + a + b) , ? k = (2k + a + b -1)(a 2 -b 2 ) 2k(k + a + b)(2k + a + b -2) , ? k = (k + a -1)(k + b -1)(2k + a + b) k(k + a + b)(2k + a + b -2) .<label>(11)</label></formula><p>P a,b k , k = 0, 1, 2, ... are orthogonal w.r.t. the weight function (1 -?) a (1 + ?) b on [-1, 1]. Therefore, we can define the Jacobi basis for graphs as g k ( L) = P a,b k (I -L).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">A Discussion on Popular Filter Bases</head><p>In this section, we compare three popular polynomial bases with the Jacobi Polynomial: Monomial (1-?) k , Chebyshev P -1/2,-1/2 k</p><p>(1 -?), and Bernstein K k (1 -? 2 ) K-k ( ? 2 ) k . For the Monomial basis, we can prove that it cannot be orthogonal on any weight function. Proposition 5.1. On any weight function f (?) which fulfills the requirements of inner product, the Monomial basis is not orthogonal.</p><p>Chebyshev basis are a special case of Jacobi basis and are only orthogonal w.r.t. a specific weight function. In contrast, Jacobi basis can adapt to a wide range of weight functions.</p><p>For non-orthogonal bases such as Bernstein, the Hessian matrix might not be diagonal, but a small condition number may still be achieved. In Appendix D, we build a connection between the condition number of polynomial regression's Gram matrix using basis g k , k = 0, 1, 2, .., K and that of linear GNNs' Hessian matrix. Therefore, some existing conclusions from polynomial regression basis choice can still be used. For example, existing study shows that Bernstein basis can also achieve a lower condition number than the Monomial basis <ref type="bibr" target="#b19">(Marco &amp; Martinez, 2010)</ref>. Though both Bernstein and Jacobi bases can outperform Monomial, Jacobi bases can perform better if the data distribution is well approximated by the weight function of Jacobi basis. Our experiments find that Jacobi bases perform better than Bernstein bases on both synthetic and real-world datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">JacobiConv Architecture</head><p>In this section, we describe our JacobiConv architecture.</p><p>There are three techniques used in JacobiConv: multiple filter functions, Jacobi polynomial bases, and a novel polynomial coefficient decomposition (PCD) technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Multiple Filters</head><p>Motivated by our analysis in Section 4.1, we adopt an individual filter function for each output dimension. The JacobiConv can be formulated as</p><formula xml:id="formula_18">Z :l = K k=0 ? kl P a,b k ( L)XW :l .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Computation of Jacobi Bases</head><p>As the dimension of X is often much larger than that of XW , we first linearly transform X and then filter it. With the recursion formula of Jacobi basis, we can compute all basis in O(K) time and do K message passing operations.</p><formula xml:id="formula_19">P a,b 0 ( ?)X = X, P a,b 1 ( ?)X = a -b 2 X + a + b + 2 2 ?X. (<label>13</label></formula><formula xml:id="formula_20">) For k ? 2, P a,b k ( ?)X = ? k ?P a,b k-1 ( ?)X + ? k P a,b k-1 ( ?)X -? k P a,b k-2 ( ?)X.</formula><p>(14)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Polynomial Coefficient Decomposition</head><p>The filter function we construct can be formulated as K-1 k=0 ? kl P a,b k . We find that in real-world datasets ? kl gets smaller as k gets higher. As ? kl 's have different magnitudes, the optimization can be hard. So we decompose ? kl to ? kl k i=1 ? i , where ? i is shared. And we set ? i = ? tanh ? i , which enforces ? i ? [-? , ? ]. We call this technique Polynomial Coefficient Decomposition (PCD). We can modify the recursion formula to implement PCD.</p><formula xml:id="formula_21">P a,b k ( ?)X = ? k ? k ?P a,b k-1 ( ?)X + ? k ? k P a,b k-1 ( ?)X -? k ? k-1 ? k P a,b k-2 ( ?)X. (<label>15</label></formula><formula xml:id="formula_22">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiment</head><p>In this section, we first conduct experiments on synthetic datasets to examine JacobiConv's ability to express filter functions, and then test JacobiConv on real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Evaluating Models on Learning Filters</head><p>Following <ref type="bibr" target="#b13">He et al. (2021)</ref>, we transform real images to 2D regular 4-neighbor grid graphs, whose nodes are pixels. We apply 5 spectral filters (low e -10? 2 , high 1 -e -10? 2 , band e -10(?-1) 2</p><p>, reject 1 -e -10(?-1) 2 , and comb | sin ??|) to the signal in each image. All models use original graph signal as node features to fit the filtered signal.</p><p>We report the average squared error (lower the better) over the 50 pictures. Results are shown in Table <ref type="table" target="#tab_0">1</ref>. For a fair comparison, we remove PCD from linear GNNs.</p><p>We compare JacobiConv with popular PFME GNNs: GPRGNN <ref type="bibr" target="#b8">(Chien et al., 2021)</ref>, ARMA <ref type="bibr" target="#b2">(Bianchi et al., 2021)</ref>, BernNet <ref type="bibr" target="#b13">(He et al., 2021)</ref>, and ChebyNet <ref type="bibr" target="#b9">(Defferrard et al., 2016)</ref>. Settings of these models are detailed in Appendix F. JacobiConv outperforms other models on all datasets and even achieves up to 50 times lower loss on two datasets: Low and Reject. Though all these models can learn arbitrary polynomial filters, JacobiConv has better optimization properties as it uses orthogonal filter bases that can adapt to a wide range of signal distributions.</p><p>We also compare linear GNNs with different bases. The results are shown in the lower part of Table <ref type="table" target="#tab_0">1</ref>. The Jacobi Polynomial basis still outperforms other bases on all datasets and achieves 1000 times lower loss than any other basis. Bernstein basis also achieves lower loss than Monomial on all datasets, which verifies our analysis in Section 5.3.</p><p>Experimental results of models with PCD on synthetic datasets are shown in Appendix G. JacobiConv still outperforms any other model on all datasets. Jacobi bases also achieve a higher convergence rate than other bases for linear GNN. See Appendix I for the convergence rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Evaluation on Real-World Datasets</head><p>For homogeneous graphs, we include three citation graph datasets, Cora, CiteSeer and PubMed <ref type="bibr" target="#b36">(Yang et al., 2016)</ref>, as well as two Amazon co-purchase graphs, Computers and Photo <ref type="bibr" target="#b29">(Shchur et al., 2018)</ref>. We also use heterogeneous graphs including Wikipedia graphs Chameleon and Squirrel <ref type="bibr" target="#b26">(Rozemberczki et al., 2021)</ref>, the Actor co-occurrence graph, and the webpage graph Texas and Cornell from We-bKB3 <ref type="bibr" target="#b25">(Pei et al., 2020)</ref>. Their statistics are listed in Appendix E. We perform the node classification task, where we randomly split the node set into train/validation/test sets with a ratio of 60%/20%/20%. JacobiConv is compared with spectral GNNs: GCN, APPNP, ChebyNet, GPRGNN, JacobiConv outperforms all existing models on 9 out of 10 datasets and achieves performance gains up to 12% on a heterogeneous dataset Squirrel. On the Actor dataset, Jaco-biConv beats all baselines except BernNet. The generally top and runner-up performance of JacobiConv and Bern-Net verify our analysis in Section 5.3. The results indicate that JacobiConv is a general spectral GNN with consistently good performance across datasets. They also show that nonlinearity is not necessary for learning powerful spectral filters given a good choice of polynomial basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Ablation Analysis</head><p>To illustrate the effectiveness of Jacobi bases, we compare JacobiConv with linear GNNs with other filter bases in the left part of Table <ref type="table" target="#tab_2">3</ref>. We also remove PCD from the models to ensure fairness as the coefficient distribution of different bases varies. Jacobi basis outperforms any other basis by more than 0.8% on average. Bernstein basis also outperforms Monomial on average, which is consistent with the results in Table <ref type="table" target="#tab_1">2</ref>.</p><p>In the right part of Table <ref type="table" target="#tab_2">3</ref>, UniFilter is JacobiConv using the same filter for all prediction dimensions. No-PCD is JacobiConv without PCD. The results illustrate that the multiple filter functions, PCD, and the Jacobi polynomial basis are all essential for JacobiConv. On average, the multiple filter technique provides 1.3% performance gain, and the PCD technique provides 0.8% performance gain.</p><p>To analyse how removing nonlinearity affects performance, we design two variants: NL and NL-Res. NL replaces the linear transformation in JacobiConv with a 2-layer ReLU MLP, whose first-layer output has the same dimension as the model output dimension. Compared with NL, NL-Res uses residual connection which adds the output of the first linear layer to the output of the MLP. NL-Res outperforms NL by 2% on average while NL leads to 6% performance loss compared with JacobiConv. These results illustrate that linear GNN is expressive enough and nonlinear transformations can hardly promote the expressive power. The better performance of NL-Res over NL might also be due to its closer relationship to linear GNNs. On the other hand, the lower performance after adding nonlinearity may be attributed to overfitting caused by extra parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Scalability</head><p>As shown in Table <ref type="table" target="#tab_3">4</ref>, compared with other baselines with comparable depth, our model on average only uses 10% parameters, as it only uses a linear layer to convert node features to the output shape, while other models use MLPs. JacobiConv also has similar computational overhead to other baselines, though taking slightly more time than APPNP and GPRGNN due to more complex bases. Theoretically, it still has the same time complexity O(dmf ) as APPNP 2K/6.6/3.3 32K/3.9/1.3 32K/11.1/4.9 32K/4.5/1.8 COMPUTERS 8K/7.0/4.7 50K/6.0/2.5 50K/29.3/8.6 50K/6.5/1.6 PHOTOS 6K/9.2/4.0 48K/5.8/2.8 48K/15.3/6.2 48K/8.0/2.0 CHAMELEON 12K/6.5/3.1 149K/3.9/0.8 149K/11.0/2.8 149K/4.4/1.0 ACTOR 5K/6.5/2.3 60K/3.8/0.8 60K/10.9/3.5 60K/4.3/0.9 SQUIRREL 11K/6.3/3.3 134K/4.3/0.9 134K/15.7/4.9 134K/4.3/2.1 TEXAS 9K/6.6/2.2 109K/3.8/0.8 109K/11.3/2.4 109K/4.3/1.0 CORNELL 9K/6.5/2.7 109K/3.8/0.8 109K/11.0/2.3 109K/4.4/0.9 and GPRGNN, where d is the order of the polynomial, m is the number of edge in the graph, and f is the number of node feature dimensions, while BernNet's time complexity is O(d 2 mf ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we analyze the expressive power of spectral GNNs. We prove that even without nonlinearity spectral GNNs can be universal under mild conditions. We further analyze the optimization of spectral GNNs, which motivates the proposed JacobiConv, a novel spectral GNN using Jacobi basis. JacobiConv outperforms the previous state-of-theart method BernNet by up to 12% on real-world datasets without using nonlinearity, which verifies our theory.  <ref type="bibr" target="#b32">(Wu et al., 2019)</ref> (1 -?) K ?, K ? APPNP <ref type="bibr">(Klicpera et al., 2019a</ref>) <ref type="bibr" target="#b40">(Zhu et al., 2021)</ref> 1</p><formula xml:id="formula_23">K k=0 ? k 1-? (1 -?) k ?, K ? GNN-LF</formula><formula xml:id="formula_24">-(1-?)(1-?) 1-(2-?+ 1 ? )(1-?)</formula><p>?, ? ? GNN-HF <ref type="bibr" target="#b40">(Zhu et al., 2021)</ref> 1+?</p><formula xml:id="formula_25">(1-?) 1-(1-?-1 ? )(1-?)</formula><p>a, b ? CHEBYNET <ref type="bibr" target="#b9">(Defferrard et al., 2016</ref>)</p><formula xml:id="formula_26">K k=0 ? k cos(k arccos(1 -?)) K ? k , K ? GPRGNN (Chien et al., 2021) K k=0 ? k (1 -?) k K ? k ? ARMA (Bianchi et al., 2021) K k=0 b k 1-a k (1-?) K a k , b k ? BERNNET (He et al., 2021) K k=0 ? k K k (1 -? 2 ) K-k ( ? 2 ) k K ? k ? JACOBICONV (OUR MODEL) K k=0 ? k k s=0 (k+a)!(k+b)!(-?) k-s (2-?) s 2 k s!(k+a-s)!(b+s)!(k-s)! K, a, b ? k ? B. Proofs B.1. Proof of Theorem 4.1</formula><p>We restate Theorem 4.1 as follows.</p><p>Theorem B.1. Assuming all rows of X are not zero vector, and no eigenvalue of L has multiplicity larger than 1, for all Z ? R n?1 , there exists a linear GNN to produce it.</p><p>Proof. First, we prove that there exists W * ? R d so that all elements of XW * are not zero.</p><p>Consider the i th row of XW equals 0. In other words, Xi W = 0. Let the solution space of W be V</p><formula xml:id="formula_27">i . As Xi = 0, V i is a proper subspace of R d . Therefore, R d - n i=1 V i = ?. All vectors W in R d - n i=1 V i = ? can</formula><p>meet the requirements, Then we filter XW * to produce the output. For all one-dimension prediction Z ? R n , Z = U T Z ? R n . If there exists a polynomial that g * (? i ) = R i , where R is a vector whose i th row R i = Zi ( XW )i , for i ? {1, 2, ..., n} , linear GNNs can produce Z.</p><p>As ? i are different from each other, consider an n -1 degree polynomial, g(?</p><formula xml:id="formula_28">i ) = n-1 k=0 ? k ? k i . The coefficient ? k of g * is the solution of the linear system B? = R, where B ? R n?n and B ij = ? j-1 i , ? ? R n and ? k = ? k-1 , R ? R n ,</formula><p>gives the coeffcient of g. As B T is a Vandermonde matrix and becomes nonsingular if eigenvalues are different from each other, a solution always exists. Therefore, linear GNNs can give arbitrary one-dimensional prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Proof of Proposition 4.2</head><p>Assuming an output Z ? R n?k , k &gt; 1, that linear GNNs can express it is equivalent to that the equation Z = g( L)XW has solution polynomial g and matrix W . The equation is equivalent to Z = g(?) XW . Let Xsi , i = 1, 2, ..., rank( X) be a maximal linearly independent subset of the set of row vectors in X. We prove that linear GNNs cannot produce the prediction described in the following lemma.</p><p>Lemma B.2. Assuming that all the elements of the s th i row of Z are the same scalar Zsi ? R -{0}, i = 1, 2, ..., n and there exists Zij1 = Zij2 , where i ? {1, 2, ..., n} -{s i |i = 1, 2, ..., rank(X)}, j 1 , j 2 ? {1, 2, ..., k}, j 1 = j 2 , no linear GNN can produce U Z.</p><p>Proof. X = U T X, where U is an orthogonal matrix. Therefore, rank( X) = rank(X) &lt; n.</p><p>Let I denote the set {s i |i = 1, 2, ..., rank(X)}. As XsI forms a maximal linearly independent row vectors of X. Therefore, there exists M ? R n?rank(X) , X = M XI . Only consider the rows in I of the equation.</p><formula xml:id="formula_29">ZI = g(?) II XI W.<label>(16)</label></formula><p>As all elements in ZI = 0, all diagonal elements of g(?) II = 0. Therefore,</p><formula xml:id="formula_30">g(?) -1 II ZI = XI W.<label>(17)</label></formula><p>Therefore, all column vectors of Z should be equal, because</p><formula xml:id="formula_31">Z = g(?) XW = g(?)M XI W = (g(?)M g(?) -1 I ) ZI .<label>(18)</label></formula><p>As all column vectors of ZI are equal, column vectors of Z are all the same, while we assume that there exists i ? {1, 2, ..., n} -{s i |i = 1, 2, ..., rank(X)}, j 1 , j 2 ? {1, 2, ..., n}, j 1 = j 2 that Zij1 = Zij2 . Therefore, such linear GNNs do not exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Proof of Proposition 4.3 and Corollary 4.4</head><p>Proof. When the filter function is a K-degree polynomial, the prediction of the linear GNN can be formulated as follows.</p><formula xml:id="formula_32">Z = K k=0 ? k ?k (XW ).<label>(19)</label></formula><p>Using the framework in <ref type="bibr" target="#b34">(Xu et al., 2019)</ref>, it can be considered as a K + 1-layer GNN. Let h (k) i denote the embeddings of node i at the k th layer. COMBINE (k) , AGGREGATE (k) are functions defined as follows.</p><formula xml:id="formula_33">a (1) i = AGGREGATE (1) ({h (k-1) j |j ? N (i)}) = |{h (k-1) j |j ? N (i)}| = D ii COMBINE (1) (a (1) i , X i ) = (D ii , ? K X i , X i ),<label>(20)</label></formula><p>where COMBINE (1) produce a tuple containing three items. For k = 2, ..., K, a</p><formula xml:id="formula_34">(k) i = AGGREGATE (k) ({(D jj , h (k-1) j , X i )|j ? N (i)}) = j?N (i) 1 D jj h (k-1) j COMBINE (k) (a (k) , (D ii , h (k-1) j )) = (D ii , 1 ? D ii a (k) i + ? K+1-k X i , X i ).<label>(21)</label></formula><p>For</p><formula xml:id="formula_35">k = K + 1, a (k) i = AGGREGATE (k) ({(D jj , h (k-1) j , X i )|j ? N (i)}) = j?N (i) 1 D jj h (k-1) j COMBINE (k) (a (k) , (D ii , h (k-1) j )) = 1 ? D ii a (k) i + ? K+1-k X i .<label>(22)</label></formula><p>Therefore, the output of the last layer in GNN produce the output of linear GNNs. According to the proof of Lemma 2 in <ref type="bibr" target="#b34">Xu et al. (2019)</ref>, if WL node labels W L k (v) = W L k (u), we always have GNN node features h</p><formula xml:id="formula_36">(k) i = h (k)</formula><p>j for any iteration i. Therefore, for all nodes i, j ? V, LG K (i) = LG K (j) if W L K+1 (i) = W L K+1 (j).</p><p>The proof of Corollary 4.4 is obvious. For any pair of non-isomorphic nodes in the graph, linear GNNs can produce different outputs for the two nodes, so 1-WL can also differentiate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Proof of Theorem 4.5</head><p>Assuming ? is a permutation function and P is a permutation matrix, ? ?(a),a , the graph is isomorphic under the permutation ?.</p><p>L = P T LP U ?U T = P U ?U T P T ? = U T P U ?U T P T U Results are shown in Table <ref type="table" target="#tab_5">8</ref>. JacobiConv still outperforms all other bases. In general, there is little performance difference between models with PCD and those without. The reason for the invalidation of PCD can be that linear GNN trained with the squared loss on synthetic datasets can converge to a global minimum, and the effect of PCD to help convergence may be unimportant.</p><formula xml:id="formula_37">? = V ?V T ,<label>(23)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Analysis Using Gradient Flow</head><p>Let I denote the training set containing nodes. The prediction of a linear GNN is,    </p><formula xml:id="formula_38">Z il = K k=0 d j=1 ? kl G k ij W jl .<label>(</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>?? :l and S (l) = ?Z Il ?W :l T ?Z Il ?W :l .Both M (l) and S (l) are symmetric semi-definite matrix. Assuming the minimum eigenvalue of M (l) + S (l) is ? l ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Training curve on some datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Average of sum of squared loss over 50 images.</figDesc><table><row><cell></cell><cell>LOW</cell><cell>HIGH</cell><cell>BAND REJECT COMB</cell></row><row><cell>GPRGNN</cell><cell cols="3">0.4169 0.0943 3.5121 3.7917 4.6549</cell></row><row><cell>ARMA</cell><cell cols="3">1.8478 1.8632 7.6922 8.2732 15.1214</cell></row><row><cell>CHEBYNET</cell><cell cols="3">0.8220 0.7867 2.2722 2.5296 4.0735</cell></row><row><cell>BERNNET</cell><cell cols="3">0.0314 0.0113 0.0411 0.9313 0.9982</cell></row><row><cell cols="4">JACOBICONV 0.0003 0.0064 0.0213 0.0156 0.2933</cell></row><row><cell cols="4">MONOMIAL 2.4076 4.2411 10.8856 8.7031 10.5596</cell></row><row><cell cols="4">CHEBYSHEV 0.9227 2.3198 7.7751 6.0065 9.1191</cell></row><row><cell>BERNSTEIN</cell><cell cols="3">0.8089 0.0104 1.0133 1.5551 1.8633</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on real-world datasets: Mean accuracy (%) ? 95% confidence interval.</figDesc><table><row><cell>DATASETS</cell><cell>GCN</cell><cell>APPNP CHEBYNET GPRGNN BERNNET JACOBICONV</cell></row><row><cell>CORA</cell><cell cols="2">87.14?1.01 88.14?0.73 86.67?0.82 88.57?0.69 88.52?0.95 88.98?0.46</cell></row><row><cell>CITESEER</cell><cell cols="2">79.86?0.67 80.47?0.74 79.11?0.75 80.12?0.83 80.09?0.79 80.78?0.79</cell></row><row><cell>PUBMED</cell><cell cols="2">86.74?0.27 88.12?0.31 87.95?0.28 88.46?0.33 88.48?0.41 89.62?0.41</cell></row><row><cell cols="3">COMPUTERS 83.32?0.33 85.32?0.37 87.54?0.43 86.85?0.25 87.64?0.44 90.39?0.29</cell></row><row><cell>PHOTO</cell><cell cols="2">88.26?0.73 88.51?0.31 93.77?0.32 93.85?0.28 93.63?0.35 95.43?0.23</cell></row><row><cell cols="3">CHAMELEON 59.61?2.21 51.84?1.82 59.28?1.25 67.28?1.09 68.29?1.58 74.20?1.03</cell></row><row><cell>ACTOR</cell><cell cols="2">33.23?1.16 39.66?0.55 37.61?0.89 39.92?0.67 41.79?1.01 41.17?0.64</cell></row><row><cell>SQUIRREL</cell><cell cols="2">46.78?0.87 34.71?0.57 40.55?0.42 50.15?1.92 51.35?0.73 57.38?1.25</cell></row><row><cell>TEXAS</cell><cell cols="2">77.38?3.28 90.98?1.64 86.22?2.45 92.95?1.31 93.12?0.65 93.44?2.13</cell></row><row><cell>CORNELL</cell><cell cols="2">65.90?4.43 91.81?1.96 83.93?2.13 91.37?1.81 92.13?1.64 92.95?2.46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results of ablation study on real-world datasets: Mean accuracy (%) ? 95% confidence interval. 06?0.24 89.16?0.47 87.09?0.38 89.22?0.39 90.39?0.29 90.45?0.34 89.22?0.42 87.45?2.15 86.85?2.67 PHOTO 95.33?0.25 95.45?0.27 94.59?0.26 95.53?0.27 95.43?0.23 95.26?0.31 95.53?0.19 94.16?0.78 85.65?8.25 CHAMELEON 65.95?1.20 74.09?0.85 69.58?0.88 72.95?0.83 74.20?1.03 73.76?1.03 72.95?0.83 72.63?0.99 72.56?1.01 ACTOR 40.31?0.82 40.61?0.64 40.16?0.39 40.70?0.98 41.17?0.64 40.01?0.96 40.70?0.98 37.80?1.32 37.56?0.88 SQUIRREL 37.93?0.62 56.71?0.89 42.52?1.19 55.77?0.55 57.38?1.25 54.11?0.82 55.77?0.55 48.66?6.65 43.73?6.94 TEXAS 91.64?2.46 88.36?3.93 89.34?2.46 92.79?1.97 93.44?2.13 90.82?2.30 92.79?1.97 89.84?3.28 89.34?3.12 CORNELL 91.31?2.13 88.03?3.28 92.46?2.63 92.30?2.79 92.95?2.46 92.62?2.46 92.30?2.62 89.67?2.30 87.54?3.11 and BernNet. Note that all these baselines use nonlinear transformations, while JacobiConv is a purely linear model. Results are shown in Table 2. Settings of these models are detailed in Appendix F.</figDesc><table><row><cell>DATASETS</cell><cell>MONOMIAL CHEBYSHEV BERNSTEIN</cell><cell>JACOBI</cell><cell cols="2">JACOBICONV UNIFILTER NO-PCD</cell><cell>NL-RES</cell><cell>NL</cell></row><row><cell>CORA</cell><cell cols="6">88.80?0.67 88.49?0.82 87.22?1.26 88.98?0.72 88.98?0.46 89.05?0.48 88.98?0.72 89.00?0.61 88.67?0.69</cell></row><row><cell>CITESEER</cell><cell cols="6">80.68?0.86 80.03?0.87 80.61?0.85 80.61?0.72 80.78?0.79 80.42?0.98 80.61?0.71 80.16?0.86 80.25?0.60</cell></row><row><cell>PUBMED</cell><cell cols="3">89.54?0.36 89.52?0.46 88.42?0.32 89.70?0.34 89.62?0.41</cell><cell cols="3">89.58?0.25 89.70?0.34 86.44?2.05 87.73?2.13</cell></row><row><cell cols="2">COMPUTERS 89.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>#parameters/per-epoch time (ms)/total training time (s).</figDesc><table><row><cell cols="2">DATASETS JACOBICONV</cell><cell>APPNP</cell><cell>BERNNET</cell><cell>GPRGNN</cell></row><row><cell>CORA</cell><cell cols="4">10K/6.4/2.7 92K/3.6/1.2 92K /10.8/2.9 92K/4.3/0.9</cell></row><row><cell cols="5">CITESEER 22K/6.3/2.3 237K/3.7/1.3 237K/11.4/3.3 237K/4.5/1.0</cell></row><row><cell>PUBMED</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The filter form of spectral GNNs.</figDesc><table><row><cell>MODEL</cell><cell>g</cell><cell>HYPERPARAMS LEARNABLE PFME</cell></row><row><cell>SGC</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Average of sum of square loss over 50 images. JACOBICONV 0.0003 0.0077 0.0253 0.0157 0.2972 MONOMIAL 2.3257 4.0960 10.9556 8.8068 10.8799 CHEBYSHEV 1.0037 2.3468 7.8522 6.0297 9.1175 BERNSTEIN 0.8089 0.0104 1.0133 1.5551 1.8633 G. Synthetic Dataset Results of Models with PCD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>37) Assume that d dt W jl = -?L ?W jl , d dt ? kl = -?L ?? kl .</figDesc><table><row><cell>Therefore,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>d dt</cell><cell cols="2">L =</cell><cell cols="3">all elements</cell><cell cols="2">dL dZ II</cell><cell cols="3">dZ II dt</cell><cell></cell><cell>= -</cell><cell cols="3">i?I</cell><cell cols="3">d l=1</cell><cell>?L ?Z il</cell><cell>(</cell><cell>K k=0</cell><cell>?Z il ?? kl</cell><cell>?L ?? kl</cell><cell>+</cell><cell>d j=1</cell><cell>?Z il ?W jl</cell><cell>?L ?W jl</cell><cell>).</cell><cell>(38)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">?L ?? kl</cell><cell cols="2">=</cell><cell cols="2">i?I</cell><cell cols="2">?L ?Z il</cell><cell cols="4">?Z il ?? kl</cell><cell cols="2">=</cell><cell>i?I</cell><cell>?L ?Z il</cell><cell>d j=1</cell><cell>G k ij W jl .</cell><cell>(39)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">?L ?W jl</cell><cell cols="2">=</cell><cell cols="2">i?I</cell><cell cols="2">?L ?Z il</cell><cell></cell><cell cols="3">?Z il ?W jl</cell><cell></cell><cell>=</cell><cell>i?I</cell><cell>?L ?Z il</cell><cell>K k=0</cell><cell>? kl G k ij .</cell><cell>(40)</cell></row><row><cell></cell><cell>d dt</cell><cell cols="2">L =</cell><cell cols="2">dL dZ II</cell><cell cols="2">dZ II dt</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">= -</cell><cell>i?I</cell><cell>d l=1</cell><cell cols="2">?L ?Z il</cell><cell cols="3">( k=0 K</cell><cell cols="5">?Z il ?? kl i ?I</cell><cell cols="3">?L ?Z i l</cell><cell>?Z i l ?? kl</cell><cell>+</cell><cell>d j=1</cell><cell>?Z il ?W jl i</cell><cell>?L ?Z i l</cell><cell>?Z i l ?W jl</cell><cell>)</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">= -</cell><cell cols="3">d l=1 i?I,i ?I</cell><cell cols="3">?L ?Z ik</cell><cell cols="2">?L ?Z i k</cell><cell cols="2">(</cell><cell cols="2">K k=0</cell><cell cols="3">?Z il ?? kl</cell><cell>?Z i l ?? kl</cell><cell>+</cell><cell>d j=1</cell><cell>?Z il ?W jl</cell><cell>?Z i l ?W jl</cell><cell>)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>d</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">= -</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">l=1 n?I,n ?I</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Comparison between FullCoef and JacobiConv. CORA CITESEER PUBMED COMPUTERS PHOTO FULLCOEF 87.37?1.20 80.95?1.06 89.37?0.64 89.94?0.46 94.93?0.50 JACOBICONV 88.98?0.46 80.78?0.79 89.62?0.41 89.96?0.29 95.43?0.23 CHAMELEON ACTOR SQUIRREL TEXAS CORNELL FULLCOEF 64.79?1.42 38.99?1.37 49.61?1.22 92.30?4.52 91.48?5.62 JACOBICONV 74.20?1.03 41.17?0.64 57.38?1.25 93.44?2.13 92.95?2.46</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Institute for Artificial Intelligence, Peking University</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>BeijingInstitute for General Artificial Intelligence. Correspondence to: Muhan Zhang &lt;muhan@pku.edu.cn&gt;.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where V is an orthogonal matrix. As all diagonal elements of ? are different, the eigenspace corresponding to each eigenvalue has only one dimension. Therefore,</p><p>where D is a diagonal matrix whose diagonal elements are ?1. Therefore,</p><p>Therefore, P is symmetric, in other words, for i ? {1, 2, ..., n}, ?(?(i)) = i. Therefore, for any graph without multiple normalized Laplacian eigenvalue, the order of permuatations is 1 or 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Proof of Theorem 4.6</head><p>For all permutation ? and its matrix P for graph.</p><p>If ? does not have multiple eigenvalues, V = D, D is a diagonal matrix whose diagonal elements are ?1. So (I -D) X = 0.</p><p>Assuming all rows of X are not zero vector (no missing frequency component), I -D = 0, D = I.</p><p>Therefore, P = U DU T = I. Therefore, all pair of nodes in this graph are not isomorphic when considering node features.</p><p>B.6. Proof of Proposition 5.1</p><p>Orthogonality require x, x = 0 while 1, x 2 = 0. However,</p><p>B.7. Proof of Proposition C.1</p><p>As X = U T X, the distribution density f 1 of X has a simple relation with the distribution density function f 2 of X,</p><p>We can extend this proposition to the multi-dimensional cases. Consider</p><p>We use a lemma from <ref type="bibr" target="#b10">(Feng &amp; Zhang, 2007)</ref>.</p><p>Lemma B.3. Let F (x 1 , ..., x m ) be a polynomial of variables x 1 , ..., x m with real coefficients, then, ? m D = 0, where D = {x|F (x) = 0, x = (x 1 , ..., x m ) T ? R m } and ? m D is the Lebesgue measure of D as the set of points in R m .</p><p>As we use individual filter parameters for each output dimension, in other words, Z :l = K k=0 d j=1 ? kl G k :j W jl is just a function of ? kl W jl , if we can produce arbitrary one-dimensional prediction, muli-dimensional prediction can also be produced. So we can assume Z ? R n?1 . Consider the linear GNNs in the frequency domain. Z = g(?) XW.</p><p>(30)</p><p>Assuming that multiple eigenvalues are in the i 1 , i 2 , ... th rows of ?. Let I be i 1 , i 2 , ..., |I| = s i . Assume that all diagonal elements in g(?) I,I are not zero. ZI = g(?) I,I XI W.</p><p>As all elements in X independently follows N (0, ? 2 ), the probability that XI degenerates is,</p><p>The probability of some rows of</p><p>Assume that all rows of X[n]-I W are not zero. As all elements of ? [n]-I,[n]-I are different, we can let g(?) ij = Zij /( Xij W ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.9. Proof of Proposition C.3</head><p>The number of different eigenvalues is O(n). Let I = {i 1 , i 2 , ...} be the set of the index of different eigenvalues, and ? i1 &lt; ? i2 &lt; ....</p><p>Consider the signal x in the frequency domain. x ? N (0, ? 2 I). For any pair of adjacent elements in xI , xij and xij+1 , the probability that two nodes have different signs is 1 2 . There, O(n) pairs of xij and xij+1 have different signs. For these pairs, after filtering, zij = g(? ij )x ij , zij+1 = g(? ij+1 )x ij+1 . There are three cases.</p><p>? zij = 0 or zij+1 = 0. A zero-point exist for g.</p><p>? zij and zij+1 have the same signs. A zero-point exist for g in (? ij , ? ij+1 ).</p><p>? zij and zij+1 have different signs.</p><p>Therefore, the number of zero points of g is the number of case 1 add that of case 2 minus the count of case 3,</p><p>Therefore, the degree of polynomial is O(n) in expectation.</p><p>C. Random Feature. Why? Why not?</p><p>Next, we study ways to break the no-missing-frequency condition in Theorem 4.1 in order to increase linear GNNs' expressive power. First, we prove that no frequency component is missing from the random feature.</p><p>Proposition C.1. Assume vector x ? N n (0, ? 2 I), where N n is the Gaussian distribution of n variables. The graph Fourier transformation of x is x ? N n (0, ? 2 I).</p><p>The proposition is proved in Appendix B.7.</p><p>We call x in Proposition C.1 random features. Therefore, the probability of some frequency components missing from the random feature is 0. If we concatenate random features to the node features, no frequency components will be missing from the node features. Moreover, random features can also help with the multiple eigenvalue problem.</p><p>Theorem C.2. Assuming that the number of multiple eigenvalues is m, and among them, the ith multiple eigenvalue has multiplicity s i . With ( m i=1 s i )-dimensional ? N n (0, ? 2 I) random node features, linear GNNs can produce arbitrary prediction with probability 1.</p><p>The proof can be found in Appendix B.8.</p><p>We have seen the power of random features for improving the expressive power of linear GNNs. However, on large graphs, this technique can worsen the performance of models. As the coefficient of components of node feature vibrates frequently, the filter function may be very complex even if we fit simple graph signals. Therefore, as formalized in Proposition C.3, O(n)-degree polynomial is needed, which is impossible to implement for large graphs.</p><p>Proposition C.3. If L has no multiple eigenvalue, O(n) degree polynomial is needed for linear GNN using Gaussian random features to predict a one-dimensional non-zero target whose coeffcients of frequency components can be expressed as a O(1)-degree polynomial.</p><p>The proof of Proposition C.3 can be found in Appendix B.9. O(n)-degree polynomial is too time-and memory-consuming for real-world datasets. In practice, we can only afford constant-degree polynomials (such as degree 10 in our experiments), which explains why random features worsen the performance in most times.</p><p>To verify our analysis, we compare JacobiConv (our proposed model) with random feature (Random), JacobiConv with learnable random feature (Learnable), and the original JacobiConv in Table <ref type="table">6</ref>. Random feature significantly worsens the performance while learnable random feature performs much better. Much to our surprise, Learnable even beats JacobiConv on two datasets, which indicates that node features may have little useful information in some datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Connection between Linear GNN and Polynomial Regression.</head><p>Let F (x) = 1 F (2) F (x). Take n independent random variables x 1 , x 2 , ..., x n from the F distribution and set these variables as the points of linear regression. The element of the Gram matrix G of the polynomial regression using g k , k = 0, 2, ..., K basis is,</p><p>By the weak law of large numbers of probability theory,</p><p>Scalar F (2) 2 will not affect condition number. Therefore, n ? ?,</p><p>, the condition number of Gram matrix of polynomial regression using bases g k , k = 0, 1, 2, .., K with points sampled from F distribution, equals to, ?(H), the condition number of linear GNNs' Hessian matrix. Therefore, we can use some conclusions on polynomial regression's bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Datasets</head><p>We summarize the statistics of these datasets in Table <ref type="table">7</ref>. Baselines. We directly use the results reported in <ref type="bibr" target="#b13">(He et al., 2021)</ref>. JacobiConv and linear GNN with other bases have less parameters than baselines, as linear GNN have a fixed number of parameters given the node feature dimension and output dimension.</p><p>Model hyperparameter for Synthetic Datasets. We use optuna to perform random searches. Hyperparameters were selected to minimize loss on the validation sets. The best hyperparameters selected for each model can be found in our code in the supplement materials. For linear GNNs, we use different learning rate and weight decay for the linear layer W , parameters of PCD ?, and the linear combination parameters ?. We select learning rate from {0.0005, 0.001, 0.005, 0.01, 0.05}, weight decay from {0.0, 5e -5, 1e -4, 5e -4, 1e -3}. We select PCD's ? from {0.5, 1.0, 1.5, 2.0}. Jacobi Basis' a and b are selected from [-1.0, 2.0].</p><p>Model hyperparameter for real-world datasets. Hyperparameters were selected to optimize accuracy scores on the validation sets. We use different dropout for X and XW . Both dropout probabilities are selected from [0.0, 0.9]. Other parameters are searched in the same way as synthetic datasets.</p><p>Training process. We utilize Adam optimizer to optimize models and set an upper bound (1000) for the number of forward and backward processes. </p><p>where ? min = min l ? l . Let L * denote the minimum loss. L * &gt; 0 and dL * dt = 0.</p><p>Assuming L t denotes the loss at time t,</p><p>Therefore, we have a prior guarantee of linear convergence to a global minimum for any graph with (g i ( L)XX T g i ( L)) II full rank. For any desired &gt; 0, we have that L 0 -L * &lt; for any T such that</p><p>Take a closer look at M (l) and S (l) ,</p><p>where g l is the learned filter function for the lth output dimension. As long as row vectors in (f * l ( L)X) I , the filtered graph signal on nodes in the training set, are linearly independent, ? min &gt; 0.</p><p>where H is a symmetric matrix, whose element H ab is i g i (? a )g i (? b ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. The Optimization of linear GNN with Different Polynomial Basis</head><p>In this section, we show the how loss drops with different polynomial filter basis for linear GNNs in Figure <ref type="figure">5</ref>.</p><p>On all five datasets, Jacobi polynomial basis achieves the lowest loss, and it also achieve higher convergence rate than Monomial and Chebyshev basis. However, Bernstein polynomial basis shows high optimization rate at in a few first epoches, which may attribute to that the parameter is far from local minimum initially and our approximation fails, while after a few epoches, jacobi Polynomial basis approaches local minimum and show higher convergence rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. FullCoef vs JacobiConv</head><p>JactobiConv first linear tranforms node features and then filter the signal in each output dimension individually. While some models like ChebyConv <ref type="bibr" target="#b9">(Defferrard et al., 2016)</ref> use individual filter function for each input dimension-output dimension pair to filter the signal in the input dimension and accumulate the filtered signal in the output dimension, which we call FullCoef. Though FullCoef may boost expressive power, extra parameters can also worsen the generalization. We compare FullCoef JacobiConv and original JacobiConv in Table <ref type="table">9</ref>. Results show that JacobiConv outperforms FullCoef on 9 out of 10 datasets. The extra express power FullCoef brings is minor.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The surprising power of graph neural networks with random node initialization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><forename type="middle">?</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2112" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing the expressive power of graph neural networks in a spectral perspective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Renton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ga?z?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Graph neural networks with convolutional ARMA filters</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Fastgcn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On graph neural networks versus graph-augmented mlps</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable graph neural networks via bidirectional propagation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15868" to="15876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Can graph neural networks count substructures?</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The rank of a random matrix</title>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics and Computation</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="689" to="694" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6533" to="6542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalization and representational limits of graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3419" to="3430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bernnet: Learning arbitrary graph spectral filters via bernstein approximation</title>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Universal invariant and equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyr?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7090" to="7099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei? Enberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distance encoding: Design provably more powerful neural networks for graph representation learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">What graph neural networks cannot learn: depth vs width</title>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations. OpenReview.net</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Polynomial least squares fitting in the bernstein basis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">433</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1254" to="1264" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2153" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the universality of invariant networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Segol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4363" to="4371" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Complex Networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Approximation ratios of graph neural networks for combinatorial problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4083" to="4092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Random features strengthen graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 2021 SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="333" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno>CoRR, abs/1811.05868</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The reduction of a graph to canonical form and the algebra which appears therein</title>
		<author>
			<persName><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NTI, Series</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12" to="16" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Optimization of graph neural networks: Implicit acceleration by skip connections and more depth</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="11592" to="11602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning</title>
		<meeting>the 33nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7370" to="7377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Interpreting and unifying graph neural networks with an optimization framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1215" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
