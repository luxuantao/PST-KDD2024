<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from Sibling Mentions with Scalable Graph Inference in Fine-Grained Entity Typing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Joint Lab of HITSZ and China Merchants Securities</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiayang</forename><surname>Cheng</surname></persName>
							<email>jchengaj@cse.ust.hk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haisong</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
							<email>xuruifeng@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from Sibling Mentions with Scalable Graph Inference in Fine-Grained Entity Typing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we firstly empirically find that existing models struggle to handle hard mentions due to their insufficient contexts, which consequently limits their overall typing performance. To this end, we propose to exploit sibling mentions for enhancing the mention representations. Specifically, we present two different metrics for sibling selection and employ an attentive graph neural network to aggregate information from sibling mentions. The proposed graph model is scalable in that unseen test mentions are allowed to be added as new nodes for inference. Exhaustive experiments demonstrate the effectiveness of our sibling learning strategy, where our model outperforms ten strong baselines. Moreover, our experiments indeed prove the superiority of sibling mentions in helping clarify the types for hard mentions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-Grained Entity Typing (FGET) aims to assign one or more fine-grained types to an entity mention given its context. For instance, the mention Steve Jobs should be classified as Person and Entrepreneur under the context "Steve Jobs cofounded Apple ...". Many tasks have witnessed the importance of FGET, such as relation extraction <ref type="bibr" target="#b12">(Jiang et al., 2020b;</ref><ref type="bibr" target="#b7">Chu et al., 2020;</ref><ref type="bibr" target="#b11">Jiang et al., 2020a;</ref><ref type="bibr" target="#b5">Cheng et al., 2021)</ref>, entity linking <ref type="bibr" target="#b26">(Onoe and Durrett, 2020)</ref>, and other tasks <ref type="bibr" target="#b13">(Jiang et al., 2020c;</ref><ref type="bibr" target="#b40">Zhang et al., 2020b;</ref><ref type="bibr" target="#b23">Liu et al., 2021b)</ref>.</p><p>It is challenging to learn effective representations for contextualized mentions<ref type="foot" target="#foot_0">1</ref> in many information extraction tasks <ref type="bibr" target="#b9">(Gao et al., 2022)</ref>, espe-cially in FGET, since the representations are required to well distinguish fine-grained types with similar but different semantics. Noticeable efforts have been made to learn type-aware representations for mentions <ref type="bibr" target="#b29">(Ren et al., 2016;</ref><ref type="bibr" target="#b35">Xin et al., 2018;</ref><ref type="bibr" target="#b6">Choi et al., 2018;</ref><ref type="bibr" target="#b42">Zhang et al., 2018;</ref><ref type="bibr" target="#b20">Lin and Ji, 2019;</ref><ref type="bibr" target="#b0">Abhishek et al., 2017;</ref><ref type="bibr" target="#b38">Xu and Barbosa, 2018;</ref><ref type="bibr" target="#b1">Ali et al., 2020;</ref><ref type="bibr" target="#b4">Chen et al., 2021)</ref> and significant progress has been achieved. However, as supported by our empirical experiments, existing SOTA models perform poorly on a certain number of "hard" mentions, leading to limited overall performance. The main reasons are the following challenges. First, the structure of some contexts surrounding the hard mentions are inherently too complex to extract informative features for identifying entity types. Second, the contexts of some hard mentions are ambiguous and thus it is insufficient to handle these mentions by learning from their contexts only.</p><p>In this paper, we show that representation learning of such hard mentions can be well handled by learning informative knowledge from their sibling mentions. Sibling mentions refer to the mentions that potentially share the same or semantically similar types (e.g., country and nation) with the target mention. We illustrate how sibling mentions assist classifying hard mentions in Figure <ref type="figure">1</ref>. Intuitively, the context of the target mention Sharp is ambiguous and insufficient for inferring the ground-truth types (i.e., organization, company, and tech company), since both a person and a company can "sign a deal with Qualcomm". Fortunately, the sibling mentions provide rich information that works as an important supplement for the target mention Sharp. By aggregating the supplementary information from siblings, it is promising to learn effective representations with less ambiguity for hard target mentions. To utilize sibling mentions, we model FGET as a heterogeneous graph learning problem. The graph is composed of two kinds of nodes, namely the mentions and the types. Besides, there are three kinds of edges connecting the nodes as shown in the left part of Figure <ref type="figure">1</ref>, which represent the sibling relationship between mentions, the hierarchical relationship between types, and the isLabel relationship between mentions and types, respectively. The sibling relationship is considered as the most important part in our graph. For detecting it, we propose two similarity metrics, based on which we design an effective sibling selection algorithm. Upon the constructed nodes and edges, we employ an attentive graph neural module to learn their representations. Particularly, the representations of mention nodes are enriched by aggregating the information from their sibling and type neighbors. It is also noteworthy that, during inference stage, our graph model is scalable to include the unseen test mentions as new nodes and connect them with their existing sibling mention nodes in the graph to derive reliable representations for predictions.</p><p>Extensive experiments are conducted to verify the effectiveness of our model. Our experimental results demonstrate that our model outperforms several strong baselines on the standard test sets with a large margin. Moreover, our model is indeed able to well handle hard mentions with the help from sibling mentions.</p><p>We summarize our contributions as follows:</p><p>• We are the first to point out a bottleneck issue suffered by existing SOTA models, i.e., they perform poorly on a certain number of hard mentions, and we quantitatively analyze its influence on typing accuracy via measuring hard mentions by entropy.</p><p>• We are the first to exploit sibling information for mention representation learning in FGET.</p><p>We design two effective metrics for sibling detection and propose a scalable graph model to take advantages of sibling mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology Overview</head><p>Given a mention m and the type set Y, an FGET model needs to predict the correct types Y m (Y m ⊂ Y) for m based on its context. In this paper, mention representations are learned and refined with the help of sibling mentions and ground-truth types. To achieve it, we propose a heterogeneous graph model enhanced by sibling mentions for FGET, as illustrated in Figure <ref type="figure">1</ref>. First, a mention-type graph G is constructed from training samples (Sec 3). Then, the features for mentions and types are learned by an attentive graph neural module upon G (Sec 4).</p><p>During inference stage (Sec 5), we add test mentions into graph G by connecting them to their sibling mentions in the training set. By aggregating sibling information, the representations of test mentions are generated and used for type prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Construction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Definition</head><formula xml:id="formula_0">Consider graph G = (V m , V y , E m , E y , E m,y ),</formula><p>where V m and V y are the set of mention nodes and type nodes, respectively. E m is the set of edges between the target mentions and their sibling mentions, while E y is the set of edges between types. E m,y denotes the edges connecting the target mentions and the ground-truth types. E m , E y and E m,y are obtained as follows:</p><formula xml:id="formula_1">E m ={(m i , m j )|m i , m j ∈ V m , isSib(m i , m j ) = 1}</formula><p>(1)</p><formula xml:id="formula_2">E y = {(y i , y j )|y i , y j ∈ V y , isA(y i , y j ) = 1} (2) E m,y ={(m i , y j )|m i ∈ V m , y j ∈ V y , isLabel(m i , y j ) = 1}<label>(3)</label></formula><p>where isA(y i , y j ) = 1 indicates y j is the parent or child type of y i in the type hierarchy<ref type="foot" target="#foot_1">2</ref> , and isLabel(m i , y j ) = 1 means mention m i is labeled with the type y j in the training set. Since type hierarchy and the ground-truth types of mentions are available in the training set, V m , V y , E y and E m,y can be easily derived. isSib(m i , m j ) = 1 means m j is the sibling mention of m i , which will be discussed in Sec 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sibling Selection</head><p>The key to construct E m is to define isSib(m i , m j ), i.e., the criterion for judging whether m j is the sibling of m i . We design two metrics to detect the sibling relationships between mentions, named (unsupervised) word distribution-based and (supervised) typing distribution-based metrics.</p><p>Word distribution-based metric The basic assumption for this metric is that mentions sharing more contextual words tend to have more similar ground-truth types. We use TF-IDF to encode mentions as sparse feature vectors. Then the sibling similarity between any two mentions is measured by the cosine similarity of their vectors.</p><p>Typing distribution-based metric In this metric, we first derive the prior score distributions over the type set Y for all the mentions in the dataset from an extra base model <ref type="bibr" target="#b20">(Lin and Ji, 2019)</ref> trained on the same dataset. Then the sibling mentions are selected by their cosine similarities to the target mention based on the score distributions.</p><p>Sibling mention selection Given one of the metrics above, we obtain the sibling mentions according to Algorithm 1. Note that for each target mention m i ∈ V m , we first select a subset V m from V m and only calculate the similarities between m i and the mentions m j ∈ V m . The contexts of mentions from V m share at least one word with that of the target mention and |V m | |V m |, which greatly reduces time complexity. Then, based on the similarity scores, we choose the top-K most similar mentions V m,K as the siblings for m i and let isSib(m i , m j ) = 1 for each m j ∈ V m,K . Be aware that, by definition, the sibling relationship is directed, i.e., isSib(m j , m i ) = 1 does not ensure isSib(m i , m j ) = 1 holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Sibling mention selection</head><p>Input :the set of mention nodes</p><formula xml:id="formula_3">V m 1 for m i , m j ∈ V m do 2 isSib(m i , m j ) ← 0 3 end 4 for m i ∈ V m do 5 select a candidate set V m from V m 6 for m j ∈ V m do 7 compute similarity sim(m i , m j ) 8 end 9 select the top-K similar mentions V m,K from V m 10 for m j ∈ V m,K do 11 isSib(m i , m j ) ← 1 12 end 13 end 4 Graph-based Typing Model 4.1 Attentive Graph Neural Module</formula><p>We employs graph neural networks (GNNs) with L layers <ref type="bibr" target="#b32">(Velickovic et al., 2018;</ref><ref type="bibr" target="#b37">Xu et al., 2019)</ref> to aggregate the information of sibling mentions and types for learning mention representations. At the first layer of G, the embedding of each type y i ∈ Y (denoted by y</p><p>(1) i ∈ R dr ) is randomly initialized. In contrast, to capture the rich features from contexts, the initial embeddings for mentions are derived by a parameterized mention encoder g(•), i.e., m</p><p>(1) i = g(m i ; θ M ) ∈ R dr (details in Sec 4.2). Given the initial mention and type embeddings (i.e., m</p><p>i and y</p><p>(1) i ), the graph module iteratively updates them to obtain m (l+1) i and y (l+1) i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Update of y (l+1) i</head><p>In the l-th (l = 1, ..., L − 1) layer, the updating formula for type embedding y</p><formula xml:id="formula_5">(l+1) i ∈ R dr is: y (l+1) i = f 0 y k ∈Yy i α (l) i,k f 1 (y (l) k ) + f 1 (y (l) i ) ,</formula><p>(4) where f 0 and f 1 are linear layers with ReLU activation. Y y i denotes the type neighbors for y i in graph G, which are the parent or child types of y i in the type hierarchy. α (l) i,k is the attention weight from type y i to y k defined as</p><formula xml:id="formula_6">α (l) i,k = σ y (l) i W (l) 1 y (l) k , y k is a child type; σ y (l) k W (l) 1 y (l) i , y k is a parent type, (<label>5</label></formula><formula xml:id="formula_7">) 2078 W (l)</formula><p>1 ∈ R dr×dr is the weight matrix to model the parent-child relationship. Note that Eq.( <ref type="formula">4</ref>) does not involve mention embeddings and only focuses on learning the hierarchical structure of types. The interaction between types and mentions will be modeled by Eq.( <ref type="formula" target="#formula_8">6</ref>) during the update process of mention embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Update of m (l+1) i</head><p>The updating formula for the mention embedding m</p><formula xml:id="formula_8">(l+1) i ∈ R dr is: m (l+1) i = f2 m j ∈Mm i µ (l) i,j f3 m (l) j + y k ∈Ym i ν (l) i,k f4(y (l) k ) ,<label>(6)</label></formula><p>where M m i and Y m i are the sibling and type neighbors 3 of m i in graph G. µ (l) i,j and ν (l) i,k are the attention weights from m i to mention m j and type y k in the l-th layer, respectively. Specifically,</p><formula xml:id="formula_9">µ (l) i,j = σ m (l) i W (l) 2 m (l) j , ν (l) i,k = σ m (l) i W (l) 3 y (l) k , (7) W (l) 2 , W (l) 3 ∈ R dr×dr are learnable parameters. f 2 , f 3 , f 4 are linear layers with ReLU activation.</formula><p>Here, we use the attention mechanism to distinguish informative neighbors. Besides, the update process of target mentions involves both the sibling and type neighbors, whose representations are also updated at the same. In this way, the learned representations for both mentions and types are more consistent and thus more reliable for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mention Encoder g(m</head><formula xml:id="formula_10">i ; θ M )</formula><p>The mention encoder uses the backbone from <ref type="bibr" target="#b20">Lin and Ji (2019)</ref>. Given a mention, we first encode the mention span and the surrounding context as the weighted sum of their ELMo <ref type="bibr" target="#b27">(Peters et al., 2018)</ref> word representations respectively. Then, the unified feature vector for the mention is derived by concatenating both representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Type Prediction</head><p>Given a mention m i , the predicted score distribution p i ∈ R |Y| over the type set Y is computed as:</p><formula xml:id="formula_11">p i = σ Y (L) W 4 m (L) i + W 5 m (L) i , (8)</formula><p>where</p><formula xml:id="formula_12">Y (L) = y (L) 1 , y (L) 2 , ..., y (L) |Y| ∈ R |Y|×dr , y (L) i and m (L) i</formula><p>are the type and mention embeddings in the L-th layer in GNN. W 4 ∈ R dr×dr and 3 We define that Mm i contains mi itself, thus the selfconnections are taken into account during graph learning.</p><p>W 5 ∈ R |Y|×dr are learnable parameters. p i [k] (the k-th element in p i ) denotes the predicted probability for type y k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Loss Function</head><p>The loss over m i is computed as:</p><formula xml:id="formula_13">i = − |Y| k=1 δ i k log p i [k] + (1 − δ i k ) log(1 − p i [k])<label>(9</label></formula><p>) where δ i k ∈ {0, 1} indicates whether y k is the ground-truth type of m i in the training set. The overall loss is the average over all the mentions, i.e., L = 1 |Vm| i i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Dropout of Y m i</head><p>The representation m (L) i</p><p>incorporates the information from ground-truth type neighbors (Eq.( <ref type="formula" target="#formula_8">6</ref>)). However, it is then used for predicting the groundtruth types in turn (Eq.( <ref type="formula">8</ref>)). The setting that Y m i contains all the ground-truth types will inevitably degenerate the model to just focus on the type neighbors while totally ignore the mention neighbors. To overcome this, each neighboring type in Y m i is randomly discarded with a certain probability γ. In this way, the prediction of discarded type will force the model to learn from the sibling mentions rather than directly from type neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Scalable Testing</head><p>In the following, we describe the prediction process for test mentions.</p><p>Step 1: Given a batch of n test mentions, we first obtain their sibling mentions. To be specific, for each test mention m t , we select a candidate set V m from the training mentions V m . Then, the cosine similarity is computed between m t and each m i in V m , based on which the top K mentions are selected as siblings (see Sec 3.2).</p><p>Step 2: We add the test mentions as nodes into the mention-type graph G, where the test mentions are connected to their sibling mentions selected at</p><p>Step 1. Note that, in the new graph, test mentions have no type neighbors since their ground-truth types are not available. Besides, there is no edge between any two test mentions in the new graph.</p><p>Step 3: Following Eq.( <ref type="formula" target="#formula_8">6</ref>), the representations of test mentions {m t } are updated by aggregating the embeddings for their sibling mentions. Note that Y mt is empty, so no information from the groundtruth types are involved. Through layers of updates, the final representations {m (L) t } are obtained.</p><p>Step 4: Based on the mention embedding m (L) t and the type embeddings Y (L) , we predict the type score distribution for m t by Eq.( <ref type="formula">8</ref>).</p><p>We conclude that, (1) our graph module is scalable to add arbitrary number of unseen test mentions as new nodes to the existing graph to derive their representations. By contrast, many popular graph settings <ref type="bibr" target="#b15">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b32">Velickovic et al., 2018;</ref><ref type="bibr" target="#b33">Wang et al., 2019)</ref> fail to extend to new nodes. (2) Since the embeddings for sibling mentions have been well learnt during training, the only need is to compute the embeddings for test mentions for prediction, which are derived simultaneously during graph inference with high efficiency.</p><p>6 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>We evaluate the proposed model on two widelyused datasets: OntoNotes and BBN.</p><p>OntoNotes The original OntoNotes dataset is annotated by distant supervision <ref type="bibr" target="#b10">(Gillick et al., 2014)</ref>. The training, development and test samples in OntoNotes are about 251K, 2K and 9K, respectively. We also conduct experiments on the augmented version<ref type="foot" target="#foot_2">4</ref>  <ref type="bibr" target="#b6">(Choi et al., 2018)</ref> with 793K training samples<ref type="foot" target="#foot_3">5</ref> . The above two versions share the same test set and development set, as well as the same type set of size 89.</p><p>BBN Different from OntoNotes, BBN is manually annotated <ref type="bibr" target="#b34">(Weischedel and Brunstein, 2005)</ref>. The training, development and test set contain about 84K, 2K and 14K samples respectively, and the type set contains 47 type in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Setup</head><p>Our model is implemented based on the PyTorch Geometric package <ref type="bibr" target="#b8">(Fey and Lenssen, 2019)</ref>. In the main experiments (Sec 6.4), we obtain the sibling mentions according to the typing distribution-based metric described in Sec 3.2. We conduct hyperparameter search on the development set and the optimal settings are presented in Appendix A.</p><p>Following the previous works <ref type="bibr" target="#b21">(Ling and Weld, 2012;</ref><ref type="bibr" target="#b29">Ren et al., 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2019)</ref>, we report the performance in terms of strict accuracy (Acc), macro-average F1 score (Ma-F1) and microaverage F1 score (Mi-F1). To guarantee the relia-bility, we repeat the experiment three times under each setting, and report the average results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Baselines</head><p>We compare our proposed model with several state-of-the-art FGET models: (1) AFET <ref type="bibr" target="#b29">(Ren et al., 2016)</ref>; (2) AAA <ref type="bibr" target="#b0">(Abhishek et al., 2017)</ref>;</p><p>(3) NFETC <ref type="bibr" target="#b38">(Xu and Barbosa, 2018)</ref>; (4) NEURAL <ref type="bibr" target="#b31">(Shimaoka et al., 2017)</ref>; (5) ACT <ref type="bibr" target="#b42">(Zhang et al., 2018)</ref>; (6) Lin and Ji ( <ref type="formula">2019</ref>  <ref type="formula">2020</ref>). Note that Lin and Ji ( <ref type="formula">2019</ref>) is considered as an important baseline in our experiments and is marked with in Table <ref type="table" target="#tab_0">1</ref>-3, since we use it as the base model to derive the prior typing distributions for sibling selection (Sec 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Results and Analysis</head><p>Table <ref type="table" target="#tab_0">1</ref>, 2 and 3 illustrate the experimental results on the original and the augmented OntoNotes, as well as the BBN dataset.</p><p>Analysis The results demonstrate that learning from sibling mentions helps our model outperform most baselines across the benchmarks. The detailed analysis is presented as follows:</p><p>(1) We select sibling mentions according to the typing distribution from <ref type="bibr" target="#b20">Lin and Ji (2019)</ref>. We observe that, after aggregating sibling information through the attentive graph neural module (Sec 4.1), our model significantly outperforms Lin and Ji (2019) on both the original OntoNotes and the BBN dataset. When trained on the augmented OntoNotes of the same size, our model increases the accuracy score by more than 5% over <ref type="bibr" target="#b20">Lin and Ji (2019)</ref> . Compared with Lin and Ji (2019) * which utilizes the full 3M augmented OntoNotes for training, our model still maintains a comparable performance and even improves the accuracy score by about 2%.</p><p>(2) Many previous works have demonstrated the effectiveness of modeling type hierarchy for entity typing <ref type="bibr" target="#b29">(Ren et al., 2016;</ref><ref type="bibr" target="#b38">Xu and Barbosa, 2018;</ref><ref type="bibr" target="#b36">Xiong et al., 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2020)</ref>. As a comparison, our model also considers the hierarchical information of types and incorporates it in a natural way (Sec 4.1). From the results, we conclude that learning jointly from type hierarchy and sibling mentions can remarkably improve the typing performance.</p><p>(3) The attention mechanism plays an important role in our graph module and some of the baselines <ref type="bibr" target="#b29">(Ren et al., 2016;</ref><ref type="bibr" target="#b0">Abhishek et al., 2017;</ref><ref type="bibr" target="#b38">Xu and Barbosa, 2018)</ref>. It not only helps identify the informative features from neighbors but also helps alleviate noise from the training data constructed by distant supervision (e.g., OntoNotes). The results reveal that our graph-based solution is more effective than the existing solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Acc Ma-F1 Mi-F1  <ref type="bibr" target="#b19">(Lin et al., 2012)</ref>, which results in the higher accuracy score on BBN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">Effect of the sibling selection metrics</head><p>In Sec 3.2, we propose two similarity metrics to discover sibling relationships in graph G, and abbreviate them as: "Word-based" and "Typing-based" metrics. Here, we provide two additional metrics for more detailed analysis: the "Gold typing-based" and the "Random-based" metrics, which are two extreme variations of the typing-based metrics. Under the gold typing-based metric, the siblings are selected by the gold typing distribution, where each dimension is 0 or 1 according to the ground-truth types of the mention. In this way, candidate mentions that share more ground-truth types with the target mention will have larger cosine similarity and thus be chosen as the siblings with a higher probability. On the contrary, under the randombased metric, siblings are selected at random. Since the type set is large, the siblings are more likely to be irrelevant with the target mention and may contain much noise.</p><p>Measuring sibling quality Intuitively, different similarity metrics will affect the quality of siblings. To quantify this effect, we measure the sibling quality for the test mentions V in the original OntoNotes and define the metrics as follows.</p><p>For each mention m i ∈ V , denote its groundtruth types as Y m i and sibling mentions in graph G (defined in Sec 3.1) as M m i . Further, for M m i , we denote their ground-truth types as Y M i , i.e.,</p><formula xml:id="formula_14">Y M i = m j ∈Mm i \{m i } Y m j . (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>Similar to the definitions of Precision, Recall and F1, we define Purity, Coverage and Quality to measure the sibling quality of V :</p><formula xml:id="formula_16">Purity = 1 |V | m i ∈V = |Ym i ∩ YM i | |YM i | Coverage = 1 |V | m i ∈V |Ym i ∩ YM i | |Ym i | Quality = 2 * Coverage * Purity Coverage + Purity (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results are presented in Table <ref type="table" target="#tab_2">4</ref>. In general, the model performance is closely related to the sibling quality. Besides, the typing-based metric performs better than the word-based metric. This indicates that the the continuous typelevel probability distribution is more reliable for sibling selection than the discrete word-level distribution. The scores from the gold typing-based and the random-based metrics reveal the upper bound and the lower bound of the scores for the typingbased metric. On the one hand, the quality of the siblings selected by the gold typing-based metric is much higher than those by other methods, with the Coverage up to 97.6%. Meanwhile, its corresponding model also outperforms the other three by a large margin. Note that the typing performance in this scenario is limited by the annotation quality to some extent. Since OntoNotes is annotated by distant-supervision, the scores for the gold typing-based metric could not reach higher due to the label noise of the siblings. On the other hand, a distinct drop of the scores is observed with the random-based metric. This is reasonable since the randomly selected siblings contain much noisy information, which is helpless and even harmful for typing of the target mention. It can be concluded from the above observations that there is still much room to improve the sibling quality as well as the typing performance of the sibling-enhanced model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2">Effect of sibling size K</head><p>The model performance is sensitive to the size of selected sibling mentions for a target mention in the graph G. Denote the sibling size as K, following the default hyper-parameter settings, we train our model on the original OntoNotes under different K ∈ {0, 5, 10, 15} using the typing-based sibling selection metric. The corresponding sibling quality and model performance are reported in Table <ref type="table" target="#tab_3">5</ref>. We observe that the best scores are obtained with the top 5 sibling mentions. When K = 0, the graph only contains the self-connections from the target mentions to themselves. Without the additional information from siblings, the Macro F1 score decreases by 2.1%, which indicates the effectiveness of sibling mentions for improving the typing performance of our model. When K = 0, the Coverage score goes up while the Purity and Quality scores go down as K ranges from 5 to 15. Meanwhile, the typing performance decreases as K increases. It suggests that, for OntoNotes, a properly smaller sibling size is a trade-off choice for the model to use siblings with higher quality and thus achieve better typing performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.3">Effect of dropout probability γ</head><p>We randomly discard some type neighbors with a dropout probability γ during training (Sec 4.5), which forces the model to learn from the sibling mentions other than the ground-truth types. Table <ref type="table">6</ref> shows the results under different values of γ on the original OntoNotes dataset. Generally, the model achieves better performance with larger γ. This indicates discarding a large proportion of groundtruth types is beneficial for learning from sibling mentions. Besides, it also narrows the difference between training and test settings where the test mentions do not have ground-truth types as neighbors. The best performance is achieved when γ equals around 0.7. However, dropping all the type neighbors (i.e., γ = 1) will block the interaction between the type and mention representations in the graph, which may slightly damage the performance. To select sibling mentions, we first derive the prior typing distribution from the base model <ref type="bibr" target="#b20">(Lin and Ji, 2019)</ref> as described in Sec 3.2. During experiments, we observe that the contextual information for some mentions are insufficient or too complex, which makes the base model confused on these mentions. Entropy measures the uncertainty of a probability distribution. Thus, we quantify the difficulty of mentions by the entropy of their corresponding prior typing distributions and define the mentions with the top-500 highest entropy values as hard mentions, which account for about 5% of the whole mentions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.2">Case Study</head><p>To further provide an intuitive understanding about how our model benefits from sibling mentions, we present an example in Table <ref type="table" target="#tab_7">8</ref>. As expected, the retrieved siblings based on the metric defined in Sec 3.2 share similar ground-truth types with the target mention. This verifies the effectiveness of our sibling selection algorithm. Moreover, we observe that the siblings even help predict the correct but out-of-gold-set types for the target mention in this case. Although the annotated types for the target mention [GM officials] only contains /person in the test set. The sibling mentions still provide a strong evidence for our model to also predict /person/title as a possible type for the target mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>FGET is an important task for the downstream NLP tasks and many efforts have been make in improving its performance <ref type="bibr" target="#b39">(Zhang et al., 2020a;</ref><ref type="bibr" target="#b22">Liu et al., 2021a)</ref>. Early works in FGET <ref type="bibr" target="#b21">(Ling and Weld, 2012;</ref><ref type="bibr" target="#b30">Shimaoka et al., 2016)</ref> mainly focus  on feature extraction for mentions, which do not consider label noise introduced by distant supervision <ref type="bibr" target="#b10">(Gillick et al., 2014;</ref><ref type="bibr" target="#b6">Choi et al., 2018;</ref><ref type="bibr" target="#b16">Li et al., 2020)</ref>. Recent years have witnessed an increasing number of researchers being dedicated to data denoising. A popular solution <ref type="bibr" target="#b29">(Ren et al., 2016;</ref><ref type="bibr" target="#b0">Abhishek et al., 2017;</ref><ref type="bibr" target="#b38">Xu and Barbosa, 2018;</ref><ref type="bibr" target="#b1">Ali et al., 2020)</ref> is to design loss functions for the clean and noisy parts of the training data separately. Nevertheless, <ref type="bibr" target="#b41">Zhang et al. (2020c)</ref> proposes an automatic relabeling framework to estimate the pseudo-truth label distribution of each sample, which treats the noisy and clean data uniformly. Besides, <ref type="bibr" target="#b2">Chen et al. (2019)</ref> groups mentions of the same type into a compact cluster to improve the robustness of the model. <ref type="bibr" target="#b1">Ali et al. (2020)</ref> refines noisy representations by corpus-level contextual clues. <ref type="bibr" target="#b25">Onoe and Durrett (2019)</ref> introduces two additional models to delete the samples that are too noisy to be useful, and repair noisy labels for the retained examples. In addition, there are some notable work which tries to build FGET with limited resources <ref type="bibr" target="#b28">(Qian et al., 2021)</ref>.</p><p>Modeling the type hierarchy is another important topic in FGET. Prior solutions <ref type="bibr" target="#b31">(Shimaoka et al., 2017)</ref> introduce a one-hot matrix to encode the hierarchy. <ref type="bibr" target="#b38">Xu and Barbosa (2018)</ref> proposes a hierarchyaware loss function. Recently, graph-based methods have been proven to be powerful in many NLP tasks <ref type="bibr" target="#b15">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b17">Liang et al., 2021;</ref><ref type="bibr" target="#b37">Xu et al., 2019;</ref><ref type="bibr" target="#b18">Liang et al., 2022)</ref>. Using graphs to model the type hierarchy in FGET is a natural idea. <ref type="bibr" target="#b14">Jin et al. (2019)</ref> models the potential type correlations for in-knowledge-base entities via hierarchical multi graph convolutional networks (GCNs). Further, <ref type="bibr" target="#b36">Xiong et al. (2019)</ref> extends GCNs to a vast number of free-form types. <ref type="bibr" target="#b3">Chen et al. (2020)</ref> designs a multi-level learning-to-rank loss to leverage hierarchical information. Recently, <ref type="bibr" target="#b24">Onoe et al. (2021)</ref> models the mention and type representations in a box space instead of the traditional vector space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we firstly point out that SOTA typing models suffer from a bottleneck issue, i.e., they perform poorly on a certain number of hard mentions, which leads to their limited overall performance. To this end, we propose to exploit sibling information for mention representation learning and define two metrics for detecting sibling relationship between mentions. Further, we model sibling learning as a graph learning problem. Our model is scalable in that, once trained, it can generate sibling-aware representations for previously unseen mentions efficiently during inference stage. Extensive experiments show that the proposed model indeed handles hard mentions well and thereby yields better overall performance than SOTA baseline models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>m 1 :m 3 :Figure 1 :</head><label>131</label><figDesc>Figure 1: Illustration of the proposed sibling enhanced heterogeneous graph model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>); (7) Chen et al. (2020); (8) LABELGCN (Xiong et al., 2019); (9) Choi et al. (2018); (10) Ali et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test results on the original OntoNotes dataset.</figDesc><table><row><cell>Ren et al. (2016)</cell><cell>55.1</cell><cell>71.1</cell><cell>64.7</cell></row><row><cell>Abhishek et al. (2017)</cell><cell>52.2</cell><cell>68.5</cell><cell>63.3</cell></row><row><cell>Shimaoka et al. (2017)</cell><cell>51.7</cell><cell>71.0</cell><cell>64.9</cell></row><row><cell>Zhang et al. (2018)</cell><cell>55.5</cell><cell>73.3</cell><cell>67.6</cell></row><row><cell cols="2">Xu and Barbosa (2018) 54.4</cell><cell>71.5</cell><cell>64.9</cell></row><row><cell>Lin and Ji (2019)</cell><cell>55.4</cell><cell>73.8</cell><cell>68.4</cell></row><row><cell>Chen et al. (2020)</cell><cell>58.7</cell><cell>73.0</cell><cell>68.1</cell></row><row><cell>Ali et al. (2020)</cell><cell>57.7</cell><cell>74.3</cell><cell>68.5</cell></row><row><cell>Our Model</cell><cell>59.2</cell><cell>76.5</cell><cell>71.0</cell></row><row><cell>Model</cell><cell cols="3">Acc Ma-F1 Mi-F1</cell></row><row><cell cols="2">Choi et al. (2018)  *  59.5</cell><cell>76.8</cell><cell>71.8</cell></row><row><cell>Lin and Ji (2019)  *</cell><cell>63.8</cell><cell>82.9</cell><cell>77.3</cell></row><row><cell cols="2">Xiong et al. (2019) 59.6</cell><cell>77.8</cell><cell>72.2</cell></row><row><cell>Lin and Ji (2019)</cell><cell>60.3</cell><cell>81.6</cell><cell>74.3</cell></row><row><cell>Our Model</cell><cell>65.7</cell><cell>82.4</cell><cell>77.4</cell></row><row><cell cols="4">Table 2: Test results on the augmented OntoNotes</cell></row><row><cell cols="4">dataset. Note that the baselines with "*" employ the</cell></row><row><cell cols="4">full version (about 3M training samples) of augmented</cell></row><row><cell cols="4">OntoNotes built from the licensed Gigaword (Choi</cell></row><row><cell cols="4">et al., 2018), while the rest only uses the open-sourced</cell></row><row><cell cols="4">subset (i.e., 793K training samples) of it, which may</cell></row><row><cell cols="2">downgrade the performance.</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Acc Ma-F1 Mi-F1</cell></row><row><cell>Ren et al. (2016)</cell><cell>67.0</cell><cell>72.7</cell><cell>73.5</cell></row><row><cell>Abhishek et al. (2017)</cell><cell>60.4</cell><cell>74.1</cell><cell>75.7</cell></row><row><cell cols="2">Abhishek et al. (2017)* 73.3</cell><cell>79.1</cell><cell>79.2</cell></row><row><cell>Shimaoka et al. (2017)</cell><cell>64.7</cell><cell>76.5</cell><cell>71.5</cell></row><row><cell>Zhang et al. (2018)</cell><cell>60.1</cell><cell>77.8</cell><cell>76.9</cell></row><row><cell cols="2">Xu and Barbosa (2018) 72.1</cell><cell>77.1</cell><cell>77.5</cell></row><row><cell>Lin and Ji (2019)</cell><cell>59.9</cell><cell>82.9</cell><cell>81.7</cell></row><row><cell>Chen et al. (2020)</cell><cell>55.9</cell><cell>79.3</cell><cell>78.1</cell></row><row><cell>Ali et al. (2020)</cell><cell>70.3</cell><cell>81.9</cell><cell>82.3</cell></row><row><cell>Our Model</cell><cell>72.2</cell><cell>85.9</cell><cell>86.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Test results on the BBN dataset. Note that Abhishek et al. (2017) * uses the feature representations learnt from an extra dataset, FIGER</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Comparison among different sibling selection metrics. Note that the sibling quality scores of "Gold typing-based" do not reach 1, since the sibling mentions are only selected from a subset V m (see Sec 3.2) for time efficiency, which are not guaranteed to have exactly the same ground-truth types with the target mention.</figDesc><table><row><cell>Macro</cell><cell>Micro</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Effect of the sibling size K on sibling quality and typing performance over the original OntoNotes dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Table7compares the performance of our model and the base model on both the whole mentions and the hard mentions from the test dataset of the original OntoNotes. we see that both models perform worse on the hard mentions than on the whole mentions. Besides, except for the superiority of our model regarding the Acc, Ma-F1 and Mi-F1 scores, it also achieves a lower entropy value than the base model especially on the hard mentions. This indicates the information from siblings makes the output type distributions more concentrated and therefore increases the confidence for model predictions. Results on the whole mentions and hard mentions of the original OntoNotes. Base denotes the base model from<ref type="bibr" target="#b20">Lin and Ji (2019)</ref>. Ep is short for Entropy.</figDesc><table><row><cell cols="3">Mention Model Ep</cell><cell cols="3">Acc Ma-F1 Mi-F1</cell></row><row><cell>whole</cell><cell>Ours</cell><cell cols="2">2.1 59.2</cell><cell>76.5</cell><cell>71.0</cell></row><row><cell>mentions</cell><cell>Base</cell><cell cols="2">2.4 55.4</cell><cell>73.8</cell><cell>68.4</cell></row><row><cell>hard</cell><cell>Ours</cell><cell cols="2">2.5 57.2</cell><cell>73.6</cell><cell>66.6</cell></row><row><cell>mentions</cell><cell>Base</cell><cell cols="2">3.3 51.0</cell><cell>65.3</cell><cell>58.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Target mention: [GM officials] told workers late last week of the following moves: production of full-sized vans will be consolidated into a single plant in Flint, Mich.</figDesc><table><row><cell cols="2">Ground-truth: /person</cell><cell></cell><cell></cell></row><row><cell cols="3">Prediction from our model: /person, /person/title</cell><cell></cell></row><row><cell cols="4">Sibling 1: "It's been a steadily improving relationship.",</cell></row><row><cell>says the [president].</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Ground-truth: /person, /person/title</cell><cell></cell></row><row><cell cols="4">Sibling 2: Apart from those two actions, Mr.Sikes and</cell></row><row><cell cols="4">the three other [commissioners] said they expect to re-</cell></row><row><cell cols="4">examine how AT&amp;T is regulated since competition has</cell></row><row><cell>increased.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Ground-truth: /person, /person/title</cell><cell></cell></row><row><cell cols="4">Sibling 3: HUD Secretary [Jack Kemp] backed an un-</cell></row><row><cell cols="4">successful effort to strike such language last week, but</cell></row><row><cell cols="3">received little support from the White House • • •</cell><cell></cell></row><row><cell>Ground-truth:</cell><cell>/person,</cell><cell>/person/artist,</cell><cell>/per-</cell></row><row><cell>son/artist/actor,</cell><cell cols="2">/person/artist/author,</cell><cell>/per-</cell></row><row><cell>son/political_figure</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>An example to illustrate the relationship between the target mention and the sibling mentions from the Original OntoNotes.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">To simplify the statement, in the rest of this paper, the term "mention" is referred to as the contextualized mention, i.e., a mention accompanied with its context.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The edges between yi and its parent or child type yj are directed, as detailed in Eq.(5)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">http://nlp.cs.washington.edu/entity_type</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">We use the open-sourced version, which is a subset of the dataset reported in<ref type="bibr" target="#b6">Choi et al. (2018)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgements</head><p>This work was partially supported by the National Natural Science Foundation of China (61876053, 62006062, 62176076), the Shenzhen Foundational Research Funding (JCYJ20200109113441941, JCYJ20210324115614039), Joint Lab of Lab of HITSZ and China Merchants Securities.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameter Settings</head><p>The default hyperparameters for our model are set as follows, where K is mentioned in Sec 3.2, L, γ and d r are mentioned in Sec 4.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fine-grained entity type classification by jointly learning representations and label embeddings</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Abhishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Awekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<title level="s">the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="797" to="807" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fine-grained named entity typing over distantly supervised data based on refined representations</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yifang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="7391" to="7398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving distantly-supervised entity typing with compact latent space clustering</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2862" to="2872" />
		</imprint>
	</monogr>
	<note>Guoping Hu, Yueting Zhuang, and Xiang Ren</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical entity typing via multi-level learning to rank</title>
		<author>
			<persName><forename type="first">Tongfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunmo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8465" to="8475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An empirical study on multiple information sources for zero-shot fine-grained entity typing</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A question-answering based framework for relation extraction validation</title>
		<author>
			<persName><forename type="first">Jiayang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02934</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ultra-fine entity typing</title>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Insrl: A multi-view learning framework fusing multiple information sources for distantly-supervised relation extraction</title>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>ArXiv, abs/2012.09370</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving event representation via simultaneous weakly supervised contrastive learning and clustering</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changlong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilfred</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Contextdependent fine-grained entity type tagging</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Huynh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1820</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoben</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04821</idno>
		<title level="m">Complex relation extraction: Challenges and opportunities</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Surface pattern-enhanced relation extraction with global constraints</title>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4509" to="4540" />
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explaining a bag of words with hierarchical conceptual labels</title>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1693" to="1713" />
			<date type="published" when="2020">2020c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fine-grained entity typing via hierarchical multi graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hailong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan-Zi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>ArXiv, abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Empirical analysis of unlabeled entity problem in named entity recognition</title>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Targetadaptive graph for cross-target stance detection</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In the Web Conference 2021 (WWW &apos;21</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aspect-based sentiment analysis via affective knowledge enhanced graph convolutional networks</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2021.107643</idno>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page">107643</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">No noun phrase left behind: detecting and typing unlinkable entities</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</title>
				<meeting>the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="893" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An attentive fine-grained entity typing model with latent type representation</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6198" to="6203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TexSmart: A system for enhanced natural language understanding</title>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dick</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanhui</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-demo.1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</title>
				<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2021">2021a</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Texsmart: A system for enhanced natural language understanding</title>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dick</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</title>
				<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2021">2021b</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Modeling fine-grained entity types with box embeddings</title>
		<author>
			<persName><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Boratko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00345</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to denoise distantly-labeled data for entity typing</title>
		<author>
			<persName><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fine-grained entity typing for domain independent entity linking</title>
		<author>
			<persName><forename type="first">Yasumasa</forename><surname>Onoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8576" to="8583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fine-grained entity typing without knowledge base</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.431</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5309" to="5319" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Afet: Automatic finegrained entity typing by hierarchical partial-label embedding</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An attentive neural architecture for fine-grained entity type classification</title>
		<author>
			<persName><forename type="first">Sonse</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AKBC@NAACL-HLT</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural architectures for fine-grained entity type classification</title>
		<author>
			<persName><forename type="first">Sonse</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1271" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>ArXiv, abs/1710.10903</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Bbn pronoun coreference and entity type corpus. Linguistic Data Consortium</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ada</forename><surname>Brunstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">112</biblScope>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving neural fine-grained entity typing with knowledge attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imposing label-relational inductive bias for extremely fine-grained entity typing</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deren</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="773" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>ArXiv, abs/1810.00826</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural finegrained entity type classification with hierarchyaware loss</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Texsmart: A text understanding system for fine-grained NER and enhanced semantic analysis</title>
		<author>
			<persName><forename type="first">Haisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanhui</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno>CoRR, abs/2012.15639</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Texsmart: A text understanding system for finegrained ner and enhanced semantic analysis</title>
		<author>
			<persName><forename type="first">Haisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Botong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianchen</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15639</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning with noise: Improving distantlysupervised fine-grained entity typing via automatic relabeling</title>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkun</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020c</date>
			<biblScope unit="page" from="3808" to="3815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fine-grained entity typing through increased discourse context and adaptive classification thresholds</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</title>
				<meeting>the Seventh Joint Conference on Lexical and Computational Semantics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="173" to="179" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
