<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Smith</forename><forename type="middle">K</forename><surname>Khare</surname></persName>
							<email>smith7khare@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">the Electronics and Communication Discipline</orgName>
								<address>
									<country>Indian</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Varun</forename><surname>Bajaj</surname></persName>
							<email>varunb@iiitdmj.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">the Electronics and Communication Discipline</orgName>
								<address>
									<country>Indian</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5AF38565B9834F8EE176048507355217</idno>
					<idno type="DOI">10.1109/TNNLS.2020.3008938</idno>
					<note type="submission">received February 4, 2020; revised April 30, 2020; accepted July 9, 2020.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural networks (CNNs)</term>
					<term>electroencephalogram (EEG)</term>
					<term>emotion recognition</term>
					<term>smoothed pseudo-Wigner-Ville distribution (SPWVD)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotions composed of cognizant logical reactions toward various situations. Such mental responses stem from physiological, cognitive, and behavioral changes. Electroencephalogram (EEG) signals provide a noninvasive and nonradioactive solution for emotion identification. Accurate and automatic classification of emotions can boost the development of human-computer interface. This article proposes automatic extraction and classification of features through the use of different convolutional neural networks (CNNs). At first, the proposed method converts the filtered EEG signals into an image using a time-frequency representation. Smoothed pseudo-Wigner-Ville distribution is used to transform time-domain EEG signals into images. These images are fed to pretrained AlexNet, ResNet50, and VGG16 along with configurable CNN. The performance of four CNNs is evaluated by measuring the accuracy, precision, Mathew's correlation coefficient, F1-score, and false-positive rate. The results obtained by evaluating four CNNs show that configurable CNN requires very less learning parameters with better accuracy. Accuracy scores of 90.98%, 91.91%, 92.71%, and 93.01% obtained by AlexNet, ResNet50, VGG16, and configurable CNN show that the proposed method is best among other existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>attention due to its simplicity of acquisition and ease of use. EEG signals measure the electrical exercises of the brain and are hard to impact intentionally.</p><p>To date, researchers have proposed multiple methods for emotion classification based on EEG signals. Emotion classification from power spectral density (PSD), wavelet, and nonlinear dynamical features extraction using a support vector machine (SVM) has been proposed by Wang et al. <ref type="bibr" target="#b3">[4]</ref>. The model proposed by them managed to achieve an accuracy of 83.55%. Nie et al. <ref type="bibr" target="#b4">[5]</ref> used a series of bandpass filters, followed by the fast Fourier transform (FFT) to separate the chaotic rhythms. Log band energy has been classified by using SVM to attain an accuracy of 84.94%. Atkinson and Campos <ref type="bibr" target="#b5">[6]</ref> used filtering-based rhythms separation to extract different feature sets. The dimensionality of features has been reduced by minimum redundancy, maximum relevance, and genetic algorithm (mRmR). The features have been classified by using SVM with an accuracy of 62.33%. Six time-domain features and five frequency-domain features extracted by FFT have been classified by using SVM, k-nearest neighbor (kNN), and multilayer perceptron (MLP) classifiers by Wang et al. <ref type="bibr" target="#b6">[7]</ref>. The average accuracies of 59.84%, 63.07%, and 66.51% have been achieved by kNN, MLP, and SVM, respectively. Lin et al. <ref type="bibr" target="#b7">[8]</ref> and Liu et al. <ref type="bibr" target="#b8">[9]</ref> used a short-time Fourier transform (STFT) with nonoverlapping Hanning window for feature extraction. These features have been classified by using SVM and linear discriminant analysis (LDA). The model proposed by Lin et al. <ref type="bibr" target="#b7">[8]</ref> and Liu et al. <ref type="bibr" target="#b8">[9]</ref> achieved an accuracy of 80.86% and 65.09%, respectively. Ullah et al. <ref type="bibr" target="#b9">[10]</ref> used multiple techniques based on STFT, discrete cosine transform, and spectrogram. These features have been classified by using SVM to identify human emotions with an accuracy of 73.5%. Lee and Lee <ref type="bibr" target="#b10">[11]</ref> used STFT and convolutional neural network (CNN) with an accuracy of 89%.</p><p>Murugappan <ref type="bibr" target="#b11">[12]</ref> used a wavelet transform (WT) with four different wavelets. Features extracted from the subbands have been classified by using kNN. An accuracy of 82.87% and 78.57% has been achieved with 62 and 24 channels. Murugappan et al. <ref type="bibr" target="#b12">[13]</ref> and Mohammadi et al. <ref type="bibr" target="#b13">[14]</ref> used discrete wavelet transform (DWT) to extract various features. These features have been classified by using kNN. The proposed method in <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b13">[14]</ref> correctly predicted 83.26% and 86.75% of the emotions. Features based on statistical parameters, WT, and higher order crossing have been analyzed by using LDA, kNN, and SVM by Petrantonakis and Hadjileontiadis <ref type="bibr" target="#b14">[15]</ref>. Their model achieved an accuracy of 62.3% and 83.33% with LDA and SVM. Guo et al. <ref type="bibr" target="#b15">[16]</ref> used the WT and fuzzy cognitive maps for emotion classification. Features extracted by using WT have been classified by hybrid SVM with an accuracy of 73.32%. Multiwavelet analysis with three multiwavelets, namely, Gernoimo-Hardin-Massopust, Chui Lian, and SA4, is presented by Bajaj and Pachori <ref type="bibr" target="#b16">[17]</ref>. Features based on the Euclidian distance from phase space reconstruction have been classified by using a multiclass least square SVM (MC-LS-SVM). Zhuang et al. <ref type="bibr" target="#b17">[18]</ref> used empirical mode decomposition (EMD) that extracts intrinsic mode functions (IMFs). Three features extracted from the IMF have been classified using SVM. The model proposed by Zhuang et al. <ref type="bibr" target="#b17">[18]</ref> achieved an accuracy of 70.5%. Taran and Bajaj <ref type="bibr" target="#b18">[19]</ref> used correlation-based filtering (CIF) method. The IMF and modes extracted by EMD and variational mode decomposition (VMD) have been filtered. Features from the modes have been classified by using MC-LS-SVM with an accuracy of 90.63%.</p><p>Bajaj et al. <ref type="bibr" target="#b19">[20]</ref> used tunable Q wavelet transform (TQWT) to decompose the signal into low-and high-pass subbands. Features extracted from the subbands have been classified by the extreme learning machine (ELM). Their method provided 87.1% accurate separation of emotions. Bajaj et al. <ref type="bibr" target="#b20">[21]</ref> and Gupta et al. <ref type="bibr" target="#b21">[22]</ref> used flexible analytic wavelet transform (FAWT). Several features have been extracted and classified by using random forest and kNN. The method in <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b21">[22]</ref> attains an accuracy of 86.1% and 87.5%, respectively. Separation of emotions using phase and angle reconstruction with Poincare feature extraction and SVM classification has been proposed in <ref type="bibr" target="#b22">[23]</ref>. A hybrid deep belief network and hidden Markov model has been used to differentiate the emotions <ref type="bibr" target="#b23">[24]</ref>. Differentiating the emotions by using self-organizing maps has been proposed in <ref type="bibr" target="#b24">[25]</ref>. Common spatial patterns and PSD-based feature extraction methods have been used for emotion recognition using linear SVM <ref type="bibr" target="#b25">[26]</ref>. Feature extraction based on asymmetric spatial patterns and Naive Bayes classifier has been used to identify the emotions <ref type="bibr" target="#b26">[27]</ref>. The hybrid model based on the Hilbert-Huang spectrum, Zhao-Atlas-Marks distribution, and spectrogram methods has been used to classify emotions using SVM and kNN <ref type="bibr" target="#b27">[28]</ref>. Quadratic time-frequency distribution and group sparse canonical correlation analysis have been used for the recognition of emotions <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>.</p><p>The methods based on filtering, FFT, and wavelet used an empirical selection of a type of filter, order, window, and wavelet. Choosing the length and type of window is an issue in STFT. The EMD-based method is purely experimental and lacks mathematical modeling. Due to the nonstationary nature of the EEG signals, an accurate selection of decomposition parameters of TQWT, FAWT, and VMD is difficult. The Zhao-Atlas-Marks distribution, Hilbert-Huang transform, and common spatial patterns are prone to noise. Moreover, the majority of the methods proposed in the literature have used manual feature extraction and classification methods. The traditional signal processing, feature extraction, and classification are time-consuming. These methods require huge qualitative and quantitative parameters analysis that greatly controls the performance of the system. Also, the methods in the literature are limited by its performance.</p><p>The problem mentioned in the abovementioned literature creates an immediate need to develop automatic decomposition and classification of the signal. In this article, smoothed pseudo-Wigner-Ville distribution (SPWVD) and CNN-based emotions recognition is proposed. SPWVD is used for the transformation of a time-domain signal into time, frequency, and amplitude representation. The images of time-frequency representation (TFR) are given as an input to different CNNs. Three pretrained CNNs and a configurable CNN are used to classify the images. Several performance parameters are evaluated to get an insight into the proposed method. Finally, the superiority of the proposed method is tested by comparing it with the existing state of the art. The rest of this article is organized as follows. Section II describes the methodology of the proposed work. Results are discussed in Section III, and the discussion of the proposed method with existing methods is covered in Section IV. Finally, Section V presents the conclusion of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data-Set</head><p>Pictures, audio, video, and audio-video can be used to elicit various human emotions. The extraction of emotions from an audio-video method has outperformed other methods. The EEG recordings of 20 students with the mean age of 23 ± 0.5 years have been used. The data set is available online and the details of the experimental setup can be found in <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>. Audio-video clips of 10 s from Indian movies shown to the volunteers. The movie clips assumed to be self-explanatory, small, and intended to elicit single emotion. A 24-channel EEG recorder with transverse bipolar montage built according to the international 10-20 system has been used. The EEG signals have been recorded at a sampling frequency of 256 Hz. All the students considered for recordings do not have any physical or mental disorder. Four basic emotions, viz., fear, happy, relax, and sad, have been captured. Steps involved in the classification of emotions are shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Preprocessing</head><p>The EEG signals are superimposed with noise sources that are not produced by neuronic actions called artifacts. EEG  <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b30">[31]</ref>. The frontal portion of human brain is significant for recording human reactions <ref type="bibr" target="#b31">[32]</ref>. The EEG recordings of six frontal electrodes, namely, FP1, FP2, F3, F4, F7, and F8, have been used. As the electrodes are bipolar, the reading of FP2-F8, FP1-F3, FP2-F4, and FP1-F7 has been considered for signal processing and classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Smoothed Pseudo-Wigner-Ville Distribution</head><p>CNNs require input as an image. Time-domain signals are converted into TFR to record the information in the spectral domain. A signal can be transformed into TFR by using STFT, Wigner-Ville distribution, SPWVD, continuous wavelet transform (CWT), and so on. TFR is a spatial representation of time, frequency, and amplitude simultaneously. TFR produced by STFT is known as a spectrogram. STFT requires the selection of window, its width, shape, and sampling frequency. This length must be maintained uniform throughout the signal. The spectrogram obtained by STFT gives poor resolution due to time-frequency localization. TFR obtained by CWT is called a scalogram. CWT needs the mother wavelet and its parameters to be chosen. The resolution of the scalogram depends on the choice of wavelet, while TFR obtained by the Wigner-Ville distribution produces cross term and attenuation for low frequency. To overcome these limitations, time-domain filtered EEG signals are transformed into a TFR by using SPWVD. SPWVD provides good time-frequency resolution that solves the problems of STFT and CWT. Limitations of the Wigner-Ville distribution is addressed by the introduction of cross-term reducing window in frequency domain. Hence, it is justified to choose SPWVD for signal transformation. SPWVD gives a direct representation of the time-frequency localization of signal energy. The length and type of cross-term reducing window in time and frequency domains can be chosen independently. Because of this, SPWVD provides good time-frequency cluster characteristics. The mathematical formulation of SPWVD can be represented by <ref type="bibr" target="#b32">[33]</ref> </p><formula xml:id="formula_0">ϕ(t, f ) = +∞ -∞ γ t -t φ t , f dt φ t , f = +∞ -∞ h(τ )z t + τ 2 z * t - τ 2 e -j 2π f t dτ (1)</formula><p>where γ (t) and h(t) are the cross-terms reducing windows in frequency and time domains. Time and frequency domain smoothing scales can be controlled easily. Length of the windows of γ (t) and h(t) can be selected independently. The TFR of filtered EEG signals obtained from SPWVD is shown in Fig. <ref type="figure" target="#fig_3">3</ref>. As evident from the figure, all the states, namely, sad, happy, relax, and fear, show a discriminative representation of time-frequency-energy analysis. The energy amplitude of happy and fear state is very high (in the range of 10 000), energy amplitude of sad is medium (in the range of 5000), and energy amplitude of relax is low (in the range of 2500). The visual inspection of Figs. <ref type="figure" target="#fig_1">2</ref> and<ref type="figure" target="#fig_3">3</ref> shows that the transformed signals provide better insight information than the filtered time-domain EEG signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Convolutional Neural Networks</head><p>They are newly added subfield of machine learning domain. Inspired by artificial neural networks, CNN is comprised of self-optimized neurons. CNN is also known as a deep learning network that automatically classifies the signals. Inspired by mice's visual system, CNN is designed to work with images. CNN takes the spacial and configural structure of the input into account <ref type="bibr" target="#b33">[34]</ref>.</p><p>Recently, CNN is one of the most widely used techniques for image classification, object detection, face recognition, and so on. CNN is composed of multilayers interconnected neurons trained rigorously for feature extraction and classification. CNN replaces the time-consuming traditional feature extraction and classification algorithms. CNN learns automatically to extract the features and to classify them. Because of its transfer and automated learning characteristics, CNN finds a humungous application in computer vision. The system is trained on one task and reused for other tasks. CNN is comprised of an input layer, multiple hidden layers, and an output layer. A hidden layer of CNN is composed of a convolutional layer (CL), a pooling layer (PL), and a fully connected (FC) layer. Extraction of high-level features is carried out by CL and PL. The classification task is governed by FC layers. The function of each layer is explained as follows.</p><p>1) CL: It is the key that decides the operation of CNN. The performance of CNN depends on the use of learnable filters. Spatial dimensionality of the kernels is usually small but spread along with the entire depth of the image. The 2-D convolution of the signal with two dimensions can be written as</p><formula xml:id="formula_1">(M * N)(m, n) = i, j M(i, j )N(m + i, n + j ). (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>The filters are usually moved by the number of pixels called stride (q). Sometimes, maintaining the size of the image zero padding may also be applied with size z.</p><p>For an image input with dimension, W m × H m × K m , where W m is the width, H m is the height, and K m are the number of channels. With K 0 filters each of size r × r , the output volume W 0 × H 0 × K 0 can be written as</p><formula xml:id="formula_3">W 0 = W m -r + 2z q + 1 H 0 = H m -r + 2z q + 1. (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>The operation of convolution is combined with an activation function. The activation function enhances the nonlinearity in the network. The most common activation function used is the rectified linear unit (ReLu). 2) PL: CL is succeeded by the PL also known as subsampling layer. The main objective of PL is to produce downsampled feature maps. It reduces the parameters and dimensions by keeping useful information. The PL also helps to regulate overfitting. It operates on each activation map by using max or mean functions. For J input maps, the output maps are generally smaller as given by</p><formula xml:id="formula_5">x l k = f α l k down x l-1 k + β l k (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where α l k and β l k are the multiplicative and additive bias terms and down(•) is the pooling function. The output of PL is given as an input to an FC layer.</p><p>3) FC Layer: Pooling layer is succeeded by an FC layer.</p><p>It is a feedforward neural network. FC converts a 2-D feature map into a 1-D feature map. The softmax layer converts the score into probabilities, and at last, based on some algorithm, the classification layer assigns a class to an object. Using the abovementioned layers, one can build their own CNN. The number of convolutional, pooling, and FC layers can be added or dropped until the desired performance of the network is obtained. With recent advances in CNN, many pretrained deep CNNs have been used for various machine learning problems. AlexNet, ResNet50, VGG16, VGG19, GoogleNet, and so on are some of the well-known pretrained transfer learning networks. These networks transfer the previously learned knowledge of one domain to another for feature extraction and classification. New images are used for training with fewer numbers as used in the previously trained data set. In this work, three benchmark CNNs, namely, AlexNet, ResNet50, and VGG16, are used for emotion recognition. The details of these networks can be found in <ref type="bibr" target="#b34">[35]</ref>. There is no standard CNN method available for the analysis and classification of EEG signals. The choice of CNN depends on the performance given by it. Many existing CNNs have a large number of layers. Complex architecture increases the number of learnable parameters significantly. Moreover, the time required for training, testing, and validation is higher for complex networks. The performance of CNN highly depends on the hyperparameters. By varying the filter size, stride, dropout, and so on, classification accuracy can be varied. Better accuracy may be achieved with fewer parameters and lesser complexity <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Motivated by this fact, a configurable CNN with a fewer number of a CL, pooling, and size of the FC layer is designed. The configurable network consists of four CL, two PL, a dropout, and two FC layers. The architecture of the proposed network can be modified according to the application. The number of CLs and PLs can be added or deleted as per user choice. Also, the number of learnable parameters required for the proposed architecture is less. The architecture of the configurable CNN is shown in Fig. <ref type="figure" target="#fig_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS</head><p>The traditional classification problems involve signal decomposition, feature extraction, feature selection, and classification. The performance of the conventional systems depends highly on parameters selected for decomposition and classification. The signal analysis and classification using conventional methods are time-consuming and laborious. With this motivation, an automatic and reliable classification of emotions is proposed in this work. EEG signals of four emotions elicited from the audio-video clips are used. The artifacts and noise are removed by filtering the EEG signals using a bandpass filter. The filtered 1-D EEG signals are converted into TFR using SPWVD. The images obtained are given as an input to three benchmark pretrained networks and configurable CNN with four CLs and two FC layers. Raw EEG signal is given to the tenth-order Butterworth filter. The passband frequency of 4-45 Hz is selected with a sampling frequency 256 Hz. The filtered signal obtained from the BPF is given as an input to TFR. In this methodology, SPWVD is used to convert the 1-D signal into a 2-D signal. The Kaiser window is used for reducing the cross terms in time and frequency domains. Too small window size may result in poor resolution and too large windows might increase the size of the image drastically. Hence, medium-size window with length 31 is chosen empirically. The window size is kept 2 n -1 for fast computation, where n is the number of bits. The TFR obtained from SPWVD is fed to AlexNet, ResNet50, VGG16, and proposed network. Multiple networks are employed as the performance of one learning algorithm can be overthrown by others due to lack of priori as stated in "No free lunch theorem" <ref type="bibr" target="#b37">[38]</ref>.</p><p>The common experimental platform is maintained for training and testing of all the networks; 70% data set is used for training the network and the rest is used for testing. Weight and bias learning rate is fixed to 20. Adam optimizer is used for scaling the learning rate for each weight of the neural network. The batch size and number of epochs are fixed to 50 and 10, respectively. The learning rate is fixed to 0.0001 and the validation frequency is set to 3. A total of 1100 iterations are carried out with 110 iterations per epoch. Table <ref type="table" target="#tab_0">I</ref> represents the confusion matrix obtained from AlexNet. The fear state is 96.91% accurately classified with very less misclassification of 2.38%, 0.35%, and 0.66% in   happy, relax, and sad states, respectively. Classification accuracy of happy, relax, and sad emotional state is 88.77%, 83.45%, and 95.09%, respectively.</p><p>The ResNet50 network consists of 50 CLs and single FCs. The filters size of ResNet50 is 1×1, 3×3, and 7×7. The input size of an image taken by this network is of 224 × 224. The resized images of 224 ×224 obtained by SPWVD are given as input. Accuracy and loss per iteration for ResNet50 are shown in Fig. <ref type="figure" target="#fig_7">6</ref>. The obtained accuracy of ResNet50 is 91.91% with the total time required for testing and validation is 3325 min and 50 s. The confusion matrix obtained using ResNet50 is shown in Table <ref type="table" target="#tab_0">II</ref>. As evident from the table, the classification accuracy of 95.70%, 87.04%, 90.99%, and 93.93% is obtained for fear, happy, relax, and sad states, respectively.</p><p>Another well-known and popular pretrained network, namely VGG16, is used to test the system performance. VGG16 takes input of image size 224 × 224. It consists of <ref type="bibr" target="#b15">16</ref> CLs and three FC layers with a filter size of 3 × 3. The classification accuracy of VGG16 is obtained as 92.71% with total time required for training and testing is 2320 min   and 11 s. Validation accuracy and loss are shown in Fig. <ref type="figure" target="#fig_8">7</ref>. The confusion matrix of VGG16 is shown in Table <ref type="table" target="#tab_2">III</ref>. Classification accuracy of 97.06% is obtained for fear with 1.62%, 0.56%, and 0.76% misclassification in happy, relax and sad states, respectively. Happy and relax states have a classification accuracy of 87.25% and 93.17%. Misclassification obtained for a fear state is 2.73%, 2.58%, and 1.21%. Misclassification for relax is 5.72% and 2.07%, misclassification for happy state is 2.68% and 1.97%, and misclassification for sad state is 4.30% and 2.94%. The classification accuracy of the sad state is 93.37%.</p><p>Finally, configurable CNN is used for testing the performance. It consists of four CLs, two PLs, and two FC layers    <ref type="table" target="#tab_3">IV</ref>; 96.71%, 86.08%, 93.83%, and 95.45% accurate classification are obtained for fear, happy, relax, and sad states, respectively. Misclassification for fear is 0.76%, 0.40%, and 0.30%. Misclassification obtained for happy state is 0.61%, 0.61%, and 3.44%.</p><p>Effectiveness of the proposed method is tested by evaluating different performance parameters. Table <ref type="table">V</ref> shows five performance parameters, namely accuracy, precision, Mathew's correlation coefficient (MCC), F1-score, and false-positive rate (FPR) obtained for different CNNs. For AlexNet, accuracy, precision, MCC, F1, and FPR of 90.98%, 91.11%, 87.85%, 0.9092, and 3.2% are obtained, respectively. ResNet50 and VGG16 provide the accuracy of 91.91% and 92.71%, the precision of 92.19% and 92.72%, the MCC of 89.21% and 90.17%, the F1-score of 0.9195 and 0.9268, and FPR of 2.82% and 2.55%, respectively. Configurable CNNs mark as the accuracy of 93.01%, the precision of 93.26%, MCC of 90.70%, F1-score of 0.9302, and FPR of 2.42%.</p><p>Deeper is better? To answer this question, a comparison of different networks is made in Table <ref type="table" target="#tab_0">VI</ref>. The number of CL in a configurable CNN is 4, which is fewer compared with benchmark networks. An FC layer with a fever number of neurons is used that reduces the computational complexity significantly. The CL is designed with a filter size of 3, 5, and 7 compared The accuracy of the configurable CNN is higher than all the benchmark networks used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION</head><p>The proposed system is tested for efficacy by comparing it with other existing methods, as shown in Table <ref type="table" target="#tab_5">VII</ref>. Their methods have been applied to the data set used in the proposed method. The same decomposition and classification techniques have been employed to test the system efficacy. Bajaj et al. <ref type="bibr" target="#b20">[21]</ref> used FAWT to decompose the signals into subbands. Six time-domain features elicited from eight subbands have been used as an input to different kernels of KNN. The accuracy achieved with weighted KNN reported highest with a value of 86.1%. The method proposed in <ref type="bibr" target="#b12">[13]</ref> used DWT to extract several features from the subbands. The features have been classified by using kNN. Their method managed to provide an 82.32% accurate system for emotion identification. Wang et al. <ref type="bibr" target="#b6">[7]</ref> extracted multiple features in the time and frequency domains. Six time-domain statistical features and five frequency-domain features have been selected with mRmR. These features have been classified by using kNN, MLP, and SVM. Their method achieved a mean accuracy 62.58%, 64.25%, 71.26% with kNN, MLP, and SVM on this data set, respectively. The features extracted using STFT with a Hanning window in the method proposed by Lin et al. <ref type="bibr" target="#b7">[8]</ref> have been classified by using SVM. Their model achieved an accuracy of 76.48% with the same data set. Another method proposed by Bajaj et al. <ref type="bibr" target="#b19">[20]</ref> employed TQWT. Their method uses eight subbands to evaluate time-domain features. Ten time-domain features have been extracted from the subbands of four frontal channels. The extracted features are given as an input to ELM that gives an accuracy of 87.1%. Taran et al. <ref type="bibr" target="#b18">[19]</ref> employed two-stage CIF. EMD and VMD have been used to filter the dominant subbands of the EEG signals.</p><p>This method used a single-channel feature evaluation. The extracted features have been classified by using four kernels of MC-LS-SVM. Mexican hat, polynomial, radial basis function, and Morlet kernel produce an accuracy of 86%, 88.66%, 88.78%, and 90.63%, respectively. The method proposed in this article uses four different CNN architectures. TFR obtained by using SPWVD is fed to AlexNet, ResNet50, VGG16, and configurable CNN. An accuracy of 90.98% is obtained by using AlexNet. Classification accuracy of 91.91% is accomplished when employing ResNet50. VGG16 architecture classifies 92.71% images accurately. The configurable-designed CNN with four CLs and two FC layers marks an accuracy of 93.01%. As evident from the table, pretrained AlexNet, VGG16, and ResNet50 architectures, and the configurable CNN outperform other states of the art. The advantages and limitations of the proposed method are listed as follows.</p><p>Advantages:</p><p>1) Reliability and simplicity.</p><p>2) Method is tunable as per the application.</p><p>3) Robust in terms of its scope with other transformation techniques and data sets. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Limitations: 1) Use of empirical parameters for signal processing and classification. 2) Testing and validation are performed on a single data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Various CNNs are examined in this article to categorize four emotions using EEG signals. Compared with the traditional approach, CNN has added edge in terms of automatic extraction and classification of features. The approach presented in this article uses the filtering technique and the SPWVD to transform time-domain EEG input signals into images. TFR obtained by SPWVD of four basic emotions, namely, fear, happy, relax, and sad, are given to four CNNs. Performance of three pretrained networks is AlexNet, ResNet50, and VGG16, and a configured CNN with four CLs and two FC layers are compared. The results obtained through these networks show that AlexNet offers quicker training and testing. VGG16 offers second-fastest training, whereas ResNet50 is slowest. Configurable CNN provides maximum precision over pretrained networks with significantly fewer learnable parameters. The comparative results demonstrate the superiority of the new approach over the existing methods. This methodology can be used in the development of EEG-based human-computer interface. An optimal selection of windows and their size can be used in the future to convert EEG signals into an image. Hyperparameter optimization can be explored to improve the performance of the system. In the near future, the proposed method will be tested on other data sets and different biopotential signals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Flowchart of the proposed framework.</figDesc><graphic coords="2,317.03,58.73,240.98,113.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Filtered EEG signals of four emotions.</figDesc><graphic coords="3,66.68,57.89,214.71,184.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The filtered EEG signals of fear, happy, relax, and sad are shown in Fig. 2. As seen from the figure, all the emotional states do not show any significant discriminative property of signals. Each signal contains a total of 2560 samples. There are four channels in total, as the bipolar montage has been used. Every channel of each class has 494 signals. A total of 1976 signals belong to each class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. TFR of EEG signals obtained by SPVWD. (a) Sad. (b) Happy. (c) Relax. (d) Fear.</figDesc><graphic coords="3,311.39,56.33,255.28,226.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Network architecture of configurable CNN.</figDesc><graphic coords="5,52.79,54.77,241.10,170.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>AlexNet is an eight-layer network with five CLs and three FC layers. AlexNet takes input images size with dimensions 227 × 227. Convolution and max pooling with local response normalization are performed in the first CL, 96 filters each with size 11 × 11 and max pooling of size 3 × 3 with a stride of 2. Second, CL composed of 256 receptive filters each of 5 × 5. The third and fourth layers comprise 384 feature maps each of 3 × 3 filters. The fifth layer has 296 filters each of size 3 × 3. Sixth and seventh are two FC layers succeeded by the dropout and softmax layer. The overall validation accuracy obtained by using AlexNet is 90.98%. Fig. 5 shows the training and validation values of accuracy and loss per iteration. The time required to reach the final iteration is 837 min and 55 s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Classification accuracy and loss of Alexnet.</figDesc><graphic coords="5,317.03,58.97,240.98,170.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Classification accuracy and loss of ResNet50.</figDesc><graphic coords="6,53.99,58.97,240.98,170.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Classification accuracy and loss of VGG16.</figDesc><graphic coords="6,53.99,265.85,240.98,170.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Classification accuracy and loss of configurable CNN.</figDesc><graphic coords="6,317.03,58.97,240.98,170.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>having a dropout of 50% with a filter size of 3 × 3, 5 × 5, and 7 × 7. This network takes input images with a size of 227 × 227 and Adam optimization for learning the weights. The training and validation accuracy per iteration is shown in Fig. 8. An accuracy of 93.01% is achieved having a total time of 2449 min and 43 s. The confusion matrix of configurable CNN is shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CONFUSION</head><label>I</label><figDesc>MATRIX OF ALEXNET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CONFUSION</head><label>III</label><figDesc>MATRIX OF VGG16</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV CONFUSION</head><label>IV</label><figDesc>MATRIX OF CONFIGURABLE CNN</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII PERFORMANCE</head><label>VII</label><figDesc>COMPARISON WITH RESPECT TO CLASSIFICATION ACCURACY WITH EXISTING STATE OF THE ART</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: University of Massachusetts Amherst. Downloaded on August 01,2020 at 02:03:02 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dr. Bajaj was a recipient of various reputed national and international awards. He has served as a Subject Editor for IET Electronics Letters from November 2018 to June 2020, where he is also serving as a Subject Editorin-Chief. He is also contributing as an active technical reviewer for leading International journals of IEEE, IET, Elsevier, and so on.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-subject EEG feature selection for emotion recognition using transfer recursive feature elimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Neurorobot</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2017-04">Apr. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech emotion recognition and intensity estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Science and Its Applications-ICCSA</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="406" to="413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coding, analysis, interpretation, and recognition of facial expressions</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="757" to="763" />
			<date type="published" when="1997-07">Jul. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotional state classification from EEG data using machine learning approach</title>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="94" to="106" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">EEG-based emotion recognition during watching movies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Int. IEEE/EMBS Conf. Neural Eng</title>
		<meeting>5th Int. IEEE/EMBS Conf. Neural Eng</meeting>
		<imprint>
			<date type="published" when="2011-04">Apr. 2011</date>
			<biblScope unit="page" from="667" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving BCI-based emotion recognition by combining EEG feature selection and kernel classifiers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="35" to="41" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">EEG-based emotion recognition using frequency domain features and support vector machines</title>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
		<editor>
			<persName><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Kwok</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="734" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">EEG-based emotion recognition in music listening</title>
		<author>
			<persName><forename type="first">Y.-P</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1798" to="1806" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time movie-induced discrete emotion recognition from EEG signals</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="550" to="562" />
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Internal emotion classification using EEG signal with sparse discriminative ensemble</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uzair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Cheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="40144" to="40153" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Arousal-valence recognition using CNN with STFT feature-combined image</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="134" to="136" />
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human emotion classification using wavelet transform and KNN</title>
		<author>
			<persName><forename type="first">M</forename><surname>Murugappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Anal</title>
		<meeting>Int. Conf. Pattern Anal</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="148" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Classification of human emotion from EEG using discrete wavelet transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Murugappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sazali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="390" to="396" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wavelet-based emotion recognition system using EEG signal</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frounchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1985" to="1990" />
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Emotion recognition from EEG using higher order crossings</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Petrantonakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Hadjileontiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Technol. Biomed</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="186" to="197" />
			<date type="published" when="2010-03">Mar. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A hybrid fuzzy cognitive map/support vector machine approach for EEG-based emotion classification using compressed sensing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="273" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human emotion classification from EEG signals using multiwavelet transform</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Pachori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Med. Biometrics</title>
		<meeting>Int. Conf. Med. Biometrics</meeting>
		<imprint>
			<date type="published" when="2014-05">May 2014</date>
			<biblScope unit="page" from="125" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Emotion recognition from EEG signals using multidimensional information in EMD domain</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioMed Res. Int</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page">8317357</biblScope>
			<date type="published" when="2017-08">Aug. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Emotion recognition from single-channel EEG signals using a two-stage correlation and instantaneous frequency-based filtering method</title>
		<author>
			<persName><forename type="first">S</forename><surname>Taran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bajaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="157" to="165" />
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Emotion classification using EEG signals based on tunable-Q wavelet transform</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Sri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y V S</forename><surname>Priyanka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Taran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bajaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Sci., Meas. Technol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="375" to="380" />
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Emotion classification using flexible analytic wavelet transform for electroencephalogram signals</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Taran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sengur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Health Inf. Sci. Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018-09">Sep. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-subject emotion recognition using flexible analytic wavelet transform from EEG signals</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Chopda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Pachori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors J</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2266" to="2274" />
			<date type="published" when="2019-03">Mar. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A novel EEG-based approach to classify emotions through phase space dynamics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Zangeneh</forename><surname>Soroush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maghooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Setarehdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal, Image Video Process</title>
		<imprint>
			<date type="published" when="2019-09">Sep. 2019</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1149" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">EEG-based emotion classification using deep belief networks</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-L</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo (ICME)</title>
		<meeting>IEEE Int. Conf. Multimedia Expo (ICME)</meeting>
		<imprint>
			<date type="published" when="2014-07">Jul. 2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">EEG-based emotion recognition using self-organizing map for boundary detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Khosrowabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Quek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Ang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Int. Conf. Pattern Recognit</title>
		<meeting>20th Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010-08">Aug. 2010</date>
			<biblScope unit="page" from="4242" to="4245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Single trial classification of EEG and peripheral physiological signals for recognition of emotions induced by music videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
		<editor>Brain Informatics, Y. Yao, R. Sun, T. Poggio, J. Liu, N. Zhong, and J. Huang</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="89" to="100" />
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Asymmetric spatial pattern for EEG-based emotion detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keng Ang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Toward an EEG-based recognition of music liking using time-frequency analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Hadjidimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Hadjileontiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3498" to="3510" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">EEG-based emotion recognition using quadratic time-frequency distribution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alazrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Homoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alwanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">2739</biblScope>
			<date type="published" when="2018-08">Aug. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multichannel EEG-based emotion recognition via group sparse canonical correlation analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cognit. Develop. Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="290" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Emotion recognition based on EEG features in movie clips with channel selection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Özerdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Polat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Informat</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="241" to="252" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Classifying different emotional states by means of EEG-based functional connectivity patterns</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Smoothed pseudo Wigner-Ville distribution as an alternative to Fourier transform in rats</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>De Souza Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Custaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frutoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Somody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gharib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-O</forename><surname>Fortrat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomic Neurosci</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="258" to="267" />
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The history began from AlexNet: A comprehensive survey on deep learning approaches</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Zahangir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01164</idno>
		<ptr target="https://arxiv.org/abs/1803.01164" />
		<imprint>
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5 MB model size</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07360</idno>
		<ptr target="https://arxiv.org/abs/1602.07360" />
		<imprint>
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<ptr target="https://arxiv.org/abs/1510.00149" />
		<imprint>
			<date type="published" when="2015-10">Oct. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The lack of a priori distinctions between learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1341" to="1390" />
			<date type="published" when="1996-10">Oct. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
