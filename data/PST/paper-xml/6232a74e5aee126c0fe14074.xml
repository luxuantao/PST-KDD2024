<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-16">16 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Azure Cognitive Services Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Azure Cognitive Services Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Azure Cognitive Services Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Azure Cognitive Services Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
							<email>siqisun@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Azure Cognitive Services Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Azure Cognitive Services Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
							<email>chezhu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Azure Cognitive Services Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
							<email>nzeng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Azure Cognitive Services Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-16">16 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.08773v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retrieval-based methods have been shown to be effective in NLP tasks via introducing external knowledge. However, the indexing and retrieving of large-scale corpora bring considerable computational cost. Surprisingly, we found that REtrieving from the traINing datA (REINA) only can lead to significant gains on multiple NLG and NLU tasks. We retrieve the labeled training instances most similar to the input text and then concatenate them with the input to feed into the model to generate the output. Experimental results show that this simple method can achieve significantly better performance on a variety of NLU and NLG tasks, including summarization, machine translation, language modeling, and question answering tasks. For instance, our proposed method achieved state-of-the-art results on XSum, Big-Patent, and CommonsenseQA. Our code is released. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In natural language processing, retrieval-based methods work by fetching textual information related to the input from large corpora. The model then takes both the input and retrieved results as input to generate results. This can often improve the performance as the model is exposed to related knowledge not present in the input. As a result, retrieval-based methods have been successfully applied in many tasks such as open-domain question answering <ref type="bibr" target="#b6">(Chen et al., 2017)</ref>, language modeling <ref type="bibr" target="#b12">(Guu et al., 2018;</ref><ref type="bibr" target="#b17">Khandelwal et al., 2020)</ref> and machine translation <ref type="bibr" target="#b16">(Khandelwal et al., 2021)</ref>. However, these methods require building an index of large-scale corpus, and the retrieval leads to a significant computational burden. For example, the kNN-MT model for machine translation has a generation speed two orders of magnitude slower than traditional MT models <ref type="bibr" target="#b16">(Khandelwal et al., 2021)</ref>. On the other hand, in the supervised learning setting, the text most similar in distribution to the data in inference is the training data. Thus, we explore whether retrieving from the training data, which is usually much smaller than a large-scale corpus, can help improve the performance. Specifically, we first index a task's labeled training data as input-label pairs. Then, during both training and testing, we retrieve the input-label pairs most similar to the current input<ref type="foot" target="#foot_1">2</ref> . Finally, we concatenate the retrieved training pairs with the input and feed it into the model. An overview of our method is shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>We note that our method is similar to recent works in prompt learning <ref type="bibr" target="#b4">(Brown et al., 2020;</ref><ref type="bibr" target="#b25">Liu et al., 2021)</ref>, where a set of labeled data is carefully chosen based on the input and then included in the prompt for few-shot learning. Our method also bears a resemblance to non-parametric instancebased learning <ref type="bibr" target="#b11">(Gu et al., 2018)</ref>. However, a critical difference is that we focus on the supervised learning setting, where the model parameters are fine-tuned to learn from given examples to achieve much higher performance than few-shot learning or non-parametric methods.</p><p>In the experiments, we evaluate our method on four popular types of NLP tasks: summarization, language modeling, machine translation, and question answering. We find that i) after integrating REINA, we can achieve significantly better performance on these tasks, 11 datasets in total, than models with different pre-trained models; ii) REINA leads to SOTA performance on the datasets of XSum, CommonsenseQA (Leaderboard No.1), and BigPatent; iii) REINA can scale up more easily by leveraging more labeled data from other datasets via retrieval, outperforming baselines which is trained on the same set of data. iv) the results on 3 summarization tasks show that BART-base with REINA rivals BART-large, which contains twice more parameters now.</p><p>The effectiveness of our approach on summarization tasks provides insights into the core of supervised learning. Even with hundreds of millions of parameters, a model cannot memorize all the patterns in the training data. Thus, recapturing related training data as a side-by-side reminder can explicitly provide needed information to enhance the model's performance at inference. It also points out that instead of building models of ever increasing sizes, we can make a decent-size model output high-quality results by leveraging those training data that resemble the instance at hand. This can significantly reduce the computational cost while achieving a similar or better performance of a megasized model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Retrieval-based Methods Even a pre-trained model as large as <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> cannot remember everything, and it is important to leverage information retrieval to collect external knowledge to solve different NLP tasks. There are two types of representations for retriever: bag-ofword (BOW) based sparse representation <ref type="bibr" target="#b6">(Chen et al., 2017)</ref> and dense representation from neural networks <ref type="bibr" target="#b15">(Karpukhin et al., 2020)</ref>.</p><p>For the sparse representation, as the method is based on BOW and usually rule-based score, such as BM25, is used for ranking, it can be easily adapted to a general large-scale search. This method has also been widely explored to solve open domain question answering <ref type="bibr" target="#b6">(Chen et al., 2017;</ref><ref type="bibr" target="#b39">Wang et al., 2018;</ref><ref type="bibr" target="#b24">Lin et al., 2018)</ref> and Machine Translation <ref type="bibr" target="#b11">(Gu et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense</head><p>representation based retrieval (DPR) <ref type="bibr" target="#b15">(Karpukhin et al., 2020)</ref> is the most widely explored area in recent years. Dense representations come from encoders, such as Transformer, trained with task-specific data. And these methods can achieve better recall performance than sparse representation on different tasks, such as open domain question answering <ref type="bibr" target="#b15">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b13">Guu et al., 2020;</ref><ref type="bibr" target="#b42">Yu et al., 2021)</ref>, knowledge-grounded generation <ref type="bibr" target="#b44">(Zhang et al., 2021)</ref>, and machine translation <ref type="bibr" target="#b5">(Cai et al., 2021)</ref>. One drawback of DPR is that it cannot process longer documents, usually less than 128 tokens <ref type="bibr" target="#b15">(Karpukhin et al., 2020)</ref>. Another drawback is that it needs parallel data for model training on specific tasks.</p><p>Considering the generalization and efficiency of sparse representation, in this paper, we use BM25 score <ref type="bibr" target="#b34">(Robertson and Zaragoza, 2009;</ref><ref type="bibr" target="#b35">Sch?tze et al., 2008)</ref> to retrieve from the training data, and our method is more flexible with no requirement of parallel data for model training. Compared to nonparametric systems guided by search engine <ref type="bibr" target="#b11">(Gu et al., 2018;</ref><ref type="bibr" target="#b17">Khandelwal et al., 2020)</ref>, our proposed method is based on supervised learning and is more general. <ref type="bibr" target="#b22">Lewis et al. (2021)</ref> is related to our work by retrieving related questions from pre-built largescale question-answer pairs. However, our method doesn't need addition data augmentation method, and we have successfully applied REINA to a wide range of downstream tasks, including summarization, question answering, machine translation and language modeling.</p><p>Prompt Engineering With the success of largescale language models (Brown et al., 2020) on fewshot learning, prompt engineering comes to be a popular research direction. The idea is to prepend several labeled instances to the input sequence and then conduct the classification or generation. <ref type="bibr" target="#b25">Liu et al. (2021)</ref> proposes to prepend the most related labeled data as prompt to help fewshot inference. Li and Liang (2021) optimizes the prompt in continuous space. Motivated by these works where a good labeled prompt can help fewshot learning, we also prepend/append the most similar labeled training data for all the data in training, validation, and test set. However, different from prompt learning, we focus on supervised learning settings. For language modeling, we prepend the retrieved data to the query data, and append the retrieved data to the query for all the other tasks. After concatenation, we will directly feed them into Transformers, either Seq2Seq or Encoder-only frameworks, for text generation and answering selection. As we focus on the question answering tasks requiring commonsense reasoning, we have another version of index integrating knowledge graph for more precise retrieval. K: external knowledge from ConceptNet and Wiktionary, src: source language, tgt: target language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we will introduce the details of our proposed method. Briefly, given the input, we first retrieve the most matched instances with labels from the training data. We then concatenate them with the input sequence to feed into the model for generating the output. An overview of the whole method is shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Retrieval-based Methods</head><p>A retrieval-based method collects information most similar to the input from a corpus and then combines it with the input to feed into the NLP model. Suppose we index the corpus into a list of key-value pairs, i.e. C = {(k i , v i )}. Then, given the input x, the retrieval engine E matches it with all keys and returns the top K most similar keys to the query together with their values:</p><formula xml:id="formula_0">{(k i 1 , v i 1 ), ..., (k i K , v i K )} = E(x|C)<label>(1)</label></formula><p>In this work, we build the retrieval engine based on the widely used BM25 score <ref type="bibr" target="#b35">(Sch?tze et al., 2008)</ref>. We choose BM25 over dense representation mainly for its faster speed. Then, these retrieved results are combined with the input x to feed into the NLP model M to generate the output O:</p><formula xml:id="formula_1">O = M(f (x, {(k i 1 , v i 1 ), ..., (k i K , v i K )}) (2)</formula><p>Here, the combination function f can be concatenation, e.g.</p><formula xml:id="formula_2">f (x, {(k i 1 , v i 1 ), ..., (k i K , v i K )}) = [x; v i 1 ; ...; v i K ].</formula><p>As data in different tasks is organized in different formats with varying lengths, we will introduce how we define different combination functions f for various tasks in the follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Retrieval from Training Data ( REINA )</head><p>As retrieval from a large corpus is computationally costly, we propose to retrieve from the labeled training data. In other words, we directly adopt the training data T = {(x 1 , y 1 ), ..., (x N , y N )} as the indexed corpus C, where x i is the input and y i is the ground-truth label.</p><p>Given an input x, the top K retrieved training instances with labels are combined with x as input to the model M, i.e., M(f (x, {(x i 1 , y i 1 ), ..., (x i K , y i K )}. Both training and inference take this retrieve-combine-generate scheme. Note that during training, as the input x is already indexed, we filter it from the retrieval results to avoid data leakage. Now, we introduce how we define the keys, values, and the combination function for different NLP tasks.</p><p>Summarization is to generate a summary for a given document. We first build an index for the document-summary pairs in the training data, where a document is the key and its summary is the value. Given a document x, we search for the most similar documents in the index. As documents are usually quite long, the combination function only keeps the values (summaries), i.e., f summ (x, {(x i 1 , y i 1 ), ...,</p><formula xml:id="formula_3">(x i K , y i K )}) = [x; y i 1 ; ...; y i K ].</formula><p>Language Modeling (LM) generates the probability of a given sequence of words. Typically, a Left-to-Right language model <ref type="bibr" target="#b8">(Dong et al., 2020)</ref> is trained on chunked sequences with an attention mask. In this paper, we use Seq2Seq based approach, i.e., given a context chunk, we predict the next chunk of text.</p><p>In detail, we first chunk all the text in the training data. The IR index is built with one chunk C i as the key x i and its next chunk C i+1 as the value y i . Given a chunk x, we look for the most similar keys in the index and prepend their corresponding next chunks to x, i,e., f LM (x, {(x i 1 , y i 1 ), ...,</p><formula xml:id="formula_4">(x i K , y i K )}) = [y i 1 ; ...; y i K ; x].</formula><p>Machine Translation is to translate text from the source language S to the target language T . We define the key to be the sentence in S and the value to be its translation in T . To keep the sequence short and speed up the training process, we only concatenate the retrieved text in target language: f M T (x, {(x i 1 , y i 1 ), ...,</p><formula xml:id="formula_5">(x i K , y i K )}) = [x; y i 1 ; ...; y i K ].</formula><p>Question Answering We mainly consider multiple-choice question answering, where commonsense knowledge is also required to reach the correct answer. For each question x i , there is a correct choice y i and several distractive candidate choices. We index the concatenation of the question and the corresponding ground-truth choice. For a new question x, the model is given several choices c 1 , ..., c M . We concatenate x with each choice c i as the query and retrieve related training instances: {(x i 1 , y i 1 ), ..., (x i K , y i K )} = E(x; c i |C). The combination function f concatenates both retrieved question and answers with the input:</p><formula xml:id="formula_6">f QA ((x, c i ), {(x i 1 , y i 1 ), ..., (x i K , y i K )}) = [x; c i ; x i 1 ; y i 1 ; ...; x i K ; y i K ].</formula><p>Then, the model predicts a score representing how likely c i is the correct choice to x.</p><p>As the task requires commonsense knowledge, we build another version of index integrating commonsense knowledge. We follow the strategy from <ref type="bibr">(Xu et al., 2021)</ref> and extract the knowledge from ConceptNet <ref type="bibr" target="#b37">(Speer et al., 2017)</ref> and Wiktionary<ref type="foot" target="#foot_2">3</ref> for the concepts in the question and choices. For each question x and choice c, we use string match to find corresponding entities in Concept-Net:</p><formula xml:id="formula_7">E (x) = {e (x) 1 , ..., e (x)</formula><p>nx } appears in the question, and</p><formula xml:id="formula_8">E (c) = {e (c) 1 , ..., e<label>(c)</label></formula><p>nc } appears in the answer. To find the most relevant concept, we choose the concept with maximum length as the question and answer concept. We find the definition of the chosen concepts from Wiktionary. To find relations in ConceptNet, we find edges that connects question and answer concepts: R = {(e 1 , r, e 2 )|e 1 ? E (x) , e 2 ? E (c) , (e 1 , e 2 ) ? KG}. Here KG is Con-ceptNet and r is a relation (e.g., AtLocation). We concatenate the Wiktionary definitions and Con-ceptNet relations R to form the knowledge, K, for a question. The knowledge K is included both in the query and index. Thus, the retrieval process becomes:</p><formula xml:id="formula_9">{(x i 1 , c i 1 , K i 1 ), ..., (x i K , y i K , K i K )} = E(x; c i ; K|C).</formula><p>The combination function f concatenates retrieved questions and answers with the input:</p><formula xml:id="formula_10">f QAK ((x, c i ), E(x; c i ; K|C)) = [x; c i ; x i 1 ; y i 1 ; ...; x i K ; y i K ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Training and Inference</head><p>After concatenating the input with the retrieved data from the training corpus, we feed the new sequence into the Seq2Seq framework for generation tasks and the encoder-only framework for question answering tasks. During training, as it will also retrieve the exact golden label, we filter it directly. During inference, we will not filter any retrieved information, as all the retrieve data only come from training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we will introduce more details about experiments and the corresponding analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We evaluate REINA on 4 different tasks with 12 datasets as shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Summarization We evaluate our method on 5 summarization datasets: 1) XSum <ref type="bibr" target="#b31">(Narayan et al., 2018)</ref> Machine Translation We evaluate our method on the translation of English-German and English-Turkish in both directions from WMT16 <ref type="bibr" target="#b3">(Bojar et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answering</head><p>We have 3 question answering datasets to evaluate our method: 1) Com-monsenseQA (CSQA, <ref type="bibr" target="#b38">Talmor et al., 2019</ref>) is a dataset for commonsense multi-choice question answering. The questions are generated based on commonsense knowledge base, ConceptNet. 2) Physical IQA (PIQA, <ref type="bibr" target="#b2">Bisk et al., 2020)</ref> is to answer questions requiring physical commonsense reasoning. 3) Abductive NLI (aNLI, <ref type="bibr" target="#b1">Bhagavatula et al., 2020</ref>) is a multiple-choice question answering task for choosing the more likely explanation. All these tasks are challenging by requiring commonsense knowledge to reach the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">REINA Details</head><p>For the task of summarization, instead of directly retrieving the most relevant summary <ref type="bibr" target="#b0">(An et al., 2021)</ref>, we find the most relevant documents by BM25 score and then leverage the corresponding summaries. Compared to the dense passage retrieval based method, our method can handle the long document retrieval and does not need to train. Moreover, REINA is easier to scale up. We also consider joint training baseline on Summarization tasks. Our setting is to test how other datasets can help improve XSum. For REINA, we build index on summarization datasets from different sources.</p><p>During model training, we will only train models with the XSum dataset along with retrieved data appended to the documents.   Table <ref type="table">2</ref>: Summarization results. In the top section, we report the results from PEGASUS <ref type="bibr" target="#b43">(Zhang et al., 2020)</ref> paper. In the bottom, we reproduce three strong baselines with PEGASUS and BART <ref type="bibr" target="#b21">(Lewis et al., 2020)</ref>, and show our REINA initialized by the same pre-trained models for fair comparison. The bolded numbers show the SOTA performance and the underlined numbers show the best performance with BART initialization. PEGASUS: PEGASUS-large, B: BART-base, L: BART-large, R-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BigPatent</head><formula xml:id="formula_11">XSum WikiHow Multi-News NEWSROOM R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L</formula><formula xml:id="formula_12">1: Rouge-1, R-2: Rouge-2, R-L: Rouge-L XSum R-1 R-2 R-L BART (XSum)</formula><p>44.7 21.6 36.5 BART (XSum+CNN) 44.6 21.6 36.9 REINA (XSum) 46.5 24.1 38.6 REINA (XSum+CNN) 47.5 25.2 39.5 REINA (XSum+NR) 47.5 24.9 39.4 REINA (XSum+160G) 47.7 25.1 39.5 For language modeling task, instead of working on word-level retrieval by KNN <ref type="bibr" target="#b17">(Khandelwal et al., 2020)</ref>, we chunk all the training data. During training, besides the retrieved chunks, we will also include the context of the query chunk to generate next chunk. Compared to KNN-LM <ref type="bibr" target="#b17">(Khandelwal et al., 2020)</ref>, REINA only needs retrieval once per chunk which is much more efficient.</p><p>For multi-choice question answering, we build two types of indexes with or without external knowledge from ConceptNet and Wiktionary. For the query, the concatenation of question and one candidate answer, we also have two versions, with or without knowledge. After adding knowledge, there would be more word overlaps when key concept words between questions are matched. The retrieved information will be treated as either a prompt or additional knowledge to encode together and then predicts the answer probability of each candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimization Details</head><p>Our information retrieval is based on Lucene Index<ref type="foot" target="#foot_3">4</ref> . Our model training is based on Transformers library<ref type="foot" target="#foot_4">5</ref> . All our experiments are based on 8-GPU machines.</p><p>For summarization tasks, we initialized the model with three types of pre-trained models, PEGASUS-large <ref type="bibr" target="#b43">(Zhang et al., 2020)</ref>, BART-base, and BART-large <ref type="bibr" target="#b21">(Lewis et al., 2020)</ref>. Optimization is based on AdamW <ref type="bibr" target="#b27">(Loshchilov and Hutter, 2019)</ref>. We tune the hyper-parameters from learning rate {2e-05, 5e-05, 7e-05}, and set dropout 0.1, batch size 32. For both baseline and our method, we set the maximal length of the input sequence to be 1024. We use the original document to generate summary in baselines. For REINA, we set the maximal length of the original document 600 and then append the top-5 retrieved summaries from training data.</p><p>For language modeling tasks, we initialized the model with BART-base and BART-large. We set the number of words in each chunk to 128 for Wiki-Text103 and 64 for WikiText2. For each chunk generation, we set the context length of baseline methods 1024. For our method, we set the context 512 and prepend the retrieved text. The maximal length of the concatenated sequence is 1024. We For machine translation tasks, we initialized the model with mBART-large <ref type="bibr" target="#b26">(Liu et al., 2020)</ref>. We follow the hyper-parameter setting from the original paper with Adam optimizer, dropout 0.3, label smoothing 0.2, warm-up steps 2500, maximum learning rate 3e-05, and training updates 40K in total.</p><p>For question answering datasets, our method is based on DeBERTa <ref type="bibr" target="#b14">(He et al., 2021)</ref> with 1.5B parameters. We use optimizer AdamW <ref type="bibr" target="#b27">(Loshchilov and Hutter, 2019)</ref> with learning rate 3e-06, batch size 8. As the datasets requiring commonsense reasoning, we also leverage knowledge bases, Con-ceptNet and Wiktionary, in REINA .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiment Results</head><p>Our experiment results on the summarization tasks are shown in Table <ref type="table">2</ref>. Our evaluation metric is based on Rouge-1/2/L scores, same as PEGA-SUS <ref type="bibr" target="#b43">(Zhang et al., 2020)</ref>. We have a broad experiment on 5 datasets, ranging from single document summarization (XSum) to multi-document summarization (Multi-News), from news domain to wiki knowledge (WikiHow) and patent (Big-  <ref type="bibr" target="#b26">(Liu et al., 2020)</ref>. REINA is initialized by mBART for fair comparison. The evaluation metric is based on SacreBLEU. Source and target languages are concatenated by "2". tr: Turkish, de: German, en: English.</p><p>Patent) domains. We re-run all of our baseline methods. Based on the experiment results, we find that REINA can significantly boost the baselines initialized with different pre-trained models, such as PEGASUS, BART-base, and BART-large, on all 5 datasets. Besides, our method with BART-large can achieve state-of-the-art performance on XSum and BigPatent datasets. Moreover, we find REINA can help base models beat larger models. For example, REINA (BART-base) is better than both PEGASUS-LARGE and BART-large on BigPatent and WikiHow datasets. We also evaluate the ability of REINA on learning from more related datasets. Our experiment results are shown in Table <ref type="table" target="#tab_3">3</ref>. The evaluation is conducted on XSum test set and we use three related data sources from CNN/Dailymail, NEWSROOM, and a 160G raw-text corpus<ref type="foot" target="#foot_5">6</ref> . Based on the experi- For summarization tasks, we will only append the document with the retrieved summaries. For CommonsenseQA, we will append the golden QA pairs to the question. The golden answer is "warm room". REINA 1/2 refers to different retrieved data. ments, we can see that simply training the model on merged dataset (XSum + other sources) doesn't lead to any gains. However, after adding one additional data source to build index and applying REINA, there's 1% improvement in Rouge scores<ref type="foot" target="#foot_6">7</ref> . Overall, our REINA can effectively leverage the most relevant data from additional datasets while being trained only on the target task.</p><p>For question answering tasks, our results are shown in Table <ref type="table">4</ref>. We test REINA on three datasets, where commonsense knowledge is usually required to answer the question. Thus we first verify whether we need external knowledge during the retrieval. According to the experiments, we find that directly retrieving the labeled data without knowledge works best for CommonsenseQA dataset, but involving knowledge can help on aNLI and PIQA datasets. And REINA can significantly improve our baselines with DeBERTa on all the datasets. Moreover, after submitting our best results to the corresponding leaderboards, REINA achieves state of the art on CommonsenseQA dataset (Leaderboard No.1) and beat strong baselines on aNLI and PIQA datasets.</p><p>Our evaluation of language modeling is shown in Table <ref type="table">5</ref>. Our method can achieve significant improvement on WikiText103 dataset over both BART-base and BART-large baselines. However, it cannot lead to better performance on WikiText2. One reason may be that WikiText2 is a much smaller dataset, and it's hard for REINA to re-trieve the most related text. Besides, we also find Seq2Seq model can be a very strong baseline which means we can leverage more pre-trained models such as PEGASUS, T5 <ref type="bibr" target="#b33">(Raffel et al., 2030)</ref>, and BART, for language modeling in future work. And Seq2Seq frame would be more flexible to integrate external knowledge to boost performance further.</p><p>For machine translation, we make use of the datasets from WMT16. We select one low-resource language, Turkish-English, and one rich-resource, German-English, for REINA evaluation, as shown in Table <ref type="table" target="#tab_5">6</ref>. We re-implement mBART baseline for translation in both directions. To make a fair comparison, REINA is also based on mBART. We can find that REINA can further boost performance under three settings, translating English to Turkish, Turkish to English, and English to German.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Further Analysis</head><p>We show a case study on the data retrieved by REINA . We list two cases from XSum and Com-monsenseQA dev sets. From the case on summarization task, we can find that the first retrieved summary from training set, REINA 1, shows the same point of "security concerns" as the golden summary. And the other case on multi-choice question answering, REINA 1 suggests that the sun can warm up a place that shares the same commonsense knowledge to answer the question. After, although we cannot visualize how the neural encoders work by leveraging the retrieved data, we have shown that the data from REINA have very strong correlation with the golden labels.</p><p>In this paper, we propose a simple and effective method to fully make use training dataset. Our proposed method is general and can be easily integrated into different models on different tasks. We prove that REINA can effectively improve baseline performance on 11 datasets covering summarization, language modeling, machine translation, and question answering tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: REINA pipeline of model training/inference with retrieval from training data. Filter only happens at training, as the same training sample will be retrieved from the index. For each instance, we concatenate the input with the retrieved content, i.e., data and/or labels, for model training and inference.</figDesc><graphic url="image-1.png" coords="1,317.06,212.60,196.43,104.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model training with retrieval from the training data ( REINA ). (a) Index on the training data and data retrieval for 4 different tasks. Box in blue is the query or the input sequence to encode. Box in green is the retrieved text. (b-e) Leveraging retrieved data for model training with different structures.For language modeling, we prepend the retrieved data to the query data, and append the retrieved data to the query for all the other tasks. After concatenation, we will directly feed them into Transformers, either Seq2Seq or Encoder-only frameworks, for text generation and answering selection. As we focus on the question answering tasks requiring commonsense reasoning, we have another version of index integrating knowledge graph for more precise retrieval. K: external knowledge from ConceptNet and Wiktionary, src: source language, tgt: target language.</figDesc><graphic url="image-2.png" coords="3,70.87,70.86,453.55,306.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statics of the evaluation datasets. The table shows the number of data in training, dev, and test sets.As we treat the language model as a Seq2Seq problem, the number here is the chunked sequences, each of which contains 64 words for WikiText2 and 128 words for WikiText103.</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>Train Dev Test</cell></row><row><cell></cell><cell>Multi-News</cell><cell>45k 5.6k 5.6k</cell></row><row><cell>Summar-ization</cell><cell>WikiHow XSum NEWSROOM</cell><cell>168k 6k 6k 204k 11k 11k 993k 108k 108k</cell></row><row><cell></cell><cell>BigPatent</cell><cell>1,207k 67k 67k</cell></row><row><cell>Language</cell><cell>WikiText2</cell><cell>32k 3.3k 3.8k</cell></row><row><cell>Modeling</cell><cell>WikiText103</cell><cell>801k 1.7k 1.9k</cell></row><row><cell>Machine</cell><cell cols="2">WMT16 (en-tr) 205k 1k 3k</cell></row><row><cell>Translation</cell><cell cols="2">WMT16 (en-de) 4,548k 2.2k 3k</cell></row><row><cell>Question Answering</cell><cell>CSQA PIQA aNLI</cell><cell>9.7k 1.2k 1.1k 16k 1.8k 3.4k 170k 1.5k 3.0k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on XSum test set with training data scale up. BART is jointly trained with datasets in bracket. REINA is trained with XSum documentsummary pairs, but the index is built on the datasets in bracket. CNN: CNN/Dailymail dataset, NR: NEWS-ROOM dataset, 160G: BART pre-training corpus.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Machine translation on WMT16. We compare with baselines XLM<ref type="bibr" target="#b20">(Lample and Conneau, 2019)</ref> and mBART</figDesc><table><row><cell></cell><cell></cell><cell cols="3">WikiText103 WikiText2</cell></row><row><cell cols="2">Transformer-XL</cell><cell>18.30</cell><cell></cell><cell>-</cell></row><row><cell>kNN-LM</cell><cell></cell><cell>15.79</cell><cell></cell><cell>-</cell></row><row><cell>GPT-2</cell><cell></cell><cell>17.48</cell><cell></cell><cell>18.34</cell></row><row><cell>BART-Base</cell><cell></cell><cell>15.88</cell><cell></cell><cell>20.41</cell></row><row><cell>REINA (B)</cell><cell></cell><cell>14.76</cell><cell></cell><cell>20.78</cell></row><row><cell>BART-Large</cell><cell></cell><cell>12.10</cell><cell></cell><cell>15.11</cell></row><row><cell>REINA (L)</cell><cell></cell><cell>11.36</cell><cell></cell><cell>15.62</cell></row><row><cell cols="5">Table 5: Language modeling results. The evaluation</cell></row><row><cell cols="5">metric is perplexity (PPL). The top part of the table</cell></row><row><cell cols="5">comes from the original papers, Transformer-XL (Dai</cell></row><row><cell cols="5">et al., 2019), kNN-LM (Khandelwal et al., 2020), GPT-</cell></row><row><cell cols="5">2 (Radford et al., 2019). The bottom part is our im-</cell></row><row><cell cols="5">plementation with fair comparison. B: BART-base, L:</cell></row><row><cell>BART-large</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">WMT16</cell><cell></cell></row><row><cell></cell><cell cols="4">en2tr tr2en en2de de2en</cell></row><row><cell>XLM</cell><cell>-</cell><cell>-</cell><cell>26.4</cell><cell>34.3</cell></row><row><cell cols="3">mBART 18.4 23.1</cell><cell>32.6</cell><cell>37.0</cell></row><row><cell>REINA</cell><cell cols="2">18.8 23.6</cell><cell>32.9</cell><cell>37.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Examples from dev sets and the corresponding labeled data retrieved from training set. The top case comes from a summarization task, XSum. The bottom case comes from a question answering task, CommonsenseQA.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/microsoft/REINA</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>During training, we exclude the training instance itself from the retrieval results to avoid data leakage.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://www.wiktionary.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://lucene.apache.org/pylucene/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/huggingface/transformers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>For the 160G data, we treat the first sentence as summary and the rest as document.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>In our experiments, we follow<ref type="bibr" target="#b40">Xu and Durrett (2021)</ref> by ignoring the retrieved data if there are over three 7-gram overlap between retrieved summary and golden summary.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07943</idno>
		<title level="m">Retrievalsum: A retrieval enhanced framework for abstractive summarization</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<title level="m">Abductive commonsense reasoning. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2016 conference on machine translation</title>
		<author>
			<persName><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Jimeno Yepes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Conference on Machine Translation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural machine translation with monolingual translation memory</title>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huayang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-news: A largescale multi-document summarization dataset and abstractive hierarchical model</title>
		<author>
			<persName><forename type="first">Irene</forename><surname>Alexander R Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyi</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Dragomir R Radev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. North American Chapter of the</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Grusky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Naaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Search engine guided neural machine translation</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating sentences by editing prototypes</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>TACL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Realm: Retrievalaugmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations (ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nearest neighbor machine translation</title>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generalization through memorization: Nearest neighbor language models</title>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Mahnaz</forename><surname>Koupaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09305</idno>
		<title level="m">Wikihow: A large scale text summarization dataset</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics (ACL</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Paq: 65 million probably-asked questions and what you can do with them</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<title level="m">Prefix-tuning: Optimizing continuous prompts for generation. Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Denoising distantly supervised open-domain question answering</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06804</idno>
		<title level="m">What makes good in-context examples for gpt-3? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics (TACL</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<title level="m">Decoupled weight decay regularization. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<title level="m">Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark. AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<title level="m">Pointer sentinel mixture models. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGNLL Conference on Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>OpenAI blog</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2030">2030</date>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bigpatent: A large-scale dataset for abstractive and coherent summarization</title>
		<author>
			<persName><forename type="first">Eva</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI conference on artificial intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
	<note>North American Chapter</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">R 3: Reinforced ranker-reader for open-domain question answering</title>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerry</forename><surname>Tesauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Dissecting generation modes for abstractive summarization models via ablation and attribution</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fusing context into knowledge graph for commonsense question answering</title>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question answering</title>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04330</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Joint retrieval and generation training for grounded text generation</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.06597</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pretraining text-to-text transformers for concept-centric common sense</title>
		<author>
			<persName><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Ho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kiran Selvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
