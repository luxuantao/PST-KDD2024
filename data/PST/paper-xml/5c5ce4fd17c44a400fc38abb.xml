<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Super-Resolution as a Defense Against Adversarial Attacks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-02">2 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aamir</forename><surname>Mustafa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Canberra</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Salman</forename><forename type="middle">H</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Data61-CSIRO</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Canberra</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Super-Resolution as a Defense Against Adversarial Attacks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-02">2 Sep 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1901.01677v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adversarial attacks</term>
					<term>gray-box setting</term>
					<term>CNNs</term>
					<term>image super-resolution</term>
					<term>image denoising</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks have achieved significant success across multiple computer vision tasks. However, they are vulnerable to carefully crafted, human-imperceptible adversarial noise patterns which constrain their deployment in critical security-sensitive systems. This paper proposes a computationally efficient image enhancement approach that provides a strong defense mechanism to effectively mitigate the effect of such adversarial perturbations. We show that deep image restoration networks learn mapping functions that can bring off-the-manifold adversarial samples onto the natural image manifold, thus restoring classification towards correct classes. A distinguishing feature of our approach is that, in addition to providing robustness against attacks, it simultaneously enhances image quality and retains models performance on clean images. Furthermore, the proposed method does not modify the classifier or requires a separate mechanism to detect adversarial images. The effectiveness of the scheme has been demonstrated through extensive experiments, where it has proven a strong defense in gray-box settings. The proposed scheme is simple and has the following advantages: (1) it does not require any model training or parameter optimization, (2) it complements other existing defense mechanisms, (3) it is agnostic to the attacked model and attack type and (4) it provides superior performance across all popular attack algorithms. Our codes are publicly available at https:// github.com/aamir-mustafa/super-resolution-adversarial-defense.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Success of Convolutional Neural Networks (CNNs) over the past several years has lead to their extensive deployment in a wide range of computer vision tasks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, including image classification <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>, object detection <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, semantic segmentation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> and visual question answering <ref type="bibr" target="#b9">[10]</ref>. Not only limited to that, CNNs now play a pivotal role in designing many critical real-world systems, including self-driving cars <ref type="bibr" target="#b10">[11]</ref> and models for disease diagnosis <ref type="bibr" target="#b11">[12]</ref>, which necessitates their robustness in such situations. Recent works <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>, however, have shown that CNNs can easily be fooled by distorting natural images with small, well-crafted, humanimperceptible additive perturbations. These distorted images, known as adversarial examples, have further been shown to be transferable across different architectures, e.g an adversarial example generated for an Inception v-3 model is able to fool other CNN architectures <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Owing to the critical nature of security-sensitive CNN applications, significant research has been carried out to devise defense mechanisms against these vulnerabilities <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b27">[28]</ref>. We can broadly categorize these defenses along two directions: the first being model-specific mechanisms, which aim to regularize a specific model's parameters through adversarial training or parameter smoothing <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Such methods often require differentiable transformations that are computationally demanding. Moreover these transformations are vulnerable to further attacks, as the adversaries can circumvent them by exploiting the differentiable modules. The second category of defenses are model-agnostic. They mitigate the effect of adversarial perturbations in the input image domain by applying various transformations. Examples of such techniques include JPEG compression <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, foveation-based methods, which crop the image background <ref type="bibr" target="#b31">[32]</ref>, random pixel deflection <ref type="bibr" target="#b25">[26]</ref> and random image padding &amp; re-sizing <ref type="bibr" target="#b17">[18]</ref>. Compared with differentiable model-specific methods, most of the model-agnostic approaches are computationally faster and carry out transformations in the input domain, making them more favorable. However, most of these approaches lose critical image content when removing adversarial noise, which results in poor classification performance on non-attacked images.</p><p>This paper proposes a model-agnostic defense mechanism against a wide range of recently proposed adversarial attacks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b35">[36]</ref> and does not suffer from information loss. Our proposed defense is based upon image super-resolution (SR), which selectively adds high frequency components to an image and removes noisy perturbations added by the adversary. We hypothesize that the learned SR models are generic enough to remap off-the-manifold samples onto the natural image manifold (see Fig. <ref type="figure" target="#fig_0">1</ref>). The effect of added noise is further suppressed by wavelet domain filtering and inherently minimized through a global pooling operation on the higher resolution version of the image. The proposed image super-resolution and wavelet filtering based defense results in a joint non-differentiable module, which can efficiently recover the original class labels for adversarially perturbed images.</p><p>The main contributions of our work are:</p><p>1) Through extensive empirical evaluations, we show image super-resolution to be an effective defense strategy against a wide range of recently proposed state-of-the-art attacks in the literature <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b35">[36]</ref>. Using Class Activation Map visualizations, we demonstrate that superresolution can successfully divert the attention of the classifier from random noisy patches to more distinctive regions of the attacked images (see Fig. <ref type="figure">8 and 9</ref>). 2) Super-resolving an adversarial image projects it back to the natural image manifold learned by deep image classification networks. 3) Unlike existing image transformation based techniques, which introduce artifacts in the process of overcoming adversarial noise, the proposed scheme retains critical image content, and thus minimally impacts the classifier's performance on clean, non-attacked images. 4) The proposed defense mechanism tackles adversarial attacks with no knowledge of the target model's architecture or parameters. This can easily complement other existing model-specific defense methods.</p><p>Closely related to our approach are the Defense-GAN <ref type="bibr" target="#b36">[37]</ref> and MagNet <ref type="bibr" target="#b24">[25]</ref>, which first estimate the manifold of clean data to detect adversarial examples and then apply a mapping function to reduce adversarial noise. Since they use generator blocks to re-create images, their studied case is restricted to small datasets (CIFAR-10, MNIST) with low-resolution images. In contrast, our approach does not require any prior detection scheme and works for all types of natural images with a more generic mapping function.</p><p>Below, we first formally define the underlying problem (Sec. II-A), followed by a brief description of existing adversarial attacks (Sec. II-B) and defenses (Sec. II-C). We then present our proposed defense mechanism (Sec. III). The effectiveness of our proposed defense is then demonstrated through extensive experiments against state-of-the art adversarial attacks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b35">[36]</ref> and comparison with other recently proposed model-agnostic defenses <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> (see Section IV).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>Here we introduce popular adversarial attacks and defenses proposed in the literature, which form the basis of our evaluations and are necessary for understanding our proposed defense mechanism. We only focus on adversarial examples in the domain of image classification, though the same can be crafted for various other computer vision tasks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Definition</head><p>Let x c ∈ R m denote a clean image sample and y c its corresponding ground-truth label, where the subscript c emphasizes that the image is clean. Untargeted attacks aim to misclassify a correctly classified example to any incorrect category. For these attacks, for a given image classifier C : R m → {1, 2, • • • , k}, an additive perturbation ρ ∈ R m is computed under the constraint that the generated adversarial example x adv = x c + ρ looks visually similar to the clean image x c i.e., d(x c , x adv ) ≤ for some dissimilarity function d(., .) and the corresponding labels are unequal i.e C(x c ) = C(x adv ). Targeted attacks change the correct label to a specified incorrect label, i.e., they seek x adv such that C(x adv ) = y tar , where y tar is a specific class label such that y tar = y c . An attack is considered successful for an image sample x c if it can find its corresponding adversarial example x adv under the given set of constraints. In practice d(., .) is the p norm between a clean image and its corresponding adversarial example, where p ∈ {1, • • • , ∞}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adversarial Attacks</head><p>(a) Fast Gradient Sign Method (FGSM): This is one of the first attack methods, introduced by Goodfellow et al. <ref type="bibr" target="#b13">[14]</ref>. Given a loss function L(x c + ρ, y c ; θ), where θ denotes the network parameters, the goal is to maximize the loss as:</p><formula xml:id="formula_0">argmax ρ∈R m L(x c + ρ, y c ; θ).<label>(1)</label></formula><p>FGSM is a single step attack which aims to find the adversarial perturbations by moving in the opposite direction to the gradient of the loss function w.r.t. the image (∇):</p><formula xml:id="formula_1">x adv = x c + .sign(∇(L(x c , y c ; θ)).</formula><p>(</p><formula xml:id="formula_2">)<label>2</label></formula><p>Here is the step size, which essentially restricts the ∞ norm of the perturbation. (b) Iterative Fast Gradient Sign Method (I-FGSM) is an iterative variant of FGSM, introduced by Kurakin et al. <ref type="bibr" target="#b14">[15]</ref>. I-FGSM performs the update as follows:</p><formula xml:id="formula_3">x m+1 = clip (x m + α.sign(∇(L(x m , y c ; θ))),<label>(3)</label></formula><p>where m ∈ [0, M ], x 0 = x c and after M iterations, x adv = x M . (c) Momentum Iterative Fast Gradient Sign Method (MI-FGSM), proposed by Dong et al. <ref type="bibr" target="#b32">[33]</ref>, is similar to I-FGSM with the introduction of an additional momentum term which stabilizes the direction of gradient and helps in escaping local maxima. MI-FGSM is defined as follows:</p><formula xml:id="formula_4">g m+1 = µ.g m + ∇L(x m , y c ; θ) ∇(L(x m , y c ; θ)) 1<label>(4)</label></formula><formula xml:id="formula_5">x m+1 = clip (x m + α.sign(g m+1 )),<label>(5)</label></formula><p>where µ is the decay factor, x 0 = x c and x adv = x M after M iterations. (d) DeepFool was proposed by Moosavi-Dezfooli et al. <ref type="bibr" target="#b33">[34]</ref> and aims to minimize the 2 norm between a given image and its adversarial counterpart. The attack assumes that a given image resides in a specific class region surrounded by the decision boundaries of the classifier. The algorithm then iteratively projects the image across the decision boundaries, which is of the form of a polyhydron, until the image crosses the boundary and is misclassified.</p><p>(e) Carlini and Wagner (C&amp;W) <ref type="bibr" target="#b34">[35]</ref> is a strong iterative attack that minimizes an auxiliary variable ζ as follows:</p><formula xml:id="formula_6">min ζ 1 2 (tanh (ζ) + 1) − x c +c.f ( 1 2 (tanh ζ + 1)),<label>(6)</label></formula><p>where 1 2 (tanh (ζ) + 1) − x c is the perturbation ρ and f (.) is defined as</p><formula xml:id="formula_7">f (x adv ) = max(Z(x adv ) yc −max{Z(x adv ) n : n = y c }, −k).</formula><p>(7) Here Z(x adv ) n are the logit values corresponding to a class n and k is the margin parameter. The C&amp;W attack works for various p norms.</p><p>(f) DI 2 FGSM and MDI 2 FGSM <ref type="bibr" target="#b35">[36]</ref>: The aforementioned attacks can be grouped into: single-step and iterative attacks. Iterative attacks have a higher success rate under white-box conditions, but they tend to overfit, and generalize poorly across black-box settings. In contrast, single-step attacks generate perturbed images with fairly improved transferability but a lower success rate in white-box conditions. The recently proposed Diverse-Input-Iterative-FGSM (DI 2 FGSM) and Momentum-Diverse-Input-Iterative-FGSM (MDI 2 FGSM) <ref type="bibr" target="#b35">[36]</ref> methods claim to fill in this gap and improve the transferability of iterative attacks. DI 2 FGSM performs random image re-sizing and padding as image transformation τ (.), thus creating an augmented set of images, which are then attacked using I-FGSM as:</p><p>x m+1 = clip (x m + α.sign(∇(L(τ (x m ; p), y c ; θ))). <ref type="bibr" target="#b7">(8)</ref> Here p is the ratio of transformed images to total number of images in the augmented dataset. MDI 2 FGSM is a variant, which incorporates the momentum term in DI 2 FGSM to stabilize the direction of gradients. The overall update for MDI 2 FGSM is similar to MI-FGSM, with Equation 4 being replaced by:</p><formula xml:id="formula_8">g m+1 = µ.g m + ∇L(τ (x m ; p), y c ; θ) ∇(L(τ (x m ; p), y c ; θ)) 1 .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Adversarial Defenses</head><p>Tremer et al. <ref type="bibr" target="#b19">[20]</ref> proposed ensemble adversarial training, which results in regularizing the network by softening the decision boundaries, thereby encompassing nearby adversarial images. Defensive distillation <ref type="bibr" target="#b16">[17]</ref> improves the model robustness in an essentially similar fashion by retraining a given model using soft labels acquired by a distillation mechanism <ref type="bibr" target="#b39">[40]</ref>. Kurakin et al. <ref type="bibr" target="#b18">[19]</ref> augmented a training batch of clean images with their corresponding adversarial images to improve robustness. Moosavi-Dezfooli et al. <ref type="bibr" target="#b40">[41]</ref>, however, showed that adversarial examples can also be generated for an already adversarially trained model.</p><p>Recently, some defense methods have been proposed in input image transformation domain. Data compression (JPEG image compression) as a defense was studied by <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. JPEG compression deploys a discrete cosine transform to suppress the human-imperceptible, high frequency noise components. Guo et al. <ref type="bibr" target="#b38">[39]</ref>, however, noted that JPEG compression alone is far from being an effective defense. They proposed image transformations using quilting and Total Variance Minimization (TVM). Feature squeezing <ref type="bibr" target="#b41">[42]</ref> reduces the image resolution either by using bit depth reduction or smoothing filters to limit the adversarial space. A foveation based method was proposed by Luo et al. <ref type="bibr" target="#b31">[32]</ref>, which shows robustness against weak attacks like L-BFGS <ref type="bibr" target="#b12">[13]</ref> and FGSM <ref type="bibr" target="#b13">[14]</ref>. Another closely related work to ours is that of Prakash et al. <ref type="bibr" target="#b25">[26]</ref>, which deflects attention by carefully corrupting less critical image pixels. This introduces new artifacts which reduce the image quality and can result in misclassification. To handle such artifacts, BayesShrink denoising in the wavelet domain is used. It has been shown that denoising in the wavelet domain yields superior performance than other techniques such as bilateral, an-isotropic, TVM and Wiener-Hunt deconvolution <ref type="bibr" target="#b25">[26]</ref>. Another closely related work is that of Xie et al. <ref type="bibr" target="#b17">[18]</ref>, which performs image transformations by randomly re-sizing and padding an image before passing it through a CNN classifier. Xie et al. <ref type="bibr" target="#b27">[28]</ref> showed that adding adversarial patterns to a clean image results in noisy activation maps. A defense mechanism was proposed to perform feature denoising using non-local means, which requires retraining the model end-to-end with adversarial data augmentation. One of the main shortcomings of the aforementioned defense techniques (JPEG compression, PD and foveation based method) is that the transformations degrade the image quality, which results in a loss of significant information from images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED PERTURBED IMAGE RESTORATION</head><p>Existing defense mechanisms against adversarial attacks aim to reduce the effects of added perturbations so as to recover the correct image class. Defenses are being developed along two main directions: (i) modifying the image classifier C(.) to C (.) such that it recovers the true label for an adversarial example, i.e. C (x adv ) = C(x c ) = y c ; and (ii) transforming the input image such that C(x c ) = C(T (x adv )) = y c , where T (.) is an image transformation function. Ideally, T (.) should be modelagnostic, complex and a non-differentiable function, making it harder for the adversary to circumvent the transformed model by back-propagating the classifier error through it.</p><p>Our proposed approach, detailed below, falls under the second category of defense mechanisms. We propose to use image restoration techniques to purify perturbed images. The proposed approach has two components, which together form a non-differentiable pipeline that is difficult to bypass. As an initial step, we apply wavelet denoising to suppress any noise patterns. The central component of our approach is the superresolution operation, which enhances the pixel resolution while simultaneously removing adversarial patterns. Our experiments show that image super-resolution alone is sufficient to reinstate classifier beliefs towards correct categories; however, the second step provides added robustness since it is a nondifferentiable denoising operation.  In the following section, we first explain the super-resolution approach (Sec. III-A) followed by a description of denoising method (Sec. III-B). Finally, we summarize the defense scheme in Sec. III-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Super Resolution as a Defense Mechanism</head><p>Our goal is to defend a classification model C(•) against the perturbed images x adv generated by an adversary. Our approach is motivated by the manifold assumption <ref type="bibr" target="#b42">[43]</ref>, which postulates that natural images lie on low-dimensional manifolds. This explains why low-dimensional deep feature representations can accurately capture the structure of real datasets. The perturbed images are known to lie off the lowdimensional manifold of natural images, which is approximated by deep networks <ref type="bibr" target="#b43">[44]</ref>. Gong et al. in <ref type="bibr" target="#b44">[45]</ref> showed that a simple binary classifier can successfully separate offthe-manifold adversarial images from clean ones and thereby concluded that adversarial and clean data are not twins, despite appearing visually identical. Fig. <ref type="figure" target="#fig_1">2</ref> shows a low-dimensional manifold of natural images. Data points from a real-world dataset (say ImageNet) are sampled from a distribution of natural images and can be considered to lie on-the-manifold. Such images are referred to as in-domain <ref type="bibr" target="#b45">[46]</ref>. Corrupting these in-domain images by adding adversarial noise takes the images off-the-manifold. A model that learns to yield images lying on-the-manifold from off-the-manifold images can go a long way in detecting and defending against adversarial attacks. We propose to use image super-resolution as a mapping function to remap off-the-manifold adversarial samples onto the natural image manifold and validate our proposal through experimentation (see Sec. IV-A). In this manner, robustness against adversarial perturbations is achieved by enhancing the visual quality of images. This approach provides remarkable benefits over other defense mechanisms that truncate critical information to achieve robustness.</p><p>Super-resolution Network: A required characteristic for defense mechanisms is the ability to suppress fraudulent perturbations added by an adversary. Since these perturbations are generally high-frequency details, we use a super-resolution network that explicitly uses residual learning to focus on such details. These details are added to the low-resolution inputs in each residual block to eventually generate a high-quality, super-resolved image. The network considered in this work is the Enhanced Deep Super-Resolution (EDSR) <ref type="bibr" target="#b46">[47]</ref> network (trained on the DIVerse 2K resolution image (DIV2K) dataset <ref type="bibr" target="#b47">[48]</ref>), which uses a hierarchy of such residual blocks. While our proposed approach achieves competitive performance with other super-resolution and up-sampling techniques, we demonstrate the added efficacy of using residual learning based EDSR model through extensive experiments (see Sec. IV).</p><p>Effect on Spectral Distribution: The underlying assumption of our method is that deep super-resolution networks learn a mapping function that is generic enough to map the perturbed image onto the manifold of its corresponding class images. This mapping function learned with deep CNNs basically models the distribution of real non-perturbed image data. We validate this assumption by analyzing the frequency-domain spectrum of the clean, adversarial and recovered images in Fig. <ref type="figure">3</ref>. It can be observed that adversarial image contains high frequency patterns and the super-resolution operation further injects high frequency patterns to the recovered image. This achieves two major benefits: first, the newly added highfrequency patterns smooth the frequency response of the image (column 5, Fig. <ref type="figure">3</ref>) and, second, the super-resolution destroys the adversarial patterns that seek to fool the model. Fig. <ref type="figure">3</ref> also shows that the super-resolved image maintains the high-frequency details close to the original (clean) input image. Still, it is quite different from the clean image, e.g., compare the magnitude spectrum at higher frequencies for the recovered and clean image, which shows a much smoother spread of frequencies in the recovered image. The exact difference between clean and recovered images is shown in the bottom left corner of Fig. <ref type="figure">3</ref>, which illustrates the fact that the recovered image is relatively cleaner but has more high-  frequency details compared to the original image. Comparing the original noise signal (top left corner in Fig. <ref type="figure">3</ref>) and the left-over noise in the recovered image (bottom left corner in Fig. <ref type="figure">3</ref>), we can observe that the SR network discards most of the noisy perturbations; however a sparse trace of noise is still present. Also, the SR network reinforces the high-frequency changes along the salient boundaries in the image (notice the response along the bird boundaries).</p><p>Effect of Adversarial Perturbations on Feature Maps: Adversarial attacks add small perturbations to images, which are often imperceptible to the human eye or generally perceived as small noise in an image in the pixel space. However, this adversarial noise amplifies in the feature maps of a convolutional network, leading to substantial noise <ref type="bibr" target="#b27">[28]</ref>. Fig. <ref type="figure" target="#fig_2">4</ref> shows the feature maps for three clean images, their adversarial counterparts and the defended images chosen from the ResNet-50 res 3 block after the activation layer. Each feature map is of 28 × 28 dimensions. The features for a clean image sample are activated only at semantically significant regions of the image, whereas those for its adversarial counterpart seem to be focused at semantically irrelevant regions as well. Xie et al <ref type="bibr" target="#b27">[28]</ref> performed feature denoising using non-local means <ref type="bibr" target="#b48">[49]</ref> to improve the robustness of convolutional networks. Their model is trained end-to-end on adversarially perturbed images. Our defense technique recovers the feature maps (Cols 2 and 4, Fig. <ref type="figure" target="#fig_2">4</ref>) without requiring any model retraining or adversarial image data augmentation.</p><p>Advantages of Proposed Method: Our proposed method offers a number of advantages. (a) The proposed approach is agnostic to the attack algorithm and the attacked model. (b) Unlike many recently proposed techniques, which degrade critical image information as part of their defense, our proposed method improves image quality while simultaneously providing a strong defense. (c) The proposed method does not require any learning and only uses a fixed set of parameters to purify input images. (d) It does not hamper the classifier's performance on clean images. (e) Due to its modular nature, the proposed approach can be used as a pre-processing step in existing deep networks. Furthermore, our purification approach is equally applicable to other computer vision tasks beyond classification, such as segmentation and object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Wavelet Denoising</head><p>Since all adversarial attacks add noise to an image in the form of well-crafted perturbations, an efficient image denoising technique can go a long way in mitigating the effect of these perturbations, if not removing them altogether. Image denoising in the spatial or frequency domain causes a loss of textural details, which is detrimental to our goal of achieving clean image-like performance on denoised images. Denosing in the wavelet domain has gained popularity in recent works. It yields better results than various other techniques including bilateral, anisotropic, Total Variance Minimization (TVM) and Wiener-Hunt de-convolution <ref type="bibr" target="#b25">[26]</ref>. The main principle behind wavelet shrinkage is that Discrete Wavelet Transform (DWT) of real world signals is sparse in nature. This can be exploited to our advantage since the ImageNet dataset <ref type="bibr" target="#b49">[50]</ref> contains images that capture real-world scenes and objects. Consider an adversarial example x adv = x c + ρ; the wavelet transform of x adv is a linear combination of the wavelet transform of the clean image and noise. Unlike image smoothing, which removes the higher frequency components in an image, DWTs of real world images have large coefficients corresponding to significant image features and noise can be removed by applying a threshold on the smaller coefficients.</p><p>1) Thresholding: The thresholding parameter determines how efficiently we shrink the wavelet coefficients and remove adversarial noise from an image. In practice, two types of thresholding methods are used: a) Hard thresholding and b) Soft thresholding. Hard thresholding is basically a non-linear technique, where each coefficient ( x) is individually compared to a threshold value (t), as follows:</p><formula xml:id="formula_9">D( x, t) = x if | x| ≥ t 0 otherwise.</formula><p>Reducing the small noisy coefficients to zero and then carrying out an inverse wavelet transform produces an image which retains critical information and suppresses the noise. Unlike hard thresholding where the coefficients larger than t are fully retained, soft thresholding modifies the coefficients as follows:</p><formula xml:id="formula_10">D( x, t) = max(0, 1 − t | x| ) x.</formula><p>In our method, we use soft-thresholding as it reduces abrupt sharp changes that otherwise occur in hard thresholding. Also, hard-hresholding over-smooths an image, which reduces the classification accuracy on clean non-adversarial images.</p><p>Choosing an optimal threshold value t is the underlying challenge in wavelet denoising. A very large threshold value means ignoring larger wavelets, which results in an oversmoothed image. In contrast, a small threshold allows even the noisy wavelets to pass, thus failing to produce a denoised image after reconstruction. Universal thresholding is employed in VisuShrink <ref type="bibr" target="#b50">[51]</ref> to determine the threshold parameter t vs for an image X with n pixels as t vs = σ ρ 2 ln (n), where σ ρ is an estimate of the noise level. BayesShrink <ref type="bibr" target="#b51">[52]</ref> is an efficient method for wavelet shrinkage which employs different thresholds for each wavelet sub-band by considering Gaussian noise. Suppose xadv = xc + ρ is the wavelet transform of an adversarial image, since xc and ρ are mutually independent, the variances σ 2</p><p>x adv , σ 2 xc and σ 2 ρ of xadv , xc , ρ, respectively, follow: σ 2</p><p>x adv = σ 2 xc + σ 2 ρ . A wavelet sub-band variance for an adversarial image is estimated as:</p><formula xml:id="formula_11">σ 2 x adv = 1 M M m=1 W 2 m ,</formula><p>where W 2 m are the sub-band wavelets and M is the total number of wavelet coefficients in a sub-band. The threshold value for BayesShrink soft-thresholding is given as:</p><formula xml:id="formula_12">t bs = σ 2 ρ /σ xc if σ 2 ρ &lt; σ 2 x adv max(|W m |) otherwise.</formula><p>In our experiments, we explore both VisuShrink and BayesShrink soft-thresholding and find the latter to perform better and provide visually superior denoising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Algorithmic Description</head><p>An algorithmic description of our end-to-end defense scheme is provided in Algorithm 1. We first smooth the effect of adversarial noise using soft wavelet denoising. This is followed by employing super resolution as a mapping function to enhance the visual quality of images. Super resolving an image maps the adversarial examples to the natural image manifold in high-resolution space, which otherwise lie offthe-manifold in low-resolution space. The recovered image is then passed through the same pre-trained models on which the adversarial examples were generated. As can be seen, our model-agnostic image transformation technique is aimed at minimizing the effect of adversarial perturbations in the image domain, with little performance loss on clean images. Our technique causes minimal depreciation in the classification accuracy of non-adversarial images. IV. EXPERIMENTS Models and Datasets: We evaluate our proposed defense and compare it with existing methods for three different classifiers: Inception-v3, ResNet-50 and InceptionResNet v-2. For these models, we obtain ImageNet pre-trained weights from TensorFlow's GitHub repository<ref type="foot" target="#foot_0">1</ref> , and do not perform any re-training or fine-tuning. The evaluations are done on a subset of 5000 images from the ILSVRC <ref type="bibr" target="#b49">[50]</ref> validation set. The images are selected such that the respective model achieves a top-1 accuracy of 100% on the clean non-attacked images. Evaluating defense mechanisms on already misclassified images is not meaningful, since an attack on a misclassified image is considered successful as per the definition. We also perform experiments on the NIPS 2017 Competition on Adversarial Attacks and Defenses DEV dataset <ref type="bibr" target="#b52">[53]</ref>. The dataset is collected by Google Brain organizers, and consists of 1000 images of size 299 × 299. An ImageNet pre-trained Inception v-3 model achieves 95.9% top-1 accuracy on NIPS 2017 DEV images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attacks:</head><p>We generate attacked images using different techniques, including Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b13">[14]</ref>, iterative FGSM (I-FGSM) <ref type="bibr" target="#b14">[15]</ref>, Momentum Iterative FGSM (MI-FGSM) <ref type="bibr" target="#b32">[33]</ref>, DeepFool <ref type="bibr" target="#b33">[34]</ref>, Carlini and Wagner <ref type="bibr" target="#b34">[35]</ref>, Diverse Input Iterative FGSM (DI<ref type="foot" target="#foot_1">2</ref> FGSM) and Momentum Diverse Input Iterative FGSM (MDI 2 FGSM) <ref type="bibr" target="#b35">[36]</ref>. We use publicly available implementations of these methods: Cleverhans <ref type="bibr" target="#b53">[54]</ref>, Foolbox <ref type="bibr" target="#b54">[55]</ref> and codes 2 3 provided by <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref>. For FGSM, we generate attacked images with ∈ {2, 5, 10} and for iterative attacks, the maximum perturbation size is restricted to 16. All the adversarial images are generated for the undefended models, after which various defense schemes are implemented in gray-box settings.</p><p>Defenses: We compare our proposed defense with a number of recently introduced state-of-the-art image transformation based defense schemes in the literature. These include JPEG Compression <ref type="bibr" target="#b37">[38]</ref>, Random Resizing and Padding <ref type="bibr" target="#b17">[18]</ref>, Image quilting + total variance minimization <ref type="bibr" target="#b38">[39]</ref> and Pixel Deflection (PD) <ref type="bibr" target="#b25">[26]</ref>. We use publicly available implementations<ref type="foot" target="#foot_3">4</ref> <ref type="foot" target="#foot_4">5</ref> 6 7 of these methods. All experiments are run on the same set of images and against the same attacks for a fair comparison.</p><p>For our experiments, we explore two broad categories of Single Image Super Resolution (SISR) techniques: i) Interpolation based methods and ii) Deep Learning (DL) based methods. Interpolation based methods like Nearest Neighbor (NN), Bi-Linear and Bi-cubic upsampling are computationally efficient, but not quite robust against stronger attacks (DI 2 FGSM and MDI 2 FGSM). Recently proposed DL based methods have shown superior performance in terms of Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index (SSIM), and the mean squared error (MSE). Here, we consider three DL based SISR techniques, i) Super Resolution using ResNet model (SR-ResNet) <ref type="bibr" target="#b55">[56]</ref>, ii) Enhanced Deep Residual Network for SISR (EDSR) <ref type="bibr" target="#b46">[47]</ref> and iii) Super Resolution using Generative Adversarial Networks (SR-GAN) <ref type="bibr" target="#b55">[56]</ref>. Our experiments show that EDSR consistently performs better. EDSR builds on a residual learning <ref type="bibr" target="#b2">[3]</ref> scheme that specifically focuses on high-frequency patterns in the images. Compared to the original ResNet, EDSR demonstrates substantial improvements by removing Batch Normalization layers (from each residual block) and ReLU activation (outside residual blocks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Manifold Assumption Validation</head><p>In this paper we propose that clean and adversarial examples lie on different manifolds and super-resolving an image to a higher dimensional space remaps the adversarial sample back to the natural image manifold.</p><p>To validate this assumption, we fine-tune a pre-trained Inception v-3 model on the ImageNet dataset as a binary classifier using 10,000 pairs of clean and adversarial examples (generated from all the aforementioned attack techniques). We re-train the top-2 blocks while freezing the rest with a learning rate reduced by a factor of 10. The global average pooling layer of the model is followed by a batch normalization layer, drop-out layer and two dense layers (1024 and 1 nodes, respectively). Our model efficiently leverages the subtle difference between clean images and their adversarial counterparts and separates the two with a very high accuracy (99.6%). To further validate our assumption on super-resolution, we test our defended images using this binary classifier. The classifier labels around 91% of the super-resolved images as clean, confirming that the vast majority of restored samples lie on the natural image manifold.</p><p>In Figure <ref type="figure">.</ref> 1, we plot the features extracted from the last layer of the binary classifier to visualize our manifold assumption validation. We reduce the dimensionality of features to 3 for visualization (containing 90% of variance) using Principle Component Analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results and Analysis</head><p>Table I shows the destruction rates of various defense mechanisms on 5000 ILSVRC validation set images. Destruction rate is defined as the ratio of successfully defended images <ref type="bibr" target="#b14">[15]</ref>. A destruction rate of 100% implies that all images are correctly classified after applying the defense mechanism. It should be noted that we define destruction rates in terms of top-1 classification accuracy, which makes defending against attacks more challenging since we have to recover the exact class label. 'No Defense' in Table <ref type="table" target="#tab_2">I</ref> shows the model performance on generated adversarial images. A lower value under 'No Defense' is an indication of a strong attack. The results show that iterative attacks are better at fooling the model compared with the single-step attacks. The iterative attacks, however, are not transferable and are easier to defend. Similarly, targeted attacks are easier to defend compared with their non-targeted counterparts, as they tend to over-fit the attacked model <ref type="bibr" target="#b34">[35]</ref>. Considering them as weak attacks, we therefore only report the performance of our defense scheme against more generic non-targeted attacks.</p><p>For the iterative attacks (C&amp;W and DeepFool), both Random Resizing + Padding and PD achieve similar performance, successfully recovering about 90% of the images. In comparison, our proposed super-resolution based defense recovers about 96% of the images. For the single-step attack categories, Random Resizing + Padding fails to defend. This is also noted in <ref type="bibr" target="#b17">[18]</ref>. To overcome this limitation, an ensemble model with adversarial augmentation is used for defense. Compared with the JPEG compression based defense <ref type="bibr" target="#b30">[31]</ref>, our proposed method achieves a substantial performance gain of 31.1% for FGSM ( = 10). In the single-step attacks category (e.g., FGSM-10), our defense model outperforms Random Resizing + Padding and PD by a large margin of 26.7% and 21.0%, respectively. For the recently proposed strong attack (MDI 2 FGSM), all defense techniques (JPEG compression, Random Resizing + Padding, Quilting + TVM and PD) largely fail, recovering only 1.3%, 5.8%, 1.7% and 21.9% of the images, respectively. In comparison, the proposed image super-resolution based defense can successfully recover 31.3% of the images.</p><p>We show a further performance comparison of our proposed defense with other methods on the NIPS-DEV dataset in Table II. Here, we only report results on Inception v-3, following the standard evaluation protocols as per the competition's guidelines <ref type="bibr" target="#b52">[53]</ref>. Inception v-3 is a stronger classifier, and we expect the results to generalize across other classifiers. Our experimental results in Table <ref type="table" target="#tab_3">II</ref> show the superior performance of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Adversarial Training</head><p>Adversarial Training has been shown to enhance many recently proposed defense methods <ref type="bibr" target="#b52">[53]</ref> under white-box attack settings. Taking insights from the effect of super-resolution under gray-box settings, we introduce a robust adversarial training paradigm that enhances the performance of traditional adversarial training. For this, we jointly train our model on an augmented dataset comprising of clean, attacked and superresolved images (for CIFAR-10 dataset) to improve the generalization of adversarial training. Our results in Table V indicate that our adversarially trained model provides enhanced robustness against white-box attacks. Below we describe our experimental settings used for training and evaluation.</p><p>Experimental Settings: The adversarial samples used for the training process are generated using Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b13">[14]</ref> by uniformly sampling from an interval of [0.01, 0.05] for CIFAR-10 dataset. These attacked images are then super-resolved to form the augmented dataset for training. We evaluate the model's robustness against singlestep as well as strong iterative attacks in white-box conditions. The number of iterations for Iterative Fast Gradient Sign Method (I-FGSM) <ref type="bibr" target="#b18">[19]</ref>, Momentum Iterative FGSM (MI-FGSM) <ref type="bibr" target="#b32">[33]</ref> and Projected Gradient Descent (PGD) <ref type="bibr" target="#b56">[57]</ref> are set to 10 with a step size of /10 for I-FGSM and MI-FGSM and /4 for the PGD attack. The iteration steps for the Carlini &amp; Wagner (C&amp;W) attack <ref type="bibr" target="#b34">[35]</ref> are 1,000 with a learning rate of 0.01. We used a ResNet-110 model for training the CIFAR-10 dataset. Table <ref type="table" target="#tab_8">VI</ref> gives the detailed architecture of the model used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>Super-resolution Methods: Image super resolution recovers off-the-manifold adversarial images from a low-resolution space and remaps them to the high-resolution space. This should hold true for different super-resolution techniques in the literature. In Table <ref type="table" target="#tab_4">III</ref>, we evaluate the effectiveness of three image super-resolution techniques-SR-ResNet, SR-GAN <ref type="bibr" target="#b55">[56]</ref> and EDSR <ref type="bibr" target="#b46">[47]</ref>. Specifically, attacked images are super-resolved to 2×, without using any wavelet denoising. Experiments are performed on Inception v-3 classifier. The results in Table <ref type="table" target="#tab_4">III</ref> show a comparable performance across the evaluated super-resolution methods. These results demonstrate the effectiveness of super-resolution in recovering images.</p><p>Besides state-of-the-art image super-resolution methods, we further consider documenting the results on enhancing image resolution using interpolation-based techniques. For this, we perform experiments by resizing the images with Nearest Neighbor, Bi-linear and Bi-cubic interpolation techniques. In Table <ref type="table" target="#tab_5">IV</ref>, we report the results achieved by three different strategies: upsample (by 2×), upsample + downsample and downsample + upsample. The results show that, although the performance of the simple interpolation based methods is inferior to more sophisticated state-of-the-art super-resolution techniques in Table <ref type="table" target="#tab_4">III</ref>, the simple interpolation based image   resizing is surprisingly effective and achieves some degree of defense against adversarial attacks.</p><p>Effect of Wavelet Denoising: Our proposed defense first deploys wavelet denoising, which aims to minimize the effect of adversarial perturbations, followed by image superresolution to selectively introduce high-frequency components into an image (as seen in Fig. <ref type="figure">3</ref>) and recover off-the-manifold attacked images. Here we investigate the individual impact of these two modules towards defending adversarial attacks. We perform extensive experiments on three classifiers: Inception v-3, ResNet-50 and InceptionResNet v-2.   performance is achieved when wavelet denoising is followed by super-resolution. These empirical evaluations demonstrate that image super-resolution with wavelet denoising is a robust model-agnostic defense technique for both iterative and non-iterative attacks.</p><p>Hyper-parameters Selection: Unlike many existing defense schemes, which require computationally expensive model re-training and parameter optimization <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b26">[27]</ref>, our proposed defense is training-free and does not require tuning a large set of hyper-parameters. Our proposed defense has two hyper-parameters: the scale of super-resolution (S) and the coefficient of BayesShrink (σ ρ ). We perform a linear search over the scaling factor S for one single-step (FGSM-2) and one iterative (C&amp;W) attack on 500 images, randomly   Row 5 (Fig. <ref type="figure">8 and 9</ref>) show the added perturbations to the clean image sample. Super-resolving an image selectively adds high-frequency components that eventually help in recovering model attention towards discriminative regions corresponding to the correct class labels (see Row 6, Fig. <ref type="figure">8 and 9</ref>).</p><p>Qualitative Analysis of SR: In Fig. <ref type="figure">5</ref> we show two clean image samples where super-resolving the image to a higher dimension alters the classifier's predictions. In one case, super resolution causes the image to be misclassified. However, in the other, it recovers the actual class of the image which was otherwise incorrectly classified by a deep learning model. These cases predominantly arise in situations where the network's confidence for a single class is low, i.e. top-two predictions are roughly equal (see Fig. <ref type="figure">5</ref>).</p><p>In Fig. <ref type="figure">6</ref> we show the effect of individual components of the defense mechanism on a sample adversarial image. In an adversarial setting the perturbed images are generated with the objective of changing the model's prediction for an input without significantly changing the image (i.e., within a small bound ). This means that the generated noise is not sampled from a predefined noise model, but is instead dependent on the loss surface and the input sample. Since SR models like EDSR may not deal with the noise, we first employ a denoiser in our proposed pipeline. As shown in Fig. <ref type="figure">6</ref>, the denoised inputs to the SR network perform reasonably well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Identifying Obfuscated Gradients</head><p>Recently, Athalye et al. <ref type="bibr" target="#b58">[59]</ref> were successful in breaking several defense mechanisms in the white-box settings by  identifying that they exhibit a false sense of security. They call this phenomenon gradient masking. Below, we discuss how our defense mechanism does not cause gradient masking on the basis of characteristics defined in <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>.</p><p>Iterative attacks perform better than one-step attacks: Our evaluations in Table <ref type="table" target="#tab_2">I</ref> indicate that stronger iterative attacks (e.g. I-FGSM, MI-FGSM) are more successful at attacking the undefended models than single-step attacks (FGSM in our case).</p><p>Robustness against gray-box settings is higher than white-box settings: In white-box settings, the adversary has complete knowledge of the model, so attacks should be more successful. In other words, if a defense does not suffer from obfuscated gradients, the robustness of the model against white-box settings should be inferior to that in the gray-box settings. In Table X, we show that the robustness under whitebox settings is lower than the robustness for settings. This validates that the proposed defense follows the desired trend and does not obfuscate gradients. Dev dataset when exposed to white-box settings (in this case the adversary has complete knowledge of the denoising process and super-resolution model). Here is the perturbation size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attacks</head><p>Params.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>White-box Gray-box</head><p>No Attack -90.9 90.9 Since our defense scheme is based on a combination of transformations, wavelet-denoising and image superresolution, we implement Backward Pass Differentiable Approximation (BPDA) to bypass the non-differentiable component of our defense. We also evaluate the robustness of our method against Expectation Over Transformation (EOT) <ref type="bibr" target="#b58">[59]</ref> attack. However, the attack methods fail to substantially break our defense, as shown in Fig. <ref type="figure">7</ref>. With EOT <ref type="bibr" target="#b58">[59]</ref>, the accuracy drops by a mere 8.9% for a strong attack (PGD) with a perturbation of = 8/255.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Adversarial perturbations can seriously compromise the security of deep learning based models. This can have wide repercussions since the recent success of deep learning has led to these models being deployed in a broad range of important applications, from health-care to surveillance. Thus, designing robust defense mechanisms that can counter adversarial attacks without degrading performance on unperturbed images is an absolute requisite. In this paper, we presented an image restoration scheme based on super-resolution, that maps offthe-manifold adversarial samples back to the natural image manifold. We showed that the primary reason that superresolution networks can negate the effect of adversarial noise is due to their addition of high-frequency information into the input image. Our proposed defense pipeline is agnostic to the underlying model and attack type, does not require any learning and operates equally well for black and white-box attacks. We demonstrated the effectiveness of the proposed defense approach compared to state-of-the-art defense schemes, where it outperformed competing models by a considerable margin.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: a) A 3D plot showing adversarial image features (red) and the corresponding clean image features (green). b) On the right, we show the features of the corresponding defended images (blue). The plot clearly shows that the super-resolution operation remaps the adversarial images to the natural image manifold, which otherwise lie off manifold. (100 randomly selected features projected to 3D space using principal component analysis are shown for better visualization)</figDesc><graphic url="image-1.png" coords="1,313.03,171.85,127.66,91.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Super-resolution as a Defense Against Adversarial Attacks: The figure illustrates mapping of a sample image from lowresolution to its highresolution manifold. Adversarial images, which otherwise lie off the manifold of natural images, are mapped in the same domain as the clean natural images, thereby recovering their corresponding true labels. (Best seen in color and enlarged)</figDesc><graphic url="image-5.png" coords="4,394.67,67.85,60.87,60.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Feature map in the res3 block of an ImageNet-trained ResNet-50 for a clean image, its adversarial counterpart and the recovered image. The adversarial perturbation was produced using FGSM with = 10. Image super-resolution essentially nullifies the effect of adversarial patterns added by the adversary.</figDesc><graphic url="image-43.png" coords="5,52.50,369.59,56.76,56.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :6</head><label>1</label><figDesc>Defending Against Adversarial Attacks with Image Restoration (Wavelet Denoising + Super Resolution) / * Image Denoising * / Input: Corrupted image x adv = xc + ρ Output: Denoised image x = D(x adv ) 1 Convert the RGB image to Y C b Cr color space, where Y and C b , Cr represent luminance and chrominance respectively. 2 Convert the image to wavelet domain Xadv = Xc + ρ using discrete wavelet transform. 3 Remove noisy wavelet coefficients using BayesShrink soft-thresholding. 4 Invert the shrunken wavelet coefficients using Inverse Wavelet Transform (IWT). 5 Revert the image back to RGB. / * Image Super-Resolution * / Input: Denoised image x = D(x adv ) Output: Super Resolved Image xt = M (x ) Map adversarial samples back to natural image manifold using deep super resolution network: M (•). 7 Forward the recovered images to the attacked model for correct prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Fig. 5: Effect of Image Super Resolution on clean image samples from the NIPS 2017 Dev Dataset. Predictions are made using a pretrained Inception v-3 model. Green color refers to the correct class, while red and blue indicate incorrect classes. It can be seen that super resolving an image to a higher dimension at some instances depreciate the model's classification prediction (top), but can also recover the correct class for an otherwise misclassified clean image sample (bottom).</figDesc><graphic url="image-46.png" coords="11,175.16,147.22,126.24,92.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FGSM = 2 Fig. 7 :</head><label>27</label><figDesc>Fig. 7: Performance of our defense model on adversarial images (NIPS 2017 dataset) generated using BPDA and EOT introduced by Athalye et al. [61]. For an undefended model, the attack has a success rate of 100%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Performance comparison with state-of-the art defense mechanisms on 5000 images from ILSVRC validation set. The images are selected such that the respective classifier achieves 100% accuracy. Our proposed defense consistently achieves superior performance across three different models and various adversarial attacks.</figDesc><table><row><cell>Model</cell><cell cols="2">Clean Images FGSM-2</cell><cell>FGSM-5</cell><cell>FGSM-10</cell><cell cols="3">I-FGSM DeepFool C&amp;W</cell><cell cols="2">MI-FGSM DI 2 FGSM</cell><cell>MDI 2 FGSM</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">No Defense</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inception v-3</cell><cell>100</cell><cell>31.7</cell><cell>28.7</cell><cell>30.5</cell><cell>11.4</cell><cell>0.4</cell><cell>0.8</cell><cell>1.7</cell><cell>1.4</cell><cell>0.6</cell></row><row><cell>ResNet-50</cell><cell>100</cell><cell>12.2</cell><cell>7.0</cell><cell>6.1</cell><cell>3.4</cell><cell>1.0</cell><cell>0.1</cell><cell>0.4</cell><cell>0.3</cell><cell>0.2</cell></row><row><cell>Inception ResNet v-2</cell><cell>100</cell><cell>59.4</cell><cell>55.0</cell><cell>53.6</cell><cell>21.6</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>1.5</cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">JPEG Compression (Das et al. [38])</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inception v-3</cell><cell>96.0</cell><cell>62.3</cell><cell>54.7</cell><cell>48.8</cell><cell>77.5</cell><cell>81.2</cell><cell>80.5</cell><cell>69.4</cell><cell>2.1</cell><cell>1.3</cell></row><row><cell>ResNet-50</cell><cell>92.8</cell><cell>57.6</cell><cell>49.0</cell><cell>42.9</cell><cell>74.8</cell><cell>77.3</cell><cell>81.3</cell><cell>70.8</cell><cell>0.7</cell><cell>0.4</cell></row><row><cell>Inception ResNet v-2</cell><cell>95.5</cell><cell>67.0</cell><cell>55.3</cell><cell>53.7</cell><cell>81.3</cell><cell>83.9</cell><cell>83.1</cell><cell>72.8</cell><cell>1.6</cell><cell>1.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Random resizing + zero padding (Xie et al. [18])</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inception v-3</cell><cell>97.3</cell><cell>69.2</cell><cell>57.3</cell><cell>53.2</cell><cell>90.6</cell><cell>88.9</cell><cell>89.5</cell><cell>89.5</cell><cell>7.0</cell><cell>5.8</cell></row><row><cell>ResNet-50</cell><cell>92.5</cell><cell>66.8</cell><cell>55.7</cell><cell>48.8</cell><cell>88.2</cell><cell>90.9</cell><cell>87.5</cell><cell>88.0</cell><cell>6.6</cell><cell>4.2</cell></row><row><cell>Inception ResNet v-2</cell><cell>98.7</cell><cell>70.7</cell><cell>59.1</cell><cell>55.8</cell><cell>87.5</cell><cell>89.7</cell><cell>88.0</cell><cell>88.3</cell><cell>7.5</cell><cell>5.3</cell></row><row><cell></cell><cell></cell><cell cols="5">Quilting + Total Variance Minimization (Guo et al. [39])</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inception v-3</cell><cell>96.2</cell><cell>70.2</cell><cell>62.0</cell><cell>54.6</cell><cell>85.7</cell><cell>85.9</cell><cell>85.3</cell><cell>84.5</cell><cell>4.1</cell><cell>1.7</cell></row><row><cell>ResNet-50</cell><cell>93.1</cell><cell>69.7</cell><cell>61.0</cell><cell>53.3</cell><cell>85.4</cell><cell>85.0</cell><cell>84.6</cell><cell>83.8</cell><cell>3.6</cell><cell>1.1</cell></row><row><cell>Inception ResNet v-2</cell><cell>95.6</cell><cell>74.6</cell><cell>67.3</cell><cell>59.0</cell><cell>86.5</cell><cell>86.2</cell><cell>85.3</cell><cell>84.8</cell><cell>4.5</cell><cell>1.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Pixel Deflection (Prakash et al. [26])</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inception v-3</cell><cell>91.9</cell><cell>71.1</cell><cell>66.7</cell><cell>58.9</cell><cell>90.9</cell><cell>88.1</cell><cell>90.4</cell><cell>90.1</cell><cell>57.6</cell><cell>21.9</cell></row><row><cell>ResNet-50</cell><cell>92.7</cell><cell>84.6</cell><cell>77.0</cell><cell>66.8</cell><cell>91.2</cell><cell>90.3</cell><cell>91.7</cell><cell>89.6</cell><cell>57.0</cell><cell>29.5</cell></row><row><cell>Inception ResNet v-2</cell><cell>92.1</cell><cell>78.2</cell><cell>75.7</cell><cell>71.6</cell><cell>91.3</cell><cell>88.9</cell><cell>89.7</cell><cell>89.8</cell><cell>57.9</cell><cell>24.6</cell></row><row><cell></cell><cell></cell><cell cols="5">Our work: Wavelet Denoising + Image Super Resolution</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Inception v-3</cell><cell>97.0</cell><cell>94.2</cell><cell>87.9</cell><cell>79.7</cell><cell>96.2</cell><cell>96.1</cell><cell>96.0</cell><cell>95.9</cell><cell>67.9</cell><cell>31.7</cell></row><row><cell>ResNet-50</cell><cell>93.9</cell><cell>86.1</cell><cell>77.2</cell><cell>64.9</cell><cell>92.3</cell><cell>91.5</cell><cell>93.1</cell><cell>92.0</cell><cell>60.7</cell><cell>31.9</cell></row><row><cell>Inception ResNet v-2</cell><cell>98.2</cell><cell>95.3</cell><cell>87.4</cell><cell>82.3</cell><cell>95.8</cell><cell>96.0</cell><cell>95.6</cell><cell>95.0</cell><cell>69.8</cell><cell>35.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Top-1 accuracy comparison for different defense mechanisms on NIPS-DEV dataset on Inception v-3 model. No Defense Das et al. [38] Xie et al. [18] Guo et al. [39] Prakash et al. [26] Ours (SR) Ours (WD + SR)</figDesc><table><row><cell>Attack</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Clean</cell><cell>95.9</cell><cell>89.7</cell><cell>92.0</cell><cell>88.8</cell><cell>86.5</cell><cell>90.4</cell><cell>90.9</cell></row><row><cell>FGSM-2</cell><cell>22.1</cell><cell>58.3</cell><cell>65.2</cell><cell>68.3</cell><cell>70.7</cell><cell>87.1</cell><cell>87.5</cell></row><row><cell>FGSM-5</cell><cell>20.0</cell><cell>50.2</cell><cell>52.7</cell><cell>58.0</cell><cell>62.9</cell><cell>79.6</cell><cell>79.9</cell></row><row><cell>FGSM-10</cell><cell>23.1</cell><cell>43.5</cell><cell>47.5</cell><cell>50.5</cell><cell>54.2</cell><cell>69.8</cell><cell>70.1</cell></row><row><cell>I-FGSM</cell><cell>10.1</cell><cell>75.8</cell><cell>85.3</cell><cell>80.9</cell><cell>86.2</cell><cell>89.7</cell><cell>90.1</cell></row><row><cell>DeepFool</cell><cell>1.0</cell><cell>77.0</cell><cell>84.7</cell><cell>80.1</cell><cell>84.2</cell><cell>90.2</cell><cell>90.4</cell></row><row><cell>C&amp;W</cell><cell>0.3</cell><cell>76.3</cell><cell>84.8</cell><cell>80.3</cell><cell>84.9</cell><cell>90.5</cell><cell>90.7</cell></row><row><cell>MI-FGSM</cell><cell>1.4</cell><cell>72.4</cell><cell>83.6</cell><cell>78.2</cell><cell>84.0</cell><cell>89.4</cell><cell>89.8</cell></row><row><cell>DI 2 FGSM</cell><cell>1.7</cell><cell>2.0</cell><cell>5.1</cell><cell>3.1</cell><cell>54.6</cell><cell>48.9</cell><cell>63.8</cell></row><row><cell>MDI 2 FGSM</cell><cell>0.6</cell><cell>1.3</cell><cell>4.0</cell><cell>1.8</cell><cell>20.4</cell><cell>26.1</cell><cell>28.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Performance comparison of various superresolution techniques in the literature. The up-scaling factor S = 2. Top-1 accuracies are reported.</figDesc><table><row><cell>Attack</cell><cell>No Defense</cell><cell cols="2">SR-ResNet [56] SR-GAN [56]</cell><cell>EDSR [47]</cell></row><row><cell>Clean</cell><cell>100.0</cell><cell>94.0</cell><cell>92.3</cell><cell>96.2</cell></row><row><cell>FGSM-2</cell><cell>31.7</cell><cell>89.5</cell><cell>85.7</cell><cell>92.6</cell></row><row><cell>FGSM-5</cell><cell>28.7</cell><cell>83.7</cell><cell>80.1</cell><cell>85.7</cell></row><row><cell>FGSM-10</cell><cell>30.5</cell><cell>69.9</cell><cell>69.0</cell><cell>73.3</cell></row><row><cell>I-FGSM</cell><cell>11.4</cell><cell>93.4</cell><cell>91.0</cell><cell>95.9</cell></row><row><cell>DeepFool</cell><cell>0.4</cell><cell>93.2</cell><cell>93.0</cell><cell>95.5</cell></row><row><cell>C&amp;W</cell><cell>0.8</cell><cell>93.3</cell><cell>91.3</cell><cell>95.6</cell></row><row><cell>MI-FGSM</cell><cell>1.7</cell><cell>92.6</cell><cell>87.6</cell><cell>95.2</cell></row><row><cell>DI 2 FGSM</cell><cell>1.4</cell><cell>54.3</cell><cell>48.9</cell><cell>57.2</cell></row><row><cell>MDI 2 FGSM</cell><cell>0.6</cell><cell>24.9</cell><cell>23.0</cell><cell>27.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Performance of Nearest Neighbor, Bi-linear and Bi-cubic image resizing techniques as a defense. Evaluation is done on NIPS-DEV dataset using a pretrained Inception v-3 model. US: Upsample; DS: Downsample. The US and DS factor is 2.</figDesc><table><row><cell>Transform</cell><cell cols="3">Nearest Neighbor</cell><cell></cell><cell>Bi-linear</cell><cell></cell><cell></cell><cell>Bi-cubic</cell><cell></cell></row><row><cell>US</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>US → DS</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell></row><row><cell>DS → US</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>Attack</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Clean</cell><cell>94.9</cell><cell cols="2">93.5 84.1</cell><cell cols="3">94.3 91.1 86.2</cell><cell cols="3">93.9 89.1 86.0</cell></row><row><cell>FGSM-2</cell><cell>74.2</cell><cell cols="2">25.9 23.8</cell><cell cols="3">73.5 24.0 21.4</cell><cell cols="3">71.2 20.3 19.5</cell></row><row><cell>FGSM-5</cell><cell>61.0</cell><cell cols="2">18.6 18.1</cell><cell cols="3">60.5 18.1 17.0</cell><cell cols="3">54.8 18.7 18.0</cell></row><row><cell>FGSM-10</cell><cell>52.9</cell><cell cols="2">16.8 16.0</cell><cell cols="3">50.1 15.7 15.4</cell><cell cols="3">49.2 16.2 15.8</cell></row><row><cell>I-FGSM</cell><cell>86.4</cell><cell cols="2">45.9 43.0</cell><cell cols="3">83.4 41.9 40.6</cell><cell cols="3">82.1 37.5 35.6</cell></row><row><cell>DeepFool</cell><cell>87.3</cell><cell cols="2">43.0 41.2</cell><cell cols="3">80.6 40.7 39.7</cell><cell cols="3">80.1 34.5 30.1</cell></row><row><cell>C&amp;W</cell><cell>82.5</cell><cell cols="2">44.4 41.2</cell><cell cols="3">80.1 41.9 37.6</cell><cell cols="3">79.3 39.6 36.0</cell></row><row><cell>MI-FGSM</cell><cell>81.4</cell><cell cols="2">41.0 38.0</cell><cell cols="3">80.0 42.9 40.3</cell><cell cols="3">80.7 41.2 39.8</cell></row><row><cell>DI 2 FGSM</cell><cell>36.0</cell><cell>5.8</cell><cell>3.9</cell><cell>34.8</cell><cell>6.1</cell><cell>4.9</cell><cell>31.2</cell><cell>7.8</cell><cell>5.6</cell></row><row><cell>MDI 2 FGSM</cell><cell>16.1</cell><cell>3.8</cell><cell>2.0</cell><cell>10.2</cell><cell>3.9</cell><cell>3.5</cell><cell>9.6</cell><cell>2.0</cell><cell>1.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table VII shows the top-1 accuracy of each of the models for different adversarial attacks. The results show that, while wavelet denoising helps suppress added adversarial noise, the major performance boost is achieved with image super-resolution. The best</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V :</head><label>V</label><figDesc>Comparison of our robust adversarial training method on CIFAR-10 dataset against various white-box attacks (numbers show robustness, higher is better). We report results without adversarial training (baseline), with adversarial training (AdvTrain) and adversarial training with training dataset augmented with super-resolved images (Robust AdvTrain). Here is the perturbation size and c is the initial constant for C&amp;W attack.</figDesc><table><row><cell>Attacks</cell><cell>Params.</cell><cell>Baseline</cell><cell>AdvTrain</cell><cell>Robust AdvTrain</cell></row><row><cell>No Attack</cell><cell>-</cell><cell>90.8</cell><cell>84.5</cell><cell>87.4</cell></row><row><cell>FGSM</cell><cell>= 0.02 = 0.04</cell><cell>36.5 19.4</cell><cell>44.3 31.0</cell><cell>48.5 37.1</cell></row><row><cell>I-FGSM</cell><cell>= 0.01 = 0.02</cell><cell>26.0 6.1</cell><cell>32.6 7.8</cell><cell>35.3 10.3</cell></row><row><cell>MI-FGSM</cell><cell>= 0.01 = 0.02</cell><cell>26.8 7.4</cell><cell>34.9 9.3</cell><cell>37.7 12.5</cell></row><row><cell></cell><cell>c = 0.001</cell><cell>61.3</cell><cell>67.7</cell><cell>70.9</cell></row><row><cell>C&amp;W</cell><cell>c = 0.01</cell><cell>35.2</cell><cell>40.9</cell><cell>45.5</cell></row><row><cell></cell><cell>c = 0.1</cell><cell>0.6</cell><cell>25.4</cell><cell>30.1</cell></row><row><cell>PGD</cell><cell>= 0.01 = 0.02</cell><cell>23.4 6.0</cell><cell>24.3 7.8</cell><cell>29.6 10.2</cell></row><row><cell></cell><cell>ResNet-110</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Conv(16, 3 × 3) + BN</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ReLU(2 × 2)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Conv(16 * k, 1 × 1) + BN</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Conv(16 * k, 3 × 3) + BN</cell><cell cols="2">×12 k∈{1, 2, 4}</cell><cell></cell></row><row><cell cols="2">Conv(64 * k, 1 × 1) + BN</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>GAP</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>FC(1024)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>FC(10)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI :</head><label>VI</label><figDesc></figDesc><table><row><cell></cell><cell>The</cell></row><row><cell cols="2">convolutional neural</cell></row><row><cell cols="2">network architecture</cell></row><row><cell cols="2">used for adversarial</cell></row><row><cell>training</cell><cell>where</cell></row><row><cell cols="2">the training set is</cell></row><row><cell>augmented</cell><cell>with</cell></row><row><cell>adversarial</cell><cell>and</cell></row><row><cell>super-resolved</cell><cell></cell></row><row><cell>images.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VII :</head><label>VII</label><figDesc>Individual contributions of Wavelet Denoising (WD) and Super Resolution (SR) towards the proposed defense scheme across three different classifiers. Parameters: σ p = 0.04 and S = 2. The proposed defense scheme works well across a range of classifiers.</figDesc><table><row><cell></cell><cell cols="3">Inception v-3 model</cell><cell></cell><cell cols="3">ResNet-50 model</cell><cell></cell><cell cols="4">Inception ResNet v-2 model</cell></row><row><cell>Attack</cell><cell>No Defense</cell><cell>WD</cell><cell>SR</cell><cell>WD+SR</cell><cell>No Defense</cell><cell>WD</cell><cell>SR</cell><cell>WD+SR</cell><cell>No Defense</cell><cell>WD</cell><cell>SR</cell><cell>WD+SR</cell></row><row><cell>Clean</cell><cell>100</cell><cell cols="2">94.0 96.2</cell><cell>97.0</cell><cell>100</cell><cell cols="2">92.7 93.2</cell><cell>93.9</cell><cell>100</cell><cell cols="2">94.0 97.2</cell><cell>98.2</cell></row><row><cell>FGSM-2</cell><cell>31.7</cell><cell cols="2">57.3 92.6</cell><cell>94.2</cell><cell>12.2</cell><cell cols="2">41.4 85.4</cell><cell>86.1</cell><cell>59.4</cell><cell cols="2">70.5 91.8</cell><cell>95.3</cell></row><row><cell>FGSM-5</cell><cell>28.7</cell><cell cols="2">36.4 85.7</cell><cell>87.9</cell><cell>7.0</cell><cell cols="2">12.7 74.0</cell><cell>77.2</cell><cell>55.0</cell><cell cols="2">57.5 85.7</cell><cell>87.4</cell></row><row><cell>FGSM-10</cell><cell>30.5</cell><cell cols="2">32.7 73.3</cell><cell>79.7</cell><cell>6.1</cell><cell>8.6</cell><cell>60.5</cell><cell>64.9</cell><cell>53.6</cell><cell cols="2">55.4 79.4</cell><cell>82.3</cell></row><row><cell>I-FGSM</cell><cell>11.4</cell><cell cols="2">76.4 95.9</cell><cell>96.2</cell><cell>3.4</cell><cell cols="2">71.2 91.0</cell><cell>92.3</cell><cell>21.6</cell><cell cols="2">82.6 94.3</cell><cell>95.8</cell></row><row><cell>DeepFool</cell><cell>0.4</cell><cell cols="2">74.9 95.5</cell><cell>96.1</cell><cell>1.0</cell><cell cols="2">71.8 89.3</cell><cell>91.5</cell><cell>0.1</cell><cell cols="2">79.1 95.4</cell><cell>96.0</cell></row><row><cell>C&amp;W</cell><cell>0.8</cell><cell cols="2">76.3 95.6</cell><cell>96.0</cell><cell>0.1</cell><cell cols="2">79.0 92.0</cell><cell>93.1</cell><cell>0.3</cell><cell cols="2">81.0 94.0</cell><cell>95.6</cell></row><row><cell>MI-FGSM</cell><cell>1.7</cell><cell cols="2">77.0 95.2</cell><cell>95.9</cell><cell>0.4</cell><cell cols="2">71.2 89.6</cell><cell>92.0</cell><cell>0.5</cell><cell cols="2">80.6 93.0</cell><cell>95.0</cell></row><row><cell>DI 2 FGSM</cell><cell>1.4</cell><cell cols="2">18.3 57.2</cell><cell>67.9</cell><cell>0.3</cell><cell cols="2">17.9 49.8</cell><cell>60.7</cell><cell>1.5</cell><cell cols="2">11.9 57.6</cell><cell>69.8</cell></row><row><cell>MDI 2 FGSM</cell><cell>0.6</cell><cell>5.8</cell><cell>27.1</cell><cell>31.7</cell><cell>0.2</cell><cell>9.4</cell><cell>22.4</cell><cell>31.9</cell><cell>0.6</cell><cell>6.9</cell><cell>29.4</cell><cell>35.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VIII :</head><label>VIII</label><figDesc>Cross-Model Transferability Test: PGD adversaries are first generated with = 8/255, using the source network, followed by our image restoration scheme and then evaluated on target model. Note that the cross-model success rate of our defense is higher under black-box settings. Numbers show robustness, higher the better. We further evaluate the black-box settings which can be adapted closest to our approach as follows: firstly, we generate adversarial examples using a source model, secondly, we apply our image restoration techniques comprising of wavelet denoising and image super resolution and finally, we test the images on a target model. The results in TableVIIIshow the robustness of our method under these black-box settings. We note a higher cross-model success rate for our defense under the above-mentioned settings.</figDesc><table><row><cell>Source</cell><cell>Target</cell><cell>ResNet 50</cell><cell>Inception v-3</cell><cell>DenseNet 121</cell></row><row><cell cols="2">ResNet 50</cell><cell>-</cell><cell>84.7</cell><cell>78.0</cell></row><row><cell cols="2">Inception v-3</cell><cell>78.0</cell><cell>-</cell><cell>75.8</cell></row><row><cell cols="2">DenseNet 121</cell><cell>72.8</cell><cell>83.1</cell><cell>-</cell></row><row><cell cols="5">selected from the ILSVRC validation set. These experiments</cell></row><row><cell cols="5">are performed on Inception v-3 model. Table IX shows</cell></row><row><cell cols="5">the classifier performance across different super-resolution</cell></row><row><cell cols="5">scaling factors. We select S = 2, since it clearly shows</cell></row><row><cell cols="5">significantly superior performance. Higher values of S</cell></row><row><cell cols="5">introduce significant high frequency components in the</cell></row><row><cell cols="5">image, which degrade the performance. For σ ρ , we follow</cell></row><row><cell cols="5">[26] and choose σ ρ ∈ {0.03, 0.04, 0.05} as σ ρ = 0.04.</cell></row><row><cell cols="4">Cross-Model Transferability Test:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE IX :</head><label>IX</label><figDesc>Selection of super-resolution scaling factor. S = 2 is selected due to its superior performance.CAMs Visualization: Class Activation Maps (CAMs)<ref type="bibr" target="#b57">[58]</ref> are weakly supervised localization techniques, which are helpful in interpreting the predictions of the CNN model by providing a visualization of discriminative regions in an image. CAMs are generated by replacing the last fully connected layer by a global average pooling (GAP) layer. A class weighted average of the outputs of the GAP results in a heat map which can localize the discriminative regions in the image responsible for the predicted class labels. Fig.8 and 9show the CAMs for the top-1 prediction of Inception v-3 model for clean, attacked and recovered image samples. It can be observed that mapping an adversarial image to higher resolution destroys most of the noisy patterns, recovering CAMs similar to the clean images.</figDesc><table><row><cell>Attack</cell><cell cols="2">No Defense S = 2</cell><cell>S = 3</cell><cell>S = 4</cell></row><row><cell>Clean</cell><cell>100</cell><cell>97.2</cell><cell>79.0</cell><cell>59.2</cell></row><row><cell>FGSM</cell><cell>31.7</cell><cell>92.9</cell><cell>76.2</cell><cell>58.8</cell></row><row><cell>C&amp;W</cell><cell>0.3</cell><cell>95.8</cell><cell>77.7</cell><cell>58.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE X :</head><label>X</label><figDesc>Performance of the proposed defense for NIPS 2017</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/tensorflow/models/tree/master/research/slim</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/dongyp13/Non-Targeted-Adversarial-Attacks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/cihangxie/DI-2-FGSM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/poloclub/jpeg-defense</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/cihangxie/NIPS2017_adv_challenge_defense</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://github.com/facebookresearch/adversarial_image_defenses</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://github.com/iamaaditya/pixel-deflection</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A deep network solution for attention and aesthetics aware photo cropping</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1531" to="1544" />
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quadruplet network with one-shot learning for fast visual object tracking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3516" to="3527" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning a driving simulator</title>
		<author>
			<persName><forename type="first">E</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.01230</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning in medical imaging: general overview</title>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Korean journal of radiology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="570" to="584" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6572" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Threat of adversarial attacks on deep learning in computer vision: A survey</title>
		<author>
			<persName><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00553</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Symposium on Security and Privacy (SP</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial defense by restricting the hidden space of deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mitigating evasion attacks to deep neural networks via region-based classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual Computer Security Applications Conference</title>
				<meeting>the 33rd Annual Computer Security Applications Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On detecting adversarial perturbations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischoff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04267</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Detecting adversarial samples from artifacts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Curtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shintre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00410</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Magnet: a two-pronged defense against adversarial examples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deflecting adversarial attacks with pixel deflection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dilillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Storer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8571" to="8580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02976</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Feature denoising for improving adversarial robustness</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03411</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Parseval networks: Improving robustness to adversarial examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.08847</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A study of the effect of jpg compression on adversarial images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00853</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression</title>
		<author>
			<persName><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanbhogue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kounavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02900</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Foveationbased mechanisms alleviate adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06292</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Improving transferability of adversarial examples with input diversity</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.06978</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Defense-gan: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06605</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Shield: Fast, practical defense and vaccination for deep learning using jpeg compression</title>
		<author>
			<persName><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanbhogue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hohman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kounavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06816</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00117</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01155</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Introduction to semi-supervised learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on artificial intelligence and machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="130" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">On the (statistical) detection of adversarial examples</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06280</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Adversarial and clean data are not twins</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Ku</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04960</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Prior networks for detection of adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02575</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition (CVPR) workshops</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1122" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005. 2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ideal spatial adaptation by wavelet shrinkage</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">biometrika</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="455" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adaptive wavelet thresholding for image denoising and compression</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1532" to="1546" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Adversarial attacks and defences competition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00097</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">on the cleverhans v2.1.0 adversarial examples library</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Matyasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Behzadan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sheatsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gierke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Long</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00768</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Foolbox: A python toolbox to benchmark the robustness of machine learning models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04131</idno>
		<ptr target="http://arxiv.org/abs/1707.04131" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Motivating the rules of the game for adversarial example research</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06732</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Synthesizing robust adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
