<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A k-Nearest Neighbor Based Algorithm for Multi-label Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
							<email>zhangml@lamda.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210093</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
							<email>zhouzh@lamda.nju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210093</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A k-Nearest Neighbor Based Algorithm for Multi-label Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C020934B2C7EB36861AEBEAC8DE8AB55</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In multi-label learning, each instance in the training set is associated with a set of labels, and the task is to output a label set whose size is unknown a priori for each unseen instance. In this paper, a multi-label lazy learning approach named ML- kNN is presented, which is derived from the traditional k-nearest neighbor (kNN) algorithm. In detail, for each new instance, its k nearest neighbors are firstly identified. After that, according to the label sets of these neighboring instances, maximum a posteriori (MAP) principle is utilized to determine the label set for the new instance. Experiments on a real-world multi-label bioinformatic data show that ML-kNN is highly comparable to existing multi-label learning algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Multi-label classification tasks are ubiquitous in real-world problems. For example, in text categorization, each document may belong to several predefined topics; in bioinformatics, one protein may have many effects on a cell when predicting its functional classes. In either case, instances in the training set are each associated with a set of labels, and the task is to output the label set for the unseen instance whose set size is not available a priori.</p><p>Traditional two-class and multi-class problems can both be cast into multi-label ones by restricting each instance to have only one label. However, the generality of multi-label problem makes it more difficult to learn. An intuitive approach to solve multi-label problem is to decompose it into multiple independent binary classification problems (one per category). But this kind of method does not consider the correlations between the different labels of each instance. Fortunately, several approaches specially designed for multi-label classification have been proposed, such as multi-label text categorization algorithms <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, multi-label decision trees <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and multi-label kernel method <ref type="bibr" target="#b5">[6]</ref>, etc. However, multi-label lazy learning approach is still not available. In this paper, this problem is addressed by a multi-label classification algorithm named ML-kNN, i.e. Multi-Label k-Nearest Neighbor, which is derived from the popular k-nearest neighbor (kNN) algorithm <ref type="bibr" target="#b6">[7]</ref>. ML-kNN first identifies the k nearest neighbors of the test instance where the label sets of its neighboring instances are obtained. After that, maximum a posteriori (MAP) principle is employed to predict the set of labels of the test instance.</p><p>The rest of this paper is organized as follows. Section 2 re- views previous works on multi-label learning and summarizes different evaluation criteria used in this area. Section 3 presents the ML-kNN algorithm. Section 4 reports experimental results on a real-world multi-label bioinformatic data. Finally, Section 5 concludes and indicates several issues for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MULTI-LABEL LEARNING</head><p>Research of multi-label learning was initially motivated by the difficulty of concept ambiguity encountered in text categorization, where each document may belong to several topics (labels) simultaneously. One famous approach to solv- ing this problem is BoosTexter proposed by Schapire and Singer <ref type="bibr" target="#b1">[2]</ref> , which is in fact extended from the popular ensemble learning method AdaBoost [8]. In the training phase, BoosTexter maintains a set of weights over both training examples and their labels, where training examples and their corresponding labels that are hard (easy) to predict correctly get incrementally higher (lower) weights. Following the work of BoosTexter, multi-label learning has attracted many atten- tions from machine learning researchers.</p><p>In 1999, McCallum [1] proposed a Bayesian approach to multi-label document classification, where a mixture probabilistic model is assumed to generate each document and EM <ref type="bibr" target="#b8">[9]</ref> algorithm is utilized to learn the mixture weights and the word distributions in each mixture component. In 2001, through defining a special cost function based on Ranking Loss (as shown in Eq.( <ref type="formula">5</ref>)) and the corresponding margin for multi-label models, Elisseeff and Weston [6] proposed a kernel method for multi-label classification. In the same year, Clare and King [4] adapted C4.5 decision tree [10] to handle multi-label data through modifying the definition of entropy. One year later, using independent word-based Bag-of- Words representation <ref type="bibr" target="#b10">[11]</ref>, Ueda and Saito [3] presented two types of probabilistic generative models for multi-label text called parametric mixture models (PMM1, PMM2), where the basic assumption under PMMs is that multi-label text has a mixture of characteristic words appearing in single-label text that belong to each category of the multi-categories. In the same year, Comite et al. [5] extended alternating decision tree <ref type="bibr" target="#b11">[12]</ref> to handle multi-label data, where the AdaBoost.MH algorithm proposed by Schapire and Singer [13] is employed to train the multi-label alternating decision tree. In 2004, Boutell et al. <ref type="bibr" target="#b13">[14]</ref> applied multi-label learning techniques to scene classification. They decomposed the multi-label learning problem into multiple independent binary classification prob- lems (one per category), where each example associated with label set Y will be regarded as positive example when building 0-7803-9017-2/05/$20.00 02005 IEEE classifier for class y E Y while regarded as negative example when building classifier for class y V Y.</p><p>It is worth noting that in multi-label learning paradigm, various evaluation criteria have been proposed to measure the performance of a multi-label learning system. Let X = RZd be the d-dimensional instance domain and let Y = {1, 2, ..., Q} be a set of labels or classes. Given a learning set S =&lt; (Xi,YI),...,(xm,Ym) &gt;e (X x 2Y)m i.i.d. drawn from an unknown distribution D, the goal of the learning system is to output a multi-label classifier h: X -รท 2y which optimizes some specific evaluation criterion. However, in most cases, the learning system will produce a ranking function of the form f X x Y -yZ R with the interpretation that, for a given instance x, the labels in Y should be ordered according to f(x, -). That is, a label 11 is considered to be ranked higher than 12 if f(x, 11) &gt; f(x, 12). If Y is the associated label set for x, then a successful learning system will tend to rank labels in Y higher than those not in Y. Note that the corresponding multi-label classifier h(.) can be conveniently derived from the ranking function f(, )</p><formula xml:id="formula_0">h(x) = {IIf(x, 1) &gt; t(x), 1 E Y} (1)</formula><p>where t(x) is the threshold function which is usually set to be the zero constant function.</p><p>Based on the above notations, several evaluation criteria can be defined in multi-label learning as shown in <ref type="bibr" target="#b1">[2]</ref>. Given a set of multi-label instances S = {(xi, Y,), ..., (xm, Ym)}, a learned ranking function f(., ) and the corresponding multi- label classifier h(.), the first evaluation criterion to be intro- duced is the so-called Hamming Loss defined as:</p><formula xml:id="formula_1">HLs(h) = - h(xi)AYj (2) i=1Q</formula><p>where A stands for the symmetric difference between two sets.</p><p>The smaller the value of HLs (h), the better the classifier's performance. When IYiI = 1 for all instances, a multi- label system is in fact a multi-class single-label one and the Hamming Loss is 2 times the loss of the usual classification Q error.</p><p>While Hamming Loss is based on the multi-label classifier h(-), the following measurements will be defined based on the ranking function f (., .). The first ranking-based measurement to be considered is One-error:</p><formula xml:id="formula_2">Im One -errs (f) = -E H (xi), where i=1 H(xi) { , if argmaXkE yf(xi,k) E Yi (3)</formula><p>The smaller the value of One-errs(f), the better the perfor- mance. Note that, for single-label classification problems, the One-error is identical to ordinary classification error.</p><p>The second ranking-based measurement to be introduced is It measures how far we need, on the average, to go down the list of labels in order to cover all the possible labels assigned to an instance. The smaller the value of Coverages(f), the better the performance. It represents the average fraction of pairs that are not correctly ordered. The smaller the value of RLs(f), the better the performance.</p><p>The fourth evaluation criterion for the ranking function is Average Precision, which is originally used in information retrieval (IR) systems to evaluate the document ranking performance for query retrieval <ref type="bibr" target="#b14">[15]</ref>. Nevertheless, it is used here to measure the effectiveness of the label rankings:</p><p>Ave.precs(f) =-ยฑ 4 P(xi), where P (xk) _ Y l{lf(xi, I) &gt; f(xi, k), I E Yi}l <ref type="bibr" target="#b5">(6)</ref> kcYi If llf(xi, 1) &gt; f (xi, k), 1 E Yll In words, this measurement evaluates the average fraction of labels ranked above a particular label 1 C Yi which actually are in Yi. Note that when Ave-precs(f) = 1, the learning system achieves the perfect performance. The bigger the value of Ave precs(f), the better the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ML-kNN</head><p>As reviewed in the above Section, although there have been several learning algorithms specially designed for multi-label learning, developing lazy learning approach for multi-label problems is still an unsolved issue. In this section, a novel k-nearest neighbor based method for multi-label classification named ML-kNN is presented. To begin, several notations are introduced in addition to those used in Section 2 to simplify the derivation of ML-kNN.</p><p>Given an instance x and its associated label set Yx C Y, suppose k nearest neighbors are considered in the kNN method. Let y. be the category vector for x, where its 1- th component ji (l) (1 c Y) takes the value of 1 if 1 c Yx and 0 otherwise. In addition, let N(x) denote the index set of the k nearest neighbors of x identified in the training set. f(ill Wif(Xi. 11) &lt;-f(xi, lo), (11, lo) c Yi x Yil [Y, rt*]=ML-kNN(S, k, t, s) %Computing the prior probabilities P(Hil)</p><p>(1) for 1 Y do m (2) P(H1) = (s-+ E ' ,j (l))/(s x 2+m) i=l1 (3) P(H' ) = 1 -P(HI); %Computing the posterior probabilities P(Ejl Hil) (4) Identify N(xi), i E {1, .... ml; (5) for 1 Ydo <ref type="bibr" target="#b5">(6)</ref> for j E {,..., k} do <ref type="bibr" target="#b6">(7)</ref> c[j] = 0; c'[j] = 0;</p><p>(8) for i E {1, . . ., m} do (</p><formula xml:id="formula_3">(I) = E Yxa (I); aEN(xi)<label>9) 6 cxi</label></formula><p>if (Y',(l</p><formula xml:id="formula_5">) == 1) then c[6] = c[S] + 1; (11) else c'[6] = c'[6] + 1;<label>(12) (13) (14)</label></formula><p>for j E {0, ..., k} do</p><formula xml:id="formula_6">P(EjIH H)= s+C[j] ; sx(k+l)+E c[p] p=O P(Ej IH H) = s+c[i] ; sx(k+l)+E c'[p]</formula><p>p=O %Computing Y and rt (15) Identify N(t); Thus, based on the label sets of these neighbors, a membership counting vector can be defined as: cx(l) S E a(l) lE Y <ref type="bibr" target="#b6">(7)</ref> aEN(x)</p><p>where CX(1) counts how many neighbors of x belong to the l-th class.</p><p>For each test instance t, ML-kNN first identifies its k nearest neighbors N(t). Let Hf be the event that t has label 1, while Hl be the event that t has not label 1. Furthermore, let El (j E {0,... ,k}) denote the event that, among the k nearest neighbors of t, there are exactly j instances which have label 1. Therefore, based on the membership counting vector Ct, the category vector Yj is determined using the following maximum a posteriori principle:</p><p>Y' (l) = arg max P(HbEC(I))l 1 I Y (8)</p><p>Using the Bayesian rule, Eq.( <ref type="formula">8</ref>) can be rewritten as: P(Hbl)P(PCt(l,Hbl)</p><p>Y') = arg max Ct(l)</p><p>= arg max P(H )P(Et (l) IH)</p><p>Note that the prior probabilities P(Hb) (1 E y, b E {0, 1})</p><p>and the posterior probabilities P(Ej IHb) (j E {0, ... , k}) can all be estimated from the training set S. Figure <ref type="figure" target="#fig_2">1</ref> illustrates the complete description of ML-kNN. The meanings of the input arguments S, k, t and the output argument jjt are the same as described previously. While the input argument s is a smoothing parameter controlling the strength of uniform prior (In this paper, s is set to be 1 which yields the Laplace smoothing). rit is a real-valued vector calculated for ranking labels in Y, where rt(1) corresponds to the posterior probability P(H' I E (i)) As shown in Figure <ref type="figure" target="#fig_2">1</ref>, based on the training instances, steps from (1) to (3) estimate the prior probabilities P(HI). Steps from (4) to (14) estimate the posterior probabilities P(EjIH), where c[j] used in each iteration of 1 counts the number of training instances with label 1 whose k nearest neighbors contain exactly j instances with label 1. Correspondingly, c'[j] counts how many training instances without label 1 whose k nearest neighbors contain exactly j instances with label 1. Finally, using the Bayesian rule, steps from (15) to (19) compute the algorithm's outputs based on the estimated probabilities.</p><p>IV. EXPERIMENTS A real-world Yeast gene functional data which has been studied in the literatures <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref> is used for experiments.</p><p>Each gene is associated with a set of functional classes whose maximum size can be potentially more than 190. In order to make it easier, Elisseeff and Weston <ref type="bibr" target="#b5">[6]</ref> preprocessed the data set where only the known structure of the func- tional classes are used. Actually, the whole set of functional classes is structured into hierarchies up to 4 levels deep (see http://mips.gsf.de/proj/yeast/catalogues/funcat/ for more details). In this paper, as what has been done in the literature [6], only functional classes in the top hierarchy are considered. For fair comparison, the same kind of data set division used in the literature <ref type="bibr" target="#b5">[6]</ref> is adopted. In detail, there are 1,500 genes in the training set and 917 in the test set. The input dimension is 103. There are 14 possible class labels and the average number of labels for all genes in the training set is 4.2 ยฑ 1.6.</p><p>Table <ref type="table">I</ref> presents the performance of ML-kNN on the Yeast data when different values of k (number of neighbors) are considered. It can be found that the value of k doesn't</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) = -E C(xi)I -1, where i=1 C(xi) = {llf(xi, 1) &gt; f (xi, l), 1 E Y} and 1' = arg min f(xi,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Let Y denote the</head><label></label><figDesc>complementary set of Y in Y, another ranking-based measurement named Ranking Loss is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Pseudo code of ML-kNN.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>Many thanks to A. Elisseeff and J. Weston for providing the authors with the Yeast data and the implementation details of Rank-SVM. This work was supported by the National Natural Science Foundation of China under the Grant No. 60473046.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table">II</ref> shows the experimental results on the Yeast data of several other multi-label learning algorithms introduced in Section 2 . It is worth noting that a re-implemented version of Rank-SVM <ref type="bibr" target="#b5">[6]</ref> is used in this paper, where polynomial kernels with degree 8 are chosen and the cost parameter C is set to be 1. As for ADTBoost.MH <ref type="bibr" target="#b4">[5]</ref>, the number of boosting steps is set to 30 considering that the performance of the boosting algorithm rarely changes after 30 iterations. Besides, the results of BoosTexter <ref type="bibr" target="#b1">[2]</ref> shown in Table <ref type="table">II</ref> are those reported in the literature <ref type="bibr" target="#b5">[6]</ref>.</p><p>As shown in Table <ref type="table">I</ref> and Table <ref type="table">II</ref>, the performance of ML- kNN is comparable to that of Rank-SVM. Moreover, it is obvious that both algorithms perform significantly better than ADTBoost.MH and BoosTexter. One possible reason for the poor results of BoosTexter may be due to the simple decision function realized by this method <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, the problem of designing multi-label lazy learning approach is addressed, where a k-nearest neighbor based method for multi-label classification named ML-kNN is proposed. Experiments on a multi-label bioinformatic data show that the proposed algorithm is highly competitive to other existing multi-label learners.</p><p>Nevertheless, the experimental results reported in this paper are rather preliminary. Thus, conducting more experiments on other multi-label data sets to fully evaluate the effectiveness of ML-kNN will be an important issue to be explored in the near future. On the other hand, adapting other traditional machine learning approaches such as neural networks to handle multi- label data will be another interesting issue to be investigated.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-label text classification with a mixture model trained by EM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of the AAAI&apos;99 Workshop on Text Learning</title>
		<meeting><address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Boostexter: a boosting-based system for text categorization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="135" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parametric mixture models for multi-label text</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="721" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge discovery in multi-label phenotype data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Clare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science 2168</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Raedt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Siebes</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="42" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning multi-label altenating decision tree from texts and data</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Comite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gilleron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Perner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">2734</biblScope>
			<biblScope unit="page" from="35" to="49" />
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A kernel method for multi-labelled classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="681" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Special Al review issue on lazy learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">M B</forename><surname>Vitanyi</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">904</biblScope>
			<biblScope unit="page" from="23" to="37" />
			<date type="published" when="1995">1995</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistics Society -B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<title level="m">C4.5: Programs for Machine Learning</title>
		<meeting><address><addrLine>San Mateo, California</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive learning algorithms and representation for text categorization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ofthe 7th ACM International Conference on Information and Knowledge Management (CIKM&apos;98)</title>
		<meeting>ofthe 7th ACM International Conference on Information and Knowledge Management (CIKM&apos;98)<address><addrLine>Bethesda, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="148" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The alternating decision tree learning algorithm</title>
		<author>
			<persName><forename type="first">L</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><surname>Mason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th International Conference on Machine Learning (ICML&apos;99)</title>
		<meeting>of the 16th International Conference on Machine Learning (ICML&apos;99)<address><addrLine>Bled, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved boosting algorithms using confidence-rated predictions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th Annual Conference on Computational Learning Theory (COLT&apos;98)</title>
		<meeting>of the 11th Annual Conference on Computational Learning Theory (COLT&apos;98)<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="80" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning multi-label scene classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Boutell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1757" to="1771" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Developments in automatic text retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">253</biblScope>
			<biblScope unit="page" from="974" to="980" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining microarray expression data and phylogenetic profiles to learn functional categories using support vector machines</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Grundy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Annual International Conference on Computational Biology, Montr6al</title>
		<meeting>the 5th Annual International Conference on Computational Biology, Montr6al</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="242" to="248" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
