<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">A</forename><surname>Ruiz</surname></persName>
							<email>aruiz@um.es</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Murcia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Engineering and Technology Department</orgName>
								<orgName type="institution">University of Murcia</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DBD59085F2AA86BBA97F5CC1A5344DDE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pedro E. López-de-Teruel was born in Lorca,  Murcia, Spain, in 1970. He received the computer science degree from the University of Murcia, Spain, where he is currently pursuing the Ph.D. degree.</p><p>In 1995, he joined the Computer Science and Systems Department at the University of Murcia. At present, he works for the Engineering and Computer Technology Department at the same institution. His current research areas include machine learning, computer vision, and parallel algorithms and architectures applied to these fields.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nonlinear Kernel-Based Statistical Pattern Analysis Alberto Ruiz, Member, IEEE, and Pedro E. López-de-Teruel Abstract-The eigenstructure of the second-order statistics of a multivariate random population can be inferred from the matrix of pairwise combinations of inner products of the samples. Therefore, it can be also efficiently obtained in the implicit, high-dimensional feature spaces defined by kernel functions. We elaborate on this property to obtain general expressions for immediate derivation of nonlinear counterparts of a number of standard pattern analysis algorithms, including principal component analysis, data compression and denoising, and Fisher's discriminant. The connection between kernel methods and nonparametric density estimation is also illustrated. Using these results we introduce the kernel version of Mahalanobis distance, which originates nonparametric models with unexpected and interesting properties, and also propose a kernel version of the minimum squared error (MSE) linear discriminant function. This learning machine is particularly simple and includes a number of generalized linear models such as the potential functions method or the radial basis function (RBF) network. Our results shed some light on the relative merit of feature spaces and inductive bias in the remarkable generalization properties of the support vector machine (SVM). Although in most situations the SVM obtains the lowest error rates, exhaustive experiments with synthetic and natural data show that simple kernel machines based on pseudoinversion are competitive in problems with appreciable class overlapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Terms-Fisher's discriminant analysis, kernel expansion, Mahalanobis distance, minimum squared error (MSE) estimation, nonlinear feature extraction, nonparametric statistics, pseudoinverse, support vector machine (SVM).</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>N ONLINEAR information processing algorithms can be designed by means of linear techniques in implicit feature spaces induced by kernel functions. This idea can be traced back to the potential functions method <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b0">[1]</ref> (see also <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b54">[55]</ref>) and it has been successfully applied to the support vector machine (SVM), a learning method with controllable capacity which obtains outstanding generalization in high (even infinite) dimensional feature spaces <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>The kernel method can be used if the interactions between elements of the domain occur only through inner products. This suggests the possibility of building nonlinear, kernel-based counterparts of standard pattern analysis algorithms. Recently, a nonlinear feature extraction method has been presented <ref type="bibr" target="#b38">[39]</ref> based on a kernel version of principal component analysis (PCA) and <ref type="bibr" target="#b26">[27]</ref> proposed a nonlinear kernel version of Fisher discriminant analysis. Kernel-based versions of other pattern analysis algorithms have been also proposed in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b16">[17]</ref> among others; the field of the so-called Kernel machines is now extremely active <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p><p>The objective of this paper is twofold. First we present a concise expression for the structure of the second-order statistics of a multivariate random sample, useful for translation into kernel versions of classical pattern analysis algorithms initially based on linear dependencies or Gaussian models. The method is illustrated with immediate derivations of the above mentioned kernel versions of Fisher's discriminant, PCA and associated procedures for nonlinear data compression and denoising. First-order methods (e.g., kernel versions of template matching) are also considered and shown to be closely related to standard nonparametric statistical techniques <ref type="bibr">(Parzen's method)</ref>.</p><p>Using this technique we introduce the kernel version of Mahalanobis distance, not previously considered in the literature, which originates nonparametric models with unexpected and interesting properties, and also propose a kernel version of the minimum squared error (MSE) linear machine. This learning algorithm is particularly simple and includes a number of generalized linear models such as the potential functions method or the radial basis function (RBF) network. In both cases learning is based on matrix pseudoinversion by diagonalization (complexity ), so the run time essentially depends on the size of the training data, and not on problem complexity (e.g., noise or class overlapping).</p><p>The second goal of this paper is to elucidate the relative responsibility of feature spaces and inductive bias in the remarkable properties of SVMs. The SVM combines two powerful ideas: maximum margin classifiers with low capacity and therefore good generalization (based on a quadratic optimization procedure giving rise to sparse solutions), and implicit feature spaces defined by kernel functions. Our experiments show that simple learning algorithms can successfully take advantage of kernel expansions to obtain very satisfactory solutions, although in most cases the highest generalization can only be achieved by the maximum margin strategy of SVMs. However, in situations characterized by large class overlapping, SVM learning is significantly slower and not more accurate than the proposed kernel machines based global statistical properties of the sample.</p><p>This paper is organized as follows. Section II describes the relationship between the eigenstructures of the sample autocorrelation (or covariance) matrix and the matrix of inner products of the sample. Section III extends the results to kernel feature spaces. In Sections IV to VII we use this result to derive kernel versions of standard pattern analysis algorithms. For clarity, exposition follows logical connections instead of chronology: Section IV explores the link between first order kernel methods and nonparametric statistics; Section V introduces the kernel version of Mahalanobis distance and describes some of its prop-erties; Section VI derives kernel PCA and related algorithms; and Section VII considers kernel versions of linear machines, focussing on the MSE criterion. In Section VIII the proposed methods are experimentally contrasted in synthetic and natural problems with state-of-the-art SVM implementations. The last section summarizes the contributions of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE ESSENTIAL RELATION BETWEEN</head><p>AND Objects in a certain domain are usually described by -dimensional attribute vectors , which can be considered as realizations of a multivariate random variable. When the probability density function is unknown or its shape is assumed to be simple, the population is typically characterized in a first approximation by its first-and second-order moments: the mean vector and the covariance matrix . They describe the location and dispersion of the population and are the sufficient statistics for many pattern recognition and data analysis algorithms.</p><p>In practice, we estimate these parameters from a finite sample. Let be a sample matrix containing as rows random observations</p><p>, drawn from the underlying law</p><p>. Consider the following statistics, concisely expressed in terms of the sample matrix:</p><formula xml:id="formula_0">(sample mean)<label>(1)</label></formula><p>(sample autocorrelation matrix)</p><p>(sample covariance matrix) <ref type="bibr" target="#b2">(3)</ref> where denotes a -dimensional vector with all the components equal to . Any real symmetric and positive semidefinite matrix of rank can be diagonalized as where is a (nonsingular) diagonal matrix containing only the positive eigenvalues of , and is a row orthonormal system containing the associated eigenvectors, which span the subspace not degenerate of . denotes the identity matrix of dimension . We will denote this decomposition by the terms "eigenstructure" or "spectral decomposition." The eigenstructure of the covariance matrix discloses some intrinsic geometric properties of the random population, independent from the reference system (e.g., principal directions and associated variances).</p><p>It turns out that the spectral decompositions of and can be obtained, respectively, from the symmetric matrix of pairwise combinations of dot products of the samples <ref type="bibr" target="#b3">(4)</ref> and from the centered matrix defined by <ref type="bibr" target="#b4">(5)</ref> where denotes a matrix with all entries equal to . The following theorem and corollaries are proved in the Appendix.</p><p>Theorem: Let the eigenstructures of and be denoted by (6) (7) (8) <ref type="bibr" target="#b8">(9)</ref> then the following relations are valid: <ref type="bibr" target="#b9">(10)</ref> (11) (12) <ref type="bibr" target="#b12">(13)</ref> This fundamental relation between and is typically applied to efficient computation of eigenstructures in the small sample size case <ref type="bibr">[14, p. 39]</ref>. It is particularly useful for standard PCA in image representation <ref type="bibr">[48, p. 268]</ref>.</p><p>The pseudoinverse of a symmetric square matrix with eigenstructure is computed by inversion of the nonzero eigenvalues:</p><p>. It gives the minimum squared error approximation to the identity and acts as the true identity for both and (e.g., , an so on). The pseudoinverse of a rectangular matrix is computed in terms of the pseudoinverse of the symmetric as , with the property that acts on as a right identity: . In general, the pseudoinverse of any matrix can be computed by inversion of its nonzero singular values. It is typically used to obtain minimum squared error solutions of overconstrained systems of linear equations. Loosely speaking, pseudoinversion restricts inversion to the range of the operator, i.e., the subspace where it is not degenerate. This is unavoidable in high dimensional feature spaces. We will show that pseudoinversion provides reasonable approximations in the algorithms of interest.</p><p>Corollary 1: The following equalities are valid:</p><formula xml:id="formula_2">(14)<label>(15)</label></formula><p>Therefore, the action of or over a target vector can be obtained from certain associated operators working on the linear span of the whole sample. Let denote the expansion of in terms of the inner products with the elements in <ref type="bibr" target="#b15">(16)</ref> For example, the counterpart of in the space of inner products is , since we can write This property is crucial for working in implicit high-dimensional feature spaces defined by kernel functions, as proposed in <ref type="bibr" target="#b38">[39]</ref> for nonlinear PCA.</p><p>The above relations are particular cases of Corollary 2.</p><p>Corollary 2: Let be any symmetric matrix. Then the following equality is valid for any vectors and and any integer power <ref type="bibr" target="#b16">(17)</ref> The equality is trivial for positive powers, but it also holds for negative powers in the pseudoinverse sense. <ref type="foot" target="#foot_0">1</ref> For appropriate and , expression <ref type="bibr" target="#b16">(17)</ref> provides a conceptually elegant tool for building nonlinear versions of standard algorithms. Note that the left side contains matrix-vector products of dimension , very large or even infinite when working in feature spaces, while the dimension involved in the right side equals the sample size . In practical situations we will find sometimes preferable to work with the explicit decomposition <ref type="bibr" target="#b14">(15)</ref>, since requires additional computing effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXTENSION TO KERNEL-BASED FEATURE SPACES</head><p>Let the kernel function define the inner product in some implicit feature space with associated transformation , which need not be computed explicitly For instance, the polynomial kernel <ref type="bibr" target="#b50">[51]</ref>, denoted by , induces features which include products of up to attributes <ref type="bibr" target="#b17">(18)</ref> The Gaussian or RBF kernel <ref type="bibr" target="#b50">[51]</ref>, denoted by , induces an infinite dimensional feature space in which all image vectors have the same norm and become orthogonal when they are distant in the input space with respect to the scale or regularization parameter <ref type="bibr" target="#b18">(19)</ref> Other kernel generating functions are also used, giving rise to neural-network structures, splines or Fourier expansions. Any function verifying Mercer's condition <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b8">[9]</ref> can be used as a kernel.</p><p>Since the interaction between target vectors occurs only in the form of inner products, the results in Section II are also valid in the feature space defined by any desired kernel function . To do this, the matrix must be replaced by <ref type="bibr" target="#b19">(20)</ref> and the expansion must be replaced by <ref type="bibr" target="#b20">(21)</ref> The above transformation is sometimes referred to as the empirical kernel map <ref type="bibr" target="#b40">[41]</ref>. In the following, we will take advantage of the results of the previous section to write kernel versions of standard pattern analysis algorithms.</p><p>IV. FIRST-ORDER KERNEL METHODS AND NONPARAMETRIC STATISTICS One of the simplest pattern classification situations occurs when the attribute vector is normally distributed in each class and the attributes are conditionally independent with the same variance (i.e., spherical classes of the same size). In this case the optimum classification rule depends essentially on the distances from the input to the mean value of each class, which can be reduced to comparison, with an adequate threshold, of the "correlations" <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>. In many realistic situations this simplifying assumption is too inaccurate and more expressive models are required. However, it is interesting to realize that some of these more powerful methods are closely related to minimum distance classifiers in certain feature spaces. Consider Parzen's method <ref type="bibr" target="#b31">[32]</ref> for nonparametric estimation of the probability density of a random sample . Let be a simple (e.g., normal) density with zero mean and standard deviation (width) . Then the unknown density can be estimated as the average value of copies of centered on the available samples The smoothing parameter controls the amount of regularization, and in practice (finite samples) it can be adjusted by crossvalidation <ref type="bibr" target="#b25">[26]</ref>. Note that the Parzen estimator can be written as the average value of the empirical map <ref type="bibr" target="#b20">(21)</ref> with the smoothing density acting as a kernel <ref type="bibr" target="#b21">(22)</ref> This suggests a fundamental relationship between kernel methods and nonparametric statistics. Using (1), can be interpreted in feature space as , the key term for minimum distance classification ( denotes the mean value of the samples in feature space). This result follows from the fact that Parzen estimations are just low-pass filtered versions of the original density, computed by Monte Carlo integration: <ref type="bibr" target="#b22">(23)</ref> In general, expanding in terms of inner products and using (1), ( <ref type="formula">4</ref>), <ref type="bibr" target="#b15">(16)</ref> and redefinitions <ref type="bibr" target="#b19">(20)</ref> and <ref type="bibr" target="#b20">(21)</ref>, the distance to the mean in feature space (denoted by KD) can be written as <ref type="bibr" target="#b23">(24)</ref> For the above case of a kernel derived from a density, we have where constant (independent from ) can be precomputed. Therefore Parzen density estimation (a very flexible kind of model) is equivalent to simple distance to the mean in the implicit feature space induced by the smoothing density . Different kind of interpolating functions (e.g., polynomial) in Parzen estimations need not produce legitimate densities nor reasonable data models. A natural question arises on the modeling quality of KD, the full-fledged distance to the mean in the feature space <ref type="bibr" target="#b23">(24)</ref>, induced by arbitrary but otherwise perfectly admissible kernels, where the Parzen-like term is adjusted by and the average of the entries. In general, this modification (suggested by the theory of kernel expansions, without immediate interpretation in the context of nonparametric statistics) actually creates somewhat crude and inaccurate data models which can be nevertheless useful in particular circumstances (e.g., pattern classification with boundaries of low complexity), although local closeness to data is in general not correctly measured. First-order kernel models are not flexible enough to capture arbitrary data distributions, unless specific kernels are used (e.g., derived from densities, hence producing ordinary Parzen estimations). However, in the following sections we will show that second-order kernel methods, which take into account data dispersion in feature space, originate remarkably satisfactory models with novel and unexpected properties not found in standard nonparametric methods.</p><p>We conclude this section with a brief commentary on the possibility of constructing kernel versions of nonparametric methods themselves. Since they are in some sense equivalent to kernel methods (e.g.,</p><p>), they actually take advantage of the infinite set of features implicitly induced by the smoothing function. Therefore, an additional "kernelization" will have probably no substantial effects. 2 In kernel feature spaces, simple (first-or second-order, e.g., Gaussian) models and complex (higher order, semiparametric, or nonparametric) ones are essentially equivalent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. KERNEL MAHALANOBIS DISTANCE (KM)</head><p>The Mahalanobis distance from a vector to a random population with sample mean and sample covariance matrix is given by the quadratic form <ref type="bibr" target="#b24">(25)</ref> which in terms of the sample matrix can be expressed as <ref type="bibr" target="#b25">(26)</ref> 2 For instance, Parzen estimation in feature space leads to the concept of iterated kernel, where the distance required by the argument of the interpolating density is derived from a generalized inner product. However, neither the iterated kernel nor the corresponding estimate are legitimate densities in input space (in the feature space induced by G no transformed vector lies at an infinite distance of any other transformed vector, so the iterated kernel never vanishes). In any case, the regularization effect of the iterated kernel can be easily accomplished by an ordinary smoothing density with equivalent bandwidth. Other nonparametric methods such as the k-n nearest neighbor classification rule also remain essentially unchanged in the feature space induced by the usual kernel functions.</p><p>where is the square root of the centering matrix required to write with structure <ref type="bibr" target="#b26">(27)</ref> Using the main relation <ref type="bibr" target="#b16">(17)</ref>, we can write <ref type="bibr" target="#b27">(28)</ref> Let us introduce the following notation:</p><p>(29) <ref type="bibr" target="#b29">(30)</ref> where exponent actually denotes the squared pseudoinverse. Clearly, and , which are directly obtained from the matrix of inner products , play, respectively, the role of the mean vector and the inverse of the covariance matrix in the space of projections over . Using the above notation, the Mahalanobis distance can be concisely written as <ref type="bibr" target="#b30">(31)</ref> In practice it is more efficient to write the Mahalanobis distance as the norm of a transformed vector, by using the spectral decomposition for the covariance matrix in terms of [( <ref type="formula">7</ref>), ( <ref type="formula">12</ref>), and ( <ref type="formula">13</ref>)]. Then we have <ref type="bibr" target="#b31">(32)</ref> where <ref type="bibr" target="#b32">(33)</ref> plays the role of the whitening transformation in the projection space.</p><p>Through appropriate redefinition of and , expressions <ref type="bibr" target="#b30">(31)</ref> or <ref type="bibr" target="#b31">(32)</ref> allow immediate computation of Mahalanobis distance in implicit feature spaces defined by kernel functions. Fig. <ref type="figure">1</ref> shows illustrative two-dimensional (2-D) data sets and the level curves and surface plots of the corresponding Mahalanobis distance in different feature spaces. <ref type="foot" target="#foot_1">3</ref> Simple (essentially Gaussian) models in the feature space project back into powerful and expressive models of complex distributions in the input space.</p><p>Through variance equalization in feature space, Kernel Mahalanobis (KM) distance, produces a new kind of nonlinear model with some unexpected properties not found in standard nonparametric methods. Note first that highly satisfactory models can be obtained even with polynomial kernels, which in general are not appropriate for models based only on first-order statistics and are completely useless for Parzen estimation.</p><p>Furthermore, KM presents interesting properties in the feature space induced by Gaussian kernels. With adequate smoothing parameters there is an "overshooting" or "Mexican hat" effect [see Fig. <ref type="figure">1</ref>(e)-(h)], which in some sense detects the boundaries of the distribution. Distance to the population is higher in the neighborhood of the data than in farther locations. This behavior may be useful in certain applications (see Section VIII). In this case we adjust Parzen estimation to obtain a good model for the large cluster, at the cost of destroying localization of the small one. Again, KM obtains a satisfactory model for the whole distribution.</p><p>in the more concentrated clusters, and conversely, small values create artifacts in low dispersion zones. Such mixture distributions require variable parameter techniques <ref type="bibr" target="#b21">[22]</ref>. Gaussian KM presents a remarkable ability to perform automatic, data dependent smoothing. For specific values of the kernel parameter , variance normalization in feature space automatically adapts the effective local bandwidth to reasonable values in different locations of the input space. Structure can be detected and modeled at a much smaller scale than the one attributed to the kernel width in ordinary Parzen estimates, as shown in Fig. <ref type="figure" target="#fig_8">2</ref>. Kernel Mahalanobis distance can be used to design very satisfactory pattern classifiers, as illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>. The classification boundaries are polynomial hypersurfaces of degree 2. So in the input space the boundaries have double degree that the expansion induced by the kernel. Nevertheless, as demonstrated in the experimental section, they do not seem to produce extreme overfitting when compared with kernel linear machines (see the next section) or with Parzen (first-order) models.</p><p>Parzen estimates correspond to spherical Gaussian densities in feature space and, by construction, they are legitimate den-sities in input space. KM distance is based essentially on full covariance Gaussian models in feature space, but unfortunately they do not induce normalized functions in input space. In probabilistic classification, normalization constants affect the decision threshold of the likelihood ratio, which also depends on the a priori probability of the classes, so classifiers directly based on minimum Mahalanobis distance may sometimes show poor performance. Although some normalization strategies could be conceived, in practice KM should be considered just as some kind of relative likelihood function, and classification design must be based on the well-known process of quadratic classifier design explained in <ref type="bibr">[14, p. 154</ref>]. Distances from the input vector to the classes of interest are used as new attributes of the input vectors, and it is in this highly discriminant and low-dimensional feature space where a simple decision function (i.e., linear) can be built with negligible computing effort. In binary classification we can even display 2-D scatter plots [see Fig. <ref type="figure" target="#fig_1">3(d)]</ref>, allowing some kind of human supervision. In this paper most KM classifiers will be automatically designed using linear discriminant functions with MSE to the desired output on the space of KM distances. The interpretation and effects of pseudoinversion are fundamental issues in our approach. In the high dimensional feature spaces considered here, the sample covariance matrices are singular, so the ordinary Mahalanobis distance is infinite almost everywhere. Pseudoinversion computes Mahalanobis distance in the subspace spanned by the samples, extrapolating to the rest of the space by orthogonal projection. This behavior is reasonable in our context, since distances, as a function of the original variables, should not change when the samples are embedded in a space with additional irrelevant attributes. However, pseudoinversion is based on inversion of the nonzero eigenvalues, which raises a numerical stability problem. Due to round-off errors it is not easy to identify the true null eigenvalues if they exist, and what is a more serious problem, certain kernels (e.g., with large ) often produce an extremely ill-conditioned matrix . Model quality and classifier accuracy depend on the amount of spectral mass assumed to be zero. Except when explicitly mentioned, in the experimental section we will always perform pseudoinversion discarding the proportion of the spectral mass , corresponding to the lowest eigenvalues, although fine tuning of this proportion may improve the results in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. KERNEL PRINCIPAL COMPONENT ANALYSIS AND NONLINEAR DENOISING</head><p>The Kernel PCA algorithm introduced in <ref type="bibr" target="#b38">[39]</ref> is a very fruitful contribution to nonlinear pattern processing via implicit feature expansions. Using <ref type="bibr" target="#b14">(15)</ref>, it can be concisely expressed as <ref type="bibr" target="#b33">(34)</ref> where the components of the transformed vector are usually limited to those corresponding to the dominant eigenvalues in . We are inspired in that work, since the principal components are related to the transformations required to compute the Mahalanobis distance and the rest of learning machines proposed here. They show the level curves of some nonlinear principal features in 2-D toy distributions, which tend to correctly separate different subgroups capturing the underlying geometric structure. They also use the transformed vectors as improved inputs for SVMs in classification tasks. Our results continue this research line in several directions.</p><p>First, our experiments suggest that the variance captured by the different eigenvalues may sometimes be a misleading cue to the importance of the associated eigenvector, since some forms of nonlinearity (e.g., powers) may increase dispersion just because of the scale of measurements. In our approach all the information in the second-order statistics is used, i.e., all the nonnegligible eigenvectors are taken into account with suitable weights.</p><p>On the other hand, the two-stage approach to classification mentioned above may not improve significantly the error rates with respect to a SVM with the same kernel working on the original data. In the implicit feature space the SVM must find a separating hyperplane with maximal margin (traded-off with error cost), and this cannot be facilitated by any linear transformation of the data. And the situation becomes worse if dimensionality is reduced. Note also that feature extraction for classification problems should be based on statistical separability criteria (e.g., Fisher's) instead of PCA, oriented to optimal data reconstruction <ref type="bibr" target="#b13">[14]</ref>.</p><p>The efficiency in the execution stage of SVMs is also lost due to the nonsparsity of the feature extraction stage. A more reasonable approach with nearly the same computing effort is building directly a classifier such as some variety of Kernel MSE (proposed in Section VII) or minimum Kernel Mahalanobis distance. As Kernel PCA, these methods do not obtain sparse solutions, <ref type="foot" target="#foot_2">4</ref> but in many cases the classification problem will be essentially solved.</p><p>We will study now the problem of approximate reconstruction of a vector from its principal components in nonlinear feature spaces. The main problem is that not every vector in a high-dimensional feature space has a preimage. We would like to find the vector in the input space whose image is closest to a desired vector in the feature space. The kernel technique can be used when is expressed as a linear combination of images of input vectors. Taking advantage of the results in Section II, we can easily derive a simple procedure for approximate reconstruction of a vector using nonlinear PCA. A more complete treatment can be found in <ref type="bibr" target="#b40">[41]</ref>, which shows attractive results in digit denoising. Let be the reconstruction of a vector from its principal components <ref type="bibr" target="#b34">(35)</ref> where are coordinates of in the base of eigenvectors of the sample covariance matrix, given by <ref type="bibr" target="#b33">(34)</ref>. Inserting (34), ( <ref type="formula">13</ref>) and (1) into the above expression we have <ref type="bibr" target="#b35">(36)</ref> Minimization of reduces to minimization of , which after some manipulation can be concisely written as <ref type="bibr" target="#b36">(37)</ref> Given a target vector , the vector that minimizes can be considered as an approximate reconstruction of from its (nonlinear) principal components. In low-dimensional input spaces, efficient numerical optimization can be performed over the whole input space. In general, (37) can be used to find the point in a given database (e.g., the sample data) whose image is closest to any desired vector. Fig. <ref type="figure" target="#fig_2">4</ref> illustrates this kind of nonlinear denoising in a number of 2-D toy data distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. KERNEL MINIMUM SQUARED ERROR (KL)</head><p>A function is a discriminant (decision) rule for a class of objects if for and otherwise. Linear discriminant functions, defined by , are frequently used because of their mathematical simplicity. A popular method to learn (find) a good weight vector from examples is based on the solution of the following system of linear equations: <ref type="bibr" target="#b37">(38)</ref> where denotes the sample matrix and is the vector of associated class labels: if and otherwise. When the sample size is reasonably large the system is overconstrained and the MSE solution is sometimes recommended <ref type="bibr" target="#b11">[12]</ref>. It provides the best linear asymptotic approximation to the Bayes rule and obtains acceptable separation of classes with simple distributions.</p><p>The vector that minimizes the MSE cost <ref type="bibr" target="#b38">(39)</ref> can be written as <ref type="bibr" target="#b39">(40)</ref> in terms of the pseudoinverse of the rectangular matrix</p><p>A kernel version of the MSE linear discriminant can be directly obtained from <ref type="bibr" target="#b13">(14)</ref>. The discriminant function on a vector can be written as</p><formula xml:id="formula_4">(42) (<label>43</label></formula><formula xml:id="formula_5">) (44)<label>(45)</label></formula><p>where, as usual, denotes the vector of projections of the input vector along the sample and is the pseudoinverse of the matrix of inner products of the sample. When is projected over the sample, the role of in ( <ref type="formula" target="#formula_3">41</ref>) is played by . Expression ( <ref type="formula" target="#formula_5">45</ref>) is particularly elegant and appealing, since it immediately provides nonlinear versions of the discriminant function just by redefinition of and using any desired kernel. This algorithm will be denoted by KL (Kernel MSE Linear machine). Fig. <ref type="figure" target="#fig_3">5</ref> shows classification boundaries in illustrative toy problems.</p><p>The KL machine has exactly the same structure (except sparsity) that the SVM. Squared error minimization does not explicitly favor low VC-dimension boundaries, but KL solutions are still acceptable and frequently similar to those obtained by SVM's. Utilization of global statistical properties of the sample prevents overfitting and controls machine capacity as long as the kernel function has an adequate regularization effect. Learning is based on standard matrix diagonalization (instead of specific quadratic optimization required by SVM). Neither explicit misclassification cost nor tolerance margin in regression must be specified. Despite the nonlinear nature of KL solutions, the average error function ( <ref type="formula">39</ref>) is a convex bowl with a single min- imum, generating deterministic solutions which do not depend on random initialization of weights, frequent in descent/iterative procedures (e.g., backpropagation). Furthermore, MSE approximation can be directly applied to regression problems, just by setting the desired vector in <ref type="bibr" target="#b37">(38)</ref> and <ref type="bibr" target="#b44">(45)</ref> to the observed values of the function (as in standard linear regression). The shape of the regression line is again very close to the one obtained by regression SVMs [see Fig. <ref type="figure" target="#fig_4">6(a)</ref>], although SVM is more resistant to outliers [Fig. <ref type="figure" target="#fig_4">6(b)-(d)]</ref>.</p><p>Note that overfitting in the MSE approach can be controlled by trading-off approximation error flatness in the solution, as in ridge regression or weight decay techniques. To do this we minimize the combined cost function <ref type="bibr" target="#b45">(46)</ref> where the threshold is now made explicit since in general it should not be minimized <ref type="foot" target="#foot_3">5</ref> (it was previously generated automatically by the usual kernel expansions), and the regularization constant specifies the cost of "large" weights. Finding the optimum solution for this new cost function is straightforward and, analogously to <ref type="bibr" target="#b44">(45)</ref>, this kind of regularized solutions can be immediately extended to the kernel case. However, although values yielding quite satisfactory solutions can be found by cross-validation <ref type="bibr" target="#b17">[18]</ref>, it is not easy to develop a theoretically sound method for selecting the best degree of regularization. In contrast, SVM solutions are more robust against the optimization parameters and recently a theoretical method has been pro- posed <ref type="bibr" target="#b36">[37]</ref> for setting them in terms of the noise level. In this context the interpretation of SVM operation as a regularization machine is also interesting <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b15">[16]</ref>. In the experimental section we will evaluate the basic MSE solution, without weight penalization, with the understanding that accuracy may improve by empirical choice of the regularization constant .</p><p>Based on pseudoinversion, KL has also the numerical stability problemalreadymentionedinthecontextofKM.Fortunately,setting to zero the smallest eigenvalues has beneficial regularization effects, avoiding unjustified interpolation components. We have also observed that discarding the small eigenvalues makes the algorithm resistant to inadequate selection of kernel parameters (for instance, too small RBF scales are prone to overfitting). Again, except when explicitly mentioned, in the experimental section we will always perform pseudoinversion discarding the proportion of the whole spectral mass, corresponding to the smallest eigenvalues, although fine tuning of this proportion may sometimes improve classification accuracy.</p><p>KL corresponds to a generalized linear machine <ref type="bibr" target="#b28">[29]</ref> where the input vector is expanded using the whole empirical kernel map. This structure is that of the method of Potential Functions, origin of the kernel method, introduced and studied by <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. They used iterative learning rules (the Perceptron rule for separable problems and stochastic approximation for nonseparable ones) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Instead, KL weights are found by direct (off-line) minimization of the average squared error (MSE criterion) using pseudoinversion. The structure of KL is also closely related to the RBF networks <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b29">[30]</ref>. In this case learning is frequently based on unsupervised heuristics for specifying location and extent (centers and widths) of a reduced number of receptive fields <ref type="bibr" target="#b27">[28]</ref>. KL is a more direct, brute-force approach, allowing any kind of kernel function.</p><p>Working implicitly in the high-dimensional feature space defined by the kernel amounts to working explicitly in the space of kernel projections, with dimension equal to sample size. <ref type="foot" target="#foot_4">6</ref>In consequence, sparsity in the solutions becomes an essential issue, especially in large databases <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b14">[15]</ref>. There is an increasing interest in this problem, and several approaches such as the reduced set method <ref type="bibr" target="#b7">[8]</ref> and the approaches proposed in <ref type="bibr" target="#b30">[31]</ref> could be relevant in our context.</p><p>An empirical approach to sparse representations for KL is suggested by its interpretation as standard linear MSE machine working on the kernel expansion: satisfactory solutions can be also obtained when the expansion is made only along a random sample of the training input vectors and the error is minimized over the whole training data. Algorithmically, this corresponds just to random elimination of a number of columns in the matrix in expression ( <ref type="formula" target="#formula_5">45</ref>) and computation of the pseudoinverse of the resultant rectangular matrix (corresponding to an overconstrained system of equations). Note that this is not at all equivalent to working only with a fraction of the dataset. Partial expansion has even more regularization effects than KL with complete kernel expansion, reducing overfitting. In fact, this strategy was proposed as an early method to locate centers in RBF networks <ref type="bibr" target="#b6">[7]</ref>. In theoretical terms, expanding the optimal (high-dimensional) weight vector as a linear combination of the selected subsample possibly deteriorates the approximation, but this is more than compensated by the regularization effect of solving an overconstrained system of equations. In contrast with ordinary MSE problems, where the pseudoinverse can be computed by ordinary inversion of the lower rank matrix , the design matrix of a partial kernel expansion may still be severely ill-conditioned, requiring again pseudoinversion with fine-tuning of the spectral mass assumed to be zero. This version of the algorithm will be denoted by KLR (KL with Reduced expansion).</p><p>The experiments (Section VIII) show that extraordinary savings in learning (and execution) time in certain large databases can be achieved without loss of accuracy (and often with significant improvements). Unfortunately, the optimal proportion of samples to be selected for the kernel expansion depends on the nature of databases. In any case, to illustrate the strength of this approach, in the experimental section we will evaluate KL both in the simple form <ref type="bibr" target="#b44">(45)</ref> and with kernel expansion restricted to 10% of the samples, randomly chosen 7 (KLR-10%). Again, fine tuning of this proportion may improve the results in practical situations.</p><p>We conclude this section with some comments on kernel versions of alternative learning rules for linear discriminant functo the ordinary MSE solution when the original data X has been transformed into K, by means of the empirical kernel map. Also, Kernel PCA and standard PCA on the kernel expansion are closely related. In contrast, nonlinear SVMs cannot be designed in this way, since distances in the feature space do not depend on the statistical distribution of the samples, which affects the kernel expansion. 7 Other heuristic strategies can be conceived, as for instance, elimination of nearly collinear samples, as measured by the inner products in matrix K. However, this takes some computing effort and our experiments showed no statistically significant improvement in accuracy with respect to random selection.</p><p>tions. The weight vector for a linear discriminant function can be obtained by maximization of Fishers's separability criterion <ref type="bibr" target="#b46">(47)</ref> where and denote, respectively, the mean values and standard deviations of the projections of the elements in each class (i.e., the objects in and the objects not in ). We look for a direction in which the two projected subsamples become as much separated and concentrated as possible, as measured by their first-and second-order statistics. It can be shown <ref type="bibr" target="#b13">[14]</ref> that the weight vector which maximizes this criterion is proportional to the most dominant eigenvector of , where denotes the covariance matrix of the mean values of all classes. In our simple two-class situation the desired direction is <ref type="bibr" target="#b47">(48)</ref> A kernel version of Fisher's discriminant has been proposed in <ref type="bibr" target="#b26">[27]</ref>. Using ( <ref type="formula" target="#formula_2">15</ref>) we obtain a straightforward derivation for this machine <ref type="bibr" target="#b48">(49)</ref> where is a modified vector of desired outputs in which values and are divided by the sample size of each class. However, it is known that the Fisher optimum solution can be obtained from the MSE/pseudoinverse procedure <ref type="bibr" target="#b39">(40)</ref> if the components in the vector of desired outputs in <ref type="bibr" target="#b37">(38)</ref> are changed to the inverse of the proportion of samples in each class <ref type="bibr" target="#b11">[12]</ref>. The computation of the MSE solution is easier and, in addition, it also provides the optimum threshold value, which Fisher's criterion itself is unable to specify. Yet another linear discriminant function that can be directly extended into a kernel version is the optimum classifier for Gaussian classes with a common covariance matrix. It turns out that the discriminant direction in this case is defined again by Fisher's linear discriminant, with a threshold value which depends on the location of the mean values and on the a priori probabilities of each class.</p><p>Note that there are no problem-independent reasons to prefer either of the above inductive principles; nonlinear deformation of the metrics in the feature space may render their assumptions useless in the input space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. EXPERIMENTAL COMPARISON WITH SUPPORT VECTOR MACHINES</head><p>In general terms, matrix pseudoinversion (i.e., finding eigenvalues and eigenvectors) is simpler (and consequently faster) than the quadratic programming (QP) algorithms <ref type="bibr" target="#b3">[4]</ref> required by SVMs in nontrivial problems <ref type="bibr" target="#b44">[45]</ref>. Nevertheless, significant improvements to traditional off-the-self quadratic optimization algorithms have been developed recently, based on a completely different approach to that of classical pivoting methods (in the style of the Simplex method for lineal programming). The so-called interior point algorithms <ref type="bibr" target="#b19">[20]</ref> came up as a response of the operations-research community to the worst-case exponential time of the pivoting methods for lineal and quadratic programming. This is a very active research field, where new variations of the method are frequently proposed, specifically adapted to optimize performance in different problem conditions (such as sparsity, possible nonfeasibility, and so on).</p><p>Moreover, some of these methods have been specifically adapted to solve SVM learning problems, perhaps being SVM <ref type="bibr" target="#b22">[23]</ref> and sequential minimal optimization (SMO) <ref type="bibr" target="#b32">[33]</ref> the most remarkable. These implementations have proved to be very efficient on a number of large-scale databases, but both of them are based on successive decomposition of the original QP problem into smaller ones, which makes them highly data-dependent. In particular, in SMO the smaller optimization problems are taken to the extreme, analytically solvable case of only two variables, thus not requiring the use of an underlying numerical QP library. These decomposition techniques have shown their advantages in many databases in terms of both storage and computation time, but also have some drawbacks. More precisely, the run-time of the methods is very sensitive to overlapped input classes, originating many support vectors. In this kind of situations, the steepest feasible direction used to update the so called working set in SVM -or the heuristic for choosing the two Lagrange multipliers to optimize in SMO-can result in an exponential growing of the decomposition iterations needed to find the correct solution.</p><p>Especially in these problems, matrix pseudoinversion using spectral decomposition of matrices (underlying all of the methods proposed in this paper) can be a reasonable alternative. The run-time of efficient methods for finding eigenvalues and eigenvectors of a symmetric matrix is not data-dependent (except in low-rank or sparse matrices, in which they can be even faster than in the general case). There exist algorithms with worst-case complexity <ref type="bibr" target="#b18">[19]</ref>, and efficient serial and parallel implementations have been developed <ref type="bibr" target="#b10">[11]</ref>. <ref type="foot" target="#foot_5">8</ref>In this section we first present an extensive comparison of the first and second order kernel methods described in this paper-kernel mean (KD) (equivalent to Parzen estimation), kernel Mahalanobis (KM), kernel minimum squared expanding input vectors with 100% of the database (KL) and only with 10% (KLR)-and SVMs. The comparison will be made in terms of both computation time and prediction accuracy over a synthetic nonlinear classification task with controllable class overlapping. Then, in a subsequent section, we will compare these methods and SVM over some standard real-world datasets from the UCI benchmark repository <ref type="bibr" target="#b4">[5]</ref>.</p><p>For simplicity, we used the RBF kernel in all the experiments, with different width values as a way to control capacity. Generally, the wider , the lower the capacity and, hopefully, the better the results on test data, particularly when classes overlap (rote memorization of the training set is avoided). However, if the kernel width is too large the learning machine loses its required nonlinear character. In practice, the optimal kernel parameter, as well as the other free parameters, such as the penalty in SVMs or the percentage of spectral mass discarded in matrix pseudoinversion in KM, KL, and KLR, are chosen by cross-validation, trying different combinations of values. Consequently, fast learning times are very convenient to speed up this first stage.</p><p>Rather than trying to outperform the accuracy of SVM, we will provide evidence that the proposed methods may constitute sometimes a reasonable and efficient alternative in nonlinear problems with large intrinsic error, where the optimal solution must misclassify a number of input samples to avoid overfitting. In these situations, samples near the separating hypersurface are not particularly informative about structure of the classes. Our results also illustrate the influence of the distribution of the input data in the performance of SVM learning QP algorithms, while KL or KM achieve similar error rates requiring fixed computation times that depend only on the size of the matrix . As we will see, in many situations KLR also shows excellent accuracy with negligible learning effort and with the additional advantage of sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Synthetic Data</head><p>We have prepared a synthetic two-attribute nonlinear classification problem with controllable class overlapping. Fig. <ref type="figure" target="#fig_5">7</ref> shows the optimum classification boundaries and the level curves of the class-conditional densities in four classification tasks with a common underlying structure (each class is a mixture of four equiprobable spherical Gaussians), with increasing degrees of overlapping and . We also indicate the Bayes error probabilities of each classification task. Fig. <ref type="figure" target="#fig_6">8</ref> shows some examples of boundaries obtained by the different kernel algorithms, with different values of noise and RBF localization parameter 9  . In order to obtain statistically meaningful conclusions, we compared KD, KM, KL, KLR, and SVM learning times and success rates in the above four classification problems with varying RBF kernel parameters and , and sample sizes and . In all the methods involving pseudoinversion of matrices (KM, KL, and KLR), a proportion of the spectral mass was discarded. That is, only the largest eigenvalues that added up to 99.999% of the total trace of were considered. In the same way, the parameter for SVMs was fixed to 1000. Ten repetitions of the learning stage with independent training sets were executed in each configuration. The error probabilities were estimated using independent test sets of size 3000. Fig. <ref type="figure" target="#fig_7">9</ref> summarizes the results. 10 The 9 Throughout this section, for computational convenience, we will use k(x ; x ) = e = G p (x ; x ). Consequently, more regularized machines are obtained with lower , corresponding to wider RBFs. 10 The implementations of the learning algorithms used in the tests were SVM <ref type="bibr" target="#b22">(Joachims 1998)</ref> in the case of SVMs, and a set of simple programs for KD, KM, KL, and KLR-10% written by the authors using the standard linear algebra libraries BLAS and LAPACK <ref type="bibr" target="#b10">(Dongarra and Walker 1995)</ref>. In the case of SVM , we disallowed the heuristic of considering an active set smaller than the whole database. Although this technique was faster in databases with low noise levels, its performance was unacceptably poor in databases with higher degrees of overlapping (growing exponentially). In consequence, the optimization problem was solved entirely by one execution the underlying QP algorithm (LOQO <ref type="bibr" target="#b52">[53]</ref>), taking into account the whole database. All the implementations were written in C and run in a Pentium III 450 MHz under the Linux O.S. The experiments show that the approximation to the optimal classifier deteriorates as the degree of overlapping increases, but the quality of the solutions obtained by all the methods remains comparable, with a very slight advantage of SVMs (particularly for small sample sizes, due to better capacity control). There are also clearly bad results [see, for example, KD <ref type="bibr">(Parzen)</ref> with and , KM with and , or KLR-10% with and ]. The main reason is that a bad choice of the parameter can affect seriously the performance, and that a fixed proportion of the spectral mass is not always adequate for pseudoinversion. This suggests the necessity of fine-tuning of these parameters in real problems. Anyway, the choice of and affects SVM too (see, for example, and ), so some form of cross-validation is needed in all the methods to adjust parameters.</p><p>Regarding computational efficiency, the experiments clearly confirm that the learning time of SVM, based on constrained quadratic optimization, eventually grows faster, especially for high degrees of overlapping. In contrast, KD and negligible learning times, while KM and KL grow approximately ten and three times slower, respectively. Compare also the variability of SVM run-time (due to the high data dependence of the SVM underlying QP algorithm) with the run-time predict- ability of matrix pseudoinversion. SVM learning is frequently slower for wider RBFs (with a higher regularization effect) required to achieve generalization, trying to avoid rote learning. This has the important consequence that fine tuning of parameters using cross-validation may suppose a heavy computational load. The run-time of methods based in pseudoinversion is more predictable, as it depends only on sample size, and not on the particular distribution of the data or the selected parameters. Finally, observe that for large sample sizes, KLR-10% obtains similar success rates to those of KL, close to the Bayes optimum, requiring negligible learning effort. In some cases, the reduction of the number of vectors for the kernel expansion has also beneficial regularization effects, even outperforming KL with full 100% expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Natural Data</head><p>We have also tested the methods in some datasets obtained from the UCI benchmark repository <ref type="bibr" target="#b4">[5]</ref>. We considered three = 0:2, triangle: = 1, and cross: = 5 (remember that the lower the , the wider the RBF, see footnote 8). Observe how, as we increase the size of the database, the success rates of all the methods get close to the Bayes optimum (see Fig. <ref type="figure" target="#fig_5">7</ref>).</p><p>real-world problems: IONOSPHERE (classification of radar returns from the ionosphere), PIMA (classification of diabetes data in Pima Indians) and ABALONE (estimation of the age of a mollusk from weight, size, and other measurements), as well as the artificial MONK's problems (widely used as benchmarks for inductive learning algorithms <ref type="bibr" target="#b46">[47]</ref>). All the databases are used for binary classification. 11  The problems have different difficulty degrees. MONK1, MONK3, and IONOSPHERE are relatively easy, with a low degree of class overlapping in the input space. MONK2 is slightly harder, in the sense that the classification boundary depends on an abstract combination of features without easy geometrical interpretation. Overlapping is also apparent in ABALONE (samples with target values around the threshold are difficult to classify). Finally, PIMA is a very noisy dataset, 11 ABALONE was originally designed to estimate a numeric value, so we transformed the problem into binary classification by setting a threshold in the target attribute that approximately divided the database into two classes of equal size. All the attributes in datasets IONOSPHERE, PIMA, and ABALONE were first normalized to have zero mean and unit deviation. The MONK's datasets were used directly, without any preprocessing. in which extreme overlapping makes difficult to improve the success rate of optimal random decision (always choosing the majority class).</p><p>Every database was partitioned into three disjoint subsets, (40% of the total size), (40% also) and (20% remainder). Given these partitions, the following procedure was applied to adjust the free parameters of the learning algorithms ( for all methods, penalty in SVM, and the proportion of spectral mass discarded for pseudoinversion, in KM, KL, and KLR): The A subset of each database was always used to train the algorithms with all combinations of in , and in for the SVM or in for the methods needing matrix pseudoinversion. Then the value for KD and the pair for SVM or for KM, KL, and KLR-10% that achieved the lowest error rate when tested on the partition were chosen as the optimal parameter set. The resulting machines were tested then on the partition to obtain an objective measure of their quality in the binary classification problem.  <ref type="table" target="#tab_0">I</ref> summarizes the results of our experiments, in terms of error rates and learning times. For each database, the number of samples and dimension of each vector is shown, as well as the sizes of the respective and subsets, and the proportion of the minority class (the error rate of optimal random classification, to be improved by the learning algorithms). Then, in each cell we show the optimal parameters and error rate for each method (obtained as explained above). The learning time (in CPU minutes) for each training algorithm execution is also shown. The computing environment is the same discussed in Section VIII-A (see footnote 10).</p><p>The results clearly confirm the remarkable generalization power of SVM, demonstrated in learning situations with small class overlapping (MONK1, IONOSPHERE, MONK3) as well as in harder problems (PIMA). However, the rest of the methods can obtain very acceptable test error rates, comparable to SVM in the same problems (see, for example, KL and KM in MONK3, KM in IONO, or KL and KLR-10% in the difficult problem PIMA). Some methods even outperform SVM in certain cases, especially in those with high class overlapping (KL and KLR-10% in ABALONE, or KM in MONK2). In these cases, the regularization effect of average error minimization in KL methods, or the ability to capture the geometry of the classes in KM (combined with elimination of small eigenvalues in pseudoinversion) seem to be more appropriate than finding large margin classifiers with a certain penalty for misclassifications.</p><p>Again, learning times of SVMs are always longer, due to the greater complexity of the underlying QP solver. Nevertheless, note that they are faster in the classification stage, due to sparsity in the solution. This is a great advantage of SVMs in situations where classification time is critical. Of all the methods discussed in this paper, only KLR is sparse in the classification machine generated; in the rest of methods a kernel expansion of the input sample must be computed with each sample of the database, which can be slow. In KM, two additional matrix-vector multiplication operations must be computed, making the classification even slower. But this last method has the additional advantage that it operates in half sized matrices (one for each class), so learning times get divided approximately by four, with respect to those of KL. <ref type="foot" target="#foot_6">12</ref>Finally, note that KLR-10% often obtains similar error rates requiring extremely short learning and classifying times, especially for large databases (PIMA, ABALONE). Here, 90% reduction for the input vector expansion can even have beneficial regularization effects. In very small databases (e.g., MONK's problems), the loss of performance of KLR may be sometimes unacceptable, but in these small problems KL itself is very fast, so reduction of the input vector expansion is not advised. KD (equivalent to Parzen estimation) has virtually null learning times (only the constant has to be computed for nondensity kernels). But its error rates seem to be worse and the method also lacks sparsity. Anyway, it is interesting to notice how a simple mean-distance classifier in feature space often obtains very acceptable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>Inspired by <ref type="bibr" target="#b38">[39]</ref>, we have presented a convenient toolkit for immediate derivation of kernel versions of classical statistical pattern analysis algorithms. The main contributions developed under this framework can be summarized as follows:</p><p>1) We have shown how this setting provides simple derivations for other previously proposed kernel variants of standard methods such as PCA and Fisher's discriminant. The learning algorithms proposed in this paper are essentially based on pseudoinversion, computed through eigendecomposition <ref type="bibr" target="#b18">[19]</ref>, with computational complexity . This is significantly more efficient than off-the-self quadratic programming algorithms <ref type="bibr" target="#b3">[4]</ref>. In problems with high class overlapping requiring a large number of support vectors it is even faster than algorithms especially designed for SVM optimization <ref type="bibr" target="#b22">[23]</ref>. 13  Such hard optimization process is unavoidable for the powerful inductive bias of SVM, which, at the same time, finds sparse solutions with low VC dimension.</p><p>Our experiments confirm that simple kernel machines such as KM and KL are competitive with SVMs in problems with 13 Differences in learning time between KL and SVM will be still larger in regression tasks, since SVM require two Lagrange multipliers for each data point, while KL regression and classification only differ in the vector of desired outputs.</p><p>appreciable statistical overlap, where location and covariance structure of the populations carry more information about optimal estimates than the situation of somewhat atypical vectors supporting a large margin classifier.</p><p>We believe that conceptual simplicity and learning efficiency make the approach defended in this paper a useful alternative in certain situations. Its limitations only emphasize the outstanding importance of Vapnik's learning theory. Now, from (51), using ( <ref type="formula">55</ref>) and (57), we have since <ref type="bibr" target="#b16">(17)</ref> immediately follows from (60) when is replaced by , for any symmetric matrix . Using the notation in ( <ref type="formula">8</ref>), <ref type="bibr" target="#b9">(10)</ref>, and (11), we have (61) which justifies (60).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Contour lines and surface plots of nonlinear Mahalanobis distance for illustrative distributions: (a) U shape. (b) Fork shape. (c)-(d) X shape. (e)-(f) Overshooting effect of RBF kernel (see text) for the X shape. (g)-(h) Overshooting effect of KM compared with standard Parzen estimation in the illustrative one-dimensional (1-D) dataset X = f01; 0; 1g. A well-known problem of certain nonparametric methods is the selection of the smoothing parameter for data distribu-tions containing clusters of different dispersion. Large values required by the more disperse clusters destroy density details</figDesc><graphic coords="5,122.58,577.98,348.24,101.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a)-(c) Classification boundaries based on kernel Mahalanobis distance in illustrative toy problems. (d) Scatter plot of dataset (c) in terms of KM distances to both classes, showing that classification based just on minimum KM distance will not work, requiring a more elaborated, though still simple (e.g., linear) decision rule in this space (see text).</figDesc><graphic coords="7,41.88,204.78,246.48,116.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of data denoising through reconstruction from the principal nonlinear components induced by the Gaussian kernel. Note that this procedure is able to find structure in data.</figDesc><graphic coords="8,41.82,206.70,243.60,125.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Classification boundaries of Kernel MSE linear machine (KL) in toy problems, compared with SVM (C = 1000). Note similar artifacts (inductive bias) on both methods.</figDesc><graphic coords="9,39.60,204.06,252.24,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. KL regression (black lines) compared with SVM (C = 1000; " = 0:01) (gray lines). Note the robustness of SVM against outliers.</figDesc><graphic coords="9,307.02,201.30,242.40,122.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Synthetic two-attribute nonlinear classification problems, with different degrees of overlapping.</figDesc><graphic coords="12,38.88,62.28,249.60,307.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Illustration of the boundaries typically obtained by KD (Parzen), KM, KL, KLR-10% and SVM in the synthetic problems, with appropriate parameters. In all cases the size of the training set is 300.</figDesc><graphic coords="12,304.08,349.38,245.28,123.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparison of test errors and execution times obtained by KD, KM, KL, KLR-10% and SVM in the synthetic problem. Line style indicates problem complexity (degree of overlapping): denser lines correspond to higher intrinsic error. Kernel width (amount of regularization) is indicated by symbols: diamond:</figDesc><graphic coords="13,47.04,62.28,499.20,377.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>2 )</head><label>2</label><figDesc>We have illustrated some connections between first order kernel methods and nonparametric density estimation: Parzen's approximation is equivalent to distance to the mean in the feature space induced by the smoothing function.3) We have introduced the kernel version of Mahalanobis distance, a new kind of nonparametric modeling technique with interesting properties caused by variance normalization in feature space. KM automatically adapts the local effective degree of smoothing using a fixed-width Gaussian kernel. It exhibits an "overshooting" effect which emphasizes both presence and absence of samples in input space. Unlike Parzen estimation, correct nonparametric models can be also built using arbitrary (e.g., polynomial) interpolating functions. 4) Finally, we have proposed a particularly simple kernel version of the MSE linear machine (KL), which includes a number of generalized linear models, and shows very satisfactory results in experiments with synthetic and natural data. In large databases, a sparse version of KL based on partial kernel expansion (KLR) has proved to be highly accurate and extremely efficient. The SVM, based on Vapnik's remarkable learning theory, combines low Vapnik-Chervonenkis (VC)-dimension solutions (through maximization of margin) and kernel-based nonlinearity. The machines proposed in this paper are especially useful to ascertain the relative responsibility of kernel feature spaces and inductive bias in the extraordinary generalization ability of SVMs. Our experiments indicate that simple algorithms take advantage of kernel-based nonlinearity to obtain very acceptable, although typically suboptimal, solutions. Best generalization in nonlinear problems with low intrinsic error can only be obtained by the superior inductive bias of the SVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>), the equality (58) is equivalent to<ref type="bibr" target="#b12">(13)</ref>, thus completing the proof. Finally, to prove the general expression<ref type="bibr" target="#b16">(17)</ref> it suffices to show that (60)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,87.06,62.28,416.16,163.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,109.38,242.46,371.52,187.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="14,79.68,108.24,431.04,272.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I ERROR</head><label>I</label><figDesc>RATES AND TRAINING TIMES OF KD, KM, KL, KLR-10%, AND SVM ON STANDARD BENCHMARKS FROM THE UCI REPOSITORY. FOR EACH DATABASE AND METHOD, WE SHOW THE SIZE OF THE TRAINING SET, THE DIMENSION OF THE EXAMPLES, AND THE RBF WIDTH USED, THE MAIN VARIABLES INFLUENCING THE LEARNING TIME</figDesc><table><row><cell>Table</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Depending on the properties of A and X and on the dimensions m and l, any of the matrices raised to n and n01, respectively, in the left and right sides of the above expression may be singular; A can be directly computed from the spectral decomposition of A, and it will be also symmetric.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>In figures, we often use logarithmic or negative exponential scaling functions to emphasize model structure.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>In fact, in classification problems with large intrinsic error, requiring high degrees of regularization, sparsity of SVM solutions also deteriorates considerably (see the experiments in Section VIII).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>For instance, in the linear regression case, this cost favors horizontal (low derivative) lines, but the offset is not penalized. When used with kernel expansions, the regularized nonlinear solutions are usually very similar whether or not the explicit threshold is taken into account.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>In some cases, kernel versions of standard algorithms can be obtained just by applying the ordinary algorithm on data preprocessed through the empirical kernel map. For instance, in the case of KL, expression (45) corresponds exactly</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>In fact, polynomial time methods for convex quadratic programming are also available<ref type="bibr" target="#b53">[54]</ref>. But they are based on iteration of O(n ) single steps to find successive approximations to the optimal solution, with the number of iterations depending also on a precision factor ". This makes them slower than the direct methods available for spectral decomposition of matrices.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6"><p>Supposing 50% of samples in each class, as pseudoinversion is O(m ), inverting two matrices m=2 2 m=2 will be four times faster.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the Associate Editor and the referees for their careful reading and useful suggestions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Spanish CICYT under Grant TIC98-0559</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>We present here a complete derivation of the relationship between and based on <ref type="bibr">[14, p. 39</ref>] and <ref type="bibr" target="#b38">[39]</ref>. We will focus on the more complex case of centered data (the analogous decomposition of the autocorrelation matrix can be derived in a similar fashion). Our goal is to prove <ref type="bibr" target="#b11">(12)</ref> and <ref type="bibr" target="#b12">(13)</ref>. Consider the decomposition of the covariance matrix given by ( <ref type="formula">3</ref>) and ( <ref type="formula">7</ref>)</p><p>Since the eigenvectors necessarily lie in the span of the centered data, they can be written as the following linear combination: <ref type="bibr" target="#b50">(51)</ref> We must find the coefficient matrix in terms of the matrix (built with the inner products of the samples ). Multiplying ( <ref type="formula">50</ref>) by from the left side, and by from the right side, we obtain (after substituting by ) <ref type="bibr" target="#b51">(52)</ref> Now, using the fact that <ref type="bibr" target="#b52">(53)</ref> and multiplying ( <ref type="formula">52</ref>) by the pseudoinverse from the left side, we get the following condition for <ref type="bibr" target="#b53">(54)</ref> Since is diagonal, the elements in are proportional to the eigenvectors of . For the same reason the eigenvalues of are , thus justifying <ref type="bibr" target="#b11">(12)</ref>. Using the notation in (9) we can write <ref type="bibr" target="#b54">(55)</ref> where is some diagonal matrix. We obtain from the orthogonality of the eigenvectors of (56) therefore (57)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Theoretical foundations of the potential function method in pattern recognition learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Aizerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Rozonoer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automat. Remote Contr</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="821" to="837" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Robbins-Monro process and the method of potential functions</title>
	</analytic>
	<monogr>
		<title level="j">Automat. Remote Contr</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1882" to="1885" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Potential function algorithms for pattern recognition learning machines</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Bashkirov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Muchnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automat. Remote Contr</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="629" to="631" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Nonlinear Programming (Theory and Algorithms)</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bazaraa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Sherali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Shetty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">UCI Repository of Machine Learning Databases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/~mlearn/ML-Repository.html" />
	</analytic>
	<monogr>
		<title level="j">Univ. California, Dept. Inform. Comput. Sci</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>Irvine, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th Annu</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</editor>
		<meeting>5th Annu<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multivariate functional interpolation and adaptive networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Broomhead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="321" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simplified support vector decision rules</title>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Int. Conf. Machine Learning, L. Saitta</title>
		<meeting>13th Int. Conf. Machine Learning, L. Saitta<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="71" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A tutorial on support vector machines for pattern recognition</title>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Support vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Software Libraries for Linear Algebra Computations on High Performance Computers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dongarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="151" to="180" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<title level="m">Pattern Classification and Scene Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Kernel-Adatron algorithm: A fast and simple learning procedure for support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Frieß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Int. Conf. Machine Learning</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<title level="m">Introduction to Statistical Pattern Recognition</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An equivalence between sparse approximation and support vector machines</title>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1455" to="1480" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Regularization theory and neural networks architectures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="269" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fuzzy topographic kernel clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th GI Workshop Fuzzy Neurosyst</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Brauer</surname></persName>
		</editor>
		<meeting>5th GI Workshop Fuzzy Neurosyst</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generalized cross-validation as a method for choosing a good ridge parameter</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="223" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Computations</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>The John Hopkins Univ. Press</publisher>
			<pubPlace>Baltimore, MD</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Interior Point Approach to Linear, Quadratic and Convex Programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hertog</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kernel-based equiprobabilistic topographic map formation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Van Hulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1847" to="1871" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recent developments in nonparametric density estimation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Izenman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">413</biblScope>
			<biblScope unit="page" from="205" to="224" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Making large-scale support vector machine learning practical</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th Int. Conf. Pattern Recognition</title>
		<meeting>14th Int. Conf. Pattern Recognition<address><addrLine>Brisbane, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Some results on Tchebycheffian spline functions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kimeldorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Anal. Applicat</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="82" to="95" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The Handbook of Brain Theory and Neural Networks</title>
		<editor>M. A. Arbib</editor>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A comparison of cross-validation techniques in density estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Marron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fisher discriminant analysis with kernels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Neural Networks for Signal Processing Workshop</title>
		<meeting>IEEE Neural Networks for Signal essing Workshop</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast learning in networks of locally tuned processing units</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Darken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="294" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The Mathematical Foundations of] Learning Machines</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Nilsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965. 1990</date>
			<publisher>Morgan Kauffman</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Introduction to Radial Basis Functions Networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J L</forename><surname>Orr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. Edinburgh, U.K., Tech. Rep. Centre Cognitive Sci</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reducing the run-time complexity in support vector machines</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods, Support Vector LearningAdvances in Kernel Methods, Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Included in [38</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On estimation of a probability density function and mode</title>
		<author>
			<persName><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast training of SVM&apos;s using sequential minimal optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods, Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Included in [38</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A sparse representation for function approximation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1445" to="1454" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Properties of support vector machines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="995" to="974" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Support vector machines for 3-D object recognition</title>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="637" to="646" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Shrinking the tube: A new support vector regression algorithm</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances Neural Inform. Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kearns</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Cohn</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m">Advances in Kernel Methods, Support Vector Learning</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">New Support Vector Algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<idno>Rep. TR-1998-031</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Royal Holloway College, Neuro COLT Tech</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Input space vs. feature space in kernel-based methods</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1000" to="1017" />
			<date type="published" when="1999-09">Sept. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Tutorial on Support Vector Regression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno>Rep TR-1998-030</idno>
	</analytic>
	<monogr>
		<title level="m">Royal Holloway College, Neuro COLT Tech</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Kernel Machines Web Page</title>
		<ptr target="http://www.kernel-machines.org" />
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The connection between regularization operators and support vector kernels</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="637" to="649" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On a kernel-based method for pattern recognition, regression, approximation, and operator inversion</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Semiparametric Support Vector and Linear Programming Machines</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Frieß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno>Rep. TR-1998-024</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Royal Holloway College, Neuro COLT Tech</publisher>
		</imprint>
	</monogr>
	<note>to appear in NIPS&apos;98</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The MONK&apos;s Problems. A performance Comparison of Different Learning Algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<idno>CMU-CS-91-197</idno>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Introductory Techniques for 3D Computer Vision</title>
		<author>
			<persName><forename type="first">E</forename><surname>Trucco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Verri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Estimation of Dependences Based on Empirical Data</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Support vector method for function approximation, regression estimation, and signal processing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances Neural Inform. Processing Syst</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Petsche</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="281" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
	</analytic>
	<monogr>
		<title level="m">Statistical Learning Theory (Adaptive and Learning Systems for Signal Processing</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">LOQO: An Interior Point Code for Quadratic Programming</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Vanderbei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Princeton Univ., Tech. Rep. Program Statist. Operations Res</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Nonlinear Optimization: Complexity Issues</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Vavasis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Oxford Sci</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">UCM) in 1990. He then joined the Institute for Industrial Automation (IAI) in Arganda</title>
	</analytic>
	<monogr>
		<title level="m">1987 and the Ph.D. degree from the Complutense University</title>
		<title level="s">Spain. His research interests include machine perception</title>
		<meeting><address><addrLine>Madison; Madrid, Spain; Madrid, Spain; Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Spanish Research Council (CSIC)</publisher>
			<date type="published" when="1964">1997. 1964</date>
		</imprint>
		<respStmt>
			<orgName>Dept. Statistics, Univ. Wisconsin ; Computer Science Department of the University of Murcia</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. 984</note>
	<note>Since 1992, he works for the. pattern recognition, computer vision, and computational learning theory</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
