<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ada-GNN: Adapting to Local Patterns for Improving Graph Neural Networks *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zihan</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center for Big Data Technology and System</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Service Computing Technology and Systems Laboratory</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
							<email>jianxun.lian@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Huang</surname></persName>
							<email>honghuang@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center for Big Data Technology and System</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Service Computing Technology and Systems Laboratory</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hai</forename><surname>Jin</surname></persName>
							<email>hjin@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">National Engineering Research Center for Big Data Technology and System</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Service Computing Technology and Systems Laboratory</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
							<email>xingx@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ada-GNN: Adapting to Local Patterns for Improving Graph Neural Networks *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3488560.3498460</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Local Adaption</term>
					<term>Meta-learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have demonstrated strong power in mining various graph-structure data. Since real-world graphs are usually on a large scale, training scalable GNNs has become one of the research trends in recent years. Existing methods only produce one single model to serve all nodes. However, different nodes may exhibit various properties thus require diverse models, especially when the graph is large. Forcing all nodes to share a unified model will decrease the model's expressiveness. What is worse, some small groups' patterns are prone to be ignored by the model due to their minority, making these nodes unpredictable and even some raising potential unfairness problems.</p><p>In this paper, we propose a model-agnostic framework Ada-GNN that provides personalized GNN models for specific sets of nodes. Intuitively, it is desirable that every node has its own model. But considering the efficiency and scalability of the framework, we generate specific GNN models at the subgroup-level rather than individual node-level. To be specific, Ada-GNN first splits the original graph into several non-overlapped subgroups and tags each node with its subgroup label. After that, a meta adapter is proposed to adapt a base GNN model to each subgroup rapidly. To better facilitate the global-to-local knowledge adaption, we design a feature enhancement module that captures the distinctions among different subgroups to improve the Ada-GNN's performance. Ada-GNN is model-agnostic and can be equipped to almost all existing scalable GNN based methods such as GraphSAGE, ClusterGCN, SIGN, and SAGN. We conduct extensive experiments with six popular scalable GNN as base methods on two large-scale datasets, and the results consistently demonstrate the generality and superiority of Ada-GNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Neural networks; Learning latent representations; • Information systems → Personalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, Graph Neural Networks (GNNs) have become increasingly popular due to their versatile abilities in learning graph structure data and widespread applications such as social networks <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b22">[23]</ref>, recommendation systems <ref type="bibr" target="#b5">[6]</ref>  <ref type="bibr" target="#b26">[27]</ref>, and bioinformatics <ref type="bibr" target="#b28">[29]</ref>  <ref type="bibr" target="#b13">[14]</ref>. Considering that the real-world graphs are usually on a large scale, to facilitate the success of GNNs applications, a lot of efforts have been devoted to designing effective strategies to train GNNs efficiently on large-scale graphs. For example, GraphSAGE <ref type="bibr" target="#b3">[4]</ref> performs neighborhood sampling to control the number of neighbors to be aggregated; ClusterGCN <ref type="bibr" target="#b1">[2]</ref> first partitions the original graph into non-overlapped subgraphs with METIS <ref type="bibr" target="#b7">[8]</ref>, then performs graph convolutions on each subgraph. By eliminating the bias during subgraph sampling, GraphSaint <ref type="bibr" target="#b25">[26]</ref> obtains a better way for subgraphs generation. On the other hand, another line of methods, such as SGC <ref type="bibr" target="#b20">[21]</ref>, SIGN <ref type="bibr" target="#b14">[15]</ref>, and SAGN <ref type="bibr" target="#b16">[17]</ref>, decouple the GNN into two parts: pre-processing and post-classification, and the latter can easily apply mini-batch training with the pre-processing output.</p><p>Although the above methods have achieved good performance on large-scale graphs, they only focus on using a unified model for representing all nodes in the graph while ignoring the diversity among nodes. In fact, some researchers have achieved some progress on the personalized model for each node, such as Policy-GNN <ref type="bibr" target="#b9">[10]</ref>, which designs specific aggregation policy for reflecting the diversity among nodes. However, since the web-scaled graphs in the real world, such as Facebook and Twitter, typically have 10 8 ∼ 10 9 nodes <ref type="bibr" target="#b14">[15]</ref>, it is almost infeasible to perform a node-level personalization on large-scale graphs due to the limitation of memory and efficiency, which leads us to consider this problem at a node-group-level (subgroup-level). For large-scale graphs, it is natural for nodes to form local communities as subgraphs, representing some common local patterns or sharing some semantic meanings. For example, in an academic network, researchers working in the same research area can form a subgraph; in the traffic network, nearby POIs can form a subgraph. Different subgraphs may demonstrate different patterns, so that one unified model may not be good enough to represent the subtle properties encoded in different subgraphs.</p><p>To better illustrate the diversity among subgroups, we take a one-million graph (Arxiv1M) for example. We divide the original graph into 30 subgroups by METIS <ref type="bibr" target="#b7">[8]</ref> algorithm. After that, we conduct separate performance tests on each subgroup using SIGN <ref type="bibr" target="#b14">[15]</ref> trained on the whole graph in advance. The results are reported in Figure <ref type="figure" target="#fig_0">1(a)</ref>. Besides the model performance, we also print the distributions for top 10 labels in each subgraph in Figure <ref type="figure" target="#fig_0">1</ref>(b). It can be seen that the performance of the unified model on the whole graph is uneven on each subgroup, and different subgroups have different subgroup-specific data distributions. Moreover, we fine-tune the unified SIGN model with the local instances from each subgroup, the result is also shown in Figure <ref type="figure" target="#fig_0">1</ref>(a). We find that simply applying the pretrain-then-finetune paradigm cannot achieve consistent improvement in general, which indicates its insufficiency in capturing different data distribution and motivates us to design a new method for model personalization at the subgroup-level.</p><p>However, it is non-trivial to design a subgroup-level personalized model for large graphs. The challenges mainly include: 1)How to effectively capture and represent the distinctions among different subgroups (which we call local distinction)? It is obvious that a model cannot perform personalized operations without perceiving the difference among subgroups; 2) How to ensure the useful common knowledge among subgroups (which we call global coherence) is preserved after we eventually acquire personalized local models? Although the distinctions play important roles in personalization, considering the fact that all subgroups are generated from the same large graph, the common characteristics of subgroups should not be ignored during personalized model learning. The performance drop of the pretrain-then-finetune model in some subgroups in Figure <ref type="figure" target="#fig_0">1</ref>(a) exactly supports this assumption.</p><p>To address these challenges, in this paper, we propose a model-agnostic framework, Ada-GNN, to generate different models for different subgraphs after considering both the global coherence and local distinction. Ada-GNN is inspired by the framework of Model-Agnostic Meta-Learning (MAML) <ref type="bibr">[3][12]</ref>, whose goal is to train a base model from a variety of tasks, which can be adapted rapidly to serve for a new task with only a small number of task-specific training instances. Specifically, firstly we use a graph partition algorithm like METIS to divide the whole graph into multiple non-overlapped subgraphs, and tag each node with its corresponding subgraph ID as the group-wise label. Each subgraph can be regarded as a task, since it includes a set of nodes as instances. Next, we design a meta adapter module to learn a good global model from all subgroups and adapt to local models with a few instances in a subgraph. The global-to-local mode can help Ada-GNN both preserve global coherence and learn local distinction. At last, in case that in a normal MAML-like framework, local patterns may not be effectively reflected from a few support instances, we further propose a feature enhancement module to enhance raw features with group-wise signals, so that Ada-GNN can learn the distinctions among subgroups more easily.</p><p>The contributions of this work are summarized as follows:</p><p>• Instead of using one unified model to learn representations for all nodes on a large graph, we propose a model-agnostic framework Ada-GNN for almost all scalable GNNs to improve their performance by generating personalized models at the subgroup-level. To the best of our knowledge, this is the first time to take group-wise personalized model into consideration on large-scale graphs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Graph Neural Networks</head><p>Many of the real-world data is organized as graph structures, such as social, bioinformatics, traffic, and citation networks. Recently, GNNs have attracted increasing attention in both academia and industry, due to their advantages in graph structure learning. For example, GCN <ref type="bibr" target="#b8">[9]</ref> utilizes a simple convolution to aggregate information from node's neighbors, which provides efficient message passing on graphs. In contrast, GAT <ref type="bibr" target="#b19">[20]</ref> uses an attention mechanism to aggregate information from neighbor nodes in a weighted way. Motivated by the Weisfeiler Lehman graph isomorphism test, GIN <ref type="bibr" target="#b23">[24]</ref> uses a graph isomorphism network architecture to enhance the performance of GNNs. In order to stack more GNN layers and aggregate information from higher-order neighborhoods, some researchers try to use skip-connections to increase GNN models' depth. For example, aiming at addressing the problems of over smoothness and vanishing gradient, DeepGCN <ref type="bibr" target="#b12">[13]</ref> borrows the ideas from ResNet <ref type="bibr" target="#b4">[5]</ref> and DenseNet <ref type="bibr" target="#b18">[19]</ref> through residual connections and dense connections to help models go deeper. Wu et al. <ref type="bibr" target="#b21">[22]</ref> offers a more comprehensive survey on existing graph neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scalable GNNs</head><p>How to train GNNs on large graphs (such as a social network with millions of nodes) becomes one bottleneck, because a gradient-based update for one node involves a large number of neighboring nodes, which poses challenges on both computational cost and memory space. To address the challenge, a lot of works have been proposed to improve the time and memory efficiency of GNNs, which can be categorized into three groups. 1) Sampling Based Methods: As an efficient and effective way to alleviate the "neighbor explosion" problem in large graphs, neighbor sampling methods received wide research focus. GraphSAGE <ref type="bibr" target="#b3">[4]</ref> randomly samples several neighbors for each node, which greatly alleviates the memory cost during neighborhood aggregation. Slightly different from GraphSAGE, PinSAGE <ref type="bibr" target="#b24">[25]</ref> uses random walk to calculate the importance of neighbors for weighted sampling. Through sampling the receptive domain of each layer with importance, FastGCN <ref type="bibr" target="#b0">[1]</ref> ensures that the important nodes will have a great chance to be sampled during aggregation.</p><p>2) Subgraph Based Methods: Subgraph based methods mainly aim at restricting the neighborhood search field through generating multiple subgraphs, and then reduce the computation cost. For example, inspired by mini-batch SGD, ClusterGCN <ref type="bibr" target="#b1">[2]</ref> uses non-overlapped partition methods to split the original graph into several subgraphs and then trains GCN in each subgraph with less time and memory consumption. GraphSAINT <ref type="bibr" target="#b25">[26]</ref> also tries to restrict nodes' receptive fields through generating subgraphs with the process of correcting bias and variance during sampling subgraphs.</p><p>3) Decoupling Based Methods: A lot of work tries to decouple the GNN model into graph pre-processing and post classification to reduce the computation complexity. By simply using linear layers as the post classifiers, SGC <ref type="bibr" target="#b20">[21]</ref> achieves competitive performance with much less time consumption after removing intermediate non-linear activation. Inspired by the inception module in computer vision, SIGN <ref type="bibr" target="#b14">[15]</ref> makes each hop's aggregation pass through a multi-layer perceptrons (MLP), and then concatenates the encoded representation as the input for a post MLP classifier. Based on SIGN, the concatenation operation is replaced by attention mechanism in SAGN <ref type="bibr" target="#b17">[18]</ref> to further enhance the expressiveness of post classifier, and label propagation modules are added to improve the performance of model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY 3.1 Problem Formulation</head><p>To stay focus, in this paper we study the node classification problem, but the method can be easily extended to other graph applications such as link prediction.</p><p>A graph is denoted by G = (V, A, X), which consists of 𝑁 = |V | vertices. A ∈ {0, 1} 𝑁 ×𝑁 is the adjacency matrix, with A 𝑖 𝑗 equal to 1 if there is an edge between node 𝑖 and node 𝑗, and 0 otherwise. X ∈ R 𝑁 ×𝐹 denotes the 𝐹 -dimensional attribute vector for each node in V. Let 𝑌 denote the set of node labels, and each node 𝑖 has one unique label 𝑦 𝑖 ∈ 𝑌 .</p><p>Generally, in node classification tasks, only part of nodes are associated with labels during training, denoted as V 𝐿 . The goal is to learn a graph-based mapping function 𝑓 𝜃 : {𝑣 𝑖 , G} ↦ → {1, 2, ..., |𝑌 |} based on V 𝐿 as the training set, so that by leveraging the graph signal, 𝑓 𝜃 can map each unlabeled node 𝑣 𝑖 in V to one label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework Overview</head><p>Traditionally, GNN models such as SGC, ClusterGCN, and SIGN generate one single model 𝑓 𝜃 after training, then use it to make predictions for all nodes. When the graph is large, it is intuitive that subgraphs located in different regions on the graph may have their own special patterns. For example, the traffic map of Beijing is different from that of Shanghai to a great extent, although they are both parts of the traffic map of China. Thus, the final model's expressiveness will be compromised if we force all the nodes in the big graph to share one unified model.</p><p>To address the problem, we propose Ada-GNN, which is a model-agnostic framework, to empower a GNN model with the ability of adapting to local patterns. An overall illustration of Ada-GNN is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. There are three main components in Ada-GNN, including node tagging, meta adapter, and feature enhancement module. Given an original graph G, we first apply a graph partition method to generate 𝑀 non-overlapped subgroups {V 1 , V 2 , ..., V 𝑀 }, then each node will be tagged with the ID of its belonging subgroup. This step is called node tagging. After that, to make subgroup-level patterns easier to be captured, we propose a feature enhancement module to generate additional subgroup features which record the distinctions among subgroups. Finally, inspired by the Model-Agnostic Meta-Learning (MAML) <ref type="bibr">[3][12]</ref> framework, we propose an adaptive learner, called meta adapter, to ensure that a global GNN model can rapidly adapt to local patterns in each subgroup and finish the coarse-to-fine model transition. In addition, we design an fairness controller to alleviate the subgroup-wise unfairness problem in Ada-GNN. In the following sections, we will introduce these components of Ada-GNN in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Node Tagging &amp; Base GNN Model</head><p>Firstly, we employ a graph partition algorithm to divide the original graph into multiple disjoint subgraphs. How to partition the graph is not the research focus of this paper, and here we choose METIS <ref type="bibr" target="#b7">[8]</ref> after considering its high efficiency and effectiveness in graph partition. After that, instead of making each subgraph a separated and independent graph, here we only tag each node with its corresponding subgroup ID, while leaving the original graph structure unchanged, which is different from the operations in ClusterGCN <ref type="bibr" target="#b1">[2]</ref>. The advantages are two-fold: 1) preserving the original graph structure makes the proposed framework compatible with all kinds of base GNN models, because various graph operations like graph partition <ref type="bibr" target="#b1">[2]</ref>[26] and inception-like <ref type="bibr" target="#b14">[15]</ref> propagation are still applicable; 2) node tagging does not cause any information loss due to the edges across subgraphs are not removed.</p><p>To fully represent the information of a node 𝑣 𝑖 , a GNN model aggregates the features from its neighbor nodes and fuses neighbor  information with its own. This aggregation can be repeated multiple times so that high-order neighborhood information can be captured. Ada-GNN is model-agnostic, but for better illustration, we take SAGN <ref type="bibr" target="#b16">[17]</ref> as an example. SAGN aggregates neighbors' messages without feature transformation and non-linear activation:</p><formula xml:id="formula_0">X(𝑘) = Ā X(𝑘−1) (1)</formula><p>where Ā is a transition matrix derived from A through a prepossessing step such as the row-stochastic random walk. X(𝑘) is the 𝑘-hop smoothed node feature matrix. X(0) is the original feature matrix (which stores the node attributes). To obtain the 𝑘−hop representation H (𝑘) , SAGN applies a MLP operation:</p><formula xml:id="formula_1">H (𝑘) = 𝑀𝐿𝑃 (𝑘) ( X(𝑘) )<label>(2)</label></formula><p>Multi-hop's neighborhood information is merged via an attention pooling:</p><formula xml:id="formula_2">H 𝑎𝑡𝑡 = 𝐿 𝑘=0 𝛼 (𝑘) H (𝑘)</formula><p>(3)</p><formula xml:id="formula_3">𝛼 (𝑘) = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 𝑘 (𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈 ([H (𝑘) ||H (0) ] • W 𝑎 ))<label>(4)</label></formula><p>where || means the concatenation operation and 𝛼 (𝑘) is the attention score of 𝑘-hop neighborhood's information. SAGN's final representation of nodes is:</p><formula xml:id="formula_4">H 𝑓 = 𝑀𝐿𝑃 (H 𝑎𝑡𝑡 + XW 𝑟 )<label>(5)</label></formula><p>To fulfill the personalization for different subgroups, possible solutions include (1) finding a most suitable base GNN model for each subgroup, e.g., GraphSAGE for subgroup #1, SAGN for subgroup #2; (2) using one base model, but searching for a set of most suitable hyper-parameters for each subgroup, e.g., different hop depth 𝑘 for different subgroups; (3) using one base model and one set of hyper-parameters, but adapting to different parameters for each subgroup, e.g., different subgroups have different attention parameters W 𝑎 and final merging parameters W 𝑟 . We argue that method (3) is the best choice as a prior study in personalized GNN models, and the reasons are three-fold. Firstly, by sharing model backbone and hyper-parameters among subgroups, the structure of the GNN model is universal so that common knowledge is easy to share across subgroups. Secondly, the flexibility of changing model structure requires some complex techniques such as AutoML, which will increase the computational cost of the framework. Thirdly, theoretically, it is feasible to acquire subgroup personalization via adapting to different model parameters. For instance, if subgroup #1 mainly relies on self-information when neighbors' message is noisy, the final local model will adjust the parameter W 𝑟 in Eq.( <ref type="formula" target="#formula_4">5</ref>) to emphasize the influence of X for subgroup #1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Meta Adapter</head><p>Inspired by the global-to-local learning framework in MAML <ref type="bibr" target="#b2">[3]</ref> <ref type="bibr" target="#b11">[12]</ref>, which aims at training a good global initialization 𝜃 that can help local models rapidly adapt to new tasks, we design a meta adapter to generate personalized models for different subgroups, while sharing common global knowledge from the original graph. The overall process is illustrated in the Meta Adapter module in Figure <ref type="figure" target="#fig_1">2</ref>. Meta adapter first initializes the global model with random parameters 𝜃 . There are two kinds of parameters updates in meta adapter, named local adaption and global optimization. Each subgroup is regarded as one task, with training instances separated into support set and query set. Support set is used for local adaption. In other words, with copying 𝜃 as initialization for a local model 𝜃 ′ 𝑖 , meta adapter trains the local model on support set to match the 𝑖 𝑡ℎ subgroup's pattern. After local updates, the quality of local models is evaluated on the local query sets, and the gradients derived from query sets will guide the updating direction for the global model, which is called global optimization. Through this step, the meta adapter achieves the ultimate goal: to generate a good global model, which can be adapted rapidly to different subgroups based on corresponding task-specific support sets. As for the inference stage, the meta adapter only has the local adaption step, because we do not have query labels for global optimization. More Specifically, meta adapter has the following two processes:</p><p>Train Process. Algorithm 1 in the appendix shows the detailed training process of Ada-GNN. Firstly, the parameters of the global model (line 1) are randomly initialized as 𝜃 . Then, based on a partitioning algorithm, 𝑀 subgroups are generated for node tagging (line 2), denoted as {V 1 , V 2 , • • • , V 𝑀 }. After that, the meta adapter enters the phrase of subgroup-aware model training (lines 4-11). In each subgroup V 𝑖 , training instances (which are labeled nodes in the training set) constitute support set V 𝑖,𝑆 and query set V 𝑖,𝑄 . On the support set, by minimizing the following loss function:</p><formula xml:id="formula_5">L 𝑆 𝑖 (𝜃, G) = 1 |V 𝑖,𝑆 | 𝑗 ∈V 𝑖,𝑆 L (𝑦 𝑗 , ŷ𝑗 (𝜃 ))<label>(6)</label></formula><p>which is also named as train loss, we can get the adapted local parameters 𝜃 ′ 𝑖 :</p><formula xml:id="formula_6">𝜃 ′ 𝑖 = 𝜃 − 𝛾 1 𝜕L 𝑆 𝑖 (𝜃, G) 𝜕𝜃<label>(7)</label></formula><p>where 𝑦 𝑗 , ŷ𝑗 represent the true label and predicted label for node 𝑗, respectively. L is the entropy loss function in most cases. 𝛾 1 is the local learning rate. We hope that through the local adaption step, errors in the query set V 𝑖,𝑄 can be minimized and local distinctions can be learned. To this end, we calculate the evaluation loss in query set:</p><formula xml:id="formula_7">L𝑖 = L 𝑄 𝑖 (𝜃 ′ 𝑖 , G) = 1 |V 𝑖,𝑄 | 𝑗 ∈V 𝑖,𝑄 L (𝑦 𝑗 , ŷ𝑗 (𝜃 ′ 𝑖 ))<label>(8)</label></formula><p>and use it to update the global model with following functions:</p><formula xml:id="formula_8">L 𝑚𝑒𝑡𝑎 = 𝑀 𝑖=1 L𝑖 (9) 𝜃 = 𝜃 − 𝛾 2 𝜕L 𝑚𝑒𝑡𝑎 𝜕𝜃<label>(10)</label></formula><p>where V 𝑖,𝑄 denotes the query set in subgroup V 𝑖 , 𝑀 denotes the subgroup number, L𝑖 denotes the evaluation loss of subgroup V 𝑖 , and 𝛾 2 is the global learning rate. Through accumulating multiple subgroups' evaluation loss and applying it to the global parameters once (line 11 and 13), 𝜃 would be forced to learn the global coherence among all subgroups. Normally, the train nodes and validation nodes in each subgroup should represent support set and query set, respectively. However, because we do not want to use validation sets during training, which might cause an unfair comparison with other scalable GNNs, we re-use the train nodes in each subgroup as query set as well. Thanks to the global-to-local mode, meta adapter could successfully find desirable parameters 𝜃 which could adapt to an optimal space with only 𝐾 steps for all subgroups. The algorithm will repeat the above procedures until 𝜃 is stable. Test Process. The test process of Ada-GNN is very similar to the train process. The only difference is that there is no global optimization for global model 𝜃 anymore. After local adaption, each local model will be directly evaluated through the test nodes in the corresponding subgroup. Note that, there is no need for meta adapter to store all local models' parameters after training. Instead, with a trained global initialization, the global model 𝜃 can rapidly adapts to 𝜃 𝑖 for subgroup 𝑖 via corresponding support sets V 𝑖,𝑆 . In other words, given the trained global parameters 𝜃 , Ada-GNN could rapidly generate all local models (lines 3-6) and perform an evaluation on each subgroup (line 8). The pseudo codes of the test process are shown in Algorithm 2 in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Feature Enhancement Module</head><p>Essentially, the classical MAML framework relies on the distribution of &lt;feature, label&gt; mapping to capture the special patterns in a local task in an implicit manner. However, when the support set is small (which is the common case for a MAML setting), the local pattern cannot be effectively reflected. We argue that to facilitate the local model adaption process, it will be helpful to provide informative group-level signals to describe the local patterns in an explicit manner. These signals should be easy to be obtained, informative to distinguish subgroups, and can directly influence the model prediction <ref type="bibr" target="#b15">[16]</ref>.</p><p>To this end, we propose a feature enhancement module that extracts and appends group-level features to node attributes for better distinction among subgroups. The group-level features include 1) normalized labels distribution; 2) one-hot encoding subgroup ID. The two auxiliary information will be appended to the original node features. Due to the limited number of classes and subgroups, feature enhancement module only adds little computation and memory costs while yielding much better performance. As a result, the new feature vector for node 𝑢 in subgroup V 𝑖 is derived as:</p><formula xml:id="formula_9">x𝑢 = [x 𝑢 , a 𝑖 ]<label>(11)</label></formula><p>where [•, •] indicates the concatenation operation, a 𝑖 denotes the group-level features for subgroup V 𝑖 , which can be represented as:</p><formula xml:id="formula_10">a 𝑖 = [𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛 𝑖 , 𝐼𝐷 𝑖 ] (<label>12</label></formula><formula xml:id="formula_11">)</formula><p>where 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛 𝑖 denotes the label distribution in subgroup V 𝑖 , and 𝐼𝐷 𝑖 denotes the one-hot encoded ID index of subgroup V 𝑖 . After feature enhancement, the meta adapter can perceive and adapt to local patterns better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Group-wise Fairness Discussion</head><p>The classical supervised learning paradigm aims to minimize the overall loss in Eq. 9, which indicates an optimal accuracy performance in the global view. To achieve this, the model will often converge to a tricky state where for some groups the performance is extremely high and for some other groups the performance is extremely low. This is one type of unfairness problems, which means that some groups' performance is sacrificed too much to pursue an overall metrics. From the view of responsible Artificial Intelligence (AI), we hope the performance of different groups should not be biased severely. A natural merit of Ada-GNN is that fairness level can be easily controlled in the framework. Specifically, we slightly modify Eq. 9 to mitigate the potential unfairness problem and let the model spare more effort on subgroups that have poor performance: </p><formula xml:id="formula_12">L * 𝑚𝑒𝑡𝑎 = 𝑀 𝑖=1 𝑤 𝑖 L𝑖 (13)</formula><formula xml:id="formula_13">𝑧 𝑖 = 1/𝑚𝑖𝑐 𝑖 𝑀 𝑖=1 1/𝑚𝑖𝑐 𝑖 (<label>14</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">𝑤 𝑖 = 𝑒𝑥𝑝 ((𝑧 𝑖 − 𝑚𝑎𝑥 (𝑧))/𝜏) 𝑀 𝑖=1 𝑒𝑥𝑝 ((𝑧 𝑖 − 𝑚𝑎𝑥 (𝑧))/𝜏)<label>(15)</label></formula><p>where 𝑚𝑖𝑐 𝑖 is the micro f1 score of subgroup V 𝑖 , and 𝜏 is temperature parameter. With this fairness module, the weights of groups with poor performance will be dynamically lifted during the training process. The fairness module is optional in Ada-GNN, it depends on the trade-off between the global accuracy and group-wise fairness concern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Experimental Settings</head><p>We evaluate Ada-GNN and other baselines on a server with Intel(R) Xeon(R) CPU E5-2680 and GPU Tesla V100 with 32GB Memory. Our experiment environment is Ubuntu 18.04 with CUDA 11.4. All methods are implemented using Python 3.7 with PyTorch 1.8.0 and Deep Graph Library (DGL) 0.6 <ref type="bibr" target="#b27">[28]</ref>. To be fair, we report the average results after repeating each method five times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets.</head><p>We perform experiments on two large-scale graph datasets. The basic statistics are shown in Table <ref type="table" target="#tab_3">1</ref>.</p><p>• Arxiv1M: This is a homogeneous network composed of a subset of papers that are published in arXiv. All paper nodes are indexed by ogbn-papers100M in advance <ref type="bibr" target="#b6">[7]</ref>. Each node represents an arXiv paper, and each directed edge represents the citation relationship between two paper nodes. By averaging the embeddings of words in the title and abstract of the paper, a 128-dimensional feature vector is attached to each paper node. In total, there are 172 classes, which represent 172 arXiv subject areas. • Amazon2M: An undirected item-item graph in which vertices are products sold by Amazon and edges represent whether two items are purchased by the same customer <ref type="bibr" target="#b6">[7]</ref>. Each product node comes with a 100-dimensional feature vector which is generated from the product's description. In total, 47 product categories are used as node labels. Therefore, unlike the large partition size in ClusterGCN, we set subgroups' number 𝑀 as 5 in our work. Besides, to make Ada-GNN adapt more rapidly, the adaption step 𝐾 should not be large, and is set to 5 except in the hyper-parameter study section. Both local learning rate 𝛾 1 and global learning rate 𝛾 2 are set to 0.005. Other hyper-parameters, such as weight decay rate and dropout rate, are all set to be the same with the settings in corresponding local models. The detailed hyper-parameters analysis in Ada-GNN will be further discussed in subsection 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Evaluation of Ada-GNN</head><p>We evaluate the node classification performance of our model -Ada-GNN and other different models on Amazon2M and Arxiv1M.</p><p>To demonstrate that Ada-GNN is a model-agnostic framework and can be applied to different base GNN models, we choose GraphSAGE, SGC, ClusterGCN, GraphSAINT, SIGN, and SAGN as base GNN models in experiments, which are denoted as Ada-GraphSAGE, Ada-SGC, Ada-ClusterGCN, Ada-GraphSAINT, Ada-SIGN, and Ada-SAGN, respectively. Table <ref type="table" target="#tab_5">2</ref> reports the overall performance of our proposed model as well as the baseline methods, from which we have following observations:</p><p>• Equipped with the Ada-GNN framework, all the base models get consistent improvement in both micro-f1 and macro-f1 performance on two datasets, which demonstrates the necessity of local adaption in big graphs and the effectiveness of our proposed framework in improving various types of GNN models. • Among the base models, SAGN achieves the best performance, and Ada-SAGN can further improve it on both Arxiv1M and Amazon2M dataset, reaching the state-of-the-art performance.</p><p>Overall, decoupling based methods (SIGN, SAGN) generally perform better than subgraph based methods (ClusterGCN, GraphSAINT), which is consistent with the conclusions in existing literature <ref type="bibr" target="#b16">[17]</ref>. • All models perform better on Amazon2M than on Arxiv1M. The main reason may be that the number of class in Arxiv1M is much more than that in Amazon2M, which makes it be much harder to predict. In addition, the density (i.e., the average degree of nodes) of the Amazon2M graph is much higher than that of the Arxiv1M graph, which yields more sufficient neighbor information, thus bring better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Subgroup-level Analysis</head><p>In order to analyze the influence of Ada-GNN on each subgroup after learning the local patterns, we set the subgroup number to 5 and compare the results between each base model and its corresponding adapted model at subgroup-level. The results are shown in Figure <ref type="figure" target="#fig_2">3</ref> and Figure <ref type="figure" target="#fig_0">A1</ref> in the appendix. As illustrated in the figures, for most of the cases, Ada-GNN's improvement is consistent across subgroups with different base models, due to the successful subgroup-level personalization. One interesting observation is that all models demonstrate the same bias on subgroup-level's performance trend and unfairness problem. For example, on the Arxiv1M dataset, all models follow the trend that subgroup #1 &gt; subgroup #5 &gt; subgroup #2 &gt; subgroup #4 &gt; subgroup #3, which indicates that the main reason for uneven performance is not due to the model's nature, but caused by diversity of data distribution across different subgroups. This phenomenon supports the motivation of enabling subgroup personalization and the following fairness controller. After applying Ada-GNN, base models' performance on all subgroups are improved, which verifies that the overall gain of Ada-GNN in Table <ref type="table" target="#tab_5">2</ref> does not come from sacrificing one subgroup's performance to remedy another subgroup's performance. Instead, Ada-GNN can help local model to better learn the local patterns of each subgroup, so almost all subgroups get improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this section, we investigate the contribution of the meta adapter and the feature enhancement module to Ada-GNN, by removing one of them from Ada-GNN in a time (for example, without the meta adapter or without the feature enhancement) and check how the performance is impacted. We further compare Ada-GNN with a simple pretrain + finetune paradigm, which regards the base model as a pretrained global model and fine-tunes it with subgroup's samples only before evaluating the subgroup's test samples. From Table <ref type="table" target="#tab_7">3</ref> we have the following observations:</p><p>• The pretrain + finetune paradigm cannot bring consistent improvement over the base model, while Ada-GNN can bring consistent improvement over the base model and in most of the cases outperforms pretrain + finetune (The only exception is with SGC. However, SGC's best performance is still far behind the other models'). This indicates that learning to keep the global coherence is indispensable during adapting to local distinctions. • Removing either the meta adapter or the feature enhancement module will lead to a significant performance drop. For example, in Ada-SIGN, when removing the meta adapter, the performance drops from 0.5960 to 0.5936; while removing the feature enhancement module, it drops from 0.5960 to 0.5714. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Fairness Analysis</head><p>Next, we evaluate the optional module -the fairness controllerto verify if the group-wise variance can be alleviated by adding this module. To be specific, we compare the standard deviations of subgroup-level performance among settings of applying Ada-GNN with (w/) or without (w/o) fairness module, as well as the base model, on the Amazon2M dataset. We follows the experimental setting in Section 4.1 and 𝜏 in Eq.15 is set to 0.01. From Table <ref type="table" target="#tab_8">4</ref> we have these observations:</p><p>• After applying fairness module, the subgroup-level performance's standard deviation of Ada-GNN is indeed reduced with all base models, which indicates that the  performance of all subgroups becomes much more even with the help of fairness module. • The overall performance of Ada-GNN will slightly decrease after applying fairness module. The main reason might be that the fairness module forces meta adapter to focus more on inferior subgroups, and this leads to less improvement on subgroups which are easy to learn. Thus, currently it is trade-off between the optimal accuracy and decent fairness. We leave the problem of fulfilling fairness while keeping optimal accuracy as a future work. Note that, although fairness module will hurt the performance of Ada-GNN to some extent, there are still obvious improvement compared with base GNN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Hyper-parameters Analysis</head><p>We analyze two most important hyper-parameters of Ada-GNN: the number of subgroups 𝑀 during node tagging, and the number of adaption steps 𝐾 in meta adapter. with the increase of 𝑀, there is a downward trend on performance over Arxiv1M, which indicates that a too large subgroups number might cause insufficient training samples, and harm the model performance. On Amazon2M, similar results can be shown in Figure <ref type="figure" target="#fig_1">A2</ref>(a) in the appendix. We also find an interesting observation that in Ada-SGC and Ada-SIGN, the performance increases at first and then drops slowly. We hypothesize that Ada-SGC might benefit from the personalization when we adjust 𝑀 from 2 to 5, while Ada-SIGN's performance get consistent improvement due to better personalization when we increase 𝑀 from 2 to 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.6.2</head><p>The number of adaption steps 𝐾. In this part, we test the impact of the adaption steps 𝐾 on model performance. In expectation, a small number of adaption steps are enough to capture the subgroup pattern, and further increasing the adaption step number will not further improve the performance but cause more computational time. To verify this, we vary the adaption steps 𝐾 in <ref type="bibr">[1, 3 ,5, 10, 20]</ref>. As shown in Figure <ref type="figure" target="#fig_3">4</ref>(b), in Arixv1M, with the increase of 𝐾, performance of Ada-SGC and Ada-SIGN both increase at first until reaching a converged status with slight fluctuation. For conciseness, the results on Amazon2M are provided in Figure <ref type="figure" target="#fig_1">A2</ref>(b) in the appendix. Note that, in some cases, Ada-GNNs even could achieve optimal performance with 𝐾 = 1. The result matches our expectation, which indicates that Ada-GNN could rapidly adapt to local models with limited adaption steps with a trained global initialization. Usually, a setting of 𝐾 = 5 could be near-optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a model-agnostic framework Ada-GNN, which learns personalized models for different subgroups by considering both their distinctions and similarities. We adopt the concept of global-to-local in MAML for designing our meta adapter, which provides Ada-GNN with the capability of rapidly adapting to local models. Moreover, node tagging and feature enhancement module are designed for splitting different subgroups and capturing the distinctions among them, respectively. At last, we discuss the issue of fairness in Ada-GNN and propose fairness module as an optional solution. Extensive experimental results on two large-scale datasets with six different base GNN models demonstrate that Ada-GNN can consistently improve various GNN models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A study of subgroup diversity on the Arxiv1M dataset. (a) We train a SIGN model on the whole dataset (a.k.a the base model), then evaluate the model's performance on each subgroup separately (the blue curve). The performance is not even. We further finetune the base model with each subgroup's data respectively, and evaluate the dedicated model (the orange curve), we find that some subgroups improve but the others drop; (b) The label distribution in different subgroups are diverse.</figDesc><graphic url="image-2.png" coords="2,55.47,83.69,116.52,93.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An overview of the Ada-GNN framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Subgroup-level comparisons between base models and adapted local models on Arxiv1M</figDesc><graphic url="image-21.png" coords="7,320.44,86.10,235.27,137.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The influence of subgroup numbers 𝑀 (a) and adaption steps 𝐾 (b) over Arxiv1M. For conciseness we only demonstrate two base models: SGC and SIGN.</figDesc><graphic url="image-24.png" coords="8,189.91,336.75,91.79,94.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4. 6 . 1</head><label>61</label><figDesc>The number of subgroups 𝑀. We show how the number of subgroups 𝑀 during node tagging would influence Ada-GNN. To study the impact of the subgroup numbers 𝑀 in our model, we investigate the performance of Ada-SGC, Ada-SIGN by varying the values of 𝑀 in [2, 5, 10, 50, 100]. As shown in Figure 4(a),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Global Model Subgroup 1 Subgroup 2 Subgroup M Subgroup feature Original feature matrix Enhanced feature matrix Data forward Backpropagation Concatenation Original graph Node Tagging Feature Enhancement . . .</head><label></label><figDesc></figDesc><table><row><cell cols="2">Local</cell><cell></cell><cell></cell></row><row><cell cols="2">Model Local</cell><cell></cell><cell></cell></row><row><cell cols="2">Model</cell><cell></cell><cell></cell></row><row><cell>𝜽 𝟏</cell><cell>Local Model 𝜽 𝟐 "</cell><cell></cell><cell></cell></row><row><cell></cell><cell>𝜽 𝑴</cell><cell></cell><cell></cell></row><row><cell cols="2">Local</cell><cell></cell><cell></cell></row><row><cell cols="2">Model Local</cell><cell></cell><cell></cell></row><row><cell cols="2">Model</cell><cell></cell><cell></cell></row><row><cell>𝜽 𝟏</cell><cell>Local Model 𝜽 𝟐 "</cell><cell></cell><cell></cell></row><row><cell></cell><cell>𝜽 𝑴</cell><cell></cell><cell></cell></row><row><cell>+ "</cell><cell>∆ 𝒈𝒍𝒐𝒃𝒂𝒍 =</cell><cell>1 𝑀</cell><cell>*  0</cell></row></table><note>" Support Set 𝓥 𝟏,𝑺 Support Set 𝓥 𝟐,𝑺 𝜽 Query Set 𝓥 𝟏,𝑸 Query Set 𝓥 𝟐,𝑸 . . . Inference on 𝓥 𝑴 Inference on 𝓥 𝟏 Inference 𝐨𝐧 𝓥 𝟐 . . . " Support Set 𝓥 𝟏,𝑺 Support Set 𝓥 𝟐,𝑺 . . . " . . . " . . . Support Set 𝓥 𝑴,𝑺 Support Set 𝓥 𝑴,𝑺 Query Set 𝓥 𝑴,𝑸 Global Optimization ∆ 𝒈𝒍𝒐𝒃𝒂𝒍 ∆ 𝑴,𝒍𝒐𝒄𝒂𝒍 ∆ 𝒊,𝒍𝒐𝒄𝒂𝒍 = 𝜕ℒ(𝑌 +,, , 𝑓 -! " 𝑋 +,, ) 𝜕𝜃 +./ 0 𝜕ℒ(𝑌 +,1 , 𝑓 -! " 𝑋 +,1 ) 𝜕𝜃</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Meta Adapter Train Process Test Process Local Adaption Labeled Node ∆ 𝑴,𝒍𝒐𝒄𝒂𝒍 Local Adaption . . . Fairness Module (Optional) Weights Reshaping</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Dataset Statistics</figDesc><table><row><cell>Dataset</cell><cell>Vertices</cell><cell>Edges</cell><cell cols="2">Class Train/Val/Test</cell></row><row><cell>Arxiv1M</cell><cell cols="2">1,546,782 13,701,428</cell><cell>172</cell><cell>0.71 / 0.17 / 0.12</cell></row><row><cell cols="3">Amazon2M 2,449,029 61,859,140</cell><cell>47</cell><cell>0.70 / 0.20 / 0.10</cell></row><row><cell cols="5">where 𝑤 𝑖 represents the weight for subgroup V 𝑖 , which is</cell></row><row><cell cols="5">determined by the accuracy of the subgroup on the validation set</cell></row><row><cell cols="2">with softmax as modulation:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>SGC reduces model complexity by eliminating non-linear functions between GCN layers and transforming nonlinear GCNs into simple linear models; ClusterGCN is a framework for training large-scale GNN via partitioning the origin graph into small disjoint subgraphs; GraphSAINT explicitly considers the deviation caused by subgraph sampling on GCN calculation, which can ensure that the aggregation process of nodes after sampling is unbiased and the variance caused by sampling is as small as possible; SIGN passes each hop's neighborhood aggregation through a MLP encoder and then concatenates the encoded representation as the input for a post MLP classifier; SAGN leverages attention module to attentively merge multi-hop's neighborhood message, and uses a Self-Label-Enhance (SLE) training mechanism to improve the model performance.4.1.3 Implementation Details. For all baselines, the hyper-parameters are set to be the same with the corresponding paper's settings, and in order to provide a fair comparison, all hidden dimensions are set to 128. We use the open-sourced codes released by the authors and DGL official forum for comparison with Ada-GNN. As for Ada-GNN, we use METIS algorithm to perform node tagging and generate subgroups. Due to the limited number of training samples, too much subgroups might cause insufficient training for each local model and huge memory cost.</figDesc><table><row><cell>4.1.2 Baselines. Intuitively, the node diversity phenomenon is</cell></row><row><cell>more severe on large graphs, we are more interested to see</cell></row><row><cell>how Ada-GNN can improve those base GNN model which are</cell></row><row><cell>scalable. Thus, we choose six different GNN methods, including</cell></row></table><note>GraphSAGE [4], SGC<ref type="bibr" target="#b20">[21]</ref>, ClusterGCN<ref type="bibr" target="#b1">[2]</ref>, GraphSAINT<ref type="bibr" target="#b25">[26]</ref>, SIGN<ref type="bibr" target="#b14">[15]</ref>, and SAGN<ref type="bibr" target="#b16">[17]</ref>. They represent different types of scalable structures: GraphSAGE samples a fixed number of neighbors for each node to avoid the neighborhood explosion problem;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Overall performance (in percentage) comparisons of Ada-GNN with six different base models ±0.03 32.17 ±0.21 83.01 ±0.04 47.77 ±0.18 SGC 52.73 ±0.17 28.96 ±0.28 74.17 ±0.01 37.88 ±0.47 ClusterGCN 55.49 ±0.02 34.01 ±0.41 85.71 ±0.02 49.87 ±0.27 GraphSAINT 54.60 ±0.02 24.31 ±0.34 87.02 ±0.01 44.40 ±0.22 SIGN 58.20 ±0.03 35.85 ±0.25 85.38 ±0.06 50.34 ±0.43 SAGN 59.42 ±0.02 36.07 ±0.15 87.56 ±0.03 55.21 ±0.13 Ada-GraphSAGE 54.54 ±0.15 33.05 ±0.23 85.47 ±0.13 50.10 ±0.57 Ada-SGC 53.87 ±0.29 32.53 ±0.37 76.15 ±0.01 42.42 ±2.78 Ada-ClusterGCN 58.25 ±0.17 35.33 ±0.31 86.56 ±0.05 51.15 ±1.23 Ada-GraphSAINT 55.90 ±0.21 33.15 ±0.44 87.73 ±0.11 45.67 ±1.31 Ada-SIGN 59.60 ±0.11 35.81 ±0.75 86.79 ±0.14 52.62 ±0.39 Ada-SAGN 59.92 ±0.12 36.19 ±0.27 87.84 ±0.08 55.32 ±0.13</figDesc><table><row><cell>Models</cell><cell>Arxiv1M micro-f1 macro-f1 micro-f1 macro-f1 Amazon2M</cell></row><row><cell>GraphSAGE</cell><cell>54.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>An ablation study on removing the meta adapter or/and the feature enhancement module (in micro-f1). Numbers in bold type indicate the best setting for each base model.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Arxiv1M</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Amazon2M</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>base</cell><cell>pretrain+</cell><cell>w/o</cell><cell>w/o</cell><cell>Ada-</cell><cell>base</cell><cell>pretrain+</cell><cell>w/o</cell><cell>w/o</cell><cell>Ada-</cell></row><row><cell></cell><cell>model</cell><cell>finetune</cell><cell>adapter</cell><cell>feature</cell><cell>GNN</cell><cell>model</cell><cell>finetune</cell><cell>adapter</cell><cell>feature</cell><cell>GNN</cell></row><row><cell>Ada-SGC</cell><cell>0.5273</cell><cell>0.5412</cell><cell>0.5329</cell><cell>0.5336</cell><cell>0.5387</cell><cell>0.7417</cell><cell>0.7852</cell><cell>0.7601</cell><cell>0.7479</cell><cell>0.7615</cell></row><row><cell>Ada-ClusterGCN</cell><cell>0.5549</cell><cell>0.5615</cell><cell>0.5806</cell><cell>0.5816</cell><cell>0.5825</cell><cell>0.8571</cell><cell>0.8602</cell><cell>0.8606</cell><cell>0.8576</cell><cell>0.8656</cell></row><row><cell>Ada-SIGN</cell><cell>0.5820</cell><cell>0.5840</cell><cell>0.5936</cell><cell>0.5714</cell><cell>0.5960</cell><cell>0.8538</cell><cell>0.8529</cell><cell>0.8644</cell><cell>0.8559</cell><cell>0.8679</cell></row><row><cell>Ada-SAGN</cell><cell>0.5942</cell><cell>0.5927</cell><cell>0.5977</cell><cell>0.5960</cell><cell>0.5992</cell><cell>0.8756</cell><cell>0.8736</cell><cell>0.8772</cell><cell>0.8764</cell><cell>0.8784</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Comparison on overall performance and standard deviations among base models, Ada-GNNs (Ada), and Ada-GNNs with fairness module (Ada 𝐹𝑎𝑖𝑟 ) .20 74.<ref type="bibr" target="#b27">28</ref> 2.48 87.02 2.90 85.38 1.70 Ada 85.47 2.54 76.10 2.35 87.73 3.01 86.79 2.41 Ada 𝐹𝑎𝑖𝑟 85.00 1.72 76.00 1.41 87.21 1.48 85.84 1.60</figDesc><table><row><cell>Model</cell><cell>SAGE mic std</cell><cell>SGC mic std</cell><cell>SAINT mic std</cell><cell>SIGN mic std</cell></row><row><cell>base</cell><cell>83.01 2</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* This work is supported by National Natural Science Foundation of China (No. 62172174).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Ada-GNN Algorithms</head><p>for each subgroup V 𝑖 do 5:</p><p>Initialize the local model parameters 𝜃 ′ 𝑖 with 𝜃 .</p><p>6:</p><p>for each adaption step in 𝐾 do 7:</p><p>Compute the train loss L 𝑖 (𝜃 ′ 𝑖 ) through support set V 𝑖,𝑆 in subgroup V 𝑖 . for each adaption step in 𝐾 do 5:</p><p>Compute the train loss L 𝑖 (𝜃 ′ 𝑖 ) through support set V 𝑖,𝑆 in subgroup V 𝑖 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Local updates:</p><p>end for 8:</p><p>Evaluation through test nodes in subgroup V 𝑖 with adapted local model 𝜃 ′ 𝑖 . 9: end for We provide the complete process of Ada-GNN's training process and test process in Algorithm 1 and Algorithm 2 respectively, which are the core components helping Ada-GNN to learn both local and global information simultaneously. Those algorithms are also literally described in Section 3.4 for better understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Subgroup-level Analysis</head><p>As shown in Figure <ref type="figure">A1</ref>, the Ada-GNN brings consistent improvement for almost all base models on the subgroup-level. The only two exceptions are with SIGN on subgroup #1 and subgroup #5 in Amazon2M. The reason may be the insufficient model personalization caused by a small subgroup number. In   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hyper-parameters Analysis</head><p>We conduct hyper-parameters experiments on Arxiv1M and Amazon2M to analyze the impact of subgroup number 𝑀 and adaption step 𝐾. Here we provide the results on Amazon2M in Figure <ref type="figure">A2</ref>. Readers can find detailed experimental analysis in Section 4.6.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rytstxWAW" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations, ICLR 2018</title>
				<meeting>the 6th International Conference on Learning Representations, ICLR 2018<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330925</idno>
		<ptr target="https://doi.org/10.1145/3292500.3330925" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
				<editor>
			<persName><forename type="first">Ankur</forename><surname>Teredesai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rómer</forename><surname>Rosales</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Evimaria</forename><surname>Terzi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</editor>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-08-04">2019. August 4-8, 2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR, 1126-1135</idno>
		<ptr target="http://proceedings.mlr.press/v70/finn17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11">2017. 2017. 6-11 August 2017</date>
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rob</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA, Isabelle Guyon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note>Ulrike von Luxburg, Samy Bengio</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
				<meeting>2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27">2016. June 27-30, 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401063</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401063" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event</title>
				<editor>
			<persName><forename type="first">Jimmy</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vanessa</forename><surname>Murdock</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</editor>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-07-25">2020. July 25-30, 2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs</title>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1137/S1064827595287997</idno>
		<ptr target="https://doi.org/10.1137/S1064827595287997" />
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJU4ayYgl" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations, ICLR 2017</title>
				<meeting>the 5th International Conference on Learning Representations, ICLR 2017<address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Policy-GNN: Aggregation Optimization for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Kwei-Herng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3403088</idno>
		<ptr target="https://doi.org/10.1145/3394486.3403088" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2020, Virtual Event</title>
				<editor>
			<persName><forename type="first">Rajesh</forename><surname>Gupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">Aditya</forename><surname>Prakash</surname></persName>
		</editor>
		<meeting>the 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2020, Virtual Event<address><addrLine>CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-08-23">2020. August 23-27, 2020</date>
			<biblScope unit="page" from="461" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fake News Detection Using Deep Learning</title>
		<author>
			<persName><forename type="first">Dong-Ho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Ri</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeong-Jun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung-Myun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Jun</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="http://www.jips-k.org:80/q.jips?cp=pp&amp;pn=707" />
	</analytic>
	<monogr>
		<title level="j">J. Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1119" to="1130" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MeLU: Meta-Learned User Preference Estimator for Cold-Start Recommendation</title>
		<author>
			<persName><forename type="first">Hoyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbae</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongwon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunsouk</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sehee</forename><surname>Chung</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330859</idno>
		<ptr target="https://doi.org/10.1145/3292500.3330859" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019</title>
				<editor>
			<persName><forename type="first">Ankur</forename><surname>Teredesai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rómer</forename><surname>Rosales</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Evimaria</forename><surname>Terzi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</editor>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD 2019<address><addrLine>Anchorage, AK, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-08-04">2019. August 4-8, 2019</date>
			<biblScope unit="page" from="1073" to="1082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DeepGCNs: Can GCNs Go As Deep As CNNs</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00936</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2019.00936" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</title>
				<meeting>2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-10-27">2019. October 27 -November 2, 2019</date>
			<biblScope unit="page" from="9266" to="9275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</title>
		<author>
			<persName><forename type="first">Gainza</forename><surname>Pablo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sverrisson</forename><surname>Freyr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monti</forename><surname>Frederico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodola</forename><surname>Emanuele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boscaini</forename><surname>Davide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bronstein</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Correia</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">SIGN: Scalable Inception Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<ptr target="https://arxiv.org/abs/2004.11198" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">One Model to Serve All: Star Topology Adaptive Recommender for Multi-Domain CTR Prediction</title>
		<author>
			<persName><forename type="first">Xiang-Rong</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guorui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binding</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingshan</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3459637.3481941</idno>
		<ptr target="https://doi.org/10.1145/3459637.3481941" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information and Knowledge Management, CIKM 2021, Virtual Event</title>
				<editor>
			<persName><forename type="first">Gianluca</forename><surname>Demartini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Shane</forename><surname>Guido Zuccon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zi</forename><surname>Culpepper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hanghang</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><surname>Tong</surname></persName>
		</editor>
		<meeting>the 30th ACM International Conference on Information and Knowledge Management, CIKM 2021, Virtual Event<address><addrLine>Queensland, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-11-01">2021. November 1 -5, 2021</date>
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Scalable and Adaptive Graph Neural Networks with Self-Label-Enhanced training</title>
		<author>
			<persName><forename type="first">Chuxiong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoshi</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09376</idno>
		<ptr target="https://arxiv.org/abs/2104.09376" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2015.7298594" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07">2015. June 7-12, 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Densely Connected Multi-Dilated Convolutional Networks for Dense Prediction Tasks</title>
		<author>
			<persName><forename type="first">Naoya</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Mitsufuji</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content/CVPR2021/html/Takahashi_Densely_Connected_Multi-Dilated_Convolutional_Networks_for_Dense_Prediction_Tasks_CVPR_2021_paper.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
				<meeting>IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</meeting>
		<imprint>
			<date type="published" when="2021-06-19">2021. June 19-25, 2021</date>
			<biblScope unit="page" from="993" to="1002" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations, ICLR 2018</title>
				<meeting>the 6th International Conference on Learning Representations, ICLR 2018<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/wu19e.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06">2019. 9-15 June 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2020.2978386</idno>
		<ptr target="https://doi.org/10.1109/TNNLS.2020.2978386" />
	</analytic>
	<monogr>
		<title level="j">A Comprehensive Survey on Graph Neural Networks. IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rumor detection based on propagation graph neural network with attention mechanism</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dechang</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junfu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianjun</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2020.113595</idno>
		<ptr target="https://doi.org/10.1016/j.eswa.2020.113595" />
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page">113595</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<ptr target="https://openreview.net/forum?id=ryGs6iA5Km" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations<address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219890</idno>
		<ptr target="https://doi.org/10.1145/3219819.3219890" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD</title>
				<editor>
			<persName><forename type="first">Yike</forename><surname>Guo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Faisal</forename><surname>Farooq</surname></persName>
		</editor>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-08-19">2018. 2018. August 19-23. 2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">GraphSAINT: Graph Sampling Based Inductive Learning Method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><forename type="middle">K</forename><surname>Prasanna</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJe8pkHFwS" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
				<meeting>the 8th International Conference on Learning Representations<address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Meta-Graph Based Recommendation Fusion over Heterogeneous Information Networks</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<idno type="DOI">10.1145/3097983.3098063</idno>
		<ptr target="https://doi.org/10.1145/3097983.3098063" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, NS, Canada; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017-08-13">2017. 2017. August 13-17, 2017</date>
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning Graph Neural Networks with Deep Graph Library</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366424.3383111</idno>
		<ptr target="https://doi.org/10.1145/3366424.3383111" />
	</analytic>
	<monogr>
		<title level="m">Companion of The</title>
				<editor>
			<persName><forename type="first">Amal</forename><forename type="middle">El</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fallah</forename><surname>Seghrouchni</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gita</forename><surname>Sukthankar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maarten</forename><surname>Van Steen</surname></persName>
		</editor>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-20">2020. 2020. 2020. April 20-24, 2020</date>
			<biblScope unit="page" from="305" to="306" />
		</imprint>
	</monogr>
	<note>ACM / IW3C2</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
