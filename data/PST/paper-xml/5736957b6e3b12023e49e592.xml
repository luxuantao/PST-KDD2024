<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An incremental meta-cognitive-based scaffolding fuzzy neural network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mahardhika</forename><surname>Pratama</surname></persName>
							<email>pratama@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Quantum Computation and Intelligent System</orgName>
								<orgName type="institution">University of Technology</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Lu</surname></persName>
							<email>jie.lu@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Quantum Computation and Intelligent System</orgName>
								<orgName type="institution">University of Technology</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sreenatha</forename><surname>Anavatti</surname></persName>
							<email>s.anavatti@adfa.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering and Information Technology</orgName>
								<orgName type="institution">University of New South Wales</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
							<email>edwin.lughofer@jku.at</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Knowledge-Based Mathematical Systems</orgName>
								<orgName type="institution">Johannes Kepler University</orgName>
								<address>
									<postCode>A-4040</postCode>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chee-Peng</forename><surname>Lim</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Centre for Intelligent System Research</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<addrLine>Geelong Waurn Ponds Campus</addrLine>
									<postCode>3216</postCode>
									<region>Victoria</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Er</surname></persName>
						</author>
						<title level="a" type="main">An incremental meta-cognitive-based scaffolding fuzzy neural network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">53956C9DB793F86E92DC5673DAC83D6A</idno>
					<idno type="DOI">10.1016/j.neucom.2015.06.022</idno>
					<note type="submission">Received 12 November 2014 Received in revised form 20 April 2015 Accepted 14 June 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Evolving fuzzy systems Fuzzy neural networks Meta-cognitive learning Sequential learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The idea of meta-cognitive learning has enriched the landscape of evolving systems, because it emulates three fundamental aspects of human learning: what-to-learn; how-to-learn; and when-to-learn. However, existing meta-cognitive algorithms still exclude Scaffolding theory, which can realize a plug-and-play classifier. Consequently, these algorithms require laborious pre-and/or post-training processes to be carried out in addition to the main training process. This paper introduces a novel meta-cognitive algorithm termed GENERIC-Classifier (gClass), where the how-to-learn part constitutes a synergy of Scaffolding Theorya tutoring theory that fosters the ability to sort out complex learning tasks, and Schema Theorya learning theory of knowledge acquisition by humans. The what-to-learn aspect adopts an online active learning concept by virtue of an extended conflict and ignorance method, making gClass an incremental semi-supervised classifier, whereas the when-to-learn component makes use of the standard sample reserved strategy. A generalized version of the Takagi-Sugeno Kang (TSK) fuzzy system is devised to serve as the cognitive constituent. That is, the rule premise is underpinned by multivariate Gaussian functions, while the rule consequent employs a subset of the non-linear Chebyshev polynomial. Thorough empirical studies, confirmed by their corresponding statistical tests, have numerically validated the efficacy of gClass, which delivers better classification rates than state-ofthe-art classifiers while having less complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The consolidation of the meta-cognitive aspect in machine learning was initiated by Suresh et al. <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> based on a prominent meta-memory model proposed by Nelson and Naren <ref type="bibr" target="#b5">[6]</ref>. The works in <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> identify that the meta-cognitive component, namely what-to-learn, how-to-learn and when-to-learn, can respectively be modelled with sample deletion strategy, sample learning strategy and sample reserved strategy. Nevertheless, their pioneering works still discount the construct of Scaffolding theory <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref>, rendering a plug-and-play classifier. They have also not addressed the issue of semi-supervised learning, since the what-to-learn phase requires the data to be fully labelled.</p><p>A novel meta-cognitive-based Scaffolding classifier, the GENERIC-classifier (gClass), is proposed in this paper. The gClass learning engine comprises three elements: what-to-learn; howto-learn; and when-to-learn. The underlying novelty of gClass lies on the use of Schema and Scaffolding theories in the how-to-learn component to realize it as a plug-and-play classifier. The plug-andplay learning paradigm emphasizes the need for all learning modules to be embedded in a single learning process without invoking any pre-and/or post-training processes. In respect of its cognitive constituent, the gClass fuzzy rule triggers a non-axisorthogonal fuzzy rule in the input space, underpinned by the multivariate Gaussian function rule premise. Unlike the standard form of TSK fuzzy rule consequents, the rule consequent of gClass is built upon a non-linear function stemming from a subset of nonlinear Chebyshev polynomials. All training mechanisms run in the strictly sequential learning mode to assure fast model updates and comply with the four principles of online learning <ref type="bibr" target="#b31">[32]</ref>: <ref type="bibr" target="#b0">(1)</ref> all training observations are sequentially presented one by one or chunk by chunk to gClass; (2) only one training datum is seen and learned in every training episode; (3) a training sample which has been seen is discarded without being reused; and (4) gClass does not require any information pertaining to the total number of training data.</p><p>The gClass learning scenario utilizes several learning modules of our previous algorithms in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>: three rule growing cursors, namely Datum Significance (DS), Data Quality (DQ), and Generalized Adaptive Recursive Theory þ (GART þ), are used to evolve fuzzy rules according to the Schema theory <ref type="bibr" target="#b13">[14]</ref>; two rule pruning Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/neucom strategies, namely Extended Rule Significance (ERS) and Potential (P þ) methods, are assembled to get rid of obsolete and inactive fuzzy rules and portray the fading aspect of Scaffolding theory. The Pþ method also deciphers the rule recall process, manifesting the problematizing component of Scaffolding theory to cope with the recurring concept drift; the Fuzzily Weighted Generalized Recursive Least Square (FWGRLS) method is integrated to adjust the rule consequent of the fuzzy rule and in turn delineates the passive supervision of the Scaffolding theory. gClass operates as its counterparts in <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, where the sample reserved strategy is employed in the when-to-learn process. Nonetheless, several new learning modules are proposed in this paper:</p><p>The what-to-learn component is built upon a new online active learning scenario, called the Extended Conflict and Ignorance (ECI) method. The ECI method is derived from the conflict and ignorance method <ref type="bibr" target="#b1">[2]</ref>, and the ignorance method is enhanced by the use of the DQ method instead of the classical rule firing strength concept. This modification makes the online active learning method more robust against outliers and more accurate in deciding the sample ignorance. Note that this mechanism can be also perceived as an enhanced version of the original what-to-learn module in <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, the whatto-learn module is limited to ruling out redundant samples for model updates, and still assumes that data are fully labelled.</p><p>A new fuzzy rule initialization strategy is proposed and is constructed by the potential per-class method. This method is used to avoid misclassifications caused by the class overlapping situation. A number of research efforts have been attempted in <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref> to circumvent the class overlapping situation, however they rely on the distance ratio method, which overlooks the existence of unclean clusters. An unclean cluster is a cluster that contains supports from different classes and is prevalent in real world-problems. This learning aspect actualizes the restructuring phase of Schema theory.</p><p>gClass is also equipped with a local forgetting scheme inspired by <ref type="bibr" target="#b27">[28]</ref> to surmount gradual concept drift, where the forgetting intensity is enumerated by a newly developed method, called the Local Data Quality (LDQ) method. It is worth stressing that gradual concept drift is more precarious than abrupt concept drift, because gradual concept drift cannot be detected by standard drift detection or the rule generation method. On the other side, it cannot be handled by the conventional parameter learning method either. This situation entails the local forgetting scheme, which adapts fuzzy rule parameters more firmly and is thereby able to pursue changing data distributions. In the realm of Scaffolding theory, the local drift-handling strategy plays a problematizing role in the active supervision of the theory. gClass enhances the Fisher Separability Criterion (FSC) in the empirical feature space method with the optimization step via the gradient ascent method. This step not only alleviates the curse of dimensionality, but it also improves the discriminatory power of input features. Noticeably, it triggers a direct impact on the classifier's generalization. The online feature weighting technique is employed to address the complexity reduction scenario in the active supervision of the scaffolding concept.</p><p>The contributions of this paper are summarized as follows:</p><p>(1) the paper proposes a new class of meta-cognitive classifiers, which consolidate the Schema and Scaffolding theories to drive the how-to-learn module. <ref type="bibr" target="#b1">(2)</ref> The paper introduces a novel type of TSK fuzzy rule, crafted by the multivariable Gaussian function in the premise component and the non-linear Chebyshev polynomial in the output component. (3) Four novel learning modules in the gClass learning engine are proposed: online feature selection; online active learning; class overlapping strategy; and online feature weighting mechanism. The viability and efficacy of gClass have been numerically validated by means of thorough numerical studies in both real-world and artificial study cases. gClass has also been benchmarked against various state-of-the-art classifiers, confirmed by rigorous statistical tests in which gClass demonstrates highly encouraging generalization power while suppressing complexity to an acceptable level. The remainder of this paper is organized as follows: Section 2 discusses related works. Section 3 illustrates the gClass inference mechanism, i.e., its cognitive aspect. Section 4 outlines the algorithmic development of gClass, i.e., its meta-cognitive component. Section 5 deliberates the empirical studies and discussions of the research gap and contribution, which detail the viability and research gap of gClass. Concluding remarks are drawn in the last section of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Literature review</head><p>In this section, two related areas are discussed. A survey of the psychological concepts implemented in gClass is undertaken, as well as a literature review of state-of-the art evolving classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human learning</head><p>The main challenge of learning sequentially from data streams is how to deal with the stability and plasticity dilemma <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b48">49]</ref>, which requires a balance between new and old knowledge. In the realm of cognitive psychology, this dilemma is deliberated in Schema theory, which is a psychological model for human knowledge acquisition and the organization of human memory <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b65">66]</ref>, in which knowledge is organized into units, or schemata (sing. schema). Information is stored within the schemata, and Schema theory is thus the foundation of a conceptual system for understanding knowledge representation.</p><p>In essence, Schema theory is composed of two parts: schemata construction and schemata activation. Schemata are built in the construction phase, and this is achieved by three possible learning scenarios that relate to the conflict level induced by an incoming datumaccretion, tuning and restructuring. Accretion pinpoints a conflict-free situation, where an incoming datum can be wellrepresented by an existing schema. Tuning represents a minor conflict circumstance in which only the adaptation of a schema is entailed. The most significant case is the restructuring phase, in which a datum induces a major conflict which demands the restructure of an existing schema or its complete replacement. Schemata activation describes a self-regulatory process to evaluate the performance of the schemata, or determines a compatible learning scenario to manage a new example.</p><p>Scaffolding theory elaborates a tutoring theory, which assists students to accomplish a complex learning task <ref type="bibr" target="#b67">[68]</ref>. This goal is achieved by passively and actively supervising the training process. Passive supervision implements a learning strategy by virtue of the experience and consequence mechanism, and depends on the predictive quality of fresh data. Passive supervision is particularly represented by the parameter learning of the rule consequent. Active supervision makes use of more proactive mechanisms and consists of three learning scenarios: complexity reduction; problematizing; and fading <ref type="bibr" target="#b66">[67]</ref>. The complexity reduction component aims to relieve the learning burden and can be actualized by data pre-processing and/or feature selection. Problematizing copes with concept drift and can be realized by a local forgetting mechanism and/or rule recall strategy. The fading constituent deciphers a structure simplification procedure which inhibits redundancy in the rule base; this concept is usually executed by the rule pruning technique.</p><p>In the psychology literature, the ability of human beings to evaluate their knowledge with respect to the environment and their capacity to self-organize that knowledge is well-known as metacognition. Nevertheless, characteristic of mainstream machine learning is cognitive in nature (see <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref>). Conventional machine learning algorithms learn all the streaming data without being able to extract important training samples and are unable to pinpoint compatible time instants in which to consume the training data <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65]</ref>. A prominent contribution was delivered by Nelson and Narens in <ref type="bibr" target="#b5">[6]</ref>, which identifies the monitoring and control connection between cognition and metacognition. This work paves the way for a simple but wellaccepted model to be emulated by a machine learning algorithm. In principle, the cognitive component memorizes pivotal information, or examples, acquired from past experiences, while the meta-cognitive element depicts learning strategies to update the cognitive module. The meta-cognitive scenario is composed of: the termination of study (when to learn); the selection of processing method (how to learn); and item selection (what to learn). The termination of study (when-tolearn) decides when the study of an item should end, the selection of processing method concerns the choice of strategies to use when an item is integrated into memory, and item selection (what-to-learn) specifies whether or not an item is worth studying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">State-of-the art learning algorithms</head><p>The notion of the meta-cognitive classifier stems from the so-called evolving classifier, which has the characteristic of being fully adaptive and evolving. The evolving classifier can start its learning process from scratch with an empty rule base, and the fuzzy rules can be autonomously generated from data streams. The evolving classifier was pioneered by Angelov and Zhou in <ref type="bibr" target="#b0">[1]</ref>. In <ref type="bibr" target="#b3">[4]</ref>, several evolving classifier architectures were introduced which were driven by eClass and FLEXIS-Class as the base classifiers. Simp_eClassþ was proposed in <ref type="bibr" target="#b2">[3]</ref> and eMG_class was put forward in <ref type="bibr" target="#b4">[5]</ref>. More recently, an all-pair classifier architecture and a conflict and ignorance online active learning method were devised. In our previous work, we put forward the GENEFIS-class method <ref type="bibr" target="#b19">[20]</ref>, which amends GENEFIS in <ref type="bibr" target="#b17">[18]</ref> to solve classification problems. This work was enhanced in <ref type="bibr" target="#b18">[19]</ref>, which produced the so-called Parsimonious Classifier (pClass). pClass and GENEFIS-class are underpinned by the non-axis-parallel ellipsoidal cluster, yet they still exploit a standard first order rule output which does not fully disclose a local approximation trait. A seminal work, namely FAOS-PFNN, was proposed in <ref type="bibr" target="#b71">[72]</ref>. This work was enhanced in <ref type="bibr" target="#b72">[73]</ref> with the use of asymmetric Gaussian function. Nevertheless, these two works merely constitute a semi-online learning algorithm, where they still require a retraining phase using an up-to-date dataset, when encountering a new training pattern. These works are modified in <ref type="bibr" target="#b73">[74]</ref>, and are called DP-and CP-ELM where they present a recursive version of orthogonal least square developed by sequential partial orthogonalization to grow and to prune the hidden nodes of the ELM. Recently, CP-and DP-ELM were extended in <ref type="bibr" target="#b74">[75]</ref> by means of the polynomial weight vector, where it is called BR-ELM and its sequential version is termed in OSR-ELM. Although in-depth studies have been conducted by researchers, their works do not yet incorporate the meta-cognitive learning paradigm.</p><p>This issue has led to the development of the meta-cognitive classifier in <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref>, which are built upon the meta-memory model in <ref type="bibr" target="#b5">[6]</ref>. Meta-cognitive learning is transformed into the machine learning context with the sample deletion strategy (what-to-learn), sample learning technology (how-to-learn), and sample reserved mechanism (when-to-learn). Arguably, the works in <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref> adopt similar learning scenarios, and their main contribution is to create a meta-cognitive learning algorithm with various cognitive components, ranging from Radial Basis Function Neural Network (RBFNN) to Fuzzy Neural Networks (FNNs). Nevertheless, the meta-cognitive learning research area deserves more profound investigation for two underlying reasons: (1) the use of the scaffolding criteria, which provide a promising direction for a plug-and-play classifier, is still uncharted. Therefore, these machine learning variants usually enforce pre-and post-training processes, which undermine the logic of the online learning machine. <ref type="bibr" target="#b1">(2)</ref> The issue of the considerable labelling effort is unsolved, because the traditional meta-cognitive classifiers are designed for a fully supervised learning environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Cognitive component of Gclass</head><p>gClass is endowed with a generalized fuzzy rule <ref type="bibr" target="#b20">[21]</ref>, in which the multivariate Gaussian function, which possesses a non-diagonal covariance matrix, is utilized as the rule antecedent. This rule premise is an attractive option for covering real-world data distributions because it can evolve non-axis parallel ellipsoids and is capable of conferring more exact coverage of data distributions. It is worth noting that this advantage cannot be achieved with axis parallel rules induced by the classical t-norm operation. This rule premise arguably has two underlying shortcomings. First, the fuzzy set cannot be explicitly formulated, thus inducing less transparent rule semantics. Second, it forces a more demanding memory burden because of the need to store extra parameters in the memory. The first issue can, in principle, be surmounted by our previous work in <ref type="bibr" target="#b17">[18]</ref>, in which two fuzzy set extraction strategies are developed. The second issue is not necessarily valid, since this rule type can be anticipated to dampen the requirement of fuzzy rules (e.g., being able to perform a more compact representation in the event of longer rotated data clouds with only one rule). Fig. <ref type="figure" target="#fig_2">1</ref> depicts two distinct ellipsoidal contours compiled by two types of rule.</p><p>Each rule premise is fuzzily associated with a local non-linear sub-model by virtue of the Chebyshev polynomial having its root in <ref type="bibr" target="#b22">[23]</ref>. This rule output consummates the local approximation ability via a non-linear mapping of the Chebyshev function augmenting the degree of freedom of rule output. It is worthnoting that the Chebyshev function possesses a simpler expression than the trigonometric rule consequent of <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. Moreover, our approach is more resilient than the approach in <ref type="bibr" target="#b23">[24]</ref>, because the rule consequents are adapted by the local learning scenario, assuring higher flexibility, stability and faster convergence speed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">38]</ref>. This rule is formed as follows:</p><formula xml:id="formula_0">R i : IF X is close to R i Then y i o ¼ x e Ω i</formula><p>where R i stands for a multi-dimensional kernel, constructed by the multidimensional Gaussian function, propelled by a non-diagonal covariance matrix, while y o i denotes a regression output of a o-th class in the i-th rule. Ω i A ℜ ð2u þ 1ÞÂm labels a weight vector, where m specifies an output dimensionality and u denotes a number of input features. Conversely, x e constitutes an expanded input vector produced by a non-linear mapping based on the Chebyshev series up to the second order. Inspired by the Chebyshev-Functional Link Artificial Neural Network <ref type="bibr" target="#b22">[23]</ref>, the mathematical expression of the Chebyshev polynomial is given as follows:</p><formula xml:id="formula_1">T n þ 1 ðxÞ ¼ 2x j T n ðx j ÞÀT n À 1 ðx j Þ ð 1Þ With T 0 ðx j Þ ¼ 1, T 1 ðx j Þ ¼ x j , T 2 ðx j Þ ¼ 2x j 2 À 1 Suppose X is a 2-D input pattern X ¼ ½x 1 ; x 2 .</formula><p>The expanded input vector turns out to be x e ¼ ½1; x 1 ; T 2 ðx 1 Þ; x 2 ; T 2 ðx 2 Þ. Note that we include the term 1 in this case to include the intercept of the rule consequent as the standard form of the TSK fuzzy rules <ref type="bibr" target="#b57">[58]</ref> (otherwise, all consequent hyper-planes will arrive at the origin, thus leading to untypical gradients). Combining piecewise local predictors y i through a non-linear kernel (rule membership function R ) leads to the predicted output of the model:</p><formula xml:id="formula_2">ŷo ¼ P p i ¼ 1 R i y i o P p i ¼ 1 R i ¼ P p i ¼ 1 expðÀðX À C i ÞA i À 1 ðX À C i Þ T Þy i o P p i ¼ 1 expðÀðX À C i ÞA i À 1 ðX À C i Þ T Þ ; y ¼ maxðŷ o Þ o ¼ 1;…;m<label>ð2Þ</label></formula><p>where p signifies the number of fuzzy rules, C i A ℜ 1Âu denotes the centre of the Gaussian function of the i-th rule and A i À 1 A ℜ uÂu stands for the inverse covariance matrix of the i-th rule, which defines the shape and orientation of the ellipsoidal contours. By extension, the Gaussian function is selected because it can forestall undefined input states due to infinite support. It can allow smooth approximation of the local data space, since it is steadily differentiable. In this paper, the MIMO architecture is used to infer the classification result <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>, where the final output is simply assembled by the maximum operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Meta-cognitive learning</head><p>An incoming datum is first vetted by the what-to-learn module (Section 4.2), which aims to rule out inconsequential samples for the model updates. The training samples, admitted by the whatto-learn component, are injected into the how-to-learn module (Section 4.1), which updates the cognitive component. The training samples, which do not satisfy the learning criteria set out in the how-to-learn component, are assigned as reserved samples. The reserved samples are utilized after the main training patterns have all been consumed, with the aim of filling the gaps unexplored by the main training patterns (Section 4.3). Fig. <ref type="figure" target="#fig_1">2</ref> illustrates the learning architecture of gClass, whose learning steps will be detailed in the next section. In addition, an overview of the gClass learning policy is articulated in Algorithm 1. </p><formula xml:id="formula_3">P p i ¼ 1 V i</formula><p>Update the premise parameters of the winning rule ( <ref type="formula" target="#formula_19">12</ref>)-( <ref type="formula" target="#formula_21">14</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Else</head><p>Append the reserved samples with the current sample </p><formula xml:id="formula_4">ðXS NS þ 1 ; TS NS þ 1 Þ ¼ ðX N ; T N Þ End IF<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Autonomous fuzzy rule recruitment</head><p>The fuzzy rules are generated by three rule growing modules which aim to find streaming data with high potential and summarization power <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. The first method, namely the Datum Significance (DS) method, is capable of appraising the statistical contribution of streaming data, indicating the expected contribution of rules to the overall system output, whereas the second approach makes use of (Generalized Adaptive Resonance Theoryþ ) GARTþ, which is useful in inhibiting the cluster delamination effect by confining the size of the fuzzy region <ref type="bibr" target="#b33">[34]</ref>. The third method, namely the Data Quality (DQ) method, determines the spatial proximity between the datum and all previous data to establish whether or not it occupies a valuable fuzzy region in the input space. This leads us to the following condition of fuzzy rule generation.</p><formula xml:id="formula_5">V P þ 1 Z maxðV i Þ i ¼ 1;…;P and V win Z ρ 1 X p i ¼ 1 V i and ðDQ N Z max i ¼ 1;…;P ðDQ i Þ or DQ N r min i ¼ 1;:::;P ðDQ i ÞÞ<label>ð3Þ</label></formula><p>where V i denotes the volume of the i-th rule and DQ N is the quality of N-th datum, while ρ 1 labels a predefined constant whose value is stipulated in the range of [0.1,0.5]. Generally speaking, ρ 1 governs the stability-plasticity dilemma, where the allocation of a lower value encourages plasticity, inducing a high number of rules and vice versa. If the first part of the condition in (3) holds, a data stream inevitably contributes well during their lifespan. The second part addresses the situation in which the volume of the winning rule is oversized; an adaptation of the winning rule would thus have the effect of enlarging the coverage span of the winning rule, exacerbating the condition of cluster delamination (covering more than one data cloud). The last part of (3) can be used as a precursor of data density. This strategy is capable of indicating an incoming datum lying on a populated fuzzy region or an uncharted input space, signifying a shift in the data trend. Note that the rule growing cursor performs a knowledge exploratory mechanism or quantifies the degree of conflict induced by the datum. Accordingly, the rule growing criteria determine suitable learning modules to be performed during the training process. In the realm of psychology, the conflict measure is in line with schemata construction in Schema theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Hyper-volume calculation</head><p>The hyper-volumes of non-axis parallel ellipsoids can be simply elicited by the use of the determinant operator. However, this is a heuristic approach, which is rather inaccurate. We can enumerate a hyper-volume of a generalized fuzzy rule more exactly as follows:</p><formula xml:id="formula_6">V i ¼ 2n∏ u j ¼ 1 ðr i =λ ij Þnπ u=2 Γðu=2Þ ; ΓðuÞ ¼ Z 1 0 x u À 1 e À x dx<label>ð4Þ</label></formula><p>where r i is the Mahalnobis distance radius of the i-th fuzzy rule, which defines its (inner) contour (with the default setting of 1), λ i;j is the j-th eigenvalue of the i-th fuzzy rule, and Γ is the gamma function. To expedite the computation of the gamma function, a look-up table can be generated a priori and used during on-line learning with the current data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Winning rule elicitation</head><p>The winning rule is selected using the Bayesian theory rather than the traditional firing strength measure, where the fuzzy rule, having the maximum posterior probability, is chosen as the winning rule, i.e., win ¼ argmax i ¼ 1;:::;p PðR i X j Þ. The advantage of this theory is its prior probability, which is capable of determining the winning rule in the probabilistic fashion. Such strategy is deemed efficient to deduce the winning rule when two or more rules occupy similar proximities to the training datum. The posterior, prior probabilities as well as the likelihood function are  illustrated respectively as follows:</p><formula xml:id="formula_7">_ P ðR i jXÞ ¼ pðXjR i Þ PðR i Þ P p i ¼ 1 pðXjR i Þ PðR i Þ ; PðR i Þ ¼ log ðN i þ1Þ P p i ¼ 1 log ðN i þ 1Þ ; PðXjR i Þ ¼ 1 ð2πÞ 1=2 V i 1=2 expðÀðX À C i ÞA i À 1 ðX À C i Þ T Þ ð<label>5Þ</label></formula><p>where N i stands for the number of populations of the i-th cluster. Note that the prior probability formula PðR i Þ is softened from the original version for allowing newly born clusters to win the competition and to develop its shape. Note that the new cluster is usually populated with a smaller number of samples than the old clusters, thus impeding them to be selected as the winning rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Recursive computation of actual data quality (DQ)</head><p>According to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b62">63]</ref>, the DS method is unappealing to be benefited as the sole rule growing method, if the data distribution is not uniformly distributed. The work in <ref type="bibr" target="#b31">[32]</ref> offers a solution to cope with this issue. Nevertheless, it is not compatible for the online learning scenario, because it is based on the sliding window-based approach. To remedy the bottleneck, we can estimate the quality of a new datum with respect to existing clusters on-the-fly without the use of past training stimuli as follows:</p><formula xml:id="formula_8">DQ N ¼ X N À 1 n ¼ 1 R N ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 1 þ P N À 1 n ¼ 1 DQ n ðXn À XN ÞAN À 1 ðXn À XN Þ T P N À 1 n ¼ 1 DQ n v u u u t ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi U N U N ð1 þ a N ÞÀ2b N þc N s<label>ð6Þ</label></formula><p>where</p><formula xml:id="formula_9">U N i ¼ U N À 1 i þDQ N À 1 , a N ¼ X N A N À 1 X N T , b N ¼ DQ N À 1 X N α N , α N ¼ α N À 1 þ A N À 1 X N À 1 T , c N ¼ c N À 1 þDQ N À 1 X N À 1 A N À 1 X N À 1 . X N</formula><p>denotes the latest incoming datum and X n labels the n-th incoming datum. This formula effectively quantifies the firing strength of a hypothetical rule (the latest datum) in recursively accommodating already-seen training samples without maintaining them in the memory. In other words, it approximates the zone of influence of a cluster with respect to other training stimuli seen thus far. In the third term in (3), the first part, i.e. DQ N Z maxðDQ i Þ i ¼ 1;::;P implies that a prospective cluster occupies a denser region in the input space than existing rules. Meanwhile, the second situation</p><formula xml:id="formula_10">DQ N o minðDQ i Þ i ¼ 1;…;P</formula><p>shows that the prospective cluster digs up an unexplored local region in the input space, or indicates a regime shifting property of the system. Note that we can arrive at the data quality for an i-th rule invokes the rule pruning scenario to be instilled in the learning engine to prevent outliers being mounted as new rules. Also, the DQ method is compatible with the generalized TSK fuzzy rule, utilizes the inverse multi-quadratic kernel in lieu of the Cauchy kernel, and engages a weighting factor to resolve a large pair-wise distance problem after receiving noisy training samples <ref type="bibr" target="#b32">[33]</ref>. In short, it can be envisioned as an extended version of the Recursive Density Estimation (RDE) method in <ref type="bibr" target="#b4">[5]</ref>.</p><formula xml:id="formula_11">DQ i;i ¼ 1;:::;p , substituting X N ; A N À 1 in (9) with C i ; A i À 1 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5.">Initialization of new fuzzy rule parameters</head><p>Initialization of the fuzzy rule parameters should be contrived circumspectly in Evolving Fuzzy Classifiers (EFC), because the class overlapping contingency may be apparent. Generally speaking, a newly composed rule should not be adjacent to clusters supported by different classes. Suresh et al. in <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> offered a concept to especially deal with this problem by exploiting a distance ratio between inter-and intra-class clusters. Nevertheless, in real-world streaming data problems, a cluster is likely to comprise supports from different classes (as classes cannot be clearly separated, or samples may be affected by noise). Therefore, a cluster cannot usually be linked to a particular class (also well-known as a clean cluster). This issue is excluded in <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>.</p><p>To remedy this stumbling block, we should first canvass the compatibility degree of the winning cluster to check its spatial proximity to a data stream. If R win Z ρ 2 , a datum is adjacent to the winning rule. ρ 2 stands for a predefined constant that can be statistically represented by the critical value of a χ 2 distribution with Z degrees of freedom and a significance level of α <ref type="bibr" target="#b35">[36]</ref>, termed as χ p 2 ðαÞ. A typical value of α is 5%, and the degree of freedom is represented by the dimensionality of the learning problem, thus setting Z ¼ u. Therefore, we set ρ 2 ¼ expðÀχ p 2 ðαÞÞ.</p><p>As R win Z ρ 2 , a new rule induced by the new datum (center equal to datum coordinates) can be claimed as a redundant rule since it lies on the tolerance region. We switch on the so-called DQ per class method, which aims to approximate the class interactions. The crux of this method is to probe the class relationship between a datum and existing data clouds recursively. In a nutshell, the potential per class method is formulated as follows: lapping problem, which jeopardizes the classification rate, is most likely to occur in this situation. Let ir be the winning intra-class cluster, and ie be the winning inter-class cluster. We initialize a new rule as follows:</p><formula xml:id="formula_12">DQ o ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi 1 1 þ P No no ¼ 1 P u þ m j ¼ 1 ðx j no À x j N Þ ðNo À 1Þ v u u u t ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ðN o À1Þ ðN o À 1Þðab n þ 1Þþcb n À 2bb n s<label>ð7Þ</label></formula><formula xml:id="formula_13">where ab n ¼ P u þ m j ¼ 1 ðx j N Þ 2 , cb n ¼ cb no À 1 þ P u þ 1 j ¼ 1 ðx j No À 1 Þ 2 , bb n ¼ P u þ m j ¼ 1 x j N d n j , d n ¼ d n À 1 þ x No</formula><formula xml:id="formula_14">c P þ 1;j ¼ x j Àρ 2 ðc ie;j Àx j Þ; dist j ¼ ρ 1 c P þ 1;j À c ie;j and A p þ 1 À 1 ¼ ðdist T distÞ À 1<label>ð8Þ</label></formula><p>where c P þ 1;j is the j-th component of the centroid of the new fuzzy rule (P þ1 st ) and c ie;j is the j-th component of the centroid of the winning inter-class fuzzy rule. A p þ 1 À 1 is the inverse covariance matrix of the new fuzzy rule and ρ 4 stands for an overlapping factor steering the overlapping degree of the new cluster and the nearest cluster. ρ 3 A ½0:01 À 0:1 denotes a shifting factor fixed as 0.01 for all our numerical studies for the sake of simplicity. The predefined parameter ρ 3 is not problem dependent according to our sensitivity analysis. That is, the variation of its values in the specified range does not affect learning performance. A plausible choice of ρ 4 can be gained by setting ρ 4 ¼ r ir j =r ie j , where r ir j labels a spatial proximity of the datum and the nearest intra-class cluster, whereas r ie j exhibits a distance between the datum and the most adjacent inter-class cluster. One can concur that ρ 4 ¼ r ir j =r ie j is coherent to serve as ρ 4 since it should be demoted when the datum neighbours the inter-class cluster and vice versa. As a result, (8) essentially shrinks the coverage span of the newly created cluster and shifts the rule centroid away from the interclass cluster to stave off the class overlapping phenomenon.</p><p>If the datum lies on the area near the data points in the same class max o ¼ 1;…;m ðDQ o Þ a true_class_label, we construct the new fuzzy rule as follows:</p><formula xml:id="formula_15">c P þ 1;j ¼ x j þ ρ 2 ðc ir;j Àc ie;j Þ; dist j ¼ ρ 1 c P þ 1;j À c ir;j and A p þ 1 À 1 ¼ ðdist T distÞ À 1<label>ð9Þ</label></formula><p>where c ir;j is the j-th component of the winning intra-class cluster.</p><p>A low risk of misclassification is observed by the potential per class method in this case, thereby allocating more confident parameters. Although the new rule may result in overlap with the winning intra-class cluster in the future, this situation does not induce a substantial amendment of the decision boundary, making worse the nonlinearity of the decision surface.</p><p>Another condition may ensue in the training process and is signified by R win o ρ 2 . In this case, a low risk of the class overlapping phenomenon is captured, since the new fuzzy rule possibly occupies a remote input region, uncharted by the existing fuzzy rules. We thus tailor the new fuzzy rule as follow:</p><formula xml:id="formula_16">c P þ 1;j ¼ x j ; dist j ¼ ρ 1 x j À c ir;j and A p þ 1 À 1 ¼ ðdist T distÞ À 1<label>ð10Þ</label></formula><p>In all cases, the output parameters and the covariance matrix of the newly crafted fuzzy rules are constructed as follows:</p><formula xml:id="formula_17">Ω p þ 1 ¼ Ω winner and Ψ p þ 1 ¼ BI<label>ð11Þ</label></formula><p>where B is a large positive constant. The desired setting of B is as verified in <ref type="bibr" target="#b36">[37]</ref>, because it can lead to the best solution as produced by the batch learning process instantaneously. Conversely, the consequent vector is determined as the rule consequent vector of the nearest rule, arguably inheriting its functional trend. This setting can also cope with discontinuities of the approximation surface and can reduce convergence time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.6.">Adaptation of existing rules (when (3) is not fulfilled)</head><p>Appending the fuzzy rules automatically as presented in Section 4.1.6 represents the restructuring phase of the Schema theory, because the datum is conflicting to the current belief of the system. Another circumstance, namely the tuning phase of the Schema theory, may occur in the training process, given that a data stream incurs a minor degree of conflict. This situation is managed by adjusting the premise of the winning rule obtained via the Bayesian concept. The tuning case is reflected as follows:</p><formula xml:id="formula_18">V P þ 1 Z maxðV i Þ i ¼ 1;…;P and V win o ρ 1 X p i ¼ 1 V i and ðDQ N Z max i ¼ 1;…;P ðDQ i Þ or DQ N r min i ¼ 1;:::;P ðDQ i ÞÞ</formula><p>The rule adjustment criterion implies that the winning cluster is allowed to expand its size without a risk of the cluster delamination phenomenon. The rule premise adaptations are carried out as follows:</p><formula xml:id="formula_19">C win N ¼ N win N À 1 N win N À 1 þ 1 C win N À 1 þ ðX N ÀC win N À 1 Þ N win N À 1 þ 1<label>ð12Þ</label></formula><formula xml:id="formula_20">A win ðNÞ À 1 ¼ A win ðN À 1Þ À 1 1 Àα þ α 1 À α ðA win ðN À 1Þ À 1 ðX N À C win N À 1 ÞÞðA win ðN À 1Þ À 1 ðX N À C win N À 1 ÞÞ T 1 þ αðX N À C win N À 1 ÞA win ðN À 1Þ À 1 ðX N À C win N À 1 Þ T<label>ð13Þ</label></formula><formula xml:id="formula_21">N win N ¼ N win N À 1 þ1<label>ð14Þ</label></formula><p>Where α ¼ 1=ðN win N À 1 þ 1Þ. Eq. ( <ref type="formula" target="#formula_20">13</ref>) is suitable in the online learning scenario, because it does not entail a re-inversion process after adjusting the winning rule. Note that the re-inversion phase is computationally considerable and results in numerical instability (i.e. matrix is ill-defined).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.7.">Rule pruning and recall strategies</head><p>The rule pruning and recall modules, which act as the fading aspect of active supervision in Scaffolding theory, play a vital role in relieving the complexity of the rule base. An over-complex network topology inflicts many detrimental effects, including the over-fitting case, prohibitive memory demand, and considerable computational load. gClass has two mechanisms to pinpoint the significance level of fuzzy rules: the Extended Rule Significance (ERS) method and the Potential þ (P þ) method. The ERS method is embedded to seize the statistical contributions of fuzzy rules, while the P þ method traces the footprint of the fuzzy rules or produces the density of fuzzy rules. In essence, the P þ method is capable of pruning out-dated fuzzy rules which are no longer relevant to the capture of recent data trends due to concept drift. The ERS method implies the contributions of fuzzy rules in the future and to the system output. The ERS strategy is therefore effective for getting rid of superfluous fuzzy rules, which contribute little during their lifespan. Both methods are defined as follows:</p><formula xml:id="formula_22">β i ¼ X m o ¼ 1 X 2u þ 1 j ¼ 1 y ij o V i u P p i ¼ 1 V i u ; χ i ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi ðN À 1Þχ n À 1;i 2 ðN À 1Þχ n À 1;i 2 þðN À 2Þð1 À χ n À 1;i 2 Þþχ n À 1;i 2 d i n s<label>ð15Þ</label></formula><p>where β i denotes the ERS of the i-th fuzzy rule, and χ i exhibits the Pþ value of the i-th fuzzy rule, d n i stands for the Mahalanobis distance between the current training sample and the focal point of interest. In principle, the volume of clusters and output parameters as investigated in the ERS method are a point of departure for the appraisal of fuzzy rule contributions with respect to the system's output and the rule significance in the future. This nevertheless disregards the issue of how strategic the cluster position is in the input space. The Pþ method is suitable for covering this gap because it can appraise the evolution of the clusters. We arrive at the following two conditions to deduce whether or not the fuzzy rules are superfluous as follows:</p><formula xml:id="formula_23">χ i o χ À 2χ σ or β i o β À 2β σ<label>ð16Þ</label></formula><p>where χ; χ σ stand for the mean and Standard Deviation (SD) of the Pþ method of existing rules, respectively, β; β σ label the mean and SD of the ERS method of existing rules, respectively. Nevertheless, the fuzzy rules deactivated in the earlier training episodes by the P þ method may become valid again due to the recurring concept drift. In other words, the old data distribution may be reactivated in the future. Adding a totally new rule to capture this concept drift would be counterproductive, because information granules conceived by the old rules would be catastrophically discarded <ref type="bibr" target="#b12">[13]</ref>. Hence, if the potential of the pruned fuzzy rules is substantiated in the future max</p><formula xml:id="formula_24">in ¼ 1;…;Pn ðχ in Þ 4 max i ¼ 1;…;p þ 1 ðDQ i Þ, where</formula><p>P n is the number of rules dispossessed by the Pþ method, already pruned fuzzy rules should be re-activated, because this situation is a firm indication of the cyclic drift in the data streams. Note that the rule recall mechanism should be synchronized with respect to (3) (the requirement of the fuzzy rule generation in (3) is satisfied). The parameter of the recalled rules is then allocated as follows:</p><formula xml:id="formula_25">C p þ 1 ¼ C in ; X p þ 1 À 1 ¼ Σ i n À 1 ; Ψ p þ 1 ¼ Ψ i n ; Ω p þ 1 ¼ Ω i n<label>ð17Þ</label></formula><p>It is worth-noting that the computational burden is still alleviated, because already pruned fuzzy rules are merely utilized to execute the Pþ method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.8.">Fuzzily weighted generalized recursive least square (FWGRLS) procedure</head><p>The passive supervision in the realm of the Scaffolding concept in gClass is governed by the FWGRLS learning methodology. It constitutes a local learning version of the Generalized Recursive Least Square (GRLS) method <ref type="bibr" target="#b38">[39]</ref>, which strengthens the implicit weight decay effect of the Recursive Least Square (RLS) concept. The implicit weight decay effect reinforces the model's generalization, because it arguably sustains the output parameters to hover around a small bounded interval. By extension, it boosts the compactness of the rule base, because the output parameters, which are too small, can be captured by the ERS method. The FWGRLS method is formulated as follows: </p><formula xml:id="formula_26">ψðnÞ ¼ Ψ i ðn À 1ÞF n ð Þ λ i ΔðnÞ Λ i ðnÞ þ FðnÞΨ i ðn À 1ÞF T ðnÞ À 1<label>ð18Þ</label></formula><formula xml:id="formula_27">Ψ i ðnÞ ¼ Ψ i ðn À1ÞÀψ ðnÞFðnÞΨ i ðn À1Þ ð 19Þ Ω i ðnÞ ¼ Ω i ðn À 1ÞÀϖΨ i ðnÞ∇ξðΩ i ðn À 1ÞÞ þ Ψ ðnÞðtðnÞÀyðnÞÞ<label>ð20Þ</label></formula><p>where Λ i ðnÞ A ℜ ðP þ 1ÞÂðP þ 1Þ indicates a diagonal matrix whose diagonal elements consist of the firing strength of fuzzy rule R i .</p><p>The covariance matrix of the modelling error is shown by ΔðnÞ, and is managed as an identity matrix <ref type="bibr" target="#b38">[39]</ref> for the sake of simplicity. λ i is a local forgetting factor, intended to compensate the gradual concept drift, and is further discussed in the next section. ϖ is a predefined constant specified as ϖ % 10 À 15 and ∇ξðΩ i ðn À 1ÞÞ stands for the gradient of the weight decay function.</p><p>In our case, we choose the quadratic weight decay function ξðy i ðn À 1ÞÞ ¼ ð1=2ÞðΩ i ðn À 1ÞÞ 2 , which results in ∇ξðΩ i ðn À 1ÞÞ ¼ Ω i ðn À 1Þ. The weight decay function is capable of reducing the weight vector proportionally to its current values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.9.">Drift detection</head><p>Several attempts in the literature were devoted to conquering concept drift in FLEXFIS þ of <ref type="bibr" target="#b39">[40]</ref> and eTS þ of <ref type="bibr" target="#b40">[41]</ref>. eTS and FLEXFIS þ intrinsically benefit from the concept of age and utility <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b63">64]</ref>, which relies on the global forgetting scheme. The major shortcoming of the global forgetting scheme is that it assigns the same forgetting level for all fuzzy regions, whereas in fact, concept drift may exist in each local region with different intensities. The local forgetting scheme was recently introduced in <ref type="bibr" target="#b25">[26]</ref>. In contrast to the global method, a local drift handling method distributes a unique forgetting value to each rule. This approach is deemed more plausible, since the drift is handled locally with a specific local forgetting degree. Hence, a cluster hampered by high drift intensity is assigned a strong forgetting level and vice versa. Another local forgetting mechanism, namely Local Data Quality (LDQ) method, is proposed below. The key idea is akin to the DQ method in eq. ( <ref type="formula" target="#formula_15">9</ref>); however, it merely quantifies the distance between the cluster focal point and the training samples supporting this cluster only, thereby being able to reliably monitor a local cluster evolution. The LDQ method of the i-th rule can be written as follows:</p><formula xml:id="formula_29">LDQ i Ni ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi 1 1 þ P N i À 1 n ¼ 1 ðXn i À CiÞAi À 1 ðXn i À CiÞ ðNi À 1Þ v u u t ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi ðN i À 1Þ ðN i À 1Þð1þ ac Ni ÞÀ2bc Ni þ cc Ni s<label>ð22Þ</label></formula><formula xml:id="formula_30">where ac N i ¼ ac N i À 1 þ X n i A À 1 i X T n i , dc N i ¼ dc N i þ X n i A À 1 i , bc N i ¼ dc N i C i and cc N i ¼ C i A À 1 i C i</formula><p>T . N i is the number of samples belonging to the i-th cluster. In principle, a decrease of Eq. ( <ref type="formula" target="#formula_32">24</ref>) signifies that the data distribution has moved away from its previous region, and hints at local concept drift. To this end, we apply the first order derivative of <ref type="bibr" target="#b21">(22)</ref> to determine the local drift rate. Referring to <ref type="bibr" target="#b25">[26]</ref>, a strong forgetting level is delivered by λ i ¼ 0:9, whereas λ i ¼ 1 designates no forgetting at all to the past data history. The drift handling strategy is organized in such a way as to assure λ i A ½0:9; 1 as follows:</p><p>λ i ¼ minðmaxð1 À 0:1ΔLDQ N i ; 0:9Þ; 1Þ;</p><formula xml:id="formula_31">N i ¼ N i ÀN i minðλ trans i ; 0:99Þ; λ trans i ¼ À9:9λ i þ 9:9<label>ð23Þ</label></formula><p>where λ trans i labels the forgetting level of the premise of the i-th rule, which in turn alleviates the cluster population. Note that the centre and spread drift phenomena of the cluster can be unravelled by lessening the cluster support (thus relaxing the strong converged position). This later enables the dispatch of stronger adjustments to the centroid and covariance matrix of the i-th local input space partition and thus shifts the cluster in respect of changing data distributions. Conversely, the drift in the output concept can be overcome by λ i explicitly encompassed in the FWGRLS method to adapt the rule output. This mechanism can be employed if the clusters hold adequate supports (containing at least 30 samples) to circumvent the unlearning effect <ref type="bibr" target="#b25">[26]</ref>.</p><p>4.1.10. Feature weighting algorithm via maximization of separability criterion in the empirical feature space Several approaches have been designed to tackle the curse of dimensionality in an evolving system, including input pruning <ref type="bibr" target="#b18">[19]</ref>, Fisher Separability Criterion (FSC)-based feature weighting <ref type="bibr" target="#b26">[27]</ref>, and optimization of the separability criterion <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. Nevertheless, the input pruning strategy is less effective in ambient learning environments because once features have been removed from the model, they cannot be re-activated without causing discontinuity in the incremental learning process (the structures and parameters of the current models have evolved in another, smaller feature space). A re-training phase has to be undertaken from scratch to guarantee the stability of the classifier <ref type="bibr" target="#b26">[27]</ref>. Conversely, the feature weighting algorithms were devised in <ref type="bibr" target="#b41">[42]</ref> with the use of the FSC method in the original feature space. FSC in the empirical feature space method is more desirable than FSC in the traditional feature space method, because it is more straightforward to acquire the information of the separability criterion in the orthogonal space. Another salient contribution of our work is the extension of the concept in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> to the scope of the on-line learning mechanism, enabling the optimization of the separability criterion in the empirical feature space. The optimization process is carried out with the use of the gradient ascent algorithm and the alignment concept. The FSC in the empirical feature space can be mathematically formulated as follows:</p><formula xml:id="formula_32">J ¼ traceðS w À 1 S b Þ ¼ ΣW À ΣK=N À Á trðKÞÀΣW<label>ð24Þ</label></formula><p>where J; S b ; S w , respectively, exhibit the new FSC method in the empirical feature space, the between class scatter matrix and the within class scatter matrix. P W signifies the sum of every dimension of matrix P i;j W, K denotes a kernel-Gram-matrix. P W and K are stipulated as follows:</p><formula xml:id="formula_33">W ¼ 1 N diagðK oô =N o Þ; o ¼ 1; …; m; K ¼ K 11 K 12 ⋯ K 1o ⋯ K 1m K 21 K 22 ⋯ K 2o ⋯ K 2m ⋯ ⋯ ⋯ ⋯ ⋯ ⋯ K m1 K m2 ⋯ K mo ⋯ K mm 2 6 6 6 4 3 7 7 7 5<label>ð25Þ</label></formula><p>Note that K 11 A ℜ N 1 ÂN 1 denotes a kernel-Gram-sub-matrix emanating from data in class 1, whileK 12 A ℜ N 1 ÂN 2 labels a kernel-Gram-sub-matrix originating from data in classes 1 and 2, and so on. N o indicates the number of populations of the o-th class. The kernel function can be seen as Linear, Gaussian RBF, and Polynomial kernels, which however do not underpin swift online operations. Therefore, we rectify this drawback with the use of the Cauchy kernel function which constitutes a Gaussian-like function due to the first order Taylor series approximation of the Gaussian function. The Cauchy kernel function is appropriate for the online learning scenario because it allows recursive operations. The elements of the kernel-Gram-matrix K can be identified by the Cauchy function as follows:</p><formula xml:id="formula_34">K oô N ¼ ðN o À1Þ ðN o À1Þðϑ No þ 1Þþθ N ô À 2ς N ô<label>ð26Þ</label></formula><p>where ϑ No ¼ can be initialized as zero. The alignment matrix is crafted afterwards <ref type="bibr" target="#b46">[47]</ref> as follows:</p><formula xml:id="formula_35">P u þ m j ¼ 1 ðx j No Þ 2 , θ N ô ¼ θ N ô À 1 þ P u þ m j ¼ 1 ðx j N ô Þ 2 , ς N ô ¼ P u þ m j ¼ 1 x j No ν No , ν N ô ¼ ν N ô À 1 þ x N ô .x j</formula><formula xml:id="formula_36">AðK; K n Þ ¼ 〈K; K n 〉 F ‖K‖ F ‖K n ‖ F ¼ trðS b Þ ‖K‖ F<label>ð27Þ</label></formula><p>where AðK; K n Þ defines the alignment matrix of the kernel-grammatrix K, and ‖K‖ F stands for the Frobenius norm of the kernel-Gram matrix K. By applying the gradient operator of the alignment matrix, we arrive at the gradient ascent optimization procedure as follows:</p><formula xml:id="formula_37">∂ θ ðAðK; K n ÞÞ ¼ ∂ θ trðS b Þ ‖∂ θ K‖ F ¼ Σð∂ θ WÞÀ Σ∂ θ ðKÞ=N À Á ‖∂ θ K‖ F θ N ¼ θ N À 1 À η N ∂ θ ðAðK; K n ÞÞ<label>ð28Þ</label></formula><p>where θ N stands for the weighting factor initialized as 1. η N ¼ 1=n exhibits the learning rate which shrinks over time in the training process and is established according to the Robbins-Monroe conditions <ref type="bibr" target="#b64">[65]</ref> to guarantee the convergence of the weights. The weighting factor is integrated in all learning scenarios including all distance calculations affecting the rule evolution criterion (thus preventing the evolution of rules when the criteria are violated as a result of unimportant features) and the rule re-setting criteria (the motivation for rule evolution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">What to learn</head><p>The major bottleneck of the what-to-learn learning component in Suresh et al. <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b45">46]</ref> concerns the operator annotation efforts, which are laborious to carry out. In this paper, we propose a novel active learning method which enhances the conflict and ignorance concept in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44]</ref>. Specifically, we embellish the ignorance aspect of the original conflict and ignorance method using the DQ method to figure out the position of the datum in the feature space. It is notable that the original version simply exploits the firing strengths of fuzzy rules to assess the need for the ignorance aspect. This is deemed inaccurate, because the compatibility measure is merely based on a single sample strategy, and clearly, other samples can affect the ignorance of this sample. In addition, the DQ method is more robust to noise or outliers than the classical method, because it executes a sort of accumulated ignorance criterion over time. Fig. <ref type="figure" target="#fig_5">3</ref> illustrates the conflict and ignorance cases.</p><p>In Fig. <ref type="figure" target="#fig_5">3</ref>, querying point 1 is redundant, thus being capable of classifying it safelylearning this sample is not important for refining the decision boundary and even exacerbates the overfitting problem. Querying point 2 represents a strong conflict condition, and the learning process of such training samples requires that the decision boundary is updated to diminish the number of misclassifications. Querying point 3 represents training samples that lie far away from the current cluster centre. It is beneficial to accommodate these samples in the classifier updates to cross unexplored regions and to avert similar extrapolation cases in the future (fuzzy classifiers, especially, worsen significantly in terms of the correctness of the classification decision in cases of extrapolation). We arrive at the following condition to rule out training samples for model updates as follows:</p><formula xml:id="formula_38">min i ¼ 1;…;P ðDQ i Þ r DQ N r max i ¼ 1;::;P ðDQ i Þ and conf f inal ¼ score 1 score 1 þscore 2 4 ð0:5þ δÞ<label>ð29Þ</label></formula><p>Where score 1 and score 2 label the outputs of the most two dominant classes, while δdenotes the tolerable constant, which is fixed as δ ¼ 0:05 in all of our empirical studies. Note that score 1 and score 2 can be given by the classifier's outputs ŷ0 , if MIMO or one against all classifier architectures are used. Alternatively, they can be concluded from the weighted voting scheme of the preference relation matrix if the all-pairs architecture is explored <ref type="bibr" target="#b1">[2]</ref>. It is conceivable that the first term in <ref type="bibr" target="#b28">(29)</ref> signifies that the datum does not bring any new information because a datum is possibly well covered by existing clusters. Such data can be regarded as inconsequential examples. Another noteworthy aspect to override the data sample is a non-conflict case, which is usually provided by a confident classifier prediction, as defined by the second term in <ref type="bibr" target="#b28">(29)</ref>. Conversely, when score 1 and score 2 are almost equal, thus arriving at conf f inal % 0:5 o ð0:5 þ δÞ, they indicate a hard decision or a conflict with one of the cases (both are almost equally supported by the sample ¼ 4 conflict), which clearly entails a learning process to correct the classifier's confusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">When to learn</head><p>If the conditions in what-to-learn or how-to-learn are not satisfied, the datum is pushed into the rear stack and is assigned as a reserved sampleðXS n ; TS n Þ. This mechanism is widely known as the sample reserved strategy, where learning by means of the reserved samples is undertaken when the system is idle or all centric data have been depleted. The reserved samples can be used to cover unexplored regions of regular training samples. In theory, the training process is complete when no further sample is available in the data stream. In practice, this is not plausible, because the number of reserved samples can be unbounded, as the nature of data streams. Therefore, the training process is terminated when the number of reserved samples remains the same <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. Fig. <ref type="figure" target="#fig_6">4</ref> visualizes the flowchart of the gClass learning procedure. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Computational complexity</head><p>The computational complexity of gClass is influenced by every gClass learning module. However, this computational load hinges on whether or not the training samples are accepted by the whatto-learn learning module. The fuzzy rule recruitment scenario charges the computational complexity OðP n þ 2PÞ, which is compiled by the DS, DQ, GARTþ and rule recall methods. Presumably, the rule pruning methods have a computational complexity cost in the order of Oð2PÞ, which is generated by the ERS and Pþ methods. The allocation of fuzzy rule parameters, enforced by the potential per class composition, bears a computational burden in the order of OððP þ mÞþU 2 Þ. Roughly speaking, the feature weighting algorithm based on the optimization of the FSC in the empirical feature space incurs a computational cost in the order of Oðm 2 þUÞ, while the FWGRLS method inflicts computational complexity in the order of OðMð2P þ 1Þ 2 Þ. Consequently, the resultant</p><formula xml:id="formula_39">computational cost is Oðρðmð2P þ 1Þ 2 þ m 2 þ U þ 5P þ m þU 2 þ P n ÞÞ,</formula><p>where ρ expresses the probability of admitting the streaming data.</p><p>Table <ref type="table">1</ref> details the computational burden and memory demand of the gClass, GENEFIS-class and pClass algorithms <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. The gClass algorithm theoretically has a lower computational burden than the pClass and GENEFIS-class algorithms, because gClass is crafted in the metacognitive learning landscape. In contrast to the pClass feature weighting strategy, the gClass feature weighting technique has less computational cost, since it is not reliant on the Leave-One-Feature-Out (LOFO) mechanism. GENEFIS-class and pClass may be expected to confer fewer parameters to be salvaged in the memory due to a lower degree of freedom rule consequent and a diagonal covariance matrix in the rule antecedent. Nonetheless, the generalized fuzzy rule of gClass can be expected to grow fewer fuzzy rules, as experimentally verified in Section 5.1 and pictorially illustrated in Fig. <ref type="figure" target="#fig_2">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Proof of concepts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Efficacy of gClass learning modules</head><p>This section is intended to evaluate the efficacy of gClass's learning modules. Three data sets, namely thyroid, wine, and ionosphere, obtained from the University of California, Irvine (UCI) machine learning repository (http://www.ics.uci.edu/ mlearn/MLRepository.html), are used to assess the qualities of the proposed learning components. The weather dataset is also used, because this dataset contains severe concept drift. In this section, we evaluate the weather data set from the Offtutt Air Force Base in Bellevue, Nebraska, which is a subset of the U.S </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table1</head><p>Computational load, memory requirement, and structural cost.</p><formula xml:id="formula_40">gClass pClass GENEFIS-Class Computational load Oðρðmð2P þ 1Þ 2 þ m 2 þ U þ 5P þ m þ U 2 þ P n ÞÞ Oðm 2 u 2 þ P n þ 4P þ mðP þ 1Þ 2 Þ OðP 2 þ 2P þ m þ mp þ mðu þ 1Þ 2 þ mu þ puÞ Structural complexity Oðp Â m Â ð2u þ 1Þþp Â ðu Â uÞþp Â uÞ Oðp Â m Â ðuþ 1Þþp Â ðu Â uÞþp Â uÞ Oðp Â m Â ðu þ 1Þþp Â ðu Â uÞþp Â uÞ</formula><p>National Oceanic and Atmospheric Administration (NOAA) data sets. It covers a long period of 50 years and is available online (ftp://ftp.ncdc.noaa.gov/pub/data/gsod/), hence this version of the weather prediction problem not only depicts a cyclical seasonal change, but also characterizes a long term climate change. The characteristics of the datasets are shown in Table <ref type="table">2</ref> and the numerical results are tabulated in Table <ref type="table" target="#tab_3">3</ref>.</p><p>The goal of the empirical study is articulated as follows: (1) the generalized fuzzy rule elaborated in this paper is numerically validated and is benchmarked with other three fuzzy rule exemplars: the axis-parallel cluster <ref type="bibr" target="#b1">[2]</ref>, linear hyper-plane consequent <ref type="bibr" target="#b18">[19]</ref> and non-linear trigonometric consequent <ref type="bibr" target="#b23">[24]</ref>. The numerical results are abstracted in Section A of Table <ref type="table" target="#tab_3">3</ref>. (2) We also vet to what extent the local drift handling technique, developed in this paper, is capable of hedging gClass from a downtrend of gClass's predictive accuracy in the presence of concept drift. We analyze the performance of gClass with the absence of the local drift handling strategy. The experimental results are presented in Section B of Table <ref type="table" target="#tab_3">3</ref>. (3) We aim to study the impact of metacognitive learning. We scrutinize the gClass performances without the meta-cognitive learning scenario and only apply the how-tolearn component. Section C of Table <ref type="table" target="#tab_3">3</ref> summarizes the numerical results of both learning configurations. (4) The leverage of the input weighting algorithm is also investigated, where we benchmark the gClass feature weighting scheme against the FSC method in the empirical feature space-based feature weighting (no optimization) <ref type="bibr" target="#b18">[19]</ref>, the FSC method in the original feature space-based feature weighting <ref type="bibr" target="#b26">[27]</ref> and without the feature weighting algorithm. Section D of Table <ref type="table" target="#tab_3">3</ref> displays the numerical results of this empirical study. We overlook other learning modules, because they were proposed in our previous works in <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. The 10-fold Cross Validation (CV) procedure is utilized as an experimental procedure for the first two data sets from the UCI machine learning repository. The experimental results are inferred from the average of 10 independent runs of the CV scheme. We carry out the periodic-hold out test as the experimental scenario in the other two study cases to simulate the training and testing phases in real time <ref type="bibr" target="#b50">[51]</ref>. Final numerical results are deduced from the average of the 10 independent sub-processes of the periodic holdout procedure. The MIMO classifier's architecture is utilized to infer the classification decision for all learning configurations. The four learning modules deliver potent impacts to refine the resultant learning performance. The functional link consequentbased Chebyshev function is effective for boosting the classification rate, while sustaining the most compact and parsimonious rule base and suppressing the training sample consumption to an economical level compared to other fuzzy rule variants. The functional link consequent-based trigonometric function produces numerical results that are equivalent to gClass on the wine and weather data sets. However, it is inevitable that it will store higher parameters than the Chebyshev function. The arbitrarily rotated ellipsoidal clusters noticeably outperform the axisparallel ellipsoidal clusters in delivering higher classification rates and a more compact rule base in all cases. The drift detection strategy does not significantly affect gClass performance in the drift-free data sets, including the wine and ionosphere data sets. Conversely, performance improvements can be observed in the weather and thyroid data sets that have concept drift. The meta-cognitive learning is capable of reinforcing the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Benchmarks with state-of-the-art evolving classifiers</head><p>In this section, gClass is benchmarked with its counterparts: pClass <ref type="bibr" target="#b18">[19]</ref>, GENEFIS-Class <ref type="bibr" target="#b17">[18]</ref>, eClass <ref type="bibr" target="#b0">[1]</ref>, OS-ELM <ref type="bibr" target="#b47">[48]</ref>, FAOS-PFNN <ref type="bibr" target="#b41">[42]</ref> and McFIS <ref type="bibr" target="#b7">[8]</ref>. McFIS is akin to gClass and can be categorized as a meta-cognitive classifier. Meanwhile, pClass, GENEFIS-class and eClass are consolidated in our numerical study because they are evolving classifiers. The evolving classifier can be perceived as the predecessor of the meta-cognitive classifier. FAOS-PFNN represents a semi-online classifier, where it adopts the batched structural learning procedure, which ought to revisit all previous data streams in each training episode. In contrast, OS-ELM is built upon an incremental learning scenario without any structural learning scenario. This machine learning algorithm is deemed to be more traditional than evolving, meta-cognitive, or even semi-online classifiers.</p><p>All consolidated classifiers are evaluated in 10 synthetic and real-world data streams, characterizing various concept drifts. The synthetic streaming data are pivotal for the analysis of learning performance because it is inconvenient to determine the drift variant and when the drift initiates to interfere to the data distribution in the real-world problems. We explore 9 data streams, namely SEA <ref type="bibr" target="#b49">[50]</ref>, Electricity pricing, 1 weather, 2 hyperplane <ref type="bibr" target="#b50">[51]</ref>, and four artificial study cases from the (DDD) database <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>, termed sin, sinh, line circle, and boolean. In addition to these data sets, we make use of our own data set <ref type="bibr" target="#b53">[54]</ref>, not only featuring various concept drifts, but also demonstrating dynamic class labels. The predefined parameters of McFIS, eClass, GENEFISclass, FAOS-PFNN, OS-ELM and pClass are arranged as with the rules of thumb in their original publications.</p><p>The memory demand evaluation can be designated by the rule base parameters. The rule base parameter of gClass, pClass and GENEFIS-class are listed in Table <ref type="table">1</ref>. eClass generates the rule base parameters in the order of OðUP þ P þ mPðU þ 1ÞþPÞ, whereas McFIS, OS-ELM, and FAOS-PFNN attract OðUP þ P þ mPÞ of the rule base parameters. eClass is driven by the TSK-based spherical cluster fuzzy system, whereas OS-ELM, FAOS-PFNN and McFIS  are propelled by the single layer feed-forward network-like topology. We can assess the computational cost of the classifiers with the runtime. Our numerical study is conducted on an Intel (R) core (TM) i7-2600 CPU @ 3.4 GHz processor with 8 GB memory. The predictive quality can be demonstrated by the classification rate in generalizing the testing data block. The experiments are conducted with the use of the periodic hold-out process, whereas the classification boundary of all classifiers is crafted by the MIMO architecture. Table <ref type="table" target="#tab_4">4</ref> lists the consolidated experimental results. Fig. <ref type="figure" target="#fig_8">5(a,</ref><ref type="figure">b</ref>) shows the evolution of the feature weight and the trace of local forgetting for each fuzzy rule in the weather data set. The fuzzy rule evolution and the trace of predictive error in the hyper-plane data set are illustrated in Fig. <ref type="figure" target="#fig_8">5(c,</ref><ref type="figure">d</ref>).</p><p>Referring to Table <ref type="table" target="#tab_4">4</ref>, gClass prevails over other benchmarked algorithms in the three evaluation criteria: classification rate, fuzzy rule, and rule base parameter. In particular, gClass is capable of delivering the most encouraging accuracy in all the study cases, with 5-20% improvement over other consolidated algorithms. gClass overcomes other benchmarked algorithms in the realm of fuzzy rules in 9 out of 10 numerical studies, showing an improvement of 30-70% compared to the secondranked classifier. From the rule base parameter standpoint, gClass is capable of outstripping other classifiers in 7 out of 10 study cases. These numerical results are a firm justification of the generalized fuzzy rule of gClass, which boosts the classification rate while maintaining a frugal memory demand. Fig. <ref type="figure" target="#fig_8">5(a,</ref><ref type="figure">b</ref>) visualizes the adaptive characteristic of the feature weighting learning mechanism and the local forgetting degree in the weather dataset. The dynamic of the input weights is in line with the learning rate characteristic of the gradient ascent method, where it shrinks over time in the training process. The regime drifting property is expected to appear in the weather dataset. The concept drift is compensated for by the local drift handling strategy, distributing a unique forgetting degree for each rule to sidestep severe misclassification, as depicted in Fig. <ref type="figure" target="#fig_8">5(b)</ref>. The evolving characteristic of gClass is illustrated in Fig. <ref type="figure" target="#fig_8">5(c)</ref>, where the fuzzy rule can be augmented, recalled, and pruned on the fly during the training process. Conversely, the system error is stable in the bounded range, as presented in Fig. <ref type="figure" target="#fig_8">5(d)</ref>, which confirms the potency of the FWGRLS to perform the stable adaptation of the weight vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Statistical tests</head><p>To confirm our numerical results, statistical tests are performed to reach a clear conclusion about the performance of each classifier <ref type="bibr" target="#b18">[19]</ref>. Classifier rankings are shown in Table <ref type="table" target="#tab_7">5</ref>. The number of training samples, exploited in the training process, is not included in the statistical test, since only gClass and McFIS have the ability to reduce the number of training samples. The first statistical test is conducted with the non-parametric Friedman statistical test <ref type="bibr" target="#b54">[55]</ref>, well-known in the machine learning literature for detecting performance difference between benchmarked algorithms. We accordingly arrive at χ 2 F ¼ 30:2; 49:9; 28:07; 45:7 for the four evaluation criteria; the critical value of α ¼ 0:1 with 6 degrees of freedom is only 10.645, and we can thereby reject the null hypothesis. The Friedman statistical test is deemed defective. We therefore continue our statistical test with the ANOVA test proposed by Iman and Davenport <ref type="bibr" target="#b55">[56]</ref>. The ANOVA test offers a better test and generalizes the Friedman statistical test. Consequently, we arrive at F F ¼ 24:3; 19:8; 15:7; 18:3for the four evaluation categories respectively, and the critical value of α ¼ 0:05with (6,30) degrees of freedom is only 2.42, thus we can reject the null hypothesis, as with the Friedman test.</p><p>These two tests function to obtain the performance difference between all consolidated classifiers. Nevertheless, they do not make a conclusive finding that gClass outperforms other classifiers. We therefore undertake another statistical test, termed a post-hoc Benferoni-Dunn test <ref type="bibr" target="#b56">[57]</ref>, where the centric notion is to investigate the difference between the performances of two classifiers. For brevity, we can claim that the performance of two classifiers is substantially dissimilar if their difference exceeds the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Conceptual comparisons</head><p>In this section, gClass is conceptually compared with nine prominent classifiers recently published in the literature: eClass <ref type="bibr" target="#b0">[1]</ref>; pClass <ref type="bibr" target="#b18">[19]</ref>; GENEFIS-class <ref type="bibr" target="#b19">[20]</ref>; PBL-McRBFNN <ref type="bibr" target="#b6">[7]</ref>; McFIS <ref type="bibr" target="#b7">[8]</ref>; FAOS-PFNN <ref type="bibr" target="#b71">[72]</ref>; GEBF-FAOSPFNN <ref type="bibr" target="#b72">[73]</ref>; CP-and DP-ELM <ref type="bibr" target="#b73">[74]</ref>; BRand OSR-ELM <ref type="bibr" target="#b74">[75]</ref>. The salient characteristics of all the consolidated algorithms are summarized in Table <ref type="table">7</ref>. Clearly, gClass employs the most sophisticated fuzzy rule, amalgamating the multivariate Gaussian function in the rule input and the nonlinear Chebysev function in the rule output. This fuzzy rule is more appealing than classical fuzzy rules deployed in other algorithms. This has been numerically validated in Section 5.2. Albeit the nonaxis parallel ellipsoidal cluster in the rule antecedent, pClass and GENEFIS-class are still equipped with the standard linear hyperplane in the rule output, which does not fully explore a local approximation trait. Although GEBF-FAOSPNN is built upon the asymmetric Gaussian function, it still triggers the axis-parallel ellipsoidal cluster. In contrast, gClass exemplifies the semi-supervised learning scenario due to its online active learning procedure and demonstrates the plugand-play learning paradigm enabled by Scaffolding theory in the how-to-learn module <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>A novel meta-cognitive classifier, namely gClass, is proposed in this paper. The major contribution of gClass has three learning attributes: (1) gClass introduces a generalized meta-cognitive learning paradigm, in which the how-to-learn module is consistent with the Schema and Scaffolding theories; (2) gClass relies on a generalized TSK fuzzy rule, exploiting the multivariate Gaussian function in the premise component and the non-linear Chebyshev function in the consequent component; (3) four brand new learning modules, namely the ECI method, the LDQ method, the class overlapping method and the enhanced FSC in empirical feature space method, are devised in this paper. The efficacy of gClass has been thoroughly examined with 10 real-world and artificial datasets, featuring various concept drifts and dynamic class labels. In summary, gClass produces more encouraging numerical results than its counterparts in achieving a tradeoff between accuracy and simplicity. In our future works, we will expand the meta-cognitive-based Scaffolding learning theory to the interval type-2 fuzzy system. We will also apply the proposed algorithm to tool wear prognosis and the diagnosis of surface roughness in the ball-nose end milling process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>/*Phase 5 :</head><label>5</label><figDesc>Rule Pruning Strategy-fading of active scaffolding/ * For i¼ 1 to P do Enumerate the ERS and P þ methods (15) IF β i o β À 2β σ Then Prune the fuzzy rules End IF /*Phase 6: Rule Pruning Strategy-problematizing of active scaffolding /* IF χ i o χ À 2χ σ Then Deactivate the fuzzy rules subject to the rule recall mechanism P* ¼P* þ 1 End IF End For /*Phase 7: Adaptation of rule consequent-Passive Scaffolding Theories/* For i¼ 1 to P do Adjust the fuzzy rule consequents (18)-(21) End For /*Phase 8: When-to-learn-sample reserved strategy/* For n ¼1 to NS do Execute all training processes from steps 1 to 7 using the reserved samples ðXS n ; TS n Þ ¼ ðxs 1;n ; :::; xs u;n ; ts 1 ; :::; ts m Þ End For End IF (29)4.1. How to learn 4.1.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Learning architecture of gClass.</figDesc><graphic coords="5,92.52,491.89,420.24,240.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Cluster/rule representations (solid lines: arbitrarily rotated ellipsoids, dotted lines: axis-parallel ellipsoids), in case of the longer thin data cloud using conventional axis-parallel rules either an inexact presentation with one rule or an exact representation with a high complexity (three rules) is enforced, whereas a rotated representation can almost perfectly model the data cloud.</figDesc><graphic coords="5,48.03,288.77,240.24,142.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>À 1 and N o denotes the number of samples falling in the o-th class. Meanwhile, x N j stands for the latest incoming datum of the o-th class and x no j denotes the streaming data falling to the o-th class. This measure is useful for understanding whether or not the newest datum is closer to the data cloud of the same class. A precarious situation may arise if the new datum has a closer relationship with the data samples of different classes max o ¼ 1;…;m ðDQ o Þ atrue_class_label. The class over-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>No signifies the j-th element of the N o -th training sample of the o-th class and x j N ô denotes the jth element of the N ô-th training sample of the ô-th class. θ o and ν o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Conflict and ignorance cases.</figDesc><graphic coords="9,78.05,625.95,180.00,106.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. A flowchart of gClass learning scenario.</figDesc><graphic coords="10,82.70,58.64,420.24,325.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>generalization ability and curtailing the execution time. The what-to-learn module relieves the over-fitting by discarding redundant samples for the how-to-learn phase, and the whento-learn module makes adjustments to the fuzzy rules, utilizing the reserved samples. The reserved samples may disclose the uncovered states of already seen samples, thus intensifying the completeness of the rule base. The training data are not fully visited and the labeling process is governed by the active learning-based what-to-learn component mitigating the execution time, thereby strengthening the scalability in the big data. The virtue of the feature weighting algorithm-based FSC optimization in the empirical feature space can be seen in its positive contribution to the classification rates and ability to expedite the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) The evolution of feature weight for the temperature input feature, (b) local weighting strategy as drift detection, (c) fuzzy rules, and (d) system error.</figDesc><graphic coords="13,92.52,58.64,420.24,360.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>McFIS outperforms gClass in the runtime category, but note that although McFIS consumes a smaller number of training samples, the what-to-learn module does not transform McFIS into the semi-supervised classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>The rule premise of FAOS-PFNN, CP-and DP-ELM, BR-and OSR-ELM, PBL-McRBFNN and McFIS is deemed more traditional, because these classifiers cannot deal with different operating intervals of input variables as a result of the hyperspherical clusters. On the other hand, the rule consequent of FAOS-PFNN, GERBF-FAOSPFNN, CP-and DP-ELM, PBL-McRBFNN and McFIS are crafted by the zero order TSK output, which relies on a lower Degree of Freedom (DoF) function than the first order TSK output of eClass, pClass, GENEFIS-class and the non-linear output of gClass, BR-and OSR-ELM. It is worth noting that BR-and OSR-ELM do not actualize the functional link output weight, because the rule consequent is generated by the polynomial function without a specific nonlinear mapping. In the realm of algorithmic development, FAOS-PFNN, GEBF-FAOSPFNN, CP-and DP-ELM, BR-and OSR-ELM, pClass, eClass and GENEFIS-class are still cognitive in nature and have not yet integrated the meta-cognitive learning principle. Furthermore, although PBL-McRBFNN and McFIS employ the sample deletion strategy, this does not relieve the annotation effort of the operator, because the sample deletion strategy necessitates all data streams to be fully labelled. PBL-McRBFNN and McFIS also suffer from the absence of important learning modules such as the local forgetting mechanism and feature selection mechanism, because they do not incorporate Scaffolding theory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>The efficacy of gClass Learning module.</figDesc><table><row><cell>Algorithms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Numerical results of consolidated algorithms.</figDesc><table><row><cell>Algorithms</cell><cell></cell><cell>FAOS-PFNN</cell><cell>pClass</cell><cell>eClass</cell><cell>GENEFIS-class</cell><cell>gClass</cell><cell>OS-ELM</cell><cell cols="2">McFIS</cell></row><row><cell>SEA dataset</cell><cell>Classification rate</cell><cell>0.737 0.14</cell><cell>0.78 70.04</cell><cell>0.76 70.03</cell><cell>0.767 0.01</cell><cell>0.877 0.09</cell><cell>0.617 0.001</cell><cell cols="2">0.737 0.11</cell></row><row><cell></cell><cell>Rule</cell><cell>149.5 791.6</cell><cell>3.5 72.42</cell><cell>15.9 7 3.4</cell><cell>2.9 7 1</cell><cell>2.3 7 0.5</cell><cell>50</cell><cell cols="2">9.9 7 0.4</cell></row><row><cell></cell><cell>Time(s)</cell><cell>1656 71633</cell><cell>3.86 70.5</cell><cell>10.077 2.3</cell><cell>3.02 7 0.26</cell><cell>1.03 70.2</cell><cell>0.006 7 0.008</cell><cell cols="2">0.137 0.03</cell></row><row><cell></cell><cell>Rule base</cell><cell>750.5 7 458.13</cell><cell>70</cell><cell>190.8</cell><cell>58</cell><cell>46</cell><cell>300</cell><cell cols="2">54.6</cell></row><row><cell></cell><cell>Num of samples</cell><cell>4200</cell><cell>4200</cell><cell>4200</cell><cell>4200</cell><cell>106.6</cell><cell>4200</cell><cell>10.9</cell><cell>n</cell></row><row><cell>Electricity pricing dataset</cell><cell>Classification rate</cell><cell>0.517 0.08</cell><cell>0.78 70.05</cell><cell>0.777 0.07</cell><cell>0.75 7 0.0</cell><cell>0.79 7 0.08</cell><cell>0.577 0.09</cell><cell cols="2">0.5 7 0.1</cell></row><row><cell></cell><cell>Rule</cell><cell>50.9 7 23.7</cell><cell>3.2 71.2</cell><cell>11.9 70.07</cell><cell>3.5 7 1.5</cell><cell>2.7 7 0.5</cell><cell>50</cell><cell cols="2">9.6 7 0.7</cell></row><row><cell></cell><cell>Time(s)</cell><cell>196.717 127.9</cell><cell>4.23 70.8</cell><cell>4.127 2.2</cell><cell>4.497 0.4</cell><cell>2.3 7 0.5</cell><cell>2.43 7 0.2</cell><cell cols="2">0.57 0.4</cell></row><row><cell></cell><cell>Rule base</cell><cell>517 7247.4</cell><cell>288</cell><cell>321.3</cell><cell>315</cell><cell>243</cell><cell>550</cell><cell cols="2">110.9</cell></row><row><cell></cell><cell>Num of samples</cell><cell>3172</cell><cell>3172</cell><cell>3172</cell><cell>3172</cell><cell>8.7</cell><cell>3172</cell><cell cols="2">10.6 n</cell></row><row><cell>Sin dataset</cell><cell>Classification rate</cell><cell>0.7770.13</cell><cell>0.82 70.2</cell><cell>0.81 7 0.5</cell><cell>0.81 7 0.2</cell><cell>0.92 7 0.3</cell><cell>0.8 7 0.2</cell><cell cols="2">0.767 0.18</cell></row><row><cell></cell><cell>Rule</cell><cell>34.8 7 5.14</cell><cell>3.3 71.2</cell><cell>4 71.14</cell><cell>5.4 7 2.2</cell><cell>3.3 7 0.9</cell><cell>50</cell><cell cols="2">9.17 1.2</cell></row><row><cell></cell><cell>Time(s)</cell><cell>0.2470.05</cell><cell>0.177 0.04</cell><cell>0.2 7 0.02</cell><cell>0.32 7 0.3</cell><cell>0.16 70.02</cell><cell>0.25 7 0.02</cell><cell cols="2">0.087 0.03</cell></row><row><cell></cell><cell>Rule base</cell><cell>141.2</cell><cell>40.6</cell><cell>44</cell><cell>58.8</cell><cell>39.6</cell><cell>500</cell><cell cols="2">68.6</cell></row><row><cell></cell><cell>Num of samples</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>56.4</cell><cell>200</cell><cell cols="2">10.1 n</cell></row><row><cell>Circle dataset</cell><cell>Classification rate</cell><cell>0.8 7 0.12</cell><cell>0.727 0.13</cell><cell>0.7 7 0.11</cell><cell>0.7 7 0.03</cell><cell>0.917 0.06</cell><cell>0.667 0.14</cell><cell cols="2">0.8 7 0.14</cell></row><row><cell></cell><cell>Rule</cell><cell>27.3 75.4</cell><cell>2.8 71.1</cell><cell>3.6 7 0.84</cell><cell>3.2 7 1.03</cell><cell>2.4 7 1.6</cell><cell>50</cell><cell cols="2">9.8 7 0.42</cell></row><row><cell></cell><cell>Time(s)</cell><cell>0.157 0.03</cell><cell>0.177 0.008</cell><cell>0.19 70.01</cell><cell>0.25 7 0.01</cell><cell>0.15 70.02</cell><cell>0.087 0.02</cell><cell cols="2">0.127 0.05</cell></row><row><cell></cell><cell>Rule base</cell><cell>111.2</cell><cell>33.6</cell><cell>32.4</cell><cell>38.4</cell><cell>28.8</cell><cell>500</cell><cell cols="2">62.8 7 2.5</cell></row><row><cell></cell><cell>Num of samples</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>56.2</cell><cell>200</cell><cell cols="2">10.8 n</cell></row><row><cell>Line dataset</cell><cell>Classification rate</cell><cell>0.917 0.06</cell><cell>0.917 0.07</cell><cell>0.89 7 0.06</cell><cell>0.9 7 0.07</cell><cell>0.94 7 0.1</cell><cell>0.917 0.08</cell><cell cols="2">0.84 7 0.13</cell></row><row><cell></cell><cell>Rule</cell><cell>267 7.2</cell><cell>2.5 70.71</cell><cell>4.4 7 0.51</cell><cell>3.6 7 0.7</cell><cell>2</cell><cell>25</cell><cell cols="2">9.4 7 1</cell></row><row><cell></cell><cell>Time(s)</cell><cell>0.177 0.04</cell><cell>0.25 70.0009</cell><cell>0.21 70.009</cell><cell>0.2470.01</cell><cell>0.14 70.06</cell><cell>0.04 70.02</cell><cell cols="2">0.17 0.03</cell></row><row><cell></cell><cell>Rule base</cell><cell>106</cell><cell>30</cell><cell>39.6</cell><cell>43.2</cell><cell>24</cell><cell>250</cell><cell cols="2">60.4 7 6.4</cell></row><row><cell></cell><cell>Num of samples</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>11.4</cell><cell>200</cell><cell cols="2">10.4 n</cell></row><row><cell>Sinh dataset</cell><cell>Classification rate</cell><cell>0.617 0.23</cell><cell>0.717 0.09</cell><cell>0.7 7 0.07</cell><cell>0.717 0.06</cell><cell>0.717 0.04</cell><cell>0.687 0.04</cell><cell cols="2">0.647 0.15</cell></row><row><cell></cell><cell>Rule</cell><cell>34.9 7 0.23</cell><cell>3.6 71.9</cell><cell>6.3 7 1.5</cell><cell>3.6 þ0.8</cell><cell>2.17 0.3</cell><cell>50</cell><cell>10</cell></row><row><cell></cell><cell>Time(s)</cell><cell>0.26 7 0.08</cell><cell>0.277 0.01</cell><cell>0.23 7 0.02</cell><cell>0.25 7 0.02</cell><cell>0.11 7 0.08</cell><cell>0.077 0.02</cell><cell cols="2">0.17 0.02</cell></row><row><cell></cell><cell>Rule base</cell><cell>141.6</cell><cell>43.2</cell><cell>56.7</cell><cell>43.2</cell><cell>33.6</cell><cell>500</cell><cell>64</cell></row><row><cell></cell><cell>Num of samples</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>37.8</cell><cell>200</cell><cell>11 n</cell></row><row><cell>Weather dataset</cell><cell>Classification rate</cell><cell>0.6770.06</cell><cell>0.790 70.03</cell><cell>0.7777 0.02</cell><cell>0.790 7 0.01</cell><cell>0.8 7 0.03</cell><cell>0.747 0.06</cell><cell cols="2">0.617 0.14</cell></row><row><cell></cell><cell>Rule</cell><cell>99.8 7 62.8</cell><cell>2.5 70.97</cell><cell>2.7 7 0.48</cell><cell>2.5 7 0.51</cell><cell>1.1 7 0.32</cell><cell>40</cell><cell>10</cell></row><row><cell></cell><cell>Time(s)</cell><cell>209.8 7 250.1</cell><cell>1.127 0.06</cell><cell>1.067 0.07</cell><cell>1.2 7 0.07</cell><cell>0.86 7 0.05</cell><cell>0.56 7 0.7</cell><cell cols="2">0.417 0.08</cell></row><row><cell></cell><cell>Rule base</cell><cell>1006 7627.5</cell><cell>205</cell><cell>72.9</cell><cell>202.5</cell><cell>99</cell><cell>1080</cell><cell>108</cell></row><row><cell></cell><cell>Num of samples</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell><cell>1000</cell><cell>37.7</cell><cell>1000</cell><cell>11 n</cell></row><row><cell>Hyper-plane dataset</cell><cell>Classification rate</cell><cell>0.58 7 0.3</cell><cell>0.92 70.02</cell><cell>0.917 0.02</cell><cell>0.917 0.01</cell><cell>0.93 7 0.02</cell><cell>0.88 7 0.03</cell><cell cols="2">0.737 0.06</cell></row><row><cell></cell><cell>Rule</cell><cell>63.8 7 3.7</cell><cell>2.2 7 0.63</cell><cell>8.6 7 2</cell><cell>3.39 7 0.12</cell><cell>2.8 7 0.6</cell><cell>35.374.16</cell><cell>10</cell></row><row><cell></cell><cell>Time(s)</cell><cell>2.7 7 0.7</cell><cell>1.86 7 0.07</cell><cell>13.487 3.61</cell><cell>3.4 7 0.05</cell><cell>1.55 70.5</cell><cell>1.22 7 0.13</cell><cell cols="2">0.57 0.1</cell></row><row><cell></cell><cell>Rule base</cell><cell>840.4</cell><cell>66</cell><cell>124.4</cell><cell>90</cell><cell>84</cell><cell>2118</cell><cell>64</cell></row><row><cell></cell><cell>Num of samples</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>33.6</cell><cell>100</cell><cell>11</cell></row><row><cell>Noise corrupted signal dataset</cell><cell>Classification rate</cell><cell>0.377 0.37</cell><cell>0.747 0.12</cell><cell>0.727 0.12</cell><cell>0.737 0.09</cell><cell>0.75 7 0.11</cell><cell>0.727 0.14</cell><cell cols="2">0.69 7 0.14</cell></row><row><cell></cell><cell>Rule</cell><cell>20.9</cell><cell>3 7 1.2</cell><cell>3.7 7 1.3</cell><cell>4.5 7 1.1</cell><cell>2</cell><cell>50</cell><cell cols="2">9.6 7 3.4</cell></row><row><cell></cell><cell>Time(s)</cell><cell>86.17 109.7</cell><cell>6.4 70.7</cell><cell>6.9 7 1.9</cell><cell>7.5 7 0.9</cell><cell>5.95 7 1.7</cell><cell>2.237 0.11</cell><cell cols="2">2.6 7 1</cell></row><row><cell></cell><cell>Rule base</cell><cell>84.6</cell><cell>24</cell><cell>29.6</cell><cell>36</cell><cell>12</cell><cell>400</cell><cell cols="2">50.2</cell></row><row><cell></cell><cell>Num of samples</cell><cell>7000</cell><cell>7000</cell><cell>7000</cell><cell>7000</cell><cell>37.2</cell><cell>7000</cell><cell cols="2">10.6 n</cell></row><row><cell>Boolean dataset</cell><cell>Classification rate</cell><cell>0.7770.13</cell><cell>0.83 70.2</cell><cell>0.85 7 0.12</cell><cell>0.82 7 0.2</cell><cell>0.92 7 0.2</cell><cell>0.8 7 0.17</cell><cell cols="2">0.86 7 0.2</cell></row><row><cell></cell><cell>Rule</cell><cell>11.7 7 3.1</cell><cell>2.6 70.8</cell><cell>4.7 7 1.3</cell><cell>2.6 7 1.1</cell><cell>2.3 7 0.5</cell><cell>50</cell><cell cols="2">7.4 7 1.9</cell></row><row><cell></cell><cell>Time (s)</cell><cell>0.047 0.007</cell><cell>0.08 70.002</cell><cell>0.05 7 0.01</cell><cell>0.09 7 0.05</cell><cell>0.017 0.03</cell><cell>0.2470.02</cell><cell cols="2">0.05 7 0.05</cell></row><row><cell></cell><cell>Rule base</cell><cell>61.5</cell><cell>52</cell><cell>56.4</cell><cell>52</cell><cell>46</cell><cell>250</cell><cell cols="2">80.2</cell></row><row><cell></cell><cell>Num of samples</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>5.2</cell><cell>100</cell><cell>8.4 n</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>The performance difference between rClass and other algorithms.</figDesc><table><row><cell>Algorithms</cell><cell>Classification</cell><cell>Fuzzy</cell><cell cols="2">Runtimes Rule base</cell></row><row><cell></cell><cell>rates</cell><cell>rules</cell><cell></cell><cell>parameters</cell></row><row><cell>gClass vs. pClass</cell><cell>1.46</cell><cell>0.94</cell><cell>1.98</cell><cell>1.56</cell></row><row><cell>gClass vs. eClass</cell><cell>3.12</cell><cell>3.02</cell><cell>2.38</cell><cell>1.98</cell></row><row><cell>gClass vs. GENEFIS-</cell><cell>2.4</cell><cell>1.77</cell><cell>3.12</cell><cell>2.49</cell></row><row><cell>class</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>gClass vs. OS-ELM</cell><cell>4.8</cell><cell>5.5</cell><cell>-0.35</cell><cell>5.3</cell></row><row><cell>gClass vs. McFIS</cell><cell>4.99</cell><cell>3.85</cell><cell>-1.04</cell><cell>2.5</cell></row><row><cell>gClass vs. FAOS-</cell><cell>4.78</cell><cell>5.62</cell><cell>2.5</cell><cell>5.2</cell></row><row><cell>PFNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>Classifier ranking according to classification rate. CD ¼ 2:31. The performance difference of classifiersR i ; R j is detailed in Table 6. gClass is clearly more reliable than eClass, GENEFIS-class, OS-ELM, FAOS-PFNN and McFIS in the realm of classification rate, whereas gClass outperforms eClass,OS-ELM,FAOS-PFNN and McFIS in the context of fuzzy rule, rule base parameters. On the other hand, gClass is slightly inferior to OS-ELM and McFIS but is substantially superior to eClass, FAOS-PFNN and GENEFIS-Class in the viewpoint of execution time.</figDesc><table><row><cell>Study cases</cell><cell>FAOS-PFNN (CR, R, ET,</cell><cell>pClass (CR, R, ET,</cell><cell>eClass (CR, R, ET,</cell><cell>GENEFIS-class (CR, R, ET,</cell><cell>gClass (CR, R, ET,</cell><cell>OS-ELM (CR, R, ET,</cell><cell>McFIS (CR, R, ET,</cell></row><row><cell></cell><cell>RB)</cell><cell>RB)</cell><cell>RB)</cell><cell>RB)</cell><cell>RB)</cell><cell>RB)</cell><cell>RB)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Please cite this article as: M. Pratama, et al., An incremental meta-cognitive-based scaffolding fuzzy neural network, Neurocomputing</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Please cite this article as: M. Pratama, et al., An incremental meta-cognitive-based scaffolding fuzzy neural network, Neurocomputing (2015), http://dx.doi.org/10.1016/j.neucom.2015.06.022i</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work presented in this paper is partly supported by the Australian Research Council (ARC) under Discovery Project nos.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sreenatha Anavatti received his Ph.D. degree in aerospace engineering from the Indian Institute of Science in 1990, his Bachelor of Engineering degree in mechanical engineering from the Mysore University, India in 1984. He is currently a Senior Lecturer at the School of Aerospace, Civil and Mechanical Engineering (ACME), University of New South Wales at Australian Defence Force Academy (UNSW@ADFA), Australia. His current research interests include control systems, flight dynamics, robotics, aeroelasticity, artificial neural networks, fuzzy systems and unmanned systems.</p><p>Edwin David Lughofer received his Ph.D. degree from the Department of Knowledge-Based Mathematical Systems, University Linz. He is now employed as key researcher at the department's branch Fuzzy Logic Laboratorium in the Softwarepark Hagenberg. He participated in several international research (EU) projects and has published more than 100 journal and conference papers in the fields of evolving fuzzy systems, machine learning and vision, clustering, fault detection, including a monograph on 'Evolving Fuzzy Systems' (Springer, Heidelberg) and an edited book on 'Learning in Non-stationary Environments' (Springer, New York). He acts as a reviewer in peer-reviewed international journals and as (co-)organizer of special sessions and issues at international conferences and journals. In 2014, he served as Main Chair of the international IEEE conference on Evolving and Adaptive Intelligent Systems. He served as programme committee member in several international conferences and is a member of the editorial board and associate editor of the international Springer journal 'Evolving Systems' and the international Elsevier Journal 'Information Fusion'. In 2006, he received the best paper award at the International Symposium on Evolving Fuzzy Systems, and in 2013 the best paper award at the IFAC conference in Manufacturing Modeling, Management and Control Conference.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evolving fuzzy-rule-based classifiers from data streams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1462" to="1475" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reliable all-pairs evolving fuzzy classifiers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Buchtala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="625" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simpl_eClass: simplified potential-free evolving fuzzy rule-based classifiers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Baruah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andreu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2011 IEEE International Conference on Systems, Man and Cybernetics</title>
		<meeting>2011 IEEE International Conference on Systems, Man and Cybernetics<address><addrLine>Anchorage, Alaska, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="2249" to="2254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evolving fuzzy classifiers using different model architectures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets Syst</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="3160" to="3182" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive fault detection and diagnosis using an evolving fuzzy classifier</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lemos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Caminhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomide</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="64" to="85" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Metamemory: a theoretical framework and new findings</title>
		<author>
			<persName><forename type="first">T.-O</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Narens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Learn. Motiv</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="125" to="173" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequential projection-based metacognitive learning in a radial basis function network for classification problems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="194" to="206" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cognitive Neuro-Fuzzy Inference System (McFIS) for sequential classification systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><surname>Meta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1060" to="1095" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A meta-cognitive sequential learning algorithm for neuro-fuzzy inference system</title>
		<author>
			<persName><forename type="first">K</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="3603" to="3614" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Meta-cognitive rbf network and its projection based learning algorithm for classification problems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sateesh Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="654" to="666" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A meta-cognitive interval type-2 fuzzy inference system classifier and its projection-based learning algorithm</title>
		<author>
			<persName><forename type="first">K</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Savitha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium Series on Computational Intelligence</title>
		<meeting>the IEEE Symposium Series on Computational Intelligence<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scaffolding complex learning: the mechanisms of structuring and problematizing student work</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Reiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Learn. Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="304" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Remembering: A study in Experimental and Social Psychology</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Bartett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1932">1932</date>
			<publisher>Cambridge Press University Press</publisher>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piagiet&apos;s legacy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Flavell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="200" to="203" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Incremental learning of concept drift in non-stationary environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Elwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1517" to="1531" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A massively parallel architecture for a selforganizing neural pattern recognition machine</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis., Graph., Image Process</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="54" to="115" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PANFIS: a novel incremental learning machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anavatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="68" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GENEFIS: towards an effective localist network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anavatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="547" to="562" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anavatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">pClass: An Effective Classifier to Streaming Examples</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="369" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evolving fuzzy rule-based classifier based on GENEFIS</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anavatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Fuzzy System (Fuzz-IEEE)</title>
		<meeting>the IEEE Conference on Fuzzy System (Fuzz-IEEE)<address><addrLine>Hyderabad, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data driven modelling based on dynamic parsimonious fuzzy neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Oentaryo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Arifin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="18" to="28" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A novel meta-cognitivebased scaffolding classifier to sequential non-stationary classification problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anavatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Arifin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE World Congress on Computational Intelligence (IEEE-WCCI)</title>
		<meeting>the IEEE World Congress on Computational Intelligence (IEEE-WCCI)<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nonlinear dynamic system identification using Chebyshev functional link artificial neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man Cybern.-Part B: Cybern</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="505" to="511" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Identification and prediction of dynamic systems using an interactively recurrent self-evolving fuzzy neural network</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="310" to="321" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hybrid active learning for reducing the annotation effort of operators in classification systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="884" to="896" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-adaptive and local strategies for a smooth treatment of drifts in data streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evol. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="239" to="257" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On-line incremental feature weighting in evolving fuzzy classifiers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets Syst</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiclass feature selection with kernel gram-matrix-based criteria</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ramona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Riachard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1611" to="1622" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimizing the kernel in the empirical feature space</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N S</forename><surname>Swamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="460" to="474" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Pao</surname></persName>
		</author>
		<title level="m">Adaptive Pattern Recognition and Neural Networks</title>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Identification of nonlinear dynamic systems using functional link artificial neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man Cybern</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="262" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Extended sequential adaptive fuzzy inference system for classification problems</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evol. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="71" to="82" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fuzzy passive-aggressive classification: a robust and efficient algorithm for online classification problems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="46" to="63" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Bayesian ARTMAP</title>
		<author>
			<persName><forename type="first">B</forename><surname>Vigdor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lerner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1628" to="1644" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Handling drifts and shifts in on-line data streams with evolving fuzzy systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2057" to="2068" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Data compression by volume prototypes for streaming data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tabata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S M</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3162" to="3176" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<title level="m">Evolving Fuzzy Systems-Methodologies, Advanced Concepts and Applications</title>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On-line assurance of interpretability criteria in evolving fuzzy systems-achievements, new concepts and open issues</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">251</biblScope>
			<biblScope unit="page" from="22" to="46" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generalized recursive least square to the training of neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="34" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Flexible evolving fuzzy inference systems from data streams (FLEXFIS þ þ)</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in Non-Stationary Environments: Methods and Applications</title>
		<editor>
			<persName><forename type="first">Moamar</forename><surname>Sayed-Mouchaweh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="205" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Evolving Takagi-Sugeno fuzzy systems from data streams (eTS þ )</title>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolving Intelligent Systems: Methodology and Applications</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Filev</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</editor>
		<imprint>
			<publisher>IEEE Press Series on Computational Intelligence</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="21" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A TS-type maximizing-discriminability-based recurrent fuzzy network for classification problems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="339" to="352" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A maximizing-discriminability-based self-organizing fuzzy network for classification problems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="362" to="373" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Single-pass active learning with conflict and ignorance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evol. Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="251" to="271" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Autonomous data stream clustering implementing incremental split-and-merge techniques-towards a plugand-play approach</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sayed-Mouchaweh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<biblScope unit="page" from="54" to="79" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unbiased online active learning in data streams</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="195" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Active learning from stream data using optimal weight classifier ensemble</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man Cybern.-Part b: Cybern</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1607" to="1621" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A fast and accurate online sequential learning algorithm for feedforward networks</title>
		<author>
			<persName><forename type="first">N.-Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1411" to="1423" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<title level="m">Neural Networks: A Comprehensive Foundation, 2nd edition</title>
		<meeting><address><addrLine>Upper Saddle River, New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall Inc</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A streaming ensemble algorithm SEA for large-scale classification</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Street</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM SIGKDD Conference</title>
		<meeting>the 7th ACM SIGKDD Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="377" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">MOA: massive online analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1601" to="1604" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">DDD: a new ensemble approach for dealing with drifts</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Minku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="619" to="633" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The impact of diversity on online ensemble learning in the presence concept of drift</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Minku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="730" to="742" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">ARPOP: an appretitive reward-based pseudoouter-product neural fuzzy inference system inspired from the operant conditioning of feeding behaviour</title>
		<author>
			<persName><forename type="first">E.-Y</forename><surname>Cheu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Quek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-K</forename><surname>Ny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="317" to="329" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Approximations of the critical region of the Friedman statistic</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Iman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Davenport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics A</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="571" to="595" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multiple comparisons among means</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="52" to="64" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fuzzy identification of systems and its appfications to modeling and control</title>
		<author>
			<persName><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugeno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man Cybern</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="116" to="132" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">LClass: error-driven antecedent learning for evolving Takagi-Sugeno classification systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Almaksour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Anquetil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="419" to="429" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Endpoint prediction model for basic oxygen furnace steelmaking based on membrane algorithm evolving extreme learning machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="430" to="437" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Self-tuning of 2 DOF control based on evolving fuzzy model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zdesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dovzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Skrjanc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="403" to="418" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Online identification of evolving Takagi-Sugeno-Kang fuzzy models for crane systems</title>
		<author>
			<persName><forename type="first">R.-E</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-I</forename><surname>Filip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-B</forename><surname>Radac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Petriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Preitl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-A</forename><surname>Dragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1155" to="1163" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Extended sequential adaptive fuzzy inference system for classification problems</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evol. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="71" to="82" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sayed-Mouchaweh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<title level="m">Learning in Non-Stationary Environments: Methods and Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Knowledge Discovery from Data Streams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<pubPlace>Boca Raton, Florida</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Vygotsky</surname></persName>
		</author>
		<title level="m">Mind and Society: The Development of Higher Psychological Processes</title>
		<meeting><address><addrLine>Cambridge, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Harvard University Press</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Scaffolding contingent tutoring and computer-based learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Artif. Intell. Educ</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="280" to="292" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Scaffolding complex learning: the mechanisms of structuring and problematizing student work</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Reiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Learn. Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="304" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A meta-cognitive algorithm for an extreme learning machine classifier</title>
		<author>
			<persName><forename type="first">R</forename><surname>Savitha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognit. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="253" to="263" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A metacognitive complex-valued interval type-2 fuzzy inference system</title>
		<author>
			<persName><forename type="first">K</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Savitha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1659" to="1672" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A meta-cognitive interval type-2 fuzzy inference system and its projection based learning algorithm</title>
		<author>
			<persName><forename type="first">K</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Savitha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolv. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="219" to="230" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A fast and accurate online self-organizing scheme for parsimonious fuzzy neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="3818" to="3829" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A generalized ellipsoidal basis function based online selfconstructing fuzzy neural network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13" to="37" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Parsimonious extreme learning machine using recursive orthogonal least squares</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1828" to="1841" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Generalized Single-Hidden Layer Feedforward Networks for Regression Problems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-J</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Nueral Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1161" to="1176" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
