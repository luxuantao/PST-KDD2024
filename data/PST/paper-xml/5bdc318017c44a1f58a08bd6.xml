<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using a Stacked Residual LSTM Model for Sentiment Intensity Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<settlement>Kunming</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<settlement>Kunming</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<settlement>Kunming</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<settlement>Kunming</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xuejie</forename><surname>Zhang</surname></persName>
							<email>xjzhang@ynu.edu.cn.</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<settlement>Kunming</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Yunnan University</orgName>
								<address>
									<settlement>Kunming</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Using a Stacked Residual LSTM Model for Sentiment Intensity Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">995215AF45A8844D720D1744AAE35536</idno>
					<idno type="DOI">10.1016/j.neucom.2018.09.049</idno>
					<note type="submission">Received date: 18 July 2017 Revised date: 9 July 2018 Accepted date: 18 September 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Sentiment Intensity Prediction</term>
					<term>Stacked Residual LSTM</term>
					<term>Neural Network</term>
					<term>Sentiment Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The sentiment intensity of a text indicates the strength of its association with positive sentiment, which is a continuous real-value between 0 and 1. Compared to polarity classification, predicting sentiment intensities for texts can achieve more fine-grained sentiment analyses. By introducing word embedding techniques, recent studies that use deep neural models have outperformed existing lexicon-and regression-based methods for sentiment intensity prediction. For better performance, a common way of a neural network is to add more layers in order to learn high-level features. However, when the depth increases, the network degrades and becomes more difficult to train. Since the errors between layers will be accumulated, and gradients will be vanished. To address this problem, this paper proposes a stacked residual LSTM model to predict sentiment intensity for a given text. By investigating the performances of shallow and deep architectures, we introduce a residual connection to every few LSTM layers to construct an 8-layer neural network. The residual connection can center layer gradients and propagated errors. Thus it makes the deeper network easier to optimize. This approach enables us to stack more layers of LSTM successfully for this task, which can improve the prediction accuracy of existing methods. Experimental results show that the proposed method outperforms lexicon-, regression-, and conventional NN-based methods proposed in previous studies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Online social networking services (SNSs), such as Twitter, Facebook and Weibo, enable users to share their thoughts, opinions, and emotions with others through texts which are informal and strongly subjective. Analysis of user-generated information is very useful for understanding how sentiments spread from person to person on the Internet. Sentiment analysis techniques <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref> provide a way to handle such affective information automatically. As an active research field in computational linguistic and affective computing <ref type="bibr" target="#b5">[6]</ref>,</p><p>A C C E P T E D M A N U S C R I P T sentiment analysis can analyze, process, induce and deduce such subjective texts with affective information.</p><p>Most existing methods of sentiment analysis focus on the polarity classification approach, which classifies the target texts into several categories, e.g., positive or negative. Such methods mostly use classification models. The methods first extract features such as n-gram, bag-of-words (BOW) or part-of-speech (POS) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Next, support vector machine (SVM), naïve Bayes, maximum entropy, logistic regression or random forest methods are applied on these features to classify texts into either positive or negative classes <ref type="bibr" target="#b8">[9]</ref>.</p><p>Alternatively, sentiment intensity prediction could be another choice for sentiment analysis. More specifically, sentiment intensity of a word, phrase or text indicates the strength of its association with positive sentiment, also known as valence values <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> or affective ratings. It is a score between 0 and 1, where the score of 0 or 1 respectively indicates the least or maximum association with positive sentiment (i.e., negative or positive). In contrast to the traditional approach, the intensity prediction is usually considered to be a regression, rather than a classification, since the sentiment intensity is defined as a continuous real-value. The following three movie reviews were rated in both polarity and sentiment intensity, as taken from the Stanford Sentiment Treebank † (SST) <ref type="bibr" target="#b11">[12]</ref> corpus:</p><p>(Text 1 negative, senti=0.375) The movie is genial but never inspired, and little about it will stay with you. (Text 2 negative, senti=0.194) However, the movie does not really deliver for country music fans or for family audiences. (Text 3 negative, senti=0.083) Bears are even worse than I imagined a movie ever could be.</p><p>All three reviews about the movie were classified to be negative. However, the 3rd review, which was rated a lower intensity (more negative) than other two texts, requires a higher priority to draw attention. In addition, the intensity prediction approach can provide more intelligent and fine-grained sentiment applications, such as hotspot detection and forecasting <ref type="bibr" target="#b12">[13]</ref>, mental illness identification <ref type="bibr" target="#b13">[14]</ref>, financial news analysis <ref type="bibr" target="#b14">[15]</ref>, question answering <ref type="bibr" target="#b15">[16]</ref>, and blog post analysis <ref type="bibr" target="#b16">[17]</ref>.</p><p>Few studies have sought to predict continuous affective ratings of texts using lexicon-and regression-based methods. The lexicon-based methods are based on the underlying assumption for most algorithms that the intensity of a given text can be estimated via the composition of the intensity of its constituent words <ref type="bibr" target="#b17">[18]</ref>. Another approach uses regression-based methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. These methods sought to learn the correlations between sentiment intensities and linguistic features of words, e.g., BoW and POS. However, the prediction performance of such methods is still low.</p><p>Recently, several classification methods have been implemented to explore the use of deep neural networks and word embedding, such as convolutional neural networks (CNN) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, recurrent neural networks (RNN) <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> and long-short term memory (LSTM) <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. CNN <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> is able to extract active local n-gram features. Conversely, LSTM <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> can sequentially model the texts. This model focuses on past information and draws conclusions from the entire text. In addition, such NN methods are used as classifiers to distinguish whether the given text is positive or negative. These models have not been thoroughly investigated for sentiment intensity prediction.</p><p>In image recognition, several recently proposed models, such as VGG <ref type="bibr" target="#b26">[27]</ref>, InceptionNet <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> and ResNet <ref type="bibr" target="#b29">[30]</ref>, have all exploited very "deep" architecture. The success of these models reveals that increasing the depth of a neural network can help improve the performance of learning models as deeper networks learn better representations of features <ref type="bibr" target="#b30">[31]</ref>. For language modeling tasks, a feasible way of applying deep architecture is to use a stacked CNN <ref type="bibr" target="#b31">[32]</ref> or LSTM <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> model. The question is becoming clear:</p><formula xml:id="formula_0">Is A C C E P T E D M A N U S C R I P T</formula><p>training better the prediction model as simple as stacking more layers? In fact, deeper networks are easily impacted by the degradation problem: when the network has more layers, prediction accuracy will becoming saturated and never increase again. Since the errors between layers will be accumulated, and gradients will be vanished. To explain this phenomenon, we trained a conventional 2-layer LSTM and a stacked 8-layer plain LSTM on SST for sentiment intensity prediction. Figure <ref type="figure" target="#fig_0">1</ref> shows the mean squared error and Pearson correlation coefficient of these two models on training and testing sets. Unexpectedly, the deeper network has higher training and test error. Similar results on other corpora are shown in Fig. <ref type="figure">4</ref>. This result indicates that the more stacked layers there are, the harder it is to optimize.</p><p>In this paper, we propose a stacked residual LSTM model to predict the sentiment intensity of a given text. To tackle the degradation problem, we introduce the residual connection to every few LSTM layers inspired by ResNet <ref type="bibr" target="#b29">[30]</ref>. The residual connection can center layer gradients and propagated errors. Thus, it makes the network easier to optimize. This approach enables us to stack more layers of LSTM successfully for NLP task. As similar as stacked deep convolution networks extract different level features from pixels to shapes and contours in previous image processing task <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref>, the proposed stacked LSTM model can extract higher level sequence features from lower level n-gram features to form a hierarchical representation. These features are linguistic function blocks, which could be words, phrases, clauses, sentences, or even a paragraphs. Experiments were conducted on four English and Chinese corpora to evaluate the performance of the stacked residual LSTM model. We first investigate the degradation problems in stacked LSTM when the model is deepened. Next, the proposed model is compared with several previously proposed methods, such as traditional lexicon-, and regression-based methods, and the conventional deep neural network-based models, including CNN, LSTM and RNN. Besides sentiment intensity prediction, this stacked model with residual connections can also be used to build various time series prediction applications, such as short-term electrical load forecasting <ref type="bibr" target="#b34">[35]</ref>, solar irradiation forecasting <ref type="bibr" target="#b35">[36]</ref>, QoS estimating of stream service <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> and video sequence recognition <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>.</p><p>The rest of this paper is organized as follows. Section 2 offers a brief review of related works. Section 3 describes the proposed neural network model and residual architecture. Section 4 summarizes the comparative results of different methods for sentiment intensity prediction. The study's conclusions are presented in Section 5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Sentiment intensity of a word, phrase or sentence indicates its strength of positive emotion. In this section, we present a brief review about sentiment intensity prediction of texts, including lexicon-, regression-and conventional neural network-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Lexicon-Based Methods</head><p>Lexicon-based methods are based on the underlying assumption that the intensity of a text can be estimated via the composition of the intensity of its constituent words. An affective lexicon, in which affective words are tagged with sentiment ratings, is always used as the basis of each text in these methods.</p><p>Given the affective scores of words, one may calculate the affective scores of a text through different composition methods. An intuitive method for composition is arithmetic mean, that is, sentiment intensities of a text t can be predicted by average sentiment intensities of each words w in this text, defined as 1</p><formula xml:id="formula_1">t w wt senti senti n    (1)</formula><p>where senti t and senti w respectively denote the sentiment intensity of text t and word w.</p><p>Instead of simply using the arithmetic mean affective values of words, Paltoglou et al. <ref type="bibr" target="#b17">[18]</ref> used three different methods to estimate the sentence's overall sentiment score, including weighted arithmetic mean, weighted geometric mean, and a Gaussian mixture model. These authors' experimental results show that weighted geometric mean method outperforms other two methods. Although the lexicon-base methods can be easily applied, they cannot model a sentence or document with complex linguistic expression. For example, if a positive review contains more negative words than positive words, it will be incorrectly predicted as a negative one. This finding means that the emotional import of a sentence or document is not simply the sum of emotional associations of its component words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Regression-Based Methods</head><p>Gokcay et al., <ref type="bibr" target="#b18">[19]</ref> applied a simple linear regression model on a sentiment lexicon to calculate the overall sentiment score of texts. The sentences are decomposed into their words to obtain sentiment intensities from an affective lexicon. A list of stop-words is used to remove words that are not found in lexicon. Next, a regression model is built between the sentiment intensity of the sentence and the average intensities of words in the sentence.</p><p>Instead of simply using the mean affective values of words, Malandrakis et al., <ref type="bibr" target="#b19">[20]</ref> extract n-grams, the weighted average and maximum intensities of component words, as features to train regression models. The authors also proposed a method that extracts n-grams with affective ratings as features to predict sentiment intensities for sentences and documents.</p><p>Paltoglou and Thelwall <ref type="bibr" target="#b16">[17]</ref> predicted the intensities of a sentence or document on an ordinal five-level scale, from very negative/low to very positive/high, respectively. These authors considered the sentiment prediction problem as both classification and regression. Both methods are based on BOW features, and support vector machine (SVM) and ε-support vector regression (ε-SVR) are used for classification and regression, respectively. Their experimental results also show that regression techniques tend to make smaller scale errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Neural Network-Based Methods</head><p>Recently, word embeddings have been shown to boost the performance in several NLP tasks <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, including semantic parsing and sentiment analysis. Given a variable-length text, one challenge of using a learning algorithm for sentiment analysis is to identify a method to take individual word vectors and transform them into a feature vector that is the same length for every text. One intuitive method is to simple average is simply to average the word vectors in a given text <ref type="bibr" target="#b42">[43]</ref>- <ref type="bibr" target="#b44">[45]</ref>. Although such a method is easy to implement and provides efficient computation, it sacrifices word order information, making it very similar to the concept of BOW.</p><p>Based on word embedding, several deep neural networks were proposed for positive-negative classification, such as CNN <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, RNN <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> and LSTM <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. A CNN model consists of the convolution and pooling layers and provides a standard architecture that maps variable-length sentences or texts into fixed-size distributed vectors. the CNN model takes as input the sequence of word embedding, summarizes the sentence meaning by convolving the slide window and the sentence, and outputs the fixed-length distributed vector with other layers, such as dropout and fully connected layers, where the activation can be sigmoid, tanh or ReLU.</p><p>RNN and LSTM were theoretically more powerful in language modeling due to their capability of representing a sentence or text with sequence order information, rather than a fixed length, as in CNN. These two models both learn the contextual information that could be beneficial to capture the semantics of long texts. However, due to the vanishing and the exploding gradient problems, the traditional RNN is difficult to train. To address these problems, LSTM introduces a gating structure that allows for explicit memory updates and delivers. Thus, an LSTM has three of these gates, to protect and control the state of a memory cell to learn the long-distance dependency of a given sentence or text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Stacked Neural Network Methods</head><p>Deep learning is built based on a theoretical hypothesis that a deep, hierarchical model can be exponentially more efficient at representing some functions than a swallow model <ref type="bibr" target="#b45">[46]</ref>. According to recent progress in computer vision <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref>, developing deep architecture are able to learn hierarchical representations of whole sentence in NLP tasks. Several works seek to use such deep architectures of stacking many convolutional or recurrent layers to approach this goal. </p><formula xml:id="formula_2">(b) (c) (a) i t c t f t h t o t c t-1 (x t , h t-1 ) c_int LSTM ...      n Input Word Word Vector x 1 LSTM x 2 LSTM x 3 LSTM x 4 LSTM Repeating Module ... ... h 1 h 2 h 3 h 4</formula><p>1 ĥ 2 ĥ 3 ĥ 4 ĥ Fig. <ref type="figure">2</ref>: Illustration example of LSTM cells with residual connection.</p><p>Zhang et al. <ref type="bibr" target="#b46">[47]</ref> perform a character-level convolutional neural network for sentiment analysis task. Their model use up to six convolutional layers, followed by a dense-connected softmax layer for classification. Other similar stacked CNN that use character-level information was proposed by Santos et al. <ref type="bibr" target="#b47">[48]</ref>. Conneau et al. <ref type="bibr" target="#b31">[32]</ref> proposed to stack 29 convolutional layers to form a very "deep" convolutional networks for text classification tasks. "Benefit of depth" was shown for those stack convolutional neural networks in NLP tasks.</p><p>An RNN or LSTM can be also extended deeper by stacking multiple recurrent layers on top of each other <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>. As same as stacked CNN model that extracts different level features, the stacked RNN and LSTM can also extract higher level features from lower-level features to form a hierarchical representation. However, we are not aware of any work that use more than six LSTM layers for sentiment classification or intensity prediction. Deeper networks were reported that they cannot improve performance or even not tried. The deeper when the network goes, the more difficult it is to optimize. Since the stacked model suffered from the degradation problem. With residual connections, we were able to show that performance improves with increased depth in stacked LSTM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Stacked Residual LSTM Model</head><p>This section presents the architecture of the proposed stacked residual LSTM. We first introduce a residual connection into LSTM layers with to form a building block. Next, by stacking LSTM building blocks, we turn the deep stacked LSTM model into its residual version. For outputting continuous affective ratings instead of discrete categories, we adopt a linear decoder in the output layer. The details of the proposed stacked residual LSTM are described in what follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Residual Connection</head><p>In most NLP tasks, LSTM is more powerful than RNN due to the problem of vanishing or exploding gradients in RNN <ref type="bibr" target="#b48">[49]</ref>. The LSTM introduces a new structure called a memory cell, as shown in Fig. <ref type="figure">2(a)</ref>. As in the RNN, the LSTM model is defined at each time step t to be a collection of vectors in </p><formula xml:id="formula_3">A C C E P T E D M A N U S C R I P T 1 1 1 ( ) ( ) ( ) t xi t hi t i t xf t hf t f t xo t ho t i i W x W h b f W x W h b o W x W h b               <label>(2)</label></formula><formula xml:id="formula_4"> Input transform c 1 _ _ tanh( ) t x t hc t c in c in W x W h b    <label>(3)</label></formula><p> Memory update 1 _ tanh( )</p><formula xml:id="formula_5">t t t t t t t t c f c i c in h o c       <label>(4)</label></formula><p>where x t is the input word vector at the time step t, σ denotes the logistic sigmoid function, W and b respectively present the weights and bias and  denotes element-wise multiplication. Intuitively, the forget gate controls the extent to which the previous memory cell is forgotten, the input gate controls how much each unit is updated, and the output gate controls the exposure of the internal memory state. Pascanu et al. <ref type="bibr" target="#b49">[50]</ref> explored multiple ways of combining one LSTM layer with another and discussed various difficulties in training such deep LSTM models. Grave <ref type="bibr" target="#b33">[34]</ref> also investigate the stacked LSTMs for text generation. In our model, we use vertical stack architecture where the hidden representation of the previous layer h (l)  t is used as the input for the next layer, where l denotes the layer. Thus, a hidden state of time step t in layer h (l)  t can be calculated as follows:</p><formula xml:id="formula_6">( ) ( ) 1 ( )<label>( 1) ( ) 1 ( , ) 1 ( , ) 1</label></formula><formula xml:id="formula_7">l l t t t l l l t t t h LSTM x h if l h LSTM h h if l       <label>(5)</label></formula><p>where x t denotes the input word vector in step t.</p><p>When multiple layers in a neural network are stacked, the network becomes very hard to train, leading to a degradation problem. As described previously, the problem arises due to the low convergence rate of the training error, instead of the vanishing or exploding gradient problem. Inspired by shortcut connections in ResNet <ref type="bibr" target="#b29">[30]</ref>, we employ residual connections to address the problem. Fig. <ref type="figure">2</ref>(b) shows an example of a residual LSTM building block. We introduce residual connections to add the hidden state h (l) t with the input vector x t of n layer. Thus, the dimension of each hidden state h (l)   t is required to match the dimension of the input vector x t . By adding residues for learning, the hidden state ĥ can be denoted as</p><formula xml:id="formula_8">( )<label>( 1) ( ) ( ) 1 ˆ</label></formula><formula xml:id="formula_9">( , ) l l l l n t t t t h LSTM h h x     <label>(6)</label></formula><p>where ĥ in layer l is updated with residual value () ln t x  , and  denotes matrix addition. As shown in Fig. <ref type="figure">2(c)</ref>, n denotes the number of intermediate LSTM layers in a residual connection. Intuitively, if n is very large, the stacked model is very expensive in terms of computations and hard to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stacked LSTM Architecture</head><p>In this paper, we propose an eight-layer stacked residual LSTM model for sentiment intensity prediction. Based on a vertical stacked LSTM model, we insert residual connections, as shown in Fig. <ref type="figure">3</ref>, which turn the network into its residual version. As noted previously, the output hidden state can be directly added by the input vector only when the input and output are of the same dimensions. By introducing residual connections, we proposed two type models, i.e., Type 1 (n=1) and Type 2 (n=2). That is, we insert the residual connection after every one or two stacked layers of LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>It is worth noting that the matrix addition of each residual connection does not add any parameters that need to be learned. Thus, this does not increase the complexity of the proposed model. To improve the performance of the proposed model, we also perform a bi-directional strategy in each LSTM layer <ref type="bibr" target="#b33">[34]</ref>. At each time step, the hidden state of the bidirectional LSTM is the concatenation of the forward and backward hidden states to capture both past and future information.</p><p>LSTM is a biased model, as is RNN. The words in the tail of a sentence are more dominant than the words in the header. Thus, LSTM could reduce the prediction performance when it is used to capture the sentiment intensity of a whole text, since the key components could appear in any parts of the text, rather than at the end. Therefore, we use a mean pooling method to learn text vectors and to make each word contribute equally to the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Linear Decoder</head><p>Since the intensity values are continuous values, the prediction task requires a regression model, as noted previously. Instead of using a softmax or sigmoid classifier, a linear activation function, also known as linear decoder, is used in the output layer, defined as</p><formula xml:id="formula_10">t o o o senti W b   x<label>(7)</label></formula><p>where x o represents the feature maps learned from the previous layer, Senti t is the intensity of the target texts, and W o and b o respectively denote the weight and bias associated with the output layer.</p><p>The proposed models are trained by minimizing the mean squared error between the actual and predicted values. Given a training set of text matrix X={x (1) , x (2) ,..., x (m) }, and their sentiment intensities set S={senti (1) , senti (2) , …, senti (m) }, the cost function is defined as follows:</p><formula xml:id="formula_11">2 ( ) ( ) 1 1 ( , ) ( ) 2 m i i i J h senti m     X S x<label>(8)</label></formula><p>where h represents the hypothesis of the proposed stacked residual LSTM model. The model training is carried out by the back propagation algorithm (BP) <ref type="bibr" target="#b50">[51]</ref> using the Adadelta optimizer <ref type="bibr" target="#b51">[52]</ref>. The learning rate is 0.5. To avoid overfitting, we added a dropout layer <ref type="bibr" target="#b52">[53]</ref> after each residual connection to make use of its regularization effect. All parameters are randomly initialized with a uniform distribution in (-0.01, 0.01) and updated by each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we first investigate the degradation problems in stacked architecture when the model was deepened. Then, we present the experimental results of the proposed stacked residual LSTM for sentiment intensity prediction against existing lexicon-, regression-, and conventional deep NN-based methods. We also investigate the training per  <ref type="bibr" target="#b57">[58]</ref> 23,503 English EmoBank <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref> 10,548 English Chinese Valence-Arousal Texts (CVAT) <ref type="bibr" target="#b53">[54]</ref> 2,100 Chinese</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>This experiment used four corpora in both English and Chinese, as listed in Table <ref type="table" target="#tab_1">1</ref>. The VADER dataset contains social texts in four different domains, including social media texts (4,000 samples), digital product reviews (3,708 samples), opinion news articles <ref type="bibr">(5,190 samples)</ref> and movie reviews (10,605 samples), for a total of 23,503 samples.</p><p>Each sentence in the SST and VADER corpora was manually assigned a sentiment score between 0 and 1. In addition, the other two corpora were rated with valence and arousal values. The valence represents the degree of pleasant and unpleasant (i.e., positive and negative) feelings; therefore, it can be considered as sentiment intensity. Each sentence in EmoBank has two different intensities assigned by different entities, i.e., reader and writer. In this experiment, we rescale the range of valence values into [0, 1].</p><p>For SST, learning models are trained on training sets to predict sentiment intensity for testing samples, and development sets are used to fine-tune the model. For other three corpora, we performed 5-fold crossvalidation (i.e., 80% of these texts were used as training samples, and 20% for testing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>To compare the results of the proposed stacked residual LSTM model against regression-based methods and other deep neural networks, we introduce the Pearson correlation coefficient (r) and mean absolute error (MAE) to evaluate the performance of each models in different four datasets.</p><p>The MAE results reflect the difference between the predicted values of sentiment intensities and the corresponding manually rated actual values in four corpora. The Pearson correlation coefficient is a measure of the linear correlation between the actual value and the predicted value, giving a value between +1 and -1 inclusive, where +1 is total positive correlation, 0 is no correlation, and -1 is total negative correlation. A higher Pearson correlation coefficient and a lower MAE value indicate more accurate forecasting performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>The proposed stacked residual LSTM model is compared with several existing methods for sentiment intensity prediction. The implementation details for each method are as follows.</p><p> wAM and wGM: weighted arithmetic mean (wAM) and weighted geometric mean (wGM) are two lexicon-based methods <ref type="bibr" target="#b17">[18]</ref>. In these methods, sentiment intensity of a candidate text can be estimated via the weighted mean of tokens in the text. <ref type="bibr" target="#b19">[20]</ref>, which extract the weighted average and maximum intensities of component words as features to train regression models. The sentiment intensities of English words used in above methods are taken from the extended ANEW <ref type="bibr" target="#b10">[11]</ref>, while sentiment intensities of Chinese words are taken from CVAW <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head></head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AVR and MVR: average values regression (AVR) and maximum values regression (MVR) are two regression-based methods</head><p> CNN, RNN and LSTM: Three other word embedding-based methods are also introduced for comparison: convolutional neural networks (CNN) <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, recurrent neural networks (RNN) <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> and long-short term memory (LSTM) <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. CNN can extract active local n-gram features while RNN and LSTM can sequentially model the texts. To enhance performance of LSTM layers, we also introduce a bi-directional strategy <ref type="bibr" target="#b33">[34]</ref>. The word embeddings used in this experiment was respectively pre-trained on Common Crawl 840B ‡ (English) and wiki dumps § (Chinese) by GloVe <ref type="bibr" target="#b54">[55]</ref>. For neural models, we also implement dropouts <ref type="bibr" target="#b52">[53]</ref> as regularization to prevent overfitting problem. The dropout rate was set to 0.25. The dimension of input word embeddings and the hidden state is 300. All word embedding-based methods are implemented using ‡ https://nlp.stanford.edu/projects/glove/ § https://dumps.wikimedia.org/ Fig. <ref type="figure">4</ref>: Training on SST, CVAT and EmoBank with plain and residual architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Gensim ** <ref type="bibr" target="#b55">[56]</ref>, TensorFlow † † <ref type="bibr" target="#b56">[57]</ref>, and Keras ‡ ‡ toolkits. These models were implemented with default parameter settings, as summarized in Table <ref type="table" target="#tab_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation of the Residual Connections.</head><p>We first evaluate 2-layer and 8-layer plain networks on different corpora. The results in Fig. <ref type="figure" target="#fig_0">1</ref> and Fig. <ref type="figure">4</ref> show that the deeper 8-layer stacked LSTM without residual connections has higher testing MAE error and lower correlation r than a shallower LSTM network with 2 layers. We observe the degradation problem that the deep plain models suffered from increased depth and exhibit higher training MAE error and lower correlation r when the architecture becomes deeper.</p><p>The phenomenon was unlikely to be due to the vanishing or explosion of gradients problem, since the gate mechanism and memory cell in LSTM model were designed to address this problem. In addition, it was also unlikely to be caused by overfitting problems, as the stacked models were regularized by dropouts.</p><p>After introducing residual connections, the performance of deeper 8-layer models (Type 2) were reversed to exceed the 2-layer swallow networks, and exhibits considerably lower training MAE error (8%-10%) and higher correlation r (8%-9%). Compared to 8-layer plain counterpart models, the residual connection also helps the models reduce MAE error by 15% and increase correlation r by 11%. The proposed models can provide better model convergence to obtain better optimization capability.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparative Results of Sentiment Intensity Prediction</head><p>Table <ref type="table" target="#tab_3">2</ref> and Table <ref type="table" target="#tab_4">3</ref> respectively present the result of the stacked residual LSTM model compared against several methods, as applied to sentiment intensity prediction in both English and Chinese corpora. For lexicon-based methods, wGM outperformed wAM, which is consistent with the results reported in Paltoglou et al. <ref type="bibr" target="#b17">[18]</ref>. Instead of directly using sentiment intensities of words to measure those of texts, the regressionbased methods learned the correlations between the sentiment intensities of words and texts, thereby yielding better performance.</p><p>Once the word embedding and deep learning techniques were introduced, the performance of NN-based methods (except RNN) increased dramatically. RNN suffered from the problems of vanishing and explosion gradients. In addition, the proposed stacked residual LSTM models outperformed the other NN-based methods, indicating the effectiveness of residual connections. By overcoming degradation problems, the residual connection can increase the performance when the architecture of stacked models gets deeper.</p><p>Another observation in Table <ref type="table" target="#tab_3">2</ref> and Table <ref type="table" target="#tab_4">3</ref> is that the Type 2 (n=2) stacked residual model also outperforms Type 1 (n=1) model. When n=1, the function in Eq. ( <ref type="formula" target="#formula_9">6</ref>) can be transformed to a standard stacked LSTM model with a bias derived from the input vector x. Therefore, the performances of Type 1 (n=1) models are very similar to those of plain stacked models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Training Time Consume Analysis</head><p>For some time-sensitive processing tasks, the time consume for training and prediction is another critical metric to determine the whole performance of the model. Although the proposed model achieved better performance than the existing models for sentiment intensity prediction, stacking more layers will result in more training time consuming. Since the more LSTM layers were introduced to the model, the more parameters were need to be trained. In Table <ref type="table" target="#tab_5">4</ref>, we compared the sizes of several trained models, including bi-  We provide the number of hidden units as well as the total number of trainable parameters on SST, EmoBank and CVAT. As shown in Table <ref type="table" target="#tab_5">4</ref>, the addition of residual connections to original stacked LSTM model does not add any trainable parameters. Thus, it does not increase the complexity of the model. The increase in training complexity depends only on the length of sentences (#units) and the number of stacked layers (#parameters).</p><p>For more detailed analysis, we trained these five models on an 8-core CPU and a GTX 1080 GPU with CuDNN 7, respectively. As shown in Table <ref type="table" target="#tab_6">5</ref>, when the model goes deeper, the training time will increase linearly on both CPU and GPU. Compared with 8-layer plain Bi-LSTM model, the proposed stacked residual model with same number of layers did not take much more time for training. When GPU is used, the training efficiency will be 20x~30x higher than that of CPU. Therefore, the efficiency of training and prediction of the proposed model on a GPU platform still remains highly competent for those time-sensitive tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we presented a stacked residual LSTM model to predict sentiment intensities of texts. By introducing residual connections to every few LSTM layers, we constructed an eight-layer stacked LSTM model. The residual connections let the deep stacked model avoid degradation problems. In addition, residual connection does not increase the complexity of the model. Since it does not add any trainable parameters to the stacked model.</p><p>In these experiments, we also provide comprehensive empirical evidence that these residual networks are easier to optimize. Comparative results show that this model achieves a lower MAE and a higher Pearson correlation coefficient in sentiment intensity prediction than existing methods, indicating improved accuracy. With a GPU support, the efficiency of training and prediction of the proposed model is still highly competitive for those time-sensitive language processing tasks.</p><p>Future work will attempt to introduce the attention or memory mechanism in order to extract more useful information between layers and to improve the performance of the residual architecture.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Mean absolute error (MAE) and Pearson correlation coefficient (r) on SST corpus with 1-layer and 8-layer plain LSTM model. The deeper network has higher training error and thus testing error.</figDesc><graphic coords="4,37.71,85.80,235.02,175.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>LSTM</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>A</head><label></label><figDesc>C C E P T E D M A N U S C R I P T 10 Jin Wang et al., Using a Stacked Residual LSTM Model for Sentiment Intensity Prediction  Stacked Residual LSTM (SR-LSTM): The proposed 8-layer stacked residual LSTM model are also implemented for comparison. By introducing residual connections, we compared two types of stacked models, i.e., Type 1 (n=1) and Type 2 (n=2). That is, we add the residual connection after every one or two stacked layers of LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>S C R I P T directional LSTM (Bi-LSTM), 2-layer Bi-LSTM, 8-layer Bi-LSTM and the proposed 8-layer Bi-LSTM model with residual connections (SR-Bi-LSTM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>d  : an input gate i t , a forget gate f t , an output gate o t , a memory cell c t and a history representation h t . The entries of the gating vectors i t , f t and o t are in the range [0, 1]. The LSTM transition equations are the following:</figDesc><table><row><cell>t</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mean-</cell><cell>Linear</cell></row><row><cell>LSTM</cell><cell>LSTM</cell><cell>LSTM</cell><cell>LSTM</cell><cell>LSTM</cell><cell>LSTM</cell><cell>pooling</cell><cell>decoder</cell></row><row><cell>Type 1 (n=1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Intensity</cell></row><row><cell>t</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>x</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Mean-</cell><cell>Linear</cell></row><row><cell>LSTM</cell><cell>LSTM</cell><cell>LSTM</cell><cell>LSTM</cell><cell>LSTM</cell><cell>LSTM</cell><cell>pooling</cell><cell>decoder</cell></row><row><cell>Type 2 (n=2)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Intensity</cell></row><row><cell cols="8">Fig. 3: Example network architectures of eight layer stacked LSTM model for sentiment intensity prediction</cell></row><row><cell> Gates</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Sentiment corpora used in this experiment.</figDesc><table><row><cell>Corpus</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell><cell>Language</cell></row><row><cell>Stanford Sentiment Treebank (SST) [12]</cell><cell>8,544</cell><cell>1,101</cell><cell>2,210</cell><cell>English</cell></row><row><cell>VADER</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Comparative results of different methods on SST, EmoBank and CVAT corpus</figDesc><table><row><cell>Methods</cell><cell>MAE</cell><cell>SST</cell><cell>r</cell><cell cols="2">EmoBank (reader) MAE r</cell><cell cols="2">EmoBank (writer) MAE r</cell><cell cols="2">CVAT MAE</cell><cell>r</cell></row><row><cell>Lexicon-wAM</cell><cell>0.202</cell><cell></cell><cell>0.350</cell><cell>0.148</cell><cell>0.365</cell><cell>0.145</cell><cell>0.372</cell><cell>0.204</cell><cell>0.406</cell></row><row><cell>Lexicon-wGM</cell><cell>0.198</cell><cell></cell><cell>0.385</cell><cell>0.132</cell><cell>0.398</cell><cell>0.136</cell><cell>0.383</cell><cell>0.199</cell><cell>0.418</cell></row><row><cell>Regression-MAR</cell><cell>0.182</cell><cell></cell><cell>0.455</cell><cell>0.112</cell><cell>0.433</cell><cell>0.118</cell><cell>0.429</cell><cell>0.171</cell><cell>0.476</cell></row><row><cell>Regression-MVR</cell><cell>0.179</cell><cell></cell><cell>0.448</cell><cell>0.107</cell><cell>0.426</cell><cell>0.112</cell><cell>0.432</cell><cell>0.174</cell><cell>0.468</cell></row><row><cell>CNN</cell><cell>0.153</cell><cell></cell><cell>0.679</cell><cell>0.072</cell><cell>0.510</cell><cell>0.057</cell><cell>0.504</cell><cell>0.143</cell><cell>0.625</cell></row><row><cell>RNN</cell><cell>0.160</cell><cell></cell><cell>0.659</cell><cell>0.082</cell><cell>0.445</cell><cell>0.068</cell><cell>0.439</cell><cell>0.165</cell><cell>0.493</cell></row><row><cell>LSTM</cell><cell>0.149</cell><cell></cell><cell>0.700</cell><cell>0.075</cell><cell>0.511</cell><cell>0.055</cell><cell>0.524</cell><cell>0.132</cell><cell>0.641</cell></row><row><cell>Bi-LSTM</cell><cell>0.148</cell><cell></cell><cell>0.715</cell><cell>0.070</cell><cell>0.531</cell><cell>0.055</cell><cell>0.529</cell><cell>0.125</cell><cell>0.652</cell></row><row><cell>2-layer LSTM</cell><cell>0.143</cell><cell></cell><cell>0.709</cell><cell>0.068</cell><cell>0.549</cell><cell>0.052</cell><cell>0.541</cell><cell>0.112</cell><cell>0.684</cell></row><row><cell>2-layer Bi-LSTM</cell><cell>0.139</cell><cell></cell><cell>0.718</cell><cell>0.067</cell><cell>0.556</cell><cell>0.050</cell><cell>0.548</cell><cell>0.108</cell><cell>0.690</cell></row><row><cell>8-layer LSTM</cell><cell>0.153</cell><cell></cell><cell>0.688</cell><cell>0.078</cell><cell>0.484</cell><cell>0.061</cell><cell>0.491</cell><cell>0.134</cell><cell>0.632</cell></row><row><cell>8-layer Bi-LSTM</cell><cell>0.151</cell><cell></cell><cell>0.692</cell><cell>0.077</cell><cell>0.490</cell><cell>0.061</cell><cell>0.498</cell><cell>0.130</cell><cell>0.638</cell></row><row><cell>8-layer SR-LSTM (Type 1)</cell><cell>0.148</cell><cell></cell><cell>0.692</cell><cell>0.076</cell><cell>0.490</cell><cell>0.061</cell><cell>0.490</cell><cell>0.098</cell><cell>0.740</cell></row><row><cell>8-layer SR-LSTM (Type 2)</cell><cell>0.131</cell><cell></cell><cell>0.764</cell><cell>0.058</cell><cell>0.615</cell><cell>0.046</cell><cell>0.592</cell><cell>0.161</cell><cell>0.611</cell></row><row><cell>8-layer SR-Bi-LSTM (Type 1)</cell><cell>0.146</cell><cell></cell><cell>0.696</cell><cell>0.071</cell><cell>0.512</cell><cell>0.060</cell><cell>0.492</cell><cell>0.096</cell><cell>0.742</cell></row><row><cell>8-layer SR-Bi-LSTM (Type 2)</cell><cell>0.128</cell><cell></cell><cell>0.772</cell><cell>0.058</cell><cell>0.612</cell><cell>0.047</cell><cell>0.596</cell><cell>0.162</cell><cell>0.610</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Comparative results of different methods on VADER corpus</figDesc><table><row><cell>Methods</cell><cell cols="2">Social MAE</cell><cell>r</cell><cell cols="2">Movie MAE</cell><cell>r</cell><cell cols="2">Product MAE</cell><cell>r</cell><cell>Article MAE</cell><cell>r</cell></row><row><cell>Lexicon-wAM</cell><cell>0.225</cell><cell cols="2">0.486</cell><cell>0.183</cell><cell cols="2">0.487</cell><cell>0.175</cell><cell cols="2">0.421</cell><cell>0.169</cell><cell>0.423</cell></row><row><cell>Lexicon-wGM</cell><cell>0.224</cell><cell cols="2">0.489</cell><cell>0.182</cell><cell cols="2">0.490</cell><cell>0.173</cell><cell cols="2">0.442</cell><cell>0.168</cell><cell>0.431</cell></row><row><cell>Regression-MAR</cell><cell>0.202</cell><cell cols="2">0.568</cell><cell>0.175</cell><cell cols="2">0.526</cell><cell>0.169</cell><cell cols="2">0.482</cell><cell>0.156</cell><cell>0.472</cell></row><row><cell>Regression-MVR</cell><cell>0.201</cell><cell cols="2">0.571</cell><cell>0.174</cell><cell cols="2">0.534</cell><cell>0.165</cell><cell cols="2">0.486</cell><cell>0.155</cell><cell>0.477</cell></row><row><cell>CNN</cell><cell>0.141</cell><cell cols="2">0.803</cell><cell>0.134</cell><cell cols="2">0.737</cell><cell>0.121</cell><cell cols="2">0.679</cell><cell>0.088</cell><cell>0.711</cell></row><row><cell>RNN</cell><cell>0.199</cell><cell cols="2">0.607</cell><cell>0.169</cell><cell cols="2">0.571</cell><cell>0.139</cell><cell cols="2">0.514</cell><cell>0.128</cell><cell>0.496</cell></row><row><cell>LSTM</cell><cell>0.134</cell><cell cols="2">0.818</cell><cell>0.124</cell><cell cols="2">0.744</cell><cell>0.124</cell><cell cols="2">0.673</cell><cell>0.082</cell><cell>0.724</cell></row><row><cell>Bi-LSTM</cell><cell>0.129</cell><cell cols="2">0.822</cell><cell>0.122</cell><cell cols="2">0.755</cell><cell>0.118</cell><cell cols="2">0.686</cell><cell>0.074</cell><cell>0.741</cell></row><row><cell>2-layer LSTM</cell><cell>0.124</cell><cell cols="2">0.821</cell><cell>0.118</cell><cell cols="2">0.762</cell><cell>0.112</cell><cell cols="2">0.692</cell><cell>0.068</cell><cell>0.749</cell></row><row><cell>2-layer Bi-LSTM</cell><cell>0.121</cell><cell cols="2">0.829</cell><cell>0.112</cell><cell cols="2">0.768</cell><cell>0.108</cell><cell cols="2">0.697</cell><cell>0.065</cell><cell>0.752</cell></row><row><cell>8-layer LSTM</cell><cell>0.146</cell><cell cols="2">0.776</cell><cell>0.133</cell><cell cols="2">0.738</cell><cell>0.128</cell><cell cols="2">0.672</cell><cell>0.096</cell><cell>0.702</cell></row><row><cell>8-layer Bi-LSTM</cell><cell>0.144</cell><cell cols="2">0.778</cell><cell>0.132</cell><cell cols="2">0.732</cell><cell>0.129</cell><cell cols="2">0.670</cell><cell>0.092</cell><cell>0.710</cell></row><row><cell>8-layer SR-LSTM (Type 1)</cell><cell>0.138</cell><cell cols="2">0.805</cell><cell>0.128</cell><cell cols="2">0.736</cell><cell>0.126</cell><cell cols="2">0.679</cell><cell>0.055</cell><cell>0.782</cell></row><row><cell>8-layer SR-LSTM (Type 2)</cell><cell>0.111</cell><cell cols="2">0.882</cell><cell>0.101</cell><cell cols="2">0.779</cell><cell>0.098</cell><cell cols="2">0.712</cell><cell>0.078</cell><cell>0.732</cell></row><row><cell>8-layer SR-Bi-LSTM (Type 1)</cell><cell>0.130</cell><cell cols="2">0.812</cell><cell>0.130</cell><cell cols="2">0.734</cell><cell>0.122</cell><cell cols="2">0.680</cell><cell>0.052</cell><cell>0.788</cell></row><row><cell>8-layer SR-Bi-LSTM (Type 2)</cell><cell>0.108</cell><cell cols="2">0.886</cell><cell>0.098</cell><cell cols="2">0.785</cell><cell>0.096</cell><cell cols="2">0.718</cell><cell>0.077</cell><cell>0.738</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>The sizes of different depth LSTM models.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell># units</cell><cell></cell></row><row><cell>Methods</cell><cell>#parameters</cell><cell>SST</cell><cell cols="2">EmoBank CVAT</cell></row><row><cell>Bi-LSTM</cell><cell>541K</cell><cell>56</cell><cell>120</cell><cell>247</cell></row><row><cell>2-layer Bi-LSTM</cell><cell>1.08M</cell><cell>112</cell><cell>240</cell><cell>494</cell></row><row><cell>8-layer Bi-LSTM</cell><cell>3.25M</cell><cell>448</cell><cell>960</cell><cell>1,976</cell></row><row><cell>8-layer SR-Bi-LSTM (Type 1)</cell><cell>3.25M</cell><cell>448</cell><cell>960</cell><cell>1,976</cell></row><row><cell>8-layer SR-Bi-LSTM (Type 2)</cell><cell>3.25M</cell><cell>448</cell><cell>960</cell><cell>1,976</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>The training time of different depth LSTM models.</figDesc><table><row><cell>Methods</cell><cell>SST Epoch (avg ± std) CPU GPU</cell><cell>EmoBank Epoch (avg ± std) CPU GPU</cell><cell>CVAT Epoch (avg ± std) CPU GPU</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>† http://nlp.stanford.edu/sentiment/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Jin Wang et al., Using a Stacked Residual LSTM Model for Sentiment Intensity Prediction</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Natural Science Foundation of China (NSFC) under Grants No.61702443 and No.61762091, and in part by Educational Commission of Yunnan Province of China under Grant No.2017ZZX030. The authors would like to thank the anonymous reviewers for their constructive comments.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bi-LSTM 44s±2.8s 3.32s±0.12s 93s±5.5s 5.54s±0.28s 112s±3.2s 3.20s±0.11s 2-layer Bi-LSTM 90s±4.3s 5.56s±0.15s 186s±7.6s 11.1s±0.62s 226s±3.8s 8.50s±0.14s 8-layer Bi-LSTM 290s±10.8s 14.2s±0.44s 691s±22.4s 26.3s±1.02s 843s±12.4s 24.2s±0.82s 8-layer SR-Bi-LSTM (Type 1) 278s±12.4s 13.8s±0.48s 682±23.2s 25.8s±1.18s 839s±10.9s 23.8s±0.77s 8-layer SR-Bi-LSTM (Type 2) 282s±13.2s 13.2s±0.51s 685s±22.8s 25.2s±1.06s 844s±10.6s 24.4s±0.80s</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends® Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="231" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sentiment Analysis and Opinion Mining</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synth. Lect. Hum. Lang. Technol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="167" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Techniques and applications for sentiment analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">82</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sentiment analysis: Detecting valence, emotions, and other affectual states from text</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emot. Meas</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Affect detection: An interdisciplinary review of models, methods, and their applications</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Member</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Society</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="37" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Affective Computing</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Thumbs up ? Sentiment Classification using Machine Learning Techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL-2012</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics (ACL-2012</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A circumplex model of affect</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Pers. Soc. Psychol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1161" to="1178" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Norms of valence, arousal, and dominance for 13,915 English lemmas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kuperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1191" to="1207" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empir. Methods Nat. Lang. Process</title>
		<meeting>Conf. Empir. Methods Nat. Lang. ess</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using text mining and sentiment analysis for online forums hotspot detection and forecast</title>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decis. Support Syst</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="354" to="368" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Affective and Content Analysis of Online Depression Communities</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="226" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledge-Based Systems Using a contextual entropy model to expand emotion words and their intensity for the sentiment classification of stock market news</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Syst</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="89" to="97" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning the meaning of scalar adjectives</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Marne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10)</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
	<note>Was it good? It was provocative</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Seeing stars of valence and arousal in blog posts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Paltoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="123" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predicting emotional responses to long informal text</title>
		<author>
			<persName><forename type="first">G</forename><surname>Paltoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Theunis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affect. Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="115" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Predicting the sentiment in sentences Based on words: An exploratory study on ANEW and ANET</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gökçay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Işbilir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yildirim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd IEEE International Conference on Cognitive Infocommunications</title>
		<meeting>the 3rd IEEE International Conference on Cognitive Infocommunications</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="715" to="718" />
		</imprint>
	</monogr>
	<note>CogInfoCom-2012</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributional semantic models for affective text analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Malandrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Iosif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2379" to="2392" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-2014)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL-2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="655" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Supervised sequence labelling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Opinion mining with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="720" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long shortterm memory networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-2014</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-2014</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting polarities of tweets by composing word embeddings with long short-term memory</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-2015</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1343" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representation (ICLR-15</title>
		<meeting>International Conference on Learning Representation (ICLR-15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-2016</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno>arXiv Prepr. arXiv1602.07261</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-2016</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">How to construct deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv Prepr. arXiv1312.6026</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional LSTM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU-2013)</title>
		<meeting>the 2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU-2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Short-term residential load forecasting based on LSTM recurrent neural network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE Trans. Smart Grid</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hourly day-ahead solar irradiance prediction using weather forecasts by LSTM</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="461" to="468" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generating highly accurate predictions for missing QoS data via aggregating nonnegative latent factor models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Ammari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alabdulwahab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="524" to="537" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An effective scheme for QoS estimation via alternating direction method-based matrix factorization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Serv. Comput</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition using spatiotemporal LSTM network with trust gates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Kot</forename><surname>Chichung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">XX</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond frame-level CNN: Saliency-aware 3-D CNN with LSTM for video action recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="510" to="514" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Advances in Neural Information Processing Systems (NIPS-2013</title>
		<meeting>the Annual Conference on Advances in Neural Information Processing Systems (NIPS-2013</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR-2013)</title>
		<meeting>the International Conference on Learning Representations (ICLR-2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep unordered composition rivals syntactic methods for text classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-2015</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics (ACL-2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>arXiv Prepr. arXiv1607.01759</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>arXiv Prepr. arXiv1607.04606</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deep architectures for AI</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends® Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="127" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems (NIPS-2015)</title>
		<meeting>Advances in Neural Information Processing Systems (NIPS-2015)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3057" to="3061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for sentiment analysis of short texts</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Computational Linguistics (COLING-2014)</title>
		<meeting>the 25th International Conference on Computational Linguistics (COLING-2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning long-rerm dependencies with gradient decent is difficult</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">How to Construct Deep Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv Prepr. arXiv1312.6026</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Efficient backprop</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Muller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="9" to="48" />
		</imprint>
	</monogr>
	<note>Neural networks: Tricks of the trade</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Adadelta: An adaptive learning rate method</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno>arXiv Prepr. arXiv1212.5701</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dropout : A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Building Chinese affective resources in valence-arousal dimensions</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL/HLT-2016</title>
		<meeting>the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL/HLT-2016</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="540" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Software Framework for Topic Modelling with Large Corpora</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rehurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks</title>
		<meeting>the LREC 2010 Workshop on New Challenges for NLP Frameworks</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="45" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Osdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
	<note>OSDI-2016</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">VADER: A parsimonious rule-based model for sentiment analysis of social media text</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hutto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International AAAI Conference on Weblogs and Social Media</title>
		<meeting>the 8th International AAAI Conference on Weblogs and Social Media</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
	<note>ICWSM-2014</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">EmoBank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Buechel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="578" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Readers vs. writers vs. texts : Coping with different perspectives of text understanding in emotion annotation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Buechel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Linguistic Annotation Workshop in EACL 2017</title>
		<meeting>the 11th Linguistic Annotation Workshop in EACL 2017</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
