<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Selection for Content-Based, Time-Varying Musical Emotion Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Erik</forename><forename type="middle">M</forename><surname>Schmidt</surname></persName>
							<email>eschmidt@drexel.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Swarthmore College</orgName>
								<address>
									<postCode>19081 2</postCode>
									<settlement>Swarthmore</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Douglas</forename><surname>Turnbull</surname></persName>
							<email>turnbull@cs.swarthmore.edu</email>
						</author>
						<author>
							<persName><forename type="first">Youngmoo</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
							<email>ykim@drexel.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Swarthmore College</orgName>
								<address>
									<postCode>19081 2</postCode>
									<settlement>Swarthmore</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Drexel University</orgName>
								<address>
									<postCode>19104</postCode>
									<settlement>Philadelphia</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Philadelphia</settlement>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Selection for Content-Based, Time-Varying Musical Emotion Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9A7A48AE5756824C6ABC53906AB0D87B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Retrieval models; H.5.5 [Sound and Music Computing]: Systems Algorithms</term>
					<term>Experimentation Emotion recognition</term>
					<term>audio features</term>
					<term>machine learning</term>
					<term>regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In developing automated systems to recognize the emotional content of music, we are faced with a problem spanning two disparate domains: the space of human emotions and the acoustic signal of music. To address this problem, we must develop models for both data collected from humans describing their perceptions of musical mood and quantitative features derived from the audio signal. In previous work, we have presented a collaborative game, MoodSwings, which records dynamic (per-second) mood ratings from multiple players within the two-dimensional Arousal-Valence representation of emotion. Using this data, we present a system linking models of acoustic features and human data to provide estimates of the emotional content of music according to the arousal-valence space. Furthermore, in keeping with the dynamic nature of musical mood we demonstrate the potential of this approach to track the emotional changes in a song over time. We investigate the utility of a range of acoustic features based on psychoacoustic and music-theoretic representations of the audio for this application. Finally, a simplified version of our system is re-incorporated into MoodSwings as a simulated partner for single-players, providing a potential platform for furthering perceptual studies and modeling of musical mood.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The training of supervised machine learning systems for determining the emotional content in music necessarily requires human-labeled "ground truth" observations, but the task often lacks a singular well-defined answer. A variety of factors contribute to a person's perception of musical mood, and there is bound to be some variation and disagreement between user ratings of the same content. Inspired by other "games with a purpose", we created MoodSwings <ref type="bibr" target="#b1">[1]</ref> to collect second-by-second labels of music clips using the wellknown Arousal-Valence (A-V) space representation of emotions, where valence reflects positive vs. negative emotions and arousal indicates emotional intensity <ref type="bibr" target="#b2">[2]</ref>. The game was designed specifically to capture data reflecting the timevarying nature of musical mood and also to collect a distribution of labels across multiple players for a given song or even a moment within a song.</p><p>Because of the time-varying nature of music, developing systems to automatically organize an entire song or clip using a single mood label, as in prior approaches <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7]</ref>, undoubtedly leads to imprecise classifications. Using initial data collected by MoodSwings, it is instead our ultimate goal to track the emotional content of music over time. In order to take full advantage of the A-V space, we formulate our problem as a regression; developing a functional mapping from high-dimensional acoustic features to emotion space coordinates. This mapping is first implemented as a leastsquares regression and later improved using support vector regression (SVR). We first demonstrate preliminary results of our system in tracking the emotional content of music over short time windows, and later implement a simplified system to be used as a simulated player "AI" for single-player MoodSwings games.</p><p>In searching for the most informative features for mood detection, no single dominant feature (e.g., loudness, timbre, and harmony all play some role) has yet emerged <ref type="bibr">[8]</ref>.</p><p>In our experiments, we also investigated multiple sets of acoustic features for each task, including psychoacoustic (mel-cepstrum and statistical frequency spectrum descriptors) and music-theoretic (estimated pitch chroma) representations of the labeled audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>The general approach to implementing automatic mood detection from audio has been to use supervised machine learning to train statistical models based on acoustic features. Li and Ogihara <ref type="bibr" target="#b3">[3]</ref> used acoustic features related to timbre, rhythm, and pitch to train support vector ma-chines (SVMs) to classify music into 13 mood categories. Using a hand-labeled library of 499 music clips (30-seconds each), they achieved an accuracy of ∼45%, with 50% of the database used for training and testing, respectively.</p><p>Lu, Liu, and Zhang <ref type="bibr" target="#b4">[4]</ref> pursued mood detection and tracking (following dynamic mood changes during a song) using a variety of acoustic features related to intensity, timbre, and rhythm. Their classifier used Gaussian Mixture Models (GMMs) for Thayer's four principal mood quadrants in the valence-arousal representation. The system was trained using a set of 800 classical music clips (from a data set of 250 pieces), each 20 seconds in duration, hand labeled to one of the 4 quadrants. Their system achieved an accuracy of ∼85% when trained on 75% of the clips and tested on the remaining 25%.</p><p>Xiao, Dellandrea, Dou, and Chen <ref type="bibr" target="#b5">[5]</ref> investigated the optimal segment length for mood classification, using the same classification scheme as <ref type="bibr" target="#b4">[4]</ref>. Using 60 unique classical pieces, each piece is broken down into segments of length 4s, 8s, 16s, and 32s, and labeled by two reviewers. They found that their system reached its peak classification performance when 16s clips were used in both the training and testing, achieving a classification performance of 88.46%.</p><p>In 2007, the Music Information Research Evaluation eXchange (MIREX) first included a "beta" task on audio music mood classification with 8 systems submitted. The audio clips used for this task were assigned to one of 5 mood clusters, aggregated from mood labels (adjectives) taken from the All Music Guide. Using 600 30-second handlabeled clips, the clips were selected to be distributed equally among the 5 mood clusters that were used in the evaluations. All participants performed reasonably well (far higher than chance) with the highest performing system achieving correct classifications slightly over 60% of the time <ref type="bibr" target="#b9">[9]</ref>. It should be noted that several of the systems were primarily designed for the genre classification task, but were also appropriated to the mood classification task <ref type="bibr" target="#b6">[6]</ref>.</p><p>Most similar to our work, Yang, Lin, Su, and Chen <ref type="bibr" target="#b7">[7]</ref> introduced the use of regression for mapping of highdimensional acoustic features to the A-V space. Support vector regression <ref type="bibr" target="#b10">[10]</ref>, as well as variety of boosting algorithms including AdaBoost.RT <ref type="bibr" target="#b11">[11]</ref>, are applied to solve the regression problem. The ground-truth A-V labels were collected by recruiting 253 college students to annotate the data, and only one label was collected per clip in their study. The work is primarily focused on the regression methods themselves as opposed to acoustic feature selection and analysis. The feature set used consists of 114 dimensions computed using publicly available extraction tools, which were then reduced to a tractable number of dimensions using principal component analysis (PCA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GROUND TRUTH DATA COLLECTION</head><p>As discussed in <ref type="bibr" target="#b1">[1]</ref>, traditional methods for collecting perceived mood labels, such as the soliciting and hiring of human subjects, can be flawed. MoodSwings is a game for online collaborative annotation based on the two-dimensional arousal-valence model. In the game, players position their cursor within the A-V space while competing (and collaborating) with a partner player to correctly annotate five 30second music clips. Glimpses of the partner's position are provided every three seconds and scoring is based on the amount of overlap between the players' cursors, which en-courages consensus and discourages nonsensical labeling. As an additional incentive for proactive and independent labeling, bonus points are awarded to the player who first reaches a particular location, making it impossible to out-score an opponent by simply following their cursor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Summary of Data Collection</head><p>The song clips used in MoodSwings are drawn from the uspop2002 database <ref type="bibr" target="#b12">[12]</ref>, and overall we have collected over 100,000 individual A-V labels spanning more than 1,000 songs. Since the database consists entirely of popular music, the labels collected thus far display an expected bias towards high-valence and high-arousal values. Although inclusion of this bias could be useful for optimizing classification performance, it is not as helpful for learning a mapping from acoustic features that provides coverage of the entire emotion space. Because of this trend, we developed a reduced dataset consisting of 15-second music clips from 240 songs selected, via labels collected through the game, to approximate an even distribution across the four primary quadrants of the A-V space. These clips were subjected to intense focus within the game in order to form a corpus, referred to here as MoodSwings Lite, with significantly more labels per song clip.</p><p>Although we used the MoodSwings Lite corpus as the basis for classification, the original (uniform) distribution across the quadrants shifted slightly as more labels were collected for each individual clip. The final distribution of "ground truth" class labels is given in Table <ref type="table" target="#tab_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ACOUSTIC FEATURE COLLECTION</head><p>As previously stated, there is no single dominant feature, but rather many that play a role (e.g., loudness, timbre, harmony) in determining the emotional content of music. Since our experiments focus on the tracking of emotion over time, we chose to focus on solely on time-varying features. Our collection contains many features that are popular in music information retrieval and speech processing encompassing both psychoacoustic as well as music-theoretic representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mel-frequency cepstral coefficients</head><p>Mel-frequency cepstral coefficients (MFCCs) are among the most widely used acoustic features in speech and audio processing. MFCCs are essentially a low-dimensional representation of the spectrum warped according to the mel-scale, which reflects the nonlinear frequency sensitivity of the human auditory system <ref type="bibr" target="#b13">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Chroma</head><p>The chromagram is a well-established method for estimating the western pitch class components within a short time-interval <ref type="bibr" target="#b14">[14]</ref>. It is essentially a circular version of the logarithmically warped spectrogram, where the frequencies corresponding to chroma in different octaves are grouped together and summed to estimate the energy at each of the 12 pitch classes. Using this feature, it is sometimes possible to obtain an indication of the overall musical key and modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Statistical Spectrum Descriptors (SSD)</head><p>In music and audio processing, statistical spectrum descriptors are often related to timbral texture <ref type="bibr" target="#b15">[15]</ref>. For each spectral shape function, we begin by dividing the data into short-overlapping segments, applying a Hanning window, and computing the magnitude DFT. A short explanation of each of the SSD features can be seen in Table <ref type="table">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Description</head><p>Centroid The weighted-average (center of mass) of the spectrum Flux</p><p>The Euclidean distance between successive spectral frames Rolloff</p><p>The frequency beneath which a given proportion of the total spectral energy lies, typically 85% Flatness Quantifies how close the spectral distribution is to uniform (white)</p><p>Table <ref type="table">2</ref>: Description of spectral shape features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Octave-Based Spectral Contrast</head><p>Many spectral features perform some averaging of the spectral distribution, which results in a loss of spectral information (note that two different spectra can yield very similar results for many spectral shape features). Spectral contrast features provide a rough representation of the harmonic content in the frequency domain based upon the identification of peaks and valleys in the spectrum, separated into different frequency sub-bands <ref type="bibr" target="#b16">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS AND RESULTS</head><p>In order to properly make the case for regression methods, we first investigated the use of classification techniques involving discrete emotion classes based upon the four quadrants of the A-V space. We next investigated the emotion regression of the same clips using traditional least-squares regression as well as support vector regression (SVR). For both classification and regression, we averaged feature dimensions across all frames of a given 15-second music clip, thus representing each clip with a single vector of features. Although this is a significant reduction, it provides a consistent representation that facilitates direct comparisons between the various classification and regression methods.</p><p>Since it is our ultimate goal to track the emotional content over time, higher-order statistics of the features become less and less meaningful as we shorten the window length. In emotion tracking we investigated regression performance at shorter window lengths to develop a system that we ultimately implemented back into MoodSwings as the second player "AI" for single player games.</p><p>In all experiments, classification and regression, we divided the MoodSwings Lite corpus 70%/30% between training and testing samples. To avoid the "album-effect" <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>, we ensured that any songs that were recorded on the same album were either placed entirely in the training or testing set. Additionally, each experiment was subject to over 50 cross-validations, varying the distribution of training and testing data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification</head><p>Support vector machines (SVMs) were chosen as our primary classification method, based upon their past successful application to similar music classification tasks (e.g., artist and genre classification) <ref type="bibr" target="#b19">[19]</ref>. Using kernel methods, SVMs can be used to construct non-linear decision boundaries and have proven to be robust to some types of noise <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22]</ref>. Given the binary nature of the SVM, to solve our four-class problem we implemented four one-versus-all SVMs, choosing the binary classifier providing the highest confidence as the class estimate.</p><p>Shown in Table <ref type="table" target="#tab_2">3</ref> are the 4-way classification results using the individual acoustic feature sets, as well as stacking all features and stacking only MFCCs and spectral contrast. As there is no single dominant feature for emotion classification, it is expected that a method which fuses multiple features is necessary to obtain higher performance.  In dividing the data into discrete classes, clips for which A-V labels are in fact quite similar may be categorized into completely different classes. Such severe quantization of essentially continuous label data is likely the primary factor resulting in generally poor 4-way classification performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Regression</head><p>Given the continuous nature of the collected A-V labels and the myriad problems produced by discrete emotional classes, we investigated multiple regression methods for mapping acoustic features into the A-V space. We start by performing regression on the same 15-second clips, but then move to regression over time, investigating multiple window lengths for optimal performance, and finally integrate a regression system back into MoodSwings as the second player "AI".</p><p>As with classification, we implement supervised methods for our regression, using least-squares and support vector regression (SVR) <ref type="bibr" target="#b10">[10]</ref> methods to create optimal projections from mean acoustic features to mean A-V values. There are many possible methods for evaluating regression performance. Our primary performance metric is the average Euclidean distance between the projected coordinates and the collected A-V labels, as a normalized percentage of the A-V space. To benchmark the significance of the regression results, we compared the projections to those of an essentially random baseline. Given a trained regressor and a set of labeled testing examples, we first determined an A-V prediction for each sample. The resulting distance to the corresponding A-V label was compared to that of another randomly selected A-V label from the test set. Comparing these cases over 50 cross-validations, we computed Student's T-test for paired samples to verify the statistical significance of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Feature Selection</head><p>Without the need for forcing the data into discrete classes, we achieve qualitatively higher performance. For single feature sets, regression using least-squares regression and spectral contrast features results in the smallest average deviation (14.90%) from the mean labels of the test samples. The corresponding T-test value for this case (5.28), given the degrees of freedom (72 test data samples), provides &gt;99.9% confidence of statistical significance. Support vector regression using single acoustic features results in a smaller average deviation in almost all cases, with the highest performance (lowest average distance) using MFCCs. Again, the T-test indicates very high confidence of statistical significance.</p><p>The primary advantage of regression over classification is for A-V labels close to an axis, where a very accurate regression projection can still lead to a misclassification, according to the discrete class labels. Shown in Figure <ref type="figure" target="#fig_1">2</ref> is the projection of six 15-second clips into the arousal-valence space resulting from multiple regression methods and acoustic features. The performance of the regression can be evaluated both in terms of the distance from the mean of the collected labels and also whether or not the regression label falls within the first standard deviation of the labels (shown as an ellipse).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Multi-level Regression</head><p>While most individual features perform reasonably in mapping to A-V coordinates, a method for combining information from these domains (more informed than simply concatenating the features) could potentially lead to higher performance. We implemented a two-level regression scheme by feeding the outputs of individual regressors, each trained using distinct features, into a second-stage regressor determining the final prediction. We investigated two topologies: in one case the secondary arousal and valence regressors receive only arousal and valence estimates, respectively; in the second case the secondary arousal and valence regressors receive both arousal and valence estimates from the first-stage. We will refer to these two topologies as multi-level separate and multi-level combined.</p><p>In all cases the secondary regressors employ linear leastsquares and are trained using a leave-one-out method (on each iteration we train the first-stage regressors leaving one example out and use the estimates of that example from the first stage to train the second stage). The results for both cases are shown in Table <ref type="table" target="#tab_5">5</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Least-Squares Regression</head><note type="other">Topology</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Emotion Tracking Over Time</head><p>Although the projection from acoustic features to A-V coordinates is quite promising, the ultimate goal of our research is to be able to track the changes of emotion within music over time. Shown in Figure <ref type="figure" target="#fig_2">3</ref> is the performance of spectral contrast regression for varying window lengths. It can be seen that it is possible to obtain the short-time information with only minimal loss in individual frame accuracy. Using these shorter window lengths we sought to implement our regression system to track the emotional changes within our clips over time. Shown in Figure <ref type="figure" target="#fig_3">4</ref> are the projections of three clips of audio obtained using only spectral contrast features and our most most simple (least-squares) regression technique. In this example, each clip has been broken down into three five second examples. Each individual label, which is shown as a dot, gets darker over time to show the overall emotional shift of the song. Of each fivesecond segment, we compute the mean as our ground truth and display that as a red X. Using our trained emotion regressor, we use the acoustic features from the five second chunks and project the predicted A-V position over time as well (blue O). As we continue to expand the label collection and pursue more intelligent feature fusion methods for regression, we are working towards the goal of accurately reflecting changing emotions within any song on a short-time basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">MoodSwings "AI"</head><p>To further test our system, and to obtain additional human feedback, we have incorporated a simplified version of our regression into MoodSwings to compensate for one of our most major issues with the game-that oftentimes no human partners are available online. Single-player games can be played using "partner" labels recorded from a prior game, but this eliminates songs for which annotations have not been previously recorded. Our previous solution was to intersperse use of an "AI" opponent that generates random labels centered around the player's position, which is relatively easy to detect and can be highly frustrating for a player. In our new solution, we implement our regression system limiting ourselves to only simple least-squares regression and spectral contrast features. During gameplay, we provide the second player's annotations at one-second intervals, which are projected from only the average spectral contrast over a two-second window. The "AI" is available for demo on the web anytime using MoodSwings Single Player<ref type="foot" target="#foot_0">1</ref> (SP), a limited version of the game which only uses the "AI" as the secondary player.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION AND FUTURE WORK</head><p>In working with a continuously labeled space such as A-V it is clear that regression provides a more informative result over classification that is less sensitive to small variations (e.g., near the quadrant boundaries). In examining acoustic features for classification and regression, spectral contrast and MFCCs consistently provided the best perfor-mance across the classification and regression tasks, while spectral shape and chroma did not perform to expectations. MFCCs and spectral contrast capture different aspects of frequency-domain variation, so it is somewhat surprising that the combination of the two features improved classification but not regression performance. This is likely due to the "curse of dimensionality" (exponentially more data is required to fill a volume in feature space as more dimensions are added). Thus, adding feature dimensions, rather than leading to more informed decisions, could have hindered overall performance. The relative scaling of different features also presents problems, since variations in magnitudes may lead to artificially inflated or reduced contributions from particular features.</p><p>The regression results are quite promising, even using the most elementary techniques (least-squares). In order to improve our regression system, we plan to continue pursuing techniques to appropriately combine information from multiple acoustic features for audio mood estimation. That is, instead of choosing a subset of features or performing dimensionality reduction (e.g., principal components analysis) on a combined feature set, we train a separate system for each feature set and use ensemble methods to determine the relative contribution of each single-feature system to improve overall performance. In addition to (or in place of) multilevel regression, feature fusion methods could be used to combine information spaces. For example, the residual error of the trained regressors could be used as a measure of the confidence of each projection, which could weight higher and lower performing feature projections accordingly.</p><p>The collection of accurate labels is clearly a crucial component for developing systems to organize music by emotion. We believe that these experiments demonstrate the MoodSwings game to be a powerful tool for such data collection, turning the labeling process into a fun activity, and capturing information reflecting the time-varying nature of musical mood. We believe that the performance of the current regression system could be believable as an anonymous "human" partner, and greatly improves our ability to collect additional data when a secondary partner is not available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The MoodSwings gameboard.</figDesc><graphic coords="2,346.36,119.69,180.00,193.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Collected A-V labels and projections resulting from regression analysis. A-V labels: second-bysecond labels per song (gray •), µ of collected labels (red •), σ of collected labels (red ellipse). Projections: least-squares spectral contrast projection (green +), least-squares MFCC projection (magenta ), SVR spectral contrast projection (blue O), and SVR MFCC projection (black X).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance data for varying window lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: A-V labels and projections over time for three example 15-second music clips (markers become darker as time advances): second-by-second labels per song (gray •), mean of the collected labels over 5-second intervals (red X), and projection from acoustic features in 5-second intervals (blue O).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>.</figDesc><table><row><cell cols="4">Class No. Arousal Valence No. of Examples</cell></row><row><cell>1</cell><cell>high</cell><cell>high</cell><cell>72</cell></row><row><cell>2</cell><cell>low</cell><cell>high</cell><cell>51</cell></row><row><cell>3</cell><cell>low</cell><cell>low</cell><cell>56</cell></row><row><cell>4</cell><cell>high</cell><cell>low</cell><cell>61</cell></row></table><note><p><p>Quadrant-based class assignments of all</p>MoodSwings Lite music clips.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results for four-way mood classification.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Regression results for individual sets of acoustic features.</figDesc><table><row><cell></cell><cell cols="2">Least-Squares Regression</cell><cell></cell></row><row><cell>Feature</cell><cell cols="3">Avg. Distance Avg. Rand. Dist. T-test</cell></row><row><cell>MFCC</cell><cell>0.158 ± 0.008</cell><cell>0.232 ± 0.015</cell><cell>4.271</cell></row><row><cell>Chroma</cell><cell>0.197 ± 0.009</cell><cell>0.207 ± 0.010</cell><cell>0.591</cell></row><row><cell>S. Shape</cell><cell>0.163 ± 0.009</cell><cell>0.222 ± 0.011</cell><cell>3.557</cell></row><row><cell>S. Contrast</cell><cell>0.149 ± 0.010</cell><cell>0.238 ± 0.014</cell><cell>5.280</cell></row><row><cell>All Feat. Stacked</cell><cell>0.154 ± 0.008</cell><cell>0.256 ± 0.015</cell><cell>5.796</cell></row><row><cell>MFCC &amp; S.C.</cell><cell>0.159 ± 0.008</cell><cell>0.248 ± 0.015</cell><cell>5.116</cell></row><row><cell></cell><cell cols="2">Support Vector Regression</cell><cell></cell></row><row><cell>MFCC</cell><cell>0.138 ± 0.007</cell><cell>0.235 ± 0.014</cell><cell>5.538</cell></row><row><cell>Chroma</cell><cell>0.178 ± 0.009</cell><cell>0.213 ± 0.012</cell><cell>2.149</cell></row><row><cell>S. Shape</cell><cell>0.170 ± 0.010</cell><cell>0.231 ± 0.014</cell><cell>3.422</cell></row><row><cell>S. Contrast</cell><cell>0.146 ± 0.009</cell><cell>0.231 ± 0.016</cell><cell>5.028</cell></row><row><cell>All Feat. Stacked</cell><cell>0.169 ± 0.008</cell><cell>0.237 ± 0.015</cell><cell>3.723</cell></row><row><cell>MFCC &amp; S.C.</cell><cell>0.141 ± 0.007</cell><cell>0.241 ± 0.013</cell><cell>5.629</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Multi-layer regression results.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>MoodSwings SP: http://music.ece.drexel.edu/mssp</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENTS</head><p>This work is supported by National Science Foundation award IIS-0644151.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Moodswings: A collaborative game for music mood label collection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Emelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Music Information Retrieval</title>
		<meeting>International Conference on Music Information Retrieval<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09">September 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Thayer</surname></persName>
		</author>
		<title level="m">The Biopsychology of Mood and Arousal</title>
		<meeting><address><addrLine>Oxford, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting emotion in music</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ogihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Music Information Retreival</title>
		<meeting>the 4th International Conference on Music Information Retreival<address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10">October 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic mood detection and tracking of music audio signals</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="18" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">What is the best segment duration for music mood analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dellandrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Content-Based Multimedia Indexing</title>
		<imprint>
			<publisher>CBMI</publisher>
			<date type="published" when="2008-05">2008. May 2008</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The 2007 MIREX results overview</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Downie</surname></persName>
		</author>
		<idno>MIREX 2007</idno>
		<ptr target="http://www.music-ir.org/mirex2007/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A regression approach to music emotion recognition</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="448" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The 2007 mirex audio mood classification task: Lessons learned</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Downie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laurier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Music Information Retrieval</title>
		<meeting>the 9th International Conference on Music Information Retrieval</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Marsyas submissions to MIREX 2007</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIREX</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Experiments with adaboost. rt, an improved boosting scheme for regression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shrestha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Solomatine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1678" to="1710" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berenzweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Whitman</surname></persName>
		</author>
		<ptr target="http://labrosa.ee.columbia.edu/projects/musicsim/uspop2002.html" />
		<title level="m">The &quot;uspop2002&quot; pop music data set</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="366" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">To catch a chorus: Using chroma-based representations for audio thumbnailing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Bartsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Wakefield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</title>
		<meeting>IEEE Workshop on Applications of Signal essing to Audio and Acoustics<address><addrLine>New Paltz, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-10">October 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Musical genre classification of audio signals</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="293" to="302" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Music type classification by spectral contrast feature</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Multimedia and Expo</title>
		<meeting>International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="113" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards quantifying the &quot;album effect&quot; in artist identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pilli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISMIR 2006 Seventh International Conference on Music Information Retrieval</title>
		<meeting>ISMIR 2006 Seventh International Conference on Music Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2006-09">September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Computational models of music similarity and their application in music information retrieval</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pampalk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006-03">March 2006</date>
			<pubPlace>Linz</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Johannes Kepler University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Support vector machine active learning for music retrieval</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Poliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2006-08">Aug 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An introduction to support vector machines and other kernel-based learning methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="295" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
