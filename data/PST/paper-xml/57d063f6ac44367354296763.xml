<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NUMFabric: Fast and Flexible Bandwidth Allocation in Datacenters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kanthi</forename><surname>Nagaraj</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dinesh</forename><surname>Bharadia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hongzi</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sandeep</forename><surname>Chinchali</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sachin</forename><surname>Katti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stanford</forename><surname>University</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mit</forename><surname>Csail</surname></persName>
						</author>
						<title level="a" type="main">NUMFabric: Fast and Flexible Bandwidth Allocation in Datacenters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">46A4ABE12FBF72952BB034E4B3EDDD8E</idno>
					<idno type="DOI">10.1145/2934872.2934890</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Resource allocation</term>
					<term>Convergence</term>
					<term>Network utility maximization</term>
					<term>Weighted max-min</term>
					<term>Packet scheduling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present NUMFabric, a novel transport design that provides flexible and fast bandwidth allocation control. NUM-Fabric is flexible: it enables operators to specify how bandwidth is allocated amongst contending flows to optimize for different service-level objectives such as weighted fairness, minimizing flow completion times, multipath resource pooling, prioritized bandwidth functions, etc. NUMFabric is also very fast: it converges to the specified allocation 2.3× faster than prior schemes. Underlying NUMFabric is a novel distributed algorithm for solving network utility maximization problems that exploits weighted fair queueing packet scheduling in the network to converge quickly. We evaluate NUMFabric using realistic datacenter topologies and highly dynamic workloads and show that it is able to provide flexibility and fast convergence in such stressful environments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Bandwidth allocation in networks has historically been at the mercy of TCP. TCP's model of allocation assumes that bandwidth should be shared equally among contending flows. However, for an increasing number of networks such as datacenters and private WANs, such an allocation is not a good fit and is often adversarial to the operator's intent. Consequently, there has been a flurry of recent work on transport designs, especially for datacenters, that target different bandwidth allocation objectives. Many aim to minimize per-packet latency <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b53">54]</ref> or flow completion time <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b3">3]</ref>, while others target multi-tenant bandwidth allocation <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b26">26]</ref>, while still others focus on sophisticated objectives like resource pooling <ref type="bibr" target="#b55">[56]</ref>, policy-based bandwidth allocation <ref type="bibr" target="#b35">[35]</ref>, or coflow scheduling <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b14">14]</ref>. In effect, each design supports one point in the bandwidth allocation policy design space, but operators ideally want a transport that can be tuned for different points in the design space depending on workload requirements.</p><p>In this paper, we present NUMFabric, a novel transport fabric that enables operators to flexibly specify bandwidth allocation policies, and then achieves these policies in the network using simple, distributed mechanisms at the switches and end-hosts. NUMFabric's design is based on the classic Network Utility Maximization (NUM) <ref type="bibr" target="#b33">[33]</ref> framework which allows per-flow resource allocation preferences to be expressed using utility functions. Utility functions encode the benefit derived by a flow for different bandwidth allocations, and can be chosen by the operator to achieve different bandwidth and fairness objectives. In §2, we show how an operator can translate high level policies such as varying notions of fairness, minimizing flow completion times, resource pooling a la MPTCP <ref type="bibr" target="#b55">[56]</ref> and bandwidth functions <ref type="bibr" target="#b35">[35]</ref> into utility functions at end-hosts. NUMFabric then realizes the bandwidth allocation that maximizes the sum of the utility functions in a completely distributed fashion.</p><p>Network utility maximization of course is not new. There is a long line of work <ref type="bibr" target="#b60">[61]</ref> on designing distributed algorithms based on gradient descent for NUM ( §3). However, these algorithms are slow to converge to the optimal rate. For datacenter workloads, where a majority of the flows may last only a few RTTs due to the high link speeds, the convergence time of these algorithms is often much larger than the lifetime of the majority of flows, and hence no guarantees on resource allocation can be made. Moreover, gradient-descent algorithms are difficult to tune since they have a "step-size" parameter that needs to be tuned for each workload and resource allocation objective. In practice, given the scale of datacenters and the variety of objectives, getting the tuning right is a formidable task.</p><p>Our main technical contribution is a transport design for solving the NUM problem that converges significantly faster than prior work and is much more robust. The key insight  underlying NUMFabric is to decouple the mechanisms for maximizing network utilization and achieving the optimal relative bandwidth allocation across competing flows. Existing NUM algorithms couple these objectives and try to accomplish both simultaneously through price variables at the links. This process is slow and brittle due to the need to balance between moving quickly towards the optimal allocation, and avoiding congestion or under-utilization ( §4). NUMFabric employs independent mechanisms for the two goals by decomposing the task of solving the NUM problem across two logical layers. Figure <ref type="figure" target="#fig_1">1</ref> shows the high level architecture. At the bottom layer, NUMFabric combines a packet scheduling mechanism based on weighted fair queuing (WFQ) <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b16">16]</ref> at the switches and a simple rate control scheme at the hosts to achieve a network wide weighted max-min rate allocation for competing flows. NUMFabric's packet scheduling differs from standard WFQ in that the flow's weights are set dynamically at the hosts and carried in packet headers. This design, which we call the Swift transport ( §4.1), exploits the fact that datacenter switches can be designed to support more sophisticated packet scheduling than simple FIFO queues <ref type="bibr" target="#b58">[59]</ref>. Swift guarantees that the network is fully utilized while keeping flows isolated using WFQ. It achieves the weighted max-min allocation rapidly as the flows, or their weights, change.</p><p>Swift provides a convenient abstraction for controlling the relative bandwidth allocation of flows (via the flow weights). NUMFabric leverages this capability via a novel distributed algorithm called eXplicit Weight Inference (xWI) which runs on top of Swift ( §4.2). In xWI, the sources and switches exchange information in packet headers to iteratively compute the weights that flows should use to achieve the optimal bandwidth allocation. Essentially, xWI iteratively guides the network from one weighted max-min allocation to the next until it finds the optimal NUM solution. Since there is no risk of congestion or under-utilization with weighted max-min allocations, xWI can aggressively update the link weights towards the optimal point and converge quickly.</p><p>We evaluate NUMFabric with detailed packet-level simulations in ns3 [49] using realistic datacenter topologies and workloads ( §6). We compare NUMFabric for several resource allocation objectives with the best in class scheme for that objective, including pFabric <ref type="bibr" target="#b3">[3]</ref> for FCT minimization, a variant of RCP <ref type="bibr" target="#b30">[30]</ref> for α-fairness <ref type="bibr" target="#b47">[47]</ref>, and a wellknown gradient-descent algorithm <ref type="bibr" target="#b40">[40]</ref> for solving general NUM problems. We find that NUMFabric is fast and flexible. Specifically, compared to gradient descent based solutions <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b30">30]</ref>, NUMFabric converges to the optimal allocations 2.3× faster at the median and 2.7× faster at the 95 th percentile; it also allows operators to realize a wide variety of fairness-utilization objectives <ref type="bibr" target="#b47">[47]</ref>, policies that optimize flow completion time <ref type="bibr" target="#b3">[3]</ref>, and sophisticated policies such as resource pooling <ref type="bibr" target="#b55">[56]</ref> and bandwidth functions <ref type="bibr" target="#b35">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">HOW TO CHOOSE UTILITY FUNCTIONS?</head><p>NUMFabric adopts NUM <ref type="bibr" target="#b33">[33]</ref> as a flexible framework for expressing fine-grained bandwidth allocation preferences as an optimization problem. In the most basic form, each network flow is associated with a utility as a function of its rate. The goal is to allocate rates to maximize the overall system utility, subject to link capacity constraints:</p><formula xml:id="formula_0">maximize i U i (x i ) subject to Rx ≤ c.<label>(1)</label></formula><p>Here, x is the vector of flow rates; R is the {0, 1} routing matrix, i.e., R(i, l) = 1 if and only if flow i traverses link l; and c is the vector of link capacities. The utility functions, U i (•) are assumed to be smooth, increasing, and strictly concave. 1 A flow is defined generically; for example, a flow can be a TCP connection, traffic between a pair of hosts, or traffic sent or received by a host.</p><p>The utility function choice depends on the bandwidth allocation objective that the operator wishes to achieve. We pick four popular and broad bandwidth allocation policies and show how they can be expressed using utility functions below; a similar exercise can be carried out for other policies. Table <ref type="table">1</ref> provides a summary. Fairness. Various notions of fairness can be expressed simply by changing the shape of the utility functions. The αfair <ref type="bibr" target="#b47">[47]</ref> class of utility functions, represented in the first row of Table <ref type="table">1</ref>, enable an operator to express different preferences on the fairness/efficiency trade-off curve by vary- 1 Concave utility functions ensure the optimization problem is tractable and has a unique global optimum <ref type="bibr" target="#b12">[12]</ref>. Some practically interesting utility functions are not concave, e.g., bandwidth guarantees for inelastic flows. In such cases, global optimization is generally intractable, but under certain conditions distributed algorithms have been shown to attain the global optimum <ref type="bibr" target="#b22">[22]</ref>. A study of non-concave utility functions is beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Allocation Objective</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NUM objective</head><p>Flexible α-fairness <ref type="bibr" target="#b47">[47]</ref> i</p><formula xml:id="formula_1">x 1-α i /(1 -α) Weighted α-fairness i w α i x 1-α i /(1 -α) Minimize FCT [3] i x i /s i Resource pooling [68] i y 1-α i /(1 -α),</formula><p>where y i = p∈P ath(i) x ip Bandwidth functions <ref type="bibr" target="#b35">[35]</ref> i xi</p><formula xml:id="formula_2">0 F i (τ ) -α dτ</formula><p>Table <ref type="table">1</ref>: Example utility functions for several resource allocation policies. The case α = 1 is to be interpreted in the limit α → 1; e.g., i log x i for the first row.</p><p>ing α, a non-negative constant. α = 0 is purely utilitarian: maximize overall throughput without concern for fairness. As α increases, the NUM solution gets "more fair", eventually converging to the egalitarian max-min fair allocation as α → ∞. An important case is α = 1, which is a compromise between these extremes and is called proportional fairness. α-fair utility functions can also be generalized to express relative priorities using different weight multipliers for different flows, as shown in the second row of Table <ref type="table">1</ref>.</p><p>Minimizing Flow Completion Time. Size-based scheduling policies that are effective for minimizing (average) flow completion time can also be approximated within the NUM framework, as shown in the third row of Table <ref type="table">1</ref>. The utility functions are linear in the rates and associate a weight to each flow inversely proportional to its size (s i ). It is not difficult to see that the solution to this problem coincides with the Shortest-Flow-First policy for flows sharing a single link. As we show in §6.3, this objective also performs very well in the multi-link case. Similarly, the weights can be chosen inversely proportional to the remaining flow size or flow deadlines to approximate Shortest-Remaining-Processing-Time (SRPT) or Earliest-Deadline-First (EDF) scheduling for meeting deadlines <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b3">3]</ref>. <ref type="foot" target="#foot_0">2</ref>Resource Pooling. The goal of resource pooling <ref type="bibr" target="#b67">[68]</ref> is to make a collection of network links behave as though they make up a single link with the aggregate capacity (see <ref type="bibr" target="#b67">[68]</ref>, Figure <ref type="figure" target="#fig_1">1</ref>, for an illustration). This is useful in datacenters, where the network fabric has a large number of paths and we would like flows to use the entire pool of capacity efficiently. The Multipath TCP (MPTCP) <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b55">56]</ref> congestion control algorithm has recently been proposed for achieving resource pooling. MPTCP divides a flow into sub-flows that traverse different paths and implements a coordinated congestion control across them to realize resource pooling. It turns out that resource pooling for multipath flows can be expressed as a NUM problem, as shown by Kelly <ref type="bibr" target="#b31">[31]</ref>.</p><p>The key idea is to consider the utility for a flow in terms of the total rate of all its sub-flows. Any sharing/fairness objective can be generalized for multipath resource pooling in this way. The fourth row of Table <ref type="table">1</ref> shows this for the /s i instead, with a small such as 0.1.  α-fairness objective. Here y i is the total rate, summed over the rates of the subflows on different paths.</p><p>Bandwidth Functions. Bandwidth functions are an intuitive abstraction for expressing bandwidth sharing policies that have been used in Google's Bandwidth Enforcer (BwE) system for their private WAN <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b35">35]</ref>. A bandwidth function, B(f ), specifies the bandwidth to be allocated to a flow as a non-decreasing function of a dimensionless variable, f , called the fair share. The bandwidth function indicates the priority and weight of a flow relative to other flows for different values of f . As an illustration, Figure <ref type="figure" target="#fig_3">2</ref> shows the bandwidth functions of two flows. Here, flow 1 has strict priority over flow 2 for the first 10 Gbps of capacity (f ≤ 2); beyond that, flow 2 receives bandwidth at twice the slope of flow 1 until it reaches 10 Gbps (2 ≤ f ≤ 2.5), and so on.</p><p>Formally, given the bandwidth functions, B i (f ), for a set of flows sharing a single link, the bandwidth allocation is determined by finding the largest value of f such that (1) flow i is allocated bandwidth B i (f ); (2) the link is not oversubscribed, i.e. i B i (f ) ≤ C, where C is the link capacity. Computing this allocation is straight forward via a waterfilling procedure: start with f = 0 and increase f until the link capacity is reached. Figure <ref type="figure" target="#fig_3">2</ref> shows the resulting bandwidth allocation for the two flows, when contending for a link of capacity 10 Gbps or 25 Gbps. This water-filling procedure can be generalized for an arbitrary number of links by calculating a max-min set of fair share values for the flows (see <ref type="bibr" target="#b35">[35]</ref> for details).</p><p>We now show how, given a set of operator-defined bandwidth functions, B i (f i ), we can derive corresponding utility functions (shown in the last row of Table <ref type="table">1</ref>) such that the NUM solution achieves the desired allocation. For technical convenience, we assume that B i (•) are strictly increasing. Let F i (x)</p><p>B -1 i (x) be the inverse bandwidth function, giving the fair share as a function of allocated bandwidth. Now consider the following utility function:</p><formula xml:id="formula_3">U i (x i ) = xi 0 F i (τ ) -α dτ,<label>(2)</label></formula><p>where α is positive constant. U i (•) is concave and increas-ing, hence the NUM problem has a unique optimal solution. It turns out that for large α, the NUM solution is close to the allocation corresponding to the bandwidth functions. 3 Informally, the reason is that for large α, the marginal utility, U i (x i ) = F i (x i ) -α , increases very sharply for smaller values of fair share, F i (x i ). Therefore, NUM favors increasing the fair share (and rate) of flows with smaller fair share. For large α, the result is an allocation that is approximately max-min in the fair shares, as desired. In practice, we find that α ≈ 5 is sufficient for very good approximation ( §6.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRIOR APPROACHES TO NUM</head><p>There are several well understood distributed algorithms for NUM <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b40">40]</ref> that have a structure similar to end-toend congestion control. At a high level, sources determine the rates of flows based on congestion feedback from network links. Each link computes a congestion price based on the aggregate traffic at the link, and each source adjusts its rate (or window size in window-based schemes like TCP) based on the aggregate congestion price along its path. To make the description precise, we focus on a standard and well-studied end-to-end algorithm for NUM first proposed by Low et al. <ref type="bibr" target="#b40">[40]</ref>. We call it the Dual Gradient Descent (DGD) algorithm for reasons which will become evident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dual Gradient Descent Algorithm</head><p>In the DGD algorithm, the flow rates and link prices are interpreted as the primal and dual variables of the NUM optimization problem <ref type="bibr" target="#b1">(1)</ref>. The DGD algorithm is an iterative gradient descent based procedure for computing the primal and dual optimal variables. We omit the derivation for brevity, but it can be shown that this can be done in a distributed fashion because the dual problem decomposes into independent subproblems, one for each flow, as shown in Low et al. <ref type="bibr" target="#b40">[40]</ref> (Sec IV). We focus on the two key operational aspects of the algorithm: how prices at each link (represented by p l for link l) are computed and updated at the switches, and how end-hosts compute and update their sending rates (represented by x i for sender i).</p><p>Updating sending rates at the end-hosts. The DGD algorithm proceeds in iterations. In iteration t, given the (fixed) link prices, each flow sets its rate, x i to:</p><formula xml:id="formula_4">x i (t) = U -1 i   l∈L(i) p l (t)   ,<label>(3)</label></formula><p>where L(i) is the set of links on flow i's path. The rate update has an intuitive interpretation; it sets the rate of each flow to be such that the marginal utility is equal to the overall sum of prices of the links on the flow's path. Note that the flow only needs to know the overall sum of prices, not the individual prices of the links along the path.</p><p>Updating link prices at the switches. Subsequently, fixing the flow rates x i (t) for iteration t, the DGD algorithm calculates the price at each link for iteration t + 1 using the 3 The desired allocation is achieved in the limit: α → ∞.</p><p>following gradient descent step:</p><formula xml:id="formula_5">p l (t + 1) =   p l (t) + γ   i∈S(l) x i (t) -c l     + .<label>(4)</label></formula><p>Here, S(l) is the set of flows incident on link l, and γ is the step size. The notation [x] + means max(x, 0). Equation ( <ref type="formula" target="#formula_5">4</ref>) has an intuitive interpretation. The term i∈S(l) x i -c l is the net traffic through the link minus its capacity, and turns out to be the gradient for the dual problem. The price is a measure of congestion: it increases when traffic exceeds the link's capacity and decreases otherwise. Further, the increase or decrease is controlled by the gradient. Equations ( <ref type="formula" target="#formula_4">3</ref>) and ( <ref type="formula" target="#formula_5">4</ref>) define the DGD algorithm. Low et al. <ref type="bibr" target="#b40">[40]</ref> prove that the iterations converge to the NUM solution, provided the step size parameter is sufficiently small. Drawbacks of DGD. The DGD algorithm is simple and elegant, but it has some key drawbacks in practice. First, it converges slowly and can take many iterations to find the optimal solution. This is important because if the underlying conditions change before the algorithm converges (e.g., a flow arrives or departs), then the algorithm is constantly trying to catch up to the new optimal allocation. Specifically, if the convergence time is greater than the coherence time of the workload, the algorithm will never converge. Second, the DGD algorithm is very sensitive to the step size parameter, γ, in Equation ( <ref type="formula" target="#formula_5">4</ref>). If γ is too small, the prices are prohibitively slow to converge, but set γ too large and the system becomes unstable and oscillatory. The "right" value of γ depends on a complex mix of network structure, flow pattern, and feedback latency. There is little theoretical guidance for tuning DGD beyond very conservative bounds that guarantee convergence <ref type="bibr" target="#b40">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DESIGN</head><p>NUMFabric solves NUM bandwidth allocation problems faster and more robustly than existing approaches. NUM-Fabric's insight is to decouple the underlying mechanisms for maximizing network utilization and achieving the optimal relative rate allocation. Existing NUM algorithms such as DGD couple these objectives and accomplish both in the price computation. Specifically, link prices in DGD directly dictate the sending rates of flows (Eq. ( <ref type="formula" target="#formula_4">3</ref>)). DGD gradually adjusts the link prices (Eq. ( <ref type="formula" target="#formula_5">4</ref>)) to (1) match the aggregate traffic at each bottleneck link to its capacity; (2) drive the relative rate allocations of the flows towards the optimal value. These two objectives are achieved simultaneously: as the prices react to local rate-capacity mismatches at each link, they also collectively converge to specific values such that the flows attain the optimal relative rate allocations.</p><p>The link prices in DGD essentially act both as a measure of congestion to control network utilization, and as a coordination signal between different flows to determine the relative rate allocation. This coupling makes for a brittle dynamic, where any change (e.g., a flow arrival or departure) that requires the link prices to change in order to achieve the correct relative allocation cannot occur without rate-capacity mismatches at the links. In fact, a link's price cannot change unless there is a rate-capacity mismatch at that link (Eq. ( <ref type="formula" target="#formula_5">4</ref>)). For this reason, DGD must adjust the link prices gradually, in order to avoid under-utilization or packet drops.</p><p>NUMFabric decouples the objectives using two separate layers, as shown in Figure <ref type="figure" target="#fig_1">1:</ref> • The Swift transport ( §4.1), a transport design that given a weight for each flow, quickly achieves the networkwide weighted max-min fair rate allocation for all flows. • The eXplicit Weight Inference (xWI) algorithm ( §4.2), a distributed algorithm that calculates the optimal weights for the flows such that the weighted max-min rate allocation (achieved by Swift) solves the NUM problem.</p><p>Swift provides the abstraction of a network with guaranteed high utilization and weighted max-min allocation <ref type="bibr" target="#b44">[44,</ref><ref type="bibr">9]</ref>, where the flow weights can be set dynamically. This allows the relative bandwidth allocation of the flows to be controlled without having to worry about high utilization or network congestion. xWI leverages this capability to quickly search for the optimal weights and link prices, which now solely act as a coordination signal -not a measure of congestion -enabling xWI to converge quickly and safely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Swift Transport</head><p>Swift flows have a weight that is set by the source and is sent to the network in packet headers. The Swift transport uses a combination of WFQ <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b16">16]</ref> in the switches and a simple rate control scheme at the end-hosts to achieve the network-wide weighted max-min rate allocation. We describe each component in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swift Switches</head><p>The switches implement a packet scheduling algorithm based on classical WFQ <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b16">16]</ref>. WFQ services a set of flows contending at a link in proportion to their weights. Swift switches do the same, except that the flow's weight is allowed to change on a packet-by-packet basis. We leave the details of the algorithm and its practical realization using recently proposed hardware mechanisms <ref type="bibr" target="#b58">[59]</ref> to §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swift Rate Control</head><p>WFQ achieves weighted max-min allocation for a single link. To achieve network-wide weighted max-min allocation, each flow must also send traffic at the rate dictated by the WFQ scheduler at its bottleneck link. If a flow sends below or above the rate that WFQ allows, it can under-utilize the available capacity or cause packet drops.</p><p>We design a simple window-based rate control scheme to achieve network-wide weighted max-min. The rate control algorithm has two requirements. First, it must set the window size to be larger than the bandwidth-delay product (BDP) for the flow, so that the flow's rate is not limited by its window. Second, it must keep the buffer occupancy small at the switches. This is important for fast convergence: large buffer occupancies slow down convergence to weighted max-min when the set of flows (or their weights) change, because large buffers may take a long time to drain when a flow's bottleneck shifts from one link to another.</p><p>Our rate control algorithm is inspired by packet-pair <ref type="bibr" target="#b34">[34]</ref> and packet-train <ref type="bibr" target="#b13">[13]</ref> techniques for estimating available bandwidth in a network of switches with WFQ packet scheduling. The receiver measures the inter-packet time for each incoming packet, and sends this value to the sender in acknowledgments. Upon receiving an ACK, the sender calculates a rate sample, bytesAcked / interPacketTime, and smoothens these values using an exponentially weighted moving averaging (EWMA) filter <ref type="bibr" target="#b61">[62]</ref> to estimate the available bandwidth, R. The sender then sets its window size to W = R × (d 0 + d t ), where d 0 is the baseline fabric RTT (without queuing delay) and d t is a small slack factor (e.g., a few packets worth of delay) chosen to ensure that the window size is larger than the BDP (estimated by R × d 0 ).</p><p>To start, the sender initially sends a small burst (e.g., 3 packets in our implementation) into the network. This burst ensures that packets are queued at the bottleneck, and thus the inter-packet time observations at the receiver reflect the true available bandwidth. Upon receiving the first inter-packet time sample, <ref type="foot" target="#foot_1">4</ref> the sender initializes R; thereafter, it updates R and the window size as explained above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The xWI Algorithm</head><p>xWI is a novel distributed algorithm for solving NUM problems that runs on top of a weighted max-min achieving transport layer like Swift. xWI is inspired by iterative message passing <ref type="bibr" target="#b43">[43]</ref> algorithms. In each iteration, sources exchange messages with switches to compute the weights to set for their flows in Swift. xWI iteratively refines the weights to arrive at the optimal NUM allocation.</p><p>The key idea in xWI is to iteratively solve the KKT system of equations <ref type="bibr" target="#b12">[12]</ref> for the NUM problem:</p><formula xml:id="formula_6">U i (x i ) = l∈L(i) p l for all flows i,<label>(5)</label></formula><formula xml:id="formula_7">p l   i∈S(l) x i -c l   = 0 for all links l,<label>(6)</label></formula><p>where x i are the flow rates, and p l are the link prices. Intuitively, the first condition implies that at the optimal point, a flow's marginal utility is equal to the total price it must pay on the path it is traversing. The second condition implies that either a link is fully utilized (i.e. the sum of the rates of flows on that link is equal to the link capacity), or if it is underutilized, the link price is zero. The flow rates and link prices are optimal iff they satisfy the above equations and are feasible, i.e., Rx ≤ c, p ≥ 0 <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b60">61]</ref>.</p><p>Flow weight assignment and rate allocation. Recall the rate assignment step in the DGD algorithm (Eq. ( <ref type="formula" target="#formula_4">3</ref>)), which sets the rate of each flow based on the sum of the link prices on its path. xWI uses the same function of the sum of the link prices, but uses it to set the flow's weight to be used by Swift, not its rate. Specifically, in iteration t, the flow's weight is assigned as</p><formula xml:id="formula_8">w i (t) = U -1 i   l∈L(i) p l (t)   .<label>(7)</label></formula><p>The Swift transport then takes these weights and allocates rates to all flows according to weighted max-min:</p><formula xml:id="formula_9">{w i (t)} weighted max-min -----------→ {x i (t)}<label>(8)</label></formula><p>The intuition for this step is that the above function of the link prices calculates rates such that Eq. ( <ref type="formula" target="#formula_6">5</ref>) is satisfied. If the prices are at the optimal values, this calculation gives the optimal rates, but generally, the calculated values are not optimal (or even feasible) rates for incorrect prices. Here, using weights has a crucial advantage: it lets Swift find a feasible and efficient allocation that approximately satisfies Eq. ( <ref type="formula" target="#formula_6">5</ref>) even if the link prices are not optimal. By contrast, assigning rates (as done in DGD), can cause over-or under-utilization if the link prices have not converged to the optimal values. As the prices reach the optimal values, the weights computed by Eq. ( <ref type="formula" target="#formula_8">7</ref>) will be the same as the optimal rates for the NUM problem; and Eq. ( <ref type="formula" target="#formula_6">5</ref>) will be satisfied exactly. Price computation. Next, link prices at iteration t + 1 are updated based on the values (link prices and flow rates) in iteration t to approach the optimal values. For this purpose, each link independently updates its price towards satisfying the two optimality conditions in Eqs. ( <ref type="formula" target="#formula_6">5</ref>) and <ref type="bibr" target="#b6">(6)</ref>.</p><p>The update rule consists of two terms, corresponding to the two optimality conditions. The first term is given by</p><formula xml:id="formula_10">p res l p l (t) + min i∈S(l) U i (x i (t)) -k∈L(i) p k (t) |L(i)| ,<label>(9)</label></formula><p>and tries to satisfy the system of equations ( <ref type="formula" target="#formula_6">5</ref>). Here, |L(i)| denotes the number of links in flow i's path. Notice that for each flow passing through link l, the corresponding equation in (5) has a residual: U i (x i (t)) -k∈L(i) p k (t). The intuition behind Eq. ( <ref type="formula" target="#formula_10">9</ref>) is to adjust the link prices to push these residual values to zero as much as possible. To see how, consider a flow with residual e traversing L links. If each link on the flow's path adds e/L to its price, the residual will become zero. Equation ( <ref type="formula" target="#formula_10">9</ref>) applies the same idea, except that each link adjusts its price based on the smallest residual, such that after the update, the residual for the flow with the smallest residual before the update becomes zero.</p><p>For the second optimality condition (Eq. ( <ref type="formula" target="#formula_7">6</ref>)), notice that Swift's weighted max-min allocation ensures that the link capacities are never exceeded; i.e. i∈S(l) x i (t) ≤ c l , for all links l and iterations t. Equation ( <ref type="formula" target="#formula_7">6</ref>) is automatically satisfied for bottleneck links, for which i∈S(l) x i (t) = c l . For underutilized links, however, the price must be driven to zero to satisfy <ref type="bibr" target="#b6">(6)</ref>. We achieve this by subtracting a term based on underutilization from p res l :</p><formula xml:id="formula_11">p new l p res l -η 1 - i∈S(l) x i (t) c l p l (t) + ,<label>(10)</label></formula><p>where η is a positive constant. The parameter η may appear similar to the step size parameter γ of the DGD algorithm in Eq. ( <ref type="formula" target="#formula_5">4</ref>). But η has a much less crucial role: it only kicks in for underutilized links to drive the price to zero. The second term above will be zero for all bottlenecks links at all times. Therefore, xWI is largely insensitive to the value of η.</p><p>The final refinement is an averaging of the new price estimate and the current value, which is a standard technique <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b62">63]</ref> for ensuring such non-linear dynamical systems converge to a fixed point:</p><formula xml:id="formula_12">p l (t + 1) = βp l (t) + (1 -β)p new l .<label>(11)</label></formula><p>Here, β ∈ (0, 1) is the averaging parameter (set to 0.5 in our implementation). We have found averaging to be important for improving system stability, particularly in the presence of noise (e.g., due to traffic burstiness, measurement noise, etc) in our packet-level simulations.</p><p>We have proven that the xWI dynamical system has a unique fixed point, and this fixed point solves the NUM optimization problem <ref type="bibr" target="#b1">(1)</ref>. We have also conducted extensive numerical simulations of the algorithm, and found that xWI converges to the NUM optimal solution across a wide range of randomly generated topologies and flow patterns. We leave these results to the extended version of this paper <ref type="bibr" target="#b48">[48]</ref>. In §6, we evaluate NUMFabric using packet level simulations for realistic datacenter topologies and workloads. Distributed realization. xWI can be implemented in a completely decentralized fashion. The flow weight calculation in Eq. ( <ref type="formula" target="#formula_8">7</ref>) requires the sources to know the sum of the link prices along their path, which can be obtained using endto-end feedback (same as in DGD). Swift then realizes the weighted max-min allocation in Eq. ( <ref type="formula" target="#formula_9">8</ref>) as described in §4.1. For the price computation, to evaluate p res l in Eq. ( <ref type="formula" target="#formula_10">9</ref>), the switches need to know the normalized residual, i.e. residual divided by path length: U i (x i (t)) -k∈L(i) p l (t) /|L(i)|, for each flow through the link. Each flow calculates this value and sends it to the switches on its path in packet headers. Finally, the underutilization term in Eq. ( <ref type="formula" target="#formula_11">10</ref>) only requires local information: the total traffic through the link and the current link price. We describe the NUMFabric protocol that implements xWI in detail next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">NUMFABRIC'S PRACTICAL DESIGN</head><p>We functionally sketch out the actions at the receiver, the sender, and the switch. NUMFabric adds five fields to packet headers in the transport layer: virtualPacketLen and interPacketTime for Swift; pathPrice, pathLen, and normalizedResidual for xWI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The NUMFabric Receiver and Sender</head><p>The receiver gets the sum of the link prices and the number of links on the path from the pathPrice and pathLen fields in received packets. It then simply reflects these values and the latest inter-packet time (used by Swift's rate control algorithm; see §4.1) back to the sender in ACKs.</p><p>The sender maintains the inverse of the marginal utility function, U -1 i (•), and uses the pathPrice obtained from each ACK to compute the flow's weight (Eq. ( <ref type="formula" target="#formula_8">7</ref>)). The weight is used to set the virtualPacketLen field for outgoing packets as the packet length divided by the weight (we discuss how switches use this field below). The sender also sets the normalizedResidual field in each outgoing packet as the marginal utility minus the path price divided by the path length (see §4.2). The marginal utility is calculated as U i ( R), where R is the current estimated rate of the flow. As explained in §4.1, R is computed by applying an EWMA filter to the inter-packet times obtained from ACKs and is also used to set the flow's window size: W = R × (d 0 + d t ). For control packets such as SYNs and pure ACKs, the virtualPacketLen field is set to zero, and the normalizedResidual field is ignored by the switches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The NUMFabric Switch</head><p>The switch implements WFQ packet scheduling ( §4.1) and the xWI price computation ( §4.2).</p><p>NUMFabric's WFQ design. The full hardware implementation of WFQ for NUMFabric is not the focus of this paper. We briefly sketch out a design conceptually based on Start Time Fair Queuing (STFQ) <ref type="bibr" target="#b20">[20]</ref>. STFQ approximates the order with which packets would depart in WFQ by assigning a virtual start and virtual finish time to each packet. It then schedules packets in ascending order of virtual start time. Let p k i be the k-th packet of flow i. Upon p k i 's arrival, STFQ computes the virtual start and finish time as follows:</p><formula xml:id="formula_13">S(p k i ) = max V, F (p k-1 i ) ,<label>(12)</label></formula><formula xml:id="formula_14">F (p k i ) = S(p k i ) + L(p k i ) w i .<label>(13)</label></formula><p>Here, L(p k i ) is the length of packet p k i and w i is the weight for flow i. V is the virtual time at the switch at the time of packet p k i 's arrival (see <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref> for details). To compute these values, the switch maintains V (a single register), as well as the virtual finish time of the last packet, F (p k-1 i ), for each active flow. The virtualPacketLen field of packet p k i provides L(p k i )/w i , which the switch uses for Eq. ( <ref type="formula" target="#formula_14">13</ref>). The design sketched above requires a priority queue to schedule packets in increasing order of virtual start times. While today's switches do not support priority queues, recent work <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref> has shown that it is feasible to implement programmable priority queues in emerging programmable switching chips <ref type="bibr" target="#b11">[11]</ref>. This design allows packets to be inserted into the queue based on a programmable rank value that is computed before the packet is enqueued. The paper <ref type="bibr" target="#b59">[60]</ref> shows how STFQ can be realized with this design. We omit the details and refer the reader to <ref type="bibr" target="#b59">[60]</ref>.</p><p>xWI price computation. The switch computes and updates the price for each of its outgoing links periodically. We assume the price updates are synchronized at all switches. This can be accomplished in datacenters with Precision Time Protocol <ref type="bibr" target="#b37">[37]</ref>, which is now a common feature in commodity switches <ref type="bibr" target="#b46">[46]</ref>. The price is computed as shown in Figure <ref type="figure" target="#fig_4">3</ref>. The procedure is a faithful implementation of the xWI price calculation described in §4.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EVALUATION</head><p>In this section, we present an extensive ns3 simulationbased evaluation of NUMFabric. The goal is to evaluate NUMFabric's (1) fast convergence to the optimal allocation in dynamic settings; and its (2) flexibility for meeting various bandwidth allocation objectives precisely and robustly. The code used for all our simulations can be found at <ref type="bibr" target="#b49">[50]</ref>.</p><p>Topology. We simulate a data center network built using a leaf-spine architecture. There are a total of 128 servers connected to 8 leaf switches with 10 Gbps links. Each leaf switch is connected to 4 spine switches using 40 Gbps links, thus ensuring full bisection bandwidth. The switches are modeled as standard output-queued switches, with a buffer of size 1 MB per port. We chose this large limit to avoid complications for comparing the convergence times of different algorithms which are sensitive to packet drops. All of the implemented schemes target a small queue occupancy, and thus avoid packet drops well below this buffer size; the queue occupancies are typically only a few packets at equilibrium. The network RTT is 16 µs.</p><p>Schemes compared. We have implemented the following: DGD rate control, an idealized rate control protocol based on the Dual Gradient Descent algorithm described in §3. The sources calculate their sending rates from the network price (obtained from ACKs) according to Eq. (3). They then transmit at exactly this rate on a packet-by-packet basis. The switches implement a price update rule similar to Eq. ( <ref type="formula" target="#formula_5">4</ref>), with an additional term to control the queue occupancy:</p><formula xml:id="formula_15">p l ← [p l + a(y -C) + bq] + .<label>(14)</label></formula><p>Here, y is the link throughput, C is the link capacity, q is the queue occupancy, and a and b are constant parameters. The price is updated periodically.</p><p>RCP <ref type="bibr" target="#b30">[30]</ref>, a generalization of RCP <ref type="bibr" target="#b18">[18]</ref> for α-fairness <ref type="bibr" target="#b47">[47]</ref>. <ref type="foot" target="#foot_2">5</ref>As in standard RCP, switches in RCP allocate a fair share rate to all flows passing through each of their links. The fair share is updated periodically according to:</p><formula xml:id="formula_16">R l ← R l 1 + T d a(C -y) -b q d C .<label>(15)</label></formula><p>Here, R l is the fair-share rate that is advertised by the switch to flows passing though link l, T is the update interval, and d is the running average of the RTT of the flows. C, y, and q have the same interpretation as in DGD. When a packet is served by link l, R -α l is added to a field in the packet header (similar to the path price field in DGD). The source calculates the sending rate for flow i as follows:</p><formula xml:id="formula_17">x i =   l∈L(i) R -α l   -1 α . (<label>16</label></formula><formula xml:id="formula_18">)</formula><p>The sum is over the links L(i) on flow i's path and is obtained through ACKs. RCP is similar to DGD since both algorithms set the sending rates directly based on explicit network feedback in the form of a sum over per-link variables that depend on link congestion. NUMFabric, our design as described in §5.</p><p>Oracle, a numerical fluid model simulation that takes the current network state, including the topology and current set of flows, as input and outputs the optimal rate allocation according to the NUM problem. We use the Oracle to test the correctness and speed of convergence of the NUMFabric, DGD and RCP algorithms. Table <ref type="table" target="#tab_1">2</ref> shows the default parameter settings for all schemes.  Note on the implementation of DGD and RCP . We enhance DGD and RCP to limit the number of unacknowledged bytes that flows can have to 2 × the Bandwidth-Delay Product. This ensures that flows are large enough to saturate the network yet restricts them from building up large queues when the rates have not converged to the correct values. Large queues adversely affect stability and convergence times for both DGD and RCP by increasing feedback delay. Therefore, the results that we report are better than what can be achieved with the standard rate-based implementation of these schemes. Also, the performance of DGD and RCP is very sensitive to the gain parameters associated with utilization and queue occupancy (a and b). We swept across the parameter space and picked parameters that gave the fastest convergence time while maintaining stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Convergence to NUM Allocations</head><p>We design two scenarios to quantify the speed of convergence: semi-dynamici scenarios where we can inject network events in a controlled manner and measure convergence times accurately, and realistic dynamic scenarios from measured datacenter workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-dynamic Workload</head><p>For this experiment, we randomly pair 1000 senders and receivers among the 128 servers in our network to create 1000 random flow paths. To inject dynamism, we create "network events," where we randomly choose 100 of these paths to either start 100 new flows or stop 100 active flows. For each network event, we define the convergence time as the time it takes for the rates of at least 95% of the flows to reach within 10% of the optimal NUM allocation (which we compute using the Oracle mentioned earlier). We also make sure that the flows stay within this margin for at least 5 ms before declaring they have converged. Once the rates of 95% of the flows converge, we trigger the next flow start/stop event. We ensure that there are 300-500 flows active after each event, and simulate 100 such events. The NUM objective for these experiments is proportional fairness: i log(x i ).</p><p>Measuring convergence time accurately at microsecond timescales is tricky. At these timescales, rate measurements are noisy because of small variations in inter-packet times (e.g., due to bursty packet scheduling at the switches <ref type="bibr" target="#b8">[8]</ref>). To overcome the noise, we use exponential averaging <ref type="bibr" target="#b61">[62]</ref> with a time constant of 80 µs to filter the rates measured at the destination. It takes log e (10) × 80 ≈ 185 µs for the filter's output to reach 90% of its final value. We subtract this additional delay from all measured convergence times since it's an artifact of our measurement that would exist even if the flows converged instantaneously.</p><p>Figure <ref type="figure" target="#fig_7">4</ref>(a) compares the convergence times for NUM-Fabric, DGD and RCP . NUMFabric converges in 335 µs at the median: ∼ 2.3× faster than DGD and RCP . At the 95 th percentile, NUMFabric converges in 495 µs: ∼ 2.7× faster than the other schemes. It is important to note that these values are for 95% of a few hundred flows to converge after a network event. NUMFabric's convergence time for individual flows is much lower.</p><p>The primary reason for NUMFabric's fast convergence is the agility that the decoupled combination of Swift's convergence to weighted max-min and xWI's fast computation of link prices provides. Since DGD and RCP have to optimize both objectives simultaneously, they move towards the optimal allocation more gingerly. Comparison with DCTCP. We were also hoping to compare NUMFabric with a deployed congestion control algorithm like DCTCP. However, although the rates achieved by DCTCP flows are stable when averaged over longer timescales (several milliseconds), they are very noisy at timescales of 100s of microseconds. As a result, DCTCP flows essentially never converge, unless we measure the rates over timescales much longer than the convergence time of the other algorithms. For example, Figure <ref type="figure" target="#fig_7">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Workloads</head><p>Next, we evaluate NUMFabric's fast convergence for more realistic dynamic settings. We consider two workloads based on measurements from a web search <ref type="bibr" target="#b3">[3]</ref> cluster and a large enterprise <ref type="bibr" target="#b4">[4]</ref>. In the web search workload, about 50% of the flows are smaller than 100 KB, but 95% of all bytes belong to the larger 30% of the flows that are larger than 1 MB. The enterprise workload is also heavy-tailed, but has many more short flows with 95% of the flows smaller than 10 KB. The flows arrive as a Poisson process of different rates to simulate different load levels.</p><p>It is difficult to define convergence time for such dynamic workloads since a majority of the flows finish before they converge (esp. flows smaller than a BDP). Hence we compare the average rates of the flows achieve to what they would have achieved with an ideal Oracle that assigns all flows their optimal NUM rates instantaneously. Specifically, we calculate the rate of a flow as its size divided by its completion time, and calculate the normalized deviation from ideal for scheme X as (rateWithX -idealRate)/idealRate, where idealRate is the rate of the flow with the Oracle. For example, a normalized rate deviation of +1 means that the flow's rate is 2× larger than its rate with the Oracle, while negative values indicate that the flow's rate was lower than with the Oracle.</p><p>We bin the flows into different sizes and compare the rate deviation for each bin separately. The bin boundaries are chosen log-scale in the bandwidth-delay product (BDP), which is 200 KB in our network. Figure <ref type="figure" target="#fig_9">5</ref>(a) shows the normalized rate deviation over all flows belonging to each bin for the web search workload. The smaller flows have larger errors for DGD and RCP since the flows may not last enough RTTs for these schemes to converge. NUMFabric achieves  rates fairly close to the rates achieved by the Oracle even in this bin. As the flow sizes increase, NUMFabric's faster convergence brings it closer to the Oracle's rates while DGD and RCP still lag. The median error of NUMFabric is around zero for all the bins beyond a flow size of 100 KB. This means that under NUMFabric, flows with size above ∼5 BDP converged on the average. One interesting observation here is that the median errors of DGD and RCP are nega- tively biased. This indicates that the flows under DGD and RCP achieved lower than the ideal rates. This is a direct consequence of the slow convergence of these algorithms and the consequent inability to grab available bandwidth. Figure <ref type="figure" target="#fig_9">5</ref>(b) plots the performance for the enterprise workload. This workload is more skewed, nearly 70% of the flows send 1 or 2 packets. Convergence for such flows is meaningless since the notion of a rate itself is hard to define. This clearly shows up in the plots: the deviation for the very small flows is relatively large for all 3 schemes. The median deviation from ideal for NUMFabric is nearly zero for all other bins for this workload as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Parameter Sensitivity Analysis</head><p>NUMFabric performs well for a wide range of values of its parameters, but requires tuning to extract the best performance. We present some insights for tuning the parameters of the two layers of NUMFabric, Swift and xWI, and the interplay between them. We also use the semi-dynamic scenario designed in §6.1 to quantify parameter sensitivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swift parameters</head><p>The Swift sender applies an EWMA filter to the interpacket times reported with each ACK to estimate the flow's rate. The EWMA filter's time constant, ewmaTime, determines how fast or slow the rate estimate matches the available capacity at the bottleneck. This parameter should be set large enough for the filter to "span" enough samples to get a reliable estimate; otherwise, the rate estimate can be noisy due to natural variations in inter-packet times. The noise causes undesirable oscillations in Swift's window size as well as the normalized residual calculation discussed in §5. We found an ewmaTime of 20 µs or more is required for good stability in a 10 Gbps network. The higher the link speed, the faster we can collect inter-packet time samples and the faster we can run the filter. The delay slack parameter, d t , used by Swift to calculate the window size ( §4.1) exhibits a trade-off: setting it too small risks underutilizing the available bandwidth, but if too large, it causes a large buffer occupancy and slows down convergence. In Figure <ref type="figure" target="#fig_10">6</ref>(a), we vary the value of d t from 3 µs to 24 µs. We observed that with d t = 3 µs, many events don't converge at all -the plot shows the median for events that did converge. This is because for WFQ to work correctly, each flow must have at least 1 packet queued at its bottleneck link at all times. With a very small d t , the window sizes become so small that this condition is sometimes not satisfied. On the other hand, a very high d t also leads to slower convergence and sometimes oscillating rates. We find that it is sufficient to set d t to the delay-equivalent of a small number (e.g., 5-10) packets. For example, for our 10Gbps network, we use d t = 6 µs which targets a buffer occupancy of 5 packets (1500 bytes each) at every bottleneck link.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>xWI parameters</head><p>Link prices are updated periodically to reflect changes in the normalized residual values of flows and utilization of links since the last price update. Once a link price is updated, the switch has to wait for the sources to learn of the new prices, adjust the flow's weight, and measure the impact on the rate achieved by Swift to calculate the new normalized residual values ( §5). There is no benefit, and indeed it can be detrimental to stability, to update prices before Swift has had time to react. This process takes at least 1.5-2 RTTs: 1 RTT for the weights to reflect the new prices; and 0.5-1 RTTs for Swift's WFQ and rate control mechanisms to achieve the new weighted max-min allocation (and measure it). In practice, achieving and measuring the new weighted max-min allocation can take longer because of dependencies among flows for convergence <ref type="foot" target="#foot_3">6</ref> and EWMA filtering for rate measurement. While worst-case bounds can be found for convergence to weighted max-min <ref type="bibr" target="#b56">[57]</ref>, they depend on the traffic pattern, and our experiments indicate the bounds are overly conservative for NUMFabric.</p><p>In Figure <ref type="figure" target="#fig_10">6</ref>(b), we measure the impact of the price update interval on NUMFabric's convergence by varying it from 30 µs to 128 µs. As expected, the median convergence time increases as the price update interval increases. In practice, we find that a price update interval of around 2 RTTs usually works well and is fast; increasing it beyond 2 RTTs improves robustness, but is slower, while decreasing it much below 2 RTTs degrades convergence and is not recommended.</p><p>The two other parameters of xWI are the multiplier for the utilization term (η; Eq. ( <ref type="formula" target="#formula_11">10</ref>)) and the averaging parameter (β; Eq. ( <ref type="formula" target="#formula_12">11</ref>)) for the price calculation. xWI is largely insensitive to these parameters. We use β = 0.5 and η = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different utility functions</head><p>We also tested NUMFabric with different utility functions by varying α in the α-fair family of utility functions. We found that while the default parameters of NUMFabric work fine for moderate values of α, for α below 0.5 or above 2.0, some events in the semi-dynamic scenario don't converge. The reason is that, at more extreme values α, some of the calculations become numerically unstable; e.g., with very low values of α, the weight calculation in Eq. ( <ref type="formula" target="#formula_8">7</ref>) is very sensitive to noise. Therefore, a faster control loop that does not smooth over enough rate samples to filter out the noise can cause oscillations. To test this, we slowed down NUM-Fabric 2×, by increasing the price update interval to 60 µs and ewmaTime to 40 µs, and found that NUMFabric converges for nearly all events at all values of α. As Figure <ref type="figure" target="#fig_10">6(c)</ref> shows, this slowdown comes at a modest cost to the median convergence time.</p><p>We repeated the same experiment with lower α with DGD and RCP , and found that the algorithms do not converge for values of α lower than 0.5 with the default parameters. We could not get either algorithm to converge reliably even after slowing down the gain factors by 4×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Flexibility</head><p>In this section, we experimentally evaluate NUMFabric's ability to achieve a wide variety of bandwidth allocation objectives. We focus on three policies discussed in §2 which have been the focus of much recent work: minimizing flow completion times <ref type="bibr" target="#b3">[3]</ref>, resource pooling <ref type="bibr" target="#b55">[56]</ref>, and bandwidth allocation objectives specified via bandwidth functions as in the recent BwE system <ref type="bibr" target="#b35">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Minimizing Flow Completion Time (FCT)</head><p>As discussed in §2, the utility function:</p><formula xml:id="formula_19">i (1/s i )x 1- i</formula><p>, where s i is the flow size and is a small constant approximates the Shortest-Flow-First allocation policy for minimizing FCT. We use = 0.125; note that this objective is similar to α-fairness with α = 0.125. For NUMFabric to converge to optimal values for such a small α, we slow down the system 2× as described in § 6.2. 7 We compare NUMFabric with pFabric <ref type="bibr" target="#b3">[3]</ref>, the state-of-the-art transport for FCT minimization. We replicate the same evaluation topology and workload as in the pFabric paper <ref type="bibr" target="#b3">[3]</ref> (the web search workload), and compare the FCTs achieved by NUMFabric and pFabric as network load varies from 20% to 80%. Figure <ref type="figure" target="#fig_11">7</ref> shows the results. We observe that NUMFabric achieves FCTs close to pFabric. The average normalized FCT of NUMFabric is within 4-20% of pFabric under different load conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resource Pooling</head><p>Next, we evaluate if NUMFabric can achieve a resource pooling bandwidth allocation objective. We use a leaf-spine 7 Mimicking pFabric <ref type="bibr" target="#b3">[3]</ref>, we also set the initial window size to be equal to the BDP, so that short flows are able to send enough data in the first RTT. topology with 128 servers, 8 leaf switches and 16 spine switches. All links have 10 Gbps speed. We use a permutation traffic pattern exactly as in the MPTCP paper <ref type="bibr" target="#b55">[56]</ref>, where servers 1-64 each send to one server among 65-128. Each source destination pair flow uses one or more sub-flows, with each sub-flow hashed onto a path at random. We vary the number of sub-flows between each source-destination pair and plot the total achieved throughput and the per-flow throughputs in Figure <ref type="figure" target="#fig_13">8</ref>.</p><p>We consider two utility functions: (1) No Resource Pooling, which is just proportional fairness at the sub-flow level;</p><p>(2) Resource Pooling, which aims for proportional fairness at the level of the aggregate rates of the source-destination pairs (summed across all their sub-flows). This is the utility function shown in the fourth row of Table <ref type="table">1</ref> with α = 1.</p><p>To implement this utility function, we need a small change to the NUMFabric sender. Specifically, every sub-flow first computes a weight according to Eq. ( <ref type="formula" target="#formula_8">7</ref>) based on the aggregate price on its path. It turns out, however, that this value corresponds to the total weight for the entire flow between the source-destination pair, from the perspective of the price (congestion) on the sub-flow's path. To compute the weight for the sub-flow itself, we multiply the total weight by the fraction of the total throughput being served by the sub-flow. This intuitive heuristic works well in our experiments, and we leave its detailed analysis to future work.</p><p>Figure <ref type="figure" target="#fig_13">8</ref>(a) shows that as the number of sub-flows increases to 8, NUMFabric with resource pooling achieves close to the optimal throughput of 1. Moreover, as shown in Figure <ref type="figure" target="#fig_13">8</ref>(b), the allocation is extremely fair across all flows (source-destination pairs). This is despite the fact that the sub-flows of these flows are mapped randomly to different paths, and thus compete with different numbers of other subflows. The flow-level fairness is a direct consequence of the resource pooling objective. In fact, without resource pooling, the allocation is much less fair across flows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bandwidth Functions</head><p>Next, we evaluate how well NUMFabric can achieve more complex bandwidth allocation objectives specified by bandwidth functions <ref type="bibr" target="#b35">[35]</ref>. Specifically, we use the utility function derived in §2 to closely approximate bandwidth functions.</p><p>We use the same scenario shown in Figure <ref type="figure" target="#fig_3">2</ref> (originally described in the BwE paper <ref type="bibr" target="#b35">[35]</ref>). Two flows with the band-   width function shown in Figure <ref type="figure" target="#fig_3">2</ref> are competing on a link. We vary the link capacity from 5 Gbps to 35 Gbps, and plot the throughput achieved by each flow in Figure <ref type="figure" target="#fig_14">9</ref>. The results show that NUMFabric's allocation is almost identical to the expected allocation at all link capacities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bandwidth Functions &amp; Resource Pooling</head><p>An interesting question is whether we can combine bandwidth functions and resource pooling. In other words, can operators specify bandwidth functions for flows over a network that is also expected to provide resource pooling? Such a system doesn't exist to the best of our knowledge, but would be quite useful in datacenters. To implement this,  To evaluate how well NUMFabric can combine bandwidth functions and resource pooling, we choose a scenario where we change the capacity of the middle link from X = 5 Gbps to X = 17 Gbps, after 5 ms of simulation. Figure <ref type="figure" target="#fig_16">10</ref> shows the throughput of each flow before and after this transition. Initially, when the speed of the middle link is 5 Gbps, we should expect an overall allocation of 10 Gbps to Flow 1 and 3 Gbps to Flow 2 according to the bandwidth function. This would imply that the middle link is solely used for Flow 1's traffic as Figure <ref type="figure" target="#fig_16">10</ref> shows. When the middle link speed increases to 17 Gbps, we would expect 15 Gbps for Flow 1 and 10 Gbps for Flow 2 given their bandwidth functions. NUM-Fabric quickly achieves the correct bandwidth allocation as shown in the figure.</p><note type="other">X=5Gbps X=17 Gbps</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">RELATED WORK</head><p>Datacenter Transport. Most existing datacenter transport techniques focus on a specific allocation objective, e.g., meeting deadlines <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b63">64]</ref>, minimizing flow completion times <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b23">23]</ref>, network latency <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref>, providing bandwidth guarantees <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b26">26]</ref> or improving convergence times <ref type="bibr" target="#b27">[27]</ref>. Our design is most closely related to pFabric <ref type="bibr" target="#b3">[3]</ref>, which also uses in-network packet scheduling to decouple the network's scheduling policy from rate control. However, pFabric only supports the SRPT scheduling policy for minimizing FCT. NUMFabric supports any policy which can be expressed as a NUM problem, including minimizing FCT. Our simulations show that NUMFabric has nearly as good performance as pFabric while being much more flexible.</p><p>Fastpass <ref type="bibr" target="#b53">[54]</ref> is a centralized arbiter for scheduling every packet in the datacenter. This gives flexibility to implement any network resource allocation policy. However, a centralized scheme is inherently prone to scaling problems due to (1) the large scale and churn in datacenter networks which will stress the communication and computation capabilities of the centralized arbiter; and (2) the overhead of communicating with a centralized arbiter for very short flows.</p><p>The XCP protocol <ref type="bibr" target="#b28">[28]</ref> designed in the Internet context is also based on the insight that utilization control and fairness should be decoupled. XCP, however, decouples these functions within the rate control layer, and is therefore similar to other gradient-based designs which need many iterations to converge. It is also not designed for flexible objectives.</p><p>FCP <ref type="bibr" target="#b21">[21]</ref> is a congestion control algorithm that provides flexibility to end-hosts to realize different objectives for resource allocation among flows originating from the same end-host. Realizing network-wide objectives in FCP requires modifying the switch algorithms to expose differential pricing tailored for specific objectives. NUMFabric supports a much wider range of network-level objectives (e.g., flexible bandwidth functions <ref type="bibr" target="#b35">[35]</ref>) in a unified framework.</p><p>Packet scheduling. The PIFO programmable scheduler <ref type="bibr" target="#b59">[60]</ref> demonstrates the feasibility of implementing programmable priority queues in modern switch hardware, which can be used to realize NUMFabric's Swift transport fabric, ( §5). Universal Packet Scheduling <ref type="bibr" target="#b45">[45]</ref> attempts to realize any given packet schedule with a single scheduling algorithm in the switches (which coincidentally also requires a priority queue). Although promising, flexible packet scheduling alone cannot achieve arbitrary network-wide objectives such as α-fairness, resource pooling, and bandwidth functions ( §2). These require coordination among competing flows, which is exactly what NUMFabric achieves via link prices and the xWI algorithm ( §4.2).</p><p>Network Utility Maximization. NUMFabric builds on a long line of work that began with Kelly's seminal paper <ref type="bibr" target="#b33">[33]</ref> on an optimization framework for resource allocation in the Internet. The NUM literature is vast: many distributed algorithms <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b65">66]</ref>, congestion control protocols <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b47">47]</ref>, and active queue management schemes <ref type="bibr" target="#b6">[6]</ref> have been designed based on NUM. In addition, the optimization viewpoint provides a theoretical framework for analyzing existing congestion control protocols (such as TCP) as distributed algorithms for solving NUM for specific utility functions <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b36">36]</ref> (refer to <ref type="bibr" target="#b60">[61]</ref> for a comprehensive survey).</p><p>Several distributed algorithms have been proposed in the theoretical literature <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b65">66]</ref> for NUM that aim to speed up convergence using Newton-method-based update steps. The calculations in these algorithms are much more involved than traditional gradient-based schemes (and NUMFabric), but they may result in faster convergence.</p><p>The NUM framework has also been generalized in various ways: utilities can be functions of rate, delay, jitter, reliability, etc. and can even be coupled across flows <ref type="bibr" target="#b50">[51]</ref>. The assumption of continuity and concavity of the utility functions can be relaxed <ref type="bibr" target="#b22">[22]</ref>. While we do not consider these extensions in this paper, we believe our new approach for solving NUM may apply more generally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSION</head><p>NUMFabric enables operators to flexibly optimize the network's bandwidth allocation in order to meet service level objectives. By decoupling network utilization from relative bandwidth allocation via a weighted max-min achieving transport layer based on WFQ, NUMFabric shows how innetwork packet scheduling can help implement diverse policies at datacenter speeds and scales.</p><p>NUMFabric pushes the envelope on the convergence speed of practical mechanisms for achieving NUM at scale, but it is still not fast enough for the shortest of flows which last fewer than 5-10 RTTs ( §6.1). In the future, NUMFabric's two layers (Figure <ref type="figure" target="#fig_1">1</ref>) could evolve to improve convergence speed or simplify the implementation. For instance, an interesting line of future work could consider alternatives to Swift ( §4.1) for achieving weighted max-min, for example, practical approximations of WFQ such as a small set of queues with different weights; or replacing WFQ altogether with a fast proactive rate control scheme such as PERC <ref type="bibr" target="#b27">[27]</ref>.</p><p>NUMFabric's theoretical analysis also warrants more attention. While we can show that NUMFabric's fixed point is unique and optimal <ref type="bibr" target="#b48">[48]</ref>, we have yet to prove that it always converges or analytically characterize any constraints on its parameters for convergence.</p><p>Finally, it is still an open question as to how an operator can take advantage of NUMFabric's capabilities to optimize application layer objectives. This is the focus of our current work. We are extending NUMFabric to support more general definitions of flows such as co-flows <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b17">17]</ref>, VM-level and tenant-level aggregates <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b35">35]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: NUMFabric's high level architecture. The operator picks utility functions based on the desired allocation objective. NUMFabric realizes the allocation with distributed mechanisms at the hosts and switches that implement two logical layers: (1) Swift, a network-wide weighted max-min achieving transport; (2) xWI, an optimal weight computation algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fair</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Flow1 (blue) and Flow2 (red) share a link and have the bandwidth functions shown. If the link speed is 10 Gbps, the blue flow gets all of the link, corresponding to a fair share of 1. But with a link speed of 25 Gbps, the blue flow gets 15 Gbps and the red flow gets 10 Gbps, for a fair share of 2.5.</figDesc><graphic coords="3,419.62,57.56,137.57,96.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Price computation in the NUMFabric switch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>priceUpdateInterval = 16 µs a = 4 × 10 - 9 1 RCP</head><label>41091</label><figDesc>SchemeParameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(b) shows the rates achieved by a typical DCTCP flow during several network Rate of a typical NUMFabric flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Convergence behavior of NUMFabric, DGD, RCP and DCTCP in semi-dynamic scenario. The rates are measured using an EWMA filter with a 80 µs time constant. To calculate the convergence time, we subtract 185 µs (the rise time of the filter) from the measured convergence times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Deviation from ideal rates for NUMFabric, DGD and RCP in two dynamic workloads. The box shows the 25th and 75th percentiles and whiskers extend to show 1.5 times the box length. Outliers are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Median convergence times as we vary difference parameters in NUMFabric. The 2× line in part (c) is for NUMFabric slowed down by a factor of 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: NUMFabric with the FCT minimization utility function vs. pFabric. The results are normalized to the lowest possible FCT for each flow given its size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Resource pooling: (a) Total throughput achieved as we increase the number of sub-flows per flow (b) Flows ranked by throughput.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The throughput achieved by two competing flows on a bottleneck link with varying capacity. The bandwidth functions of the flows are shown in Figure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The middle link in the topology has variable link capacity (X), which is initially set to 5 Gbps and changes to 17 Gbps after 5 ms. The plot shows the time series of the aggregate throughput achieved by the two flows before and after the capacity change. The two flows use the bandwidth functions shown in Figure 2.we modify the utility function modeling the bandwidth function to consider the aggregate throughput achieved by a flow across all its sub-flows, instead of the throughput of individual sub-flows. As in the previous experiment, we have two flows over a topology of three links as shown in Figure10. The link in the middle is shared by the two flows, while only Flow 1 sends on the top link and only Flow 2 sends on the bottom link. These two flows use the same bandwidth functions as in previous section (shown in Figure2).To evaluate how well NUMFabric can combine bandwidth functions and resource pooling, we choose a scenario where we change the capacity of the middle link from X = 5 Gbps to X = 17 Gbps, after 5 ms of simulation. Figure10shows the throughput of each flow before and after this transition. Initially, when the speed of the middle link is 5 Gbps, we should expect an overall allocation of 10 Gbps to Flow 1 and 3 Gbps to Flow 2 according to the bandwidth function. This would imply that the middle link is solely used for Flow 1's traffic as Figure10shows. When the middle link speed increases to 17 Gbps, we would expect 15 Gbps for Flow 1 and 10 Gbps for Flow 2 given their bandwidth functions. NUM-Fabric quickly achieves the correct bandwidth allocation as shown in the figure.</figDesc><graphic coords="12,423.47,51.36,135.90,114.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2.    </figDesc><table><row><cell># enqueueing packet p upon arrival</cell></row><row><cell>def enqueue(p):</cell></row><row><cell>if p is DATA: # not control (e.g., SYN)</cell></row><row><cell>minRes = min(p.normalizedResidual, minRes)</cell></row><row><cell># dequeueing packet p for departure</cell></row><row><cell>def dequeue(p):</cell></row><row><cell>bytesServiced += p.length</cell></row><row><cell>p.pathPrice += price</cell></row><row><cell>p.pathLen += 1</cell></row><row><cell># price update timeout</cell></row><row><cell>def priceUpdateTimeout():</cell></row><row><cell>u = bytesServiced / (priceUpdateTime *</cell></row><row><cell>linkCapacity) # link utilization</cell></row><row><cell>newPrice = max(price + minRes -eta * (1 -u) *</cell></row><row><cell>price, 0)</cell></row><row><cell>price = beta * price + (1 -beta) * newPrice</cell></row><row><cell>bytesServiced = 0</cell></row><row><cell>minRes = inf</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Default parameter settings in simulations.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The linear objective is not strictly concave, hence the NUM solution may not be unique. We can avoid this in practice by using i x 1- i</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>The first inter-packet time arrives with the second ACK (following the standard three-way handshake). The sender ignores the first ACK and sends nothing.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Standard RCP achieves max-min fairness<ref type="bibr" target="#b18">[18]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>See<ref type="bibr" target="#b56">[57]</ref> for an analysis of dependency chains during convergence to max-min.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank our shepherd, David Wetherall, and the anonymous SIGCOMM reviewers for their valuable feedback which helped improve the presentation of the paper. This work was partly supported by a Google Faculty Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Data Center TCP (DCTCP)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Less is more: trading a little bandwidth for ultra-low latency in the data center</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">pFabric: Minimal Near-optimal Datacenter Transport</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CONGA: Distributed congestion-aware load balancing for datacenters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimization flow control with Newton-like algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Athuraliya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GLOBECOM</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">REM: active queue management</title>
		<author>
			<persName><forename type="first">S</forename><surname>Athuraliya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Network</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="48" to="53" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards Predictable Datacenter Networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">WF2Q: worst-case fair weighted fair queueing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C R</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Data networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gallager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Humblet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Prentice-Hall International New Jersey</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed large scale network utility maximization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zymnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dolev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISIT</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Forwarding Metamorphosis: Fast Programmable Match-action Processing in Hardware for SDN</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bosshart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Izzard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mujica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Measuring Bottleneck Link Speed in Packet-switched Networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Crovella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perform. Eval</title>
		<imprint>
			<biblScope unit="page" from="297" to="318" />
			<date type="published" when="1996-10">Oct. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient coflow scheduling without prior knowledge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient Coflow Scheduling with Varys</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analysis and Simulation of a Fair Queueing Algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Demers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keshav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Decentralized Task-aware Scheduling for Data Center Networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Dogar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>SIGCOMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Processor sharing flows in the internet</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dukkipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang-Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWQoS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A self-clocked fair queueing scheme for broadband applications</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Golestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Start-time fair queueing: a scheduling algorithm for integrated services packet switching networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Vin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking (TON)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="690" to="704" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fcp: a flexible transport framework for accommodating diversity</title>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed rate allocation for inelastic flows</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shengyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking (TON)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Finishing Flows Quickly with Preemptive Scheduling</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Godfrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fixed points by a new iteration method</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="147" to="150" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Experience with a Globally-deployed Software Defined Wan</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">EyeQ: Practical Network Performance Isolation at the Edge</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jeyakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">High Speed Networks Need Proactive Congestion Control</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotNets</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Congestion control for high bandwidth-delay product networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Handley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rohrs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Charging and rate control for elastic traffic</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European transactions on Telecommunications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="37" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Stability and fairness of explicit congestion control with small buffers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Voice</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>ACM SIGCOMM Computer Communication Review</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stability of end-to-end algorithms for joint routing and rate control</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Voice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGCOMM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mathematical modeling of the internet. Mathematics unlimited-2001 and beyond</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Kelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="685" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rate control for communication networks: Shadow prices, proportional fairness and stability</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Maulloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K H</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="252" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Control-theoretic Approach to Flow Control</title>
		<author>
			<persName><forename type="first">S</forename><surname>Keshav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">BwE: Flexible, Hierarchical Bandwidth Allocation for WAN Distributed Computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end congestion control schemes: Utility functions, random losses and ecn marks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kunniyur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="689" to="702" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ieee 1588-standard for a precision clock synchronization protocol for networked measurement and control systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Eidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mohl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on IEEE</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1588</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ishikawa and mann iterative process with errors for nonlinear strongly accretive mappings in banach spaces</title>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="125" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A duality model of tcp and queue management algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Low</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking (TON)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="536" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optimization flow control i: basic algorithm and convergence</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Lapsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking (TON)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="861" to="874" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Internet congestion control</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Doyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control Systems, IEEE</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="43" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Understanding tcp vegas: a duality model</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="235" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Information theory, inference and learning algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bandwidth sharing: objectives and algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Massoulié</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Universal packet scheduling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Timed Consistent Network Updates</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mizrahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Saat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Moses</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fair end-to-end window-based congestion control</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Walrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking (ToN)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="556" to="567" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Nagaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bharadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chinchali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katti</surname></persName>
		</author>
		<ptr target="https://people.csail.mit.edu/alizadeh/papers/numfabric-techreport.pdf" />
		<title level="m">NUMFabric: Fast and Flexible Bandwidth Allocation in Datacenters</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<ptr target="https://knagaraj@bitbucket.org/knagaraj/numfabric.git" />
		<title level="m">NUMFabric public release</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A tutorial on decomposition methods for network utility maximization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Palomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1439" to="1451" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Alternative distributed algorithms for network utility maximization: Framework and applications</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Palomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2254" to="2269" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A generalized processor sharing approach to flow control in integrated services networks: the single-node case</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gallager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking (ToN)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="344" to="357" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fastpass: A Centralized &quot;Zero-queue&quot; Datacenter Network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fugal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Faircloud: sharing the network in cloud computing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Improving Datacenter Performance and Robustness with Multipath TCP</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raiciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A theory of convergence order of maxmin rate allocation and an optimal protocol</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sharing the data center network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Towards Programmable Packet Scheduling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sivaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotNets</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Programmable Packet Scheduling at Line Rate</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sivaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">The mathematics of Internet congestion control</title>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Core-stateless Fair Queueing: Achieving Approximately Fair Bandwidth Allocations in High Speed Networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Approximating fixed points of nonexpansive mappings by the ishikawa iteration process</title>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="308" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deadline-Aware Datacenter TCP (D2TCP)</title>
		<author>
			<persName><forename type="first">B</forename><surname>Vamanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fast tcp: motivation, architecture, algorithms, performance</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hegde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking (ToN)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1246" to="1259" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A distributed newton method for network utility maximization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jadbabaie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CDC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Better never than late: meeting deadlines in datacenter networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rowtron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Wischik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Handley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Braun</surname></persName>
		</author>
		<title level="m">The resource pooling principle. SIGCOMM</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Implementation and Evaluation of Congestion Control for Multipath TCP</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wischik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raiciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Greenhalgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Handley</surname></persName>
		</author>
		<author>
			<persName><surname>Design</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
