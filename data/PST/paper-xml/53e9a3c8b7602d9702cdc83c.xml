<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Let a biogeography-based optimizer train your Multi-Layer Perceptron</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-02-03">3 February 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Seyedali</forename><forename type="middle">Mohammad</forename><surname>Mirjalili</surname></persName>
							<email>seyedali.mirjalili@griffithuni.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<postCode>4111</postCode>
									<settlement>Nathan, Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<postCode>4111</postCode>
									<settlement>Nathan, Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Lewis</surname></persName>
							<email>a.lewis@griffith.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="institution">Griffith University</orgName>
								<address>
									<postCode>4111</postCode>
									<settlement>Nathan, Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Let a biogeography-based optimizer train your Multi-Layer Perceptron</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2014-02-03">3 February 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">108C528FB5261D8A2C093752F2F128B2</idno>
					<idno type="DOI">10.1016/j.ins.2014.01.038</idno>
					<note type="submission">Received 7 October 2012 Received in revised form 10 October 2013 Accepted 26 January 2014</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>FNN Neural network Learning neural network Biogeography-Based Optimization BBO Evolutionary algorithm</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Multi-Layer Perceptron (MLP), as one of the most-widely used Neural Networks (NNs), has been applied to many practical problems. The MLP requires training on specific applications, often experiencing problems of entrapment in local minima, convergence speed, and sensitivity to initialization. This paper proposes the use of the recently developed Biogeography-Based Optimization (BBO) algorithm for training MLPs to reduce these problems. In order to investigate the efficiencies of BBO in training MLPs, five classification datasets, as well as six function approximation datasets are employed. The results are compared to five well-known heuristic algorithms, Back Propagation (BP), and Extreme Learning Machine (ELM) in terms of entrapment in local minima, result accuracy, and convergence rate. The results show that training MLPs by using BBO is significantly better than the current heuristic learning algorithms and BP. Moreover, the results show that BBO is able to provide very competitive results in comparison with ELM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the more significant inventions in the field of soft computing is Neural Networks (NN), inspired by biological neurons in the human brain. The rudimentary concepts of NN were first mathematically modeled by McCulloch and Pitts <ref type="bibr" target="#b32">[33]</ref>. The simplicity, low computational cost, and high performance have made this computational tool remarkably popular over the last decade. Among different types of NNs, the Feedforward Neural Network (FNN) <ref type="bibr" target="#b13">[14]</ref> is the simplest and most widelyused.</p><p>FNNs receive information as inputs on one side and provide outputs from the other side using one-directional connections between the neurons in different layers. There are two types of FNN: Single-Layer Perceptron (SLP) <ref type="bibr" target="#b53">[54]</ref> and Multi-Layer Perceptron (MLP) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b72">73]</ref>. In the SLP there is only a single perceptron that makes it suitable for solving linear problems. However, an MLP has more than one perceptron, established in different layers. This makes it capable of solving non-linear problems.</p><p>Generally speaking, the applications of MLPs are categorized as pattern classification <ref type="bibr" target="#b34">[35]</ref>, data prediction <ref type="bibr" target="#b18">[19]</ref>, and function approximation <ref type="bibr" target="#b15">[16]</ref>. Pattern classification implies classifying data into pre-defined discrete classes <ref type="bibr" target="#b3">[4]</ref>, whereas prediction refers to the forecasting of future trends according to current and previous data <ref type="bibr" target="#b18">[19]</ref>. Finally, function approximation involves the process of modeling relationships between input variables. It has been proven that MLPs with one hidden layer are able to approximate any continuous or discontinuous functions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref>. Regardless of the applications, the distinguishing capability of MLPs is learning <ref type="bibr" target="#b2">[3]</ref>. MLPs are equipped with a learning concept that gives them the ability to learn from experience, similar to a human brain. This component is an essential part of all NNs. It may be divided into two types: supervised <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b73">74]</ref> and unsupervised <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b57">58]</ref> learning. For MLPs, most applications use the standard <ref type="bibr" target="#b25">[26]</ref> or improved Back-Propagation (BP) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b80">81]</ref> algorithms as their learning methods, which belong to the supervised learning family. Back Propagation (BP) is a gradient-based algorithm that has some drawbacks such as slow convergence <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b48">49]</ref> and a tendency to get trapped in local minima <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>, making it unreliable for practical applications.</p><p>The ultimate goal of the learning process is to find the best combination of connection weights and biases in the NN to achieve the minimum error for training and test samples. However, often the error of MLP stays constantly large for some extended period of time during the learning process, as the learning algorithm leads MLPs to local minima rather than the global minimum. This problem is quite common in gradient-based learning approaches such as BP. The convergence of BP is also highly dependent on the initial values of learning rate and momentum. Unsuitable values for these variables may even result in divergence. There are many studies focused on resolving these problems of BP <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b71">72]</ref>, but there is no reported significant improvement, and each method has its own side effects. The literature shows that heuristic optimization methods are promising alternatives for gradient-based learning algorithms <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b77">78]</ref> since the stochastic nature of these algorithms allows them to avoid local minima better than gradient-based techniques and optimize challenging problems <ref type="bibr" target="#b43">[44]</ref>. Moreover, convergence rates of heuristic methods to the global minimum can be faster than BP, as investigated in <ref type="bibr" target="#b17">[18]</ref>.</p><p>Various heuristic optimization methods have been utilized to train FNNs, such as Particle Swarm Optimization (PSO) <ref type="bibr" target="#b35">[36]</ref>, Genetic Algorithm (GA) <ref type="bibr" target="#b56">[57]</ref>, Ant Colony Optimization (ACO) <ref type="bibr" target="#b5">[6]</ref>, and Evolutionary Strategies (ES) <ref type="bibr" target="#b74">[75]</ref>. It has been proven by the well-known No Free Lunch theorem (NFL) that there is no heuristic algorithm best suited to solving all optimization problems <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b38">39]</ref>. This theory on one hand, and the problems of gradient-based methods on another, motivates many researchers to investigate the effectiveness of different heuristic algorithms in learning MLPs and other different field <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>. In this paper the efficiency of the recently proposed heuristic algorithm, Biogeography-Based Optimization (BBO) <ref type="bibr" target="#b59">[60]</ref> is investigated in training MLPs. As discussed in Section 3, the BBO algorithm is an Evolutionary Algorithm (EA) that offers specific evolutionary mechanisms to each individual in a population. Generally speaking, the search space of an MLP is changed for different datasets. Therefore, EAs such as BBO and GAs can provide more flexible training procedures compared to gradient-based algorithms. Despite the merits of GAs for training MLPs, the variety of evolutionary operators of BBO for each individual potentially allow BBO to outperform a GA.</p><p>The mutation operator of BBO is another motivation for us to design a BBO-based trainer for MLPs. In contrast to Swarm Intelligence (SI) techniques (PSO and ACO for instance), EAs mostly have mutation operators, which enhances their exploitation capability. This potentially allows BO to outperform SI techniques in training MLPs as well. Moreover, different mutation constants for each individual in a population may also help BBO outperform a GA, which usually has a single mutation operator for the whole population. Finally, the intrinsically different adaptive mechanisms of evolutionary operators and mutations for each individual assist BBO to provide diverse exploration and exploitation behaviors when solving different problems.</p><p>There is currently little in the literature focusing on the efficiency of BBO in training MLPs. The only related work was by Ovreiu and Simon in 2010 <ref type="bibr" target="#b50">[51]</ref>. They trained a neuro-fuzzy network for classifying P wave features for the diagnosis of cardiomyopathy. However, the main focus was the application of a trained neuro-fuzzy network since the data set was a real data set. In this work we employ 11 standard datasets to provide a comprehensive test bed for investigating the abilities of BBO in training MLPs.</p><p>The rest of the paper is organized as follows: Section 2 presents a brief introduction to the MLP. Section 3 discusses the principles of Biogeography-Based Optimization. The method of applying BBO as a heuristic training algorithm for MLP is described in Section 4. The experimental results are discussed in Section 5. Finally, Section 6 provides concluding remarks and suggests some directions for future research.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref>. An MLP with one hidden layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Multi-Layer Perceptron</head><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows an MLP with three layers, where the number of input nodes is n, the number of hidden nodes is h, and the number of output nodes is m. It can be seen that there are one-way connections between the nodes, since the MLP belongs to the FNN family. The output of the MLP is calculated as follows:</p><p>The weighted sums of inputs are first calculated by Eq. (2.1).</p><formula xml:id="formula_0">s j ¼ X n i¼1 ðW ij :X i Þ À h j ; j ¼ 1; 2; . . . ; h ð2:1Þ</formula><p>where n is the number of the input nodes, W ij shows the connection weight from the ith node in the input layer to the jth node in the hidden layer, h j is the bias (threshold) of the jth hidden node, and X i indicates the ith input. The output of each hidden node is calculated as follows:</p><formula xml:id="formula_1">S j ¼ sigmoidðs j Þ ¼ 1 ð1 þ expðÀs j ÞÞ ; j ¼ 1; 2; . . . h ð2:2Þ</formula><p>After calculating the outputs of hidden nodes, the final outputs are defined as follows:</p><formula xml:id="formula_2">o k ¼ X h j¼1 ðW jk :S j Þ À h 0 k ; k ¼ 1; 2; . . . ; m ð2:3Þ O k ¼ sigmoidðo k Þ ¼ 1 ð1 þ expðÀo k ÞÞ ; k ¼ 1; 2; . . . ; m ð2:4Þ</formula><p>where w jk is the connection weight from the jth hidden node to the kth output node, and h 0 k is the bias (threshold) of the kth output node.</p><p>The most important parts of MLPs are the connection weights and biases. As may be seen in the above equations, the weights and biases define the final values of output. Training an MLP involves finding optimum values for weights and biases in order to achieve desirable outputs from certain given inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Biogeography-Based Optimization algorithm</head><p>The BBO algorithm was first proposed by Simon in 2008 <ref type="bibr" target="#b59">[60]</ref>. The basic idea of this algorithm was inspired by biogeography, which refers to the study of biological organisms in terms of geographical distribution (over time and space). The case studies might include different islands, lands, or even continents over decades, centuries, or millennia. In this field of study, different ecosystems (habitats or territories) are investigated to find the relationships between different species (habitants) in terms of immigration, emigration, and mutation. The evolution of ecosystems to reach a stable situation while considering different kinds of species (such as predator and prey), and the effects of migration and mutation was the main inspiration for the BBO algorithm.</p><p>Similarly to other EAs <ref type="bibr" target="#b64">[65]</ref><ref type="bibr" target="#b65">[66]</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref>, BBO employs a number of search agents called habitats. These habitats are analogous to chromosomes in GAs. The BBO algorithm assigns each habitat a vector of habitants (similar to genes in a GA), which represents the variables of problems. In addition, a Habitat Suitability Index (HSI) defines the overall fitness of a habitat. The higher the HSI, the more fit the habitat. The habitats evolve over time based on three main rules as follows <ref type="bibr" target="#b30">[31]</ref>:</p><p>Habitants living in habitats with high HSI are more likely to emigrate to habitats with low HSI. Habitats with low HSI are more prone to attract new immigrant habitants from those with high HSI. Habitats might face random changes in their habitants regardless of their HSI values.</p><p>In nature, these concepts bring a balance between different ecosystems. In other words nature tends to improve the overall stability of different geographical regions. The BBO algorithm utilizes these concepts to improve the HSI of all habitats, which results in evolving the initial random solutions for a particular problem.</p><p>The BBO algorithm starts with a random set of habitats. Each habitat has n different habitants that correspond to the number of variables of a particular problem. In addition, each habitat has its own immigration, emigration, and mutation rates. This mimics the characteristic of various geographically separated locations in nature.</p><p>Emigration (l k ) and immigration (k k ) are formulated as functions of the number of habitants as follows:</p><formula xml:id="formula_3">l k ¼ E Â n N ð3:1Þ k k ¼ I Â 1 À n N ð3:2Þ</formula><p>where n is the current number of habitants, N is the allowed maximum number of habitants, which is increased by HSI (the more suitable the habitat, the higher the number of habitants), E is the maximum emigration rate, and I indicates the maximum immigration rate. These latter two rates are depicted in Fig. <ref type="figure">2</ref>. It can be inferred from this figure that a high number of habitants coincides with a high probability of emigration and a low probability of immigration <ref type="bibr" target="#b19">[20]</ref>.</p><p>The third component of BBO, mutation, improves the exploration of BBO and keeps habitats as diverse as possible. This component is defined as follows:</p><formula xml:id="formula_4">m n ¼ M Â 1 À p n p max ð3:3Þ</formula><p>where M is an initial value for mutation defined by the user, p n is the mutation probability of the nth habitat, and</p><formula xml:id="formula_5">p max = argmax(p n ), n = 1,2,. . ., N.</formula><p>The general steps of the BBO algorithm are illustrated in the flowchart of Fig. <ref type="figure">2</ref>. This figure shows that the BBO algorithm starts with a random set of habitats. After calculating the HSI of each habitat, the emigration, immigration, and mutation rates are updated. The non-elite habitants are migrated and mutated according these rates. A pre-defined number of the best habitats are saved as elites for the next generation. Finally, the BBO algorithm is terminated by the satisfaction of a termination criterion. Note that elitism prevents the best solutions from being corrupted by immigration. To do this, we retain some of the best solutions (habitats) at each iteration. So, the best solutions can be recovered if their HSI is ruined by mutation.</p><p>Simon proved that the BBO algorithm is able to outperform some well-known heuristic algorithms such as PSO, GA, ACO, ES, and Probability-based incremental learning (PBIL) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b51">52]</ref> on fourteen benchmark functions and a real problem <ref type="bibr" target="#b59">[60]</ref>. He offered BBO as a competitive algorithm in the field of optimization <ref type="bibr" target="#b60">[61]</ref>. In the following sections BBO is first applied to an MLP, and then compared to PSO, GA, ACO, ES, PBIL, BP, and Extreme Learning Machine (ELM) on 11 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BBO for training an MLP</head><p>Generally, there are three methods of using a heuristic algorithm for training MLPs. Firstly, heuristic algorithms are utilized for finding a combination of weights and biases that provide the minimum error for an MLP. Secondly, heuristic algorithms are employed to find a proper architecture for an MLP in a particular problem. The last method is to use a heuristic algorithm to tune the parameters of a gradient-based learning algorithm, such as the learning rate and momentum.</p><p>In the first method, the architecture does not change during the learning process. The training algorithm is required to find proper values for all connection weights and biases in order to minimize the overall error of the MLP.</p><p>In the second approach, the structure of the MLPs varies. In this case, a training algorithm determines the best structure for solving a certain problem. Changing the structure can be accomplished by manipulating the connections between neurons, the number of hidden layers, and the number of hidden nodes in each layer. For example, Yu et al. employed PSO to define the structure of MLP to solve two real problems <ref type="bibr" target="#b78">[79]</ref>.</p><p>Leung et al. used the last method to tune the parameters of an FNN utilizing EAs <ref type="bibr" target="#b29">[30]</ref>. There are also some studies that utilized a combination of methods simultaneously. For instance, Mizuta et al. <ref type="bibr" target="#b47">[48]</ref> and Leung et al. <ref type="bibr" target="#b29">[30]</ref> employed a GA and improved GA to define the structure of an FNN.</p><p>In this study, the BBO algorithm is applied to an MLP using the first method. In order to design a BBO trainer for MLPs, the following main phases need to be completed:</p><p>1. Representation strategy: the weights and biases should be represented in the proper format (habitats) for BBO. 2. HSI: a fitness function using the error of the MLP should be defined to evaluate habitats. These phases are explained in detail in the following sections.  There are three methods for representing (encoding) the weights and biases: vector, matrix, and binary <ref type="bibr" target="#b79">[80]</ref>. In vector encoding, every agent is encoded as a vector. To train an MLP, each individual represents all the weights and biases. In matrix representation, every individual is encoded as a matrix. For the binary representation, agents are encoded as strings of binary bits. Each of these strategies has its own advantages and disadvantages that can be useful in a particular application <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generate initial habitats</head><p>In the first strategy, the encoding phase is easy, but the decoding process (decoding particles' vectors to weights and biases) is complicated. This method is often used for simple NNs. In the second strategy, the decoding stage is easy but the encoding is difficult for NNs with complex structures. This method is very suitable for learning algorithms from generalized NN toolboxes. In the third strategy, we need to represent variables in binary form. In this case, the length of each particle will be increased when the structure becomes more complex, so the process of decoding and encoding also becomes very complicated.</p><p>In this study, the vector method is used since we are not dealing with MLPs with complex structure. We do not use any generic toolboxes because the run time is much less for our hand-coded MLPs. As an example of this encoding strategy, the final vector of the MLP presented in Fig. <ref type="figure">3</ref>  After representing MLPs in the form of habitat vectors, an HSI formulation (fitness function) is required for evaluating each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Habitat Suitability Index (fitness function)</head><p>Generally speaking, the ultimate goal of learning methods is to train an MLP so that it is able to recognize training, validation, and test sets completely. The most important set in the learning phase is the training set. Each training sample should be involved in calculating the overall HSI of each individual. The following HSI function, which is the Mean Square Error (MSE) for all training samples, is utilized in this work:</p><formula xml:id="formula_6">E ¼ X q k¼1 P m i¼1 ðo k i À d k i Þ 2 q ð4:2Þ</formula><p>where q is the number of training samples, m is the number of outputs, d k i is the desired output of the ith input unit when the kth training sample is used, and o k i is the actual output of the ith input unit when the kth training sample appears in the input.</p><p>As an example, the HSI value for the ith habitat is calculated by (4.3):</p><formula xml:id="formula_7">HSIðHabitat i Þ ¼ EðHabitat i Þ ð4:3Þ</formula><p>Learning in the MLP can be formulated for BBO algorithms using these two phases. The flowchart of the proposed method is illustrated in Fig. <ref type="figure" target="#fig_1">4</ref>. As may be seen in this figure, the proposed method starts by generating a random set of MLPs based on the defined number of habitats. Each MLP corresponds to a habitat, and each weight/bias corresponds to habitants in the habitat. After the initialization step, the MSE for each MLP is calculated by Eq. (4.2). The next step is to update emigration, immigration, and mutation rates by Eqs. (3.1)-(3.3), respectively. Afterwards, the MLPs are combined based on the emigration and immigration rates. Each MLP is then mutated based on its habitat's mutation rate. The last step of the proposed method is elite selection, in which some of the best MLPs are saved in order to prevent them from being corrupted by the evolutionary and mutation operators in the next generation. These steps (from calculating MSE for every MLP to elitism) are iterated until satisfaction of a termination criterion.</p><p>The computational complexity of the proposed method depends on the number of training samples in datasets, the structure of the MLP, the number of habitats, the number of generations, the migration mechanism, mutation mechanism, and elitism mechanism. So, the overall computational complexity is as follows:</p><formula xml:id="formula_8">Fig. 3. MLP with the structure of 2-3-1. OðMLP; BBOÞ ¼ OðgðOðMLPÞ þ OðmigrationÞ þ OðmutationÞ þ OðelitismÞÞÞ ð4:4Þ</formula><p>where g is the maximum number of generations.</p><p>The computational complexity of an MLP with h hidden nodes, o outputs, and t training samples is equal to O(t(h + o)). The computational complexity of migration is of O(mn 2 ) in our implementation where m is the number of habitants and n is the number of habitats. Note that the migration operator is of O(mn) in the best case. The computational complexity of the mutation operator is of O(nm) in the worst case. Since we utilized quicksort to find the best habitats in the elitism phase, the computational complexity of the elitism is of O(nlogn) in the best case and O(n 2 ) in the worst case. Therefore, the final computational complexity of the proposed method is as follows:</p><formula xml:id="formula_9">OðMLP; BBOÞ ¼ Oðgðtðh þ oÞ þ mn 2 þ nm þ n 2 ÞÞ ð4:5Þ</formula><p>where g is the maximum number of generations, t is the number of training samples, h is the number of hidden nodes, o is the number of output nodes, m is the number of habitants, and n is the number of habitats.</p><p>To see how the proposed method works, a conceptual picture of the migration between the habitats for learning in an MLP using BBO is visualized in Fig. <ref type="figure" target="#fig_2">5</ref>. In this figure, habitat 1 is the fittest (least HSI, which indicates the least MSE for all training samples), followed by habitat 2, habitat 3, and habitat 4. The habitants represent the weights and biases of the MLPs. It can be seen that habitat 1 has the highest emigration, whereas habitat 4 provides the highest immigration. So the fourth habitat accepts many habitants (weights and biases) from other habitats as illustrated in different colors. The green nodes and connections also depict mutation which happen for all the habitats regardless of their HSI values. This example shows how the MLPs evolve using the proposed method.</p><p>To see how the proposed BBO-based technique theoretically has the potential to improve the training of MLPs, some observations are: Various values of emigration and immigration rates provide different evolutionary mechanism for every habitat, which promotes exploration. Higher exploration prevents BBO from easily getting trapped in local optima and provides higher probability of resolving stagnation situations (in case of local optima stagnation). The MSEs (HSI) of all MLPs (habitats) are improved over the generations since weights/biases of better MLPs tend to migrate to the worse MLPs. This guarantees the convergence of the proposed method and the improvement of all MLPs during generations. Different mutation rates for each habitat assist the proposed method to show diverse exploitation mechanisms. Elitism assists the proposed method to save and retrieve some of the best solutions, so they are never lost.</p><p>In the following section, a comparative study is conducted to investigate these theoretical claims in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and discussion</head><p>In this section the BBO algorithm is benchmarked on 5 classification and 6 function approximation datasets. The classification datasets are XOR, balloon, iris, breast cancer, and heart, obtained from the University of California at Irvine (UCI) Machine Learning Repository <ref type="bibr" target="#b4">[5]</ref>. The function approximation datasets are a one-dimensional sigmoid, one-dimensional cosine with one peak, one-dimensional sine with four peaks, two-dimensional sphere, two-dimensional Griewank, and five-dimensional Rosenbrock functions. The BBO algorithm is compared to PSO, GA, ACO, ES, and PBIL over these benchmark datasets in order to verify its performance. The comparison with BP and ELM are discussed in Sections 5.3 and 5.4, respectively. Note that the source codes of the BBO trainer are available in http://www.alimirjalili.com/Projects.html. It is assumed that every habitat is randomly initialized in the range [À10, 10]. The population size is 50 for the XOR and balloon datasets; and 200 for the iris, breast cancer, heart, and all function approximation datasets. Tables <ref type="table" target="#tab_3">2</ref> and<ref type="table" target="#tab_8">8</ref> show how the datasets are divided in terms of training and test sets. It can be seen that the training and test samples are similar for the XOR, balloon, and iris datasets. However, the test sets are different from the training sets in the breast cancer and heart datasets. The breast cancer dataset has 599 training samples and 100 test samples, while the heart dataset has 80 training samples and 187 test samples. It can be seen in Table <ref type="table" target="#tab_8">8</ref> that the training and test samples for the function approximation datasets are chosen using different step sizes from the domains of the functions. In most of the function approximation datasets the test samples are twice the training samples.</p><p>The other assumptions and initial values are presented in Table <ref type="table" target="#tab_2">1</ref>. Fine-tuning of the algorithms is beyond the scope of this paper. Each algorithm was run 10 times and the average (AVE) and standard deviation (STD) are reported in the table of results. These two measures indicate the ability of algorithms to avoid local minima. The lower the value of AVE ± STD, the greater the capability of the algorithm to avoid local minima. AVE indicates the average of MSE over 10 runs, so a lower value for this metric is evidence of an algorithm more successfully avoiding local optima and finding solutions near the global optimum. However, AVE is not a good metric alone because two algorithms can have equal averages, but have different performance in terms of finding the global optimum in each run. Therefore, STD (standard deviation) can help to determine the dispersion of results. The lower the STD, the lower the dispersion of results. So, AVE ± STD may be a good combination to indicate the performance of an algorithm in terms of avoiding local minima. Note that the best results are highlighted in bold type in Tables 3-7 and 9-15.</p><p>According to Derrac et al. <ref type="bibr" target="#b11">[12]</ref>, statistical tests should be conducted to properly evaluate the performance of heuristic algorithms. It is not enough to compare algorithms based on the mean and standard deviation values <ref type="bibr" target="#b14">[15]</ref>; a statistical test is necessary to prove that a proposed new algorithm presents a significant improvement over other existing methods for a particular problem <ref type="bibr" target="#b37">[38]</ref>.</p><p>In order to judge whether the results of BBO differ from PSO, GA, ACO, ES, and PBIL in a statistically significant way, a nonparametric statistical test, Wilcoxon's rank-sum test <ref type="bibr" target="#b75">[76]</ref>, was carried out at 5% significance level. The calculated p-values in the Wilcoxon's rank-sum are given in the results as well. In the tables, N/A indicates ''Not Applicable'' which means that the corresponding algorithm cannot be compared with itself in the rank-sum test. Conventionally, p-values less than 0.05 are considered as strong evidence against the null hypothesis. Note that p-values greater than 0.05 are underlined in the tables.</p><p>The other comparative measures shown in the results are: classification rates and test errors. We chose the best trained MLP among 10 runs and used it to classify or approximate the test set. To provide a fair comparison, all algorithms were terminated when a maximum number of iterations (250) was reached. Finally, the convergence behavior is also investigated in the results to provide a comprehensive comparison.</p><p>It should be noted that min-max normalization was used for those datasets containing data with different ranges. The normalization method was formulated as follows:</p><p>Suppose that we are going to map</p><formula xml:id="formula_10">x in the interval of [a, b] to [c, d].</formula><p>The normalization process is done by the following equation: x 0 ¼ ðx À aÞ Â ðd À cÞ ðb À aÞ ð5:1Þ</p><p>Regarding the structure of the MLPs, we used the suggestions in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref> for classification datasets, since there is no standard rule for selecting the number of hidden nodes, i.e.:</p><formula xml:id="formula_11">H ¼ 2 Â N þ 1 ð5:2Þ</formula><p>where N indicates the number of inputs and H is the number of hidden nodes. Note that we used 15 hidden nodes for function approximation datasets.</p><p>In the following sections the simulation results of benchmark datasets are presented and discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Classification problems</head><p>The classification problems are listed in Table <ref type="table" target="#tab_3">2</ref>. It can be seen that these datasets are chosen with various levels of difficulty: XOR is the simplest and the Heart dataset is the most difficult <ref type="bibr" target="#b61">[62]</ref>. Note that the larger the number of training samples, the lower the difficulty of the classification problem. However, with a large number of features, the size of the neural network is larger, so that more weights need to be correctly determined. The results of the training algorithms on these datasets are as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">XOR classification problem</head><p>The N-bit XOR problem is a famous non-linear benchmark problem. The problem is to recognize the number of ''1''s in the input vector. The XOR result of the input vector should be returned; if the input vector contains an odd number of ''1''s, the output is ''1''. If the input vector contains an even number of ''1''s, the output is ''0''.</p><p>We used an MLP with the structure 3-7-1 to solve this problem. The experimental results for this problem are shown in Table <ref type="table" target="#tab_4">3</ref>. It can be seen that the results of BBO and the GA are better than for the other training algorithms. There is no statistically significant difference between BBO and GA in terms of avoiding local minima, and the classification rate is 100% for both algorithms. However, as shown in Fig. <ref type="figure" target="#fig_3">6</ref>, the convergence rate of BBO is much faster than all other algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Balloon classification problem</head><p>This is a simple dataset that is based on different conditions of an experiment in blowing up a balloon. There are 16 instances with 4 attributes such as color, size, act, and age in this dataset. The attributes are in string format. There is one output that indicates whether the balloon is inflated or not. MLPs with the structure 4-9-1 were used for classifying this dataset. The experimental results are shown in Table <ref type="table" target="#tab_5">4</ref> and Fig. <ref type="figure">7</ref>.</p><p>The MSE results show that BBO has the best performance in terms of avoiding local minima, and this is confirmed to be statistically significantly better than for other algorithms. However, the classification rates are equal in this dataset. Fig. <ref type="figure">7</ref> shows the convergence curves of the algorithms. As may be observed, the fastest convergence rate is that of the BBO algorithm, followed by the GA and ACO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Iris classification problem</head><p>The Iris dataset has 150 samples that can be divided into three classes: Setosa, Versicolor, and Virginica. All samples have four features: sepal length, sepal width, petal length, and petal width. This dataset was solved using MLPs with the structure 4-9-3. The results of training algorithms to solve this problem are presented in Table <ref type="table">5</ref>.</p><p>According to the results of AVE, STD, and p-values, BBO provides the best ability to avoid local minima for this dataset. Moreover, this algorithm is able to classify 90% of the test samples, more than all the other algorithms. Fig. <ref type="figure" target="#fig_4">8</ref> illustrates the convergence curves for this dataset. As may be seen in this figure, the convergence rate of BBO is dramatically better than the other algorithms. Overall, the results of BBO are quite impressive for this dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4.">Breast cancer classification problem</head><p>This dataset was established by William H. Wolbergby from the University of Wisconsin Hospitals, Madison, and has 699 instances and 9 attributes such as clump thickness, uniformity of cell size, uniformity of cell shape, and marginal adhesion <ref type="bibr" target="#b32">[33]</ref>.</p><p>The output is equal to 2 for benign and 4 for malignant cancers. The architecture of MLPs for solving this dataset was 9-19-1. The experimental results of algorithms for this dataset are presented in Table <ref type="table" target="#tab_6">6</ref>. The averages and standard deviations indicate that BBO has the best ability to avoid local minima for this dataset. However, p-values show that the  results of BBO are not significantly better than the GA. The classification rate of the GA is also better. The results of BBO and GA are very close in this dataset, and they both significantly outperform the other algorithms. Fig. <ref type="figure">9</ref> shows the convergence curves of algorithm for this dataset. It is evident that BBO has the fastest convergence rate, followed by the GA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.5.">Heart classification problem</head><p>This dataset was created for diagnosing cardiac Single Proton Computed Tomography (SPECT) images. There are 267 images in this dataset, and 22 features are extracted from them to summarize these images. All the features are in binary  format and the output indicates whether the situation of a patient is normal or abnormal. We trained MLPs with the structure of 22-45-1 using 80 instances and examined them for the remaining 187 instances. The experimental results are provided in Table <ref type="table">7</ref>. The results of BBO are significantly better than all the other algorithms according to the MSEs and p-values. Moreover, BBO has the highest classification accuracy as well.</p><p>The convergence curves in Fig. <ref type="figure" target="#fig_5">10</ref> show that BBO has very fast convergence compared to the other algorithms.</p><p>Statistically speaking, BBO has the best results in all of the classification datasets in terms of avoiding local minima, significantly better than the other algorithms in 2 cases. The classification rates attained by BBO are better than the other algorithm in three out of five datasets. According to the convergence curves, BBO tends to have the fastest convergence behavior on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Function approximation problems</head><p>The function approximation datasets are presented in Table <ref type="table" target="#tab_8">8</ref>. In this section, MLPs with structures 1-15-1, 2-15-1, and 5-15-1 are trained for approximating the one, two, and five-dimensional functions, respectively. Similarly to the classification datasets, the function approximation datasets are also chosen with a range of difficulty to challenge the training algorithms. The dimension and training samples are increased which increases the level of difficulty for datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Sigmoid function</head><p>The sigmoid dataset is in the interval [À3, 3] with increments of 0.   the other algorithms. The test errors in Table <ref type="table">9</ref> and approximated curves in Fig. <ref type="figure" target="#fig_6">11</ref> show that the BBO algorithm has the best approximate accuracy as well. Fig. <ref type="figure" target="#fig_7">12</ref> shows that BBO has the fastest convergence rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Cosine function</head><p>This dataset has 31 training samples and 38 test samples. The results are shown in Table <ref type="table" target="#tab_9">10</ref>. It can be seen that the GA has the best results for AVE, STD, and test errors. However, the results of BBO are very close to this: the statistical tests show that the differences between the GA and BBO are not statistically significant. Figs. 13 and 14 illustrate the approximated curve and convergence, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3.">Sine function</head><p>This dataset is created by a four-peak sine function in the interval [À2p, 2p] with a step size of 0.1 for training samples and 0.05 for test samples. For training the algorithms use 126 samples and are tested by 252 samples. The experimental results are provided in Table <ref type="table" target="#tab_10">11</ref>, Figs. <ref type="figure" target="#fig_10">15</ref> and<ref type="figure" target="#fig_11">16</ref>. The results show that the BBO algorithm is significantly better than the other algorithms in terms of avoiding local minima. In addition, test errors and Fig. <ref type="figure" target="#fig_10">15</ref> verify the superior accuracy of this algorithm. Finally, the curves of Fig. <ref type="figure" target="#fig_11">16</ref> confirm that BBO provides the fastest convergence speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4.">Sphere function</head><p>The sphere function dataset consists of 441 training samples and 1681 test samples. The results of this dataset are reported in Table <ref type="table" target="#tab_11">12</ref>. The averages and standard deviations of the MSEs show that BBO is the most reliable algorithm in terms  of avoiding local minima. However, the results for the test error criterion and the best approximation surfaces in Fig. <ref type="figure" target="#fig_12">17</ref> indicate that the GA provides better accuracy. The convergence behaviors of the training algorithm are shown in Fig. <ref type="figure" target="#fig_13">18</ref>. The convergence rates are entirely consistent with the previous datasets: BBO has the fastest rate, followed by the GA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5.">Griewank function</head><p>The Griewank dataset includes 441 training instances and 1681 test samples. The results for this dataset are shown in Table <ref type="table" target="#tab_12">13</ref>, Figs. <ref type="figure" target="#fig_14">19</ref> and<ref type="figure" target="#fig_15">20</ref>. The results are quite similar to the previous datasets in that the BBO algorithm significantly   outperforms the other algorithms in terms of avoiding local minima. According to test errors, Figs. <ref type="figure" target="#fig_14">19</ref> and<ref type="figure" target="#fig_15">20</ref>, the BBO algorithm has the best approximation accuracy and convergence speed as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.6.">Rosenbrock function</head><p>This dataset includes 1024 training samples and 7776 test samples. The results for this dataset are provided in Table . 14. According to the values of AVE, STD, and p-values, the BBO algorithm significantly outperforms others in terms of avoiding         local minima for this dataset. The test errors show that BBO has the highest accuracy for approximating the Rosenbrock function. Fig. <ref type="figure" target="#fig_0">21</ref> shows that the BBO algorithm has the fastest convergence speed.</p><p>Statistically speaking, the BBO algorithm provides significant ability to avoid local minima and a high convergence rate on four out of six function approximation datasets. Moreover, the approximated functions using BBO are more accurate than the other training algorithms on four out of six function approximation datasets.</p><p>Overall, the statistical results of all comparative measures are presented in Fig. <ref type="figure">22</ref>. It appears that the BBO algorithm has significantly superior performance. The algorithm shows improved ability to avoid local minima in seven of the datasets. The classification rates and test errors of BBO are the best in eight of the datasets. Furthermore, BBO provides the fastest convergence rate in ten out of eleven datasets.</p><p>Note that the test errors of all algorithms are not very small for all function approximation datasets because the end criterion for all algorithms is the maximum number of iterations and the number of search agents is fixed throughout the experiments. Of course, increasing the number of iterations and population size would improve the absolute classification rate or test error but it was the comparative performance between algorithms that was of interest. Determining the ability of algorithms in terms of avoiding local minima in the classification or approximation search spaces was the main objective of this work, so we did not focus on finding the best maximum number of iterations and population size.</p><p>One of the things that can be inferred from the statistical results of Fig. <ref type="figure">22</ref> is the superior performance of BBO and the GA in all search spaces. This is possibly due to the nature of these algorithms which have migration and crossover strategies to avoid local minima. These operators cause abrupt changes in the candidate solutions that results in enhancing the exploration ability of BBO and the GA significantly. Since the problem of training MLPs has a very difficult search space and is changed for every dataset, these algorithms performed very well in this work because of their very good exploration. The poor results of ACO and PSO are also interesting, originating from the nature of these problems. ACO and PSO do not have operators that promote sudden changes in the candidate solutions, so they are trapped in local minima more often than BBO and the GA. Moreover, ACO uses a pheromone matrix that enhances reinforcement learning and exploitation, an advantage for combinatorial problems but again leading to a tendency toward local minima entrapment. The PSO algorithms are also highly dependent on the distribution of the initial swarm, and their prime motivators are based on attraction between members of the swarm. If many of the particles become trapped in a wide local optimum, there is little in the algorithm to prevent other particles from also being attracted and becoming trapped in that same local optimum. Another fact worth mentioning in the results is the poor performance of ES, especially on the function approximation datasets. The ES algorithm mainly relies on sophisticated mutation operators and the selection procedure of the individuals is deterministic compared to the GA. Consequently, the mechanism of selection and mutation mostly emphasizes exploitation rather than exploration which results in very poor outcomes on the datasets. The PBIL algorithm also performed very badly on most of the datasets. PBIL evolves a vector consisting of the entire population rather than each individual itself. This vector is updated based on the fittest individual and mutated randomly. This mechanism appears to provide less exploration than the GA, where the production of new individuals by crossover operators, introducing bulk changes in their structure, enhances exploration. This may be the main reason of PBIL's poor performance.</p><p>The reason for superior performance of the BBO algorithm compared to GA in the majority of the datasets (as suggested by the final statistical results of Fig. <ref type="figure">22</ref>) is due to the various emigration and immigration rates for habitats. In contrast to the GA that has a general reproduction rate for all individuals, the BBO algorithm assigns two rates (emigration and immigration) to each habitat, which results in it providing different evolutionary behaviors and eventually greater exploration. As the results of Tables 3-7 and 9-14 show, the significant superiority of the BBO algorithm is increased as the complexity of the data sets increases. This is again due to diverse evolutionary mechanisms (greater exploration) of the BBO algorithm that assists this algorithm to outperform the GA algorithm with a unified mechanism for reproduction. The superior performance of the BBO algorithm compared to PSO and ACO is due to the migration mechanism of this algorithm. The PSO and ACO algorithms are not equipped with an evolutionary operator, so they do not provide sudden changes in the candidate solutions, as discussed above. The BBO algorithm, however, promotes diverse abrupt change mechanisms for candidate solutions, making this algorithm able to outperform PSO and ACO in almost all of the datasets.</p><p>The superiority of BBO compared to other evolutionary algorithms employed in this paper (EA and PBIL) can be reasoned from both the exploration and exploitation mechanisms. The selection process of the EA algorithm is deterministic, and the main characteristic is sophisticated mutation operators. These two facts promote exploitation. The BBO algorithm, however, promotes exploration and exploitation simultaneously. On one hand, the various migration rates assist BBO with greater exploration. On the other hand, the different mutation rates for each habitat allow BBO to use different exploitation behaviors for every habitat. These are the main reasons for the results shown in Fig. <ref type="figure">22</ref> which shows BBO can outperform EA not only in terms of local minima avoidance but also convergence rate. Comparing the results of BBO and PBIL, we can come to the same conclusion. As discussed above, the PBIL evolves a vector consisting of the entire population rather than each individual itself. This vector is updated based on the fittest individual and mutated randomly. This mechanism appears to provide less exploration than BBO, where the production of new habitats by emigration and immigration operators enhances exploration. This is the main reason of BBO's significantly better performance.</p><p>To sum up, it can be stated that exploration is very important in the problem of training MLPs. There is a need to have more random and abrupt search steps (emphasizing exploration) to avoid local minima for solving complex data sets using MLPs. This study shows that the operators of BBO (migration) are highly suitable for addressing this issue.</p><p>To further investigate the efficiency of the BBO algorithm in training FNNs, a comparative study between this algorithm and the BP algorithm on all the datasets is provided in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparative study with the BP algorithm</head><p>The BP algorithm is a gradient-based algorithm that uses gradient information for minimizing the error function. We chose this algorithm for comparison because it is totally based on mathematical concepts, quite different from the other, bio-inspired algorithms. This can benchmark the performance of the proposed approach on a contrasting test bed. We solved the datasets using the BP algorithm 10 times and provide the statistical results in Table <ref type="table" target="#tab_14">15</ref>, as well as those of BBO. Note that the learning rate, momentum, and maximum number of iterations are set to 0.01, 0.001, and 250 respectively. It can be seen that the results of the BBO algorithm for AVE and STD are better than BP in most of the datasets. This shows the superior performance of BBO in terms of improved avoidance of local minima. The classification rates and test errors also show that BBO is better than BP in most of the datasets. However, the BP algorithm provided very competitive results for simple datasets. This shows that the performance of BP is degraded as the complexity of the search space is increased (due to the problem of entrapment in local minima).</p><p>The reasons for the better performance of BBO compared to BP and other heuristic algorithms are as follows:</p><p>Varying values of emigration and immigration rates provide diverse information exchange behavior between habitats, and consequently improve exploration. Over the course of generations, the HSI of all habitats are improved since habitants living in high-HSI habitats tend to migrate to the low-HSI habitats. This guarantees the convergence of BBO. Migration operators emphasize exploration and consequently prevent BBO from easily getting trapped in local minima. Different mutation rates keep habitats as diverse as possible.</p><p>Elitism assists BBO to save and retrieve some of the best solutions, so they are never lost.</p><p>In the following section the BBO algorithm is compared to a very effective method called ELM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparative study with Extreme Learning Machines (ELM)</head><p>ELM was first proposed by <ref type="bibr">Huang et al. in 2006 [24]</ref>. ELM can be considered as an MLP with one hidden layer without any need to tune the hidden layer <ref type="bibr" target="#b69">[70]</ref>. In ELM the hidden nodes are independent of each other and training samples <ref type="bibr" target="#b70">[71]</ref>. It has been demonstrated that this technique is very efficient for classification problems <ref type="bibr" target="#b24">[25]</ref>. We again chose this method with the purpose of benchmarking the proposed technique. We obtained the results from Huang et al. <ref type="bibr" target="#b23">[24]</ref> and Silva et al. <ref type="bibr" target="#b58">[59]</ref> in order to compare them with the BBO algorithm. The available results for three datasets that match our experiment are: Iris, Heart, and Cancer. Note that training and testing sample sizes of these three datasets are quite similar to those of this work.</p><p>As the results of these studies show, ELM is able to classify the Iris dataset with an accuracy of 95.60%. The BBO algorithm reached 90% accuracy for the Iris dataset. It should be noted that BBO achieved 90% accuracy with 9 hidden nodes, whereas ELM shows 95.60% accuracy with 15 hidden nodes. ELM also classified the Cancer and Heart datasets with accuracies of 96.09% and 80.48% respectively. Comparing these results with those of BBO (Tables <ref type="table" target="#tab_6">6</ref> and<ref type="table">7</ref>) shows BBO is capable of providing very competitive results compared to ELM. For ELM, the average MSE for the Cancer dataset is 0.06101 with standard deviation equal to 0.00014, which is much worse than that of BBO as shown in Table <ref type="table" target="#tab_6">6</ref>. This shows that BBO can show better local minima avoidance.</p><p>According to this comparative study and discussion, it appears that BBO is highly suitable for training MLPs. This algorithm successfully reduces the problem of entrapment in local minima in training MLPs, with very fast convergence rates. These improvements are accompanied by high classification rates and low test errors as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, an MLP was trained by the recently proposed BBO algorithm. Eleven benchmark datasets (balloon, iris, breast cancer, heart, sigmoid, cosine with one peak, sine with four peaks, sphere, Griewank, and Rosenbrock) were employed to investigate the effectiveness of BBO in training MLPs. The results were statistically compared with PSO, GA, ACO, ES, PBIL, BP, and ELM to verify the performance. The results demonstrate that BBO is significantly better able to avoid local minima </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4. 1 .</head><label>1</label><figDesc>Representing the MLP training problem for BBO (habitat formation)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Flow chart of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Conceptual model of migration between the habitats for training MLP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Convergence curves of algorithms for XOR dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Convergence curves of algorithms for iris dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Convergence curves of algorithms for heart dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Approximated curves for sigmoid function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Convergence curves of algorithms for sigmoid function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Approximated curves for cosine function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Convergence curves of algorithms for cosine function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Approximated curves for sine function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Convergence curves of algorithms for sine function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Test errors and approximated surfaces for sphere function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Convergence curves of algorithms for sphere function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Test errors and approximated surfaces for Griewank function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Convergence curves of algorithms for Griewank function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>is as follows: habitat ¼ ½w 13 w 23 w 14 w 24 w 15 w 25 w 36 w 46 w 56 h 1 h 2 h 3 h 4 ð 4:1Þ</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>The initial parameters of algorithms.</figDesc><table><row><cell>Algorithm</cell><cell>Parameter</cell><cell>Value</cell></row><row><cell>BBO</cell><cell>Habitat modification probability</cell><cell>1</cell></row><row><cell></cell><cell>Immigration probability bounds per gene</cell><cell>[0, 1]</cell></row><row><cell></cell><cell>Step size for numerical integration of probabilities</cell><cell>1</cell></row><row><cell></cell><cell>Max immigration (I) and Max emigration (E)</cell><cell>1</cell></row><row><cell></cell><cell>Mutation probability</cell><cell>0.005</cell></row><row><cell></cell><cell>Population size</cell><cell>50 for XOR and Balloon, 200 for the rest</cell></row><row><cell></cell><cell>Maximum number of generations</cell><cell>250</cell></row><row><cell>PSO</cell><cell>Topology</cell><cell>Fully connected</cell></row><row><cell></cell><cell>Cognitive constant (C 1 )</cell><cell>1</cell></row><row><cell></cell><cell>Social constant (C 2 )</cell><cell>1</cell></row><row><cell></cell><cell>Inertia constant (w)</cell><cell>0.3</cell></row><row><cell></cell><cell>Population size</cell><cell>50 for XOR and Balloon, 200 for the rest</cell></row><row><cell></cell><cell>Maximum number of iterations</cell><cell>250</cell></row><row><cell>GA</cell><cell>Type</cell><cell>Real coded</cell></row><row><cell></cell><cell>Selection</cell><cell>Roulette wheel</cell></row><row><cell></cell><cell>Crossover</cell><cell>Single point (probability = 1)</cell></row><row><cell></cell><cell>Mutation</cell><cell>Uniform (probability = 0.01)</cell></row><row><cell></cell><cell>Population size</cell><cell>50 for XOR and Balloon, 200 for the rest</cell></row><row><cell></cell><cell>Maximum number of generations</cell><cell>250</cell></row><row><cell>ACO</cell><cell>Initial pheromone (s 0 )</cell><cell>1 e À06</cell></row><row><cell></cell><cell>Pheromone update constant (Q)</cell><cell>2 0</cell></row><row><cell></cell><cell>Pheromone constant (q 0 )</cell><cell>1</cell></row><row><cell></cell><cell>Global pheromone decay rate (p g )</cell><cell>0.9</cell></row><row><cell></cell><cell>Local pheromone decay rate (p t )</cell><cell>0.5</cell></row><row><cell></cell><cell>Pheromone sensitivity (a)</cell><cell>1</cell></row><row><cell></cell><cell>Visibility sensitivity (b)</cell><cell>5</cell></row><row><cell></cell><cell>Population size</cell><cell>50 for XOR and Balloon, 200 for the rest</cell></row><row><cell></cell><cell>Maximum number of iterations</cell><cell>250</cell></row><row><cell>ES</cell><cell>k</cell><cell>10</cell></row><row><cell></cell><cell>r</cell><cell>1</cell></row><row><cell></cell><cell>Population size</cell><cell>50 for XOR and Balloon, 200 for the rest</cell></row><row><cell></cell><cell>Maximum number of generations</cell><cell>250</cell></row><row><cell>PBIL</cell><cell>Learning rate</cell><cell>0.05</cell></row><row><cell></cell><cell>Good population member</cell><cell>1</cell></row><row><cell></cell><cell>Bad population member</cell><cell>0</cell></row><row><cell></cell><cell>Elitism parameter</cell><cell>1</cell></row><row><cell></cell><cell>Mutational probability</cell><cell>0.1</cell></row><row><cell></cell><cell>Population size</cell><cell>50 for XOR and Balloon, 200 for the rest</cell></row><row><cell></cell><cell>Maximum number of generations</cell><cell>250</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>Classification datasets.</figDesc><table><row><cell>Classification datasets</cell><cell>Number of attributes</cell><cell>Number of training samples</cell><cell>Number of test samples</cell><cell>Number of classes</cell></row><row><cell>3-bits XOR</cell><cell>3</cell><cell>8</cell><cell>8 as training samples</cell><cell>2</cell></row><row><cell>Balloon</cell><cell>4</cell><cell>16</cell><cell>16 as training samples</cell><cell>2</cell></row><row><cell>Iris</cell><cell>4</cell><cell>150</cell><cell>150 as training samples</cell><cell>3</cell></row><row><cell>Breast cancer</cell><cell>9</cell><cell>599</cell><cell>100</cell><cell>2</cell></row><row><cell>Heart</cell><cell>22</cell><cell>80</cell><cell>187</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>Experimental results for XOR dataset.</figDesc><table><row><cell>Algorithm</cell><cell>MSE (AVE ± STD)</cell><cell>p-Values</cell><cell>Classification rate (%)</cell></row><row><cell>BBO</cell><cell>3.65eÀ07 ± 0.000000</cell><cell>N/A</cell><cell>100.00</cell></row><row><cell>PSO</cell><cell>0.084050 ± 0.035945</cell><cell>6.39eÀ05</cell><cell>37.50</cell></row><row><cell>GA</cell><cell>0.000181 ± 0.000413</cell><cell>0.1153</cell><cell>100.00</cell></row><row><cell>ACO</cell><cell>0.180328 ± 0.025268</cell><cell>6.39eÀ05</cell><cell>62.50</cell></row><row><cell>ES</cell><cell>0.118739 ± 0.011574</cell><cell>6.39eÀ05</cell><cell>62.50</cell></row><row><cell>PBIL</cell><cell>0.030228 ± 0.039668</cell><cell>6.39eÀ05</cell><cell>62.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Experimental results for balloon dataset. Convergence curves of algorithms for balloon dataset.</figDesc><table><row><cell>Algorithm</cell><cell cols="3">MSE (AVE ± STD)</cell><cell></cell><cell></cell><cell></cell><cell>p-Values</cell><cell>Classification rate (%)</cell></row><row><cell>BBO</cell><cell cols="3">8.09eÀ27 ± 1.51eÀ42</cell><cell></cell><cell></cell><cell></cell><cell>N/A</cell><cell>100.00</cell></row><row><cell>PSO</cell><cell cols="3">0.000585 ± 0.000749</cell><cell></cell><cell></cell><cell></cell><cell>1.83eÀ04</cell><cell>100.00</cell></row><row><cell>GA</cell><cell cols="3">5.08eÀ24 ± 1.06eÀ23</cell><cell></cell><cell></cell><cell></cell><cell>2.20eÀ03</cell><cell>100.00</cell></row><row><cell>ACO</cell><cell cols="3">0.004854 ± 0.007760</cell><cell></cell><cell></cell><cell></cell><cell>1.83eÀ04</cell><cell>100.00</cell></row><row><cell>ES</cell><cell cols="3">0.019055 ± 0.170260</cell><cell></cell><cell></cell><cell></cell><cell>1.83eÀ04</cell><cell>100.00</cell></row><row><cell>PBIL</cell><cell cols="3">2.49eÀ05 ± 5.27eÀ05</cell><cell></cell><cell></cell><cell></cell><cell>1.83eÀ04</cell><cell>100.00</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell>0</cell><cell></cell><cell cols="2">Balloon dataset</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell>-10</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>log(MSE)</cell><cell>10</cell><cell>-20</cell><cell></cell><cell>BBO PSO GA</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ACO</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ES</cell><cell></cell></row><row><cell></cell><cell></cell><cell>10</cell><cell>-30</cell><cell>50</cell><cell>100 PBIL</cell><cell>150</cell><cell>200</cell><cell>250</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Iteration</cell></row><row><cell cols="2">Fig. 7. Table 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Experimental results for iris dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell cols="3">MSE (AVE ± STD)</cell><cell></cell><cell></cell><cell></cell><cell>p-Values</cell><cell>Classification rate (%)</cell></row><row><cell>BBO</cell><cell cols="3">0.019150 ± 3.66eÀ18</cell><cell></cell><cell></cell><cell></cell><cell>N/A</cell><cell>90.00</cell></row><row><cell>PSO</cell><cell cols="3">0.228680 ± 0.057235</cell><cell></cell><cell></cell><cell></cell><cell>6.39eÀ05</cell><cell>37.33</cell></row><row><cell>GA</cell><cell cols="3">0.089912 ± 0.123638</cell><cell></cell><cell></cell><cell></cell><cell>6.39eÀ05</cell><cell>89.33</cell></row><row><cell>ACO</cell><cell cols="3">0.405979 ± 0.053775</cell><cell></cell><cell></cell><cell></cell><cell>6.39eÀ05</cell><cell>32.66</cell></row><row><cell>ES</cell><cell cols="3">0.314340 ± 0.052142</cell><cell></cell><cell></cell><cell></cell><cell>6.39eÀ05</cell><cell>46.66</cell></row><row><cell>PBIL</cell><cell cols="3">0.116067 ± 0.036355</cell><cell></cell><cell></cell><cell></cell><cell>6.39eÀ05</cell><cell>86.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Experimental results for breast cancer dataset. Convergence curves of algorithms for breast cancer dataset.</figDesc><table><row><cell>Algorithm</cell><cell>MSE (AVE ± STD)</cell><cell>p-Values</cell><cell>Classification rate (%)</cell></row><row><cell>BBO</cell><cell>0.002807 ± 0.000000</cell><cell>N/A</cell><cell>95.00</cell></row><row><cell>PSO</cell><cell>0.034881 ± 0.002472</cell><cell>6.39eÀ05</cell><cell>11.00</cell></row><row><cell>GA</cell><cell>0.003026 ± 0.001500</cell><cell>1.0000</cell><cell>98.00</cell></row><row><cell>ACO</cell><cell>0.013510 ± 0.002137</cell><cell>6.39eÀ05</cell><cell>40.00</cell></row><row><cell>ES</cell><cell>0.040320 ± 0.002470</cell><cell>6.39eÀ05</cell><cell>06.00</cell></row><row><cell>PBIL</cell><cell>0.032009 ± 0.003065</cell><cell>6.39eÀ05</cell><cell>07.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>1, so the number of training data is 61. The number of test samples is 121, lying in the same range. The results of training algorithms for this dataset are presented in Table 9, Figs. 11 and 12. The results for AVE, STD, and p-values indicate that BBO is much better at avoiding local minima than</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc>Function approximation datasets.</figDesc><table><row><cell>Function approximation datasets</cell><cell></cell><cell>Training samples</cell><cell>Test samples</cell></row><row><cell>Sigmoid: y = 1/(1 + e Àx )</cell><cell></cell><cell>61: x in [À3:0.1:3]</cell><cell>121: x in [À3:0.05:3]</cell></row><row><cell>Cosine: y = (cos (xp/2)) 7</cell><cell></cell><cell>31: x in [1.25:0.05:2.75]</cell><cell>38: x in [1.25:0.04:2.75]</cell></row><row><cell cols="2">Sine: y = sin (2x) Sphere: z ¼ P 2 i¼1 x 2 i ; x ¼ x 1 and y = x 2 Griewank: z ¼ 1 4000 P 2 i¼1 x 2 i À P 2 i¼1 cos xi ffi p i Rosenbrock: z ¼ P i¼1 ½x 2 i À 10 cosð2px i Þ þ 10; i ¼ 1; 2; 3; 4; 5 þ 1; x ¼ x1 and y = x 2</cell><cell>126: x in [À2p:0.1:2p] 21 Â 21: x, y in [À2:0.2:2] 21 Â 21: x, y in [À1:0.1:1] 1024: x 1 À x 5 in [À5:3:5]</cell><cell>252: x in [À2p:0.05:2p] 41 Â 41: x, y in [À2:0.1:2] 41 Â 41: x, y in [À1:.05:1] 7776: x 1 À x 5 in [À5:2:5]</cell></row><row><cell>Table 9</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Experimental results for sigmoid dataset (one dimensional).</cell><cell></cell><cell></cell></row><row><cell>Algorithm</cell><cell>MSE (AVE ± STD)</cell><cell>p-Values</cell><cell>Test error</cell></row><row><cell>BBO</cell><cell>1.33eÀ05 ± 3.57eÀ21</cell><cell>N/A</cell><cell>0.14381</cell></row><row><cell>PSO</cell><cell>0.022989 ± 0.009429</cell><cell>6.39eÀ05</cell><cell>3.35630</cell></row><row><cell>GA</cell><cell>0.001093 ± 0.000916</cell><cell>6.39eÀ05</cell><cell>0.44969</cell></row><row><cell>ACO</cell><cell>0.023532 ± 0.010042</cell><cell>6.39eÀ05</cell><cell>3.99740</cell></row><row><cell>ES</cell><cell>0.075575 ± 0.016410</cell><cell>6.39eÀ05</cell><cell>8.80150</cell></row><row><cell>PBIL</cell><cell>0.004046 ± 2.74eÀ17</cell><cell>6.34eÀ05</cell><cell>2.9446</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10</head><label>10</label><figDesc>Experimental results for one-peak cosine dataset (one dimensional).</figDesc><table><row><cell>Algorithm</cell><cell>MSE (AVE ± STD)</cell><cell>p-Values</cell><cell>Test error</cell></row><row><cell>BBO</cell><cell>0.013674 ± 1.83eÀ18</cell><cell>0.4429</cell><cell>1.4904</cell></row><row><cell>PSO</cell><cell>0.058986 ± 0.021041</cell><cell>0.0001</cell><cell>2.0090</cell></row><row><cell>GA</cell><cell>0.010920 ± 0.006316</cell><cell>N/A</cell><cell>0.7105</cell></row><row><cell>ACO</cell><cell>0.050872 ± 0.010809</cell><cell>0.0001</cell><cell>2.4498</cell></row><row><cell>ES</cell><cell>0.086640 ± 0.022208</cell><cell>0.0001</cell><cell>3.1461</cell></row><row><cell>PBIL</cell><cell>0.094342 ± 0.018468</cell><cell>0.0001</cell><cell>3.7270</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11</head><label>11</label><figDesc>Experimental results for four-peak sine dataset (one dimensional).</figDesc><table><row><cell>Algorithm</cell><cell>MSE (AVE ± STD)</cell><cell>p-Values</cell><cell>Test error</cell></row><row><cell>BBO</cell><cell>0.102710 ± 0.000000</cell><cell>N/A</cell><cell>64.261</cell></row><row><cell>PSO</cell><cell>0.526530 ± 0.072876</cell><cell>6.39eÀ05</cell><cell>124.89</cell></row><row><cell>GA</cell><cell>0.421070 ± 0.061206</cell><cell>6.39eÀ05</cell><cell>111.25</cell></row><row><cell>ACO</cell><cell>0.529830 ± 0.053305</cell><cell>6.39eÀ05</cell><cell>117.71</cell></row><row><cell>ES</cell><cell>0.706980 ± 0.077409</cell><cell>6.39eÀ05</cell><cell>142.31</cell></row><row><cell>PBIL</cell><cell>0.483340 ± 0.007935</cell><cell>6.39eÀ05</cell><cell>149.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12</head><label>12</label><figDesc>Experimental results for sphere dataset (two dimensional).</figDesc><table><row><cell>Algorithm</cell><cell>MSE (AVE ± STD)</cell><cell>p-Values</cell><cell>Test error</cell></row><row><cell>BBO</cell><cell>1.7740 ± 2.34eÀ16</cell><cell>N/A</cell><cell>770.4425</cell></row><row><cell>PSO</cell><cell>7.1094 ± 0.8528</cell><cell>6.38eÀ05</cell><cell>1.2686e+03</cell></row><row><cell>GA</cell><cell>2.9276 ± 0.7289</cell><cell>6.38eÀ05</cell><cell>452.3744</cell></row><row><cell>ACO</cell><cell>6.5626 ± 0.9543</cell><cell>6.38eÀ05</cell><cell>1.222e+03</cell></row><row><cell>ES</cell><cell>10.530 ± 0.8427</cell><cell>6.38eÀ05</cell><cell>1.500e+03</cell></row><row><cell>PBIL</cell><cell>10.459 ± 0.8529</cell><cell>6.38eÀ05</cell><cell>1.805e+03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13</head><label>13</label><figDesc>Experimental results for Griewank dataset (two dimensional).</figDesc><table><row><cell>Algorithm</cell><cell>MSE (AVE ± STD)</cell><cell>p-Values</cell><cell>Test error</cell></row><row><cell>BBO</cell><cell>2.02560 ± 0.0000</cell><cell>N/A</cell><cell>5.0200e+03</cell></row><row><cell>PSO</cell><cell>16.9043 ± 1.1482</cell><cell>6.38eÀ05</cell><cell>7.6328e+03</cell></row><row><cell>GA</cell><cell>16.4218 ± 1.3306</cell><cell>6.38eÀ05</cell><cell>1.0585e+04</cell></row><row><cell>ACO</cell><cell>14.9586 ± 1.2976</cell><cell>6.38eÀ05</cell><cell>6.1790e+03</cell></row><row><cell>ES</cell><cell>19.1069 ± 1.4984</cell><cell>6.38eÀ05</cell><cell>1.1185e+04</cell></row><row><cell>PBIL</cell><cell>13.5587 ± 0.7532</cell><cell>6.29eÀ05</cell><cell>1.2409e+04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14</head><label>14</label><figDesc>Experimental results for Rosenbrock dataset (five-dimensional). Convergence curves of algorithms for Rosenbrock function.</figDesc><table><row><cell>Algorithm</cell><cell cols="5">MSE (AVE ± STD)</cell><cell></cell><cell></cell><cell>p-Values</cell><cell>Test error</cell></row><row><cell>BBO</cell><cell cols="5">104.4251 ± 1.59eÀ14</cell><cell></cell><cell></cell><cell>N/A</cell><cell>2.3503e+06</cell></row><row><cell>PSO</cell><cell cols="5">215.1669 ± 3.5521</cell><cell></cell><cell></cell><cell>0.007937</cell><cell>6.4955e+09</cell></row><row><cell>GA</cell><cell cols="5">264.6713 ± 16.3603</cell><cell></cell><cell></cell><cell>0.007937</cell><cell>5.2351e+06</cell></row><row><cell>ACO</cell><cell cols="5">200.9749 ± 7.0641</cell><cell></cell><cell></cell><cell>0.007937</cell><cell>3.5399e+07</cell></row><row><cell>ES</cell><cell cols="5">252.8454 ± 8.1445</cell><cell></cell><cell></cell><cell>0.007937</cell><cell>7.3733e+09</cell></row><row><cell>PBIL</cell><cell cols="5">208.6148 ± 4.6813</cell><cell></cell><cell></cell><cell>0.007937</cell><cell>2.3463e+08</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Rosenbrock function</cell></row><row><cell></cell><cell></cell><cell></cell><cell>10</cell><cell>2.4</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>log(MSE)</cell><cell>10 10</cell><cell>2.2 2.3</cell><cell>BBO PSO</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GA</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>10</cell><cell>2.1</cell><cell>ACO ES</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PBIL</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Iteration</cell></row><row><cell cols="2">Fig. 21. Comparison measures</cell><cell>Number of dataset</cell><cell cols="2">0 2 4 6 8 10 12 14 16 18 20 22 24 26 28</cell><cell>BBO</cell><cell>PSO</cell><cell cols="2">GA Training algorithm ACO</cell><cell>ES</cell><cell>PBIL</cell></row><row><cell cols="2">Significant AVE ± STD</cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="5">Classification rate and test error</cell><cell>8</cell><cell>1</cell><cell>5</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Convergence rate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1 0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2 6</cell><cell>1</cell><cell>6</cell><cell>1</cell><cell>1</cell><cell>1</cell></row></table><note><p><p><p>Fig.</p><ref type="bibr" target="#b21">22</ref></p>. Final statistical results (each bar represents the number of datasets each algorithm ''won'' on a variety of measures).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15</head><label>15</label><figDesc>Comparison results between BBO and BP. GA, ACO, ES, and PBIL. Moreover, the superior performance of BBO for training MLPs in terms of accuracy of results and convergence rate can clearly be seen from the results.In future studies, it would be interesting to investigate the efficiency of BBO in training other types of NNs such as recurrent, Kohonen, or Radial basis function (RBF) networks. In addition, employing BBO to define the structure of MLPs is worth investigating.</figDesc><table><row><cell>Dataset</cell><cell>Algorithm</cell><cell>MSE (AVE ± STD)</cell><cell>p-Values</cell><cell>Classification rate/test error</cell></row><row><cell>XOR</cell><cell>BBO</cell><cell>3.65eÀ07 ± 0.000000</cell><cell>6.3864eÀ005</cell><cell>100.00%</cell></row><row><cell></cell><cell>BP</cell><cell>1.17eÀ03 ± 4.12eÀ04</cell><cell></cell><cell>100.00%</cell></row><row><cell>Balloon</cell><cell>BBO</cell><cell>8.09eÀ27 ± 1.51eÀ42</cell><cell>6.3864eÀ005</cell><cell>100.00%</cell></row><row><cell></cell><cell>BP</cell><cell>3.50eÀ06 ± 1.44eÀ06</cell><cell></cell><cell>100.00%</cell></row><row><cell>Iris</cell><cell>BBO</cell><cell>0.019150 ± 3.66eÀ18</cell><cell>0.0014</cell><cell>90.00%</cell></row><row><cell></cell><cell>BP</cell><cell>0.016960 ± 0.00170</cell><cell></cell><cell>78.66%</cell></row><row><cell>Breast cancer</cell><cell>BBO</cell><cell>0.002807 ± 0.000000</cell><cell>6.3864eÀ005</cell><cell>95.00%</cell></row><row><cell></cell><cell>BP</cell><cell>0.072640 ± 0.005612</cell><cell></cell><cell>94.00%</cell></row><row><cell>Heart</cell><cell>BBO</cell><cell>0.065625 ± 1.46eÀ17</cell><cell>0.0049</cell><cell>76.25%</cell></row><row><cell></cell><cell>BP</cell><cell>0.053650 ± 8.78eÀ03</cell><cell></cell><cell>72.50%</cell></row><row><cell>Sigmoid</cell><cell>BBO</cell><cell>1.33eÀ05 ± 3.57eÀ21</cell><cell>6.3864eÀ005</cell><cell>0.14381</cell></row><row><cell></cell><cell>BP</cell><cell>3.70eÀ04 ± 1.26eÀ04</cell><cell></cell><cell>1.3894</cell></row><row><cell>Cosine</cell><cell>BBO</cell><cell>0.013674 ± 1.83eÀ18</cell><cell>1.8267eÀ004</cell><cell>1.4904</cell></row><row><cell></cell><cell>BP</cell><cell>0.007932 ± 0.003799</cell><cell></cell><cell>2.5663</cell></row><row><cell>Sine</cell><cell>BBO</cell><cell>0.102710 ± 0.000000</cell><cell>6.3864eÀ005</cell><cell>64.261</cell></row><row><cell></cell><cell>BP</cell><cell>0.020270 ± 0.00745</cell><cell></cell><cell>23.3423</cell></row><row><cell>Sphere</cell><cell>BBO</cell><cell>1.774000 ± 2.34eÀ16</cell><cell>1.8267eÀ004</cell><cell>770.4425</cell></row><row><cell></cell><cell>BP</cell><cell>1.544630 ± 0.396995</cell><cell></cell><cell>1.3618e+03</cell></row><row><cell>Griewank</cell><cell>BBO</cell><cell>2.025600 ± 0.0000</cell><cell>6.3864eÀ005</cell><cell>5.0200e+03</cell></row><row><cell></cell><cell>BP</cell><cell>3.325595 ± 6.1857</cell><cell></cell><cell>9.8313e+003</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Professor Dan Simon for his comments and recommendations throughout this project. The first author also gratefully thanks Hossein Agha Moradian Sardroudi for his support during this project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An optimized virtual network mapping using PSO in cloud computing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Abedifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<idno type="DOI">10.1109/IranianCEE.2013.6599723</idno>
		<ptr target="http://dx.doi.org/10.1109/IranianCEE.2013.6599723" />
	</analytic>
	<monogr>
		<title level="m">Electrical Engineering (ICEE), 2013 21st Iranian Conference on</title>
		<imprint>
			<date type="published" when="2013-05-16">14-16 May, 2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An adaptive conjugate gradient learning algorithm for efficient training of neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Comput</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="81" to="102" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A learning rule for very simple universal approximators consisting of a single layer of perceptrons</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Burgsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="786" to="795" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parameter selection algorithm with self adaptive growing neural network classifier for diagnosis issues</title>
		<author>
			<persName><forename type="first">M</forename><surname>Barakat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lefebvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Druaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mustapha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Mach. Learn. Cybern</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="217" to="233" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">UCI Repository of Machine Learning Databases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="&lt;http://www.ics.uci.edu/~mlearn/MLRepository.html&gt;" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Training feed-forward neural networks with ant colony optimization: an application to pattern classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Socha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey on optimization metaheuristics</title>
		<author>
			<persName><forename type="first">I</forename><surname>Boussaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lepagnot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Siarry</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2013.02.041</idno>
		<idno>02.041 (10.07.13</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ins" />
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<idno type="ISSN">0020-0255</idno>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page" from="82" to="117" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Evolutionary Algorithms for Neural Network Design and Training</title>
		<author>
			<persName><forename type="first">J</forename><surname>Branke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conjugate gradient algorithm for efficient training of artificial neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Charalambous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET</title>
		<imprint>
			<biblScope unit="page" from="301" to="310" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Approximation with Artificial Neural Networks</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Csáji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<pubPlace>Hungary</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Faculty of Sciences, Etvs Lornd University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Michalewicz</surname></persName>
		</author>
		<title level="m">Evolutionary Algorithms in Engineering Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Derrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm Evol. Comput</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An Empirical Study of Learning Speed in Back-propagation Networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fahlman</surname></persName>
		</author>
		<ptr target="&lt;http://repository.cmu.edu/cgi/viewcontent.cgi?article=2799&amp;context=compsci&gt;" />
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Fine</surname></persName>
		</author>
		<title level="m">Feedforward Neural Network Methodology</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A study on the use of non-parametric tests for analyzing the evolutionary algorithms&apos; behaviour: a case study on the CEC&apos;2005 special session on real parameter optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Heuristics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="617" to="644" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Artificial neural networks (the multilayer perceptron)-a review of applications in the atmospheric sciences</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Dorling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atmos. Environ</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2627" to="2636" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the problem of local minima in backpropagation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="76" to="86" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Comparison of particle swarm optimization and backpropagation as training algorithms for neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Gudise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Venayagamoorthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="110" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sparsely connected neural network-based time series forecasting</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2012.01.011</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ins.2012.01.011(15.06.12" />
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<idno type="ISSN">0020-0255</idno>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="54" to="71" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An analysis of the migration rates for biogeography-based optimization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2013.07.018</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ins.2013.07.018(01.01.14" />
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<idno type="ISSN">0020-0255</idno>
		<imprint>
			<biblScope unit="volume">254</biblScope>
			<biblScope unit="page" from="111" to="140" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training feedforward networks with the Marquardt algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Hagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Menhaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="989" to="993" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simple explanation of the no-free-lunch theorem and its implications</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Pepyne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Optim. Theory Appl</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="549" to="570" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Extreme learning machine: theory and applications</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="489" to="501" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extreme learning machines: a survey</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Mach. Learn. Cybern</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Progress in supervised neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Horne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="8" to="39" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ismail Wdaa</surname></persName>
		</author>
		<title level="m">Differential Evolution for Neural Network Learning Enhancement, Master</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Universiti Teknologi Malaysia (UTM)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Increased rates of convergence through learning rate adaptation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An analysis of premature saturation in back propagation learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="719" to="728" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tuning of the structure and parameters of a neural network using an improved genetic algorithm</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H F</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K S</forename><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="79" to="88" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Variations of biogeography-based optimization and Markov analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2012.07.007</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ins.2012.07.007(20.01.13" />
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<idno type="ISSN">0020-0255</idno>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="492" to="506" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the alleviation of the problem of local minima in back-propagation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Magoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vrahatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Androulakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Anal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4545" to="4550" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wolberg</surname></persName>
		</author>
		<title level="m">Cancer Diagnosis via Linear Programming</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison, Computer Sciences Department</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A logical calculus of the ideas immanent in nervous activity</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pitts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Math. Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Genetic optimization of modular neural networks with fuzzy response integration for human recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Patricia Melin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Castillo</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2012.02.027</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ins.2012.02.027(15.08.12" />
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<idno type="ISSN">0020-0255</idno>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Particle swarms for feedforward neural network training, in: IJCNN&apos;02</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 International Joint Conference on Neural Networks</title>
		<meeting>the 2002 International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1895" to="1899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hybrid Particle Swarm Optimization and Gravitational Search Algorithm for Multilayer Perceptron Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Master</publisher>
		</imprint>
		<respStmt>
			<orgName>Universiti Teknologi Malaysia (UTM</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">S-shaped versus V-shaped transfer functions for binary particle swarm optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.swevo.2012.09.002</idno>
		<ptr target="http://dx.doi.org/10.1016/j.swevo.2012.09.002" />
	</analytic>
	<monogr>
		<title level="j">Swarm Evol. Comput</title>
		<idno type="ISSN">2210-6502</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2013-04">2013. April 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A new hybrid PSOGSA algorithm for function optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z M</forename><surname>Hashim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 International Conference on Computer and Information Application (ICCIA)</title>
		<imprint>
			<date type="published" when="2010-12">December 2010</date>
			<biblScope unit="page">377</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Grey wolf optimizer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.advengsoft.2013.12.007</idno>
		<ptr target="http://dx.doi.org/10.1016/j.advengsoft.2013.12.007" />
	</analytic>
	<monogr>
		<title level="j">Adv. Eng. Softw</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="46" to="61" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Binary bat algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-013-1525-5</idno>
		<ptr target="http://dx.doi.org/10.1007/s00521-013-1525-5" />
	</analytic>
	<monogr>
		<title level="m">press</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Training feedforward neural networks using hybrid particle swarm optimization and gravitational search algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Mohd Hashim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Moradian</forename><surname>Sardroudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Comput</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="11125" to="11137" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Light property and optical buffer performance enhancement using particle swarm optimization in oblique Ring-Shape-Hole photonic crystal waveguide</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Abedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Photonics Global Conference</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optical buffer performance enhancement using particle swarm optimization in ring-shape-hole photonic crystal waveguide</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Abedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijleo.2013.04.114</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ijleo.2013.04.114" />
	</analytic>
	<monogr>
		<title level="j">Optik -Int. J. Light Electron Opt</title>
		<idno type="ISSN">0030-4026</idno>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="5989" to="5993" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Oval-Shaped-Hole photonic crystal waveguide design by MoMIR framework</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<idno type="DOI">10.1109/LPT.2014.2302478</idno>
		<ptr target="http://dx.doi.org/10.1109/LPT.2014.2302478" />
	</analytic>
	<monogr>
		<title level="m">press</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A novel multi-objective optimization framework for designing photonic crystal waveguides</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.1109/LPT.2013.2290318</idno>
		<ptr target="http://dx.doi.org/10.1109/LPT.2013.2290318" />
	</analytic>
	<monogr>
		<title level="j">Photonics Technol. Lett. IEEE</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="146" to="149" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A tri-objective particle swarm optimizer for designing line defect photonic crystal waveguides</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Abedi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.photonics.2013.11.001</idno>
		<ptr target="http://dx.doi.org/10.1016/j.photonics.2013.11.001" />
	</analytic>
	<monogr>
		<title level="j">Photon Nanostruct. Fundam. Appl</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Structure design of neural networks using genetic algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mizuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shimizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="161" to="176" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Fast convergence for backpropagation network with magnified gradient function</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>IEEE</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1903" to="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised learning in neural computation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoret. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">287</biblScope>
			<biblScope unit="page" from="187" to="207" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Biogeography-based optimization of neuro-fuzzy system parameters for diagnosis of cardiac disease</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ovreiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic and Evolutionary Computation Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1235" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Parmee</surname></persName>
		</author>
		<title level="m">Evolutionary and Adaptive Computing in Engineering Design</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Marks</surname></persName>
		</author>
		<title level="m">Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">The Perceptron, A Perceiving and Recognizing Automaton Project Para</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
		</imprint>
		<respStmt>
			<orgName>Cornell Aeronautical Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unit cell topology optimization of line defect photonic crystal waveguide</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.protcy.2013.12.472</idno>
		<ptr target="http://dx.doi.org/10.1016/j.protcy.2013.12.472" />
	</analytic>
	<monogr>
		<title level="j">Procedia Technol</title>
		<idno type="ISSN">2212-0173</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="174" to="179" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Chaotic krill herd optimization algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.protcy.2013.12.473</idno>
		<ptr target="http://dx.doi.org/10.1016/j.protcy.2013.12.473" />
	</analytic>
	<monogr>
		<title level="j">Procedia Technol</title>
		<idno type="ISSN">2212-0173</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="180" to="185" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Multiple Layer Perceptron Training using Genetic Algorithms</title>
		<author>
			<persName><forename type="first">U</forename><surname>Seiffert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Citeseer</publisher>
			<biblScope unit="page" from="159" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<title level="m">Unsupervised Learning: Foundations of Neural Computation</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">An evolutionary extreme learning machine based on group search optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pacifico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ludermir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Congress on Evolutionary Computation (CEC)</title>
		<imprint>
			<biblScope unit="page" from="574" to="580" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Biogeography-based optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="702" to="713" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Analytical and numerical comparisons of biogeography-based optimization and genetic algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rarick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ergezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2010.12.006</idno>
		<ptr target="http://dx.doi.org/10.1016/j.ins.2010.12.006(01.04.11" />
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<idno type="ISSN">0020-0255</idno>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1224" to="1248" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<ptr target="&lt;http://archive.ics.uci.edu/ml/datasets.html&gt;" />
		<title level="m">UCI Machine Learning Repository</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Improving the convergence of the back-propagation algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Ooyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nienhuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="465" to="471" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Accelerating the convergence of the back-propagation method</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Vogl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mangis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alkon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="257" to="263" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Incorporating mutation scheme into krill herd algorithm for global numerical optimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-012-1304-8</idno>
		<ptr target="http://dx.doi.org/10.1007/s00521-012-1304-8" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A chaotic particle-swarm krill herd algorithm for global numerical optimization</title>
		<author>
			<persName><forename type="first">G.-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Alavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kybernetes</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="962" to="978" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">An effective krill herd algorithm with migration operator in biogeography-based optimization</title>
		<author>
			<persName><forename type="first">G.-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Alavi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.apm.2013.10.052</idno>
		<ptr target="http://dx.doi.org/10.1016/j.apm.2013.10.052" />
	</analytic>
	<monogr>
		<title level="m">press</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Stud krill herd algorithm</title>
		<author>
			<persName><forename type="first">G.-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Alavi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2013.08.031</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neucom.2013.08.031" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="363" to="370" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Hybrid krill herd algorithm with differential evolution for global numerical optimization</title>
		<author>
			<persName><forename type="first">G.-G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gandomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-S</forename><surname>Hao</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-013-1485-9</idno>
		<ptr target="http://dx.doi.org/10.1007/s00521-013-1485-9" />
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Upper integral network with extreme learning mechanism</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2520" to="2525" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Architecture selection for networks trained with extreme learning machine using localized generalization error model</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">Y</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="3" to="9" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A method for self-determination of adaptive learning rates in back propagation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="371" to="379" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Werbos</surname></persName>
		</author>
		<title level="m">Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences</title>
		<imprint>
			<date type="published" when="1974">1974</date>
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Neurocontrol and supervised learning: An overview and evaluation, Handbook Intell</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Control</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">89</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Minimizing the system error in feedforward neural networks with evolution strategy</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wienholt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN &apos;93</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Gielen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Kappen</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="490" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<author>
			<persName><forename type="first">F</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Individual comparisons by ranking methods</title>
		<imprint>
			<date type="published" when="1945">1945</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="80" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">No free lunch theorems for optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Acready</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="67" to="82" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Evolutionary artificial neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="203" to="222" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Evolving artificial neural networks using an improved PSO and DPSO</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1054" to="1060" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A hybrid particle swarm optimization -back-propagation algorithm for feedforward neural network training</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Lok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Comput</title>
		<imprint>
			<biblScope unit="volume">185</biblScope>
			<biblScope unit="page" from="1026" to="1037" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">An online gradient method with momentum for two-layer feedforward neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Comput</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page" from="488" to="498" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
