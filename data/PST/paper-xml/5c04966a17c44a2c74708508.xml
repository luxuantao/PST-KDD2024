<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BYTES ARE ALL YOU NEED: END-TO-END MULTILINGUAL SPEECH RECOGNITION AND SYNTHESIS WITH BYTES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-11-22">22 Nov 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
							<email>boboli@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tara</forename><surname>Sainath</surname></persName>
							<email>tsainath@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
							<email>yonghui@google.com</email>
						</author>
						<author>
							<persName><forename type="first">William</forename><surname>Chan</surname></persName>
							<email>williamchan@google.com</email>
						</author>
						<title level="a" type="main">BYTES ARE ALL YOU NEED: END-TO-END MULTILINGUAL SPEECH RECOGNITION AND SYNTHESIS WITH BYTES</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-11-22">22 Nov 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1811.09021v1[eess.AS]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multilingual</term>
					<term>end-to-end speech recognition</term>
					<term>end-to-end speech synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present two end-to-end models: Audio-to-Byte (A2B) and Byteto-Audio (B2A), for multilingual speech recognition and synthesis. Prior work has predominantly used characters, sub-words or words as the unit of choice to model text. These units are difficult to scale to languages with large vocabularies, particularly in the case of multilingual processing. In this work, we model text via a sequence of Unicode bytes, specifically, the UTF-8 variable length byte sequence for each character. Bytes allow us to avoid large softmaxes in languages with large vocabularies, and share representations in multilingual models. We show that bytes are superior to grapheme characters over a wide variety of languages in monolingual end-toend speech recognition. Additionally, our multilingual byte model outperform each respective single language baseline on average by 4.4% relatively. In Japanese-English code-switching speech, our multilingual byte model outperform our monolingual baseline by 38.6% relatively. Finally, we present an end-to-end multilingual speech synthesis model using byte representations which matches the performance of our monolingual baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Expanding the coverage of the world's languages in Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) systems have been attracting much interest in both academia and industry <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Conventional phonetically-based speech processing systems require pronunciation dictionaries that map phonetic units to words. Building such resources require expert knowledge for each language. Even with the costly human effort involved, many languages do not have sufficient linguistic resources available for building such dictionaries. Additionally, the inconsistency in the phonetic systems is also challenging to resolve <ref type="bibr" target="#b2">[3]</ref> when merging different languages.</p><p>Graphemes have been used as an alternative modeling unit to phonemes for speech processing <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>. For these systems, an orthographic lexicon instead of a pronunciation dictionary is used to provide a vocabulary list. With recent advances in end-to-end (E2E) modeling, graphemes have become a popular choice. For example, <ref type="bibr" target="#b7">[8]</ref> built a Connectionist Temporal Classification (CTC) model to directly output graphemes, while <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> used graphemes in sequenceto-sequence (seq2seq) models. Sub-word units were used in seq2seq <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> and RNNT <ref type="bibr" target="#b14">[15]</ref> models, and word units were used by <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Similarly, graphemes are also commonly used to build end-toend TTS systems <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>.</p><p>The use of graphemes bring model simplicity and enables end-to-end optimization, which has been shown to yield better performance than phoneme-based models <ref type="bibr" target="#b20">[21]</ref>. However, unlike phonemes, the size of the grapheme vocabulary varies greatly across languages. For example, many eastern languages, such as Chinese, Japanese and Korean, have tens of thousands of graphemes. With limited amounts of training data, many graphemes may have little or no coverage. The label sparsity issue becomes even more severe for multilingual models, where one needs to pool all the distinct graphemes from all languages together resulting in a very large vocabulary that often has long tail graphemes with very poor coverage.</p><p>To address these problems, <ref type="bibr" target="#b2">[3]</ref> explored the use of features from Unicode character descriptions to construct decision trees for clustering graphemes. However, when the model changes to support a new language, the decision tree needs to be updated. Recently, there has been work on exploring the use of Unicode bytes to represent text. <ref type="bibr" target="#b21">[22]</ref> presented an LSTM-based multilingual byte-to-span model. The model consumes the input text byte-by-byte and outputs span annotations. The Unicode bytes are language independent and hence a single model can be used for many languages. The vocabulary size of Unicode bytes is always 256 and it does not increase when pooling more languages together, which is more preferable to graphemes for multilingual applications.</p><p>In this work, we investigate the potential of representing text using byte sequences introduced in <ref type="bibr" target="#b21">[22]</ref> for speech processing. For ASR, we adopt the Listen, Attend and Spell (LAS) <ref type="bibr" target="#b8">[9]</ref> model to convert input speech into sequences of Unicode bytes which correspond to the UTF-8 encoding of the target texts. This model is referred to as the Audio-to-Byte (A2B) model. For TTS, our model is based on the Tacotron 2 architecture <ref type="bibr" target="#b19">[20]</ref>, and generates speech signals from an input byte sequence. This model is referred to as the the Byte-To-Audio (B2A) model. Since both the A2B model and the B2A model operate directly on Unicode bytes, they can handle any number of languages written in Unicode without any modification to the input processing. Due to the small vocabulary size being used, 256 in this case, our models can be very compact and very suitable for on-device applications.</p><p>We report recognition results for the A2B model on 4 different languages -English, Japanese, Spanish and Korean. First, for each individual language, we compare our A2B models to Audioto-Char (A2C) models which emit grapheme outputs. For English and Spanish where the graphemes are single-byte characters, A2B has the exact same performance as A2C as expected. However, for languages that have a large grapheme vocabulary, such as Japanese and Korean, the label sparsity issue hurts the performance of A2C models, whereas the A2B model shares bytes across graphemes and performs better than A2C models. Benefiting from the language independence representation of Unicode bytes, we find it is possible to progressively add support for new languages when building a multilingual A2B model. Specifically, we start with an A2B model trained on English and Japanese and add in a new language after convergence. When adding a new language we usually make sure the new language has the highest mixing ratio but meanwhile keeping small portion for each of the existing languages to avoid forgetting older ones. We experiment adding Spanish and Korean one at a time. In this way, we can reuse the previously built model and expand the language coverage without modifying the model structure. For multilingual ASR, we find that the A2B trained in this way is better than training from scratch. In addition, by adding a 1-hot language vector to the A2B system, which has been shown to boost multi-dialect <ref type="bibr" target="#b22">[23]</ref> and multilingual <ref type="bibr" target="#b23">[24]</ref> system performance, we find that the multilingual A2B system outperforms all the language dependent ones.</p><p>We evaluate the B2A model on 3 different languages, which include English, Mandarin and Spanish. Again, we compare B2A models with those take graphemes as input. For all three languages, B2A has similar performance on quantitative subjective evaluations as graphemes trained on single languages, this providing a more compact multilingual TTS model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MULTILINGUAL AUDIO-TO-BYTE (A2B)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model Structure</head><p>The Audio-to-Byte (A2B) model is based on the Listen, Attend and Spell (LAS) <ref type="bibr" target="#b8">[9]</ref> model, with the output target changed from graphemes to Unicode bytes. The encoder network consists of 5 unidirectional Long Short-Term Memory (LSTMs) <ref type="bibr" target="#b24">[25]</ref> layers, with each layer having 1, 400 hidden units. The decoder network consists of 2 unidirectional LSTM layers with 1, 024 hidden units. Additive content-based attention <ref type="bibr" target="#b25">[26]</ref> with 4 attention heads are used to learn the alignment between the input audio features and the output target units. The output layer is a 256 dimensional softmax, corresponding to the 256 possible byte values.</p><p>Our front-end consists of 80-dimensional log-mel features, computed with a 25ms window and shifted every 10ms. Similar to <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, at each current frame, these features are stacked with 3 consecutive frames to the left and then down-sampled to a 30ms frame rate. The amount of training data usually varies across languages. For example, for English we have around 3.5 times the amount of data compared to the other languages. More details about data can be found in Section 4. In this work, we adjust the data sampling ratio of the different languages to help tackle the data imbalance. We choose the sampling ratio based on intuition and empirical observations. Specially, we start with mixing the language equally and increase the ratio for a language where the performance needs more improvement. In addition, a simple 1-hot language ID vector has been found to be effective improving multilingual systems <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. We also adopt this 1-hot language ID vector as additional input passed into the A2B models, and concatenate it to all the layers including both the encoder and decoder layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Output Unit</head><p>End-to-end speech recognition models have typically used characters <ref type="bibr" target="#b8">[9]</ref>, sub-words <ref type="bibr" target="#b11">[12]</ref>, word-pieces <ref type="bibr" target="#b14">[15]</ref> or words <ref type="bibr" target="#b15">[16]</ref> as the output unit of choice. Word-based units are difficult to scale for languages with large vocabularies, which makes the softmax prohibitively large, especially in multilingual models. One solution is to use data-driven word-piece models. Word-pieces learned from data can be trained to have a fixed vocabulary size. But it requires building a new word-piece model when a new language or new data is added. Additionally, building a multilingual word-piece model is challenging due to the unbalanced grapheme distribution. Grapheme units give the smallest vocabulary size among these units; however, some languages still have very large vocabularies. For example our Japanese vocabulary has over 4.8k characters. In this work, we explore decomposing graphemes into a sequence of Unicode bytes.</p><p>Our A2B model generates the text sequence one Unicode byte at a time. We represent text as a sequence of variable length UTF-8 bytes. For languages with single-byte characters (e.g., English), the use of byte output is equivalent to the grapheme character output. However, for languages with multi-byte characters, such as Japanese and Korean, the A2B model needs to generate a sequence of correct bytes to emit one grapheme token. This requires the model to learn both the short-term within-grapheme byte dependencies, and the long-term inter-grapheme or even inter-word/phrase dependencies, which would be a harder task than grapheme based system.</p><p>The main advantage of byte representation is its language independence. Any script of any language representable by Unicode can be represented by a byte sequence, and there is no need to change the existing model structure. However, for grapheme models, whenever there is a new symbol added, there is a need to change the output softmax layer. This language independence makes it more preferable for modeling multiple languages and also code-switching <ref type="bibr" target="#b28">[29]</ref> speech within a single model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MULTILINGUAL BYTE-TO-AUDIO (B2A)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Structure</head><p>The Byte-to-Audio (B2A) model is based on Tacotron 2 <ref type="bibr" target="#b19">[20]</ref> model. The input byte sequence embedding is encoded by three convolutional layers, which contain 512 filters with shape 5 × 1, followed by a bidirectional long short-term memory (LSTM) layer of 256 units for each direction. The resulting text encodings are accessed by the decoder through a location sensitive attention mechanism, which takes attention history into account when computing a normalized weight vector for aggregation.</p><p>The autoregressive decoder network takes as input the aggregated byte encoding, and conditioned on a fixed speaker embedding for each speaker, which is essentially the language ID since our training data has only one speaker per language. Similar to Tacotron 2, we separately train a WaveRNN <ref type="bibr" target="#b29">[30]</ref> to invert mel spectrograms to a time-domain waveform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Byte for ASR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Data</head><p>Our speech recognition experiments are conducted on a human transcribed supervised training set consisting speech from 4 different languages, namely English (EN), Japanese (JA), Spanish (ES) and Korean (KO). The total amount of data is around 76,000 hours and the language-specific information can be found in Table <ref type="table" target="#tab_1">2</ref>. These training utterances are anonymized and hand-transcribed, and are representative of Google's voice search and dictation traffic. These utterances are further artificially corrupted using a room simulator <ref type="bibr" target="#b30">[31]</ref>, adding varying degrees of noise and reverberation such that the overall SNR is between 0dB and 30dB, with an average SNR of 12dB. The noise sources are from YouTube and daily life noisy environmental recordings. For each utterance, we generated 10 different noisy versions for training. For evaluation, we report results on language-specific test sets, each contains roughly 15K anonymized, hand-transcribed utterances from Google's voice search traffic without overlapping with the training data. This amounts to roughly 20  <ref type="table" target="#tab_1">2</ref>. We use word error rates (WERs) as the evaluation criterion for all the languages except for Japanese, where token error rates (TERs) are used to exclude the ambiguity of word segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Language Dependent Systems</head><p>We first build language dependent A2B models to investigate the performance of byte-based language representations for ASR. For comparison, we also build corresponding Audio-to-Char (A2C) models that have the same model structure but output graphemes. For all the four languages, the model which outputs byte always has a 256dimensional softmax output layer. However, for the grapheme models, different grapheme vocabularies have to be used for different languages. The grapheme set is complete for English and Spanish as it contains all possible letters in each of the languages. However, for Japanese and Korean, we use the training data vocabularies which are 4.8K and 2.7K respectively. The corresponding test set grapheme OOV rates are 2.1% and 1.0%. Whereas with byte outputs, we do not have OOV problem for any language. Experimental results are presented as A1 for the A2C models and A2 for the A2B models in Table <ref type="table" target="#tab_0">1</ref>. The difference between grapheme and byte representations mainly lies in languages which use multi-byte characters, such as Japanese and Korean. Comparing A1 to A2, byte outputs give better results for Japanese and Korean. While for languages with single-byte characters, namely English and Spanish, they have exactly the same performance as expected. Byte output requires the model to learn both the short-term withingrapheme byte dependencies and the long-term inter-grapheme or even inter-word/phrase dependencies; it would possibly be a harder task than grapheme based systems. However, the A2B model yields a 4.0% relative WER reduction on Japanese and 2.6% on Korean over the grapheme systems. It is interesting to see that even with the same model structure, we are able to get better performance with the byte representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Multilingual ASR Systems</head><p>In this experiment, we justify the effectiveness of byte based models over graphemes for multilingual speech recognition. We first build a joint English and Japanese model by equally mixing the training data. For grapheme system, we combine the grapheme vocab of English and Japanese which leads to a large softmax layer. The same model structure except for the softmax layer, where a 256 dimensional softmax is used, is used to build the A2B model. Although the model now needs to recognize two languages, we keep the model size the same as those language dependent ones. From Table <ref type="table" target="#tab_0">1</ref>, the multilingual byte system (B2) is better than the grapheme system (B1) on both English and Japanese test sets. However, its performance is worse than those language dependent ones, which we will address later in this work. For the following experiments, we continue with only the A2B models as they are better than A2C models.</p><p>To increase the model's language coverage, e.g., Spanish, one way is to start from a random initialization and train on all the training data. We equally mix the data from these three languages for training. The results are presented as C1 in Table <ref type="table" target="#tab_0">1</ref>. Due to the language independence of the byte representation, we, alternatively, can add a new language by simply training on new data. Hence, we reuse the B2 model to continue training with Spanish data. To avoid the model forgetting previous languages, namely English and Japanese, we also mix in those languages but with a slightly lower mixing ratio which is 3:3:4 for English, Japanese and Spanish. The results are presented as C2 in Table <ref type="table" target="#tab_0">1</ref>. With this method, the byte model not only trains faster but also achieves better performance than C1. Most importantly, C2 matches the performance of language dependent models on Japanese and is even slightly better for Spanish.</p><p>To add support for Korean, we simply continued the training of C2 with the new training data mixture. We use a ratio of 0.23:0.23:0.23:0.31, which is based on heuristics to balance the existing languages and use a higher ratio for the new languages. We did not specifically tune the mixing ratio. The results (D1 in Table <ref type="table" target="#tab_0">1</ref>) show that we are able to get closer to the language dependent models except for English. Even though worse than the English only model, D1 gives the best multilingual performance on English so far. To improve the performance of the multilingual systems, we first increase the number of decoder layers from 2 to 6 in consideration of the increased variations in byte sequences when mixing more languages. However, experimental results show that the larger model improves performance on English but degrades on Japanese due to potential over-fitting (comparing B3 to B2). To address this problem, we brings in the 1-hot language ID vector to all the layers in the A2B model. This enables the learning of language independent weight matrices together with language dependent biases to cater the specific needs for each language. Experiment B4 shows dramatic error reduction with this simple 1-hot vector comparing to B3.</p><p>Similarly, to support the recognition of Spanish, we continue the training of B4 by mixing the languages at the ratio of 3:3:4 where more weight is given to the new language. This gives us the model C3 which outperforms language dependent ones on both Japanese and Spanish. Furthermore, we add Korean in a similar way with the ratio of 0.3:0.15:0.15:0.4. This time while making sure the ratio for the new language, Korean, is the highest, we also increase the ratio for English as we have more English training data. The model D3 wins over language dependent models except for English. One assumption for the degradation on English is that when mixing in other languages, the multilingual model sees less data from each language than those single language models. To justify this, we continue the training of D3 with an increased English data presence ratio in the mixture, specifically we use the ratio of 2:1:1:1. We didn't specifically tune these mixing ratios used for training. The final model D4 wins over all the language dependent ones on average by 4.4% relatively. For comparison, we include the results for a randomly initialized model with equal training data mixing ratio D2, which is much worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Error analysis</head><p>To further understand the gains of using bytes versus graphemes as language representations, we take Japanese for this study and compare the decoding hypotheses between A1 and A2. Interestingly, the A2B model wins over the A2C models mainly on English words in utterances with mixed English and Japanese. The Japanese test set was not particularly created to include code-switching utterances. Examining the English words appeared in Japanese test set, they are mostly proper nouns such as "Google", "wi-fi", "LAN" etc. One example of such cases is the A2B generates the correct hypothesis "wi-fi オ ン" while the A2C outputs "i-i オ ン". Another example is "google 音 声 認 識" where the A2B recognizes it correctly, but the A2C model drops the initial "g" and gives "oogle 音 声 認 識".</p><p>One of the potential benefits of using byte-based models is for code-switching speech. Collecting such data is challenging. The quality of artificially concatenated speech is far from real. In this study we use data filtered from the Japanese test set, where utterances having transcript that contains 5 or more consecutive English characters are kept. These utterances mostly contain only a single English word in Japanese texts. Out of the 17.6K utterances, we get  <ref type="bibr" target="#b31">[32]</ref>, with rating scores from 1 to 5 in 0.5 point increments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Multilingual TTS System</head><p>Table <ref type="table" target="#tab_3">4</ref> compares subjective naturalness MOS of the proposed model to the baseline using graphemes for English, Mandarin and Spanish respectively. Both results indicate that the proposed multilingual B2A model is comparable as the state-of-the-art monolingual model <ref type="foot" target="#foot_0">1</ref> . Moreover, we observed that the B2A model was able to read code-switching text. However, we don't have good metric to evaluate the quality of code-switching for TTS, e.g. the speech is fluent but the speaker is changed for different language. Future work may explore how to evaluate TTS on code-switching scenario and how to disentangle language and speaker given more training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we investigated the use of Unicode bytes as a new language representation for both ASR and TTS. We proposed Audioto-Byte (A2B) and Byte-to-Audio (B2A) as multilingual ASR and TTS end-to-end models. The use of bytes allows us to build a single model for many languages without modifying the model structure for new ones. This brings representation sharing across graphemes, and is crucial for languages with large grapheme vocabularies, especially in multilingual processing. Our experiments show that byte models outperform grapheme models in both multilingual and monolingual models. Moreover, our multilingual A2B model outperforms our monolingual baselines by 4.4% relatively on average. The language independence of byte models provides a new perspective to the code-switching problem, where our multilingual A2B model achieves 38.6% relative improvement over our monolingual baselines. Finally, we also show our multilingual B2A models match the performance of our monolingual baselines in TTS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Speech recognition performance of monolingual and multilingual with Audio-to-Byte (A2B) or Audio-to-Char (A2C) models.</figDesc><table><row><cell cols="3">Model ExpId Configuration</cell><cell>Training Languages</cell><cell cols="4">English WER(%) TER(%) WER(%) WER(%) Japanese Spanish Korean</cell></row><row><cell>Mono-lingual</cell><cell>A1 A2</cell><cell>A2C A2B</cell><cell>EN/JA/ES/KO</cell><cell>6.9 6.9</cell><cell>13.8 13.2</cell><cell>11.2 11.2</cell><cell>26.5 25.8</cell></row><row><cell></cell><cell>B1 B2</cell><cell>A2C A2B</cell><cell>EN+JA</cell><cell>9.5 8.9</cell><cell>13.9 13.3</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>C1 C2</cell><cell>A2B, Random Init A2B, Init From B2</cell><cell>EN+JA+ES</cell><cell>9.7 8.6</cell><cell>13.6 13.2</cell><cell>11.1 11.0</cell><cell>--</cell></row><row><cell>Multi-</cell><cell>D1</cell><cell>A2B, Init From C2</cell><cell>EN+JA+ES+KO</cell><cell>8.4</cell><cell>13.4</cell><cell>11.3</cell><cell>26.0</cell></row><row><cell>lingual</cell><cell>B3 B4</cell><cell>A2B, Larger Model A2B, Larger Model, LangVec</cell><cell>EN+JA</cell><cell>8.8 7.5</cell><cell>13.6 13.3</cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>C3</cell><cell>A2B, Init From B4</cell><cell>EN+JA+ES</cell><cell>7.5</cell><cell>12.9</cell><cell>10.8</cell><cell>-</cell></row><row><cell></cell><cell>D2</cell><cell>A2B, Larger Model, LangVec</cell><cell></cell><cell>8.6</cell><cell>13.5</cell><cell>11.2</cell><cell>25.4</cell></row><row><cell></cell><cell>D3</cell><cell>A2B, Init From C3</cell><cell>EN+JA+ES+KO</cell><cell>7.0</cell><cell>12.8</cell><cell>10.8</cell><cell>25.0</cell></row><row><cell></cell><cell>D4</cell><cell>A2B, Init From D3</cell><cell></cell><cell>6.6</cell><cell>12.6</cell><cell>10.7</cell><cell>24.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the training and testing data used in our experiments. "utts" denotes the total number of utterances in each set and "time" is the total duration of audio for each set.</figDesc><table><row><cell>Languages</cell><cell cols="4">Train utts (M) time (Kh) utts (K) time (h) Test</cell></row><row><cell>English (EN)</cell><cell>35.0</cell><cell>27.5</cell><cell>15.4</cell><cell>20.0</cell></row><row><cell>Japanese (JA)</cell><cell>9.9</cell><cell>16.5</cell><cell>17.6</cell><cell>22.2</cell></row><row><cell>Spanish (ES)</cell><cell>8.9</cell><cell>16.3</cell><cell>16.6</cell><cell>22.3</cell></row><row><cell>Korean (KO)</cell><cell>9.6</cell><cell>16.1</cell><cell>12.6</cell><cell>15.0</cell></row></table><note>hours of test data per language. Details of each language dependent test set can be found in Table</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on A2B and A2C models on English-Japanese code-switching data.</figDesc><table><row><cell cols="3">Model ExpId Configuration</cell><cell>TER(%)</cell></row><row><cell>Mono-</cell><cell>A1</cell><cell>A2C</cell><cell>36.5</cell></row><row><cell>lingual</cell><cell>A2</cell><cell>A2B</cell><cell>22.4</cell></row><row><cell>Multi-lingual</cell><cell>B1 B2 D4</cell><cell>A2C A2B A2B Larger Model, LangVec</cell><cell>21.4 20.5 21.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Speech naturalness Mean Opinion Score (MOS) with 95% confidence intervals across different language and systems. Spanish speech by a female speaker. For all compared models, we synthesize raw audio at 24 kHz in 16-bit format. We rely on crowdsourced Mean Opinion Score (MOS) evaluations based on subjective listening tests. All our MOS evaluations are aligned to the Absolute Category Rating scale</figDesc><table><row><cell>Languages</cell><cell>EN</cell><cell>CN</cell><cell>ES</cell></row><row><cell cols="4">Monolingual C2A 4.24±0.12 3.48±0.11 4.21±0.11</cell></row><row><cell cols="4">Multilingual B2A 4.23±0.14 3.42±0.12 4.23±0.10</cell></row><row><cell cols="4">476 code-switching sentences and we report the TERs on this subset</cell></row><row><cell cols="4">in Table 3. With Japanese monolingual models (A1 and A2), our</cell></row><row><cell cols="4">A2B model outperforms the A2C model by 38.6% relatively. With</cell></row><row><cell cols="4">English and Japanese multilingual models (B1 and B2), our A2B</cell></row><row><cell cols="4">model wins over the A2C model by 4.2% relatively. We also test</cell></row><row><cell cols="4">system D4 on these code-switch data. However, due to the language</cell></row><row><cell cols="4">1-hot vector used in D4 is utterance-level, the performance is worse</cell></row><row><cell cols="4">than B2. Using frame/segment level language information may ad-</cell></row><row><cell cols="3">dress this problem, which will be explored in future.</cell><cell></cell></row><row><cell>4.2. Byte for TTS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.2.1. Data</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Text-to-speech models were trained on (1) 44 hours of North Amer-</cell></row><row><cell cols="4">ican English speech recorded by a female speaker; (2) 37 hours of</cell></row><row><cell cols="4">Mandarin speech by a female speaker; (3) 44 hours of North Amer-</cell></row><row><cell>ican</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">MOS is worse than<ref type="bibr" target="#b19">[20]</ref> because we have OOV in the test set.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multilingual speech processing</title>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Current trends in multilingual speech processing</title>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathew</forename><surname>Magimai-Doss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">N</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Imseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lakshmi</forename><surname>Saheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Valente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sadhana</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="885" to="915" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unicodebased graphemic systems for limited resource languages</title>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><forename type="middle">M</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName><surname>Ragni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Context-dependent acoustic modeling using graphemes for large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Kanthak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Grapheme based speech recognition</title>
		<author>
			<persName><forename type="first">Mirjam</forename><surname>Killer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Stuker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth European Conference on Speech Communication and Technology</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A grapheme based speech recognition system for russian</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th Conference Speech and Computer</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Comparing grapheme-based and phoneme-based speech recognition for afrikaans</title>
		<author>
			<persName><forename type="first">D</forename><surname>Willem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marelie H</forename><surname>Basson</surname></persName>
		</author>
		<author>
			<persName><surname>Davel</surname></persName>
		</author>
		<editor>PRASA</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition</title>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Philemon Brakel, and Yoshua Bengio</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On Online Attention-based Speech Recognition and Joint Mandarin Character-Pinyin Training</title>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Lane</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Latent Sequence Decompositions</title>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">State-ofthe-art Speech Recognition With Sequence-to-Sequence Models</title>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowsk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploring Architectures, Data and Units for Streaming End-to-End Speech Recognition with RNN-Transducer</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>ASRU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Hagen</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09975</idno>
		<title level="m">Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Advancing Acoustic-to-Word CTC Model</title>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoli</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Char2wav: End-to-end speech synthesis</title>
		<author>
			<persName><forename type="first">Jose</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><forename type="middle">Felipe</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR: Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<title level="m">Tacotron: A fully endto-end text-to-speech synthesis model</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Skerry-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A comparison of sequence-tosequence models for speech recognition</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leif</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="939" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multilingual language processing from bytes</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Brunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Oriol Vinyals, and Amarnag Subramanya</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-dialect speech recognition with a single sequence-to-sequence model</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khe</forename><forename type="middle">Chai</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multilingual speech recognition with a single end-to-end model</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lower Frame Rate Neural Network Acoustic Models</title>
		<author>
			<persName><forename type="first">Golan</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fast and accurate recurrent neural network acoustic models for speech recognition</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Has ¸im Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><surname>Franc ¸oise Beaufays</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06947</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Code-switching in conversation: Language, interaction and identity</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient neural audio synthesis</title>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seb</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generated of Large-scale Simulated Utterances in Virtual Rooms to Train Deep-Neural Networks for Far-field Speech Recognition in Google Home</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bacchiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Methods for subjective determination of transmission quality</title>
		<author>
			<persName><surname>Itut Rec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Telecommunication Union</title>
		<imprint>
			<biblScope unit="volume">800</biblScope>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
