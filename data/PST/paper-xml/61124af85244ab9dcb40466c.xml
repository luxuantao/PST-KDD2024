<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Noisy Channel Language Model Prompting for Few-Shot Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
							<email>mikelewis@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
							<email>hannaneh@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Allen Institute of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Noisy Channel Language Model Prompting for Few-Shot Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worstcase accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive models (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Prompting large language models, by prepending natural language text or continuous vectors (called prompts) to the input, has shown to be promising in few-shot learning <ref type="bibr" target="#b2">(Brown et al., 2020)</ref>. Prior work has proposed methods for finding better prompt <ref type="bibr" target="#b26">(Shin et al., 2020;</ref><ref type="bibr" target="#b16">Li and Liang, 2021;</ref><ref type="bibr" target="#b14">Lester et al., 2021)</ref> or better scoring of the output from the model <ref type="bibr" target="#b36">(Zhao et al., 2021;</ref><ref type="bibr" target="#b6">Holtzman et al., 2021)</ref>. These studies directly predict target tokens to determine the prediction for an end task. Despite promising results, they can be unstable with high variance across different verbalizers (text expression for labels) and seeds, and the worst-case performance is often close to random guessing performance <ref type="bibr">(Perez et al., 2021;</ref><ref type="bibr" target="#b18">Lu et al., 2021)</ref>.</p><p>In this paper, we introduce alternative channel models for prompted few-shot text classification with large language models, inspired by noisy channel models in machine translation <ref type="bibr" target="#b1">(Brown et al., 1993;</ref><ref type="bibr" target="#b11">Koehn et al., 2003;</ref><ref type="bibr" target="#b34">Yu et al., 2017;</ref><ref type="bibr" target="#b32">Yee et al., 2019)</ref> and their extensions to other tasks <ref type="bibr" target="#b33">(Yogatama et al., 2017;</ref><ref type="bibr">Lewis and Fan, 2018)</ref>. Unlike direct models that compute the conditional probability of the label token given the input, channel models compute the conditional probability of the input given the output (Figure <ref type="figure">1</ref>). Intuitively, channel models are required to explain every word in the input, potentially amplifying training signals in the low data regime. We study the impact of channel models for language model prompting where the parameters of the language model are frozen. In particular, we compare channel models with their direct counterparts for (1) demonstration methods, either concatenation-based <ref type="bibr" target="#b2">(Brown et al., 2020)</ref> or our proposed, ensemble-based (Section 4.1.3), and (2) prompt tuning <ref type="bibr" target="#b14">(Lester et al., 2021)</ref>.</p><p>Our experiments on eleven text classification datasets show that channel models outperform their direct counterparts by a large margin. We attribute the strong performance of channel models to their stability: they have lower variance and significantly higher worst-case accuracy then their direct counterparts over different verbalizers and seeds. We additionally find a direct model with head tuning-tuning the LM head while freezing other parameters-is surprisingly effective, often outper-arXiv:2108.04106v1 [cs.CL] 9 Aug 2021 forming direct models with other forms of tuning. While different methods are preferred given different conditions, the channel model with prompt tuning (denoted as channel prompt tuning) significantly outperforms all direct baselines when (1) the training data is imbalanced, or (2) generalization to unseen labels is required.</p><p>In summary, our contributions are three-fold:</p><p>1. We introduce a noisy channel approach for language model prompting in few-shot text classification, showing that they significantly outperform their direct counterparts for both demonstration methods and prompt tuning.</p><p>2. We find particularly strong performance of channel models over direct models when the training data is imbalanced or generalization to unseen labels is required.</p><p>3. Based on our extensive ablations, we provide recommendations between different models (direct vs. channel and prompt tuning vs. head tuning) based on given conditions such as the target task, the size of training data, the number of classes, the balance between labels in the training data, and whether generalization to unseen labels is required.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-shot Learning</head><p>Prior work in few-shot learning has used different approaches, including semi-supervised learning with data augmentation or consistency training <ref type="bibr" target="#b20">(Miyato et al., 2017;</ref><ref type="bibr" target="#b2">Clark et al., 2018;</ref><ref type="bibr" target="#b30">Xie et al., 2020;</ref><ref type="bibr" target="#b2">Chen et al., 2020)</ref> and meta learning <ref type="bibr" target="#b3">(Finn et al., 2017;</ref><ref type="bibr" target="#b9">Huang et al., 2018;</ref><ref type="bibr">Bansal et al., 2020)</ref>. Recent work has introduced prompting (or priming) of a large language model. For example, <ref type="bibr" target="#b2">Brown et al. (2020)</ref> proposes to use a concatenation of training examples as a demonstration, so that when it is prepended to the input and is fed to the model, the model returns the output following the pattern in the training examples. This is especially attractive as it eliminates the need for updating parameters of the language model, which is often expensive and impractical. Subsequent work proposes alternative ways of scoring labels through better model calibration <ref type="bibr" target="#b36">(Zhao et al., 2021;</ref><ref type="bibr" target="#b6">Holtzman et al., 2021)</ref>. Other work explores learning better prompts, either in a discrete space <ref type="bibr" target="#b26">(Shin et al., 2020;</ref><ref type="bibr">Jiang et al., 2020;</ref><ref type="bibr" target="#b4">Gao et al., 2021)</ref> or in a continuous space <ref type="bibr" target="#b16">(Li and Liang, 2021;</ref><ref type="bibr" target="#b14">Lester et al., 2021;</ref><ref type="bibr">Liu et al., 2021;</ref><ref type="bibr">Zhong et al., 2021;</ref><ref type="bibr" target="#b24">Qin and Eisner, 2021)</ref>. Almost all of them are direct models, computing the likelihood of the output given the input with the prompts.<ref type="foot" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Channel Model</head><p>Let x and y be the input and the output, respectively, the most widely-used models, denoted as direct models, compute P (y|x). In contrast, the noisy channel models maximize P (x|y)P (y) <ref type="bibr" target="#b26">(Shannon, 1948;</ref><ref type="bibr" target="#b1">Brown et al., 1993)</ref>.<ref type="foot" target="#foot_1">2</ref> While the noisy channel approach has been the most successful in machine translation <ref type="bibr" target="#b31">(Yamada and Knight, 2001;</ref><ref type="bibr" target="#b11">Koehn et al., 2003;</ref><ref type="bibr" target="#b34">Yu et al., 2017;</ref><ref type="bibr" target="#b32">Yee et al., 2019)</ref>, it has also been studied in more general NLP tasks.</p><p>Prior work provides a theoretical analysis that channel models approach their asymptotic errors more rapidly than their direct counterparts <ref type="bibr" target="#b21">(Ng and Jordan, 2002)</ref>, and empirically shows that channel models are more robust to distribution shift in text classification <ref type="bibr" target="#b33">(Yogatama et al., 2017)</ref> or question answering <ref type="bibr">(Lewis and Fan, 2018)</ref>, and in a few-shot setup <ref type="bibr">(Ding and Gimpel, 2019)</ref>.</p><p>In this paper, we explore channel models using a large language model on a wide range of text classification tasks, focusing on prompt-based fewshot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Formulation</head><p>We focus on text classification tasks. The goal is to learn a task function f : X − → C, where X is the set of all natural language texts and C = {c 1 ...c m } is a set of labels. We consider three formulations.</p><p>Direct computes distributions of labels c i ∈ C given the input x ∈ X : P (c i |x). This is the most widely used method in modern neural networks.</p><p>Direct++ is a stronger direct model that computes P (c i |x) P (c i |NULL) instead of P (c i |x), following the method from <ref type="bibr" target="#b6">Holtzman et al. (2021)</ref> and the nonparametric method from <ref type="bibr" target="#b36">Zhao et al. (2021)</ref>. This approach is motivated by the fact that language models can be poorly calibrated and suffer from </p><formula xml:id="formula_0">P (ci|x) PLM(v(ci)|x) PLM(v(ci)|C({x j , v(c j )} K j=1 ), x) Π K j=1 PLM(v(ci)|x j , v(c j ), x) Direct++ P (c i |x) P (c i |NULL) P LM (v(c i )|x) P LM (v(c i )|NULL) P LM (v(c i )|C({x j ,v(c j )} K j=1 ),x) P LM (v(c i )|C({x j ,v(c j )} K j=1 ),NULL) Π K j=1 P LM (v(c i )|x j ,v(c j ),x) P LM (v(c i )|x j ,v(c j ),NULL) Channel P (x|ci) PLM(x|v(ci)) PLM(x|C({v(c j ), x j } K j=1 ), v(ci)) Π K j=1 PLM(x|v(c j ), x j , v(ci))</formula><p>Table <ref type="table">1</ref>: Comparison of zero-shot, concat-based demonstrations, and ensemble-based demonstrations. {(x j , c j )} K j=1 is training data, v is the verbalizer, and</p><formula xml:id="formula_1">C({a j , b j } K j=1 ) is a concatenation of a 1 , b 1 , • • • , a K , b K .</formula><p>competition between different strings with the same meaning. This approach is used for the demonstration methods in Section 4.1.</p><p>Channel uses Bayes' rule to reparameterize P (c i |x) as P (x|c i )P (c i )</p><formula xml:id="formula_2">P (x)</formula><p>. As we are generally interested in argmax c i ∈C</p><formula xml:id="formula_3">P (x|c i )P (c i ) P (x)</formula><p>and P (x) is independent from c i , it is sufficient to model P (x|c i )P (c i ). We assume P (c i ) = 1</p><p>|C| and only compute P (x|c i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>We explore direct and channel models using a causal language model (LM) P LM that gives the conditional probability of the text y when followed by x. More precisely, given the text x = x 1 ...x tx and y = y 1 ...y ty (x 1 ...x tx , y 1 ...y ty ∈ V, where V is the vocabulary set), P LM (y|x) indicates Π ty t =1 P LM (y t |x 1 ...x tx y 1 ...y t −1 ).<ref type="foot" target="#foot_2">3</ref> When learning a task function f : X − → C, we also assume a pre-defined verbalizer v : C − → X which maps each label into a natural language expression. For example, if the task is sentiment analysis with C = {c + , c − }, an example input text x would be "A three-hour cinema master class" and an example v would have v(c + ) ="It was great" and v(c − ) ="It was terrible". In a few-shot setup, we are also given a set of</p><formula xml:id="formula_4">K training examples D = {(x 1 , c 1 ), • • • , (x K , c K )}.</formula><p>We are mainly interested in methods where there is no trainable parameters (Section 4.1), or the number of trainable parameters is roughly smaller than 0.01% of the total (Section 4.2). This shares motivations from prior work that updating and saving a large number of parameters for every task is expensive and often infeasible <ref type="bibr">(Rebuffi et al., 2017;</ref><ref type="bibr" target="#b7">Houlsby et al., 2019;</ref><ref type="bibr" target="#b14">Lester et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Demonstration methods</head><p>In demonstration methods, there are no trainable parameters. We explore three ways of making a prediction (Table <ref type="table">1</ref>), two of which are from <ref type="bibr" target="#b2">Brown et al. (2020)</ref> and the third from this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Zero-shot</head><p>We follow <ref type="bibr" target="#b2">Brown et al. (2020)</ref> in computing P (c i |x) and P (x|c i ) as P LM (v(c i )|x) and P LM (x|v(c i )), respectively. For example, given x ="A three-hour cinema master class", the direct model compares the probabilities of "It was great" and "It was terrible" when following "A three-hour cinema master class", while the channel model considers the probabilities of "A three-hour cinema master class" when following "It was great" or "It was terrible".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Concat-based demonstrations</head><p>We follow the few-shot learning method in <ref type="bibr" target="#b2">Brown et al. (2020)</ref>. The key idea is to prepend a concatenation of K training examples to the input so that a language model can learn the task setup from the input. The original method was used for a direct model, but can be naturally extended for a channel model. Concretely, P (c i |x) in direct models is obtained via</p><formula xml:id="formula_5">P LM (v(c i )|x 1 , v(c 1 ), • • • , x K , v(c K ), x), and P (x|c i ) in channel models is obtained via P LM (x|v(c 1 ), x 1 , • • • , v(c K ), x K , v(c i )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Ensemble-based demonstrations</head><p>We propose a new method as an alternative to the concat-based method, which we find to be a stronger direct model. The key idea is, instead of concatenating K training examples as one sequence and getting output probabilities from an LM once, we obtain output probabilities from an LM K times conditioned on one training example at a time, and multiply the resulting probabilities. Specifically, P (c i |x) is computed ..v(c m ), respectively. All finetuning is a typical finetuning method that updates all parameters of the LM (illustrated as a reference). Head tuning, Transformation tuning and Prompt tuning are described in Section 4.2; all of these methods update a very limited number of parameters.</p><p>via</p><formula xml:id="formula_6">Π K j=1 P LM (v(c i )|x j , v(c j ), x) and P (x|c i ) is computed via Π K j=1 P LM (x|v(c j ), x j , v(c i ))</formula><p>. This method also reduces the memory consumptionthe concat-based method uses O(K 2 ) while this method uses O(K)-and eliminates the dependency on the ordering of training examples, which has been shown to significantly impact the model performance <ref type="bibr" target="#b36">(Zhao et al., 2021;</ref><ref type="bibr" target="#b18">Lu et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tuning methods</head><p>We explore methods that tune a very limited number of model parameters, as summarized in Figure 2. Head tuning (Section 4.2.1) and transformation tuning (Section 4.2.2) are for direct models. Prompt tuning (Section 4.2.3) can be used for both direct and channel models, which we refer as direct prompt tuning and channel prompt tuning, respectively, for simplicity. All models share the same input-output interface with the zero-shot setup in Table <ref type="table">1</ref> during training and inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Head tuning</head><p>Head tuning finetunes the head-the matrix in the LM which transforms the hidden representation from the last transformer layer to the logit values. Let O ∈ R |V|×h be the head and h x ∈ R h be the hidden representations from the last transformer layer given x, P LM (v i |x) for a token v i ∈ V is computed via an i-th element of Softmax(Oh x ). We finetune O while freezing all other parameters of the LM. Although O is tied with the embedding matrix of the LM during language model pretraining, we separate them during finetuning. The number of trainable values is |v(C)|h where v(C) denotes vocabularies in v(c 1 )...v(c m ).<ref type="foot" target="#foot_3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Transformation tuning</head><p>As an alternative to head tuning, we transform O with a new transformation matrix U ∈ R h×h . Specifically, P LM (v i |x) for a token v i ∈ V is computed via an i-th element of Softmax(OUh x ). We train U, initialized from an identity matrix, and freeze other parameters including O. The number of trainable values is h 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Prompt tuning</head><p>Prompt tuning is the method that has recently gathered much attention (Li and Liang, 2021; <ref type="bibr" target="#b14">Lester et al., 2021;</ref><ref type="bibr">Liu et al., 2021)</ref>. The key idea is to consider the LM as a black-box model and instead learn continuous prompt embeddings. We follow the method from <ref type="bibr" target="#b14">Lester et al. (2021)</ref> where n prompt tokens u 1 ...u n are prepended to the input, and the embeddings of u 1 ...u n are learned. In other words, direct models compute P (c i |x) = P LM (v(c i )|u 1 ...u n , x), and channel models compute P (x|c i ) = P LM (x|u 1 ...u n , v(c i )). The parameters in the LM are frozen except the embeddings of u 1 ...u n , 5 so that the number of trainable values is nh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We report results for eleven text classification datasets, following <ref type="bibr" target="#b35">Zhang et al. (2015)</ref> and <ref type="bibr" target="#b4">Gao et al. (2021)</ref>: SST-2 <ref type="bibr" target="#b27">(Socher et al., 2013)</ref>, SST-5 <ref type="bibr" target="#b27">(Socher et al., 2013)</ref>, MR <ref type="bibr" target="#b22">(Pang and Lee, 2005)</ref>, CR <ref type="bibr" target="#b8">(Hu and Liu, 2004)</ref> and Leskovec, 2013), Yelp <ref type="bibr" target="#b35">(Zhang et al., 2015)</ref>, TREC (Voorhees and Tice, 2000), AGNews <ref type="bibr" target="#b35">(Zhang et al., 2015)</ref>, Yahoo <ref type="bibr" target="#b35">(Zhang et al., 2015)</ref>, DBPedia <ref type="bibr" target="#b13">(Lehmann et al., 2015)</ref> and Subj <ref type="bibr">(Pang and Lee, 2004)</ref>. The datasets include a varied number of classes per task, from 2 to 14. See Table <ref type="table">10</ref> in Appendix A for dataset samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training Data</head><p>For few-shot learning, we primarily use training set size K = 16, but explore K = {4, 16, 64, Full} in the ablations. We uniformly sample the K examples and relax the assumption from prior work of an equal number of training examples per label <ref type="bibr" target="#b4">(Gao et al., 2021;</ref><ref type="bibr" target="#b17">Logan IV et al., 2021)</ref>, for more realistic and challenging evaluation.</p><p>We do not use a held-out validation set and instead follow all the hyperameters and details from prior work (Appendix B). The very limited data is better used for training rather than validation, and cross-validation is less helpful when the validation set is extremely small <ref type="bibr">(Perez et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Language Models</head><p>We use GPT-2 <ref type="bibr" target="#b25">(Radford et al., 2019)</ref> for the LM. We primarily use GPT-2 Large but also experiment with varying sizes (Small, Medium, Large and X-Large) in the ablations. While we only experiment with GPT-2, our experiments are easily extendable to other causal language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation</head><p>We use accuracy as a metric for all datasets.</p><p>We experiment with 4 different verbalizers (taken from <ref type="bibr" target="#b4">Gao et al. (2021)</ref>; full list provided in Appendix A), 5 different random seeds for sampling training data, and 4 different random seeds for training. This means we have (1) 4 runs for a zero-shot setup (as data seeds and train seeds do not matter), (2) 20 runs for demonstration methods (as train seeds do not matter), and (3) 80 runs for tuning methods. We then report Average accuracy and Worst-case accuracy. <ref type="foot" target="#foot_4">6</ref> We consider the worst-case accuracy to be as important as the average accuracy given significantly high variance of few-shot learning models, as shown in previous work <ref type="bibr" target="#b36">(Zhao et al., 2021;</ref><ref type="bibr">Perez et al., 2021;</ref><ref type="bibr" target="#b18">Lu et al., 2021)</ref>. The worst-case accuracy is likely of more interest in high-risk applications <ref type="bibr" target="#b0">(Asri et al., 2016;</ref><ref type="bibr" target="#b5">Guo et al., 2017)</ref>.</p><p>Other implementation details are in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>This section reports results from demonstration methods (Section 6.1), tuning methods (Section 6.2) and ablations (Section 6.3). Discussion is provided in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Main Results: Demonstration Methods</head><p>Table <ref type="table" target="#tab_2">3</ref> shows the performance of demonstration methods.</p><p>Direct vs. Direct++ Direct++ significantly outperforms the naive direct model across all setups, indicating that using P (c i |x) P (c i |NULL) instead of P (c i |x) is highly beneficial as claimed by <ref type="bibr" target="#b6">Holtzman et al. (2021)</ref>; <ref type="bibr" target="#b36">Zhao et al. (2021)</ref>.</p><p>Concat vs. Ensemble Our proposed, ensemblebased method is better than the concat-based method in direct models, by 7% absolute in the average accuracy and the worst-case accuracy, when macro-averaged across all datasets.</p><p>In contrast, the ensemble-based method is not always better in channel models; it is better only on the datasets with long inputs. We conjecture that the ensemble-based method may suffer when labels in the training data are not balanced, which direct++ explicitly takes into account as described in <ref type="bibr" target="#b36">Zhao et al. (2021)</ref>.</p><p>Direct++ vs. Channel In a few-shot setting, channel models outperform direct models in almost all cases. The strongest channel model outperforms the strongest direct model by 3.1% and 7.2% absolute, in terms of the average accuracy and the worst-case accuracy, respectively.</p><p>Standard deviation and the best-case accuracy are reported in pendix. They indicate strong performance of channel models can be attributed to their low variance.</p><p>The highest best-case accuracy is achieved by di-rect++ on most datasets, but it has a higher variance, having lower average and the worst-case accuracy than channel models.</p><p>Zero-shot vs. Few-shot Performance of direct models sometimes degrades in a few-shot setting, which is also observed by prior work <ref type="bibr" target="#b36">(Zhao et al., 2021)</ref>. This is likely because demonstrations provided by the training data may cause the model to be miscalibrated and easily biased by the choice of demonstrations. However, channel models achieve few-shot performance that is significantly better than zero-shot methods on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Main Results: Tuning Methods</head><p>Table <ref type="table">4</ref> shows the performance of tuning methods.</p><p>Comparison when prompt tuning When using prompt tuning, channel models consistently outperform direct models by a large margin on all datasets. Improvements are 13.3% and 23.5% absolute in the average and the worst-case accuracy, respectively. Standard deviation and the best-case accuracy are reported in Table <ref type="table" target="#tab_2">13</ref> in the Appendix. Consistent with the findings in Section 6.1, the strong performance of channel prompt tuning can be explained by the low variance of channel prompt tuning. Direct prompt tuning often achieves higher best-case accuracy; however, due to its high variance, its overall accuracy is lower, with significantly lower worst-case accuracy. being omitted as a baseline in prior work. It significantly outperforms direct prompt tuning in all cases. It also outperforms channel prompt tuning on some datasets, particularly significantly on TREC and Subj. For these datasets, the task-finding the type of the answer to the question or identifying the subjectivity of the statement-is inherently different from language modeling, and likely benefits from directly updating the LM parameters, rather than using the LM as a black box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head tuning vs. prompt tuning</head><p>Still, channel prompt tuning outperforms direct head tuning on most datasets. The largest gains are achieved on Yahoo and DBPedia. In fact, on these datasets, channel prompt tuning even outper-  forms all finetuning-finetuning all parameters of the LM-which achieves 48.9/43.8 on Yahoo and 66.3/50.4 on DBPedia. We conjecture that using K = 16 on these datasets naturally requires generalization to unseen labels due to the large number of classes (|C| = 10 and 14), where channel prompt tuning significantly outperforms direct models, as shown in Section 6.4.</p><p>Demonstration (Section 6.1) vs. Tuning Logan IV et al. ( <ref type="formula">2021</ref>) claims that prompt tuning does not outperform the demonstration method, which we find is true in direct models. When the channel models are used, prompt tuning outperforms the demonstration method by 3% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablations</head><p>For the ablations, we report experiments on SST-2, MR, TREC and AGNews, using one train seed (instead of four), and four verbalizers and five data seeds (as in main experiments).</p><p>Varying the size of LMs We vary the size of LMs and report the average and the worst-case accuracy in Figure <ref type="figure">3</ref>. The trends-no matter the best performance is achieved by channel prompt tuning or direct head tuning-are fairly consistent across varying size of LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Varying the number of training examples</head><p>We vary the number of training examples (K) and report the average accuracy in Figure <ref type="figure" target="#fig_2">4</ref>. All methods achieve higher accuracy as K increases. While we confirm strong performance of channel prompt tuning with K ≤ 16, head tuning outperforms channel head tuning when K = 64. When K = Full, both direct prompt tuning and head tuning outperform channel prompt tuning. We think this is because (1) training signals amplified by channel models <ref type="bibr">(Lewis and Fan, 2018</ref>) are more significant when K is small, and (2) channel models are more beneficial when labels on the training data are imbalanced (confirmed in the next ablation), which is less likely to happen with larger K.</p><p>It is also worth noting that our experiment with K = Full confirms the finding from <ref type="bibr" target="#b14">Lester et al. (2021)</ref> that direct prompt tuning matches the performance of all finetuning-finetuning all parameters of the LM-while being much more parameter-  efficient. This only holds with K = Full; in a few-shot setup, all finetuning significantly outperforms other methods. This contradicts traditional analysis that having less trainable parameters is better when the training data is scarce <ref type="bibr" target="#b21">(Ng and Jordan, 2002)</ref>. It is likely because such analysis did not take into account language model pretraining, which gives supervision to the model yet is not the training data for an end task. Results are reported in Figure <ref type="figure" target="#fig_3">5</ref>. All direct models are sensitive to the imbalance in training data, even though they benefit from upsampling when p − is small. Channel prompt tuning is insensitive to the imbalance, and significantly outperforms direct models when p − is small; it even outperforms all finetuning when p − &lt; 0.25. When p − is near to 0.5, direct head tuning matches or outperforms channel prompt tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of imbalance in labels</head><p>It is also worth noting that direct prompt tuning with upsampling matches or outperforms all finetuning and head tuning when p − is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Generalization to unseen labels</head><p>We experiment with a challenging scenario where the model must generalize to unseen labels. While it may be seen as an extreme scenario, this is often a practical setting, e.g., the problem is defined with a set of labels but later an addition of the new label may be needed.</p><p>In the first experiment, we sample K = 16 training examples as in main experiments, but excluding one random label. This means that there is at least one label at test time that was unseen during training, i.e., |C \ {c 1 ...c K }| &gt; 0. Table <ref type="table" target="#tab_5">5</ref> reports the results. All of the direct models are not able to predict the label that is unseen at training time. However, channel prompt tuning successfully predicts unseen labels and achieves considerably better performance than zero-shot. It outperforms all finetuning on 2-way classification datasets, and outperforms head tuning on five datasets except for TREC on which head tuning achieves very strong performance on seen labels.</p><p>In the next experiment, we run zero-shot transfer learning, where the model is trained on one dataset and is tested on another dataset. Here, head tuning is not applicable when the labels are not shared between two datasets. Figure <ref type="figure">6</ref> shows the results. Channel prompt tuning outperforms all direct models including all finetuning on all datasets except for TREC. It is particularly competitive when the tasks are inherently similar, e.g., transfer between 2-way sentiment analysis and 5-way sentiment analysis in the first three figures. In fact, in such cases, they achieve performance that is close to the models trained on in-domain data: 36.3, 43.4 and 43.9 vs. 38.0, 40.2 and 39.5 on SST-5, Amazon and Yelp, respectively. When tasks are inherently different, e.g., the rest of the figures in Figure <ref type="figure">6</ref>, gains over zero-shot performance are relatively small; we think more work should be done to make cross-task transfer more competitive and to discover when it is possible. This experiment is related to the ablation in <ref type="bibr" target="#b14">Lester et al. (2021)</ref> that shows that in a full-shot setup, prompt tuning has strong generalization capacity when tested on out-of-domain data. However, unlike their findings, direct prompt tuning is worse than zero-shot in our experiments. We think this is because (1) we use small K where direct prompt tuning is overall less competitive, and (2) our setup is more challenging in that it requires generalization to unseen labels, in contrast to the setting in <ref type="bibr" target="#b14">Lester et al. (2021)</ref> where the training and the test data are inherently the same task but are drawn from different distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussions &amp; Conclusion</head><p>In this work, we introduced a noisy channel approach for few-shot text classification through language model prompting, where we either provide demonstrations to the language model or tune the prompt embeddings in the continuous space. Our experiments on eleven text classification datasets show that channel models outperform their direct counterparts by a significant margin, mainly because of their stability, i.e., lower variance and better worst-case accuracy. We also found that direct head tuning is more competitive than previously thought, and different methods are preferred given different conditions. Specifically, channel prompt tuning is preferred when: K is small Channel prompt tuning is more competitive when there are fewer training examples. This is in line with previous work: analysis in Ng and Jordan <ref type="bibr">(2002)</ref> shows that channel models are better when there are fewer training examples; <ref type="bibr">Lewis and Fan (2018)</ref> claims that channel models provide more signals by requiring the model to explain the input word-by-word, which would be beneficial in the low data regime. We additionally think stability (i.e., low variance and high worstcase accuracy) of the channel prompt tuning is key to its merits with small K, where direct models are highly unstable <ref type="bibr" target="#b36">(Zhao et al., 2021;</ref><ref type="bibr">Perez et al., 2021;</ref><ref type="bibr" target="#b18">Lu et al., 2021)</ref>.</p><p>Data is imbalanced or |C| is large When the training data is imbalanced, head tuning is uncompetitive, likely because the head relies too much on unconditional distributions of labels the model is exposed to. Channel prompt tuning is less sensitive to such a problem because labels are only a conditioning variable. Label imbalance in the training data is a real-world problem, especially when k is small and |C| is large. We thus suggest future work should explore relaxing the assumption of perfect balance in the training data.</p><p>Generalization to unseen labels is required All direct models are unable to predict labels that are unseen during training, indicating that they overfit in the label space. In contrast, channel models can predict unseen labels, likely because the label space is indirectly modeled. This is in line with prior work that shows channel models are more competitive under a distribution shift <ref type="bibr" target="#b33">(Yogatama et al., 2017;</ref><ref type="bibr">Lewis and Fan, 2018)</ref>.</p><p>Task is closer to language modeling If the task is too different from language modeling even with carefully chosen verbalizers (e.g., TREC and Subj), head tuning outperforms prompt tuning, likely because it benefits from directly updating the parameters of the LM. This may mean that causal LMs are not suitable for all tasks, or we need more sophisticated methods to apply causal LMs for such tasks without updating the LM parameters.</p><p>While we show that channel models are competitive in few-shot text classification, there are limitations that are avenues for future work. First, it is not as easy to use channel models for non classification tasks where modeling prior distributions is non-trivial. We think future work can obtain the prior with a separate model and incorporate it to the conditional LM as done by <ref type="bibr">Lewis and Fan (2018)</ref>, potentially with beam search decoding as in <ref type="bibr" target="#b34">Yu et al. (2017)</ref>; <ref type="bibr" target="#b32">Yee et al. (2019)</ref>. Second, while this paper focuses on causal LMs, it is an open question how to use a channel model with masked LMs. Although we think channel models are not inherently restricted to causal LMs, the specific way in which existing masked LMs are pretrained makes it hard to use channel models without updating the LM parameters, e.g., masked LMs are not trained to generate a long sentence. We think addressing such limitations would be important for using channel models in a wider range of scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Samples &amp; Verbalizers</head><p>Table <ref type="table">10</ref> shows samples from each dataset. Table 6 shows a list verbalizers (four for each dataset), mainly taken from <ref type="bibr" target="#b4">Gao et al. (2021)</ref> and label words included in the original data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>We use PyTorch <ref type="bibr" target="#b23">(Paszke et al., 2019)</ref> and Huggingface Transformers <ref type="bibr" target="#b29">(Wolf et al., 2020)</ref>. For MR, we use the sentence polarity dataset version 1.0. We use the batch size of 32 and the sequence length of 128 for datasets with short input text (SST-2, SST-5, MR, TREC) and the batch size of 16 and the sequence length of 256 for datasets with long input text (AGNews, Amazon, Yelp, DBPedia, Yahoo, Subj). When the concat-based demonstration method is used, the sequence length is multiplied by the number of training examples, yet is bounded by 1024 which is a strict limit of GPT-2.</p><p>For all finetuning experiments, we train the model for 100 global steps. We use the loss divided by the number of all tokens in the batch. We use Adam optimizer <ref type="bibr" target="#b10">(Kingma and Ba, 2015)</ref> with no weight decay and no warmup steps. For head tuning, transformation tuning and prompt tuning, we use the learning rate {0.1, 0.01, 0.001} and choose the one that gives the lowest training loss on average in order to eliminate the need of the validation data. The chosen learning rate values are reported in Table <ref type="table" target="#tab_7">7</ref>. For all finetuning, we use the learning rate of 10 −5 . For prompt tuning, we use n = 20 prompt tokens which embeddings are initialized from a random subset of the top 5000 vocabularies, following the original paper <ref type="bibr" target="#b14">(Lester et al., 2021)</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: An illustration of the direct model and the channel model for language model prompting in the sentiment analysis task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Different finetuning methods, which compute the distributions of the next token given "A three-hour cinema master class". Yellow boxes indicate trainable parameters; white boxes are frozen parameters. h and V denote the hidden dimension of the LM and the vocabulary size of v(c 1 )...v(c m ), respectively. All finetuning is a typical finetuning method that updates all parameters of the LM (illustrated as a reference). Head tuning, Transformation tuning and Prompt tuning are described in Section 4.2; all of these methods update a very limited number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Varying the number of training examples (K). All models use GPT-2 Large. All, Head and Prompt indicate finetuning all parameters of the LM, head tuning and prompt tuning, respectively. Direct++ Demon and Channel Demon indicate demonstration-based methods (the best out of concat-based and ensemble-based is taken). Models are run 4 times for K = full (4 verbalizers) and 20 times for others (4 verbalizers and 5 data seeds).Channel models are more competitive with smaller K; less competitive with larger K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of imbalance in labels. The average accuracy on SST-2 and MR of different methods with varying ratios of negative labels on the training data (denoted as p − ), when K = 16 (left) or 64 (right). As p − increases, the data is more balanced. Channel models are more robust to imbalanced training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>On binary datasets (SST-2 and MR), we vary the label imbalance in the training data with K = {16, 64}. Specifically, let C = {c + , c − } and p − = |{(x, c) ∈ D|c = c − }|/|D|, i.e., the ratio of negative labels in the training data. We vary p − to be {0, 0.125, 0.250, 0.375, 0.5}. p − = 0.5 means the labels are perfectly balanced, and p − = 0 means that labels in the training data only include c + . We additionally compare with upsampling baselines where we upsample training examples with infrequent labels so that the model has seen an equal number of examples per label during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Liu et al. (2021)  which jointly trains prompt embeddings and the parameters of the LM. Datasets used for experiments. |C| denotes the number of classes.See Appendix A for samples.</figDesc><table><row><cell>Dataset</cell><cell>Task</cell><cell>|C|</cell></row><row><cell>SST-2</cell><cell>Sentiment analysis (movie)</cell><cell>2</cell></row><row><cell>SST-5</cell><cell>Sentiment analysis (movie)</cell><cell>5</cell></row><row><cell>MR</cell><cell>Sentiment analysis (movie)</cell><cell>2</cell></row><row><cell>CR</cell><cell>Sentiment analysis (electronics)</cell><cell>2</cell></row><row><cell>Amazon</cell><cell>Sentiment analysis (Amazon)</cell><cell>5</cell></row><row><cell>Yelp</cell><cell>Sentiment analysis (Yelp)</cell><cell>5</cell></row><row><cell>TREC</cell><cell>Question classification (answer type)</cell><cell>6</cell></row><row><cell cols="2">AGNews News classification (topic)</cell><cell>4</cell></row><row><cell>Yahoo</cell><cell>Question classification (topic)</cell><cell>10</cell></row><row><cell cols="2">DBPedia Ontology classification</cell><cell>14</cell></row><row><cell>Subj</cell><cell>Subjectivity classification</cell><cell>2</cell></row></table><note>, Amazon (McAuley seperate, randomly initialized head instead of the LM head. 5 This is different from prompt tuning in Gao et al. (2021);</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Table 11 and Table 12 in the Ap-Results from demonstration methods. All with GPT-2 Large. Two numbers respectively indicate the average and the worst-case accuracy over different verbalizers (zero-shot and few-shot) and data seeds (few-shot). 'Avg.' in the last row indicate the macro-average across all datasets.</figDesc><table><row><cell>Data</cell><cell></cell><cell>Zero-shot (4 runs)</cell><cell cols="2">Concat-based (20 runs)</cell><cell cols="2">Ensemble-based (20 runs)</cell></row><row><cell></cell><cell>Direct</cell><cell>Direct++ Channel</cell><cell>Direct</cell><cell>Direct++ Channel</cell><cell>Direct</cell><cell>Direct++ Channel</cell></row><row><cell>SST-2</cell><cell cols="6">63.0/51.1 80.3/76.9 77.1/74.8 58.9/50.6 66.8/51.7 85.0/83.1 57.5/50.9 79.7/68.0 77.5/59.5</cell></row><row><cell>SST-5</cell><cell cols="6">27.5/24.4 33.3/28.8 29.2/27.7 27.6/23.0 23.7/14.4 36.2/32.7 25.6/23.2 33.8/23.3 33.6/30.2</cell></row><row><cell>MR</cell><cell cols="6">61.7/50.3 77.4/73.2 74.3/69.3 56.4/50.0 60.2/50.5 80.5/76.8 58.8/50.0 76.8/60.1 76.1/60.0</cell></row><row><cell>CR</cell><cell cols="6">59.2/50.0 77.9/69.7 65.8/60.2 54.7/50.0 66.8/50.0 80.8/74.8 51.0/50.0 72.8/54.6 79.7/69.3</cell></row><row><cell cols="7">Amazon 31.2/22.4 37.6/35.0 37.1/31.6 33.0/21.4 40.8/35.7 39.4/34.3 31.7/23.1 39.8/32.0 40.4/36.2</cell></row><row><cell>Yelp</cell><cell cols="6">33.2/25.6 36.8/31.8 38.0/31.9 32.6/23.3 38.5/31.6 39.8/36.5 31.4/23.6 39.2/29.6 41.5/38.5</cell></row><row><cell cols="7">AGNews 59.8/47.8 59.9/44.0 61.8/59.7 34.0/25.0 51.2/34.4 68.5/60.6 51.9/34.2 73.1/58.6 74.3/69.3</cell></row><row><cell>TREC</cell><cell cols="6">38.7/26.0 27.7/12.6 30.5/19.4 27.2/9.4 31.6/13.0 42.0/26.8 32.1/13.0 22.9/9.8 31.5/23.8</cell></row><row><cell>Yahoo</cell><cell cols="6">20.7/17.8 35.3/28.7 48.7/48.1 13.0/10.0 29.6/19.4 56.2/52.3 16.6/10.7 50.6/46.5 58.6/57.4</cell></row><row><cell cols="7">DBPedia 32.3/18.6 37.6/30.4 51.4/42.7 32.5/7.1 71.1/55.2 58.5/40.0 46.8/17.1 72.6/55.7 64.8/57.0</cell></row><row><cell>Subj</cell><cell cols="6">51.0/49.9 52.0/48.8 57.8/51.5 53.7/49.9 56.9/50.0 60.5/40.8 51.6/49.6 52.2/41.8 52.4/46.9</cell></row><row><cell>Avg.</cell><cell cols="6">43.5/34.9 50.5/43.6 52.0/47.0 38.5/29.1 48.8/36.9 58.9/50.8 41.4/31.4 55.8/43.6 57.3/49.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>We find that head tuning is a very strong method, despite often</figDesc><table><row><cell>Data</cell><cell></cell><cell>Direct</cell><cell></cell><cell>Channel</cell></row><row><cell></cell><cell>Head</cell><cell>Trans</cell><cell>Prompt</cell><cell>Prompt</cell></row><row><cell>SST-2</cell><cell cols="4">80.2/68.6 77.3/57.5 72.6/50.9 85.8/81.3</cell></row><row><cell>SST-5</cell><cell cols="4">34.9/30.0 33.0/25.5 30.9/19.1 36.3/27.9</cell></row><row><cell>MR</cell><cell cols="4">73.7/56.4 71.3/51.6 67.4/50.1 81.7/78.0</cell></row><row><cell>CR</cell><cell cols="4">67.6/50.0 63.9/50.0 65.7/50.0 79.6/76.4</cell></row><row><cell cols="5">Amazon 34.5/28.8 32.1/18.2 31.2/20.0 43.4/39.2</cell></row><row><cell>Yelp</cell><cell cols="4">40.6/32.8 38.9/31.5 31.9/20.6 43.9/37.2</cell></row><row><cell>TREC</cell><cell cols="4">54.1/42.4 48.0/31.0 35.9/13.0 37.1/20.8</cell></row><row><cell cols="5">AGNews 74.1/61.2 66.9/47.0 61.9/25.2 73.4/63.9</cell></row><row><cell>Yahoo</cell><cell cols="4">39.1/31.4 33.8/23.0 27.4/15.7 54.0/46.7</cell></row><row><cell cols="5">DBPedia 49.3/37.5 42.4/28.6 41.8/9.9 67.7/52.9</cell></row><row><cell>Subj</cell><cell cols="4">86.3/79.1 86.0/71.6 65.5/49.9 75.5/58.8</cell></row><row><cell>Avg.</cell><cell cols="4">57.7/47.1 54.0/39.6 48.4/29.5 61.7/53.0</cell></row><row><cell cols="5">Table 4: Performance of tuning methods with a limited</cell></row><row><cell cols="5">number of trainable parameters. All methods use GPT-</cell></row><row><cell cols="5">2 Large, and are run 80 times. Head, Trans, Prompt</cell></row><row><cell cols="5">indicate head tuning, transformation tuning and prompt</cell></row><row><cell cols="5">tuning, respectively. We report the average / worst-case</cell></row><row><cell cols="5">accuracies, separated by a slash. 'Avg.' is the macro-</cell></row><row><cell cols="3">average across all datasets.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Varying the size of LMs from GPT-2 Small to GPT-2 X-Large. The average accuracy (top) and the worst-case accuracy (bottom) are reported. All models are run 20 times (4 verbalizers and 5 data seeds). Head and Prompt indicate head tuning and prompt tuning, respectively. Trends are consistent across different sizes of LM.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>SST-2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TREC</cell><cell></cell><cell></cell><cell></cell><cell>AGNews</cell></row><row><cell>Avg</cell><cell>Accuracy (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50 90</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>10 60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30 80</cell></row><row><cell>Worst-case</cell><cell>Accuracy (%)</cell><cell>Dis Label 40 90</cell><cell>Dis Prompt</cell><cell></cell><cell>Gen Prompt</cell><cell>50 90</cell><cell>Dis Label</cell><cell cols="2">Dis Prompt</cell><cell>Gen Prompt</cell><cell>10 60</cell><cell>Dis Label</cell><cell cols="2">Dis Prompt</cell><cell>Gen Prompt</cell><cell>30 80</cell><cell>Dis Label</cell><cell>Dis Prompt</cell><cell>Gen Prompt</cell></row><row><cell></cell><cell></cell><cell>S</cell><cell>M</cell><cell>L</cell><cell>XL</cell><cell></cell><cell>S</cell><cell>M</cell><cell>L</cell><cell>XL</cell><cell></cell><cell>S</cell><cell>M</cell><cell>L</cell><cell>XL</cell><cell></cell><cell>S</cell><cell>M</cell><cell>L</cell><cell>XL</cell></row><row><cell></cell><cell></cell><cell>Dis Label</cell><cell>Dis Prompt</cell><cell></cell><cell>Gen Prompt</cell><cell></cell><cell cols="3">Dis Label Direct Head Dis Prompt</cell><cell cols="3">Gen Prompt Direct Prompt Dis Label</cell><cell cols="3">Dis Prompt Channel Prompt Gen Prompt</cell><cell></cell><cell>Dis Label</cell><cell>Dis Prompt</cell><cell>Gen Prompt</cell></row><row><cell cols="3">100 Figure 3: 60 Avg Accuracy (%)</cell><cell>SST-2</cell><cell></cell><cell></cell><cell>60 100</cell><cell></cell><cell>MR</cell><cell></cell><cell></cell><cell>20 100</cell><cell></cell><cell>TREC</cell><cell></cell><cell></cell><cell>55 100</cell><cell>AGNews</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>16</cell><cell>64</cell><cell>Full</cell><cell></cell><cell>4</cell><cell>16</cell><cell>64</cell><cell>Full</cell><cell></cell><cell>4</cell><cell>16</cell><cell>64</cell><cell>Full</cell><cell></cell><cell>4</cell><cell>16</cell><cell>64</cell><cell>Full</cell></row><row><cell></cell><cell></cell><cell cols="2">Direct All</cell><cell></cell><cell cols="2">Direct Head</cell><cell></cell><cell cols="2">Direct Prompt</cell><cell></cell><cell cols="3">Channel Prompt</cell><cell></cell><cell cols="3">Direct++ Demon</cell><cell>Channel Demon</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Model performance when there is at least one label at test time that was unseen during training. All models are run 20 times (4 verbalizers and 5 data seeds). All, Head, Trans and Prompt indicate finetuning all parameters of the LM, head tuning, transformation tuning and prompt tuning, respectively. We report the average and the worst-case accuracy, separated by a slash.</figDesc><table><row><cell>Data</cell><cell></cell><cell></cell><cell cols="2">Zero-shot</cell><cell></cell><cell></cell><cell>Finetuning</cell></row><row><cell></cell><cell></cell><cell cols="2">Direct++</cell><cell cols="2">Channel</cell><cell>Direct All</cell><cell>Direct Head</cell><cell>Direct Trans</cell><cell>Direct Prompt</cell><cell>Channel Prompt</cell></row><row><cell cols="2">SST-2</cell><cell cols="4">80.3/76.9 77.1/74.8</cell><cell cols="2">50.2/49.1 50.2/49.1 50.2/49.1 50.2/49.1 85.5/82.5</cell></row><row><cell cols="2">SST-5</cell><cell cols="4">33.3/28.8 29.2/27.7</cell><cell cols="2">40.1/34.8 34.3/28.0 32.6/24.5 30.0/18.1 37.5/32.6</cell></row><row><cell>MR</cell><cell></cell><cell cols="4">77.4/73.2 74.3/69.3</cell><cell cols="2">50.0/50.0 50.0/50.0 50.0/50.0 50.0/50.0 80.9/74.8</cell></row><row><cell>CR</cell><cell></cell><cell cols="4">77.9/69.7 65.8/60.2</cell><cell cols="2">50.0/50.0 50.0/50.0 50.0/50.0 50.0/50.0 80.9/74.8</cell></row><row><cell cols="2">TREC</cell><cell cols="4">27.7/12.6 30.5/19.4</cell><cell cols="2">50.8/31.0 44.8/29.6 44.6/32.8 33.9/17.4 34.3/26.0</cell></row><row><cell>Subj</cell><cell></cell><cell cols="4">52.0/48.8 57.8/51.5</cell><cell cols="2">50.0/50.0 50.0/50.0 50.0/50.0 50.0/50.0 66.6/57.6</cell></row><row><cell>Direct All</cell><cell cols="2">Direct Head</cell><cell cols="2">Direct Prompt</cell><cell cols="2">Channel Prompt</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Model performance when transferred to unseen data, where x-axis indicates training data. Direct Head is not applicable when label space is not shared (when test datasets are TREC, AGNews and Subj). Channel models have better generalization capacity than direct models.</figDesc><table><row><cell>40</cell><cell cols="2">Test data: SST-5</cell><cell>41</cell><cell cols="3">Test data: Amazon</cell><cell>40</cell><cell>Test data: Yelp</cell><cell>35</cell><cell>Test data: TREC</cell></row><row><cell>20</cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell>20</cell></row><row><cell></cell><cell>SST-2</cell><cell>MR</cell><cell></cell><cell cols="2">SST-2</cell><cell>MR</cell><cell></cell><cell>SST-2</cell><cell>MR</cell><cell>AGNews</cell><cell>Yahoo</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Test data: AGNews</cell><cell></cell><cell></cell><cell>70</cell><cell>Test data: Subj</cell></row><row><cell></cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35</cell></row><row><cell></cell><cell></cell><cell>SST-2</cell><cell>MR</cell><cell cols="4">TREC Subj Yahoo DBPedia</cell><cell>SST-2</cell><cell>MR</cell><cell>TREC AGNews Yahoo DBPedia</cell></row><row><cell></cell><cell></cell><cell>Direct All</cell><cell cols="2">Direct Head</cell><cell cols="2">Direct Prompt</cell><cell cols="2">Channel Prompt</cell><cell>Zero-shot Direct++</cell><cell>Zero-shot Channel</cell></row><row><cell>Figure 6:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>. Learning rates of the models in Table4.</figDesc><table><row><cell>Data</cell><cell></cell><cell>Direct</cell><cell></cell><cell>Channel</cell></row><row><cell></cell><cell cols="4">Head Trans Prompt Prompt</cell></row><row><cell>SST-2</cell><cell cols="2">0.001 0.001</cell><cell>0.01</cell><cell>0.001</cell></row><row><cell>SST-5</cell><cell cols="2">0.001 0.001</cell><cell>0.01</cell><cell>0.001</cell></row><row><cell>MR</cell><cell cols="2">0.001 0.001</cell><cell>0.01</cell><cell>0.1</cell></row><row><cell>CR</cell><cell cols="2">0.001 0.001</cell><cell>0.01</cell><cell>0.001</cell></row><row><cell cols="4">Amazon 0.001 0.001 0.001</cell><cell>0.1</cell></row><row><cell>Yelp</cell><cell cols="3">0.001 0.001 0.001</cell><cell>0.01</cell></row><row><cell>TREC</cell><cell cols="2">0.001 0.001</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell cols="3">AGNews 0.001 0.001</cell><cell>0.01</cell><cell>0.1</cell></row><row><cell>Yahoo</cell><cell cols="2">0.001 0.001</cell><cell>0.01</cell><cell>0.001</cell></row><row><cell cols="3">DBPedia 0.001 0.001</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Subj</cell><cell cols="2">0.001 0.001</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Data</cell><cell>Size</cell><cell cols="2">Direct</cell><cell>Channel</cell></row><row><cell></cell><cell></cell><cell cols="3">Head Prompt Prompt</cell></row><row><cell>SST-2</cell><cell cols="2">S,M,XL 0.001</cell><cell>0.01</cell><cell>0.001</cell></row><row><cell>MR</cell><cell cols="2">S,M,XL 0.001</cell><cell>0.01</cell><cell>0.1</cell></row><row><cell>TREC</cell><cell>S</cell><cell>0.01</cell><cell>0.01</cell><cell>0.1</cell></row><row><cell>TREC</cell><cell>M</cell><cell>0.01</cell><cell>0.01</cell><cell>1.0</cell></row><row><cell>TREC</cell><cell>XL</cell><cell>0.001</cell><cell>0.01</cell><cell>0.1</cell></row><row><cell cols="2">AGNews S</cell><cell>0.001</cell><cell>0.01</cell><cell>0.1</cell></row><row><cell cols="2">AGNews M</cell><cell>0.001</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell cols="2">AGNews XL</cell><cell>0.001</cell><cell>0.01</cell><cell>0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Learning rates of the models in Figure3.</figDesc><table><row><cell>Data</cell><cell>k</cell><cell cols="2">Direct</cell><cell>Channel</cell></row><row><cell></cell><cell></cell><cell cols="3">Head Prompt Prompt</cell></row><row><cell>SST-2</cell><cell>4</cell><cell cols="2">0.001 0.001</cell><cell>0.001</cell></row><row><cell>SST-2</cell><cell>64</cell><cell>0.001</cell><cell>0.01</cell><cell>0.001</cell></row><row><cell>SST-2</cell><cell>Full</cell><cell>0.001</cell><cell>0.01</cell><cell>0.1</cell></row><row><cell>MR</cell><cell>4</cell><cell cols="2">0.001 0.001</cell><cell>0.001</cell></row><row><cell>MR</cell><cell cols="2">64,Full 0.001</cell><cell>0.01</cell><cell>0.1</cell></row><row><cell>TREC</cell><cell>4</cell><cell cols="2">0.001 0.001</cell><cell>0.001</cell></row><row><cell>TREC</cell><cell cols="2">64,Full 0.001</cell><cell>0.01</cell><cell>0.1</cell></row><row><cell cols="2">AGNews 4</cell><cell cols="2">0.001 0.001</cell><cell>0.1</cell></row><row><cell cols="2">AGNews 64</cell><cell>0.001</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell cols="2">AGNews Full</cell><cell>0.001</cell><cell>0.01</cell><cell>0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Learning rates of the models in Figure 4. MASK one.; It was MASK.; All in all MASK.; A MASK piece. (MASK={great, terrible}) SST-5, Amaon, Yelp (Same as above.) (MASK={great,good,okay,bad terrible}) TREC MASK: ; Q: MASK: ; Why MASK? ; Answer: MASK (MASK={Description, Entity, Expression, Human, Location, Number}) AGNews Topic: MASK.; Subject: MASK.; This is about MASK.; It is about MASK.</figDesc><table><row><cell>Dataset</cell><cell>Verbalizers</cell></row><row><cell>SST-2, MR</cell><cell>A (MASK={World, Sports, Business, Technology})</cell></row><row><cell></cell><cell>(Same as above) (MASK={Society &amp; Culture, Science &amp; Mathematics, Health, Education &amp;</cell></row><row><cell>Yahoo</cell><cell>Reference, Computers &amp; Internet, Sports, Business &amp; Finance, Entertainment &amp; Music,</cell></row><row><cell></cell><cell>Family &amp; Relationships, Politics &amp; Government})</cell></row></table><note>DBPedia(Same as above) (MASK={Company, Educational Institution, Artist, Athlete, Office Holder, Mean of Transportation, Building, Natural Place, Village, Animal, Plant, Album, Film, Written Work}) Subj This is MASK.; It's all MASK.' It's MASK.; Is it MASK? (MASK={subjective, objective})</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Four different verbalizers for each dataset used in the experiments, separated by ';'. Verbalizers are taken from<ref type="bibr" target="#b4">Gao et al. (2021)</ref> and label words included in the original data.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><ref type="bibr" target="#b6">Holtzman et al. (2021)</ref> explores a zero-shot model that computes the probability of the input given the output based on Pointwise Mutual Information, but with a restriction that the input and the output are interchangeable.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We follow<ref type="bibr" target="#b34">Yu et al. (2017)</ref>;<ref type="bibr" target="#b32">Yee et al. (2019)</ref> in using the terms direct models and channel models. They are often referred as discriminative models and generative models in prior work<ref type="bibr" target="#b33">(Yogatama et al., 2017;</ref> Lewis and Fan, 2018). In principle, these two distinctions are not always equivalent, e.g., a model that computes P (x, y) = P (y|x)P (x) is generative but not the channel model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">In practice, we use length normalization that was found to be effective by<ref type="bibr" target="#b6">Holtzman et al. (2021)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">This is different from head tuning from prior work, e.g., Le Scao and Rush (2021), which finetunes PLM and uses a</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">We also report standard deviation and best-case accuracy in the Appendix.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Ari Holtzman, Eric Wallace, Gabriel Ilharco, Jungsoo Park, Myle Ott, Peter West and Ves Stoyanov for their helpful comments and discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Using machine learning algorithms for breast cancer risk prediction and diagnosis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hiba Asri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Mousannif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Moatassime</surname></persName>
		</author>
		<author>
			<persName><surname>Noël</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<editor>
			<persName><forename type="first">Seit</forename><forename type="middle">Trapit</forename><surname>Ant</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rishikesh</forename><surname>Bansal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tsendsuren</forename><surname>Jha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Munkhdalai</surname></persName>
		</editor>
		<editor>
			<persName><surname>Mccallum</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2016">2016. 2020</date>
		</imprint>
	</monogr>
	<note>Self-supervised meta-learning for few-shot natural language classification tasks</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">A</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Della</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietra</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mix-Text: Linguistically-informed interpolation of hidden space for semi-supervised text classification</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. Xiaoan Ding and Kevin Gimpel</title>
				<editor>
			<persName><forename type="first">Acl</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2020. 2020. 2018. 2019</date>
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vered</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08315</idno>
		<title level="m">Surface form competition: Why the highest probability answer isn&apos;t always right</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for nlp</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language to structured query generation via meta-learning</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">How can we know what language models know? TACL</title>
				<imprint>
			<date type="published" when="2018-06">2018. Jun Araki, and Graham Neubig. 2020</date>
		</imprint>
	</monogr>
	<note>NAACL. Zhengbao Jiang, Frank F Xu</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How many data points is a prompt worth</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<title level="m">Dbpedia-a large-scale, multilingual knowledge base extracted from wikipedia</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Semantic web</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generative question answering: Learning to answer the whole question</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
	</analytic>
	<monogr>
		<title level="m">ACL. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt understands, too</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cutting down on prompts and parameters: Simple few-shot learning with language models</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Balaževic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13353</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08786</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM conference on Recommender systems</title>
				<meeting>the 7th ACM conference on Recommender systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adversarial training methods for semisupervised text classification</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<editor>
			<persName><forename type="first">Neurips</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2002">2002. 2004</date>
		</imprint>
	</monogr>
	<note>On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11447</idno>
	</analytic>
	<monogr>
		<title level="m">True few-shot learning with language models</title>
				<editor>
			<persName><forename type="first">Ethan</forename><surname>Neurips</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douwe</forename><surname>Perez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kyunghyun</forename><surname>Kiela</surname></persName>
		</editor>
		<editor>
			<persName><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning how to ask: Querying lms with mixtures of soft prompts</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners. OpenAI blog. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2019. 2017</date>
		</imprint>
	</monogr>
	<note>Learning multiple visual domains with residual adapters. In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Claude</forename><surname>Elwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="1948">1948. 2020</date>
		</imprint>
	</monogr>
	<note>A mathematical theory of communication. The Bell system technical journal</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building a question answering test collection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><surname>Tice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><surname>Gugger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP: System Demonstrations</title>
				<editor>
			<persName><forename type="first">Mariama</forename><surname>Drame</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Quentin</forename><surname>Lhoest</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A syntaxbased statistical translation model</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simple and effective noisy channel modeling for neural machine translation</title>
		<author>
			<persName><forename type="first">Kyra</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Generative and discriminative text classification with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01898</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The neural noisy channel</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName><forename type="first">Tony</forename><forename type="middle">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05240</idno>
		<editor>ICML. Zexuan Zhong, Dan Friedman, and Danqi Chen</editor>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Factual probing is [mask]: Learning vs. learning to recall</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>Avg</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Full results from demonstration methods when a concat-based method is used; analogous to Table 3. Avg, Std, Best and Worst indicate the average accuracy, standard deviation, the best-case accuracy and the worstcase accuracy, respectively</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>Best when combined with Table 12</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">DBPedia</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><surname>Avg</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Full results from tuning methods when a ensemble-based method is used; analogous to Table 3. Avg, Std, Best and Worst indicate the average accuracy, standard deviation, the best-case accuracy and the worst-case accuracy, respectively. Bold: Best when combined with Table 11</title>
	</analytic>
	<monogr>
		<title level="j">Data Direct Head Direct Trans Direct Prompt Channel Prompt</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>Table</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><surname>Avg</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Prompt indicate head tuning, transformation tuning and prompt tuning, respectively. Avg, Std, Best and Worst indicate the average accuracy</title>
	</analytic>
	<monogr>
		<title level="m">Full results from tuning methods; analogous to Table 4. Head</title>
				<meeting><address><addrLine>Trans</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
	<note>standard deviation, the best-case accuracy and the worst-case accuracy, respectively</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
