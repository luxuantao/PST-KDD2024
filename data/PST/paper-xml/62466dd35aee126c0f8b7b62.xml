<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Equivariant Diffusion for Molecule Generation in 3D</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-31">31 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Emiel</forename><surname>Hoogeboom</surname></persName>
							<email>&lt;e.hoogeboom@uva.nl&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">UvA-Bosch Delta Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Victor</forename><forename type="middle">Garcia</forename><surname>Satorras</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">UvA-Bosch Delta Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cl?ment</forename><surname>Vignac</surname></persName>
							<email>&lt;clement.vignac@epfl.ch&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">UvA-Bosch Delta Lab</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Victor Garcia Satorras</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Equivariant Diffusion for Molecule Generation in 3D</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-31">31 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.17003v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model (EDM) learns to denoise a diffusion process with an equivariant network that jointly operates on both continuous (atom coordinates) and categorical features (atom types). In addition, we provide a probabilistic analysis which admits likelihood computation of molecules using our model. Experimentally, the proposed method significantly outperforms previous 3D molecular generative methods regarding the quality of generated samples and efficiency at training time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modern deep learning methods are starting to make an important impact on molecular sciences. Behind the success of Alphafold in protein folding prediction <ref type="bibr" target="#b0">(AlQuraishi, 2019)</ref>, an increasing body of literature develops deep learning models to analyze or synthesize (in silico) molecules <ref type="bibr" target="#b36">(Simonovsky &amp; Komodakis, 2018;</ref><ref type="bibr" target="#b11">Gebauer et al., 2019;</ref><ref type="bibr" target="#b19">Klicpera et al., 2020;</ref><ref type="bibr" target="#b35">Simm et al., 2021)</ref>.</p><p>Molecules live in the physical 3D space, and as such are subject to geometric symmetries such as translations, rotations, and possibly reflections. These symmetries are referred to as the Euclidean group in 3 dimensions, E(3). Leveraging these symmetries in molecular data is important for good generalization and has been extensively studied <ref type="bibr" target="#b39">(Thomas et al., 2018;</ref><ref type="bibr" target="#b8">Fuchs et al., 2020;</ref><ref type="bibr" target="#b7">Finzi et al., 2020)</ref>.</p><p>Although developed for discriminative tasks, E(n) equivariant layers can also be used for molecule generation in 3D. In particular, they have been integrated into autoregressive models <ref type="bibr" target="#b10">(Gebauer et al., 2018;</ref><ref type="bibr" target="#b0">2019)</ref> which artificially introduce an order in the atoms and are known to be difficult to scale during sampling <ref type="bibr">(Xu et al., 2021b)</ref>. Alternatively, Figure <ref type="figure">1</ref>. Overview of the EDM. To generate a molecule, a normal distributed set of points is denoised into a molecule consisting of atom coordinates x in 3D and atom types h. As the model is rotation equivariant, the likelihood is preserved when a molecule is rotated by R.</p><p>continuous-time normalizing flows such as <ref type="bibr" target="#b20">K?hler et al. (2020)</ref> or E-NF <ref type="bibr">(Satorras et al., 2021a)</ref> are expensive to train since they have to integrate a differential equation, leading to limited performance and scalability.</p><p>In this work, we introduce E(3) Equivariant Diffusion Models (EDMs). EDMs learn to denoise a diffusion process that operates on both continuous coordinates and categorical atom types. To the best of our knowledge, it is the first diffusion model that directly generates molecules in 3D space. Our method does not require a particular atom ordering (in contrast to autoregressive models) and can be trained much more efficiently than normalizing flows. To give an example, EDMs generate up to 16 times more stable molecules than E-NFs when trained on QM9, while requiring half of the training time. This favourable scaling behaviour allows EDMs to be trained on larger drug-like datasets such as GEOM-Drugs <ref type="bibr" target="#b3">(Axelrod &amp; Gomez-Bombarelli, 2020)</ref>.</p><p>Our contributions can be summarized as follows. We introduce an equivariant denoising diffusion model that operates on atom coordinates and categorical features. We add a probabilistic analysis which allows likelihood computation and this analysis is consistent with continuous and categorical features. We show that our method outperforms previous molecule generation models in log-likelihood and molecule stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Diffusion Models</head><p>Diffusion models learn distributions by modelling the reverse of a diffusion process: a denoising process. Given a data point x, a diffusion process that adds noise to z t for t = 0, . . . , T is defined by the multivariate normal distribution:</p><formula xml:id="formula_0">q(z t |x) = N (z t |? t x t , ? 2 t I),<label>(1)</label></formula><p>where ? t ? R + controls how much signal is retained and ? t ? R + controls how much noise is added. In general, ? t is modelled by a function that smoothly transitions from ? 0 ? 1 towards ? T ? 0. A special case of noising process is the variance preserving process <ref type="bibr" target="#b37">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b15">Ho et al., 2020)</ref> for which ? t = 1 -? 2 t . Following <ref type="bibr" target="#b18">Kingma et al. (2021)</ref>, we define the signal to noise ratio SNR(t) = ? 2 t /? 2 t , which simplifies notations. This diffusion process is Markov and can be equivalently written with transition distributions as:</p><formula xml:id="formula_1">q(z t |z s ) = N (z t |? t|s z s , ? 2 t|s I),<label>(2)</label></formula><p>for any t &gt; s with ? t|s = ? t /? s and ? 2 t|s = ? 2 t -? 2 t|s ? 2 s . The entire noising process is then written as: q(z 0 , z 1 , . . . , z T |x) = q(z 0 |x) T t=1 q(z t |z t-1 ). (3)</p><p>The posterior of the transitions conditioned on x gives the inverse of the noising process, the true denoising process. It is also normal and given by: q(z s |x, z t ) = N (z s |? t?s (x, z t ), ? 2 t?s I),</p><p>where the definitions for ? t?s (x, z t ) and ? t?s can be analytically obtained as</p><formula xml:id="formula_3">?t?s(x, zt) = ? t|s ? 2 s ? 2 t zt + ?s? 2 t|s ? 2 t</formula><p>x and ?t?s = ? t|s ?s ?t .</p><p>The Generative Denoising Process In contrast to other generative models, in diffusion models, the generative process is defined with respect to the true denoising process.</p><p>The variable x, which is unknown to the generative process, is replaced by an approximation x = ?(z t , t) given by a neural network ?. Then the generative transition distribution p(z s |z t ) is chosen to be q(z s | x(z t , t), z t ). Similarly to Eq. 4, it can be expressed using the approximation x as:</p><formula xml:id="formula_4">p(z s |z t ) = N (z s |? t?s ( x, z t ), ? 2 t?s I).<label>(5)</label></formula><p>With the choice s = t -1, a variational lower bound on the log-likelihood of x given the generative model is given by:</p><formula xml:id="formula_5">log p(x) ? L 0 + L base + T t=1 L t ,<label>(6)</label></formula><p>where L 0 = log p(x|z 0 ) models the likelihood of the data given z 0 , L base = -KL(q(z T |x)|p(z T )) models the distance between a standard normal distribution and the final latent variable q(z T |x), and</p><formula xml:id="formula_6">L t = -KL(q(z s |x, z t )|p(z s |z t )) for t = 1, . . . , T.</formula><p>While in this formulation the neural network directly predicts x, <ref type="bibr" target="#b15">Ho et al. (2020)</ref> found that optimization is easier when predicting the Gaussian noise instead. Intuitively, the network is trying to predict which part of the observation z t is noise originating from the diffusion process, and which part corresponds to the underlying data point x. Specifically, if z t = ? t x + ? t , then the neural network ? outputs ? = ?(z t , t), so that:</p><formula xml:id="formula_7">x = (1/? t ) z t -(? t /? t ) ?<label>(7)</label></formula><p>As shown in <ref type="bibr" target="#b18">(Kingma et al., 2021)</ref>, with this parametrization L t simplifies to:</p><formula xml:id="formula_8">Lt = E ?N (0,I) 1 2 (1 -SNR(t -1)/SNR(t))|| -? || 2 (8)</formula><p>In practice the term L base is close to zero when the noising schedule is defined in such a way that ? T ? 0. Furthermore, if ? 0 ? 1 and x is discrete, then L 0 is close to zero as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Equivariance</head><p>A function f is said to be equivariant to the action of a group</p><formula xml:id="formula_9">G if T g (f (x)) = f (S g (x)</formula><p>) for all g ? G, where S g , T g are linear representations related to the group element g <ref type="bibr" target="#b32">(Serre, 1977)</ref>. In this work, we consider the Euclidean group E(3) generated by translations, rotations and reflections, for which S g and T g can be represented by a translation t and an orthogonal matrix R that rotates or reflects coordinates. f is then equivariant to a rotation or reflection R if transforming its input results in an equivalent transformation of its output, or Rf (x) = f (Rx).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equivariant Distributions and Diffusion</head><p>In our setting, a conditional distribution p(y|x) is equivariant to the action of rotations and reflections when</p><formula xml:id="formula_10">p(y|x) = p(Ry|Rx) for all orthogonal R. (<label>9</label></formula><formula xml:id="formula_11">)</formula><p>A distribution is invariant to R transformations if p(y) = p(Ry) for all orthogonal R. (10) <ref type="bibr" target="#b20">K?hler et al. (2020)</ref> showed that an invariant distribution composed with an equivariant invertible function results</p><p>Figure <ref type="figure">2</ref>. Overview of the Equivariant Diffusion Model. To generate molecules, coordinates x and features h are generated by denoising variables zt starting from standard normal noise zT . This is achieved by sampling from the distributions p(zt-1|zt) iteratively. To train the model, noise is added to a datapoint x, h using q(zt|x, h) for the step t of interest, which the network then learns to denoise.</p><p>in an invariant distribution. Furthermore, <ref type="bibr" target="#b42">Xu et al. (2022)</ref> proved that if x ? p(x) is invariant to a group and the transition probabilities of a Markov chain y ? p(y|x) are equivariant, then the marginal distribution of y at any time step is invariant to group transformations as well. This is helpful as it means that if p(z T ) is invariant and the neural network used to parametrize p(z t-1 |z t ) is equivariant, then the marginal distribution p(x) of the denoising model will be an invariant distribution as desired.</p><p>Points and Features in E(3) In this paper, we consider point clouds</p><formula xml:id="formula_12">x = (x 1 , . . . , x M ) ? R M ?3 with correspond- ing features h = (h 1 , . . . , h M ) ? R M ?nf .</formula><p>The features h are invariant to group transformations, and the positions are affected by rotations, reflections and translations as Rx + t = (Rx 1 + t, . . . , Rx M + t) where R is an orthogonal matrix 1 . The function</p><formula xml:id="formula_13">(z x , z h ) = f (x, h) is E(3)</formula><p>equivariant if for all orthogonal R and t ? R 3 we have:</p><formula xml:id="formula_14">Rz x + t, z h = f (Rx + t, h)<label>(11)</label></formula><p>E(n) Equivariant Graph Neural Networks (EGNNs) <ref type="bibr">(Satorras et al., 2021b)</ref> are a type of Graph Neural Network that satisfies the equivariance constraint (11). In this work, we consider interactions between all atoms, and therefore assume a fully connected graph G with nodes v i ? V.</p><p>Each node v i is endowed with coordinates x i ? R 3 as well as features h i ? R d . In this setting, EGNN consists of the composition of Equivariant Convolutional Layers x l+1 , h l+1 = EGCL[x l , h l ] which are defined as:</p><formula xml:id="formula_15">m ij = ? e h l i , h l j , d 2 ij , a ij , h l+1 i = ? h (h l i , j =i ?ij m ij ), x l+1 i = x l i + j =i x l i -x l j d ij + 1 ? x h l i , h l j , d 2 ij , a ij ,<label>(12)</label></formula><p>1 As a matrix-multiplication the left-hand side would be written xR T . Formally Rx can be seen as a group action of R on x.</p><p>where l indexes the layer, and d ij = x l i -x l j 2 is the euclidean distance between nodes (v i , v j ), and a ij are optional edge attributes. The difference (x l i -x l j ) in Equation 12 is normalized by d ij + 1 as done in <ref type="bibr">(Satorras et al., 2021a)</ref> for improved stability, as well as the attention mechanism which infers a soft estimation of the edges ?ij = ? inf (m ij ). All learnable components (? e , ? h , ? x and ? inf ) are parametrized by fully connected neural networks (cf. Appendix B for details). An entire EGNN architecture is then composed of L EGCL layers which applies the following non-linear transformation x, ? = EGNN[x 0 , h 0 ]. This transformation satisfies the required equivariant property in Equation <ref type="formula" target="#formula_14">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EDM: E(3) Equivariant Diffusion Model</head><p>In this section we describe EDM, an E(3) Equivariant Diffusion Model. EDM defines a noising process on both node positions and features, and learns the generative denoising process using an equivariant neural network. We also determine the equations for log-likelihood computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Diffusion Process</head><p>We first define an equivariant diffusion process for coordinates x i with atom features h i that adds noise to the data. Recall that we consider a set of points {(x i , h i )} i=1,...,M , where each node has associated to it a coordinate representation x i ? R n and an attribute vector h i ? R nf . Let [?, ?] denote a concatenation. We define the equivariant noising process on latent variables</p><formula xml:id="formula_16">z t = [z (x) t , z (h) t ] as: q(z t |x, h) = N xh (z t |? t [x, h], ? 2 t I)<label>(13)</label></formula><p>for t = 1, . . . , T where N xh is concise notation for the product of two distributions, one for the noised coordinates N x and another for the noised features N given by:</p><formula xml:id="formula_17">N x (z (x) t |? t x, ? 2 t I) ? N (z (h) t |? t h, ? 2 t I)<label>(14)</label></formula><p>Algorithm 1 Optimizing EDM Input: Data point x, neural network ? Sample t ? U(0, . . . , T ), ? N (0, I) Subtract center of gravity from</p><formula xml:id="formula_18">(x) in = [ (x) , (h) ] Compute z t = ? t [x, h] + ? t Minimize || -?(z t , t)|| 2</formula><p>Algorithm 2 Sampling from EDM Sample z T ? N (0, I) for t in T, T -1, . . . , 1 where s = t -1 do Sample ? N (0, I) Subtract center of gravity from (x) in = [ (x) , (h) ]</p><formula xml:id="formula_19">z s = 1 ? t|s z t - ? 2 t|s ? t|s ?t ? ?(z t , t) + ? t?s ? end for Sample x, h ? p(x, h|z 0 )</formula><p>These equations correspond to Equation 1 in a standard diffusion model. Also, a slight abuse of notation is used to aid readability: technically x, h, z t are two-dimensional variables with an axis for the point identifier and an axis for the features. However, in the distributions they are treated as if flattened to a vector.</p><p>As explained in <ref type="bibr">(Satorras et al., 2021a)</ref> it is impossible to have a non-zero distribution that is invariant to translations, since it cannot integrate to one. However, one can use distributions on the linear subspace where the center of gravity is always zero. Following <ref type="bibr" target="#b42">(Xu et al., 2022)</ref> that showed that such a linear subspace can be used consistently in diffusion, N x is defined as a normal distribution on the subspace defined by i x i = 0 for which the precise definition is given in Appendix A.</p><p>Since the features h are invariant to E(n) transformations, the noise distribution for these features can be the conventional normal distribution N . Although similar to standard diffusion models, depending on whether data is categorical (for the atom type), ordinal (for atom charge), or continuous, different starting representations of h may be desirable and require different treatment in L 0 , on which we will expand in Section 3.3.</p><p>The Generative Denoising Process To define the generative process, the noise posteriors q(z s |x, h, z t ) of Equation 13 can be used in the same fashion as in Equation 4 by replacing the data variables x, h by neural network approximations x, ?:</p><formula xml:id="formula_20">p(z s |z t ) = N xh (z s |? t?s ([ x, ?], z t ), ? 2 t?s I)<label>(15)</label></formula><p>where x, ? depend on z t , t and the neural network ?. As conventional in modern diffusion models, we use the noise parametrization to obtain x, ?. Instead of directly predicting them, the network ? outputs ? = [? (x) , ? (h) ] which is then used to compute:</p><formula xml:id="formula_21">[ x, ?] = z t /? t -? t ? ? t /? t (16)</formula><p>If ? t is computed by an equivariant function ? then the denoising distribution in Equation 15 is equivariant. To see this, observe that rotating z t to Rz t gives R? t = ?(Rz t , t). Furthermore, the mean of the denoising equation rotates</p><formula xml:id="formula_22">R x = Rz (x) t /? t -R? (x)</formula><p>t ? t /? t and since the noise is isotropic, the distribution is equivariant as desired.</p><p>To sample from the model, one first samples z T ? N xh (0, I) and then iteratively samples z t-1 ? p(z t-1 |z t ) for t = T, . . . , 1 and then finally samples x, h ? p(x, h|z 0 ), as described in Algorithm 2.</p><p>Optimization Objective Recall that a likelihood term of this model is given by</p><formula xml:id="formula_23">L t = -KL(q(z s |x, z t )||p(z s |z t )).</formula><p>Analogous to Equation <ref type="formula">8</ref>, in this parametrization the term simplifies to:</p><formula xml:id="formula_24">L t = E t?Nxh(0, I) 1 2 w(t) || t -? t || 2 , (<label>17</label></formula><formula xml:id="formula_25">)</formula><p>where w(t) = (1-SNR(t-1)/SNR(t)) and ? t = ?(z t , t). This is convenient: even though parts of the distribution of N xh operate on a subspace, the simplification in Equation <ref type="formula">8</ref>also holds here, and can be computed for all components belonging to x and h at once. There are three reason why this simplification remains true: firstly, N x and N within N xh are independent, so the divergence can be separated into two divergences. Further, the KL divergence between the N x components are still compatible with the standard KL equation for normal distributions, as they rely on a Euclidean distance (which is rotation invariant) and the distributions are isotropic with equal variance. Finally, because of the similarity in KL equations, the results can be combined again by concatenating the components in x and h. For a more detailed argument see Appendix A. An overview of the optimization procedure is given in Algorithm 1.</p><p>Following <ref type="bibr" target="#b15">(Ho et al., 2020)</ref> during training we set w(t) = 1 as it stabilizes training and it is known to improve sample quality for images. Experimentally we also found this to hold true for molecules: even when evaluating the probabilistic variational objective for which w(t) = (1-SNR(t-1)/SNR(t)), the model trained with w(t) = 1 outperformed models trained with the variational w(t).</p><p>In summary, we have defined a diffusion process, a denoising model and an optimization objective between them. To further specify our model, we need to define the neural network ? that is used within the denoising model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Dynamics</head><p>We learn the E(n) equivariant dynamics function</p><formula xml:id="formula_26">[? (x) t , ? (h) t ] = ?(z (x) t , z<label>(h)</label></formula><p>t , t) of the diffusion model using the equivariant network EGNN introduced in Section 2.2 in the following way:</p><formula xml:id="formula_27">? (x) t , ? (h) t = EGNN(z (x) t , [z (h) t , t/T ]) -[z (x) t , 0 ] Notice that we simply input z (x) t , z (h) t</formula><p>to the EGNN with the only difference that t/T is concatenated to the node features. The estimated noise ? (x) t is given by the output of the EGNN from which the input coordinates z (x) t are removed. Importantly, since the outputs have to lie on a zero center of gravity subspace, the component ? (x) t is projected down by subtracting its center of gravity. This then satisfies the rotational and reflection equivariance on x with the parametrization in Equation <ref type="formula">16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Zeroth Likelihood Term</head><p>In typical diffusion models <ref type="bibr" target="#b15">(Ho et al., 2020)</ref>, the data being modelled is ordinal which makes the design of L (h) 0 = log p(h|z (h) 0 ) relatively simple. Specifically, under very small noise perturbations of the original data distribution p data (h) (when ? 0 ? 1 and ? 0 ? 0) we have</p><formula xml:id="formula_28">q(h|z (h) 0 ) = q(z (h) 0 |h)p data (h) h q(z (h) 0 |h)p data (h) ? 1, when z (h) 0 is sampled from the noising process q(z (h) 0 |h). Because q(z (h) 0 |h</formula><p>) is a highly peaked distribution, in practice it tends to zero for all but one single discrete state of h. Furthermore, p data is constant over this small peak, and thus q(h|z</p><formula xml:id="formula_29">(h) 0 ) ? 1 when h is the closest integer value to z (h) 0 .</formula><p>This can be used to model integer molecular properties such as the atom charge. Following standard practice we let:</p><formula xml:id="formula_30">p(h|z (h) 0 ) = h+ 1 2 h-1 2 N (u|z (h) 0 , ? 0 )du,<label>(18)</label></formula><p>which most likely equals 1 for reasonable noise parameters ? 0 , ? 0 and it computed as</p><formula xml:id="formula_31">?((h + 1 2 -z (h) 0 )/? 0 ) -?((h - 1 2 -z (h) 0 )/? 0 )</formula><p>where ? is the CDF of a standard normal distribution. For categorical features such as the atom types, this model would however introduce an undesired bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categorical features</head><p>For categorical features such as the atom type, the aforementioned integer representation is unnatural and introduces bias. Instead of using integers for these features, we operate directly on a one-hot representation. Suppose h is an array whose values represent categories in {c 1 , ..., c d } such as atom types. Then h is encoded with a one-hot function h ? h onehot such that h onehot i,j = 1 hi=cj . The noising process over z (h) t can then directly be defined using the one-hot representation h onehot equivalent to its definition for integer values, i.e. q(z (h)</p><formula xml:id="formula_32">t |h) = N (z (h) t |? t h onehot , ? 2 t I</formula><p>) with the only difference that z (h) t has an additional dimension axis with the size equal to the number of categories. Since the data is discrete and the noising process is assumed to be well defined, by the same reasoning as for integer data we can define probability parameters p to be proportional to the normal distribution integrated from 1 -1 2 to 1 + 1 2 . Intuitively, when a small amount of noise is sampled and added to the one-hot representation, then the value corresponding to the active class will almost certainly be between 1 -1 2 and 1 + 1 2 :</p><p>p(h|z</p><formula xml:id="formula_33">(h) 0 ) = C(h|p), p ? 1+ 1 2 1-1 2 N (u|z (h) 0 , ? 0 )du</formula><p>where p is normalized to sum to one and C is a categorical distribution. In practice this distribution will almost certainly equal one for values z (h) 0 that were sampled from the diffusion process given h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous positions For continuous positions, defining L</head><formula xml:id="formula_34">(x) 0 = log p(x|z (x) 0</formula><p>) is a little more involved than for discrete features. A similar analysis assuming p data (x) is constant results in:</p><formula xml:id="formula_35">q(x|z (x) 0 ) = q(z (x) 0 |x)p data (x) x q(z (x) 0 |x)p data (x) ? q(z (x) 0 |x) x q(z (x) 0 |x) ,</formula><p>which by completing the square we find is equal to</p><formula xml:id="formula_36">N x (x|z (x) 0 /? 0 , ? 2 0 /? 2 0 I).</formula><p>Empirically, we find that our model achieves better likelihood performance, especially with lower SNR rates, when we use x as a prediction for the mean. After simplifying the terms it turns out that this essentially adds an additional correction term containing the estimated noise ? (x) 0 , which originates from Equation <ref type="formula">16</ref>and can be written as:</p><formula xml:id="formula_37">p(x|z 0 ) = N x z (x) 0 /? 0 -? 0 /? 0 ? 0 , ? 2 0 /? 2 0 I . (19)</formula><p>When this parametrization is chosen the log likelihood component L (x) 0 can be re-written to:</p><formula xml:id="formula_38">L (x) 0 = E (x) ?Nx(0,I) log Z -1 - 1 2 || (x) -? (x) (z0, 0)|| 2 ,</formula><p>with the normalization constant Z. Conveniently, this allows Equation 17 for losses L t to also be used for t = 0 in its x component, by defining w(0) = -1. The normalization constant then has to be added separately. This normalization constant Z = (</p><formula xml:id="formula_39">? 2? ? ? 0 /? 0 ) (M -1)</formula><p>?n where the (M -1) ? n arises from the zero center of gravity subspace is described in Appendix A.</p><p>Scaling Features Since coordinates, atom types and charges represent different physical quantities, we can define a relative scaling between them. While normalizing the features simply makes them easier to process for the neural network, the relative scaling has a deeper impact on the model: when the features h are defined on a smaller scale than the coordinates x, the denoising process tends to first determine rough positions and decide on the atom types only afterwards. Whereas scaling x requires a correction in the log-likelihood since it is continuous, scaling h does not require a correction and is not problematic as long as the difference in discrete values is large compared to ? 0 . We find empirically that defining the input to our EDM model as [x, 0.25 h onehot , 0.1 h atom charge ] significantly improves performance over non-scaled inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Atoms</head><p>In the above sections we have considered the number of atoms M to be known beforehand. To adapt to different molecules with different sizes, we compute the categorical distribution p(M ) of molecule sizes on the training set. To sample from the model p(x, h, M ), M ? p(M ) is first sampled and then x, h ? p(x, h|M ) are sampled from the EDM. For clarity this conditioning on M is often omitted, but it remains an important part of the generative process and likelihood computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Conditional generation</head><p>In this section we describe a straightforward extension to the proposed method to do conditional generation x, h ? p(x, h|c) given some desired property c. We can define the optimization lower bound for the conditional case as log p(x, h|c) ? L c,0 + L c,base + T t=1 L c,t , where the different L c,t for 1 ? t &lt; T -1 are defined similarly to Equation <ref type="formula" target="#formula_24">17</ref>, with the important difference that the function ? t = ?(z t , [t, c]) takes as additional input a property c which is concatenated to the nodes features. Given a trained conditional model we define the generative process by first sampling the number of nodes M and a property value c from a parametrized distribution c, M ? p(c, M ) defined in Appendix E. Next, we can generate molecules x, h given c, M using our Conditional EDM x, h ? p(x, h|c, M ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Diffusion models <ref type="bibr" target="#b37">(Sohl-Dickstein et al., 2015)</ref> are generative models that have recently been connected to score-based methods via denoising diffusion models <ref type="bibr" target="#b38">(Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b15">Ho et al., 2020)</ref>. This new family of generative models has proven to be very effective for the generation of data such as images <ref type="bibr" target="#b15">(Ho et al., 2020;</ref><ref type="bibr" target="#b27">Nichol &amp; Dhariwal, 2021)</ref>.</p><p>In molecule generation, there are some recent methods that generate molecules directly in their 3D form. G-Schnet <ref type="bibr" target="#b11">(Gebauer et al., 2019)</ref> defines an autoregressive distribution from which atoms can be iteratively sampled, and can be used for targeted generation <ref type="bibr" target="#b12">(Gebauer et al., 2021)</ref>. E-NF <ref type="bibr">(Satorras et al., 2021a)</ref> defines an equivariant normalizing flow via a differential equation. Instead of integrating a differential equation, our method learns to denoise a diffusion process, which scales more favourably during training.</p><p>A related branch of literature is concerned by solely pre-dicting coordinates from molecular graphs, referred to as the conformation. Examples of such methods utilize conditional VAEs <ref type="bibr" target="#b34">(Simm &amp; Hern?ndez-Lobato, 2019)</ref>, Wasserstein GANs <ref type="bibr" target="#b16">(Hoffmann &amp; No?, 2019)</ref>, and normalizing flows <ref type="bibr" target="#b28">(No? et al., 2019)</ref>, with adaptions for Euclidean symmetries in <ref type="bibr" target="#b20">(K?hler et al., 2020;</ref><ref type="bibr">Xu et al., 2021a;</ref><ref type="bibr" target="#b35">Simm et al., 2021;</ref><ref type="bibr" target="#b9">Ganea et al., 2021;</ref><ref type="bibr" target="#b14">Guan et al., 2022)</ref> resulting in performance improvements. In recent works <ref type="bibr" target="#b33">(Shi et al., 2021;</ref><ref type="bibr" target="#b25">Luo et al., 2021;</ref><ref type="bibr" target="#b42">Xu et al., 2022)</ref> it was shown that score-based and diffusion models are effective at coordinate prediction, especially when the underlying neural network respects the symmetries of the data. Our work can be seen as an extension of these methods that incorporates discrete atom features, and furthermore derives the equations required for log-likelihood computation. In the context of diffusion for discrete variables, unrelated to molecule modelling, discrete diffusion processes have been proposed <ref type="bibr" target="#b37">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b17">Hoogeboom et al., 2021;</ref><ref type="bibr" target="#b2">Austin et al., 2021)</ref>. However, for 3D molecule generation these would require a separate diffusion process for the discrete features and the continuous coordinates. Instead we define a joint process for both of them.</p><p>Tangentially related, other methods generate molecules in graph representation. Some examples are autoregressive methods such as <ref type="bibr" target="#b24">(Liu et al., 2018;</ref><ref type="bibr" target="#b44">You et al., 2018;</ref><ref type="bibr" target="#b23">Liao et al., 2019)</ref>, and one-shot approaches such as <ref type="bibr" target="#b36">(Simonovsky &amp; Komodakis, 2018;</ref><ref type="bibr" target="#b6">De Cao &amp; Kipf, 2018;</ref><ref type="bibr" target="#b5">Bresson &amp; Laurent, 2019;</ref><ref type="bibr" target="#b21">Kosiorek et al., 2020;</ref><ref type="bibr" target="#b22">Krawczuk et al., 2021)</ref>. However such methods do not provide conformer information which is useful for many downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Molecule Generation -QM9</head><p>QM9 <ref type="bibr" target="#b29">(Ramakrishnan et al., 2014)</ref> is a standard dataset that contains molecular properties and atom coordinates for 130k small molecules with up to 9 heavy atoms (29 atoms including hydrogens). In this experiment we train EDM to unconditionally generate molecules with 3-dimensional coordinates, atom types (H, C, N, O, F) and integer-valued atom charges. We use the train/val/test partitions introduced in <ref type="bibr" target="#b1">(Anderson et al., 2019)</ref>, which consists of 100K/18K/13K samples respectively for each partition.</p><p>Metrics Following <ref type="bibr">(Satorras et al., 2021a)</ref>, we use the distance between pairs of atoms and the atom types to predict bond types (single, double, triple or none). We then measure atom stability (the proportion of atoms that have the right valency) and molecule stability (the proportion of generated molecules for which all atoms are stable).</p><p>Baselines: We compare EDM to two existing E(3) equivariant models: G-Schnet <ref type="bibr" target="#b11">(Gebauer et al., 2019)</ref> and Equivariant Normalizing Flows (E-NF) <ref type="bibr">(Satorras et al., 2021a)</ref>. For G-  Schnet we extracted 10000 samples from the publicly available code to run the analysis. In order to demonstrate the benefits of equivariance, we also perform an ablation study and run a non-equivariant variation of our method that we call Graph Diffusion Models (GDM). The Graph diffusion model is run with the same configuration as our method, except that the EGNN is replaced by a non-equivariant graph network defined in Appendix C. We also experiment with GDM-aug, where the GDM model is trained on data augmented with random rotations. All models use 9 layers, 256 features per layer and SiLU activations. They are trained using Adam with batch size 64 and learning rate 10 -4 .</p><p>Results are reported in Table <ref type="table" target="#tab_0">1</ref>. Our method outperforms previous methods (E-NF and G-Schnet), as well as its nonequivariant counterparts on all metrics. It is interesting to note that the negative log-likelihood of the EDM is much lower than other models, which indicates that it is able to create sharper peaks in the model distribution.</p><p>Further, EDMs are compared to one-shot graph-based molecule generation models that do not operate on 3D coordinates: GraphVAE <ref type="bibr" target="#b36">(Simonovsky &amp; Komodakis, 2018)</ref>, GraphTransformerVAE <ref type="bibr" target="#b26">(Mitton et al., 2021)</ref>, and Set2GraphVAE <ref type="bibr" target="#b40">(Vignac &amp; Frossard, 2021)</ref>. For G-Schnet and EDM, the bonds are directly derived from the distance between atoms. We report validity (as measured by RDKit) and uniqueness of the generated compounds. Following <ref type="bibr" target="#b40">(Vignac &amp; Frossard, 2021)</ref> novelty is not included here. For a discussion on the issues with the novelty metric, see Appendix C. As can be seen in Table <ref type="table" target="#tab_1">2</ref>, the EDM is able to generate a very high rate of valid and unique molecules. This is impressive since the 3D models are at a disadvantage in this metric, as the rules to derive bonds are very strict. Interestingly, even when including hydrogen atoms in the model, the performance of the EDM does not deteriorate much. A possible explanation is that the equivariant diffusion model scales effectively and learn very precise distributions, as evidenced by the low negative log-likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Conditional Molecule Generation</head><p>In this section, we aim to generate molecules targeting some desired properties. This can be of interest towards the process of drug discovery where we need to obtain molecules that satisfy specific properties. We train our conditional diffusion model from Section 3.4 in QM9 conditioning the generation on properties ?, gap, homo, lumo, ? and C v described in more detail in Appendix E. In order to assess the quality of the generated molecules w.r.t. to their con- Baselines: We provide two baselines in which molecules are to some extent agnostic to their respective property c. In the first baseline we simply remove any relation between molecule and property by shuffling the property labels in D b and then evaluating ? c on it. We name this setting "Naive (Upper-Bound)". The second baseline named "#Atoms" predicts the molecular properties in D b by only using the number of atoms in the molecule. If "EDM" overcomes "Naive (Upper-Bound)" it should be able to incorporate conditional property information into the generated molecules.</p><p>If it overcomes "#Atoms" it should be able to incorporate it into the molecular structure beyond the number of atoms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results (quantitative):</head><p>Results are reported in Table <ref type="table">3</ref>. EDM outperforms both "Naive (U-bound)" and "#Atoms" baselines in all properties (except ?) indicating that it is able to incorporate property information into the generated molecules beyond the number of atoms for most properties. However, we can see there is still room for improvement by looking at the gap between "EDM" and "QM9 (L-bound)".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results (qualitative):</head><p>In Figure <ref type="figure" target="#fig_1">4</ref>, we interpolate the conditional generation among different Polarizability values ? while keeping the noise fixed. The Polarizability is the tendency of a molecule to acquire an electric dipole moment when subject to an external electric field. We can expect less isometrically shaped molecules for large ? values. This is the obtained behavior in Figure <ref type="figure" target="#fig_1">4</ref> -we show that this behavior is consistent across different runs in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">GEOM-Drugs</head><p>While QM9 features only small molecules, GEOM (Axelrod &amp; Gomez-Bombarelli, 2020) is a larger scale dataset of molecular conformers. It features 430,000 molecules Since molecules in this dataset are bigger and have more complex structures, predicting the bond types using the atom types and the distance between atoms with lookup tables results in more errors than on QM9. For this reason, we only report the atom stability, which measures 86.5% stable atoms on the dataset. Intuitively, this metric describes the percentage of atoms that have bonds in typical ranges -ideally, generative models should generate a comparable number of stable atoms. We also measure the energy of generated compounds with the software used to generate the conformations of the GEOM dataset <ref type="bibr" target="#b4">(Bannwarth et al., 2019)</ref>. After computing the energies of sampled molecules and the dataset, we measure the Wasserstein distance between their histograms. In Table <ref type="table" target="#tab_2">4</ref> we can see that the EDM outperforms its non-equivariant counterparts on all metrics.</p><p>In particular, EDM is able to capture the energy distribution well, as can be seen on the histograms in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We presented EDM, an E(3) equivariant diffusion model for molecule generation in 3D. While previous nonautoregressive models mostly focused on very small molecules with up to 9 atoms, our model scales better and can generate valid conformations while explicitly modeling hydrogen atoms. We also evaluate our model on the larger GEOM-DRUGS dataset, setting the stage for models for drug-size molecule generation in 3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The zero center of gravity, normal distribution</head><p>Consider the Euclidean variable x ? R M ?n in the linear subspace i x i = 0. In other words, x is a point cloud where its center of gravity is zero. One can place a normal distribution N x over this subspace and its likelihood can be expressed as:</p><formula xml:id="formula_40">N x (x|?, ? 2 I) = ( ? 2??) -(M -1)?n exp - 1 2? 2 ||x -?|| 2</formula><p>Here ? also lies in the same subspace as x. Also note a slight abuse of notation: x, ? are technically two-dimensional matrices but are treated in the distribution as single-dimensional (flattened) vectors. To sample from this distribution, there are multiple options. For instance, one could sample from a normal distribution with dimensionality (M -1) ? n and then map the sample to the M ? n dimensional ambient space so that its center of gravity equals zero. However there is an easier alternative: One can sample in the M ? n dimensional ambient space directly, and subtract i x i . Because the normal distributions are isotropic (meaning its variance in any direction you pick is ? 2 ) this is equivalent to the aforementioned method. More detailed analysis are given in <ref type="bibr">(Satorras et al., 2021a)</ref> and <ref type="bibr" target="#b42">(Xu et al., 2022)</ref>.</p><p>KL Divergence A standard KL divergence for between two isotropic normal distributions q = N (? 1 , ? 2 1 I) and p = N (? 2 , ? 2 2 I) is given by:</p><formula xml:id="formula_41">KL(q||p) = d ? log ? 2 ? 1 + 1 2 d ? ? 2 1 + ||? 1 -? 2 || 2 ? 2 2 -d , (<label>20</label></formula><formula xml:id="formula_42">)</formula><p>where d is the dimensionality of the distribution. Recall that in our case the diffusion and denoising process have the same variance ? 2 Q,s,t . If ? 1 = ? 2 = ?, then the KL divergence simplifies to:</p><formula xml:id="formula_43">KL(q||p) = 1 2 ||? 1 -? 2 || 2 ? 2 . (<label>21</label></formula><formula xml:id="formula_44">)</formula><p>Suppose now that N 1 ( ?1 , ?I) and N 2 ( ?2 , ?I) are defined on a linear subspace, where the mean ? is defined with respect to any coordinate system in the subspace. The KL divergence between these distributions then includes a term containing the Euclidean distance || ?1 -?2 || 2</p><p>Similar to the arguments in <ref type="bibr">(Satorras et al., 2021a;</ref><ref type="bibr" target="#b42">Xu et al., 2022)</ref>, an orthogonal transformation Q can be constructed that maps an ambient space where i ? i = 0 to the subspace in such a way that</p><formula xml:id="formula_45">? 0 = Q?. Observe that || ?|| = || ? 0 || = ||?||, and therefore || ?1 -?2 || 2 = ||? 1 -? 2 || 2</formula><p>. This shows that Equation 21 can be consistently computed in the ambient space. This also shows an important caveat: in some diffusion models, different variances are used in the posterior of the diffusion process and the denoising process. In those cases one can see from Equation 20 that the divergence depends on the dimensionality of the subspace, not to be confused with the dimensionality of the ambient space.</p><p>The combined KL divergence for positions and features In the previous section we have shown that the KL divergence for distributions such as N x , can still be computed in the ambient space as long as standard deviations between two such distributions are the same. Let us know consider the combined KL divergence for distributions q = N xh (? 1 , ? 2 I) and p = N xh (? 2 , ? 2 I). Note that here the means consist of two parts ? = [? (x) , ? (h) ] where the x part lies in a subspace and the h part is defined freely. The distributions factorize as N xh (?, h) , ? 2 I). Then the KL divergence simplifies as:</p><formula xml:id="formula_46">? 2 I) = N x (? (x) , ? 2 I) ? N (? (</formula><formula xml:id="formula_47">KL(q||p) = KL N x (? (x) 1 , ? 2 I)||N x (? (x) 2 , ? 2 I) + KL N (? (h) 1 , ? 2 I)||N (? (h) 2 , ? 2 I) = 1 2 ||? (x) 1 -? (x) 2 || 2 ? 2 + 1 2 ||? (h) 1 -? (h) 2 || 2 ? 2 = 1 2 ||? 1 -? 2 || 2 ? 2 . (<label>22</label></formula><formula xml:id="formula_48">)</formula><p>Here we have used that products of independent distributions sum in their independent KL terms, and that the sum of the Euclidean distance of two vectors squared is equal to the squared Euclidean distance of the two vectors concatenated. In summary, even though parts of our distribution are defined on a linear subspace, all computation for the KL divergences is still consistent and does not require special treatment. This is however only valid under the condition that the variances of the denoising process and posterior noising process are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Details for the Method</head><p>Noise schedule: A diffusion process requires a definition for ? t , ? t for t = 0, . . . , T . Since ? t = 1 -? 2 t , it suffices to define ? t . The values should monotonically decrease, starting ? 0 ? 1 and ending at ? T ? 0. In this paper we let</p><formula xml:id="formula_49">? t = (1 -2s) ? f (t) + s where f (t) = (1 -(t/T ) 2 ),</formula><p>for a precision value 10 -5 that avoids numerically unstable situations. This schedule is very similar to the cosine noise schedule introduced in <ref type="bibr" target="#b27">(Nichol &amp; Dhariwal, 2021)</ref>, but ours is somewhat simpler in notation. To avoid numerical instabilities during sampling, we follow the clipping procedure of <ref type="bibr" target="#b27">(Nichol &amp; Dhariwal, 2021)</ref> and compute ? t|t-1 = ? t /? t-1 , where we define ? -1 = 1. The values ? 2 t|t-1 are then clipped from below by 0.001. This avoids numerical instability as 1/? t|t-1 is now bounded during sampling. Then the ? t values can be recomputed using the cumulative product</p><formula xml:id="formula_50">? t = t ? =0 ? ? |? -1 . Recall that SNR(t) = ? 2 t /? 2 t .</formula><p>As in <ref type="bibr" target="#b18">(Kingma et al., 2021)</ref>, we compute the negative log SNR curve defined as ?(t) = -(log ? 2 t -log ? 2 t ) for ? 2 t = 1 -? 2 t . ?(t) is a monotonically increasing function from which all required components can be computed with high numerical precision. For instance, ? 2 t = sigmoid(-?(t)), ? 2 t = sigmoid(?(t)), and SNR(t) = exp(-?(t)).</p><p>Log-likelihood estimator: As discussed, the simplified objective described in Algorithm 1 is optimized during training. However, when evaluating the log-likelihood of samples, the true weighting w(t) = 1 -SNR(t -1)/SNR(t) needs to be used. For this purpose, we follow the procedure described in Algorithm 3. An important detail is that we choose to put an estimator over L t for t = 1, . . . , T using E t?U (1,...,T ) [T ? L t ] = T t=1 L t , but we require an additional forward pass for L 0 .</p><p>In initial experiments, we found the contribution of L 0 very large compared to other loss terms, which would result in very high variance of the estimator. For that reason, the L 0 is always computed at the expense of an additional forward pass. The resulting L is an unbiased estimator for the log-likelihood.</p><p>Algorithm 3 Log-likelihood estimator for EDMs Input: Data point x, neural network ? Sample t ? U(1, . . . , T ), t ? N (0, I), subtract center of gravity from</p><formula xml:id="formula_51">(x) t in t = [ (x) t , (h) t ] z t = ? t [x, h] + ? t t L t = 1</formula><p>2 (1 -SNR(t -1)/SNR(t))|| t -?(z t , t)|| 2 Sample 0 ? N (0, I), subtract center of gravity from</p><formula xml:id="formula_52">(x) 0 in 0 = [ (x) 0 , (h) 0 ] z 0 = ? 0 [x, h] + ? 0 0 L 0 = L (x) 0 + L (h) 0 = -1 2 || -?(z 0 , 0)|| 2 -log Z + log p(h|z (h) 0 ) L base = -KL(q(z T |x, h)|p(z T )) = -KL(N xh (? T [x, h], ? 2 T I)|N xh (0, I)) Return L = T ? L t + L 0 + L base</formula><p>The Dynamics In Section 3.2 we explained that the dynamics of our proposed Equivariant Diffusion Model (EDM) are learned by the EGNN introduced in Section 2.2. The EGNN consists of a sequence of Equivariant Graph Convolutional Layers (EGCL). The EGCL is defined in Eq. 12. All its learnable components ? e , ? h , ? x , ? inf by Multilayer Perceptrons:</p><p>Edge operation ? e . Takes as input two node embeddings. The squared distance d 2 ij = x l i -x l j 2 2 , and the squared distance at the first layer as the optional attribute</p><formula xml:id="formula_53">a ij = x 0 i -x 0 j 2 2 and outputs m ij ? R nf . concat[h l i , h l j , d 2 ij , a ij ] - ? {Linear(nf ? 2 + 2, nf) - ? Silu - ? Linear(nf, nf) - ? Silu} - ? m ij</formula><p>Edge inference operation ? inf . Takes as input the message m ij and outputs a scalar value ?ij ? (0, 1).</p><formula xml:id="formula_54">m ij - ? {Linear(nf, 1) - ? Sigmoid} - ? ?ij</formula><p>Node update ? h Takes as input a node embedding and the aggregated messages and outputs the updated node embedding.</p><formula xml:id="formula_55">concat[h l i , m ij ] - ? {Linear(nf ? 2, nf) - ? Silu - ? Linear(nf, nf) - ? add(?, h l i )} - ? h l+1 i</formula><p>Coordinate update ? x . Has the same inputs as ? e and outptus a scalar value.</p><formula xml:id="formula_56">concat[h l i , h l j , d 2 ij , a ij ] - ? {Linear(nf ? 2 + 2, nf) - ? Silu - ? Linear(nf, nf) - ? Silu - ? Linear(nf, 1)} - ? Output</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Details on Experiments</head><p>Baseline model While our EDM model is parametrized by an E(3) equivariant EGNN network, the GDM model used for the ablation study uses a non equivariant graph network. In this network, the coordinates are simply concatenated with the other node features: h0</p><formula xml:id="formula_57">i = [x i , h].</formula><p>A message passing neural network <ref type="bibr" target="#b13">(Gilmer et al., 2017)</ref> is then applied, that can be written:</p><formula xml:id="formula_58">hl+1 i = ? h ( hl i , j =i ?ij m ij ) for m ij = ? e hl i , hl j , a ij</formula><p>The MLPs ? e , ? h are parametrized in the same way as in EGNN, with the sole exception that the input dimension of ? e in the first layer is changed to accommodate the atom coordinates.</p><p>QM9 On QM9, the EDM and GDMs are trained using EGNNs with 256 hidden features and 9 layers. The models are trained for 1100 epochs, which is around 1.7 million iterations with a batch size of 64. The models are saved every 20 epochs when the validation loss is lower than the previously obtained number. The diffusion process uses T = 1000.</p><p>Training takes approximately 7 days on a single NVIDIA GeForce GTX 1080Ti GPU. When generating samples the model takes on average 1.7 seconds per sample on the 1080Ti GPU. The EDM that only models heavy atoms and no hydrogens has the same architecture but is faster to train because it operates over less nodes: it takes about 3.2 days on a single 1080Ti GPU for 1100 epochs and converges even earlier to its final performance.</p><p>GEOM-DRUGS On GEOM, the EDM and GDMs are trained using EGNNs with 256 hidden features and 4 layers. The models are trained for 13 epochs, which is around 1.2 million iterations with a batch size of 64. Training takes approximately 5.5 days on three NVIDIA RTX A6000 GPUs. The model then takes on average 10.3 seconds to generate a sample. Limitations of RDKit-based metrics Most commonly, unconstrained molecule generation methods are evaluated using RDKit. First, a molecule is built that contains only heavy atoms. Then, RDKit processes this molecule. In particular, it adds hydrogens to each heavy atoms in such a way that the valency of each atom matches its atom type. As a result, invalid molecules mostly appear when an atom has a valency bigger than expected.</p><p>Experimentally, we observed that validity could artificially be increased by reducing the number of bonds. For example, predicting only single bonds was enough to obtain close to 100% of valid molecules on GEOM-DRUGS. Such a change also increases the novelty of the generated molecules, since these molecules typically contain more hydrogens than the training set. On the contrary, our stability metrics directly model hydrogens and cannot be tricked as easily.</p><p>Regarding novelty, <ref type="bibr" target="#b40">Vignac &amp; Frossard (2021)</ref> argued that QM9 is the exhaustive enumeration of molecules that satisfy a predefined set of constraints. As a result, a molecule that is novel does not satisfy at least one of these constraints, which means that the algorithm failed to capture some properties of the dataset. Experimentally, we observed that novelty decreases during training, which is in accordance with this observation. The final novelty metrics we obtain are the following: Bond distances In order to check the validity and stability of the generated structures, we compute the distance between all pairs of atoms and use these distances to predict the existence of bonds and their order. Bond distances in Table <ref type="table" target="#tab_4">6</ref>, 7 and 8 are based on distances in chemistry 23 . In addition, margins are defined for single, double, triple bonds m 1 , m 2 , m 3 = 10, 5, 3 which were found empirically to describe the QM9 dataset well. If an two atoms have a distance shorter than the typical bond length plus the margin for the respective bond type, the atoms are considered to have a bond between them. The allowed number of bonds per atom are: H: 1, C: 4, N: 3, O: 2, F: 1, B: 3, Al: 3, Si: 4, P: [3, 5], S: 4, Cl: 1, As: 3, Br: 1, I: 1. After all bonds have been created, we say that an atom is stable if its valency is precisely equal to the allowed number of bonds. An entire molecule is considered stable if all its atoms are stable. Although this metric does not take into account more atypical distances or aromatic bonds, it is still an extremely important metric as it measures whether the model is positioning the atoms precisely enough. On the QM9 dataset it still considers 95.2% molecules stable and 99.0% of atoms stable. For Geom-Drugs the molecules are much larger which introduces more atypical behaviour.</p><p>Here the atom stability, which is 86.5%, can still be used since it describes how many atoms satisfy the typical bond length description. However, the molecule stability is 2.8% on the dataset, which is too low to draw meaningful conclusions.   Ablation on scaling features In Table <ref type="table" target="#tab_7">9</ref> a comparison between the standard and proposed scaling is shown. Interestingly, there is quite a large difficulty in performance when measuring atom and molecule stability. From these results, it seems that it is easier to learn a denoising process where the atom type is decided later, when the atom coordinates are already relatively well defined.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Selection of samples generated by the denoising process of our EDM trained on QM9 (up) and GEOM-DRUGS (down).</figDesc><graphic url="image-7.png" coords="7,55.44,127.63,485.97,60.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Generated molecules by our Conditional EDM when interpolating among different Polarizability ? values with the same reparametrization noise . Each ? value is provided on top of each image. ditioned property, we use the property classifier network ? c from Satorras et al. (2021b). We split the QM9 training partition into two halves D a , D b of 50K samples each. The classifier ? c is trained on the first half D a , while the Conditional EDM is trained on the second half D b . Then, ? c is evaluated on the EDM conditionally generated samples.We also report the loss of ? c on D b as a lower bound named "QM9 (L-bound)". The better EDM approximates D b the smaller the gap between "EDM" and "QM9 (L-bound)". Further implementation details are reported in Appendix E.</figDesc><graphic url="image-8.png" coords="8,55.44,85.83,485.98,60.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Distribution of estimated energies for the molecules generated by all methods trained on GEOM-DRUGS. We observe that EDM captures the dataset distribution well, while other methods tend to produce too many low-energy compounds.</figDesc><graphic url="image-10.png" coords="14,55.44,346.33,486.00,162.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8</head><label>8</label><figDesc>Figure 8 depicts the generation of molecules from a model trained on GEOM-Drugs. The model starts at random normal noise at time t = T = 1000 and iteratively sample z t-1 ? p(z t-1 |z t ) towards t = 0 to obtain x, h, which is the resulting sample from the model. The atom type part of z (h) t is visualized by taking the argmax of this component.</figDesc><graphic url="image-14.png" coords="17,159.91,119.17,58.04,58.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Selection of sampling chains at different steps from a model trained on GEOM-Drugs. The final column shows the resulting sample from the model.</figDesc><graphic url="image-19.png" coords="17,87.36,252.67,58.04,58.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Molecules generated by our Conditional EDM when interpolating among different ? polarizability values (from left to right). ?'s are reported on top of the image. All samples within each row have been generated with the same reparametrization noise .</figDesc><graphic url="image-31.png" coords="19,55.44,101.20,485.99,563.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-11.png" coords="16,55.44,134.42,485.97,176.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-12.png" coords="16,55.44,405.30,485.95,172.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Neg. log-likelihood -log p(x, h, M ), atom stability and molecule stability with standard deviations across 3 runs on QM9, each drawing 10000 samples from the model.</figDesc><table><row><cell># Metrics</cell><cell>NLL</cell><cell cols="2">Atom stable (%) Mol stable (%)</cell></row><row><cell>E-NF</cell><cell>-59.7</cell><cell>85.0</cell><cell>4.9</cell></row><row><cell>G-Schnet</cell><cell>N.A</cell><cell>95.7</cell><cell>68.1</cell></row><row><cell>GDM</cell><cell>-94.7</cell><cell>97.0</cell><cell>63.2</cell></row><row><cell>GDM-aug</cell><cell>-92.5</cell><cell>97.6</cell><cell>71.6</cell></row><row><cell cols="2">EDM (ours) -110.7?1.5</cell><cell>98.7?0.1</cell><cell>82.0?0.4</cell></row><row><cell>Data</cell><cell></cell><cell>99.0</cell><cell>95.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Validity and uniqueness over 10000 molecules with standard deviation across 3 runs. Results marked (*) are not directly comparable, as they do not use 3D coordinates to derive bonds.</figDesc><table><row><cell cols="3">H: model hydrogens explicitly</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="5">H Valid (%) Valid and Unique (%)</cell></row><row><cell>Graph VAE (*)</cell><cell></cell><cell>55.7</cell><cell></cell><cell></cell><cell>42.3</cell></row><row><cell>GTVAE (*)</cell><cell></cell><cell>74.6</cell><cell></cell><cell></cell><cell>16.8</cell></row><row><cell cols="2">Set2GraphVAE (*)</cell><cell cols="2">59.9?1.7</cell><cell></cell><cell>56.2?1.4</cell></row><row><cell>EDM (ours)</cell><cell></cell><cell cols="2">97.5?0.2</cell><cell></cell><cell>94.3?0.2</cell></row><row><cell>E-NF</cell><cell></cell><cell>40.2</cell><cell></cell><cell></cell><cell>39.4</cell></row><row><cell>G-Schnet</cell><cell></cell><cell>85.5</cell><cell></cell><cell></cell><cell>80.3</cell></row><row><cell>GDM-aug</cell><cell></cell><cell>90.4</cell><cell></cell><cell></cell><cell>89.5</cell></row><row><cell>EDM (ours)</cell><cell></cell><cell cols="2">91.9?0.5</cell><cell></cell><cell>90.7?0.6</cell></row><row><cell>Data</cell><cell></cell><cell>97.7</cell><cell></cell><cell></cell><cell>97.7</cell></row><row><cell cols="6">Table 3. Mean Absolute Error for molecular property prediction by</cell></row><row><cell cols="6">a EGNN classifier ?c on a QM9 subset, EDM generated samples</cell></row><row><cell cols="6">and two different baselines "Naive (U-bounds)" and "# Atoms".</cell></row><row><cell>Task</cell><cell cols="4">? ?? ?HOMO ?LUMO</cell><cell>?</cell><cell>Cv</cell></row><row><cell>Units</cell><cell cols="2">Bohr 3 meV</cell><cell>meV</cell><cell>meV</cell><cell>D cal mol K</cell></row><row><cell cols="3">Naive (U-bound) 9.01 1470</cell><cell>645</cell><cell cols="2">1457 1.616 6.857</cell></row><row><cell>#Atoms</cell><cell cols="2">3.86 866</cell><cell>426</cell><cell cols="2">813 1.053 1.971</cell></row><row><cell>EDM</cell><cell cols="2">2.76 655</cell><cell>356</cell><cell cols="2">584 1.111 1.101</cell></row><row><cell>QM9 (L-bound)</cell><cell>0.10</cell><cell>64</cell><cell>39</cell><cell cols="2">36 0.043 0.040</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Neg. log-likelihood, atom stability and Wasserstein distance between generated and training set energy distributions.with up to 181 atoms and 44.4 atoms on average. For each molecule, many conformers are given along with their energy. From this dataset we retain the 30 lowest energy conformations for each molecule. The models learn to generate the 3D positions and atom types of these molecules. All models use 4 layers, 256 features per layer, and are trained using Adam with batch size 64 and learning rate 10 -4 .</figDesc><table><row><cell># Metrics</cell><cell cols="2">NLL Atom stability (%)</cell><cell>W</cell></row><row><cell>GDM</cell><cell>-14.2</cell><cell cols="2">75.0 3.32</cell></row><row><cell>GDM-aug</cell><cell>-58.3</cell><cell cols="2">77.7 4.26</cell></row><row><cell>EDM</cell><cell>-137.1</cell><cell cols="2">81.3 1.41</cell></row><row><cell>Data</cell><cell></cell><cell>86.5</cell><cell>0.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Novelty among valid and unique molecules (starting from 10000 molecules) with standard deviations across 3 runs on QM9. Experimentally, we observed that novelty is initially close to 100%, and decreases during training. On QM9, it reflects the fact that the algorithm progressively learns to capture the data distribution, which is an exhaustive enumeration under a predefined set of constraints.</figDesc><table><row><cell>Method</cell><cell cols="3">GDM-aug EDM (with H) EDM (no H)</cell></row><row><cell>Novelty (%)</cell><cell>74.6</cell><cell>65.7 ?1.3</cell><cell>34.5?0.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Typical bond distances for a single bond.</figDesc><table><row><cell></cell><cell>H</cell><cell>C</cell><cell>O</cell><cell>N</cell><cell>P</cell><cell>S</cell><cell>F</cell><cell>Si</cell><cell>Cl</cell><cell>Br</cell><cell>I</cell><cell>B</cell><cell>As</cell></row><row><cell>H</cell><cell cols="2">74 109</cell><cell cols="4">96 101 144 134</cell><cell cols="7">92 148 127 141 161 119 152</cell></row><row><cell>C</cell><cell cols="11">109 154 143 147 184 182 135 185 177 194 214</cell><cell>-</cell><cell>-</cell></row><row><cell>O</cell><cell cols="11">96 143 148 140 163 151 142 163 164 172 194</cell><cell>-</cell><cell>-</cell></row><row><cell>N</cell><cell cols="7">101 147 140 145 177 168 136</cell><cell cols="4">-175 214 222</cell><cell>-</cell><cell>-</cell></row><row><cell>P</cell><cell cols="7">144 184 163 177 221 210 156</cell><cell cols="3">-203 222</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>S</cell><cell cols="11">134 182 151 168 210 204 158 200 207 225 234</cell><cell>-</cell><cell>-</cell></row><row><cell>F</cell><cell cols="11">92 135 142 136 156 158 142 160 166 178 187</cell><cell>-</cell><cell>-</cell></row><row><cell>Si</cell><cell cols="3">148 185 163</cell><cell>-</cell><cell cols="7">-200 160 233 202 215 243</cell><cell>-</cell><cell>-</cell></row><row><cell cols="11">Cl 127 177 164 175 203 207 166 202 199 214</cell><cell cols="2">-175</cell><cell>-</cell></row><row><cell cols="11">Br 141 194 172 214 222 225 178 215 214 228</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>I</cell><cell cols="4">161 214 194 222</cell><cell cols="4">-234 187 243</cell><cell>-</cell><cell cols="2">-266</cell><cell>-</cell><cell>-</cell></row><row><cell>B</cell><cell>119</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">-175</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">As 152</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Typical bond distances for a double bond.</figDesc><table><row><cell></cell><cell>C</cell><cell>O</cell><cell>N</cell><cell>P</cell><cell>S</cell></row><row><cell cols="4">C 134 120 129</cell><cell cols="2">-160</cell></row><row><cell cols="5">O 120 121 121 150</cell><cell>-</cell></row><row><cell cols="4">N 129 121 125</cell><cell>-</cell><cell>-</cell></row><row><cell>P</cell><cell cols="2">-150</cell><cell>-</cell><cell cols="2">-186</cell></row><row><cell>S</cell><cell>-</cell><cell>-</cell><cell cols="2">-186</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 .</head><label>8</label><figDesc>Typical bond distances for a triple bond.</figDesc><table><row><cell>C</cell><cell>O</cell><cell>N</cell></row><row><cell cols="3">C 120 113 116</cell></row><row><cell>O 113</cell><cell>-</cell><cell>-</cell></row><row><cell>N 116</cell><cell cols="2">-110</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 .</head><label>9</label><figDesc>Ablation study on the scaling of features of the EDM. Comparing our proposed scaling to no scaling.</figDesc><table><row><cell># Metrics</cell><cell>Scaling</cell><cell>NLL</cell><cell cols="2">Atom stable (%) Mol stable (%)</cell></row><row><cell cols="3">EDM (ours) [x, 1.00 h onehot , 1.0 h atom charge ] -103.4</cell><cell>95.7</cell><cell>46.9</cell></row><row><cell cols="3">EDM (ours) [x, 0.25 h onehot , 0.1 h atom charge ] -110.7?1.5</cell><cell>98.7?0.1</cell><cell>82.0?0.4</cell></row><row><cell>Data</cell><cell></cell><cell></cell><cell>99.0</cell><cell>95.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://www.wiredchemist.com/chemistry/data/bond_energies_lengths.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>http://chemistry-reference.com/tables/Bond%20Lengths%20and%20Enthalpies.pdf</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Equivariant Processes To be self-contained, a version of the proof from <ref type="bibr" target="#b42">(Xu et al., 2022)</ref> is given here. It shows that if the transition distributions p(z t-1 |z t ) are equivariant and p(z T ) is invariant, then every marginal distribution p(z t ) is invariant which importantly includes p(z 0 ). Here induction is used to derive the result.</p><p>Base case: Observe that p(z T ) = N (0, I) is equivariant with respect to rotations and reflections, so p(z T ) = p(Rz T ).</p><p>Induction step: For some t ? {1, . . . , T } assume p(z t ) to be invariant meaning that p(z t ) = p(Rz t ) for all orthogonal R. Let p(z t-1 |z t ) be equivariant meaning that p(z t-1 |z t ) = p(Rz t-1 |Rz t ) for orthogonal R. Then:</p><p>and thus p(z t-1 ) is invariant. By induction, p(z T -1 ), . . . , p(z 0 ) are all invariant. Compared to <ref type="bibr" target="#b42">(Xu et al., 2022)</ref>, this proof makes explicit the dependency on a change of variables to rotate the reference frame of integration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Samples from our models</head><p>Additional samples from the model trained on QM9 are depicted in Figure <ref type="figure">6</ref> and, and samples from the model trained on GEOM-DRUGS in Figure <ref type="figure">7</ref>. These samples are not curated or cherry picked in any way. As a result, their structure may sometimes be difficult to see due to an unfortunate viewing angle. The samples from the model trained on the drugs partition of GEOM show impressive large 3D structures. Interestingly, the model is sometimes generating disconnected component, which only happens QM9 models in early training stages. This may indicate that further training and increasing expressitivity of the models may further help the model bring these components together.</p><p>Figure <ref type="figure">7</ref>. Random samples taken from the EDM trained on geom drugs. While most samples are very realistic, we observe two main failure cases: some molecules that are disconnected, and some that contain long rings. We note that the model does not feature any regularization to prevent these phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Conditional generation</head><p>Conditional Method The specific definition for the loss components L c,t is given in Equation <ref type="formula">23</ref>. Essentially, a conditioning on a property c is added where relevant. The diffusion process that adds noise is not altered. The generative denoising process is conditioned on c by adding it as input to the neural network ?:</p><p>c,0 = log p(h|z</p><p>Given a trained conditional model p(x, h|c, M ), we define the generative process by first sampling c, M ? p(c, M ) and then x, h ? p(x, h|c, M ). We compute c, M ? p(c, M ) on the training partition as a parametrized two dimensional categorical distribution where we discretize the continuous variable c into small uniformly distributed intervals.</p><p>Implementation details: In this conditional experiment, our Equivariant Diffusion Model uses an EGNN with 9 layers, 192 features per hidden layer and SiLU activation functions. We used the Adam optimizer with learning rate 10 -4 and batch size 64. Only atom types (categorical) and positions (continuous) have been modelled but not atom charges. All methods have been trained for ? 2000 epochs while doing early stopping by evaluating the Negative Log Likelihood on the validation partition proposed by <ref type="bibr" target="#b1">(Anderson et al., 2019)</ref>.</p><p>Additionaly, the obtained molecule stabilities in the conditional generative case was similar to the the ones obtained in the non-conditional setting. The reported molecule stabilities for each conditioned property evaluated on 10K generated samples are: (80.4%) ?, (81.73%) ??, (82.81%) ? HOMO , (83.6 %) ? LUMO , (83.3%) ?, (81.03 %) C v .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QM9 Properties</head><p>? Polarizability: Tendency of a molecule to acquire an electric dipole moment when subjected to anexternal electric field.</p><p>? HOMO : Highest occupied molecular orbital energy.</p><p>? LUMO : Lowest unoccupied molecular orbital energy.</p><p>?? Gap: The energy difference between HOMO and LUMO.</p><p>?: Dipole moment.</p><p>C v : Heat capacity at 298.15K</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional generation results</head><p>In this Section we sweep over 9 different ? values in the range <ref type="bibr">[73.6, 101.6</ref>] while keeping the reparametrization noise fixed and the number of nodes M = 19. We plot 10 randomly selected sweeps in Figure <ref type="figure">9</ref> with different reparametrization noises each. Samples have been generated using our Conditional EDM. We can see that for larger Polarizability values, the atoms are distributed less isotropically encouraing larger dipole moments when an electric field is applied. This behavior is consistent among all reported runs.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alphafold at casp13</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="4862" to="4865" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cormorant: Covariant molecular neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/03573" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>b32b2746e6e8ca98b9123f2249b-Paper. pdf</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structured denoising diffusion models in discrete state-spaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Geom: Energyannotated molecular conformations for property prediction and molecular generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gomez-Bombarelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05531</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gfn2-xtb-an accurate and broadly parametrized self-consistent tightbinding quantum chemical method with multipole electrostatics and density-dependent dispersion contributions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bannwarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ehlert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grimme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical theory and computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1652" to="1671" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A two-step graph convolutional for molecule generation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03412</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Molgan: An implicit generative model for small molecular graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Theoretical Foundations and Applications of Deep Generative Models</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML</title>
		<meeting>the 37th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="3165" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">)transformers: 3d roto-translation equivariant attention networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Se</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pattanaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><surname>Geomol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07802</idno>
		<title level="m">Torsional geometric generation of molecular 3d conformer ensembles</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Gebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gastegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11347</idno>
		<title level="m">Generating equilibrium molecules with deep neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Symmetryadapted generation of 3d point sets for the targeted discovery of molecules</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Gebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gastegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00957</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inverse design of 3d molecular structures with conditional generative neural networks</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Gebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gastegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Hessmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04824</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Energy-inspired molecular conformation optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11239</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>No?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03131</idno>
		<title level="m">Generating valid euclidean distance matrices</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Argmax flows and multinomial diffusion: Learning categorical distributions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Forr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.00630</idno>
		<title level="m">Variational diffusion models</title>
		<imprint>
			<date type="published" when="2021">2, 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gro?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Equivariant flows: Exact likelihood generative learning for symmetric densities</title>
		<author>
			<persName><forename type="first">J</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>No?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML</title>
		<meeting>the 37th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="5361" to="5370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Conditional set generation with transformers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gggan: A geometric graph generative adversarial network</title>
		<author>
			<persName><forename type="first">I</forename><surname>Krawczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abranches</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qiAxL3Xqx1o" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient graph generation with graph recurrent attention networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00760</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09076</idno>
		<title level="m">Constrained graph variational autoencoders for molecule design</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Predicting molecular conformation via dynamic graph score matching. Advances in Neural Information Processing Systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A graph vae and graph transformer approach to generating molecular graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Senn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wynne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04345</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09672</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>No?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">365</biblScope>
			<biblScope unit="issue">6457</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">E(n) equivariant normalizing flows. Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">E (n) equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09844</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Serre</surname></persName>
		</author>
		<title level="m">Linear representations of finite groups</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1977">1977</date>
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning gradient fields for molecular conformation generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A generative model for molecular distance geometry</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11459</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Symmetry-aware actor-critic for 3d molecular design</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N C</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pinsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cs?nyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=jEYKjPE1xYN" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graphvae: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="412" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML</title>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno>CoRR, abs/1907.05600</idno>
		<ptr target="http://arxiv.org/abs/1907.05600" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<idno>CoRR, abs/1802.08219</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Top-n: Equivariant set and graph generation without exchangeability</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vignac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02096</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">An end-to-end framework for molecular conformation generation via bilevel programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gomez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07246</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Geodiff: A geometric diffusion model for molecular conformation generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=PzcvxEMzvQC" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Anytime sampling for autoregressive models via ordered autoencoding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02473</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
