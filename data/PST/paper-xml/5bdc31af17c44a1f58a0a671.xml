<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A New Varying-Parameter Recurrent Neural-Network for Online Solution of Time-Varying Sylvester Equation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Zhijun</forename><surname>Zhang</surname></persName>
							<email>drzhangzhijun@gmail.com</email>
							<idno type="ORCID">0000-0002-6859-3426</idno>
						</author>
						<author>
							<persName><forename type="first">Lunan</forename><surname>Zheng</surname></persName>
							<email>aulnzheng@sina.com</email>
							<idno type="ORCID">0000-0002-7671-6051</idno>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Weng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yijun</forename><surname>Mao</surname></persName>
							<email>yijunmao@163.com</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
							<email>luwei3@mail.sysu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><surname>Xiao</surname></persName>
							<idno type="ORCID">0000-0003-3172-3490</idno>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Automation Science and Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Jinan University</orgName>
								<address>
									<postCode>510632</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">College of Mathematics and Informatics</orgName>
								<orgName type="institution">South China Agricultural University</orgName>
								<address>
									<postCode>510640</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Sun Yat-sen University</orgName>
								<address>
									<postCode>510275</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Jishou University</orgName>
								<address>
									<postCode>416000</postCode>
									<settlement>Jishou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A New Varying-Parameter Recurrent Neural-Network for Online Solution of Time-Varying Sylvester Equation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8EA03A85696A10F73E3DFADC930C5F8D</idno>
					<idno type="DOI">10.1109/TCYB.2017.2760883</idno>
					<note type="submission">Manuscript received March 21, 2017; revised August 12, 2017; accepted October 1, 2017.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computer simulations</term>
					<term>convergence and robustness</term>
					<term>recurrent neural networks</term>
					<term>time-varying equation solving</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Solving Sylvester equation is a common algebraic problem in mathematics and control theory. Different from the traditional fixed-parameter recurrent neural networks, such as gradient-based recurrent neural networks or Zhang neural networks, a novel varying-parameter recurrent neural network, [called varying-parameter convergent-differential neural network (VP-CDNN)] is proposed in this paper for obtaining the online solution to the time-varying Sylvester equation. With time passing by, this kind of new varying-parameter neural network can achieve super-exponential performance. Computer simulation comparisons between the fixed-parameter neural networks and the proposed VP-CDNN via using different kinds of activation functions demonstrate that the proposed VP-CDNN has better convergence and robustness properties.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S YLVESTER equation, AX -XB + C = 0, is a crucial kind of linear matrix equation in linear algebra and matrix theory. It plays an important role in analysis and synthesis of computer vision problems or dynamic control systems, e.g., image fusion <ref type="bibr" target="#b0">[1]</ref>, clustering <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, object detection <ref type="bibr" target="#b3">[4]</ref>, controllability and observability of system <ref type="bibr" target="#b4">[5]</ref>, optimization <ref type="bibr" target="#b5">[6]</ref>, and so on. Various methods of solving online linear algebraic equation problems were reported in the past few years, such as Bartels-Stewart algorithm, GNNs, and ZNNs. For solving time-invariant Sylvester equation, Bartels-Stewart algorithm <ref type="bibr" target="#b6">[7]</ref> and its extension <ref type="bibr" target="#b7">[8]</ref> were utilized appropriately when the sampling period was large enough. In recent years, recurrent neural network approaches have attracted more and more researchers. A classic kind of recurrent neural networks is GNNs. To solve different time-invariant problems, such as matrix inversion and Sylvester equation solving, various kinds of traditional GNNs were designed <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. In general, GNN methods for solving the Sylvester equation can be summarized as below.</p><p>1) Design a norm-based error function E = AX -XB + C 2 /2 of the Sylvester equation. The solution to the Sylvester equation will be obtained when the error function converges to zero. 2) For reaching the minimum point of the error function, a negative gradient-based descent direction is used. GNN methods possess a constant scalar-type parameter, which is applied to scale the convergence rate of the neural networks for solving the Sylvester equation. However, for solving the time-varying problem, since the state variables of the equation are time varying, it is difficult for the GNN to track, and the norm-based error function cannot converge to zero in time <ref type="bibr" target="#b11">[12]</ref>. That is to say, the GNN method cannot satisfy the need of solving time-varying case. In order to solve this kind of time-varying algebraic problems, like Sylvester equations, Zhang et al. proposed an implicit dynamics method, called ZNN, by utilizing the time derivative form of error function <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b20">[21]</ref>. However, ZNN methods require that the convergence parameter (corresponding to inductance parameters or the reciprocals of capacitive parameters) should be set as large as possible for obtaining the fastest convergence performance. This condition are sometimes unpractical and difficult to meet when the neural network is performed on hardware systems <ref type="bibr" target="#b21">[22]</ref>. In addition, if matrix coefficients of functions are very large and initial values are generated randomly in a large range, a neural network with much faster convergence performance is necessary. Moreover, in a practical system, the inductance parameters or reciprocals of capacitive parameters are time-varying, especially the large power electronics system, ac motor control system, power grid capacitor switched, it is unreasonable to set parameters of the systems as fixed ones <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Considering that both problems to solve and the practical parameters of the hardware systems are time-varying, a novel varying-parameter recurrent neural network (i.e., VP-CDNN), is proposed for solving time-varying Sylvester equation problems. This kind of neural network has matrix-type time-varying convergence coefficients.</p><p>The remainder of this paper is organized as below. Section II reviews the applications of Sylvester equation and some effective methods for solving the Sylvester equation problem. Section III presents the problem formulation and gives some preliminaries. Section IV proposes the novel neural network. In Section V, the convergence and robustness of the proposed neural network are proved theoretically. Comparison results based on computer simulations are illustrated in Section VI. In Section VII, such VP-CDNN method is applied to resolving the inverse kinematic problem of a three-link planar robot manipulator.</p><p>Before ending this section, the main contributions of this paper are summarized as follows.</p><p>1) A novel VP-CDNN is proposed for solving time-varying Sylvester equation problems for the first time. 2) Super-exponential convergence and strong robustness of VP-CDNN have been proved.</p><p>3) The convergence performance and bounds of the VP-CDNN with different activation functions (i.e., linear-type, sigmoid-type, power-type and powersigmoid-type) are analyzed and proved. 4) Simulation comparisons between the proposed VP-CDNN and the FP-CDNN illustrate that VP-CDNN has better convergence and robustness performance. 5) A specific application to the kinematic control of a three-link planar robot manipulator is presented for illustrating the effectiveness of the VP-CDNN method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>As a generalized linear algebraic equation, Sylvester equation is applied to various fields. In the field of image processing, a fast multiband image fusion method based on Sylvester equation was proposed by Wei et al. <ref type="bibr" target="#b0">[1]</ref>. Recently, Lin et al. <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> proposed an efficient simulation method for simulating the post-exposure bake based on solving a 3-D Sylvester equation with the consideration of standing wave effects and resolution enhancement techniques. Based on the solution to Sylvester equation, Wang et al. <ref type="bibr" target="#b1">[2]</ref> presented a manifold regularized multiview subspace clustering method to better incorporate the correlated and complementary information from different views and Hu et al. <ref type="bibr" target="#b2">[3]</ref> proposed a smooth representation clustering method, which is a new subspace clustering method with stronger grouping property. Sylvester equation is a crucial formula for system controllability and observability in the control field. By solving the generalized Sylvester equation, Shaker and Tahavori <ref type="bibr" target="#b4">[5]</ref> proposed a method for control configuration selection for MIMO bilinear processes. Tondu <ref type="bibr" target="#b26">[27]</ref> proposed a weak generalized inverse approach to a fast and compact computation of redundancy problem based on a right pseudoinverse (another kind of special form about Sylvester equation) of the Jacobian matrix to generate closed-form and compact expressions of the robot manipulator joint velocities.</p><p>A variety of numerical methods have been proposed for solving the Sylvester equation, such as Bartels-Stewart algorithm <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and Hessenberg-Schur method <ref type="bibr" target="#b27">[28]</ref>. Timeinvariant Sylvester equation can be solved with those classical algorithms by transforming the coefficient matrices A and B into triangular or Hessenberg form and utilizing an iteration solving procedure. Some classic iteration methods, e.g., the ADI method <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>, rational Krylov subspace method <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref>, the conjugate gradient-based method <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b36">[37]</ref>, and the HSS method <ref type="bibr" target="#b37">[38]</ref>- <ref type="bibr" target="#b39">[40]</ref>, are often efficient and accurate for solving the Sylvester equation with large, sparse, and time-invariant coefficient matrices A, B, and C. Wolf et al. <ref type="bibr" target="#b28">[29]</ref> proposed a new version of the ADI iteration based on tangential generalization for obtaining the solution to large-scale Lyapunov equations. Kolesnikov and Oseledets <ref type="bibr" target="#b31">[32]</ref> proposed a novel algorithm based on an extended rational Krylov subspace approximation method with adaptively computed shifts to approximate the solution to the well-known Lyapunov equation (rank-1 righthand side). Huang and Ma <ref type="bibr" target="#b34">[35]</ref> proposed a modified conjugate gradient method, which was applied to the generalized coupled Sylvester-conjugate matrix equations. Zak and Toutounian <ref type="bibr" target="#b37">[38]</ref> presented a novel iterative method based on the HSS for solving the continuous Sylvester equation by emphasizing the role of skew-Hermitian part of the coefficient matrices.</p><p>With the emergence of parallel computing, neural network methods attracted many researchers and engineers all over the world <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b44">[45]</ref>. As a kind of classic recurrent neural network for solving the algebra problems, GNN has been widely applied <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. Zhang et al. <ref type="bibr" target="#b9">[10]</ref> investigated the gradient dynamics systems (including a linear gradient dynamics system and a nonlinear gradient dynamics system) with their exponential convergence uniformly. Since the GNN method is not effective and appropriate for solving the time-varying problems, Li et al. <ref type="bibr" target="#b12">[13]</ref>, Shen et al. <ref type="bibr" target="#b13">[14]</ref>, Li and Li <ref type="bibr" target="#b14">[15]</ref>, Jin et al. <ref type="bibr" target="#b15">[16]</ref>, Mao et al. <ref type="bibr" target="#b16">[17]</ref>, Xiao and Liao <ref type="bibr" target="#b17">[18]</ref>, Liao et al. <ref type="bibr" target="#b18">[19]</ref>, Xiao <ref type="bibr" target="#b19">[20]</ref>, and Zhang et al. <ref type="bibr" target="#b20">[21]</ref> proposed a novel recurrent neural network, termed ZNN, for solving the time-varying problem and accelerating the convergence speed. Shen et al. <ref type="bibr" target="#b13">[14]</ref> investigated the finite-time stability of ZNN and its application for the online solution to the timevarying Sylvester equation. The complex-valued Sylvester equation problem was discussed in <ref type="bibr" target="#b14">[15]</ref>. Jin et al. <ref type="bibr" target="#b15">[16]</ref> proposed a modified ZNN method with global exponential convergence property for solving time-varying quadratic programming problem in the presence of measurement noises and Mao et al. <ref type="bibr" target="#b16">[17]</ref> applied this modified ZNN method to time-variant matrix inversion problems. All the existing neural networks for solving time-varying problems are FP-CDNNs. Different from the FP-CDNNs, a VP-CDNN is proposed to improve the convergence and robustness performance when solving the time-varying Sylvester equation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM FORMULATION AND PRELIMINARIES</head><p>Consider the following smooth time-varying Sylvester equation:</p><formula xml:id="formula_0">A(t)X(t) -X(t)B(t) + C(t) = 0, ∀t ∈ [0, +∞)<label>(1)</label></formula><p>where t denotes time, and In order to facilitate solving the aforementioned (1), the Sylvester equation first has to be transformed from matrixtype to vector-type via the following Theorem 1 about transformation.</p><formula xml:id="formula_1">A(t) ∈ R m×m , B(t) ∈ R</formula><p>Theorem 1: The matrix-type Sylvester equation ( <ref type="formula" target="#formula_0">1</ref>) is equivalent to the following vector-type equation:</p><formula xml:id="formula_2">I n ⊗ A(t) -B T (t) ⊗ I m vec(X(t)) + vec(C(t)) = 0 (2)</formula><p>where I n and I m denote identity matrices, and symbol ⊗ denotes the Kronecker product. It means that A ⊗ B is a large matrix made by replacing the (i, j)th element a ij of A with the matrix a ij B. Operator vec(X) ∈ R nm is a constructional column vector, which is composed of putting all column vectors of X ∈ R m×n into one column together.</p><p>Proof: See Section II and <ref type="bibr" target="#b45">[46,</ref><ref type="bibr">Appendix]</ref>.</p><p>In addition, to guarantee obtaining the unique solution, the regularity condition has to be satisfied and the following Theorem 2 is presented <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>.</p><p>Theorem 2: Equation (1) has the unique solution if and only if it satisfies the regularity condition, that is</p><formula xml:id="formula_3">I n ⊗ A T (t)A(t) -B T (t) ⊗ A T (t) -B(t) ⊗ A(t) + B(t)B T (t) ⊗ I m αI nm , ∃ α &gt; 0, ∀ t ∈ [0, +∞). (3)</formula><p>Proof: See Section II and <ref type="bibr" target="#b45">[46,</ref><ref type="bibr">Appendix]</ref>. To obtain the unique solution to Sylvester equation (1), a matrix-type error function is defined as</p><formula xml:id="formula_4">E(t) := A(t)X(t) -X(t)B(t) + C(t). (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>The unique solution X * (t) to the Sylvester equation ( <ref type="formula" target="#formula_0">1</ref>) could be readily obtained if the error function E(t) approaches to zero. According to neural dynamic design methods <ref type="bibr" target="#b45">[46]</ref>, the time derivative of error function E(t) should be negativedefinite, and can be expressed as</p><formula xml:id="formula_6">dE(t) dt = Ė(t) = -F(E(t))</formula><p>where F(•) : R m×n → R m×n denotes the activation-function array. F(•) has various types corresponding to different mapping functions, such as linear-type, sigmoid-type, and powertype activation functions, etc. ∈ R m×m is a positive-definite matrix used to scale the convergence rate of solving process <ref type="bibr" target="#b21">[22]</ref>. According to differential equation theory <ref type="bibr" target="#b47">[48]</ref>, the parameter should be set as large as possible for getting faster convergence property. For simplicity and for the convenience of discussion, is usually set as a diagonal matrix with the same elements. That is to say, matrix can be replaced by γ I with constant scalar-valued parameter γ &gt; 0 for simplicity, that is</p><formula xml:id="formula_7">Ė(t) = -γ F(E(t)). (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>Based on (5), the following implicit dynamic equation will be obtained as:</p><formula xml:id="formula_9">A(t) Ẋ(t) -Ẋ(t)B(t) = -Ȧ(t)X(t) + X(t) Ḃ(t) -Ċ(t) -γ F(A(t)X(t) -X(t)B(t) + C(t))<label>(6)</label></formula><p>where X(t) has a starting value X(0) = X 0 ∈ R m×n . In addition, X(t) is an asymptotic approximate solution, which approaches to the unique theoretical solution X * (t) to <ref type="bibr" target="#b0">(1)</ref>. Since the convergence rate parameter γ is fixed and predefined, ( <ref type="formula" target="#formula_9">6</ref>) is called FP-CDNN. According to the following lemma <ref type="bibr" target="#b45">[46]</ref>, exponential convergence performance can be guaranteed by FP-CDNN. Lemma 1: Given matrix function A(t) ∈ R m×m , B(t) ∈ R n×n , and C(t) ∈ R m×n of Sylvester equation ( <ref type="formula" target="#formula_0">1</ref>), if the regularity condition (3) is satisfied and the linear activation function is used, then the state matrix X(t) of the implicit dynamical system (6) starting from any initial state X 0 converges exponentially to the time-varying theoretical solution X * (t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. VARYING-PARAMETER RECURRENT NEURAL NETWORK</head><p>Inspired by the design thought of classic FP-CDNN and considering the time-varying character of the hardware system, a new type of neural networks with time-varying design parameter is exploited and developed, which is thus called VP-CDNN. Specifically, a power-type time-varying parameter is used here and ( <ref type="formula" target="#formula_7">5</ref>) can be reformulated as</p><formula xml:id="formula_10">Ė(t) = -ϒ(t)F(E(t)) = -t p + p F(E(t)) (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>where the design parameter</p><formula xml:id="formula_12">ϒ(t) = t p + p &gt; 0, t ∈ [0, +∞).</formula><p>Therefore, VP-CDNN can be formulated with the implicit dynamic equation form as</p><formula xml:id="formula_13">A(t) Ẋ(t) -Ẋ(t)B(t) = -Ȧ(t)X(t) + X(t) Ḃ(t) -Ċ(t) -t p + p F(A(t)X(t) -X(t)B(t) + C(t)).<label>(8)</label></formula><p>As discussed in Section III, based on Theorem 1, we can transform the implicit dynamic equation ( <ref type="formula" target="#formula_13">8</ref>) of VP-CDNN into vector-type</p><formula xml:id="formula_14">M(t)ẋ(t) = -t p + p F(M(t)x(t) + vec(C(t))) -Ṁ(t)x(t) -vec( Ċ(t))<label>(9)</label></formula><p>where matrix M(t) := I n ⊗ A(t) -B T (t) ⊗ I m , vector x(t) := vec(X(t)), and activation-function array F(•) : R mn×1 → R mn×1 with a delicate difference on transformation dimensions compared with <ref type="bibr" target="#b4">(5)</ref>.</p><p>According to implicit dynamic equation ( <ref type="formula" target="#formula_13">8</ref>), block diagram realization of VP-CDNN is designed as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Fig. <ref type="figure" target="#fig_0">1</ref> is not only the illustration of ( <ref type="formula" target="#formula_13">8</ref>), but also the realization of the proposed VP-CDNN system. It is worth mentioning that VP-CDNN can be realized by using some electronic components and the block diagram can contribute to the design procedure of neural networks for physical implementation. In Fig. <ref type="figure" target="#fig_0">1</ref>, represents the accumulator and represents the integrator. Left multiply and right multiply mean two kinds of multiply operation of matrices. In <ref type="bibr" target="#b7">(8)</ref> and the block diagram (Fig. <ref type="figure" target="#fig_0">1</ref>), ϒ(t) = t p + p with p &gt; 0 is the varying-parameter function, of which the curves with different p are shown in Fig. <ref type="figure" target="#fig_1">2</ref>. It is worth pointing out that the VP-CDNN would be degenerated into FP-CDNN when t = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONVERGENCE AND ROBUSTNESS</head><p>Following the framework designed in Section IV, further more discussion on convergence and robustness will be illustrated in this section. First, the proof of the convergence of VP-CDNN when solving Sylvester function is presented. Second, the theoretical analysis results of the faster convergence performance of the proposed VP-CDNN with different activation functions are deduced. Third, the robustness of the VP-CDNN is analyzed according to coefficient matrix perturbation, differentiation error and model-implementation error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convergence Analysis</head><p>To lay a basis for the discussion of global convergence, the following theorem is presented.</p><p>Theorem 3: Consider the online solution to time-varying Sylvester equation <ref type="bibr" target="#b0">(1)</ref>. Given time-varying matrices A(t) ∈ R m×m , B(t) ∈ R n×n , and C(t) ∈ R m×n satisfying (1), if the regularly condition (3) holds true and the odd-monotonically increasing activation function F(•) is used, then the state matrix X(t) ∈ R m×n of VP-CDNN system (8), starting from any initial state X 0 ∈ R m×n , can globally converge to the time-varying theoretical solution X * (t) to Sylvester equation <ref type="bibr" target="#b0">(1)</ref>.</p><p>Proof: Through the following two steps, Theorem 3 is proved.</p><p>First, the scalar-type error function is derived. Substitute theoretically optimum value X * (t) into the error function ( <ref type="formula" target="#formula_4">4</ref>), and we can get</p><formula xml:id="formula_15">C(t) = -A(t)X * (t) + X * (t)B(t). (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>Substitute <ref type="bibr" target="#b9">(10)</ref> and its time derivative into (8), and a new form of implicit dynamic equation of VP-CDNN is presented as</p><formula xml:id="formula_17">A(t) ˙ X(t) + Ȧ(t) X(t) -˙ X(t)B(t) -X(t) Ḃ(t) = -t p + p F A(t) X(t) -X(t)B(t) (11)</formula><p>where X(t) := X(t) -X * (t) denotes the residual error between X(t) and X * (t) and the initial value of <ref type="bibr" target="#b10">(11)</ref> can be written as</p><formula xml:id="formula_18">X(t) is X(0) = X 0 - X * (0). Since E(t) = A(t) X(t) -X(t)B(t),</formula><formula xml:id="formula_19">Ė(t) = -t p + p F(E(t)). (<label>12</label></formula><formula xml:id="formula_20">)</formula><p>The scalar-type of ( <ref type="formula" target="#formula_19">12</ref>) is</p><formula xml:id="formula_21">ėij (t) = -(t p + p)f (e ij (t)) ∀i ∈ {1, 2, . . . , m}, j ∈ {1, 2, . . . , n}<label>(13)</label></formula><p>where </p><formula xml:id="formula_22">(t) = e ij (t)ė ij (t) = -(t p + p)e ij (t)f (e ij (t)).</formula><p>For every activation subfunctions f (•) being oddmonotonically increasing and t p + p &gt; 0 when t &gt; 0, -(t p + p)e ij (t)f (e ij (t)) ≤ 0 for e ij (t) = 0 and -(t p + p)e ij (t)f (e ij (t)) = 0 if and only if e ij (t) = 0. It means that we have vij (t) = dv ij (t)/dt ≤ 0, ∀t &gt; 0. On the basis of Lyapunov stability theory <ref type="bibr" target="#b48">[49]</ref>, the global asymptotical convergence of error function ( <ref type="formula" target="#formula_21">13</ref>) can be guaranteed. Therefore, scalar-type error e ij (t) will converge to zero for any i ∈ {1, 2, . . . , m}, j ∈ {1, 2, . . . , n}. This means that when t → +∞, we have X(t) → X * (t).</p><p>To sum up, the proof of the globally convergence Theorem 3 is complete.</p><p>The specific convergence rate with three different activation functions, i.e., linear-type, sigmoid-type, and power-type functions, can refer to the following theorem.</p><p>Theorem 4: Given time-varying matrices A(t) ∈ R m×m , B(t) ∈ R n×n , and C(t) ∈ R m×n of (1), if the regularity condition (3) is satisfied, and a monotonically increasing odd function array is used, then the state matrix of neural system (8) starting from any initial state X 0 converges to the time-varying theoretical solution X * (t). In addition, the specific convergence rate of the neural system ( <ref type="formula" target="#formula_13">8</ref>) is:</p><p>1) super-exponential convergence with the rate t p /(p+1)+ p, if a linear-type activation function is used; 2) super-exponential convergence with the rate ξ(t p /(p + 1) + p)/2 with ξ ≥ 1 for error range e ij (t) ∈ (ln 2/ξ, +∞), if a bipolar-sigmoid-type activation function is used; 3) super-exponential convergence and the convergence performance for error range |e ij (t)| &gt; 1, if a power-type activation function is used. Proof: Based on Theorem 3, we have the following three cases corresponding to three different activation functions.</p><p>1) Linear-Type: For the simple linear case, (13) can be written as</p><formula xml:id="formula_23">ėij (t) = -t p + p e ij (t). (<label>14</label></formula><formula xml:id="formula_24">)</formula><p>According to the differential function theory <ref type="bibr" target="#b47">[48]</ref>, the solution to differential equation ( <ref type="formula" target="#formula_23">14</ref>) can be obtained</p><formula xml:id="formula_25">e ij (t) = exp - t p+1 p + 1 -pt e ij (0). (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>The convergence of state matrix</p><formula xml:id="formula_27">X(t) → X * (t) is equiv- alent the convergence of X(t) F = X(t) -X * (t) F → 0.</formula><p>According to the definition of Frobenius norm <ref type="bibr" target="#b46">[47]</ref> </p><formula xml:id="formula_28">X(t) F = m i=1 n j=1 x 2 ij (t) = x(t) 2<label>(16)</label></formula><p>where x(t) := vec( X(t)). According to the definition of Kronecker product <ref type="bibr" target="#b46">[47]</ref>,</p><formula xml:id="formula_29">E(t) = A(t) X(t) -X(t)B(t) can be</formula><p>rewritten in vector-type as</p><formula xml:id="formula_30">e(t) = M(t) x(t) = M(t) x(t) -x * (t) (17)</formula><p>where e(t) := vec(E(t)) and x * (t) := vec(X * (t)). Based on Theorem 2 and the regularity condition (3) we mentioned in Section III, we know that M(t) is a invertible matrix. Based on the definition of Frobenius norm, we have</p><formula xml:id="formula_31">M -1 (t) F = SVD(M -1 (t)) 2 2 = SVD(M -1 ) 2 ≤ ϕ 1 ,</formula><p>where SVD(•) is a vector containing the singular values of a matrix. So the computation error of the state matrix of the neural system with linear-type activation function is</p><formula xml:id="formula_32">X(t) F ≤ M -1 (t) F e(t) 2 ≤ ϕ 1 m i=1 n j=1 e 2 ij (t).</formula><p>Substitute <ref type="bibr" target="#b14">(15)</ref> into the above inequality, and we can get</p><formula xml:id="formula_33">X(t) F ≤ ϕ 1 m i=1 n j=1 e ij (0) exp - t p+1 p + 1 -pt 2 = κ 1 ϕ 1 exp - t p+1 p + 1 -pt = V L</formula><p>where κ 1 = E(0) F . It means that the solving process of Sylvester equation via using VP-CDNN method with varying-parameter function ϒ(t) = t p + p possesses the global super-exponential convergence when using linear-type activation function.</p><p>2) Bipolar-Sigmoid-Type: For the bipolar-sigmoid-type activation function</p><formula xml:id="formula_34">f (u) = (1 -exp(-ξ u))/(1 + exp(-ξ u))</formula><p>where ξ ≥ 1 and u ∈ R are scalar parameters, (13) can be written as</p><formula xml:id="formula_35">ėij (t) = -t p + p 1 -exp -ξ e ij (t) 1 + exp -ξ e ij (t) . (<label>18</label></formula><formula xml:id="formula_36">)</formula><p>According to differential equation theory, the solution to ( <ref type="formula" target="#formula_35">18</ref>) is</p><formula xml:id="formula_37">e ij (t) = - 1 ξ ln ⎛ ⎜ ⎜ ⎝ 1 + ψ exp -ξ t p+1 p+1 + pt 2 -sgn e ij (0) ⎛ ⎝ 1 + ψ exp -ξ t p+1 p+1 + pt 2 ⎞ ⎠ 2 -1 ⎞ ⎟ ⎟ ⎠ = - 1 ξ ln(1 + k(t)) (<label>19</label></formula><formula xml:id="formula_38">)</formula><p>where</p><formula xml:id="formula_39">k(t) = h(t) -sgn(e ij (0)) (1 + h(t)) 2 -1 with h(t) = ψ exp(-ξ(t p+1 /(p + 1) + pt))/2</formula><p>, and sign function </p><formula xml:id="formula_40">sgn(τ ) = ⎧ ⎨ ⎩ 1, if τ &gt; 0 0, if τ = 0 -1, if τ &lt; 0. Constant ψ = (exp(-ξ e ij (0)) -1)</formula><formula xml:id="formula_41">|e ij (t)| = 1 ξ |k(t) - k 2 (t) 2 + k 3 (t) 3 -• • • | ≤ 1 ξ |k(t)| + |k 2 (t)| + |k 3 (t)| + • • • = |k(t)| ξ (1 -|k(t)|)</formula><p>.</p><p>According to <ref type="bibr" target="#b21">[22]</ref>, there exists</p><formula xml:id="formula_42">k 0 = |k(t)| &lt; 1, ∃t ∈ [0, +∞) such that |e ij (t)| ≤ ψ + ψ 2 + 4ψ 2ξ (1 -k 0 ) exp ⎛ ⎝ - ξ t p+1 p+1 + pt 2 ⎞ ⎠ . (<label>20</label></formula><formula xml:id="formula_43">)</formula><p>It means that when -1 &lt; k(t) &lt; 1, according to <ref type="bibr" target="#b18">(19)</ref>, ln 2/ξ &lt; e ij (t) &lt; +∞. According to inequality (20), we have</p><formula xml:id="formula_44">X(t) F ≤ ϕ 1 m i=1 n j=1 e 2 ij (t) ≤ √ mnϕ 1 max |e ij (t)| ≤ √ mnϕ 1 κ 2 exp ⎛ ⎝ - ξ t p+1 p+1 + pt 2 ⎞ ⎠ = V S (<label>21</label></formula><formula xml:id="formula_45">)</formula><p>where</p><formula xml:id="formula_46">κ 2 = max(|(ψ + ψ 2 + 4ψ)/(2ξ(1 -k 0 ))|).</formula><p>This implies that the proposed VP-CDNN has super-exponential convergence rate for e ij (t) &gt;ln 2/ξ [i.e., |k(t)| &lt; 1] via using bipolar-sigmoid-type activation function.</p><p>3) Power-Type: Take odd-monotonically increasing power function f (u) = u μ as the activation function, where u ∈ R and μ is an odd number. Equation ( <ref type="formula" target="#formula_21">13</ref>) can be written as</p><formula xml:id="formula_47">ėij (t) = -t p + p e μ ij (t). (<label>22</label></formula><formula xml:id="formula_48">)</formula><p>The solution to differential equation ( <ref type="formula" target="#formula_47">22</ref>) is</p><formula xml:id="formula_49">e ij (t) = e ij (0) 1 + (μ -1)e μ-1 ij (0) t p+1 p + 1 + pt 1 1-μ .</formula><p>Consider the Lyapunov function v Pij = e 2 ij /2, and its time derivative is</p><formula xml:id="formula_50">vPij (t) = e ij (t)ė ij (t) = -t p + p e μ+1 ij (t) ≤ 0 (<label>23</label></formula><formula xml:id="formula_51">)</formula><p>where μ + 1 is even number and e μ+1 ij (t) ≥ 0. Similar to inequality <ref type="bibr" target="#b20">(21)</ref>, the Frobenius norm of the computing residual error is</p><formula xml:id="formula_52">X(t) F ≤ √ mnϕ 1 max |e ij (t)| = √ mnϕ 1 κ 3 = V P</formula><p>where κ 3 = max(|e ij (t)|). For comparison, the time derivative of Lyapunov function with linear-type activation function and with bipolar-sigmoid function are</p><formula xml:id="formula_53">vLij (t) = -t p + p e 2 ij (t) ≤ 0 (24) vBij (t) = -t p + p e ij (t) 1 -exp -ξ e ij (t) 1 + exp -ξ e ij (t) ≤ 0. (<label>25</label></formula><formula xml:id="formula_54">)</formula><p>Compared ( <ref type="formula" target="#formula_50">23</ref>)- <ref type="bibr" target="#b24">(25)</ref>, since e</p><formula xml:id="formula_55">μ+1 ij e 2 ij</formula><p>|e ij |, the following conclusions can be obtained.</p><p>1) When the error absolute value |e ij | 1, the VP-CDNN with the power-type activation function have a faster convergence rate than those with linear-type and sigmoid-type functions. Since the neural system (8) with linear function is super-exponential convergence, the neural system (8) with power-type activation function is super-exponential.</p><p>2) When the error absolute value |e ij | &lt; 1, the neural system (8) with power-type activation function has a lower convergence rate than those with linear-type or sigmoid-type functions. The proof thus is complete.</p><p>Remark 1: The design parameter of the proposed VP-CDNN can be determined by the initial value E(0), error criterion E, the upper bound of M -1 (t) F (i.e., ϕ 1 ) and the target convergence time t T .</p><p>Taking the linear-type activation function case as an example, if the computational error satisfies the error criterion, it means that the upper bound of the computational error is less than or equal to E, that is</p><formula xml:id="formula_56">κ 1 ϕ 1 exp - t p+1 T p + 1 -pt T ≤ E.<label>(26)</label></formula><p>The above inequality can be rewritten as</p><formula xml:id="formula_57">0 ≤ t p+1 T + t T p 2 + h 1 p + h 2<label>(27)</label></formula><p>where (t T + ln(E/κ 1 ϕ 1 ) = h 1 and ln(E/κ 1 ϕ 1 ) = h 2 .</p><p>Finding the design parameter p of the proposed VP-CDNN is equivalent to soling the transcendental inequality <ref type="bibr" target="#b26">(27)</ref>. Since it is difficult to get the analytical solution to the transcendental inequality, numerical methods (e.g., the bisection method, Newton method, and so on) can be used. By using numerical methods, the approximate numerical solution to the transcendental inequality ( <ref type="formula" target="#formula_57">27</ref>) can be obtained.</p><p>In order to have a larger convergence rate, power-sigmoidtype activation function can be used <ref type="bibr" target="#b49">[50]</ref>, that is</p><formula xml:id="formula_58">f (u) = u μ , if |u| &gt; 1 1+exp(-ξ) 1-exp(-ξ) • 1-exp(-ξ u) 1+exp(-ξ u) , otherwise.</formula><p>Through the above analysis, VP-CDNN with time-varying parameter function ϒ(t) = t p + p can solve the Sylvester equation appropriately via using the mentioned three activation functions.</p><p>Compared with that synthesized by FP-CDNN (6), the following theorem shows that the convergence performance synthesized by VP-CDNN (8) is better.</p><p>Theorem 5: Given matrices function A(t) ∈ R m×m , B(t) ∈ R n×n , and C(t) ∈ R m×n of Sylvester equation ( <ref type="formula" target="#formula_0">1</ref>), if the regularity condition (3) is satisfied and the linear-type, bipolarsigmoid-type, or power-type activation functions are used, the upper-bounds of VP-CDNN <ref type="bibr" target="#b7">(8)</ref> are smaller than upper-bounds of the FP-CDNN <ref type="bibr" target="#b5">(6)</ref>.</p><p>Proof: For the convenience of discussion, assume that VP-CDNN [with ϒ(t) = t p + p] and FP-CDNN (with γ = p) have the same initial state and parameter p without external disturbance.</p><p>First, the computation errors of FP-CDNN can be obtained via the similar method above. Specifically, with linear-type, bipolar-sigmoid-type, and power-type activation functions, we can get the following inequations about the computation errors X (t) F .</p><p>1) For linear-type activation function, we have</p><formula xml:id="formula_59">X (t) F ≤ κ 1 ϕ 1 exp(-pt) = F L . (<label>28</label></formula><formula xml:id="formula_60">)</formula><p>2) For bipolar-sigmoid-type activation function, we have</p><formula xml:id="formula_61">X (t) F ≤ √ mnϕ 1 κ 2 exp - ξ pt 2 = F S . (<label>29</label></formula><formula xml:id="formula_62">)</formula><p>3) For power-type activation function, we have</p><formula xml:id="formula_63">X (t) F ≤ √ mnϕ 1 κ 3 = F P (<label>30</label></formula><formula xml:id="formula_64">)</formula><p>where (1/1-μ) . Second, in order to compare the bounds between the VP-CDNN and the FP-CDNN, a series of ratios can be obtained as bellow.</p><formula xml:id="formula_65">κ 3 = max(|e ij (t)|) and e ij (t) = e ij (0){1 + (μ - 1)e μ-1 ij (0)pt}</formula><p>1) For linear-type activation function, we have</p><formula xml:id="formula_66">σ 1 = V L F L = exp - t p+1 p + 1 ≤ 1.</formula><p>2) For bipolar-sigmoid-type activation function, we have</p><formula xml:id="formula_67">σ 2 = V S F S = exp - ξ t p+1 2(p + 1) ≤ 1.</formula><p>3) For power-type activation function, we have</p><formula xml:id="formula_68">σ 3 = V P F P = 1 μ-1 1 + t p p+1 max e μ-1 ij (0) 1 μ-1 + p max e μ-1 ij (0) ≤ 1.</formula><p>It can be seen that all the ratio σ 1 , σ 2 and σ 3 are less than 1. It implies that VP-CDNN has smaller bounds than FP-CDNN with linear, bipolar-sigmoid and power-type activation functions. The proof thus is complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Robustness Analysis</head><p>As is known that errors and interference always exist in a real system. There is no doubt that some errors may be involved in the realization of VP-CDNN model <ref type="bibr" target="#b7">(8)</ref>. In practical applications, coefficient matrix perturbation, differentiation error and model-implementation error are three kinds of common errors. To illustrate the anti-interference performance of the proposed VP-CDNN, some theorems about robustness are discussed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Coefficient Matrix Perturbation: Theorem 6: Consider the VP-CDNN model with unknown smooth coefficient matrices perturbation</head><formula xml:id="formula_69">A(t) ∈ R m×m , B(t) ∈ R n×n and C(t) ∈ R m×n . If the mass matri- ces M -1 (t) = (I n ⊗ A(t) -B T (t) ⊗ I m ) -1 , M-1 (t) = (I n ⊗ (A(t) + A(t)) -(B(t) + B(t)) T ⊗ I m ) -1 , C(t) and perturbation function C(t) have uniformly bounded norm for any t ∈ [0, ∞), i.e., M -1 (t) F &lt; ϕ 1 , M-1 (t) F &lt; ϕ 2 , C(t) F &lt; β 1 and</formula><p>C(t) F &lt; β 2 , the computational error X(t) -X(t) * F will be bounded. Furthermore, this solving process has a power-type-like convergence rate.</p><p>Proof: Consider the perturbation, and the implicit dynamic equation ( <ref type="formula" target="#formula_13">8</ref>) is converted to</p><formula xml:id="formula_70">Â(t) Ẋ(t)-Ẋ(t) B(t) = -Ȧ(t) X(t) + X(t) Ḃ(t)-Ċ(t)-t p + p × F Â(t) X(t) -X(t) B(t) + Ĉ(t)</formula><p>where</p><formula xml:id="formula_71">Â(t) = A(t) + A(t), B(t) = B(t) + B(t), Ĉ(t) = C(t) + C(t)</formula><p>, and X(t) denotes the solution to the perturbed VP-CDNN system.</p><p>We can get the simple variation of ( <ref type="formula">2</ref>)</p><formula xml:id="formula_72">vec X * (t) = -M -1 (t) vec(C(t)). (<label>31</label></formula><formula xml:id="formula_73">)</formula><p>That is to say, the theoretical solution X * (t) has</p><formula xml:id="formula_74">X * (t) F = vec X * (t) F = M -1 (t) vec(C(t)) F ≤ M -1 (t) F C(t) F ≤ ϕ 1 β 1 .</formula><p>Similar to the analysis of X * (t) F , X * (t) F can be discussed as</p><formula xml:id="formula_75">X * (t) F ≤ M-1 (t) F Ĉ(t) F ≤ ϕ 2 ( C(t) F + C(t) F ) ≤ ϕ 2 (β 1 + β 2 ).</formula><p>By using the linear-type activation function, the computation error X(t) F can be obtained as</p><formula xml:id="formula_76">X(t) F = X(t) -X * (t) F ≤ X(t) -X * (t) F + X * (t) -X * (t) F ≤ κϕ 2 exp - t p+1 p + 1 -pt + X * (t) F + X * (t) F ≤ κϕ 2 exp - t p+1 p + 1 -pt + ϕ 1 β 1 + ϕ 2 (β 1 + β 2 )</formula><p>where κ = Ê(0) F is the initial value of Ê(t) F . It can be seen that the computation error with the linear-type activation function is bounded.</p><p>It is worth pointing out that the computation errors of the VP-CDNN system with sigmoid-type, power-type, or powersigmoid-type activation functions are also bounded. Due to the space limitation and the similar analysis process, the detailed analysis is omitted here. The robustness theorem of coefficient matrix perturbation is thus proved.</p><p>2) Differentiation and Model-Implementation Errors: Differentiation errors and model-implementation errors, which result from truncating/roundoff errors in digital realization or high-order residual errors of circuit components, may appear more frequently than matrix perturbation does in IEEE TRANSACTIONS ON CYBERNETICS neural network realization <ref type="bibr" target="#b50">[51]</ref>. Assume that (t) ∈ R m×m and (t) ∈ R n×n are the differentiation errors of the time derivatives matrices Ȧ(t) and Ḃ(t), respectively, (t) ∈ R m×n is the model-implementation error. The implicit dynamic equation of the perturbed VP-CDNN system can be written as</p><formula xml:id="formula_77">A(t) Ẋ(t) -Ẋ(t)B(t) = -Ȧ(t) + (t) X(t) + X(t) Ḃ(t) + (t) -Ċ(t) + (t) -ϒ(t)F(A(t)X(t) -X(t)B(t) + C(t)). (<label>32</label></formula><formula xml:id="formula_78">)</formula><p>Theorem 7: Consider the VP-CDNN model with unknown smooth differentiation error (t), (t) and modelimplementation error (t) in <ref type="bibr" target="#b31">(32)</ref>.</p><formula xml:id="formula_79">If (t) F ≤ 1 , (t) F ≤ 2 , (t) F ≤ 3 , M -1 (t) F ≤ ϕ 1 and X * (t) F ≤ ϕ 3 for any t ∈ [0, ∞), then the com- putational error X(t) -X * (t) F will be bounded by ϕ 1 (mn+ √ mn)ζ 2 /2(χ ϒ(t)-ζ 1 ϕ 1 ), where ζ 1 = √ n 1 + √ m 2 , ζ 2 = 3 + ζ 1 ϕ 3 , parameter χ &gt; 0 is defined between f (e ij (0))/e ij (0) and f (0). In view of the design-parameter requirement, ϒ(t) &gt; ( √ n 1 + √ m 2</formula><p>)ϕ 1 /χ should be guaranteed. Moreover, under the conditions of using power-type varying-parameter function ϒ(t) = t p + p, when t → +∞, residual error will converge to zero.</p><p>Proof: Similar to ( <ref type="formula" target="#formula_14">9</ref>), ( <ref type="formula" target="#formula_77">32</ref>) can be reformulated into the following vector-type:</p><formula xml:id="formula_80">M(t)ẋ(t) = -t p -p F(M(t)x(t) + c(t)) -Ṁ(t) + (t) x(t) -ċ(t) + φ(t) (<label>33</label></formula><formula xml:id="formula_81">)</formula><p>where (t) := I n ⊗ (t) -T (t) ⊗ I m and φ(t) := vec( (t)).</p><p>Inspired by Zhang and Ge <ref type="bibr" target="#b21">[22]</ref>, from <ref type="bibr" target="#b16">(17)</ref>, we get e(t) = M(t)x(t)+c(t), and ė(t) = Ṁ(t)x(t)+M(t)ẋ(t)+ċ(t). Substitute them into <ref type="bibr" target="#b32">(33)</ref>, and we can obtain</p><formula xml:id="formula_82">ė(t) = -t p + p F(e(t)) -(t)x(t) + φ(t).</formula><p>According to <ref type="bibr" target="#b16">(17)</ref>, we have</p><formula xml:id="formula_83">x(t) = M -1 (t)(e(t) -c(t)) = M -1 (t)e(t) + x * (t)</formula><p>. Therefore, we can get</p><formula xml:id="formula_84">ė(t) = -t p + p F(e(t)) + D(t)e(t) + R(t)</formula><p>where</p><formula xml:id="formula_85">D(t) = -(t)M -1 (t) ∈ R mn×mn and R(t) = φ(t) - (t)x * (t) ∈ R mn×1 .</formula><p>Lyapunov function candidate is defined as v = e T (t)e(t)/2 ≥ 0 and its time-derivative is v = e T (t)ė(t) =t p + p e T (t)F(e(t))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ e T (t)D(t)e(t) + e T (t)R(t) .</head><p>(</p><formula xml:id="formula_86">)<label>34</label></formula><p>Similar to the proof of [22, Th. 2], we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>e T (t)D(t)e(t) ≤ e T (t)e(t) (t)</head><formula xml:id="formula_87">F M -1 (t) F . (<label>35</label></formula><formula xml:id="formula_88">)</formula><p>According to the above inequality ( <ref type="formula" target="#formula_87">35</ref>), ( <ref type="formula" target="#formula_86">34</ref>) can be rewritten as v ≤t p + p e T (t)F(e(t))</p><formula xml:id="formula_89">+ (t) F M -1 (t) F e T (t)e(t) + e T (t)R(t). (<label>36</label></formula><formula xml:id="formula_90">)</formula><p>The scalar form of ( <ref type="formula" target="#formula_89">36</ref>) is</p><formula xml:id="formula_91">v ≤ - m i=1 n j=1 |e ij (t)| t p + p f |e ij (t)| -ζ 1 ϕ 1 |e ij (t)| -ζ 2 = D where (t) F ≤ I n ⊗ (t) F + (t) ⊗ I m F ≤ √ n 1 + √ m 2 = ζ 1 and R(t) F ≤ φ(t) F + (t)x * (t) F ≤ 3 + ζ 1 ϕ 3 = ζ 2 .</formula><p>In order to analyze the stability of the VP-CDNN with error interference, (36) can be discussed in two cases</p><formula xml:id="formula_92">⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ (t p + p)f |e ij | -ζ 1 ϕ 1 |e ij | -ζ 2 ≥ 0 ∀i ∈ {1, . . . , m}, j ∈ {1, . . . , n} (t p + p)f |e ij | -ζ 1 ϕ 1 |e ij | -ζ 2 &lt; 0 ∃i ∈ {1, . . . , m}, j ∈ {1, . . . , n}.</formula><p>According to Lyapunov stability theory <ref type="bibr" target="#b46">[47]</ref>, the following results can be induced responding to the above two cases. 1) For the first case, D ≤ 0, i.e., v ≤ 0, deviation function e(t) will decrease, which implies that X(t) will converge toward X * (t) asymptotically. 2) For the second case, the sign of D is unknown and this case will divide into two situations. If D ≤ 0, i.e., v ≤ 0, the result is similar to case 1). Otherwise, D &gt; 0 and according to <ref type="bibr" target="#b51">[52]</ref>, we can readily get the upper bound of |e ij (t)|, that is</p><formula xml:id="formula_93">|e ij (t)| ≤ 1 + √ mn ζ 2 2(χ (t p + p) -ζ 1 ϕ 1 ) . (<label>37</label></formula><formula xml:id="formula_94">)</formula><p>Since ζ 2 ≥ 0 and (1 + √ mn) ≥ 0, to guarantee inequality (37) hold true, χ(t p + p) -ζ 1 ϕ 1 &gt; 0, i.e., t p + p &gt; ζ 1 ϕ 1 /χ , where parameter χ &gt; 0 is defined between f (e ij (0))/e ij (0) and f (0). According to the proof of Theorem 4 in Section V-A, the Frobenius norm of X(t) = X(t) -X * (t) with linear-type activation function is</p><formula xml:id="formula_95">X(t) F ≤ ϕ 1 mn + √ mn ζ 2 2(χ (t p + p) -ζ 1 ϕ 1 )</formula><p>.</p><p>The computational error X(t) F will approach to zero when t → +∞, i.e., lim t→+∞ X(t) F = 0. In summary, if the situation is in the first case, the computation error X(t) F will converge toward 0. Otherwise, computation error X(t) F will be bounded by</p><formula xml:id="formula_96">ϕ 1 (mn + √ mn)ζ 2 /2(χ ϒ(t)-ζ 1 ϕ 1</formula><p>). In addition, error X(t) F will converge to zero when t → +∞. The robustness theorem with differentiation error and model-implementation error is thus proved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ILLUSTRATIVE EXAMPLE</head><p>For better discussion and comparison, a time-varying Sylvester equation (1) with the following matrices is considered:  </p><formula xml:id="formula_97">A(t) = ⎡ ⎢ ⎣ sin t 10 cos t 10 - cos t 10 sin t 10 ⎤ ⎥ ⎦, B(t) = ⎡ ⎢ ⎣ sin t 100 0 0 cos t 50 ⎤ ⎥ ⎦</formula><formula xml:id="formula_98">C(t) = ⎡ ⎢ ⎣ sin 2 t 10 -1 - cos 2 t 5 sin t cos t 10 sin t cos t 5 -1 ⎤ ⎥ ⎦.</formula><p>Substituting above matrices into (1), and the following theoretical solution X * (t) to Sylvester equation can be calculated as:</p><p>X * (t) = 10 sin t -10 cos t 10 cos t 10 sin t .</p><p>For VP-CDNN, we have the following neural network computation solution X(t) as:</p><formula xml:id="formula_99">X(t) = x 11 x 12 x 21 x 22</formula><p>where x ij denotes the (i, j)th element of X(t). In order to obtain better comparison results, the following initial value X 0 is considered:</p><formula xml:id="formula_100">X 0 = X(0) = x 0a x 0b x 0c x 0d</formula><p>where x 0a , x 0b , x 0c , and x 0d are created by the random function in MATLAB within range [-40, +40]. We assume that the solving process of Sylvester equation will be completed if the computation error X(t) -X * (t) F is smaller than the 0.1% of the range, i.e., X(t) -X * (t) F ≤ 0.1% × 80 = 0.08. In addition, we assume that the cost time is obtained when all the errors are under 0.08 in the random-repetitive simulations.</p><p>For activation function, parameter ξ = 4 and index μ = 3 in sigmoid and power type are considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convergence Discussion</head><p>First, the comparisons based on computer simulations between FP-CDNN and VP-CDNN for solving the practical Sylvester equation with different parameters are illustrated in Figs. <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref>. When γ = 0.1, by using FP-CDNN, it has to cost more than 65 s that the error approaches to zero [see Fig. <ref type="figure" target="#fig_2">3</ref>(a) and (b)]. By increasing γ , the convergence performance can be improved a bit [see Fig. <ref type="figure" target="#fig_2">3(c)-(f)</ref>]. Specifically, when γ = 1 and γ = 5, errors X(t) F = X(t) -X * (t) F approach to zero within 6.7 and 1.4 s. For comparison and for illustration, the simulation results of VP-CDNN with the same parameters are shown in Fig. <ref type="figure" target="#fig_3">4</ref>. As can be seen from Fig. <ref type="figure" target="#fig_3">4</ref>(a), it only needs 5.7 s, which is much less than 68-s cost by FP-CDNN in the same condition. The same situations happen with p = 1 and p = 5. More tests and detailed convergence time of FP-CDNN and VP-CDNN with different parameters are shown in Table <ref type="table" target="#tab_3">I</ref>. In short, the performance of VP-CDNN is much better than that of FP-CDNN because VP-CDNN costs less time and obtains better convergence results.</p><p>Second, to illustrate the different convergence performance of FP-CDNN and VP-CDNN with four different kinds of activation functions (i.e., linear-type, sigmoid-type, power-type, and power-sigmoid-type), Fig. <ref type="figure">5</ref> illustrate the different error convergence situations. Simulation results verify that all the convergence situations synthesized by the VP-CDNN are better than those synthesized by using the traditional FP-CDNN. For instance, with linear-type activation function, FP-CDNN cannot approach to 0 within 5 s, but VP-CDNN can converge IEEE TRANSACTIONS ON CYBERNETICS VP-CDNN. This result matches very well with the conclusion of Theorem 5.</p><p>To sum up, the convergence performance synthesized by VP-CDNN is much better than that of FP-CDNN when solving time-varying Sylvester equation. In addition, the upper bounds of the errors are always smaller than that synthesized by FP-CDNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Robustness Discussion</head><p>In this section, comparison results of robustness are shown in Fig. <ref type="figure">7</ref>. For simplicity and without loss of generality, we consider differentiation and model-implementation errors. To conduct the computer simulations, the following differentiation and model-implementation errors are considered:</p><formula xml:id="formula_101">(t) = w 1 cos 2t 0 0 sin2t , (t) = w 2 0 cos 2t sin 2t 0 (t) = w 3 sin 2t 0 0 cos 2t</formula><p>where w 1 , w 2 , and w 3 denote the degree of disturbance, i.e., different w i leading to different disturbance to the different coefficient matrices in the neural networks.</p><p>It can be seen from Fig. <ref type="figure">7</ref> that the robustness performance of VP-CDNN is better than that of FP-CDNN with the same parameters. When w 1 = 0.05, w 2 = 0.05, and w 3 = 0.05, although the error is not large, FP-CDNN cannot approach to 0. From Fig. <ref type="figure">7</ref>, we can see that the computation errors X(t) F = X(t) -X * (t) F of FP-CDNN decrease at the beginning, but finally it cannot decrease to zero. This implies that X(t) cannot approach to X * (t) finally when differentiation errors and model-implementation errors exist. Different from FP-CDNN, when disturbed by the same factors, the computation errors X(t) F synthesized by VP-CDNN converge to 0 asymptotically, even for the large disturbance. It is  <ref type="bibr" target="#b12">[13]</ref> AND <ref type="bibr" target="#b13">[14]</ref>   <ref type="bibr" target="#b12">[13]</ref> AND <ref type="bibr" target="#b13">[14]</ref> worth pointing out that as the design parameter p increases, the convergence is expedited and the computation error is decreased.</p><p>In summary, VP-CDNN can guarantee good convergence performance even there exist intense interference or large differentiation and model-implementation errors. The influence of large errors will be weaken when t → +∞ gradually and thus, the aforementioned performance of robustness is well verified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparisons With Finite-Time FP-CDNN</head><p>The classic FP-CDNN method for solving the time-varying Sylvester equation (1) needs infinitely long time due to the exponential convergence rate if the traditional activation functions (e.g., linear-type, sigmoid-type, power-type, and power-sigmoid-type) are used. In order to remedy this disadvantage, some FP-CDNNs with the finite-time activation function were proposed <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. For illustrating and verifying the effectiveness of VP-CDNN, comparison results between the proposed VP-CDNN and the finite-time FP-CDNN are presented in this section. The specific differences and similarities among the proposed VP-CDNN in this paper, the methods in <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b13">[14]</ref> are shown in Tables <ref type="table" target="#tab_4">II</ref> and<ref type="table" target="#tab_5">III</ref>.</p><p>According to the definition of finite-time FP-CDNN, the design formula is formulated as</p><formula xml:id="formula_102">Ė(t) = -γ (E(t))</formula><p>where (•) is the finite time activation function. Specifically, (•) is defined as the following sign-bi-power activation function in <ref type="bibr" target="#b12">[13]</ref>, that is:</p><formula xml:id="formula_103">(ε(t)) = sign(ε(t)) 1 2 |ε(t)| r + 1 2 |ε(t)| 1 r</formula><p>and the following tunable activation function in <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_104">(ε(t)) = sign(ε(t)) k 1 |ε(t)| r + k 2 |ε(t)| + k 3 |ε(t)| 1 r</formula><p>where sign(•) is the sign function of a vector, k 1 &gt; 0, k 2 &gt; 0, and k 3 &gt; 0 are tunable parameters. Substituting the error function (4) into the finite-time FP-CDNN design formula, the implicit dynamic model of the neural network is obtained</p><formula xml:id="formula_105">A(t) Ẋ(t) -Ẋ(t)B(t) = -Ȧ(t)X(t) + X(t) Ḃ(t) -Ċ(t) -γ (A(t)X(t) -X(t)B(t) + C(t)).</formula><p>If the coefficient matrix perturbation is considered, the perturbed implicit dynamic model is</p><formula xml:id="formula_106">Â(t) Ẋ(t) -Ẋ(t) B(t) = -Ȧ(t) X(t) + X(t) Ḃ(t) -Ċ(t) -γ Â(t) X(t) -X(t) B(t) + Ĉ(t)</formula><p>where Â(t), B(t), and Ĉ(t) possess the same definitions in Section V-B. In addition, the perturbed neural dynamic equation with model-implementation error (t) and differential errors (t) and (t) is shown as follows:</p><formula xml:id="formula_107">A(t) Ẋ(t) -Ẋ(t)B(t) = -Ȧ(t) + (t) X(t) + X(t) Ḃ(t) + (t) -Ċ(t) + (t) -γ (A(t)X(t) -X(t)B(t) + C(t)).</formula><p>The comparison results of computer simulations among the proposed VP-CDNN with a power-sigmoid-type activation function (ξ = 4 and μ = 3), the FP-CDNN with a sign-bipower activation function (r = 0.5) and the FP-CDNN with a tunable activation function (r = 0.5 and k 1 = k 2 = k 3 = 1) when they are applied to solving the time-varying Sylvester equation with the same coefficient matrices are illustrated in Fig. <ref type="figure" target="#fig_5">8</ref>. From Fig. <ref type="figure" target="#fig_5">8</ref>(a), the convergence speed of the VP-CDNN with a power-sigmoid-type activation function is faster than those of both FP-CDNNs with sign-bi-power and tunable activation functions. Fig. <ref type="figure" target="#fig_5">8</ref>(b) shows that, under the same coefficient matrix perturbation, the computational errors of all the neural networks tend to oscillation, but the VP-CDNN converges faster than the other two FP-CDNNs. In Fig. <ref type="figure" target="#fig_5">8(c)</ref>, with considering the model-implementation error and differential errors, the computational error of the VP-CDNN decreases very fast and even finally converges to zero, while the computational errors of the FP-CDNNs when using the sign-bi-power or tunable activation activations tend to oscillation. The simulation results further verify the effectiveness of the proposed VP-CDNN when it is used to solve the time-varying Sylvester equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. APPLICATION TO ROBOT KINEMATICS</head><p>Recurrent neural network methods are usually applied to kinematic control problems of robot manipulators <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. In this section, in order to verify the effectiveness of the VP-CDNN, a specific application to the kinematic control of a three-link planar robot manipulator is illustrated.</p><p>Consider a three-link planar robot manipulator with jointspace vector θ(t) = [θ 1 (t), θ 2 (t), θ 3 (t)] T ∈ R 3 , where θ 1 (t), θ 2 (t), and θ 3 (t) denote the first, second, and third joint angles. l 1 = l 2 = l 3 = 1.0 m are the lengths of each link. The position vector r(t) = [r X (t), r Y (t)] T of the end-effector in Cartesian space is determined by the joint-space vector θ(t) through the differentiable nonlinear forward-kinematic equation</p><formula xml:id="formula_108">r X (t) r Y (t) = l 1 c 1 + l 2 c 2 + l 3 c 3 l 1 s 1 + l 2 s 2 + l 3 s 3 = T(θ (t))<label>(38)</label></formula><p>where T(•) : R 3 → R 2 , c 1 = cos(θ 1 (t)), c 2 = cos(θ 1 (t)+θ 2 (t)), c 3 = cos(θ 1 (t)+θ 2 (t)+θ 3 (t)), s 1 = sin(θ 1 (t)), s 2 = sin(θ 1 (t)+ θ 2 (t)), and s 3 = sin(θ 1 (t) + θ 2 (t) + θ 3 (t)).</p><p>Since the inverse solving process is nonlinear, it is difficult to obtain the inverse mapping function T -1 (•). The usual method for solving the inverse kinematics problem is to solve the problem at the joint-velocity level by differentiating <ref type="bibr" target="#b37">(38)</ref>, that is ṙ(t) = J(θ (t)) θ (t) <ref type="bibr" target="#b38">(39)</ref> where J(θ (t)) = ∂T(θ (t))/∂θ (t) is the Jacobian matrix.</p><p>The inverse kinematics problem can be solved if the inverse matrix J -1 (θ (t)) is obtained. Since J(θ (t)) is not a square matrix, its inverse does not exit and its pseudoinverse J † (θ (t)) has to be computed. In fact, J † (θ (t)) will be obtained if the solution X(t) to the following linear equation is solved, that is:</p><formula xml:id="formula_109">X(t)J(θ (t))J T (θ (t)) = J T (θ (t)). (<label>40</label></formula><formula xml:id="formula_110">)</formula><p>As long as the solution X(t) to ( <ref type="formula" target="#formula_109">40</ref>) is calculated, the inverse/pseudo-inverse J † (θ (t)) of J(θ (t)) is known. Equation ( <ref type="formula" target="#formula_109">40</ref>) is actually a Sylvester equation, where A(t) = 0. The three-link robot manipulator and the corresponding trajectory of tracking a circular path are shown in Fig. <ref type="figure" target="#fig_6">9</ref>. The initial joint angle state of the manipulator is θ(0) = [π/3, -π/6, -π/3] T and the end-effector task is to track a circular trajectory with radius l R = 0.5 m.</p><p>The computer simulation results solved by VP-CDNN method with p = 100 and linear-type activation are shown in Fig. <ref type="figure" target="#fig_7">10</ref>. The position error of the end-effector is defined as ε VP-CDNN = r(t) -T(θ (t)) and ε X and ε Y denote the position error at x-and y-axis. The expected path and the  In conclusion, the computer simulation results verify the effectiveness of the proposed VP-CDNN method for solving the time-varying inverse kinematics problem in robot motion planning fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>A VP-CDNN has been proposed in this paper for online solution to time-varying Sylvester equation with difference kinds of activation functions. Different from the traditional FP-CDNN, this novel VP-CDNN with time-varying parameter function ϒ(t) = t p + p obtains better convergence and robustness performance for solving time-varying Sylvester equation problem. Some theorems of super-exponential convergence and strong robustness performance are proved in mathematics. Simulation comparisons between the proposed VP-CDNN and the traditional FP-CDNN verify the superior convergence and robustness of VP-CDNN. The application to the kinematic control verifies that VP-CDNN method can be applied to solving a practical engineering problem. It is worth pointing out that such a VP-CDNN has the potential to apply to electronic circuits, mathematical programming problems or control problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Block diagram of VP-CDNN model (8) for Sylvester equation solving problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Power-type varying-parameter function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Online solution to Sylvester equation (1) and the computational error X(t) -X * (t) F by FP-CDNN (5). (a) Solution to FP-CDNN with γ = 0.1. (b) Error X(t) -X * (t) F of FP-CDNN with γ = 0.1. (c) Solution to FP-CDNN with γ = 1. (d) Error X(t) -X * (t) F of FP-CDNN with γ = 1. (e) Solution to FP-CDNN with γ = 5. (f) Error X(t) -X * (t) F of FP-CDNN with γ = 5. The red dashed lines represent the trajectories of the theoretical solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Online solution to Sylvester equation (1) and the computational error X(t) -X * (t) F by VP-CDNN<ref type="bibr" target="#b6">(7)</ref>. (a) Solution to VP-CDNN with p = 0.1, i.e.,ϒ(t) = t 0.1 + 0.1. (b) Error X(t) -X * (t) F of VP-CDNN with p = 0.1. (c) Solution to VP-CDNN with p = 1, i.e., ϒ(t) = t + 1. (d) Error X(t) -X * (t) Fof VP-CDNN with p = 1. (e) Solution to VP-CDNN with p = 5, i.e., ϒ(t) = t 5 + 5. (f) Error X(t) -X * (t) F of VP-CDNN with p = 5. The red dashed lines represent the trajectories of the theoretical solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .Fig. 7 .</head><label>567</label><figDesc>Fig. 5. Convergence comparisons of X(t) -X * (t) F between FP-CDNN and VP-CDNN with different activation functions for solving Sylverster equation without perturbation (γ 1). The blue lines represent FP-CDNN. The black lines represent VP-CDNN. (a) Linear function. (b) Sigmoid function. (c) Power function. (d) Power-sigmoid function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparison results among the VP-CDNN with power-sigmoidtype activation function, FP-CDNN with sign-bi-power activation function and FP-CDNN with tunable function. (a) Convergence results. Robustness results with (b) coefficient matrix perturbation and (c) model-implementation error and differential errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. (a) Three-link planar robot manipulator. (b) Tracking trajectory of the end-effector with a circular path tracking task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Simulation results solved by VP-CDNN with p = 100. (a) Expected path and end-effector trajectory. (b) Position error of the end-effector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>f (•) denotes the activation subfunction of matrix elements. Second, Lyapunov function is used to prove the global convergence. Define Lyapunov function candidate v ij (t) = e 2 ij (t)/2, ∀i ∈ {1, 2, . . . , m}, j ∈ {1, 2, . . . , n}, and its time derivative is vij</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>a 2 /2 + a 3 /3 -• • • and a/(1a) = a + a 2 + a 3 + • • • for |a| &lt; 1 as well as<ref type="bibr" target="#b18">(19)</ref>, we can obtain the following results for |k(t)| &lt; 1:</figDesc><table /><note><p><p><p><p>2 </p>/ exp(-ξ e ij (0)) ≥ 0 is determined by the initial conditions e ij (0). In</p><ref type="bibr" target="#b18">(19)</ref></p>, k(t) will converge to zero when t → +∞, and this can lead e ij (t) asymptotically approaches to zero. By utilizing the Taylor series expansion formulas ln(1 + a) = a</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I CONVERGENCE</head><label>I</label><figDesc>TIME OF FP-CDNN AND VP-CDNN WHEN SOLVING SYLVESTER EQUATION</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II DIFFERENCES</head><label>II</label><figDesc>AMONG THE PROPOSED VP-CDNN IN THIS PAPER, THE METHODS IN</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III SIMILARITIES</head><label>III</label><figDesc>AMONG THE PROPOSED METHOD IN THIS PAPER, THE METHODS IN</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation under Grant 61603142, in part by the Science and Technology Program of Guangzhou under Grant 201707010225, in part by the Fundamental Research Funds for Central Universities under Grant 2017MS049, in part by the Guangdong Natural Science Funds for Distinguished Young Scholar under Grant 2017A030306009, and in part by the Scientific Research Starting Foundation of South China University of Technology. This paper was recommended by Associate Editor S. Cruces.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He is currently an Associate Professor with the College of Information Science and Engineering, Jishou University, Jishou, China. His current research interests include neural networks, robotics, and intelligent information processing.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">R-FUSE: Robust fast fusion of multiband images based on solving a Sylvester equation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dobigeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Tourneret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Godsill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1632" to="1636" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Manifold regularized multi-view subspace clustering for image representation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recognit</title>
		<meeting>Int. Conf. Pattern Recognit<address><addrLine>Cancún, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="283" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smooth representation clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3834" to="3841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Local graph regularized coding for salient object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Phys. Technol</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="124" to="131" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Control configuration selection for bilinear systems via generalised Hankel interaction index array</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Shaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tahavori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Control</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast tensor product solvers for optimization problems with fractional differential equations as constraints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dolgov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Savostyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Comput</title>
		<imprint>
			<biblScope unit="volume">273</biblScope>
			<biblScope unit="page" from="604" to="623" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solution of the matrix equation AX + XB = C [F4]</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Bartels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="820" to="826" />
			<date type="published" when="1972-09">Sep. 1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extensions to the Bartels-Stewart algorithm for linear matrix equations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kleinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="87" />
			<date type="published" when="1978-02">Feb. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simulink comparison of varyingparameter convergent-differential neural-network and gradient neural network for solving online linear time-varying equations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intell. Control Autom</title>
		<meeting>Intell. Control Autom<address><addrLine>Guilin, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="887" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On exponential convergence of nonlinear gradient dynamics system with application to square root finding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlin. Dyn</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="983" to="1003" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gradient-based neural network for online solution of Lyapunov matrix equation with Li activation function</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf</title>
		<meeting>Int. Conf</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="955" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MATLAB simulink modeling and simulation of Zhang neural networks for online time-varying Sylvester equation solving</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Ijcnn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Joint Conf. Neural Netw</title>
		<meeting>IEEE Int. Joint Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="285" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accelerating a recurrent neural network to finite-time convergence for solving time-varying Sylvester equation by using a sign-bi-power activation function</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="205" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finite-time stability and its application for solving time-varying Sylvester equation by recurrent neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="763" to="784" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nonlinearly activated neural network for solving timevarying complex Sylvester equation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1397" to="1407" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modified ZNN for timevarying quadratic programming with inherent tolerance to noises and its application to kinematic redundancy resolution of robot manipulators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Electron</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6978" to="6988" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhanced discrete-time Zhang neural network for time-variant matrix inversion in the presence of bias noises</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">207</biblScope>
			<biblScope unit="page" from="220" to="230" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A convergence-accelerated Zhang neural network and its solution application to Lyapunov equation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page" from="213" to="218" />
			<date type="published" when="2016-06">Jun. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Taylor O(h 3 ) discretization of ZNN models for dynamic equality-constrained quadratic programming with application to manipulators</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="237" />
			<date type="published" when="2016-02">Feb. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A finite-time convergent Zhang neural network and its application to real-time matrix square root finding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-017-3010-z</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Link between and comparison and combination of Zhang neural network and quasi-Newton BFGS method for time-varying quadratic minimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="490" to="503" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Design and analysis of a general recurrent neural network model for time-varying matrix inversion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1477" to="1490" />
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Power Electronics Handbook: Devices, Circuits and Applications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Rashid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Butterworth-Heinemann</publisher>
			<pubPlace>Boston, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Initial position estimation in SRM using bootstrap circuit without predefined inductance parameters</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Electron</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2449" to="2456" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient three-dimensional resist profile-driven source mask optimization optical proximity correction based on Abbe-principal component analysis and Sylvester equation</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Micro/Nanolithograph. MEMS MOEMS</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">11006</biblScope>
			<date type="published" when="2015">2015</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">High speed and flexible PEB 3D diffusion simulation based on Sylvester equation</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE Adv. Lithography</title>
		<meeting>SPIE Adv. Lithography</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Closed-form redundancy solving of serial chain robots with a weak generalized inverse approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tondu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robot. Auton. Syst</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="360" to="370" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Hessenberg-Schur method for the problem AX + XB= C</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="909" to="913" />
			<date type="published" when="1979-12">Dec. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ADI iteration for Lyapunov equations: A tangential approach and adaptive shift selection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K F</forename><surname>Panzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lohmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Numer. Math</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="85" to="95" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Computing real low-rank solutions of Sylvester equations by the factored ADI method</title>
		<author>
			<persName><forename type="first">P</forename><surname>Benner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kürschner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Math. Appl</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1656" to="1672" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The ADI iteration for Lyapunov equations implicitly performs h 2 pseudo-optimal model order reduction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K F</forename><surname>Panzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Control</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="481" to="493" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From low-rank approximation to a rational Krylov subspace method for the Lyapunov equation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1622" to="1637" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Krylov subspace methods for large-scale constrained Sylvester equations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Shank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Simoncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1448" to="1463" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Sylvester-Arnoldi type method for the generalized eigenvalue problem with two-by-two operator determinants</title>
		<author>
			<persName><forename type="first">K</forename><surname>Meerbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Plestenjak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1131" to="1146" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modified conjugate gradient method for obtaining the minimum-norm solution of the generalized coupled Sylvester-conjugate matrix equations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Model</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1260" to="1275" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The scaling conjugate gradient iterative method for two types of linear matrix equations</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Math. Appl</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1098" to="1113" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Nested splitting CG-like iterative method for solving the continuous Sylvester equation and preconditioning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Zak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toutounian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="865" to="880" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An iterative method for solving the continuous Sylvester equation by emphasizing on the skew-Hermitian parts of the coefficient matrices</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Zak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toutounian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Math</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="633" to="649" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On modified HSS iteration methods for continuous Sylvester equations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Math. Comput</title>
		<imprint>
			<biblScope unit="volume">263</biblScope>
			<biblScope unit="page" from="84" to="93" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On normal and skew-Hermitian splitting iteration methods for large sparse continuous Sylvester equations</title>
		<author>
			<persName><forename type="first">Q.-Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="145" to="154" />
			<date type="published" when="2014-10">Oct. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Trajectory predictor by using recurrent neural networks in visual tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3172" to="3183" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Inverse-free extreme learning machine with optimal information updating</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1229" to="1241" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A one-layer recurrent neural network for pseudoconvex optimization problems with equality and inequality constraints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3063" to="3074" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Synchronization of discretetime neural networks with delays and Markov jump topologies based on tracker information</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="157" to="164" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Comparing the learning effectiveness of BP, ELM, I-ELM, and SVM for corporate credit ratings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="285" to="295" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A recurrent neural network for solving Sylvester equation with time-varying coefficients</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1053" to="1063" />
			<date type="published" when="2002-09">Sep. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Hom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Topics in Matrix Analysis</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge Univ Press</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Strang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aarikka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Introduction to Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="1986">1986</date>
			<publisher>Wellesley-Cambridge Press</publisher>
			<pubPlace>Wellesley, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adaptive Neural Network Control of Robotic Manipulators. Singapore: World Sci</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Revisit the analog computer and gradient-based neural system for matrix inversion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Mediterr. Conf. Control Autom. Intell. Control</title>
		<meeting>IEEE Int. Symp. Mediterr. Conf. Control Autom. Intell. Control<address><addrLine>Limassol, Cyprus</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1411" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ismail</surname></persName>
		</author>
		<title level="m">Analog VLSI Implementation of Neural Systems</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">80</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Robustness analysis of the Zhang neural network for online time-varying quadratic optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. A Math. Theor</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">245202</biblScope>
			<date type="published" when="2010">2010</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dynamic neural networks for kinematic redundancy resolution of parallel Stewart platforms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1538" to="1550" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Human-like behavior generation based on head-arms model for robot tracking external targets and body parts</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Magnenat-Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1390" to="1400" />
			<date type="published" when="2015-08">Aug. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
