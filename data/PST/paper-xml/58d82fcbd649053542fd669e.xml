<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEEP VARIATIONAL INFORMATION BOTTLENECK</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
							<email>alemi@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Ian</forename><surname>Fischer</surname></persName>
							<email>iansf@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
							<email>jvdillon@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
							<email>kpmurphy@google.com</email>
						</author>
						<title level="a" type="main">DEEP VARIATIONAL INFORMATION BOTTLENECK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a variational approximation to the information bottleneck of <ref type="bibr" target="#b33">Tishby et al. (1999)</ref>. This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method "Deep Variational Information Bottleneck", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We adopt an information theoretic view of deep networks. We regard the internal representation of some intermediate layer as a stochastic encoding Z of the input source X, defined by a parametric encoder p(z|x; θ).<ref type="foot" target="#foot_0">1</ref> Our goal is to learn an encoding that is maximally informative about our target Y , measured by the mutual information between our encoding and the target I(Z, Y ; θ), where I(Z, Y ; θ) = dx dy p(z, y|θ) log p(z, y|θ) p(z|θ)p(y|θ)</p><p>. <ref type="foot" target="#foot_1">2</ref>(1)</p><p>Given the data processing inequality, and the invariance of the mutual information to reparameterizations, if this was our only objective we could always ensure a maximally informative representation by taking the identity encoding of our data (Z = X), but this is not a useful representation of our data. Instead we would like to find the best representation we can obtain subject to a constraint on its complexity. A natural and useful constraint to apply is on the mutual information between our encoding and the original data, I(X, Z) ≤ I c , where I c is the information constraint. This suggests the objective: max θ I(Z, Y ; θ) s.t. I(X, Z; θ) ≤ I c .</p><p>(2)</p><p>Equivalently, with the introduction of a Lagrange multiplier β, we can maximize the objective function R IB (θ) = I(Z, Y ; θ) − βI(Z, X; θ).</p><p>(3) Here our goal is to learn an encoding Z that is maximally expressive about Y while being maximally compressive about X, where β ≥ 0 controls the tradeoff. <ref type="foot" target="#foot_2">3</ref> This approach is known as the information bottleneck (IB), and was first proposed in <ref type="bibr" target="#b33">Tishby et al. (1999)</ref>. Intuitively, the first term in R IB encourages Z to be predictive of Y ; the second term encourages Z to "forget" X. Essentially it forces Z to act like a minimal sufficient statistic of X for predicting Y .</p><p>The IB principle is appealing, since it defines what we mean by a good representation, in terms of the fundamental tradeoff between having a concise representation and one with good predictive power <ref type="bibr" target="#b32">(Tishby &amp; Zaslavsky, 2015a)</ref>. The main drawback of the IB principle is that computing mutual information is, in general, computationally challenging. There are two notable exceptions: the first is when X, Y and Z are all discrete, as in <ref type="bibr" target="#b33">Tishby et al. (1999)</ref>; this can be used to cluster discrete data, such as words. The second case is when X, Y and Z are all jointly Gaussian <ref type="bibr" target="#b8">(Chechik et al., 2005)</ref>. However, these assumptions both severely constrain the class of learnable models.</p><p>In this paper, we propose to use variational inference to construct a lower bound on the IB objective in Equation 3. We call the resulting method VIB (variational information bottleneck). By using the reparameterization trick <ref type="bibr" target="#b17">(Kingma &amp; Welling, 2014)</ref>, we can use Monte Carlo sampling to get an unbiased estimate of the gradient, and hence we can optimize the objective using stochastic gradient descent. This allows us to use deep neural networks to parameterize our distributions, and thus to handle high-dimensional, continuous data, such as images, avoiding the previous restrictions to the discrete or Gaussian cases.</p><p>We also show, by a series of experiments, that stochastic neural networks, fit using our VIB method, are robust to overfitting, since VIB finds a representation Z which ignores as many details of the input X as possible. In addition, they are more robust to adversarial inputs than deterministic models which are fit using (penalized) maximum likelihood estimation. Intuitively this is because each input image gets mapped to a distribution rather than a unique Z, so it is more difficult to pass small, idiosyncratic perturbations through the latent bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The idea of using information theoretic objectives for deep neural networks was pointed out in <ref type="bibr" target="#b34">Tishby &amp; Zaslavsky (2015b)</ref>. However, they did not include any experimental results, since their approach for optimizing the IB objective relied on the iterative Blahut Arimoto algorithm, which is infeasible to apply to deep neural networks.</p><p>Variational inference is a natural way to approximate the problem. Variational bounds on mutual information have previously been explored in <ref type="bibr" target="#b1">Agakov (2004)</ref>, though not in conjunction with the information bottleneck objective. <ref type="bibr" target="#b20">Mohamed &amp; Rezende (2015)</ref> also explore variational bounds on mutual information, and apply them to deep neural networks, but in the context of reinforcement learning. We recently discovered <ref type="bibr">Chalk et al. (2016)</ref>, who independently developed the same variational lower bound on the IB objective as us. However, they apply it to sparse coding problems, and use the kernel trick to achieve nonlinear mappings, whereas we apply it to deep neural networks, which are computationally more efficient. In addition, we are able to handle large datasets by using stochastic gradient descent, whereas they use batch variational EM.</p><p>In the supervised learning literature, our work is related to the recently proposed confidence penalty (entropy regularization) method of <ref type="bibr">(Pereyra et al., 2016)</ref>. In this work, they fit a deterministic network by optimizing an objective that combines the usual cross entropy loss with an extra term which penalizes models for having low entropy predictive distributions. In more detail, their cost function has the form</p><formula xml:id="formula_0">J CP = 1 N N n=1 [H(p(y|y n ), p(y|x n )) − βH(p(y|x n ))]<label>(4)</label></formula><p>where H(p, q) = − y p(y) log q(y) is the cross entropy, H(p) = H(p, p) is the entropy, p(y|y n ) = δ yn (y) is a one-hot encoding of the label y n , and N is the number of training examples. (Note that setting β = 0 corresponds to the usual maximum likelihood estimate.) In <ref type="bibr">(Pereyra et al., 2016)</ref> they show that CP performs better than the simpler technique of label smoothing, in which we replace the zeros in the one-hot encoding of the labels by &gt; 0, and then renormalize so that the distribution still sums to one. We will compare our VIB method to both the confidence penalty method and label smoothing in Section 4.1.</p><p>In the unsupervised learning literature, our work is closely related to the work in <ref type="bibr" target="#b17">Kingma &amp; Welling (2014)</ref> on variational autoencoders. In fact, their method is a special case of an unsupervised version of the VIB, but with the β parameter fixed at 1.0, as we explain in Appendix B. The VAE objective, but with different values of β, was also explored in <ref type="bibr">Higgins et al. (2016)</ref>, but from a different perspective.</p><p>The method of <ref type="bibr" target="#b36">Wang et al. (2016b)</ref> proposes a latent variable generative model of both x and y; their variational lower bound is closely related to ours, with the following differences. First, we do not have a likelihood term for x, since we are in the discriminative setting. Second, they fix β = 1, since they do not consider compression.</p><p>Finally, the variational fair autoencoder of <ref type="bibr" target="#b18">Louizos et al. (2016)</ref> shares with our paper the idea of ignoring parts of the input. However, in their approach, the user must specify which aspects of the input (the so-called "sensitive" parts) to ignore, whereas in our method, we can discover irrelevant parts of the input automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>Following standard practice in the IB literature, we assume that the joint distribution p(X, Y, Z) factors as follows:</p><formula xml:id="formula_1">p(X, Y, Z) = p(Z|X, Y )p(Y |X)p(X) = p(Z|X)p(Y |X)p(X)<label>(5)</label></formula><p>i.e., we assume p(Z|X, Y ) = p(Z|X), corresponding to the Markov chain Y ↔ X ↔ Z. This restriction means that our representation Z cannot depend directly on the labels Y . (This opens the door to unsupervised representation learning, which we will discuss in Appendix B.) Besides the structure in the joint data distribution p(X, Y ), the only content at this point is our model for the stochastic encoder p(Z|X), all other distributions are fully determined by these and the Markov chain constraint.</p><p>Recall that the IB objective has the form I(Z, Y ) − βI(Z, X). We will examine each of these expressions in turn. Let us start with I(Z, Y ). Writing it out in full, this becomes</p><formula xml:id="formula_2">I(Z, Y ) = dy dz p(y, z) log p(y, z) p(y)p(z) = dy dz p(y, z) log p(y|z) p(y) .<label>(6)</label></formula><p>where p(y|z) is fully defined by our encoder and Markov Chain as follows:</p><formula xml:id="formula_3">p(y|z) = dx p(x, y|z) = dx p(y|x)p(x|z) = dx p(y|x)p(z|x)p(x) p(z) . (<label>7</label></formula><formula xml:id="formula_4">)</formula><p>Since this is intractable in our case, let q(y|z) be a variational approximation to p(y|z). This is our decoder, which we will take to be another neural network with its own set of parameters. Using the fact that the Kullback Leibler divergence is always positive, we have</p><formula xml:id="formula_5">KL[p(Y |Z), q(Y |Z)] ≥ 0 =⇒ dy p(y|z) log p(y|z) ≥ dy p(y|z) log q(y|z) ,<label>(8)</label></formula><p>and hence</p><formula xml:id="formula_6">I(Z, Y ) ≥ dy dz p(y, z) log q(y|z) p(y)<label>(9)</label></formula><p>= dy dz p(y, z) log q(y|z) − dy p(y) log p(y)</p><p>= dy dz p(y, z) log q(y|z) + H(Y ) .</p><p>Notice that the entropy of our labels H(Y ) is independent of our optimization procedure and so can be ignored.</p><p>Focusing on the first term in Equation 11, we can rewrite p(y, z) as p(y, z) = dx p(x, y, z) = dx p(x)p(y|x)p(z|x) (leveraging our Markov assumption), which gives us a new lower bound on the first term of our objective: I(Z, Y ) ≥ dx dy dz p(x)p(y|x)p(z|x) log q(y|z) .</p><p>(</p><formula xml:id="formula_9">)<label>12</label></formula><p>This only requires samples from both our joint data distribution as well as samples from our stochastic encoder, while it requires we have access to a tractable variational approximation in q(y|z).</p><p>We now consider the term βI(Z, X):</p><formula xml:id="formula_10">I(Z, X) = dz dx p(x, z) log p(z|x) p(z) = dz dx p(x, z) log p(z|x) − dz p(z) log p(z) . (<label>13</label></formula><formula xml:id="formula_11">)</formula><p>In general, while it is fully defined, computing the marginal distribution of Z, p(z) = dx p(z|x)p(x), might be difficult. So let r(z) be a variational approximation to this marginal. Since KL[p(Z), r(Z)] ≥ 0 =⇒ dz p(z) log p(z) ≥ dz p(z) log r(z), we have the following upper bound:</p><formula xml:id="formula_12">I(Z, X) ≤ dx dz p(x)p(z|x) log p(z|x) r(z) . (<label>14</label></formula><formula xml:id="formula_13">)</formula><p>Combining both of these bounds we have that I(Z, Y ) − βI(Z, X) ≥ dx dy dz p(x)p(y|x)p(z|x) log q(y|z)</p><formula xml:id="formula_14">− β dx dz p(x)p(z|x) log p(z|x) r(z) = L . (<label>15</label></formula><formula xml:id="formula_15">)</formula><p>We now discuss how to compute the lower bound L in practice. We can approximate p(x, y) = p(x)p(y|x) using the empirical data distribution p(x, y) = 1 N N n=1 δ xn (x)δ yn (y), and hence we can write</p><formula xml:id="formula_16">L ≈ 1 N N n=1 dz p(z|x n ) log q(y n |z) − β p(z|x n ) log p(z|x n ) r(z) . (<label>16</label></formula><formula xml:id="formula_17">)</formula><p>Suppose we use an encoder of the form p(z|x) = N (z|f µ e (x), f Σ e (x)), where f e is an MLP which outputs both the K-dimensional mean µ of z as well as the K × K covariance matrix Σ. Then we can use the reparameterization trick <ref type="bibr" target="#b17">(Kingma &amp; Welling, 2014)</ref> to write p(z|x)dz = p( )d , where z = f (x, ) is a deterministic function of x and the Gaussian random variable . This formulation has the important advantage that the noise term is independent of the parameters of the model, so it is easy to take gradients.</p><p>Assuming our choice of p(z|x) and r(z) allows computation of an analytic Kullback-Leibler divergence, we can put everything together to get the following objective function, which we try to minimize:</p><formula xml:id="formula_18">J IB = 1 N N n=1 E ∼p( ) [− log q(y n |f (x n , ))] + β KL [p(Z|x n ), r(Z)] .<label>(17)</label></formula><p>As in <ref type="bibr" target="#b17">Kingma &amp; Welling (2014)</ref>, this formulation allows us to directly backpropagate through a single sample of our stochastic code and ensure that our gradient is an unbiased estimate of the true expected gradient.<ref type="foot" target="#foot_3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>In this section, we present various experimental results, comparing the behavior of standard deterministic networks to stochastic neural networks trained by optimizing the VIB objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BEHAVIOR ON MNIST</head><p>We start with experiments on unmodified MNIST (i.e. no data augmentation). In order to pick a model with some "headroom" to improve, we decided to use the same architecture as in the <ref type="bibr">(Pereyra et al., 2016)</ref> paper, namely an MLP with fully connected layers of the form 784 -1024 -1024 -10, and ReLu activations. (Since we are not exploiting spatial information, this correpsonds to the "permutation invariant" version of MNIST.) The performance of this baseline is 1.38% error. <ref type="bibr">(Pereyra et al., 2016)</ref> were able to improve this to 1.17% using their regularization technique. We were able to improve this to 1.13% using our technique, as we explain below.</p><p>In our method, the stochastic encoder has the form p(z|x) = N (z|f µ e (x), f Σ e (x)), where f e is an MLP of the form 784 − 1024 − 1024 − 2K, where K is the size of the bottleneck. The first K outputs from f e encode µ, the remaining K outputs encode σ (after a softplus transform).</p><p>Model error Baseline 1.38% Dropout 1.34% Dropout <ref type="bibr">(Pereyra et al., 2016)</ref> 1.40%</p><p>Confidence Penalty 1.36% Confidence Penalty <ref type="bibr">(Pereyra et al., 2016)</ref> 1.17%</p><p>Label Smoothing 1.40% Label Smoothing <ref type="bibr">(Pereyra et al., 2016)</ref> 1.23%</p><p>VIB (β = 10 −3 ) 1.13%</p><p>Table <ref type="table">1</ref>: Test set misclassification rate on permutation-invariant MNIST using K = 256. We compare our method (VIB) to an equivalent deterministic model using various forms of regularization.</p><p>The discrepancy between our results for confidence penalty and label smoothing and the numbers reported in <ref type="bibr">(Pereyra et al., 2016)</ref> are due to slightly different hyperparameters.</p><p>The decoder is a simple logistic regression model of the form q(y|z) = S(y|f d (z)), where</p><formula xml:id="formula_19">S(a) = [exp(a c )/ C c =1 exp(a c )]</formula><p>is the softmax function, and f d (z) = W z + b maps the K dimensional latent code to the logits of the C = 10 classes. (In later sections, we consider more complex decoders, but here we wanted to show the benefits of VIB in a simple setting.) Finally, we treat r(z) as a fixed K-dimensional spherical Gaussian, r(z) = N (z|0, I).</p><p>We compare our method to the baseline MLP. We calso consider the following deterministic limit of our model, when β = 0. In this case, we obtain the following objective function:</p><formula xml:id="formula_20">J IB0 = − 1 N N n=1 E z∼N (f µ e (xn),f Σ e (xn)) [log S(y n |f d (z)]<label>(18)</label></formula><p>When β → 0, we observe the VIB optimization process tends to make f Σ e (x) → 0, so the network becomes nearly deterministic. In our experiments we also train an explicitly deterministic model that has the same form as the stochastic model, except that we just use z = f µ e (x) as the hidden encoding, and drop the Gaussian layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">HIGHER DIMENSIONAL EMBEDDING</head><p>To demonstrate that our VIB method can achieve competitive classification results, we compared against a deterministic MLP trained with various forms of regularization. We use a K = 256 dimensional bottleneck and a diagonal Gaussian for p(z|x). The networks were trained using Ten-sorFlow for 200 epochs using the Adam optimizer <ref type="bibr" target="#b16">(Kingma &amp; Ba, 2015)</ref> with a learning rate of 0.0001. Full hyperparameter details can be found in Appendix A.</p><p>The results are shown in Table <ref type="table">1</ref>. We see that we can slightly outperform other forms of regularization that have been proposed in the literature while using the same network for each. Of course, the performance varies depending on β. These results are not state of the art, nor is our main focus of our work to suggest that VIB is the best regularization method by itself, which would require much more experimentation. However, using the same architecture for each experiment and comparing to VIB as the only source of regularization suggests VIB works as a decent regularizer in and of itself. Figure <ref type="figure" target="#fig_0">1</ref>(a) plots the train and test error vs β, averaged over 5 trials (with error bars) for the case where we use a single Monte Carlo sample of z when predicting, and also for the case where we average over 12 posterior samples (i.e., we use p(y|x) = 1 S S s=1 q(y|z s ) for z s ∼ p(z|x), where S = 12). In our own investigations, a dozen samples seemed to be sufficient to capture any additional benefit the stochastic evaluations had to offer in this experiment<ref type="foot" target="#foot_4">5</ref> .</p><p>We see several interesting properties in Figure <ref type="figure" target="#fig_0">1(a)</ref>. First, we notice that the error rate shoots up once β rises above the critical value of β ∼ 10 −2 . This corresponds to a setting where the mutual information between X and Z is less than log 2 (10) bits, so the model can no longer represent the fact that there are 10 different classes. Second, we notice that, for small values of β, the test error is higher than the training error, which indicates that we are overfitting. This is because the network learns to be more deterministic, forcing σ ≈ 0, thus reducing the benefits of regularization. Third, we notice that for intermediate values of β, Monte Carlo averaging helps. Interestingly, the region with the best performance roughly corresponds to where the added benefit from stochastic averaging goes away, suggesting an avenue by which one could try to optimize β using purely statistics on the training set without a validation set. We have not extensively studied this possibility yet.</p><p>In Figure <ref type="figure" target="#fig_0">1</ref>(c), we plot the IB curve, i.e., we plot I(Z, Y ) vs I(Z, X) as we vary β. As we allow more information from the input through to the bottleneck (by lowering β), we increase the mutual information between our embedding and the label on the training set, but not necessarily on the test set, as is evident from the plot.</p><p>In Figure <ref type="figure" target="#fig_0">1</ref>(d) we plot the second term in our objective, the upper bound on the mutual information between the images X and our stochastic encoding Z, which in our case is simply the relative entropy between our encoding and the fixed isotropic unit Gaussian prior. Notice that the y-axis is a logarithmic one. This demonstrates that our best results (when β is between 10 −3 and 10 −2 ) occur where the mutual information between the stochastic encoding and the images is on the order of 10 to 100 bits. We see that for a good value of β, such as 10 −2 , we only need to store about 10 bits of information about the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">TWO DIMENSIONAL EMBEDDING</head><p>To better understand the behavior of our method, we refit our model to MNIST using a K = 2 dimensional bottleneck, but using a full covariance Gaussian. (The neural net predicts the mean and the Cholesky decomposition of the covariance matrix.) Figure <ref type="figure" target="#fig_0">1</ref>(b) shows that, not surprisingly, the classification performance is worse (note the different scaled axes), but the overall trends are the same as in the K = 256 dimensional case. The IB curve (not shown) also has a similar shape to before, except now the gap between training and testing is even larger.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> provides a visualization of what the network is doing. We plot the posteriors p(z|x) as a 2d Gaussian ellipse (representing the 95% confidence region) for 1000 images from the test set. Colors correspond to the true class labels. In the background of each plot is the entropy of the variational classifier q(y|z) evaluated at that point.  We also report the test error using a single sample, err 1 , and using 12 Monte Carlo samples, err mc . For "good" values of β, a single sample suffices.</p><p>We see several interesting properties. First, as β increases (so we pass less information through), the embedding covariances increase in relation to the distance between samples, and the classes start to overlap. Second, once β passes a critical value, the encoding "collapses", and essentially all the class information is lost. Third, there is a fair amount of uncertainty in the class preditions (q(y|z)) in the areas between the class embeddings. Fourth, for intermediate values of β (say 10 −1 in Figure <ref type="figure" target="#fig_2">2</ref>(b)), predictive performance is still good, even though there is a lot of uncertainty about where any individual image will map to in comparison to other images in the same class. This means it would be difficult for an outside agent to infer which particular instance the model is representing, a property which we will explore more in the following sections.</p><p>4.2 BEHAVIOR ON ADVERSARIAL EXAMPLES <ref type="bibr">Szegedy et al. (2013)</ref> was the first work to show that deep neural networks (and other kinds of classifiers) can be easily "fooled" into making mistakes by changing their inputs by imperceptibly small amounts. In this section, we will show how training with the VIB objective makes models significantly more robust to such adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">TYPES OF ADVERSARIES</head><p>Since the initial work by <ref type="bibr">Szegedy et al. (2013)</ref> and <ref type="bibr" target="#b31">Goodfellow et al. (2014)</ref>, many different adversaries have been proposed. Most attacks fall into three broad categories: optimization-based attacks <ref type="bibr">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b7">Carlini &amp; Wagner, 2016;</ref><ref type="bibr">Moosavi-Dezfooli et al., 2016;</ref><ref type="bibr" target="#b25">Papernot et al., 2015;</ref><ref type="bibr" target="#b28">Robinson &amp; Graham, 2015;</ref><ref type="bibr" target="#b29">Sabour et al., 2016)</ref>, which directly run an optimizer such as L-BFGS or ADAM <ref type="bibr" target="#b16">(Kingma &amp; Ba, 2015)</ref> on image pixels to find a minimal perturbation that changes the model's classification; single-step gradient-based attacks <ref type="bibr" target="#b31">(Goodfellow et al., 2014;</ref><ref type="bibr">Kurakin et al., 2016;</ref><ref type="bibr" target="#b15">Huang et al., 2015)</ref>, which choose a gradient direction of the image pixels at some loss and then take a single step in that direction; and iterative gradient-based attacks <ref type="bibr">(Kurakin et al., 2016)</ref>, Published as a conference paper at ICLR 2017 which take multiple small steps along the gradient direction of the image pixels at some loss, recomputing the gradient direction after each step. 6   Many adversaries can be formalized as either untargeted or targeted variants. An untargeted adversary can be defined as A(X, M ) → X , where A(.) is the adversarial function, X is the input image, X is the adversarial example, and M is the target model. A is considered successful if M (X) = M (X ). Recently, Moosavi-Dezfooli et al. ( <ref type="formula">2016</ref>) showed how to create a "universal" adversarial perturbation δ that can be added to any image X in order to make M (X + δ) = M (X) for a particular target model.</p><p>A targeted adversary can be defined as A(X, M, l) → X , where l is an additional target label, and A is only considered successful if M (X ) = l. 7 Targeted attacks usually require larger magnitude perturbations, since the adversary cannot just "nudge" the input across the nearest decision boundary, but instead must force it into a desired decision region.</p><p>In this work, we focus on the Fast Gradient Sign (FGS) method proposed in It is also important to have a measure of the magnitude of the adversarial perturbation. Since adversaries are defined relative to human perception, the ideal measure would explicitly correspond to how easily a human observer would notice the perturbation. In lieu of such a measure, it is common to compute the size of the perturbation using L 0 , L 1 , L 2 , and L ∞ norms <ref type="bibr">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b31">Goodfellow et al., 2014;</ref><ref type="bibr" target="#b7">Carlini &amp; Wagner, 2016;</ref><ref type="bibr" target="#b29">Sabour et al., 2016)</ref>. In particular, the L 0 norm measures the number of perturbed pixels, the L 2 norm measures the Euclidean distance between X and X , and the L ∞ norm measures the largest single change to any pixel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">EXPERIMENTAL SETUP</head><p>We used the same model architectures as in Section 4.1, using a K = 256 bottleneck. The architectures included a deterministic (base) model trained by MLE; a deterministic model trained with dropout (the dropout rate was chosen on the validation set); and a stochastic model trained with VIB for various values of β.</p><p>For the VIB models, we use 12 posterior samples of Z to compute the class label distribution p(y|x). This helps ensure that the adversaries can get a consistent gradient when constructing the perturbation, and that they can get a consistent evaluation when checking if the perturbation was successful 6 There are also other adversaries that don't fall as cleanly into those categories, such as "fooling images" from <ref type="bibr">Nguyen et al. (2014)</ref>, which remove the human perceptual constraint, generating regular geometric patterns or noise patterns that networks confidently classify as natural images; and the idea of generating adversaries by stochastic search for images near the decision boundary of multiple networks from <ref type="bibr" target="#b2">Baluja et al. (2015)</ref>.</p><p>7 <ref type="bibr" target="#b29">Sabour et al. (2016)</ref> proposes a variant of the targeted attack, A(XS, M, XT , k) → X S , where XS is the source image, XT is a target image, and k is a target layer in the model M . A produces X S by minimizing the difference in activations of M at layer k between XT and X S . The end result of this attack for a classification network is still that M (X S ) yields a target label implicitly specified by XT in a successful attack. 8 Carlini &amp; Wagner (2016) shared their code with us, which allowed us to perform the attack with exactly the same parameters they used for their paper, including the maximum number of iterations and maximum C value (see their paper for details).</p><p>(i.e., it reduces the chance that the adversary "gets lucky" in its perturbation due to an untypical sample). We also ran the VIB models in "mean mode", where the σs are forced to be 0. This had no noticeable impact on the results, so all reported results are for stochastic evaluation with 12 samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">MNIST RESULTS AND DISCUSSION</head><p>We selected the first 10 zeros in the MNIST test set, and use the L 2 optimization adversary of <ref type="bibr" target="#b7">Carlini &amp; Wagner (2016)</ref> to try to perturb those zeros into ones.<ref type="foot" target="#foot_5">9</ref> Some sample results are shown in Figure <ref type="figure" target="#fig_4">3</ref>. We see that the deterministic models are easily fooled by making small perturbations, but for the VIB models with reasonably large β, the adversary often fails to find an attack (indicated by the green borders) within the permitted number of iterations. Furthermore, when an attack is succesful, it needs to be much larger for the VIB models. To quantify this, Figure <ref type="figure" target="#fig_5">4</ref> plots the magnitude of the perturbation (relative to that of the deterministic and dropout models) needed for a successful attack as a function of β. As β increases, the L 0 norm of the perturbation decreases, but both L 2 and L ∞ norms increase, indicating that the adversary is being forced to put larger modifications into fewer pixels while searching for an adversarial perturbation.</p><p>Figure <ref type="figure" target="#fig_6">5</ref> plots the accuracy on FGS adversarial examples of the first 1000 images from the MNIST test set as a function of β. Each point in the plot corresponds to 3 separate executions of three different models trained with the same value of β. All models tested achieve over 98.4% accuracy on the unperturbed MNIST test set, so there is no appreciable measurement distortion due to underlying model accuracy. We generated both untargeted and targeted adversarial examples for Figure <ref type="figure" target="#fig_3">6</ref>. For targeting, we generate a random target label different from the source label in order to avoid biasing the results with unevenly explored source/target pairs. We see that for a reasonably broad range of β values, the VIB models have significantly better accuracy on the adversarial examples than the deterministic models, which have an accuracy of 0% (the L 2 optimization attack is very effective on traditional model architectures).</p><p>Figure <ref type="figure" target="#fig_3">6</ref> also reveals a surprising level of adversarial robustness even when β → 0. This can be explained by the theoretical framework of <ref type="bibr" target="#b11">Fawzi et al. (2016)</ref>. Their work proves that quadratic classifiers (e.g., x T Ax, symmetric A) have a greater capacity for adversarial robustness than linear classifiers. As we show in Appendix C, our Gaussian/softmax encoder/decoder is approximately quadratic for all β &lt; ∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">IMAGENET RESULTS AND DISCUSSION</head><p>VIB improved classification accuracy and adversarial robustness for toy datasets like MNIST. We now investigate if VIB offers similar advantages for ImageNet, a more challenging natural image classification. Recall that ImageNet has approximately 1M images spanning 1K classes. We preprocess images such that they are 299x299 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>We make use of publicly available, pretrained checkpoints<ref type="foot" target="#foot_6">10</ref> of Inception Resnet V2 <ref type="bibr" target="#b32">(Szegedy et al., 2016)</ref> on ImageNet <ref type="bibr" target="#b10">(Deng et al., 2009)</ref>. The checkpoint obtains 80.4% classification accuracy on the ImageNet validation set. Using the checkpoint, we transformed the original training set by applying the pretrained network to each image and extracting the representation at the penultimate layer. This new image representation has 1536 dimensions. The higher layers of the network continue to classify this representation with 80.4% accuracy; conditioned on this extraction the classification      In the case of the deterministic baseline and original Inception ResNet V2 network, the perturbations are hardly noticable in the perturbed images, but in many instances, the perturbations for the VIB network can be percieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adversarial Robustness</head><p>We next show that the VIB-trained network improves resistance to adversarial attack.</p><p>We focus on the Carlini targeted L 2 attack (see Section 4.2.1). We show results for the VIB-trained network and a deterministic baseline (both on top of precomputed features), as well as for the original pretrained Inception ResNet V2 network itself. The VIB network is more robust to the targeted L 2 optimization attack in both magnitude of perturbation and frequency of successful attack.</p><p>Figure <ref type="figure" target="#fig_8">7</ref> shows some example images which were all misclassified as "soccer balls" by the deterministic models; by contrast, with the VIB model, only 17 out of 30 of the attacks succeeded in being mislabeled as the target label. <ref type="foot" target="#foot_8">11</ref> We find that the VIB model can resist about 43.3% of the attacks, but the deterministic models always fail (i.e., always misclassify into the targeted label).</p><p>Figure <ref type="figure">8</ref> shows the absolute pixel differences between the perturbed and unperturbed images for the examples in Figure <ref type="figure" target="#fig_8">7</ref>. We see that the VIB network requires much larger perturbations in order to fool the classifier, as quantified in Turning our attention to the second term, note that:</p><formula xml:id="formula_21">p(z|i) = dx p(z|x)p(x|i) = dx p(z|x)δ(x − x i ) = p(z|x i ),<label>(25)</label></formula><p>and that we will take p(i) = 1 N . So that we can bound our second term from above</p><formula xml:id="formula_22">I(Z, i) = i dz p(z|i)p(i) log p(z|i) p(z) (26) = 1 N i dz p(z|x i ) log p(z|x i ) p(z) (27) ≤ 1 N i dz p(z|x i ) log p(z|x i ) r(z) ,<label>(28)</label></formula><p>Where we have replaced the intractable marginal p(z) with a variational marginal r(z).</p><p>Putting these two bounds together we have that our unsupervised information bottleneck objective takes the form</p><formula xml:id="formula_23">I(Z, X) − βI(Z, i) ≤ dx p(x) dz p(z|x) log q(x|z) − β 1 N i KL[p(Z|x i ), r(Z)]. (<label>29</label></formula><formula xml:id="formula_24">)</formula><p>And this takes the form of a variational autoencoder <ref type="bibr" target="#b17">(Kingma &amp; Welling, 2014)</ref>, except with the second KL divergence term having an arbitrary weight β.</p><p>It is interesting that while this objective takes the same mathematical form as that of a Variational Autoencoder, the interpretation of the objective is very different. In the VAE, the model starts life as a generative model with a defined prior p(z) and stochastic decoder p(x|z) as part of the model, and the encoder q(z|x) is created to serve as a variational approximation to the true posterior p(z|x) = p(x|z)p(z)/p(x). In the VIB approach, the model is originally just the stochastic encoder p(z|x), and the decoder q(x|z) is the variational approximation to the true p(x|z) = p(z|x)p(x)/p(z) and r(z) is the variational approximation to the marginal p(z) = dx p(x)p(z|x). This difference in interpretation makes natural suggestions for novel directions for improvement.</p><p>This precise setup, albeit with a different motivation was recently explored in <ref type="bibr">Higgins et al. (2016)</ref>, where they demonstrated that by changing the weight of the variational autoencoders regularization term, there were able to achieve latent representations that were more capable when it came ot zeroshot learning and understanding "objectness". In that work, they motivated their choice to change the relative weightings of the terms in the objective by appealing to notions in neuroscience. Here we demonstrate that appealing to the information bottleneck objective gives a principled motivation and could open the door to better understanding the optimal choice of β and more tools for accessing the importance and tradeoff of both terms.</p><p>Beyond the connection to existing variational autoencoder techniques, we note that the unsupervised information bottleneck objective suggests new directions to explore, including targetting the exact marginal p(z) in the regularization term, as well as the opportunity to explore tighter bounds on the first I(Z, X) term that may not require explicit variational reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C QUADRATIC BOUNDS FOR STOCHASTIC LOGISTIC REGRESSION DECODER</head><p>Consider the special case when the bottleneck Z is a multivariate Normal, i.e., z|x ∼ N (µ x , Σ x ) where Σ x is a K × K positive definite matrix. The parameters µ x , Σ x can be constructed from a deep neural network, e.g., µ x = γ 1:K (x) chol(Σ x ) = diag(log(1 + exp(γ K+1:2K ))) + subtril(γ 2K+1:K(K+3)/2 ), where γ(x) ∈ R K(K+3)/2 is the network output of input x.</p><p>Suppose that the prediction is a categorical distribution computed as S(W z) where W is a C × K weight matrix and log S(x) = x − lse(x) is the log-soft-max function with lse(x) = log K k=1 exp(x k ) being the log-sum-exp function. This setup (which is identical to our experiments) induces a classifier which is bounded by a quadratic function, which is interesting because the theoretical framework <ref type="bibr" target="#b11">Fawzi et al. (2016)</ref> proves that quadratic classifiers have greater capacity for adversarial robustness than linear functions.</p><p>We now derive an approximate bound using second order Taylor series expansion (TSE). The bound can be made proper via <ref type="bibr" target="#b6">Browne &amp; McNicholas (2015)</ref>. However, using the TSE is sufficient to sketch the derivation.</p><p>Jensen's inequality implies that the negative log-likelihood soft-max is upper bounded by:</p><formula xml:id="formula_25">− log E [S(W Z)|µ x , Σ x ] ≤ − E [log S(W Z)|µ x , Σ x ] = −W µ x + E [lse(W Z)|µ x , Σ x ] = −W µ x + E [lse(Z)|W µ x , W Σ x ] .</formula><p>The second order Taylor series expansion (TSE) of lse is given by, lse(x + δ) ≈ lse(x) + δ T S(x) + 1 2 δ T diag(S(x)) − S(x)S(x) T δ.</p><p>Taking the expectation of the TSE at the mean yields, Putting this altogether, we conclude,</p><formula xml:id="formula_26">E [S(W Z)|µ x , Σ x ] S(W µ x ) exp − 1 2 S(W µ x ) T W Σ x W T S(W µ x ) + 1 2 S(W µ x ) T W Σ x W T S(W µ x ) .</formula><p>As indicated, rather than approximate the lse via TSE, we can make a sharp, quadratic upper bound via <ref type="bibr" target="#b6">Browne &amp; McNicholas (2015)</ref>. However this merely changes the S(W µ x ) scaling in the exponential; the result is still log-quadratic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Results of VIB model on MNIST. (a) Error rate vs β for K = 256 on train and test set. "1 shot eval" means a single posterior sample of z, "avg eval" means 12 Monte Carlo samples. The spike in the error rate at β ∼ 10 −2 corresponds to a model that is too highly regularized. Plotted values are the average over 5 independent training runs at each β. Error bars show the standard deviation in the results. (b) Same as (a), but for K = 2. Performance is much worse, since we pass through a very narrow bottleneck. (c) I(Z, Y) vs I(Z, X) as we vary β for K = 256. We see that increasing I(Z, X) helps training set performance, but can result in overfitting. (d) I(Z, X) vs β for K = 256. We see that for a good value of β, such as 10 −2 , we only need to store about 10 bits of information about the input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>β = 10 −3 , errmc = 3.18%, err1 = β = 10 −1 , errmc = 3.44%, err1 = β = 10 0 , errmc = 33.82%, err1 = 62.81%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Visualizing embeddings of 1000 test images in two dimensions. We plot the 95% confidence interval of the Gaussian embedding p(z|x) = N (µ, Σ) as an ellipse. The images are colored according to their true class label. The background greyscale image denotes the entropy of the variational classifier evaluated at each two dimensional location. As β becomes larger, we forget more about the input and the embeddings start to overlap to such a degree that the classes become indistinguishable. We also report the test error using a single sample, err 1 , and using 12 Monte Carlo samples, err mc . For "good" values of β, a single sample suffices.</figDesc><graphic url="image-1.png" coords="7,140.16,182.34,92.48,92.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6</head><label>6</label><figDesc>Figure 6 plots the accuracy on L 2 optimization adversarial examples of the first 1000 images from the MNIST test set as a function of β. The same sets of three models per β were tested three times, as with the FGS adversarial examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The adversary is trying to force each 0 to be classified as a 1. Successful attacks have a red background. Unsuccessful attacks have a green background. In the case that the label is changed to an incorrect label different from the target label (i.e., the classifier outputs something other than 0 or 1), the background is purple. The first column is the original image. The second column is adversarial examples targeting our deterministic baseline model. The third column is adversarial examples targeting our dropout model. The remaining columns are adversarial examples targeting our VIB models for different β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: (a) Relative magnitude of the adversarial perturbation, measured using L 0 , L 2 , and L ∞ norms, for the images in Figure3as a function of β. (We normalize all values by the corresponding norm of the perturbation against the base model.) As β increases, L 0 decreases, but both L 2 and L ∞ increase, indicating that the adversary is being forced to put larger modifications into fewer pixels while searching for an adversarial perturbation. (b) Same as (a), but with the dropout model as the baseline. Dropout is more robust to the adversarial perturbations than the base deterministic model, but still performs much worse than the VIB model as β increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Classification accuracy of VIB classifiers, divided by accuracy of baseline classifiers, on FGS-generated adversarial examples as a function of β. Higher is better, and the baseline is always at 1.0. For the FGS adversarial examples, when β = 0 (not shown), the VIB model's performance is almost identical to when β = 10 −8 . (a) FGS accuracy normalized by the base deterministic model performance. The base deterministic model's accuracy on the adversarial examples ranges from about 1% when = 0.5 to about 5% when = 0.35. (b) Same as (a), but with the dropout model as the baseline.The dropout model is more robust than the base model, but less robust than VIB, particularly for stronger adversaries (i.e., larger values of ). The dropout model's accuracy on the adversarial examples ranges from about 5% when = 0.5 to about 16% when = 0.35. As in the other results, relative performance is more dramatic as β increases, which seems to indicate that the VIB models are learning to ignore more of the perturbations caused by the FGS method, even though they were not trained on any adversarial examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Classification accuracy (from 0 to 1) on L 2 adversarial examples (of all classes) as a function of β. The blue line is for targeted attacks, and the green line is for untargeted attacks (which are easier to resist). In this case, β = 10 −11 has performance indistinguishable from β = 0. The deterministic model and dropout model both have a classification accuracy of 0% in both the targeted and untargeted attack scenarios, indicated by the horizontal red dashed line at the bottom of the plot. This is the same accuracy on adversarial examples from this adversary reported in<ref type="bibr" target="#b7">Carlini &amp; Wagner (2016)</ref> on a convolutional network trained on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The results of our ImageNet targeted L 2 optimization attack. In all cases we target a new label of 222 ("soccer ball"). Figure (a) shows the 30 images from the first 40 images in the ImageNet validation set that the VIB network classifies correctly. The class label is shown in green on each image. The predicted label and targeted label are shown in red. Figure (b) shows adversarial examples of the same images generated by attacking our VIB network with β = 0.01. While all of the attacks change the classification of the image, in 13 out of 30 examples the attack fails to hit the intended target class ("soccer ball"). Pink crosses denote cases where the attack failed to force the model to misclassify the image as a soccer ball.Figure (c) shows the same result but for our deterministic baseline operating on the whitened precomputed features. The attack always succceeds. Figure (d) is the same but for the original full Inception ResNet V2 network without modification. The attack always succceeds. There are slight variations in the set of adversarial examples shown for each network because we limited the adversarial search to correctly classified images.In the case of the deterministic baseline and original Inception ResNet V2 network, the perturbations are hardly noticable in the perturbed images, but in many instances, the perturbations for the VIB network can be percieved.</figDesc><graphic url="image-36.png" coords="12,121.82,310.72,178.19,285.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>E</head><label></label><figDesc>N (0,W ΣxW T ) [ lse(W µ x + δ)] ≈ lse(W µ x ) + E N (0,W ΣxW T ) [δ T ]S(W µ x )+ + 1 2 E N (0,W ΣxW T ) [δ T diag(S(W µ x )) − S(W µ x )S(W µ x ) T δ] = lse(W µ x ) + 1 2 tr(W Σ x W T diag(S(W µ x )) − S(W µ x )S(W µ x ) T ) = lse(W µ x ) + 1 2 tr(W Σ x W T diag(S(W µ x ))) − 1 2 S(W µ x ) T W Σ x W T S(W µ x ) = lse(W µ x ) + 1 2 S(W µ x ) T W Σ x W T S(W µ x ) − 1 2 S(W µ x ) T W Σ x W T S(W µ x )The second-moment was calculated by noting, E[X T BX] = E tr(XX T B) = tr(E[XX T ]B) = tr(ΣB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-38.png" coords="13,117.90,81.86,376.20,200.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b31">Goodfellow et al. (2014)</ref> and the L 2 optimization method proposed in<ref type="bibr" target="#b7">Carlini &amp; Wagner (2016)</ref>. FGS is a standard baseline attack that takes a single step in the gradient direction to generate the adversarial example. As originally described, FGS generates untargeted adversarial examples. On MNIST,<ref type="bibr" target="#b31">Goodfellow et al. (2014)</ref> reported that FGS could generate adversarial examples that fooled a maxout network approximately 90% of the time with = 0.25, where is the magnitude of the perturbation at each pixel. The L 2 optimization method has been shown to generate adversarial examples with smaller perturbations than any other method published to date, which were capable of fooling the target network 100% of the time. We consider both targeted attacks and untargeted attacks for the L 2 optimization method.</figDesc><table /><note>84.2.2 ADVERSARIAL ROBUSTNESSThere are multiple definitions of adversarial robustness in the literature. The most basic, which we shall use, is accuracy on adversarially perturbed versions of the test set, called adversarial examples.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table><row><cell>Metric Determ IRv2 VIB(0.01) Sucessful target 1.0 1.0 0.567 L 2 6.45 14.43 43.27 L ∞ 0.18 0.44 0.92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results showing how the different Inception Resnet V2-based architectures (described in Section 4.2.5) respond to targeted L 2 adversarial examples. Determ is the deterministic architecture, IRv2 is the unmodified Inception Resnet V2 architecture, and VIB(0.01) is the VIB architecture with β = 0.01. Successful target is the fraction of adversarial examples that caused the architecture to classify as the target class (soccer ball). Lower is better. L 2 and L ∞ are the average L distances between the original images and the adversarial examples. Larger values mean the adversary had to make a larger perturbation to change the class.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In this work, X, Y, Z are random variables, x, y, z and x, y, z are instances of random variables, and F (•; θ) and f (•; θ) are functionals or functions parameterized by θ.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Note that in the present discussion, Y is the ground truth label which is independent of our parameters so p(y|θ) = p(y).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Note that, in our notation, large β results in a highly compressed representation. In some works, the IB principle is formulated as the minimization of I(Z, X) − βI(Z, Y ), in which case large β corresponds to high mutual information between Z and Y , and hence low compression.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">Even if our choice of encoding distribution and variational prior do not admit an analytic KL, we could similarly reparameterize through a sample of the divergence<ref type="bibr" target="#b17">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b5">Blundell et al., 2015)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">A dozen samples wasn't chosen for any particular reason, except the old addage that a dozen samples are sufficient, as mirrored in David MacKay's book<ref type="bibr" target="#b19">(MacKay, 2003)</ref>. They proved sufficient in this case.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5">We chose this pair of labels since intuitively zeros and ones are the digits that are least similar in terms of human perception, so if the adversary can change a zero into a one without much human-noticeable perturbation, it is unlikely that the model has learned a representation similar to what humans learn.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6">  10  Available at the Tensorflow Models repository in the Slim directory: https://github.com/ tensorflow/models/tree/master/slim</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_7">FUTURE DIRECTIONSThere are many possible directions for future work, including: putting the VIB objective at multiple or every layer of a network; testing on real images; using richer parametric marginal approximations, rather than assuming r(z) = N (0, I); exploring the connections to differential privacy (see e.g.,<ref type="bibr" target="#b35">Wang et al. (2016a)</ref>;<ref type="bibr" target="#b9">Cuff &amp; Yu (2016)</ref>); and investigating open universe classification problems (see e.g.,<ref type="bibr" target="#b3">Bendale &amp; Boult (2015)</ref>). In addition, we would like to explore applications to sequence prediction, where X denotes the past of the sequence and Y the future, while Z is the current representation of the network. This form of the information bottleneck is known as predictive information<ref type="bibr" target="#b4">(Bialek et al., 2001;</ref><ref type="bibr" target="#b24">Palmer et al., 2015)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8">The attacks still often cause the VIB model to misclassify the image, but not to the targeted label. This is a form of "partial" robustness, in that an attacker will have a harder time hitting the target class, but can still disrupt correct function of the network.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">8</ref>: Shown are the absolute differences between the original and final perturbed images for all three networks. The left block shows the perturbations created while targeting the VIB network. The middle block shows the perturbations needed for the deterministic baseline using precomputed whitened features. The right block shows the perturbations created for the unmodified Inception ResNet V2 network. The contrast has been increased by the same amount in all three columns to emphasize the difference in the magnitude of the perturbations. The VIB network required much larger perturbations to confuse the classifier, and even then did not achieve the targeted class in 13 of those cases. model is simply logistic regression. To further speed training, we whitened the 1536 dimensional representation.</p><p>Under this transformation, the experiment regime is identical to the permutation invariant MNIST task. We therefore used a similar model architecture. Inputs are passed through two fully connected layers, each with 1024 units. Next, data is fed to a stochastic encoding layer; this layer is characterized by a spherical Gaussian with 1024 learned means and standard deviations. The output of the stochastic layer is fed to the variational classifier-itself a logistic regression, for simplicity. All other hyperparameters and training choices are identical to those used in MNIST, more details in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification</head><p>We see the same favorable VIB classification performance in ImageNet as in MNIST. By varying β, the estimated mutual information between encoding and image (I(Z, X)) varies as well. At large values of β accuracy suffers, but at intermediate values we obtain improved performance over both a deterministic baseline and a β = 0 regime. In all cases our accuracy is somewhat lower than the original 80.4% accuracy. This may be a consequence of inadequate training time or suboptimal hyperparameters.</p><p>Overall the best accuracy we achieved was using β = 0.01. Under this setting we saw an accuracy of 80.12%-nearly the same as the state-of-the-art unmodified network-but with substantially smaller information footprint, only I(X, Z) ∼ 45 bits. This is a surprisingly small amount of information; β = 0 implies over 10,000 bits yet only reaches an accuracy of 78.87%. The deterministic baseline, which was the same network, but without the VIB loss and a 1024 fully connected linear layer instead of the stochastic embedding similarly only achieved 78.75% accuracy. We stress that regressions from the achievable 80.4% are likely due to suboptimal hyperparameters settings or inadequate training.</p><p>Considering a continuum of β and a deterministic baseline, the best classification accuracy was achieved with a β = 0.01 ∈ (0, 1). In other words, VIB offered accuracy benefit yet using a mere ∼ 45 bits of information from each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A HYPERPARAMETERS AND ARCHITECTURE DETAILS FOR EXPERIMENTS</head><p>All of the networks for this paper were trained using TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. All weights were initialized using the default TensorFlow Xavier initialization scheme <ref type="bibr" target="#b12">(Glorot &amp; Bengio, 2010)</ref> using the averaging fan scaling factor on uniform noise. All biases were initialized to zero. The Adam optimizer <ref type="bibr" target="#b16">(Kingma &amp; Ba, 2015)</ref> was used with initial learning rate of 10 −4 , (β 1 = 0.5, β 2 = 0.999) and exponential decay, decaying the learning rate by a factor of 0.97 every 2 epochs. The networks were all trained for 200 epochs total. For the MNIST experiments, a batch size of 100 was used, and the full 60,000 training and validation set was used for training, and the 10,000 test images for test results. The input images were scaled to have values between -1 and 1 before fed to the network.</p><p>All runs maintained an exponential weighted average of the parameters during the training run; these averaged parameters were used at test time. This is in the style of Polyak averaging <ref type="bibr" target="#b27">Polyak &amp; Juditsky (1992)</ref>, with a decay constant of 0.999. Our estimate of mutual informations were measured in bits. For the VIB experiments in all sections, no other form of regularization was used.</p><p>For the 256 dimensional gaussian embeddings of Section 4.1.1, a linear layer of size 512 was used to create the 256 mean values and standard deviations for the embedding. The standard deviations were made to be positive by a softplus transformation with a bias of -5.0 to have them initially be small.</p><p>For the 1024 dimensional Imagenet embeddings of Section 4.2.5, a sigma bias of 0.57 was used to keep the initial standard deviations near 1 originally, and a batch size of 200 was used.</p><p>For the 2 dimensional gaussian embeddings of Section 4.1.2, a linear layer was used with 2+4 = 6 outputs, the first two of which were used for the means, and the other 4 were reshaped to a 2 × 2 matrix, the center was transformed according to a softplus with a bias of -5.0, and the off diagonal components were multiplied by 10 −2 , while the upper triangular element was dropped to form the Cholesky decomposition of the covariance matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B CONNECTION TO VARIATIONAL AUTOENCODERS</head><p>We can also consider unsupervised versions of the information bottleneck objective. Consider the objective: max I(Z, X) − βI(Z, i), (20) similar to the information theoretic objective for clustering introduced in <ref type="bibr" target="#b30">Slonim et al. (2005)</ref>.</p><p>Here the aim is to take our data X and maximize the mutual information contained in some encoding Z, while restricting how much information we allow our representation to contain about the identity of each data element in our sample (i). We will form a bound much like we did in the main text. For the first term, we form a variational decoder q(x|z) and take a bound: </p><p>Here we have dropped the entropy in our data H(X) because it is out of our control and we have used the nonnegativity of the Kullbach-Leibler divergence to replace our intractable p(x|z) with a variational decoder q(x|z).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The IM algorithm: a variational approach to information maximization</title>
		<author>
			<persName><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Agakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The virtues of peer pressure: A simple method for discovering high-value mistakes</title>
		<author>
			<persName><forename type="first">Shumeet</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. Computer Analysis of Images and Patterns</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards open world recognition</title>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predictability, complexity, and learning</title>
		<author>
			<persName><forename type="first">William</forename><surname>Bialek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Nemenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2409" to="2463" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural networks</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multivariate sharp quadratic bounds via Σ-strong convexity and the fenchel connection</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Mcnicholas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relevant sparse codes with variational information bottleneck</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<editor>
			<persName><forename type="first">Matthew</forename><surname>Chalk</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Olivier</forename><surname>Marre</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gasper</forename><surname>Tkacik</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>Towards evaluating the robustness of neural networks</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Information bottleneck for gaussian variables</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Globersonand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">165188</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Differential privacy as a mutual information constraint</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Cuff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanqing</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Computer and Communications Security (CCS)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robustness of classifiers: from adversarial to random noise</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AI/Statistics</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">beta-VAE: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=Sy2fzU9gl" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning with a strong adversary</title>
		<author>
			<persName><forename type="first">Ruitong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
		<idno>CoRR, abs/1511.03034</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=S1OufnIlx" />
	</analytic>
	<monogr>
		<title level="m">Alexey Kurakin, Ian Goodfellow, and Samy Bengio</title>
				<imprint>
			<date type="published" when="2014">2014. 2017</date>
		</imprint>
	</monogr>
	<note>ICLR Workshop</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The variational fair autoencoder</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.00830" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Information theory, inference and learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Variational information maximisation for intrinsically motivated reinforcement learning</title>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2125" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arxiv</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.1897" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predictive information in a sensory population</title>
		<author>
			<persName><forename type="first">Stephanie</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Marre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="6908" to="6913" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st IEEE European Symposium on Security and Privacy</title>
				<meeting>the 1st IEEE European Symposium on Security and Privacy</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output predictions</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tuckery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf?id=HyhbYrGYe" />
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anatoli</forename><forename type="middle">B</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Confusing deep convolution networks by relabelling</title>
		<author>
			<persName><forename type="first">Leigh</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<idno>1510.06925</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adversarial manipulation of deep representations</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanshuai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Information-based clustering</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurinder</forename><surname>Singh Atwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gašper</forename><surname>Tkačik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Bialek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>PNAS</publisher>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="18297" to="18302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.6199" />
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inception-v4, inceptionresnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Information Theory Workshop</title>
				<imprint>
			<date type="published" when="2015">2016. April 2015a</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deep learning and the information bottleneck principle</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The information bottleneck method</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Biale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 37th annual Allerton Conf. on Communication, Control, and Computing</title>
				<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="368" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory Workshop (ITW)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015. 2015b</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the relation between identifiability, differential privacy and Mutual-Information privacy</title>
		<author>
			<persName><forename type="first">Weina</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junshan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="5018" to="5029" />
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep variational canonical correlation analysis</title>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno>arXiv [cs.LG]</idno>
		<ptr target="https://arxiv.org/abs/1610.03454" />
		<imprint>
			<date type="published" when="2016-10-11">11 October 2016b</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
