<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-level Hyperedge Distillation for Social Linking Prediction on Sparsely Observed Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangguo</forename><surname>Sun</surname></persName>
							<email>sunxiangguo@seu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
							<email>h.yin1@uq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
							<email>bliu@seu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
							<email>hongxu.chen@uts.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Qing</forename><surname>Meng</surname></persName>
							<email>qmeng@seu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Wang</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiuxin</forename><surname>Cao</surname></persName>
							<email>jx.cao@seu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Southeast University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Southeast University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Southeast University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-level Hyperedge Distillation for Social Linking Prediction on Sparsely Observed Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449912</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>hypergraph learning</term>
					<term>sparsely observed networks</term>
					<term>linking prediction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Social linking prediction is one of the most fundamental problems in online social networks and has attracted researchers' persistent attention. Most of the existing works predict unobserved links using graph neural networks (GNNs) to learn node embeddings upon pair-wise relations. Despite promising results given enough observed links, these models are still challenging to achieve heartstirring performance when observed links are extremely limited. The main reason is that they only focus on the smoothness of node representations on pair-wise relations. Unfortunately, this assumption may fall when the networks do not have enough observed links to support it. To this end, we go beyond pair-wise relations and propose a new and novel framework using hypergraph neural networks with multi-level hyperedge distillation strategies. To break through the limitations of sparsely observed links, we introduce the hypergraph to uncover higher-level relations, which is exceptionally crucial to deduce unobserved links. A hypergraph allows one edge to connect multiple nodes, making it easier to learn better higher-level relations for link prediction. To overcome the restrictions of manually designed hypergraphs, which is constant in most hypergraph researches, we propose a new method to learn high-quality hyperedges using three novel hyperedges distillation strategies automatically. The generated hyperedges are hierarchical and follow the power-law distribution, which can significantly improve the link prediction performance. To predict unobserved links, we present a novel hypergraph neural networks named HNN. HNN takes the multi-level hypergraphs as input and makes the node embeddings smooth on hyperedges instead of pair-wise links only. Extensive evaluations on four real-world datasets demonstrate our model's superior performance over state-of-the-art baselines, especially when the observed links are extremely reduced.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the popularisation of online networks such as the world wide web, citation networks, and social platforms, people transfer their social life mostly online, thus resulting in considerable heterogeneous social interactions. Predicting these social relations plays a fundamental role in analyzing users' online behaviors and social phenomena <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42]</ref>. It has been widely used in recommendation systems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>, anomaly detection <ref type="bibr" target="#b18">[19]</ref>, and sentiment analysis <ref type="bibr" target="#b31">[32]</ref>. Formally, social linking prediction aims to distinguish whether a pair of nodes in a network has a specific type of link or not. Although researchers have paid much attention <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40]</ref>, there is still significant potential for improvement, especially when the observed links are extremely limited, which is universal in real-world situations <ref type="bibr" target="#b44">[45]</ref>.</p><p>Previous studies usually leverage network sampling strategies <ref type="bibr" target="#b25">[26]</ref> to make node embeddings smooth on pair-wise links. For example, Grover et al. <ref type="bibr" target="#b11">[12]</ref> propose a bias random walk framework and then use the Skip-gram model to maximize the probability of target node given specific contents. Many variants such as metapath2vec <ref type="bibr" target="#b8">[9]</ref>, HHNE <ref type="bibr" target="#b36">[37]</ref>, and HeteSpaceyWalk <ref type="bibr" target="#b16">[17]</ref> come out recently to handle heterogeneous relations following this idea. However, converting graphs to linear paths limits these models' further improvement because these paths can not reconstruct the original graphs without missing information. To this end, graph neural networks (GNNs) <ref type="bibr" target="#b38">[39]</ref> have been recently introduced to this subject because they have effectively addressed non-linear relations in graphs. Specifically, <ref type="bibr">Wang et al.</ref> propose HAN <ref type="bibr" target="#b34">[35]</ref>, which integrates node-level attention and semantic-level attention to learn node embeddings from heterogeneous graphs. Hu et al. <ref type="bibr" target="#b17">[18]</ref> leverage generative adversarial networks (GANs) to learn node distribution to generate better negative samples for node embedding. Fu et al. <ref type="bibr" target="#b10">[11]</ref> optimise GNN-based methods with intra-metapath aggregation and inter-metapath aggregation and make the model flexible. Although performance has been further improved, these models heavily rely on the network connectivity, which means the node smoothness may be destroyed when the observed links are extremely limited.</p><p>To improve linking prediction performance on sparsely observed networks, we need to explore higher-level relations to support underlying links further. Most of the existing works learn higher-level relations from ordinary local proximities. For example, Chen et al. <ref type="bibr" target="#b4">[5]</ref> put forward PME, which integrates first-order and second-order proximities via metric learning. They first project node features to different semantic spaces, and then tighten each pair of nodes on pair-wise links. Similarly, Lu et al. <ref type="bibr" target="#b43">[44]</ref> present RHINE, and they utilise different models to exploit affiliation and interaction relations. Wang et al. <ref type="bibr" target="#b35">[36]</ref> use their proposed model StHNE to learn meta-path based first-order and second-order proximities, and then optimise the distance of similar nodes with spectral theory. However, all these works learn high-order relations via minimising the distance of similar nodes and maximising dissimilar nodes on observed pair-wise links, which is far from expressive on sparsely observed networks. Taking Figure <ref type="figure" target="#fig_0">1</ref> as an example, when we predict links only based on pair-wise relations, the model may easily treats unobserved positive links as a negative node pair, making this pair of nodes weakly correlated and the node embeddings are under-smooth.</p><p>Unlike typical graph models, which only focus on pair-wise relations, hypergraphs <ref type="bibr" target="#b30">[31]</ref> can capture and preserve more diverse, complicated, and higher-level semantics. Hypergraphs allow one edge (a.k.a hyperedge) to connect multiple nodes, which are perfectly suitable for heterogeneous networks (HINs) and naturally promising to improve the link prediction performance on HINs. However, in many graph-structured datasets, hyperedges are not always given. To apply hypergraph models, most of the existing works have to assign hyperedges manually. For instance, Feng et al. <ref type="bibr" target="#b9">[10]</ref> design a hypergraph neural network via analogy with normal graphs. They build hyperedges by calculating the distance between nodes features. Zhang et al. <ref type="bibr" target="#b46">[47]</ref> treat hyperedges as node feature tuples and use the attention mechanism to fuse information within a hyperedge. Jin et al. <ref type="bibr" target="#b20">[21]</ref> propose a convolutional manifold network guided by manually designed hyperedges. They construct their hypergraphs with k-nearest nodes. Chen et al. <ref type="bibr" target="#b3">[4]</ref> put forward MGCN, and they discuss the impact of different manually created hypergraphs. Jiang et al. <ref type="bibr" target="#b19">[20]</ref> and Zhang et al. <ref type="bibr" target="#b47">[48]</ref> study dynamic networks using hypergraph learning. They treat hyperedges as clusters and network neighbours. Although these works achieve satisfactory results on various downstream tasks, they still suffer from the limitation of toneless hyperedges, leading to over-smooth expressiveness for pair-wise linking prediction. As shown in the right of Figure <ref type="figure" target="#fig_0">1</ref>, nodes in a toneless hyperedge can be also converted as a complete graph, where each pair of nodes are tightened no matter their real connections. This may easily make node embeddings over-smooth <ref type="bibr" target="#b7">[8]</ref>, leading to the opposite corner compared with the models only based on pair-wise relations.</p><p>From the above discussion, we have realised that the vital barrier for linking prediction on a sparsely observed network is how to learn multi-level hyperedges automatically from data. To fill the research gap, we need to solve the following challenges:</p><p>• Challenge 1: How to learn better higher-level relations on sparsely observed networks. Traditional methods mostly rely on pair-wise links heavily, and they usually follow the idea that each pair of nodes should be closer on an observed link but keep away from each other if there is no observed link connecting them. However, when the observed links are dramatically reduced, the smoothness can not be guaranteed anymore, limiting the performance improvement. • Challenge 2: How to learn multi-level hyperedges automatically from data. Most of related works use manually designed hyperedges. However, these hyperedges are far from hierarchical and flexible, resulting in over-smooth problems for linking prediction. To solve this problem, we need to break away from constant hyperedges and generate multi-level hyperedges automatically from data. • Challenge 3: How to learn better node embeddings for linking inference. Traditional works usually use matrix decomposition and GNNs on typical graphs, which can not be directly applied to hypergraphs. To integrate hyperedges information, we need to design a uniform model from both pair-wise links and hyperedges. To address challenge 1, we go beyond limited observed pair-wise links and use the hypergraph to learn latent higher-level information. To address challenge 2, we present three delicate hyperedge expansion strategies to distill multi-level hyperedges. The hyperedges are generated automatically, follow the long-tailed distribution, and significantly improve link prediction performance. To address challenge 3, we design a multi-level hypergraph neural network for heterogeneous graphs, which can take both pair-wise links and hyperedges together. In summary, our principal contributions are as follows:</p><p>• We propose a novel hyperedge generation framework using three well-designed hyperedge expansion strategies. The hyperedges start from basic graphlets and then expand themselves automatically. The generated hyperedges follow the power-law distribution and outperform previous manually designed hypergraphs on network embeddings. • We focus on sparsely observed networks when predicting links, which is more common in practical applications. To overcome the limited links, we propose to leverage hypergraphs to learn better higher-level informations and put forward a multi-level hypergraph neural network, which can dramatically improve the performance. • We extensively evaluate our approach with state-of-the-art baselines on four real-world datasets. Experimental results demonstrate that our method can achieve significant improvements over existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY AND PROBLEM FORMULATION</head><p>In this section, we briefly present related concepts and give the formal definition of our target problem. Definition 1. (Heterogeneous Networks and Meta-path). A heterogeneous network (HIN) refers to a graph like G = {V, E, T v } where V denotes the objects, and E is the pair-wise relations between these objects. Each object is treated as a node with a specific type, and T v denotes all node types. When |T v | ≥ 2, the network becomes heterogeneous. A meta-path is a pre-defined path scheme where each position on the path has one assigned node type.</p><p>This paper studies three heterogeneous networks (DBLP, ACM, Yelp), and one homogeneous network (Cora). We treat the homogeneous network as a special case of HINs, which means it only contains one type of relation and the same kind of nodes. More details on DBLP, ACM, Yelp, and Cora are summarised in section 4.1. Definition 2. (Graphlets and Orbits). The graphlets of a given large network are a set of smaller connected induced subgraphs, and each graphlet is structurally distinct from all the other graphlets. Nodes within the same graphlet and topologically identical with others are put into the same orbits.</p><p>Take Figure <ref type="figure" target="#fig_1">2</ref> as an illustration, we present all graphlets with 2,3,4, and 5 nodes. There are 30 graphlets in total. For simplicity, we only show these graphlets in a homogeneous graph. We can use meta-path to extract different induced subgraphs and find graphlets independently for the graph with multiple node types. Definition 3. (Hypergraph). A hypergraph can be represented as G ▷ = {V ▷ , E ▷ }. Here V ▷ denotes node set, and E ▷ is the set of edges in the hypergraph (a.k.a hyperedges). Different from normal graphs, a hypergraph allows one hyperedge to connect multiple nodes, which means each hyperedge can be denoted as a subset of nodes</p><formula xml:id="formula_0">e ▷ = {v 1 , v 2 , • • • , v k }, v i ∈ V ▷ , e ▷ ∈ E ▷ . k is the size of hyperedge e ▷ .</formula><p>The presence of nodes in hyperedges can be represented as an incidence matrix H ∈ {0, 1} | V ▷ |× | E ▷ | where each entry can be calculated as follows:</p><formula xml:id="formula_1">H (i, j) = 1, if node i is in hyperedge j 0, otherwise Let D v ∈ R | V ▷ |× | V ▷ | and D e ∈ R | E ▷ |× | E ▷ |</formula><p>be two diagonal matrices, which denote the degrees of nodes and hyperedges, respectively. Then the degree of node i is defined as follows:</p><formula xml:id="formula_2">D v (i, i) = | E ▷ | j=1 U e (j, j) • H (i, j)</formula><p>where U e ∈ R | E ▷ |×| E ▷ | is a diagonal matrix, and the diagonal entries are hyperedge weights. Likewise, the degree of hyperedge j is defined as follows:</p><formula xml:id="formula_3">D e (j, j) = |V ▷ | i=1 U v (i, i) • H(i, j)</formula><p>where each diagonal entry in the diagonal matrix</p><formula xml:id="formula_4">U v ∈ R |V ▷ |×|V ▷ |</formula><p>stands for the weights of nodes.</p><p>Definition 4. (Hyperedge Distillation) We define the hyperedge distillation as the process of generating multi-level hyperedges automatically from data. The process starts from some basic graphlets and then expands small-scaled hyperedges to develop multi-level hyperedges.</p><p>With the above concepts, our target problem can be formulated as follows: Problem 1. (Social Linking Prediction). Given a sparsely observed network G = {V, E, T v }, and any pair of nodes v i , v j , v i , v j ∈ V, we wish to predict whether v i , v j has a given type of pair-wise relation or not, which is not observed before. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HYPERGRAPH LEARNING WITH MULTI-LEVEL EXPANSION</head><p>In this section, we first review our framework of social linking prediction, and then introduce the principal components of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Overview</head><p>Our model framework is shown in Figure <ref type="figure" target="#fig_2">3</ref>. The model is a stack of multi-level hypergraph neural networks (HNNs), and each HNN layer utilises the corresponding hypergraph, which is updated from the previous state. The initial hyperedges are built on graphlets shown in Figure <ref type="figure" target="#fig_1">2</ref>. Afterwards, we design three hyperedge expansion strategies (depth-first expansion, breadth-first expansion, and hybrid expansion) to expand these simple hyperedges as higherlevel hyperedges. In the end, we output the final network embeddings and the hypergraph incidence matrix, which can hint the hypergraph structure after multi-level hyperedge distillation. To predict social links, we concatenate each layer's latent representations and then send them to the downstream link prediction classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graphlets-driven Hyperedges</head><p>As previously discussed, our first target is how to generate multilevel hyperedges. Here we introduce graphlets as the initial hyperedges to launch later hyperedge distillation. We choose graphlets because they are structurally complete, and each graphlet preserves an exclusive structural unit. They go beyond pair-wise relations, which means they are informative for link prediction on sparsely observed networks. However, they are not sufficient to preserve global structures and thus make the performance limit. Besides, a larger network usually contains enormous graphlets, leading to a large number of hyperedges. At last, graphlets are still not hierarchical enough owing to their small number of nodes.</p><p>In light of these, we initialise our hyperedges with only 2,3,4,5node graphlets and ignore larger graphlets. For heterogeneous networks, we induce subgraphs with different meta-paths and then combine the graphlets in each subgraph. There are plenty of implements and algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33]</ref> that can efficiently find graphlets, making our initialisation entirely feasible. Each graphlet instance is treated as an initial hyperedge. Then we design three hyperedge expansion strategies (depth-first expansion, breadth-first expansion, and hybrid expansion) so that these hyperedges can be expanded or merged with other hyperedges, making the total hyperedge number reduced and hyperedge multi-level. We elaborate on this in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hyperedge Expansion</head><p>As shown in Figure <ref type="figure" target="#fig_3">4</ref>, a hyperedge usually adjoins other hyperedges with some shared nodes, making the hyperedge expansion executable. The transition probability from one hyperedge to another relies on two aspects: (i) the correlations of hyperedges, which can be measured by the product of hyperedge representations; and (ii) the connectivity between two hyperedges, which can be evaluated by the Jaccard similarity.</p><p>Let the observed network be G • = {V, E • } where V is node set and E • contains observed pair-wise links. The initial hyperedge set is</p><formula xml:id="formula_5">E ▷ 0 = {e 1 , e 2 , • • • , e n } where each hyperedge includes a set of nodes e i = {v i 1 , v i 2 , • • • , v i m }. Let node representation be Z v ∈ R |V |×d</formula><p>where d is node embedding dimension, then we denote hyperedges via the weighted summation over all nodes in the same hyperedge:</p><formula xml:id="formula_6">Z e = D −1 e • U e • H T • U v • Z v<label>(1)</label></formula><p>With the above formula, the correlation between hyperedges i and j can be calculated as :</p><formula xml:id="formula_7">α (i, j) = σ Z T e • Z e i j<label>(2)</label></formula><p>where σ (•) is a normalization operator such as the sigmoid function. To measure the connectivity between hyperedges i and j, we calculate the Jaccard similarity of i and j as follows:</p><formula xml:id="formula_8">β (i, j) = | V | k =1 H (k, i) • H (k, j) | V | p=1 H (p, i) + | V | q=1 H (q, j)<label>(3)</label></formula><p>With the above definitions, we define the transition probability from hyperedge i to hyperedge j as follows:</p><formula xml:id="formula_9">p (j |i) = α (i, j) + β (i, j) k ∈N i (α (i, k) + β (i, k))<label>(4)</label></formula><p>where N i denote all hyperedges sharing nodes with hyperedge i.</p><p>Based on formula (4), we present three novel hyperedge expansion strategies to distil multi-level hyperedges: depth-first expansion, breadth-first expansion, and hybrid expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Depth-First Expansion.</head><p>Depth-first expansion follows the idea that a hyperedge has the momentum to accommodate other hyperedges along a hyperedge path without revisiting the same hyperedges. As shown in the top of Figure <ref type="figure" target="#fig_3">4</ref>, there exists a hyperedge path starting from e 1 and ending at e 3 , which can be denoted as: e 1 → e 2 → e 3 . When we expand e 1 , it will first choose one adjacent hyperedge such as e 2 with the corresponding transition probability, then it continues to select the next hyperedge from the neighbours of e 2 which has never been visited before (such as e 3 ). We use p a (0 ≤ p a ≤ 1) to represent how aggressive the depthfirst expansion wants to continue. At each step, the process has the probability of p a to keep going or stop with the possibility of 1 − p a . More details on the depth-first expansion can be seen in Algorithm 1. Note that each hyperedge can be expanded independently because they only rely on the initial hyperedge set E ▷ 0 , thus the depth-first expansion algorithm can be efficiently conducted in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Breadth-First Expansion.</head><p>Breadth-First Expansion first explores a sampled neighbours of the target hyperedge e i , then adds </p><formula xml:id="formula_10">; hyperedge's adjacent set {N 1 , • • • , N n }; Output: updated hyperedges E ▷ 1 = {e 1 , e 2 , • • • , e m }; 1 E ▷ 1 = ∅. 2 for e i ∈ E ▷ 0 do 3 P : e i1 → e i2 → • • • e it , e i1 ∈ N e i 4</formula><p>//generate a hyperedge random walk path using formula (4) with the stop probability 1 − p a .</p><formula xml:id="formula_11">5 e ▷ i = e i ∪ e i1 ∪ e i2 ∪ • • • e it 6 E ▷ 1 = E ▷ 1 ∪ e ▷ i . 7 end 8 remove duplicated hyperedges in E ▷ 1 . 9 return E ▷ 1 Algorithm 2: Breadth-First Expansion Algorithm Input: sampled ratio r ; initial hyperedges E ▷ 0 ; hyperedge's adjacent set {N 1 , • • • , N n }; Output: updated hyperedges E ▷ 1 = {e 1 , e 2 , • • • , e m }; 1 E ▷ 1 = ∅. 2 for e i ∈ E ▷ 0 do 3 C i = {e i1 , e i2 • • • , e ik }, where e i j ∈ N e i , j = 1, • • • , k, k = r × |N e i |.</formula><p>//sample k adjacent hyperedges from N e i for e i using formula <ref type="bibr" target="#b3">(4)</ref>. </p><formula xml:id="formula_12">4 e ▷ i = e i ∪ e i1 ∪ e i2 ∪ • • • e ik 5 E ▷ 1 = E ▷ 1 ∪ e ▷ i</formula><formula xml:id="formula_13">; hyperedge's adjacent set {N 1 , • • • , N n }; Output: updated hyperedges E ▷ 1 = {e 1 , e 2 , • • • , e m }; 1 E ▷ 1 = ∅. 2 for e i ∈ E ▷ 0 do 3 C i = {e i1 , e i2 • • • , e ik }, where e i j ∈ N e i , j = 1, • • • , k, k = r × |N e i |.</formula><p>//sample k adjacent hyperedges from N e i for e i using formula (4).</p><p>4 for e i j ∈ C i do 5</p><p>P :</p><formula xml:id="formula_14">e 1 i j → e 2 i j → • • • e t i j , e 1 i j ∈ N e i j 6</formula><p>//generate a hyperedge random walk path using formula (4) with the stop probability 1 − p a .</p><formula xml:id="formula_15">7 e ▷ i j = e i j ∪ e 1 i j ∪ e 2 i j • • • ∪ e t i j 8 end 9 e ▷ i = e ▷ i1 ∪ e ▷ i2 • • • ∪ e ▷ ik 10 E ▷ 1 = E ▷ 1 ∪ e ▷ i 11 end 12 remove duplicated hyperedges in E ▷ 1 . 13 return E ▷ 1 .</formula><p>all nodes in the visiting hyperedges to e i . Take the middle of Figure <ref type="figure" target="#fig_3">4</ref> as an example; when we expand hyperedge e 1 , breadth-first expansion first select a portion of the neighbours of e 1 such as e 2 and e 3 , then e 1 adopts all nodes in e 2 and e 3 . We use r to denote the sampled ratio of e 1 's neighbour hyperedges, and then present the breadth-first expansion strategy in Algorithm 2. To further illustrate the multiple levels of the hyperedges generated by our proposed expansion strategies, we take Figure <ref type="figure" target="#fig_6">5</ref> as an example. When we try to expand hyperedges e 1 and e 2 , they may all succeed and include with each other. As a result, the previous two hyperedges now overlap and are reduced as one hyperedge. On the other case, e 1 may manage to swallow e 2 , but e 2 fail to embrace e 1 , thus e 1 becomes larger than before while e 2 stay unchanged, making the hyperedges hierarchical with multilayer structures. We can learn the optimal hyperedge numbers, hyperedge hierarchy, and keep the best balance between local and global structures. Next, we present a hypergraph neural network to integrate learned hyperedges and node features for linking prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">HNN: Hypergraph Neural Network for Linking Prediction</head><p>Having obtained the hyperedges generated by our proposed hyperedge expansion strategies, we now present a novel hypergraph neural network (HNN) to handle each level's hypergraph. The model takes hyperedges from each level as input and then aggregate node representations via node-level attention, hyperedge-level attention, and semantic-level attention.</p><p>3.4.1 Node-level Attention. Nodes in the same hyperedge usually have different importance, and the mutual influences are not uniform. To this end, we use the node-level attention to learn the correlations of nodes within the same hyperedge. Let z l i ∈ R 1×d and z l j ∈ R 1×d be the input representations of nodes i and j at level l. Here level l means we use the hypergraph generated by the l-th hyperedge expansion for l-th HNN layers. The conditional probability of node i given j within hyperedge e can be defined as follows:</p><formula xml:id="formula_16">p l v (i |j, e) = exp σ H l (i, e) • U l v (i, i) • z l i ⊕ z l j • p ⊺ l | V | t =1 exp σ H l (t, e) • U l v (t, t) • z l t ⊕ z l j • p ⊺ l</formula><p>(5) where p l ∈ R 1×2d is the fusion parameter. H l is the hypergraph incidence matrix, which can be obtained after hyperedges expansions at the l-th level. U l v is the pre-defined node weight at l-th level. In this paper, nodes' weights are given with the original networks and keep unchanged, thus we have</p><formula xml:id="formula_17">U 0 v = U 1 v = • • • U L v .</formula><p>We let p l v (i |j, e) = 0 if node j is not in hyperedge e. With the above formula, we update node i within hyperedge e as z l i |e , which can be aggregated from other nodes in the same hyperedge using corresponding coefficients:</p><formula xml:id="formula_18">z l i |e = σ |V | j=1 U l v (j, j) H l (j, e) D l</formula><p>e (e, e)</p><formula xml:id="formula_19">• p l v (i |j, e) • z l j<label>(6)</label></formula><p>Note that when l = 0, the input representation z 0 j is the initial feature of node j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Hyperedge-level Attention.</head><p>Many nodes in the network belong to multiple hyperedges because these hyperedges are hierarchical, and the impact from different hyperedges is also diverse. To evaluate this impact, we first obtain hyperedge representations, and then calculate each hyperedge's weight. Specifically, the representation of hyperedge e can be calculated as follows:</p><formula xml:id="formula_20">m l e = |V | i=1 H l (i, e) • z l i |e<label>(7)</label></formula><p>where m l e is the representation of hyperedge e. Following the above, the weight of hyperedge e is defined as follows:</p><formula xml:id="formula_21">α l e = σ U e (e, e) • tanh m l e • W l α + b l α • q ⊺ l<label>(8)</label></formula><p>where W l α , b l α , and q l are all learnable variables. Note that previous equations ( <ref type="formula">5</ref>)-( <ref type="formula" target="#formula_21">8</ref>) are all calculated under the same meta-path. For simplicity, we omit the meta-path notation Φ without loss of generality. To deal with heterogeneous networks, we just need to generate induced graphs according to different metapaths, and then obtain corresponding notations simultaneously. Furthermore, the representation of node i under meta-path Φ can be aggregated by all hyperedges which includes i:</p><formula xml:id="formula_22">z l |Φ i = | E ▷|Φ l | t =1 H l |Φ (i, t) D l |Φ v (i, i) • α l |Φ e t • z l |Φ i |e t (9)</formula><p>Here z l |Φ i is the representation of node i after aggregated by all hyperedges under meta-path Φ. E ▷ |Φ l is the hyperedge set under meta-path Φ generated after l-th hyperedge expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Semantic-level Attention.</head><p>Note that the node embeddings from just one meta-path only reflect one aspect of information. Therefore we need to fuse the information from all candidate metapaths. The importance of each meta-path can be defined as the average weights of all node embeddings in the same meta-path:</p><formula xml:id="formula_23">β l Φ i = exp | V | j=1 tanh W l β • z l |Φ i j + b l β • f T l P t =1 exp | V | j=1 tanh W l β • z l |Φ t j + b l β • f T l (<label>10</label></formula><formula xml:id="formula_24">)</formula><p>where P is the total number of all meta-paths. W l β , b l β , and f l are learnable parameters. Based on formula <ref type="bibr" target="#b9">(10)</ref>, the final embedding is aggregated by all selected meta-paths:</p><formula xml:id="formula_25">Z l +1 = P i=1 β l Φ i • Z l |Φ i (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training Framework for Linking Prediction</head><p>Let us assume there exist L HNN layers in our framework. HNN at l-th layer takes the previous embedding Z l as input, and utilises the hypergraph generated by l-th hyperedges expansion to generate Z l +1 . Then we present the training framework for linking prediction as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Training Method.</head><p>To train our framework efficiently, we first train the 1-st layer by minimising the Cross-Entropy over all observed pair-wise links and sampled negative links. Then we start the 1-st hyperedge expansion and generate a new hypergraph. With the new hypergraph and the node embedding of the 1-st layer, we then fix the 1-st layer and start to train the 2-nd layer. We repeat this process until the L-th layer training is finished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Negative Sampling.</head><p>To train and test our model, we also need to generate some negative links. In this paper, we use a bidirectional negative sampling strategy mentioned in <ref type="bibr" target="#b4">[5]</ref>, which will draw K negative samples from both sides of a positive link, and generate 2K negative links in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">Linking Prediction.</head><p>To predict a pair-wise link with a type ϕ, we concatenate all the embeddings from layer 1 to layer L, and send them to a multilayer perceptron (MLP) for linking prediction. The predicted score is defined as follows:</p><formula xml:id="formula_26">p (i, j |ϕ) = MLP ϕ f z 1 i ⊕ z 2 i ⊕ • • • z L i ⊕ f z 1 j ⊕ z 2 j ⊕ • • • z L j (<label>12</label></formula><formula xml:id="formula_27">)</formula><p>where ⊕ is the concatenation operator, f (•) is a fully connected network to reduce the dimensions of the input vectors. MLP ϕ (•) is the multilayer perceptron predicting pair-wise links with type ϕ. f (•) and MLP ϕ (•) are also trainable using the cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETTINGS 4.1 Datasets</head><p>We conduct our experiments on the following widely used datasets: • Cora <ref type="bibr" target="#b23">[24]</ref>: It is a citation network which contains 2,708 papers from 7 research areas. Each paper has a one-hot code denoting the presence of 1433 unique words. There are 5,429 pair-wise links in this dataset, which stand for the citation relations of papers.</p><p>• DBLP <ref type="bibr" target="#b43">[44]</ref>: It is an academic network that includes four types of nodes and three types of relations. There are 14,376 paper nodes (P), 14,475 author nodes (A), 20 conference nodes (C), and 8,811 term nodes (T) in total. The network has 41,794 P-A links, 14,376 P-C links, and 114,624 P-T links. In this paper, we predict both P-A links and P-C links. We transfer P-T links as one-hot codes and treat them as the features of papers.</p><p>• ACM <ref type="bibr" target="#b34">[35]</ref>: The ACM dataset contains 3,025 papers and 5,835 authors related to 58 subjects. The network comprises two types of relations: 9,744 paper-author links and 3025 papersubject links.</p><p>• Yelp <ref type="bibr" target="#b43">[44]</ref>: Yelp is an online social network. The dataset contains five types of nodes, including user (U), service (S), business (B), star level (L), and reservation (R), and four types of relations: B-U, B-S, B-R, and B-L. In this paper, we only predict links with types of B-U and B-L.</p><p>Specifically, we use Cora, DBLP, ACM, and Yelp to evaluate linking prediction performance with only 10% training links. Then we discuss more profound topics on our model using the Cora dataset. We use Cora for further analysis because it has only one type of relation, which can eliminate unnecessary disturbance from relations types and more clearly uncover our model's properties. More details on these datasets are illustrated in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare our model with the following state-of-the-art methods:</p><p>• Metapath2vec [9]: This method samples a corpus of walk paths from the network according to different meta-paths and maximises the conditional probability of the target node given its context. • PME <ref type="bibr" target="#b4">[5]</ref>: This method first projects node embeddings into different semantic spaces and then measures each pair of nodes' distance. It learns node embeddings via minimising nodes distances on positive links and maximising the distances on negative links. • HAN <ref type="bibr" target="#b34">[35]</ref>: This method uses novel-level attention and semanticlevel attention on normal graphs and learn node embeddings via the cross-entropy loss over all labeled nodes. In this paper, we concatenate the embeddings of a pair of nodes and then predict whether they have a link or not. • HGNN <ref type="bibr" target="#b9">[10]</ref>: This method uses a hypergraph convolutional network to learns node embeddings from a pre-defined hypergraph. The hypergraph is manually designed via features clustering and keeps fixed in the whole learning process.</p><p>• MGCN <ref type="bibr" target="#b3">[4]</ref>: This method first uses a graph convolutional network to learn a temporal node embeddings, and then further refines these embeddings via a hypergraph neural network. The hypergraph is manually constructed via community finding, and each community is treated as one hyperedge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics and Parameter Settings</head><p>We compare our model's performance with other baselines on AUC (a.k.a AUC@ROC), AP (a.k.a AUC@PR), and F1 value. For a fair comparison, the dimension of node embeddings is set as 100 for all models. We sampled 10 negative links for each positive link, then sample 10% links as training set, 10% links as valid set, and another 10% links as testing set. We also change the training ratio from 30% to only 1% to further evaluate our model's reliability and potential. For all models, we use PP as a meta-path in Cora. The meta-paths considered in DBLP include PAP, and PCP. The meta-paths in ACM include PAP and PSP. The meta-paths in Yelp are BUB, BLB, BSB, and BRB. Parameters settings for the baselines are selected by the grid search method. For Metapath2vec, we set the number of walks n = 5, walk length ℓ = 10, window size w = 5; For PME, we set margin m = 10. For our model, we expand hyperedges 3 times and get 4 hypergraphs, which are numbered from level 0 to level 3. Each level uses one layer HNN and an MLP with two layers. In the hyperedge expansion algorithm, we let r = 0.3, p a = 0.65. The model is trained using Adam optimizer with learning rate 0.001 and weight decay 0.0005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS AND ANALYSIS</head><p>In this section, we analyse the experimental results. Before all, we wish to answer the following research questions (RQ):</p><p>• RQ1: How does our model work compared to the other baselines? • RQ2: How do the hyperedge expansion strategies designed for multi-level hyperedges work? • RQ3: How robust and potential when our model deal with network sparsity? • RQ4: How does our model benefit from different hyperedge expansion strategies? • RQ5: How does hyperedge expansion level impact the performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Effectiveness Analysis on Linking Prediction (RQ1)</head><p>We predict social links with type P-P on Cora, P-C and P-A on DBLP, P-A, P-S on ACM, and B-U, B-L on Yelp. Note that all these datasets follow the power-law distribution, which means they are all sparse networks. Based on these networks, we sampled only 10% observed links as training set to further evaluate how these models perform with extremely limited links. We compare our model and the other baselines within 50 epoch and repeat the evaluation for 10 times. Then we report the averaged AUC, AP, and F1 values in Table <ref type="table" target="#tab_2">2</ref>.</p><p>From Table <ref type="table" target="#tab_2">2</ref>, we find that our model outperforms all the other baselines. Specifically, on the Cora dataset, our model exceeds the Consistent with our previous discussion, we notice that GNNbased methods such as HAN, HGNN, and MGCN achieve better performance than Metapath2vec, which only sample the graph as linear paths. This suggests the importance of information aggregation when we learn node embeddings because it can make the embeddings smooth on neighbours. However, when the observed links are extremely limited, the GNN model on normal graphs such as HAN, is challenging to beat hypergraph-based methods such as HGNN and MGCN, demonstrating the advantages of hypergraphs when the network is sparsely observed. Compared with manually designed hypergraphs, our approach uses hyperedge expansion strategies to learn multi-level hyperedges and significantly improves the performance, which can be reflected in the comparison to HGNN and MGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hierarchy Analysis on Hyperedge Expansion (RQ2)</head><p>Compared with manually designed hypergraphs, our model uses hyperedge expansion to distill multi-level hyperedges, significantly improving linking prediction results. To further illustrate the hierarchy of our generated hyperedges, we use depth-first hyperedge expansion strategy to generate hyperedges with 4 levels. Then we count the hyperedge size and node size in the hypergraph, and draw Figure <ref type="figure" target="#fig_7">6</ref>, which presents the distributions of the hyperedge size and the node degree (the number of hyperedges a node has) in each level hypergraph. Before the hyperedge expansion, we treat graphlets with 2,3,4,5 nodes as the starting hyperedges. The distributions of the hyperedge size and the node degree at level 0 are shown in Figure <ref type="figure" target="#fig_7">6a</ref> and Figure <ref type="figure" target="#fig_7">6e</ref>, from which we can see the initial hyperedges only suggest a very weak hierarchy and are insufficient to support better performance. However, after the hyperedge expansion, both node degree and hyperedge size show the power-law distribution obviously, which can be seen in Figure <ref type="figure" target="#fig_7">6b</ref>, 6c, 6d, and 6f, 6g, 6h. The distributions are very similar to Manh et al. <ref type="bibr" target="#b7">[8]</ref>, in which they discuss a simulated hypergraph with multi-level structures and also suggest similar hyperedge distributions. Through these analyses, we can find that our hyperedge expansion methods successfully learn multi-level hyperedges automatically from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analysis on Network Sparsity (RQ3)</head><p>To further explore the potential of exiting methods and analyse the stability of our model, we also reduce the ratio of training links from 30% to only 1%. The performance of our model and other baselines is shown in Figure <ref type="figure" target="#fig_8">7</ref>, from which we have the following observations:</p><p>When the training links reduced to only 1%, most baselines have dropped down to the ground bottom (nearly 0.5 in AUC and 0.0909 in AP, which is the worst case for the dataset). This means most baselines have been infeasible to learn useful representations for link predictions. Compared with baselines, our model still keeps meaningful performance on AUC and AP. The improvements are even more considerable when the training ratio increases from 1% to 5%, which can further demonstrate our model's outstanding performance in sparsely observed networks. With the training ratio increase from 5% to 30%, most baselines start to work. In particular, This observation suggests that GNN-based methods perform better in making node embeddings smooth on pair-wise links.</p><p>With the above analysis, we can confirm that our model is robust and keeps ahead with larger tolerance to the observed link ratio. This is especially helpful in the real-world situations because most social networks are sparsely connected or sparsely observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis on Different Hyperedge Expansion Strategies (RQ4)</head><p>To figure out the impact on our proposed hyperedge expansion strategies, we repeat the evaluation for linking prediction with different hyperedge expansion strategies on Cora (P-P), DBLP (P-C, P-A), ACM (P-A, P-S), and Yelp (B-U, B-L). Considering the network properties, we can conclude that breadthfirst expansion does better when the nodes in a network have larger degrees. Depth-first expansion is better than the breadth-first expansion if the network contains longer paths. Although the hybrid expansion does better than the other strategies in most cases, it relies on both depth-first and breadth-first expansion strategies and inherits two external parameters (aggressive parameter p a , and sampled ratio r ), which need more work to fit data. Intuitively, hypergraphs' hierarchical structures should be achieved within limited expansion levels, which means there is no more need to expand existing hyperedges and thus save the training time. Take depth-first expansion as an example, if we expand hyperedges too many times, we have to reduce the aggressive parameter p a at higher levels so that the sizes of learned hyperedges are in a reasonable range. To illustrate this, we use depth-first expansion strategy with the aggressive parameter p a = 0.65 to expand the initial hyperedges for 8 times and report the results at each level in Figure <ref type="figure" target="#fig_10">9</ref> where the horizontal axis is the hyperedge expansion level, the vertical axis in left is AUC and F1 results, and the right vertical axis is AP values.</p><p>As shown in Figure <ref type="figure" target="#fig_10">9</ref>, we can see that our model's performance becomes better with the hyperedge expansion level increase. However, this trend turns to grow slowly after we conduct 4 times of hyperedge expansion. This means we do not need to conduct more hyperedge expansions because only a few times of hyperedge expansions are sufficient to make the hyperedges hierarchal and support our model to achieve the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>In this section, we discuss related topics in our paper, including network embedding, hypergraph learning, and link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Network Embedding</head><p>Network embedding aims to learn low-dimensional representations for nodes in the network. Traditional methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref> usually sample paths from the network and then use the skip-gram method to preserve neighbouring proximity in the path. Following this idea, some extensions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref> have been proposed recently so that the model can deal with heterogeneous networks. Besides these works, other researches leverage graph neural networks such as attention networks <ref type="bibr" target="#b34">[35]</ref> and convolutional networks <ref type="bibr" target="#b15">[16]</ref> to learn high-quality embeddings via information aggregation <ref type="bibr" target="#b2">[3]</ref>. For example, Hu et al. <ref type="bibr" target="#b17">[18]</ref> design an adversarial learning framework with a relation generator and discriminator to learn node embeddings preserving the heterogeneous information in the network. Fu et al. <ref type="bibr" target="#b10">[11]</ref> propose a new information aggregation method based on meta path. The model includes intra-metapath and inter-metapath aggregations so that their learned embeddings can fit multiple semantics well. Similarly, Wang et al. <ref type="bibr" target="#b35">[36]</ref> also leverage meta-path to calculate nodes proximities and learn node embeddings for dynamic networks. In summary, most of the related works pursue the smoothness of node embeddings so that the learned representations can support the downstream task such as node classification and link prediction. The smoothness of nodes plays a crucial role in the performance on these tasks, and most of these works achieve node smoothness using pair-wise links. However, when the pair-wise links are limited, it will be hard to keep smoothness on the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Hypergraph Learning</head><p>Recently, hypergraph learning has been widely used in relational inference <ref type="bibr" target="#b6">[7]</ref>, location-based social networks <ref type="bibr" target="#b40">[41]</ref>, personality perception <ref type="bibr" target="#b48">[49]</ref>, and recommender systems <ref type="bibr" target="#b49">[50]</ref> because of its capacity for modelling high-level relations. To benefit from hypergraphs, researchers need to study hypergraph structures and have raised many interesting works. For example, Zhang et al. <ref type="bibr" target="#b45">[46]</ref> propose an EM algorithm to predict whether a set of objects belong to the same tulpe, which they called hyperlink. Yoon et al. <ref type="bibr" target="#b42">[43]</ref> research the correlation between the hyperedge size and the hyperedge prediction, and find the limitations of pair-wise interactions. Zhang et al. <ref type="bibr" target="#b47">[48]</ref> study the dynamic hypergraph structure for node classification using spectral theory. Manh et al. <ref type="bibr" target="#b7">[8]</ref> analyse a simulated hypergraph and demonstrate the importance of hierarchical structures for the hypergraphs. Besides hypergraph structures, there also exist some works trying to learn node embeddings from hypergraphs. Inspired by graph neural networks on normal graphs, hypergraph neural networks on hypergraphs are also proposed recently. For example, Feng et al. <ref type="bibr" target="#b9">[10]</ref> offer a hypergraph convolutional networks based on spectral factorization on the Laplacian matrix of a hypergraph. Zhang et al. <ref type="bibr" target="#b46">[47]</ref> use self-attention to learn node embeddings from the hypergraph. Jiang et al. <ref type="bibr" target="#b19">[20]</ref> conduct the convolutional operation on both vertices and hyperedges and then they present the hypergraph neural networks on dynamic data. Although much progress has been achieved, most related works still need to use manually created hyperedges, which are far from multi-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Linking Prediction</head><p>Linking prediction aims to predict whether a pair of nodes in the graph has a link with the specific type. This problem can be explored by learning effective node embeddings, which can preserve neighbour proximity <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b43">44]</ref>. Recently, GNN-based methods have been widely applied to this area and suggest the advantages over traditional methods <ref type="bibr" target="#b3">[4]</ref>. For example, Li et al. <ref type="bibr" target="#b22">[23]</ref> use a graph attention network to learn the representations of two networks and then present a type-aware algorithm to align nodes from the two networks. Qu et al. <ref type="bibr" target="#b26">[27]</ref> predict continuous-time links via temporal graph neural networks. In real-world situations, most networks follow power-law distributions, which means many nodes have only limited neighbours. To fit this situation, Hao et al. <ref type="bibr" target="#b14">[15]</ref> use nodes' external attributes to find the most likely positions of nodes. However, this work can only be used in attributed graphs. To rely less on external attributes, Ostapuk et al. <ref type="bibr" target="#b24">[25]</ref> propose to learn link embeddings via active learning. But this work needs persistent manual annotations. To further improve the performance, researchers have realised the importance of higher-level relations on the network <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref>. For example, Shao et al. study the correlations between network community and link prediction, then they propose a link prediction method based on low-rank matrix completion. Wang et al. <ref type="bibr" target="#b37">[38]</ref> extract multi-level subgraphs and then use a graph neural network to learn the representations of each pair of nodes for link prediction. Joshi et al. <ref type="bibr" target="#b21">[22]</ref> further propose a method to prune network subgraphs to that the neighbour proximity can be learned more effectively in knowledge graphs. Although these works have achieved good performance, rare works try to consider how their models keep stable results when the observed links are extremely limited, and most of them still heavily rely on pair-wise links. In the real world, however, most pair-wise relations are not easy to be observed, making the linking prediction under sparsely observed networks still challengeable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we aim to predict social links on sparsely observed networks, and propose a novel hypergraph-based framework. We uncover the importance of hyperedge hierarchy in the linking prediction and present three novel methods to generate multi-level hyperedges. We use a well-designed hypergraph neural network to learn node embeddings, and the experimental results confirm the superiorities of our model over state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: pair-wise relations (left), multi-level hyperedges (middle), and toneless hyperedges (right).</figDesc><graphic url="image-1.png" coords="3,104.24,83.69,403.53,117.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: 2,3,4,5-node graphlets. There are 30 graphlets denoted by G i , i = 0, • • • , 29. In each graphlet, we use different colors to denote different orbits. Nodes in the same orbit are topologically identical.</figDesc><graphic url="image-2.png" coords="3,324.59,239.53,226.97,109.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The flowchart of our framework.</figDesc><graphic url="image-3.png" coords="4,66.41,83.69,479.19,191.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Three hyperedge expansion strategies. top: depthfirst expansion; middle: breadth-first expansion; bottom: hybrid expansion.</figDesc><graphic url="image-4.png" coords="5,90.70,83.69,166.45,182.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 :</head><label>1</label><figDesc>Depth-First Expansion AlgorithmInput: aggressive parameter p a ; initial hyperedges E ▷ 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3. 3 . 3</head><label>33</label><figDesc>Hybrid Expansion. Intuitively, depth-first expansion intends to check remote hyperedges, building connections between different regions in the network. On the contrary, breadth-first expansion tries to enlarge the hyperedge with the nearest neighbours, which allows hyperedges to have more overlap. Based on depth-first expansion and breadth-first expansion strategies, we further fuse them and propose a hybrid expansion strategy shown in Algorithm 3. As depicted in the bottom of Figure4, hybrid expansion first selects a portion of e 1 's neighbours such as e 2 and e 4 , then for each selected neighbour, it generates a hyperedge path and treats all nodes in these hyperedges as the new members of e 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Hyperedges expansion cases. Through hyperedge expansion, we can reduce the number of hyperedges and construct multi-level hyperedges.</figDesc><graphic url="image-5.png" coords="6,337.20,83.69,201.76,131.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Hyperedges/nodes distributions after multi-level expansions</figDesc><graphic url="image-10.png" coords="9,56.04,211.14,126.10,94.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance w.r.t observed links ratio</figDesc><graphic url="image-14.png" coords="10,56.04,95.03,166.46,115.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Performance w.r.t hyperedge expansion strategies depicted in Figure 8, we notice that the hybrid expansion leads by a narrow margin in most cases such as Cora (P-P), DBLP (P-A), ACM (P-A, P-S), and Yelp (B-L). The advantage of the hybrid expansion is more observable in Yelp (B-U). However, in DBLP (P-C), depth-first expansion beats hybrid expansion and keep to the top. Compare the depth-first expansion with the breadth-first expansion, we can find that the breadth-first expansion performs better in Yelp (B-U), keeps similar performances with breadth-first expansion in Yelp (B-L), ACM (P-S), and DBLP (P-A), and shows lower results in the rest cases.</figDesc><graphic url="image-17.png" coords="10,57.91,449.55,232.03,118.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Performance w.r.t hyperedge expansion level</figDesc><graphic url="image-18.png" coords="10,324.59,362.88,226.98,140.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets If the network is not connected, we calculate the average path length for the largest connected component.</figDesc><table><row><cell cols="4">Datasets Node Type #Number Relation Type #Number Ave. Degree</cell><cell>Ave. Path Length</cell></row><row><cell>Cora</cell><cell>Paper (P) #2,708</cell><cell>P-P #5,429</cell><cell>3.9</cell><cell>6.3</cell></row><row><cell></cell><cell>Paper (P) #14,376</cell><cell>P-A #41,794</cell><cell>2.9</cell><cell>2.9</cell></row><row><cell>DBLP</cell><cell>Author (A) #14,475 Conference (C) #20</cell><cell>P-C #14,376 P-T #114,624</cell><cell>2.0 9.8</cell><cell>2.0 7.9</cell></row><row><cell></cell><cell>Term (T) #8,811</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Paper (P) #3,025</cell><cell>P-A #9,744</cell><cell>2.4</cell><cell>11.1</cell></row><row><cell>ACM</cell><cell>Author (A) #5,835</cell><cell>P-S #3,025</cell><cell>2.0</cell><cell>2.0</cell></row><row><cell></cell><cell>Subjects (S) #58</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>User (U) #1,286</cell><cell>B-U #30,838</cell><cell>15.8</cell><cell>4.1</cell></row><row><cell></cell><cell>Service (S) #2</cell><cell>B-S #2,614</cell><cell>2.0</cell><cell>2.0</cell></row><row><cell>Yelp</cell><cell>Business (B) #2,614</cell><cell>B-R #2,614</cell><cell>2.0</cell><cell>2.0</cell></row><row><cell></cell><cell>Star Level (L) #9</cell><cell>B-L #2,614</cell><cell>2.0</cell><cell>2.0</cell></row><row><cell></cell><cell>Reservation (R) #2</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Link prediction results (10% observed links)</figDesc><table><row><cell>Metrics</cell><cell>Methods</cell><cell cols="7">Cora (P-P) DBLP (P-C) DBLP (P-A) ACM (P-A) ACM (P-S) Yelp (B-U) Yelp (B-L)</cell></row><row><cell></cell><cell>Ours</cell><cell>0.6288</cell><cell>0.7251</cell><cell>0.6661</cell><cell>0.6284</cell><cell>0.5975</cell><cell>0.5672</cell><cell>0.5924</cell></row><row><cell></cell><cell>Metapath2vec</cell><cell>0.5013</cell><cell>0.5534</cell><cell>0.5429</cell><cell>0.5432</cell><cell>0.5367</cell><cell>0.5016</cell><cell>0.5035</cell></row><row><cell>AUC</cell><cell>PME HAN</cell><cell>0.5187 0.5564</cell><cell>0.6070 0.6206</cell><cell>0.5859 0.6032</cell><cell>0.5509 0.5673</cell><cell>0.5477 0.5564</cell><cell>0.5054 0.5203</cell><cell>0.5002 0.5326</cell></row><row><cell></cell><cell>HGNN</cell><cell>0.5552</cell><cell>0.6531</cell><cell>0.5678</cell><cell>0.5298</cell><cell>0.5435</cell><cell>0.5191</cell><cell>0.5498</cell></row><row><cell></cell><cell>MGCN</cell><cell>0.5673</cell><cell>0.6904</cell><cell>0.6035</cell><cell>0.5964</cell><cell>0.5975</cell><cell>0.5367</cell><cell>0.5564</cell></row><row><cell></cell><cell>Ours</cell><cell>0.1455</cell><cell>0.1606</cell><cell>0.1469</cell><cell>0.1868</cell><cell>0.1438</cell><cell>0.1104</cell><cell>0.1206</cell></row><row><cell></cell><cell>Metapath2vec</cell><cell>0.0993</cell><cell>0.1025</cell><cell>0.1054</cell><cell>0.1003</cell><cell>0.0997</cell><cell>0.0993</cell><cell>0.0919</cell></row><row><cell>AP</cell><cell>PME HAN</cell><cell>0.0979 0.1203</cell><cell>0.1009 0.1326</cell><cell>0.1033 0.1207</cell><cell>0.0941 0.1154</cell><cell>0.0913 0.1207</cell><cell>0.0930 0.1069</cell><cell>0.0914 0.1204</cell></row><row><cell></cell><cell>HGNN</cell><cell>0.1133</cell><cell>0.1393</cell><cell>0.1061</cell><cell>0.0943</cell><cell>0.1372</cell><cell>0.1009</cell><cell>0.1165</cell></row><row><cell></cell><cell>MGCN</cell><cell>0.1208</cell><cell>0.1524</cell><cell>0.1203</cell><cell>0.1206</cell><cell>0.1438</cell><cell>0.1096</cell><cell>0.1201</cell></row><row><cell></cell><cell>Ours</cell><cell>0.8705</cell><cell>0.8804</cell><cell>0.8742</cell><cell>0.8972</cell><cell>0.8964</cell><cell>0.7506</cell><cell>0.8002</cell></row><row><cell></cell><cell>Metapath2vec</cell><cell>0.5602</cell><cell>0.6932</cell><cell>0.7321</cell><cell>0.5564</cell><cell>0.6037</cell><cell>0.5735</cell><cell>0.5942</cell></row><row><cell>F1</cell><cell>PME HAN</cell><cell>0.6336 0.7806</cell><cell>0.7348 0.8802</cell><cell>0.7675 0.8697</cell><cell>0.6194 0.8742</cell><cell>0.6857 0.8703</cell><cell>0.6015 0.7562</cell><cell>0.6088 0.8009</cell></row><row><cell></cell><cell>HGNN</cell><cell>0.7952</cell><cell>0.8697</cell><cell>0.8635</cell><cell>0.8597</cell><cell>0.8713</cell><cell>0.7438</cell><cell>0.7929</cell></row><row><cell></cell><cell>MGCN</cell><cell>0.8079</cell><cell>0.8702</cell><cell>0.8633</cell><cell>0.8864</cell><cell>0.8806</cell><cell>0.7506</cell><cell>0.8002</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by National Key R&amp;D Program of China (Grants No. 2017YFB1003000, 2019YFC1521403), National Natural Science Foundation of China (Grants No. 61972087, 61772133, 61632008), National Social Science Foundation of China (Grants No. 19@ZH014), Natural Science Foundation of Jiangsu province (Grants No. SBK2019022870), Jiangsu Provincial Key Laboratory of Network and Information Security (Grants No. BM2003201), Key Laboratory of Computer Network Technology of Jiangsu Province (Grants No. BE2018706), Key Laboratory of Computer Network and Information Integration of Ministry of Education of China (Grants No. 93K-9), ARC Discovery Project (Grant No.DP190101985). The first author Mr. Xiangguo Sun, in particular, wants to thank his parents for their support during his tough period.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Counting graphlets: Space vs time</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Bressan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavio</forename><surname>Chierichetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Leucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Panconesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Multi-Scale Approach for Graph Link Prediction</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3308" to="3315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representation learning for attributed multiplex heterogeneous network</title>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1358" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-level Graph Convolutional Networks for Crossplatform Anchor Link Prediction</title>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangguo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Gabrys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarzyna</forename><surname>Musial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1503" to="1511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PME: projected metric embedding on heterogeneous networks for link prediction</title>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1177" to="1186" />
		</imprint>
	</monogr>
	<note>Quoc Viet Hung Nguyen, and Xue Li</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Air: Attentional intention-aware recommender systems</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno>ICDE. 304-315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Quoc Viet Hung Nguyen, and Xue Li</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast relational probabilistic inference and learning: Approximate counting via hypergraphs</title>
		<author>
			<persName><forename type="first">Mayukh</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Dhami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Kunapuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriraam</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7816" to="7824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structural patterns and generative models of real-world hypergraphs</title>
		<author>
			<persName><forename type="first">Se-Eun</forename><surname>Manh Tuan Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kijung</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="176" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Metapath2vec: Scalable representation learning for heterogeneous networks</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hypergraph neural networks</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3558" to="3565" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2331" to="2341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Streaming session-based recommendation</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viet</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1569" to="1577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic Link Prediction by Integrating Node Vector Evolution and Local Neighborhood Representation</title>
		<author>
			<persName><forename type="first">Xiaorong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1717" to="1720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Inductive Link Prediction for Nodes Having Only Attribute Information</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xike</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibo</forename><surname>Wang</surname></persName>
		</author>
		<idno>IJCAI. 1209-1215</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">HeteS-paceyWalk: a heterogeneous spacey random walk for heterogeneous information network embedding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial learning on heterogeneous information networks</title>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism</title>
		<author>
			<persName><forename type="first">Binbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="946" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic hypergraph neural networks</title>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingxuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2635" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hypergraph induced convolutional manifold networks</title>
		<author>
			<persName><forename type="first">Taisong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baochang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2670" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Searching for Embeddings in a Haystack: Link Prediction on Knowledge Graphs with Subgraph Pruning</title>
		<author>
			<persName><forename type="first">Unmesh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Urbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2817" to="2823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Type-Aware Anchor Link Prediction across Heterogeneous Networks Based on Graph Attention Network</title>
		<author>
			<persName><forename type="first">Xiaoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanmin</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Activelink: deep active learning for link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Ostapuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Cudré-Mauroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1398" to="1408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Continuous-Time Link Prediction via Temporal Dependent Graph Neural Network</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaisheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3026" to="3032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast and Accurate Estimation of Typed Graphlets</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anup</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tung</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen K</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
				<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="32" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond triplets: hyper-relational knowledge graph embedding for link prediction</title>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Cudré-Mauroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1885" to="1896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Community Detection and Link Prediction via Cluster-driven Low-rank Matrix Completion</title>
		<author>
			<persName><forename type="first">Junming</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongjing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinli</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3382" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Heterogeneous Hypergraph Embedding for Graph Classification</title>
		<author>
			<persName><forename type="first">Xiangguo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Jiuxin Cao, Yingxia Shao, and Nguyen Quoc Viet Hung</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">SHINE: Signed heterogeneous information network embedding for sentiment link prediction</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<idno>WSDM. 592-600</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MOSS-5: A fast method of approximating counts of 5-node graphlets in large graphs</title>
		<author>
			<persName><forename type="first">Pinghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiefeng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Cs</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Don</forename><surname>Towsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohong</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICDE</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="73" to="86" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Online user representation learning across heterogeneous social networks</title>
		<author>
			<persName><forename type="first">Weiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingzhong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="545" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic Heterogeneous Information Network Embedding with Meta-path based Proximity</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hyperbolic heterogeneous information network embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5337" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust Embedding with Multi-Level Structures for Link Prediction</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5240" to="5246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Link prediction with signed latent factors in signed social networks</title>
		<author>
			<persName><forename type="first">Pinghua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1046" to="1054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Revisiting user mobility and social relationships in LBSNs: A hypergraph embedding approach</title>
		<author>
			<persName><forename type="first">Dingqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingqing</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Cudre-Mauroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2147" to="2157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Social influence-based group representation learning for group recommendation</title>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Hongzhi Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="566" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">How Much and When Do We Need Higher-order Informationin Hypergraphs? A Case Study on Hyperedge Prediction</title>
		<author>
			<persName><forename type="first">Hyungseok</forename><surname>Se-Eun Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kijung</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2627" to="2633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Relation Structure-Aware Heterogeneous Information Network Embedding</title>
		<author>
			<persName><forename type="first">Linmei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Social media mining: an introduction</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Zafarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ali Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Beyond link prediction: Predicting hyperlinks in adjacency space</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shali</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hyper-SAGNN: a self-attention based graph neural network for hypergraphs</title>
		<author>
			<persName><forename type="first">Ruochi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuesong</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic Hypergraph Structure Learning</title>
		<author>
			<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3162" to="3169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Personality-Aware Personalized Emotion Recognition from Physiological Signals</title>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1660" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A novel social network hybrid recommender system based on hypergraph topologic structure</title>
		<author>
			<persName><forename type="first">Xiaoyao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="985" to="1013" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
