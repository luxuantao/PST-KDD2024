<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding the Behaviors of BERT in Ranking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Qiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
							<email>chenyan.xiong@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding the Behaviors of BERT in Ranking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies the performances and behaviors of BERT in ranking tasks. We explore several different ways to leverage the pre-trained BERT and fine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web Track ad hoc document ranking. Experimental results on MS MARCO demonstrate the strong effectiveness of BERT in question-answering focused passage ranking tasks, as well as the fact that BERT is a strong interaction-based seq2seq matching model. Experimental results on TREC show the gaps between the BERT pre-trained on surrounding contexts and the needs of ad hoc document ranking. Analyses illustrate how BERT allocates its attentions between query-document tokens in its Transformer layers, how it prefers semantic matches between paraphrase tokens, and how that differs with the soft match patterns learned by a click-trained neural ranker.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the past several years, neural information retrieval (Neu-IR) research has developed several effective ways to improve ranking accuracy. Interaction-based neural rankers soft match querydocuments using their term interactions <ref type="bibr" target="#b2">[3]</ref>; Representation-based embeddings capture relevance signals using distributed representations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>; large capacity networks learn relevance patterns using large scale ranking labels <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>. These techniques lead to promising performances on various ranking benchmarks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>Recently, BERT, the pre-trained deep bidirectional Transformer, has shown strong performances on many language processing tasks <ref type="bibr" target="#b1">[2]</ref>. BERT is a very deep language model that is pre-trained on the surrounding context signals in large corpora. Fine-tuning its pre-trained deep network works well on many downstream sequence to sequence (seq2seq) learning tasks. Different from seq2seq learning, previous Neu-IR research considers such surroundingcontext-trained neural models not as effective in search as relevance modeling <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. However, on the MS MARCO passage ranking task, fine-tuning BERT and treating ranking as a classification problem outperforms existing Neu-IR models by large margins <ref type="bibr" target="#b3">[4]</ref>.</p><p>This paper studies the performances and properties of BERT in ad hoc ranking tasks. We explore several ways to use BERT in ranking, as representation-based and interaction-based neural rankers, as in combination with standard neural ranking layers. We study the behavior of these BERT-based rankers on two benchmarks: the MS MARCO passage ranking task, which ranks answer passages for questions, and TREC Web Track ad hoc task, which ranks ClueWeb documents for keyword queries.</p><p>Our experiments observed rather different performances of BERTbased rankers on the two benchmarks. On MS MARCO, fine-tuning BERT significantly outperforms previous state-of-the-art Neu-IR models, and the effectiveness mostly comes from its strong cross question-passage interactions. However, on TREC ad hoc ranking, BERT-based rankers, even further pre-trained on MS MARCO ranking labels, perform worse than feature-based learning to rank and a Neu-IR model pre-trained on user clicks in Bing log.</p><p>We further study the behavior of BERT through its learned attentions and term matches. We illustrate that BERT uses its deep Transformer architecture to propagate information more globally on the text sequences through its attention mechanism, compared to interaction-based neural rankers which operate more individually on term pairs. Further studies reveal that BERT focuses more on document terms that directly match the query. It is similar to the semantic matching behaviors of previous surrounding contextbased seq2seq models, but different from the relevance matches neural rankers learned from user clicks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BERT BASED RANKERS</head><p>This section describes the notable properties of BERT and how it is used in ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notable Properties of BERT</head><p>We refer readers to the BERT and Transformer papers for their details <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref>. Here we mainly discuss its notable properties that influence its usage in ranking.</p><p>Large Capacity. BERT uses standard Transformer architecturemulti-head attentions between all term pairs in the text sequencebut makes it very deep. Its main version, BERT-Large, includes 24 Transformer layers, each with 1024 hidden dimensions and 16 attention heads. It in total has 340 million learned parameters, much bigger than typical Neu-IR models.</p><p>Pretraining. BERT learns from the surrounding context signals in Google News and Wikipedia corpora. It is learned using two tasks: the first predicts random missing words (15%) using the rest of the sentence (Mask-LM); the second predicts whether two sentences appear next to each other. In the second task, the two sentences are concatenated to one sequence; a special token "[SEP]" marks the sequence boundaries. Its deep network is very resource consuming in training: BERT-Large takes four days to train on 64 TPUs and easily takes months on typical GPUs clusters.</p><p>Fine Tuning. End-to-end training BERT is unfeasible in most academic groups due to resource constraints. It is suggested to use the pre-trained BERT as a fine-tuning method <ref type="bibr" target="#b1">[2]</ref>. BERT provides a arXiv:1904.07531v4 [cs.IR] 26 Apr 2019 "[CLS]" token at the start of the sequence, whose embeddings are treated as the representation of the text sequence(s), and suggests to add task-specific layers on the "[CLS]" embedding in fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Ranking with BERT</head><p>We experiment with four BERT based ranking models: BERT (Rep), BERT (Last-Int), BERT (Mult-Int), and BERT (Term-Trans). All four methods use the pre-trained BERT to obtain the representation of the query q, the document d, or the concatenation of the two qd. In the concatenation sequence qd, the query and document are concatenated to one sequence with boundary marked by a marker token ("[SEP]").</p><p>The rest of this section uses subscript i, j, or cls to denote the tokens in q, d, or qd, and superscript k to denote the layer of BERT's Transformer network: k = 1 is the first layer upon word embedding and k = 24 or "last" is the last layer. For example, ì qd k cls is the embedding of the "[CLS]" token, in the k-th layer of BERT on the concatenation sequence qd.</p><p>BERT (Rep) uses BERT to represent q and d:</p><formula xml:id="formula_0">BERT (Rep)(q, d) = cos(ì q last cl s , ì d last cl s ).<label>(1)</label></formula><p>It first uses the last layers' "[CLS]" embeddings as the query and document representations, and then calculates the ranking score via their cosine similarity (cos). Thus it is a representation-based ranker. BERT (Last-Int) applies BERT on the concatenated qd sequence:</p><formula xml:id="formula_1">BERT (Last-Int)(q, d) = w T ì qd last cl s .<label>(2)</label></formula><p>It uses the last layer's "[CLS]" as the matching features and combines them linearly with weight w. It is the recommended way to use BERT <ref type="bibr" target="#b1">[2]</ref> and is first applied to MARCO passage ranking by Nogueira and Cho <ref type="bibr" target="#b3">[4]</ref>. The ranking score from BERT (Last-Int) includes all term pair interactions between the query and document via its Transformer's cross-match attentions <ref type="bibr" target="#b5">[6]</ref>. Thus it is an interaction-based ranker.</p><p>BERT (Mult-Int) is defined as:</p><formula xml:id="formula_2">BERT (Mult-Int)(q, d) = 1≤k ≤24 (w k Mul t ) T ì qd k cls .<label>(3)</label></formula><p>It extends BERT (Last-Int) by adding the matching features ì qd k cls from all BERT's layers, to study whether different layers of BERT provide different information.</p><p>BERT (Term-Trans) adds a neural ranking network upon BERT, to study the performance of their combinations:</p><formula xml:id="formula_3">s k (q, d) = Mean i, j (cos(relu(P k ì q k i ), relu(P k ì d k j )))<label>(4)</label></formula><formula xml:id="formula_4">BERT (Term-Trans)(q, d) = k w k t r ans s k (q, d).<label>(5)</label></formula><p>It first constructs the translation matrix between query and document, using the cosine similarities between the projections of their contextual embeddings. Then it combines the translation matrices from all layers using mean-pooling and linear combination. All four BERT based rankers are fine-tuned from the pre-trained BERT-Large model released by Google. The fine-tuning uses classification loss, i.e., to classify whether a query-document pair is relevant or not, following the prior research <ref type="bibr" target="#b3">[4]</ref>. We experimented with pairwise ranking loss but did not observe any difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL METHODOLOGIES</head><p>Datasets. Our experiments are conducted on MS MARCO passage reranking task and TREC Web Track ad hoc tasks with ClueWeb documents.</p><p>MS MARCO includes question-alike queries sampled from Bing search log and the task is to rank candidate passages based on whether the passage contains the answer for the question <ref type="foot" target="#foot_0">1</ref> . It includes 1,010,916 training queries and a million expert annotated answer passage relevance labels. We follow the official train/develop split, and use the given "Train Triples Small" to fine-tune BERT.</p><p>ClueWeb includes documents from ClueWeb09-B and queries from TREC Web Track ad hoc retrieval task 2009-2012. In total, 200 queries with relevance judgements are provided by TREC. Our experiments follow the same set up in prior research and use the processed data shared by their authors <ref type="bibr" target="#b0">[1]</ref>: the same 10-fold cross validation, same data pre-processing, and same top 100 candidate documents from Galago SDM to re-rank.</p><p>We found that the TREC labels alone are not sufficient to finetune BERT nor train other neural rankers to outperform SDM. Thus we decided to first pre-train all neural methods on MS MARCO and then fine-tune them on ClueWeb.</p><p>Evaluation Metrics. MS MARCO uses MRR@10 as the official evaluation. Results on the Develop set re-rank top 100 from BM25 in our implementation. Results on Evaluations set are obtained from the organizers and re-rank top 1000 from their BM25 implementation. ClueWeb results are evaluated by NDCG@20 and ERR@20, the official evaluation metrics of TREC Web Track.</p><p>Statistical significance is tested by permutation tests with p &lt; 0.05, except on MS MARCO Eval where per query scores are not returned by the leader board.</p><p>Compared Methods. The BERT based rankers are compared with the following baselines:</p><p>• Base is the base retrieval model that provides candidate documents to re-rank. It is BM25 on MS MARCO and Galago-SDM on ClueWeb. • LeToR is the feature-based learning to rank. It is RankSVM on MS MARCO and Coordinate Ascent on ClueWeb. • K-NRM is the kernel-based interaction-based neural ranker <ref type="bibr" target="#b6">[7]</ref>.</p><p>• Conv-KNRM is the n-gram version of K-NRM. K-NRM and Conv-KNRM results on ClueWeb are obtained by our implementations and pre-trained on MS MARCO. We also include Conv-KNRM (Bing) which is the same Conv-KNRM model but pre-trained on Bing clicks by prior research <ref type="bibr" target="#b0">[1]</ref>. The rest baselines reuse the existing results from prior research. Keeping experimental setups consistent makes all results directly comparable.</p><p>Implementation Details. All BERT rankers are trained using Adam optimizer and learning rate 3e-6, except Term-Trans which trains the projection layer with learning rate 0.002. On one typical GPU, the batch size is 1 at most; fine-tuning takes on average one day to converge. Convergence is determined by the loss on a small sample of validation data (MS MARCO) or the validation fold (ClueWeb). In comparison, K-NRM and Conv-KNRM take about 12 hours to converge on MS MARCO and one hour on ClueWeb. On MS MARCO all rankers take about 5% training triples to converge.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATIONS AND ANALYSES</head><p>This section evaluates the performances of BERT-based rankers and studies their behaviors. BERT (Rep) applies BERT on the query and document individually and discard these cross sequence interactions, and its performance is close to random. BERT is an interaction-based matching model and is not suggested to be used as a representation model. The more complex architectures in Multi-Int and Term-Trans perform worse than the simplest BERT (Last-Int), even with a lot of MARCO labels to fine-tune. It is hard to modify the pre-trained BERT dramatically in fine-tuning. End-to-end training may make modifying pre-trained BERT more effective, but that would require more future research in how to make BERT trainable in accessible computing environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Performances</head><p>BERT-based rankers behave rather differently on ClueWeb. Although pre-trained on large corpora and then on MARCO ranking labels, none of BERT models significantly outperforms LeToR on ClueWeb. In comparison, Conv-KNRM (Bing), the same Conv-KNRM model but pre-trained on Bing user clicks <ref type="bibr" target="#b0">[1]</ref>, performs the best on ClueWeb, and much better than Conv-KNRM pretrained on MARCO labels. These results demonstrate that MARCO passage ranking is closer to seq2seq task because of its question-answering focus, and BERT's surrounding context based pre-training excels in this setting. In comparison, TREC ad hoc tasks require different signals other than surrounding context: pre-training on user clicks is more effective than on surrounding context based signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learned Attentions</head><p>This experiment illustrates the learned attention in BERT, which is the main component of its Transformer architecture.</p><p>Our studies focus on MS MARCO and BERT (Last-Int), the best performing combination in our experiments, and randomly sampled 100 queries from MS MARCO Dev. We group the terms in the candidate passages into three groups: Markers ("[CLS]" and "[SEP]"), Stopwords, and Regular Words. The attentions allocated to each group is shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>The markers received most attention. Removing these markers decreases the MRR by 15%: BERT uses them to distinguish the two text sequences. Surprisingly, the stopwords received as much attention as non-stop words, but removing them has no effect in MRR performances. BERT learned these stopwords not useful and dumps redundant attention weights on them.</p><p>As the network goes deeper, less tokens received the majority of other tokens attention: the attention spreads more on the whole sequence and the embeddings are contextualized. However, this does not necessarily lead to more global matching decisions, as studied in the next experiment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Learned Term Matches</head><p>This experiment studies the learned matching patterns in BERT (Last-Int) and compares it to Conv-KNRM. The same MS MARCO Dev sample from last experiment is used.</p><p>We first study the influence of a term by comparing the ranking score of a document with and without the term. For each querypassage pair, we randomly remove a non-stop word, calculate the ranking score using BERT (Last-Int) or Conv-KNRM, and plot it w.r.t the original ranking score in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> illustrates two interesting behaviors of BERT. First, it assigns more extreme ranking scores: most pairs receive either close to 1 or 0 ranking scores in BERT, while the ranking scores in Conv-KNRM are more uniformly distributed. Second, there are a few terms in each document that determine the majority of BERT's ranking scores; removing them significantly changes the ranking score-drop from 1 to near 0, while removing the majority of terms does not matter much in BERT-most points are grouped in the corners. It indicates that BERT is well-trained from the large scale pre-training. In comparison, terms contribute more evenly in Conv-KNRM; removing single term often varies the ranking scores of Conv-KNRM by some degree, shown by the wider band near the diagonal in Figure <ref type="figure" target="#fig_1">2</ref>, but not as dramatically as in BERT.</p><p>We manually examined those most influential terms in BERT (Last-Int) and Conv-KNRM. Some examples of those terms are listed in Table <ref type="table" target="#tab_2">2</ref>. The exact match terms play an important role in BERT (Last-Int); we found many of the influential terms in BERT are those appear in the question or close paraphrases. Conv-KNRM, on the other hand, prefers terms that are more loosely related to the query in search <ref type="bibr" target="#b0">[1]</ref>. For example, on MS MARCO, it focuses more on the terms that are the role of milk in macchiato ("visible mark"), the show and the role Sinbad played ("Cosby" and "Coach Walter"), and the task related to Personal Meeting ID ("schedule").</p><p>These observations suggest that, BERT's pre-training on surrounding contexts favors text sequence pairs that are closer in their semantic meanings. It is consistent with previous observations in Neu-IR research, that such surrounding context trained models are not as effective in TREC-style ad hoc document ranking for keyword queries <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>.  Our results suggest the need of training deeper networks on user clicks signals. In the future, it will be interesting to study how a much deeper model-as big as BERT-behaves compared to both shallower neural rankers when trained on relevance-based labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The attentions to Markers, Stopwords, and Regular Words in BERT (Last-Int). X-axes mark layer levels from shallow (1) to deep (24). Y-axes are the number of tokens sending More than Average or Majority attentions to each group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Influences of removing regular terms in BERT (Last-Int) and Conv-KNRM on MS MARCO. Each point corresponds to one query-passage pair with a random regular term removed from the passage. X-axes mark the original ranking scores and Y-axes are the scores after term removal.</figDesc><graphic url="image-1.png" coords="4,56.79,83.69,116.01,116.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Query: "What is a macchiato coffee drink" BERT (Last-Int) macchiato, coffee Conv-KNRM visible mark Query: "What shows was Sinbad on" BERT (Last-Int) Sinbad Conv-KNRM Cosby, Coach Walter Query: "What is a PMI id" BERT (Last-Int) PMI Conv-KNRM schedule a meeting 5 CONCLUSIONS AND FUTURE DIRECTION This paper studies the performances and behaviors of BERT in MS MARCO passage ranking and TREC Web Track ad hoc ranking tasks. Experiments show that BERT is an interaction-based seq2seq model that effectively matches text sequences. BERT based rankers perform well on MS MARCO passage ranking task which is focused on question-answering, but not as well on TREC ad hoc document ranking. These results demonstrate that MS MARCO, with its QA focus, is closer to the seq2seq matching tasks where BERT's surrounding context based pre-training fits well, while on TREC ad hoc document ranking tasks, user clicks are better pre-training signals than BERT's surrounding contexts. Our analyses show that BERT is a strong matching model with globally distributed attentions over the entire contexts. It also assigns extreme matching scores to query-document pairs; most pairs get either one or zero ranking scores, showing it is well tuned by pre-training on large corpora. At the same time, pre-trained on surrounding contexts, BERT prefers text pairs that are semantically close. This observation helps explain BERT's lack of effectiveness on TREC-style ad hoc ranking which is considered to prefer pretraining from user clicks than surrounding contexts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ranking performances. Relative performances in percentages are compared to LeToR, the feature-based learning to rank. Statistically significant improvements are marked by † (over Base), ‡ (over LeToR), § (over K-NRM), and ¶ (over Conv-KNRM). Neural methods on ClueWeb are pre-trained on MS MARCO, except Conv-KNRM (Bing) which is trained on user clicks.</figDesc><table><row><cell></cell><cell cols="4">MS MARCO Passage Ranking</cell><cell cols="4">ClueWeb09-B Ad hoc Ranking</cell></row><row><cell>Method</cell><cell cols="2">MRR@10 (Dev)</cell><cell cols="2">MRR@10 (Eval)</cell><cell cols="2">NDCG@20</cell><cell cols="2">ERR@20</cell></row><row><cell>Base</cell><cell>0.1762</cell><cell cols="4">−9.45% 0.1649 +13.44% 0.2496  §</cell><cell cols="2">−6.89% 0.1387</cell><cell>−14.25%</cell></row><row><cell>LeToR</cell><cell>0.1946</cell><cell>-</cell><cell>0.1905</cell><cell>-</cell><cell>0.2681</cell><cell>-</cell><cell>0.1617</cell><cell>-</cell></row><row><cell>K-NRM</cell><cell>0.2100  † ‡</cell><cell cols="4">+7.92% 0.1982 +4.04% 0.1590</cell><cell cols="2">−40.68% 0.1160</cell><cell>−28.26%</cell></row><row><cell>Conv-KNRM</cell><cell>0.2474  † ‡ §</cell><cell cols="4">+27.15% 0.2472 +29.76% 0.2118  §</cell><cell cols="2">−20.98% 0.1443  §</cell><cell>−10.78%</cell></row><row><cell>Conv-KNRM (Bing)</cell><cell>n.a.</cell><cell>n.a.</cell><cell>n.a.</cell><cell>n.a.</cell><cell cols="4">0.2872  † ‡ §  ¶ +7.12% 0.1814  † ‡ §  ¶ +12.18%</cell></row><row><cell>BERT (Rep)</cell><cell>0.0432</cell><cell cols="4">−77.79% 0.0153 −91.97% 0.1479</cell><cell cols="2">−44.82% 0.1066</cell><cell>−34.05%</cell></row><row><cell>BERT (Last-Int)</cell><cell cols="5">0.3367  † ‡ §  ¶ +73.03% 0.3590 +88.45% 0.2407  §  ¶</cell><cell cols="2">−10.22% 0.1649  † §  ¶</cell><cell>+2.00%</cell></row><row><cell>BERT (Mult-Int)</cell><cell cols="5">0.3060  † ‡ §  ¶ +57.26% 0.3287 +72.55% 0.2407  §  ¶</cell><cell cols="2">−10.23% 0.1676  † §  ¶</cell><cell>+3.64%</cell></row><row><cell cols="2">BERT (Term-Trans) 0.3310  † §  ¶</cell><cell cols="4">+70.10% 0.3561 +86.93% 0.2339  §  ¶</cell><cell cols="2">−12.76% 0.1663  † §  ¶</cell><cell>+2.81%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note>lists the evaluation results on MS MARCO (left) and ClueWeb (right). BERT-based rankers are very effective on MS MARCO: All interaction-based BERT rankers improved Conv-KNRM, a previous state-of-the-art, by 30%-50%. The advantage of BERT in MS MARCO lies in the cross query-document attentions from the Transformers:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Example of most influential terms in MS MARCO passages in BERT and Conv-KNRM.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://msmarco.org</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search</title>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WSDM 2018</title>
				<meeting>WSDM 2018</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="126" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A deep relevance matching model for ad-hoc retrieval</title>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM 2016</title>
				<meeting>CIKM 2016</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04085</idno>
		<title level="m">Passage Re-ranking with BERT</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeprank: A new deep architecture for relevance ranking in information retrieval</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM 2017</title>
				<meeting>CIKM 2017</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeuIPS 2017</title>
				<meeting>NeuIPS 2017</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2017</title>
				<meeting>SIGIR 2017</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relevance-based word embedding</title>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2017</title>
				<meeting>SIGIR 2017</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="505" to="514" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
