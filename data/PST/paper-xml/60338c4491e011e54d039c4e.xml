<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interest-aware Message-Passing GCN for Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-19">19 Feb 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fan</forename><surname>Liu</surname></persName>
							<email>liufancs@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">Shandong University § Shandong Artificial Intelligence Institute</orgName>
								<orgName type="institution" key="instit2">Qilu University of Technology (Shandong Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">Shandong University § Shandong Artificial Intelligence Institute</orgName>
								<orgName type="institution" key="instit2">Qilu University of Technology (Shandong Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">Shandong University § Shandong Artificial Intelligence Institute</orgName>
								<orgName type="institution" key="instit2">Qilu University of Technology (Shandong Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Shandong Normal University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Shandong Normal University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Shandong Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zan</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
							<email>nieliqiang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">Shandong University § Shandong Artificial Intelligence Institute</orgName>
								<orgName type="institution" key="instit2">Qilu University of Technology (Shandong Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">Shandong University § Shandong Artificial Intelligence Institute</orgName>
								<orgName type="institution" key="instit2">Qilu University of Technology (Shandong Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution" key="instit1">Shandong University § Shandong Artificial Intelligence Institute</orgName>
								<orgName type="institution" key="instit2">Qilu University of Technology (Shandong Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Interest-aware Message-Passing GCN for Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-19">19 Feb 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449986</idno>
					<idno type="arXiv">arXiv:2102.10044v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommendation</term>
					<term>Graph Convolution Networks</term>
					<term>Message-Passing Strategy</term>
					<term>Interest-aware</term>
					<term>Subgraph</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolution Networks (GCNs) manifest great potential in recommendation. This is attributed to their capability on learning good user and item embeddings by exploiting the collaborative signals from the high-order neighbors. Like other GCN models, the GCN based recommendation models also suffer from the notorious over-smoothing problem -when stacking more layers, node embeddings become more similar and eventually indistinguishable, resulted in performance degradation. The recently proposed LightGCN and LR-GCN alleviate this problem to some extent, however, we argue that they overlook an important factor for the over-smoothing problem in recommendation, that is, high-order neighboring users with no common interests of a user can be also involved in the user's embedding learning in the graph convolution operation. As a result, the multi-layer graph convolution will make users with dissimilar interests have similar embeddings. In this paper, we propose a novel Interest-aware Message-Passing GCN (IMP-GCN) recommendation model, which performs high-order graph convolution inside subgraphs. The subgraph consists of users with similar interests and their interacted items. To form the subgraphs, we design an unsupervised subgraph generation module, which can effectively identify users with common interests by exploiting both user feature and graph structure. To this end, our model can avoid propagating negative information from high-order neighbors into embedding learning. Experimental results on three large-scale benchmark datasets show that our model can gain performance improvement by stacking more layers and outperform the state-of-the-art GCN-based recommendation models significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Recommender systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommendation system has become one of the most important techniques for various online platforms. It can not only provide personalized information for an specific user from overwhelming information, but also increase the revenue for service providers. Among them, Collaborative filtering (CF) based models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref> have made substantial progress in learning user and item representations by modeling historical user-item interactions. For example, matrix factorization (MF) can directly embed user/item as a feature vector and model the user-item interactions with inner product <ref type="bibr" target="#b0">[1]</ref>. Neural collaborative filtering models replace the MF interaction function of inner product with nonlinear neural networks to learn better user and item representations <ref type="bibr" target="#b14">[15]</ref>.</p><p>Recently, GCN-based models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> have achieved great success in recommendation due to the powerful capability on representation learning from non-Euclidean structure. The core of GCN-based models is to iteratively aggregate feature information from local graph neighbors. It has been proved to be an efficient way to distill additional information from graph structure, and thus improves user and item representation learning and alleviates the sparse problem. For example, NGCF <ref type="bibr" target="#b32">[33]</ref> has proved that exploiting high-order connectivity can help alleviate the sparsity problem in recommendation. However, it is also well-recognized that GCNs suffer from the over-smoothing problem <ref type="bibr" target="#b32">[33]</ref>, because the graph convolution operation is actually a special kind of graph Laplacian smoothing <ref type="bibr" target="#b32">[33]</ref>, making node representations become indistinguishable after multi-layer graph convolution <ref type="bibr" target="#b39">[40]</ref>. As a result, most current GCN based models obtain their peak performance by stacking only few layers (e.g., 2 or 3 layers), and continuing increasing the depth will lead to sharp performance degradation. In the domain of recommendation, Chen et al. <ref type="bibr" target="#b2">[3]</ref> have empirically demonstrated that the user/item embeddings become more similar when stacking more layers in NGCF due to the over-smoothing effect. In other words, the preferences of different users become homogeneous, resulted in performance degradation in recommendation. Based on the observations, they proposed a LR-GCN model, which removes the non-linearities in GCNs to simply the network structure and introduced a residual network structure to alleviate the over-smoothing problem, achieving substantially improvement over NGCF on recommendation accuracy.</p><p>It is worth mentioning that the LightGCN proposed by He et al. <ref type="bibr" target="#b13">[14]</ref> has a similar formulation as LR-GCN. With careful experimental studies, He et al. pointed out that the feature transformation and nonlinear activation have no positive effect (or even negative effect due to the increase of training difficult) to the final performance. Therefore, they only keep the neighborhood aggregation in the LightGCN for collaborative filtering. Comparing to LR-GCN, LightGCN further removes the "self-loop" in the aggregation operation. Although LightGCN is not dedicatedly designed for tacking the over-smoothing problem, it has almost the same formulation as LR-GCN and thus can also alleviate the over-smoothing problem to some extent. In fact, both LR-GCN and LightGCN are consistent with the recent theories in simplifying GCNs <ref type="bibr" target="#b36">[37]</ref> and can obtain the best performance with a deeper structure (e.g., 4 layers). Despite the two success GCN based models are designed for recommendation, we argue that they still design the model from the perspective of graph convolution, while have not well considered the over-smoothing problem in the domain of recommendation.</p><p>The GCN based recommendation model is built upon a useritem graph, in which the user and item are linked according to the historical user-item interactions. The user embedding is learned by iteratively aggregating messages passed from the neighboring (both user and item) nodes. Note that the passed messages are distilled from the embeddings of neighboring nodes. When stacking 𝑘 layers, the information from the 𝑘-order neighbors, which are indirectly connected via items and users, are also involved in the embedding learning of a target node. An underlying assumption is that the collaborative signals from high-order neighbors are beneficial to the embedding learning. However, not all the information from high-order neighbors are positive in reality. In the user-item interaction graph, the high-order neighboring users could have no common or even contradictory interest with a target user. This is highly possible, especially when the graph is constructed based on implicit feedbacks (e.g., click). In fact, the implicit feedback is more widely used over the explicit feedbacks in modern recommendation systems. The core idea behind collaborative filtering is that similar users like similar items. Therefore, the collaborative signals that we would like to exploit should be from similar users (i.e., users with similar interests). However, existing GCN-based recommendation models have not distinguished the high-order neighbors, and just simply aggregate the messages from all those neighbors to update user embeddings. As a result, the embeddings of dissimilar users are also involved in the embedding learning of a target user, negatively affecting the performance. This is also a reason of the over-smoothing effect in the GCN-based recommendation models -making the embeddings of dissimilar users to be similar.</p><p>Motivated by the above considerations, in this paper, we propose a novel Interest-aware Messaging-Passing GCN (IMP-GCN) recommendation model, which groups users and their interacted items into different subgraphs and operates high-order graph convolutions inside subgraphs. More specific, we adopt the simplified network structure of LightGCN, as its effectiveness has been well demonstrated in <ref type="bibr" target="#b13">[14]</ref> and it can alleviate the over-smoothing problem to some extent. The first-order graph convolution is the same as that of LightGCN. For the high-order graph convolution, only the messages from nodes in the same subgraph are exploited to learn the node embeddings. The subgraph is generated by a proposed graph generation module, which integrates users features and graph structure to identify users with similar interests, and then constructs the subgraphs by retaining those users and their interacted items.</p><p>To this end, our model can filter out the negative information propagation in the high-order graph convolution operations for the embedding learning, and thus can keep the uniqueness of users by stacking more graph convolution layers. Extensive experiments have been conducted on three large-scale real-world datasets to validate the effectiveness of our model. Results show that our model outperforms the state-of-the-art methods by a large margin and can obtain better performance with more layers (till 7 layers) <ref type="foot" target="#foot_0">1</ref> . This indicates that our model can benefits from higher-order neighbors by excluding negative nodes. Besides, with deep analysis on the results, we found that the negative information in the embedding propagation is the major reason for the performance degradation of existing GCN-based recommendation models in deep structure. We released the codes and involved parameter settings to facilitate others to repeat this work <ref type="foot" target="#foot_1">2</ref> .</p><p>In summary, the main contributes of this work are as follows: • We step into the over-smoothing problem in existing GCN-based recommendation models and point out an overlooked factor: exploiting high-order neighbors indiscriminately makes the embeddings of users with dissimilar interests to be similar. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">METHODOLOGY 2.1 Recap</head><p>Let 𝑨 ∈ R 𝑁 ×𝑀 be the user-item interaction matrix, where 𝑁 and 𝑀 indicate the number of users and items, respectively. An nonzero entry 𝑎 𝑢𝑖 ∈ 𝑨 indicates that user 𝑢 ∈ U has interacted with item 𝑖 ∈ I before; otherwise, the entry is zero. A user-item bipartite graph G = (W, E) can be constructed based on the interaction matrix, where the node set W consists of the two types of user nodes and item nodes and E represents for the set of edges. For a nonzero 𝑎 𝑢𝑖 , there is an edge between the user 𝑢 and item 𝑖. The above information is taken as the input of GCN model to learn the user and item representations by iteratively aggregating features from neighboring nodes in the bipartite graph.</p><p>Here we take LightGCN as an example to describe the GCN-based recommendation model, because it achieves the state-of-the-art performance with a very light design. Our model is also developed based on its design. <ref type="foot" target="#foot_2">3</ref> Let 𝒆 operation in LightGCN is described as follows:</p><formula xml:id="formula_0">𝒆 (𝒌) 𝒖 = ∑︁ 𝑖 ∈N 𝑢 1 √︁ |N 𝑢 | √︁ |N 𝑖 | 𝒆 (𝒌−1) 𝒊 , 𝒆 (𝒌) 𝒊 = ∑︁ 𝑢 ∈N 𝑖 1 √︁ |N 𝑖 | √︁ |N 𝑢 | 𝒆 (𝒌−1) 𝒖 ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">𝒆 (𝒌)</formula><p>𝒖 and 𝒆 (𝒌) 𝒊 represent the embeddings of the user 𝑢 and item 𝑖 after 𝑘 layers propagation, respectively; N 𝑢 denotes the set of items that interact with user 𝑢, and N 𝑖 denotes the set of users that interact with item 𝑖;</p><formula xml:id="formula_2">1 √ | N 𝑢 | √ | N 𝑖 |</formula><p>is symmetric normalization terms, which can avoid the scale of embeddings increasing with graph convolution operations <ref type="bibr" target="#b17">[18]</ref>. After K layers graph convolution, the final embeddings of a user 𝑢 and an item 𝑖 are the combination of their embeddings obtained at each layer in LightGCN:</p><formula xml:id="formula_3">𝒆 𝒖 = 𝐾 ∑︁ 𝑘=0 𝛼 𝑘 𝒆 (𝒌) 𝒖 ; 𝒆 𝒊 = 𝐾 ∑︁ 𝑘=0 𝛼 𝑘 𝒆 (𝒌) 𝒊 ,<label>(2)</label></formula><p>where 𝛼 𝑘 ≥ 0 is a hyper-parameter assigned to the k-th layer. It denotes the importance of this layer in constituting the final embedding. From Eq. 2, it is expected that after iteratively aggregating features from higher-order neighbors, the nodes will fail to preserve their own distinct features and their embeddings become more and more similar, leading to the over-smoothing problem. Besides, it does not distinguish the heterogeneous features of high-order nodes in the aggregation process. The noisy information from high-order neighbors could hurt the embedding learning. For example, the embeddings of users with no common interests or even contradictory interests in the high-order neighbors are aggregated to learn a target user's embedding via the graph convolution operation. Fig. <ref type="figure" target="#fig_0">1</ref> shows the average coverage ratio of the number of nodes that a target node reaches in the propagation by stacking different numbers of layers to all the nodes in the graph. It can be seen that after 6-or 7-layer graph convolution, a node can almost receive information from all the other nodes in embedding propagation. Therefore, by aggregating information from all the connected highorder neighbors, it is unavoidable that the node embeddings become homogeneous in the current GCN-based models after stacking more layers, especially for the densely connected ones, whose embeddings will become more and more similar. In the recommendation scenario, this means the uniqueness of users will be neglected in deep structure.</p><p>Actually, current GCN-based recommendation models achieve their peak performance at most 3 or 4 layers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b36">37]</ref>. Besides the over-smoothing effect, we deem that a node also takes noisy or negative information in the embedding propagation process, which hurts the final performance. This is because a user's interests often span a range of items. Different users can have very different interests or even exhibit contradictory attitudes to some items. Without distinguishing those users, the embedding propagation may perform among users with very different interests to learn their embeddings in the graph convolution operation. To avoid the situation and alleviate the over-smoothing problem, it is important to group users with similar interests (and their interacted items) into subgraphs and constrain the embedding propagation to operate inside the subgraph. To achieve the goal we propose the interest-aware message-passing GCN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">IMP-GCN MODEL</head><p>2.2.1 Interest-aware Message-passing Strategy. With constructing subgraphs, we would like that all the information propagated in a subgraph can contribute to the embedding learning of all the nodes in this subgraph. In other words, we aim to exclude the negative information propagation in the graph convolution operation using subgraphs. To achieve the goal, we rely on user nodes to form subgraphs in the user-item bipartite graph. The general idea is that users with more similar interests are grouped into a subgraph, and the items which directly linked to those users also belong to this subgraph. Therefore, each user only belongs to one subgraph, and an item can be associated with multiple subgraph. Let 𝐺 𝑠 with 𝑠 ∈ {1, • • • , 𝑁 𝑠 } denotes a subgraph, where 𝑁 𝑠 is the number of subgraphs. In the next, we introduce the graph convolution operation in our model.</p><p>Because the direct interactions between users and items provide the most important and reliable information of user interests, in the first-order propagation, all the first-order neighbors are involved in the graph convolution operation. Let 𝒆 (0) 𝒖 and 𝒆 (0) 𝒊 denote the ID embeddings of user 𝑢 and item 𝑖, respectively. The first-order graph convolution is:</p><formula xml:id="formula_4">𝒆 (1) 𝒖 = ∑︁ 𝑖 ∈N 𝑢 1 √︁ |N 𝑢 | √︁ |N 𝑖 | 𝒆 (0) 𝒊 , 𝒆<label>(1)</label></formula><formula xml:id="formula_5">𝒊 = ∑︁ 𝑢 ∈N 𝑖 1 √︁ |N 𝑖 | √︁ |N 𝑢 | 𝒆 (0) 𝒖 ,<label>(3)</label></formula><p>where 𝒆</p><p>𝒖 and 𝒆</p><p>(1) 𝒊 represent the first layer embeddings of the target user 𝑢 and item 𝑖, respectively.</p><p>For the high-order graph convolution, to avoid introducing noisy information, a node in a subgraph can only exploit the information from its neighbor nodes in this subgraph. Because the items interacted by a user all belong to the subgraph of this user, the user can still receive information from all the linked items. However, for an item node, its direct user neighbors can be distributed in different </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User-item Interaction Graph G</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>First-order Graph Convolution Layer</head><formula xml:id="formula_7">u 1 u 2 u 3 u 4 i 1 i 2 i 3 i 4 u 1 u 2 u 3 u 4 i 1 i 2 i 3 i 4 u 1 u 2 u 3 u 4 i 1 i 2 i 3 i 4 u 1 u 2 u 3 u 4 i 1 i 2 i 3 i 4 u 1 u 2 u 3 u 4 i 1 i 2 i 3 i 4 u 1 u 2 u 3 u 4 i 1 i 2 i 3 i 4 Layer k-1 u 1 u 2 u 3 u 4 i 1 i 2 i 3 i 4 u 1 u 2 u 3 u 4 i 1 i 2 i 3 i 4 Layer 1 … Layer 1 u 1 u 2 u 3 u 4 i 1 i 2 i 3 i 4 u 1 u 2 u 3 u 4 i 1 i 2 i 3 i 4 … u 1 u 2 u 3 u 4 i 1 i 2 i 3 i 4 u 1 u 2 u 3 u 4 i 1 i 2 i 3 i 4 Layer k-1 Subgraph G1 Subgraph G2 High-order Graph Convolution Layer … 𝑬 (𝒌−𝟏) Layer 1 (G)</formula><p>Figure <ref type="figure">2</ref>: An overview of our IMP-GCN model with two subgraphs as illustration. In IMP-GCN, the first-order propagation operates on whole graph, and high-order propagation operates inside the subgraphs. subgraphs. To learn the embeddings of an item 𝑖, for each subgraph 𝐺 𝑠 it belongs to, we learn an embedding for this item. Let 𝒆 (𝒌) 𝒊𝒔 denotes the embedding of item 𝑖 in subgraph 𝑠 after 𝑘 layers graph convolution, the high-order propagation in IMP-GCN is defined as:</p><formula xml:id="formula_8">𝒆 (𝒌+1) 𝒖 = ∑︁ 𝑖𝑠 ∈N 𝑢 1 √︁ |N 𝑢 | √︁ |N 𝑖 | 𝒆 (𝒌) 𝒊𝒔 , 𝒆 (𝒌+1) 𝒊𝒔 = ∑︁ 𝑢 ∈N 𝑠 𝑖 1 √︁ |N 𝑖 | √︁ |N 𝑢 | 𝒆 (𝒌) 𝒖 .<label>(4)</label></formula><p>In this way, we guarantee that the embedding of a node learned in a subgraph only contributes to the embedding learning of other nodes in this subgraph. This can avoid the noisy information propagated from unrelated nodes. 𝒆</p><formula xml:id="formula_9">(•)</formula><p>𝒊𝒔 can be regarded as the features learned from the users with a similar interest in the subgraph 𝐺 𝑠 . This make senses since users with similar interests often prefer the same feature of an item. The final representation of an item 𝑖 after 𝑘 layers graph convolution is a combination of its embeddings learned in different subgraphs, i.e., 𝒆</p><formula xml:id="formula_10">(𝒌) 𝒊 = ∑︁ 𝑠 ∈S 𝒆 (𝒌) 𝒊𝒔 ,<label>(5)</label></formula><p>where 𝑆 is the subgraph set that item 𝑖 belongs to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Layer Combination and Prediction.</head><p>We combine the embeddings obtained at each layer to form the final representation of user 𝑢 and item 𝑖 as Eq. 2. Similar to LightGCN, 𝛼 𝑘 is set uniformly as 1/(𝐾 + 1) <ref type="bibr" target="#b13">[14]</ref>.</p><p>With the learned embeddings of users (i.e., 𝒆 𝑢 ) and items 𝒆 𝑖 , given a user 𝑢 and a target item 𝑖, the preference of the user to the item is computed by inner product:</p><formula xml:id="formula_11">r𝑢𝑣 = 𝒆 𝑇 𝒖 𝒆 𝒊 .<label>(6)</label></formula><p>Notice that other interaction functions can be also applied, such as Euclidean distance. Because the main focus of this work is to study the effects of distinguishing user interests in the graph convolution in the GCN-based recommendation model, we adopt the inner product as previous work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b41">42]</ref> for fair comparisons in the empirical studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.3</head><p>Matrix-form propagation rule. We implement our algorithm with the matrix form propagation rule (see <ref type="bibr" target="#b32">[33]</ref> for more details), by which we can simultaneously update the representations of all users and items in a rather efficient way. It is a commonly used approach to make graph convolution network feasible for large-scale graph <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33]</ref>. Let 𝑬 (0) be the representations matrix for users ID and items ID; 𝑬 (𝒌) represents the representation of users and items at the 𝑘-th layer. Similarly, 𝑬 (𝒌) 𝒔 is defined as the representation of users and items at the 𝑘-th layer in subgraph 𝐺 𝑠 . As shown in Fig. <ref type="figure">2</ref>, the first layer embedding propagation in our model can be described as follows:</p><formula xml:id="formula_12">𝑬 (1) = L𝑬 (0) ,<label>(7)</label></formula><p>where L is the Laplacian matrix for the user-item interaction graph. As we involve the subgraphs in high-order graph convolution layers,the embeddings propagation on subgraphs is formulated as follows:</p><p>𝑬</p><formula xml:id="formula_13">(𝒌−1) 𝒔 = L 𝑠 𝑬 (𝒌−2) 𝒔 ,<label>(8)</label></formula><p>where 𝑘 ⩾ 2; L 𝑠 represent the Laplacian matrix for the subgraph 𝐺 𝑠 . And then, the (𝑘 − 1)-th layer embeddings are propagated on the user-item graph and obtained the embeddings in the 𝑘-th layer:</p><formula xml:id="formula_14">𝑬 (𝒌) 𝒔 = L𝑬 𝒔 (𝒌−1) . (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>We aggregate all the 𝑘-th layer embeddings involved different subgraphs to formulate the final 𝑘-th layer embeddings:</p><formula xml:id="formula_16">𝑬 (𝒌) = ∑︁ 𝑠 ∈𝐺 𝑠 𝑬 (𝒌) 𝒔 .<label>(10)</label></formula><p>Lastly, we combine all the layers' embeddings and get the final representations of users and items, this formulation keeps consistent with it in LightGCN <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_17">𝑬 = 𝛼 0 𝑬 (0) + 𝛼 1 𝑬 (1) + • • • + 𝛼 𝐾 𝑬 (𝑲 )<label>(11)</label></formula><p>2.2.4 Optimization. In this work, we target at the top-𝑛 recommendation, which aims to recommend a set of 𝑛 top-ranked items matching the target user's preference. Compared to rating prediction, this is a more practical task in real commercial systems <ref type="bibr" target="#b26">[27]</ref>. Similar to other rank-oriented recommendation works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b41">42]</ref>, we adopt the pairwise learning method for optimization. To perform the pairwise learning, it needs to constructs a triplet of {𝑢, 𝑖 + , 𝑖 − }, with an observed interaction between 𝑢 and 𝑖 + and an unobserved interaction between 𝑢 and 𝑖 − . This method assumes that a positive item (i.e., 𝑖 + ) should rank higher than an negative item (i.e., 𝑖 − ). The objective function is formulated as:</p><formula xml:id="formula_18">arg min ∑︁ (u,i + ,i − ) ∈ O − ln 𝜙 ( r𝑢𝑖 + − r𝑢𝑖 − ) + 𝜆 ∥Θ∥ 2 2 (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>where O = {(𝑢, 𝑖 + , 𝑖 − )|(𝑢, 𝑖 + ) ∈ R + , (𝑢, 𝑖 − ) ∈ R − } denotes the training set; R + indicates the observed interactions between user 𝑢 and 𝑖 + in the training dataset, and R − is the sampled unobserved interaction set. 𝜆 and Θ represent the regularization weight and the parameters of the model, respectively. The 𝐿 2 regularization is used to prevent overfitting. The mini-batch Adam <ref type="bibr" target="#b16">[17]</ref> is adopted to optimize the prediction model and update the model parameters. Specifically, for a batch of randomly sampled triples (𝑢, 𝑣 + , 𝑣 − ) ∈ (𝑂), the representation of those users and items are first learned by the propagation rules and then the model parameters are updated by using the gradients of the loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Subgraph Generation Module</head><p>In this section, we introduce our proposed subgraph generation module which is designed to construct the subgraphs 𝐺 𝑠 with 𝑠 ∈ {1, • • • , 𝑁 𝑠 } from a given input graph G. Remind that the subgraphs are used to group users with common interests in our model. We formulate the user grouping as a classification task, i.e., each user is classified to a group. Specifically, each user is represented by a feature vector, which is a fusion of the graph structure and the ID embedding:</p><formula xml:id="formula_20">𝑭 𝒖 = 𝜎 (𝑾 1 (𝒆 (0) 𝒖 + 𝒆 (1) 𝒖 ) + 𝒃 1 ),<label>(13)</label></formula><p>where 𝑭 𝒖 is the obtained user feature via feature fusion. 𝒆</p><p>𝒖 is the embedding of user ID and 𝒆 <ref type="bibr" target="#b0">(1)</ref> 𝒖 is the feature obtained by aggregating local neighbor in the graph (i.e., the user embedding after the first layer propagation.). 𝑾 1 ∈ 𝑅 𝑑×𝑑 and 𝒃 1 ∈ 𝑅 1×𝑑 are respectively the trainable weight matrix and bias vector of the fusion method. 𝜎 is the activation function. LeakyReLU <ref type="bibr" target="#b23">[24]</ref> is adopted, because it can encode both positive and small negative signals. To classify the users into different subgraphs, we cast the obtained user feature to a prediction vector with a 2-layer neural networks:</p><formula xml:id="formula_22">𝑼 𝒉 = 𝜎 (𝑾 2 𝑭 𝒖 + 𝒃 2 ), 𝑼 𝒐 = 𝑾 3 𝑼 𝒉 + 𝒃 3 ,<label>(14)</label></formula><p>where 𝑼 𝒐 is the prediction vector. The position of maximum value in 𝑈 𝑜 represents which group/subgraph the user belongs to. 𝑾 2 ∈ 𝑅 𝑑×𝑑 , 𝑾 3 ∈ 𝑅 𝑑×𝑁 𝑠 and 𝒃 2 ∈ 𝑅 1×𝑑 , 𝒃 3 ∈ 𝑅 1×𝑁 𝑠 are respectively the trainable weight matrices and bias vectors of the two layers. The dimension of the prediction vector dimensions is the same as the number of subgraphs, which is a pre-selected hyper-parameter. Note that it is an unsupervised method to classify users into different groups and thus does not need ground-truth label. For users with similar embeddings, Eq. 14 will generate similar prediction vector, namely, they will be classified into the same group. The subgraph generation aims to construct a matrix, which represents the user-item adjacency relation in a subgraph based on the user grouping results and the Laplacian matrix of the original user-item graph. For the matrix of each subgraph, according to the obtained user group information, we filter out the user-item adjacency relations in the Laplacian matrix of the original user-item graph if the corresponding users are not in the user group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS 3.1 Experimental Setup</head><p>3.1.1 Data Description. To evaluate the effectiveness of IMP-GCN, we conducted experiments on three benchmark datasets: Amazon-Kindle Store, Amazon-Home&amp;Kitchen and Gowalla. The first two datasets are from the public Amazon review dataset <ref type="foot" target="#foot_3">4</ref> , which has been widely used for recommendation evaluation in previous studies. The third dataset is a check-in dataset collected from Gowalla, where users share their locations by checking-in. We followed the general setting in recommendation to filter users and items with few interactions. For all the datasets, we used the 10-core settings, i.e., retaining users and items with at least 10 interactions. The statistics of three datasets are shown in Table <ref type="table" target="#tab_1">1</ref>. As we can see, the datasets are of different sizes and sparsity levels, which are useful for analyzing the performance of our method and the competitors in different situations.</p><p>For each datasets, we randomly split it into training, validation, and testing set with the ratio 80:10:10 for each user. The observed user-item interactions were treated as positive instances. For the methods which adopt the pairwise learning strategy, we randomly sample a negative instance, that the user did not consume before, to pair with each positive instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Evaluation Metrics.</head><p>For each user in the test set, we treat all the items that the user did not interact with as negative items. Two widely used evaluation metrics for top-𝑛 recommendation are adopted in our evaluation: Recall and Normalized Discounted Cumulative Gain <ref type="bibr" target="#b12">[13]</ref>. For each metric, the performance is computed based on the top 20 results. Notice that the reported results are the average values across all the testing users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Experimental Settings.</head><p>We implemented our model with Tensorflow <ref type="foot" target="#foot_4">5</ref> and carefully tuned the key parameters. The embedding size is fixed to 64 for all models and the embedding parameters are initialized with the Xavier method <ref type="bibr" target="#b38">[39]</ref>. We optimized our method with Adam <ref type="bibr" target="#b16">[17]</ref> and used the default learning rate of 0.001 and default mini-batch size of 1024 (on gowalla, we increased the minibatch size to 2048 for speed). The 𝐿 2 regularization coefficient 𝜆 is searched in the range of {1𝑒 −6 , 1𝑒 −5 , • • • , 1𝑒 −2 }. The early stopping and validation strategies are kept the same as those in LightGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Study of IMP-GCN</head><p>In this section, we first evaluated the performance of our IPM-GCN model when stacking different layers in graph convolution. This is to examine whether our interest-aware message-passing strategy can alleviate the over-smoothing problem. In the next, we study the effects of the subgraph numbers on the performance of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Effect of Layer Numbers.</head><p>To investigate the effectiveness of IMP-GCN in deeper structure, we increased the model depth and performed detailed comparison with LightGCN. Since the adopted message-passing strategy is the same as LightGCN in the first-order convolution layer, we increased the layer number from 2 to 7. The experimental results are shown in Fig. <ref type="figure" target="#fig_2">3</ref>, in which IMP-GCN 2 , IMP-GCN 3 and IMP-GCN 4 indicate the model with 2, 3, and 4 subgraphs, respectively. We omitted the results on 𝐻𝑜𝑚𝑒&amp;𝐾𝑖𝑡𝑐ℎ𝑒𝑛 for space limitation, because they show exactly the same trend. From the results, we had some interesting observations. Firstly, the proposed IMP-GCN outperforms LightGCN consistently when stacking more than 2 or 3 layers over both datasets. This indicates that our model can learn better embeddings by the interest-aware message-passing strategy. Secondly, the peak performance of LightGCN is obtained when stacking 3 or 4 layers, and increasing more layers will cause dramatic performance degradation, indicating it suffers from the over-smoothing problem in a deep structure. In contrast, IMP-GCN continues to achieve better performance with deeper structure (notice that when stacking more than 7 layers, a node already aggregates information from almost all the nodes, see Fig. <ref type="figure" target="#fig_0">1</ref>. The results demonstrate the capability of our model on alleviating the over-smoothing problem. Moreover, it also 1) justifies our claim that exploiting information from all nodes indiscriminately causes the over-smoothing in GCN-based recommendation model, and 2) validates the effectiveness of our  subgraph generation algorithm on classifying users with common interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Effect of Subgraph.</head><p>The performance of IPM-GCN with different numbers (i.e., {2, 3, 4}) of subgraphs can also be observed in Fig. <ref type="figure" target="#fig_2">3</ref>. From the results, we can see that the (1) IMP-GCN 2 with 2 subgraphs can obtain the best results when stacking no more than 3 layers. This is because a node in the subgraphs of IMP-GCN 2 can reach more nodes in short distance than the on in IMP-GCN 3 or IMP-GCN 4 in the embedding propagation operation. ( <ref type="formula" target="#formula_3">2</ref>) When stacking more than 3 layers, IMP-GCN 3 performs the best. After 3 layers graph convolution, the number of involved nodes increasing sharply in embedding propagation (see the examples in Fig. <ref type="figure" target="#fig_0">1</ref>). On average, each node in IMP-GCN 2 should reach more nodes that the one in IMP-GCN 3 and IMP-GCN 4 , however, the performance improvement of IMP-GCN 2 is smaller or even negative (on th Kindle Stores) than that of IMP-GCN 3 and IMP-GCN 4 . This indicates that there is still noisy information in embedding propagation by discriminating user interests in a coarse-level (i.e., 2 subgraphs), negatively impacting the performance. Note that IMP-GCN 3 can still benefit from high-order neighbors. (3) With more subgraphs, on the one hand, IMP-GCN 4 can distinguish users with similar interests in a finer level and thus can better distill information from high-order neighbors; on the other hand, it also cuts more connections to other nodes, especially the ones in short distance which provide more valuable information in embedding learning. As a result, when stacking more layers, its performance is only comparable to that of IMP-GCN 2 . Therefore, there is a trade-off on selecting the number of subgraphs. We further studied the effects of subgraphs by analyzing the average coverage ratio of each node and the corresponding performance based on the LightGCN and our IPM-GCN model. Due to the space limitation, we only provide the results on Kindle Store and omit the performance 𝑤 .𝑟 .𝑡 ndcg which has the similar trend as recall. In this experiment, we used the LightGCN with 4 layers and IPM-GCN with 3 subgraphs<ref type="foot" target="#foot_5">6</ref> and 6 layers, which are their optimal setting on Kindle Store. The average recall and average cover ratio of each user in a subgraph based on LightGCN and IPM-GCN are shown in Fig. <ref type="figure" target="#fig_4">4</ref>(a) and Fig. <ref type="figure" target="#fig_4">4</ref>(b), respectively. Notably, by grouping users with similar interest in subgraphs to make information only propagate inside subgraphs, IPM-GCN can benefit from more layers of graph convolution and distill positive information from high-order neighors. In contrast, LightGCN is limited by the negative information from high-order neighbors and can only gain improvements over 4 layers. Comparing the performance of different subgraphs, we can see that with a higher coverage ratio, the performance of IPM-GCN increases clearly.</p><p>Another interesting finding is that, by stacking 6 layers, a user node in a subgraph almost connects to all the other nodes in the whole graph. This indicates that the users in a subgraph almost interact all the items in the graph (otherwise, the coverage ratio cannot be that high). More importantly, IPM-GCN can still achieve improvement with such high coverage without over-smoothing. This indicates that the embeddings of items learned in a graph contributes to the embedding learning of users in this graph, and the distilled information in a subgraph during graph convolution is useful for the embedding learning for all the nodes in this subgraph. It demonstrates the effectiveness of our interest-aware messagepassing strategy and the subgraph generation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with SOTA Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Baselines.</head><p>To demonstrate the effectiveness, we compared our proposed method with several recently proposed competitive methods, including</p><p>• NeuMF <ref type="bibr" target="#b14">[15]</ref>: It is a state-of-the-art neural collaborative filtering method. This method uses multiple hidden layers above the element-wise and concatenation of user and item embeddings to capture their non-linear feature interactions. • HOP-Rec <ref type="bibr" target="#b41">[42]</ref>: This method exploits the high-order user-item interactions by random walks to enrich the original training data.</p><p>In experiments, we used the codes released by the authors<ref type="foot" target="#foot_6">7</ref> . • CSE <ref type="bibr" target="#b1">[2]</ref>: This recently proposed graph-based model also exploits the high-order proximity in the user-item bipartite graph. Different from HOP-Rec, this method explores the user-user and item-item relations by random walks to improve the performance. We used the codes released by the authors ( the same link as HOP-Rec). The symbol * denotes that the improvement is significant with 𝑝 − 𝑣𝑎𝑙𝑢𝑒 &lt; 0.05 based on a two-tailed paired t-test.</p><p>• GCMC <ref type="bibr" target="#b28">[29]</ref>: This method applies the GCN techniques on useritem bipartite graph and employs one convolutional layer to exploit the direct connections between users and items.</p><p>• NGCF <ref type="bibr" target="#b32">[33]</ref>: This method explicitly encodes the collaborative signal in the form of high-order connectivities by performing embedding propagation in the user-item bipartite graph. • LightGCN <ref type="bibr" target="#b13">[14]</ref>: It is an simplified version of NGCF by removing the feature transformation and nonlinear activation module. It makes GCN-based methods more concise and appropriate for recommendation and achieves the state-of-the-art performance.</p><p>For fair comparisons, all the methods are optimized by the same pairwise learning strategy. We put great efforts to tune these methods based on the validation dataset and reported their best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Overall Comparison.</head><p>Table <ref type="table" target="#tab_2">2</ref> shows the performance comparison results. The best and second best results were highlighted in bold. From the results, we had following observations. The performance of NeuMF is relatively poor as it not explicitly leverages the high-order connectivities between users and items, resulting in suboptimal performance. For the graph-based methods, CSE makes use of the implicit associates of user-user and item-item similarities via high-order neighborhood proximity by performing random walks on the user-item interaction graph. GCMC obtains better performance over CSE, demonstrating the advantages of GCN-based approaches, which can exploit graph structure information. However, it does not perform well on 𝐻𝑜𝑚𝑒&amp;𝐾𝑖𝑡𝑐ℎ𝑒𝑛 because the useful information in neighbors cannot be efficiently aggregated. Hop-Rec outperforms the above methods on the three datasets, because it samples user-item interactions from high-order neighbors to enrich the training data. NGCF achieves consistent much better performance over the above baselines. This is because it adopts the GCN techniques to explicitly and directly exploit the high-order connectivities in the embedding. In contrast, the GCMC method only utilizes the first-order neighbors for representation learning; HOP-Rec and CSE leverage the high-order neighbors to enrich the training data rather than using them in embedding function for direct representation learning. This demonstrates the powerful representation learning capability of GCN and the importance of utilizing high-order information directly in representation learning. Similar to the results reported in <ref type="bibr" target="#b13">[14]</ref>, LightGCN achieves substantially improvement over NGCF by simplifying it with the removal of two common designs in GCN. IMP-GCN outperforms all the baselines consistently over all the datasets. In particular, compared to the strongest baseline in terms of NDCG@20, IMP-GCN can reach a relative improvement over LightGCN by 7.85%, 7.19%, 3.66% on 𝐾𝑖𝑛𝑑𝑙𝑒𝑆𝑡𝑜𝑟𝑒, 𝐻𝑜𝑚𝑒&amp;𝐾𝑖𝑡𝑐ℎ𝑒𝑛 and 𝐺𝑜𝑤𝑎𝑙𝑙𝑎, respectively. The great improvement over LightGCN demonstrates the importance of distinguishing nodes in high-order neighbors in the graph convolution operation, as well as the effectiveness of our proposed interest-aware message-passing strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>In this section, we examined the contribution of different components in our model to the final performance by comparing IMP-GCN with the following two variants:</p><p>• IMP-GCN 𝑠 : This variant removes the graph structure information from the subgraph generation module (i.e., removing 𝒆</p><p>(1) 𝒖 in Eq. 13). • IMP-GCN 𝑓 : In this variant, the first-order propagation is also performed inside each subgraph (i.e., The equation for 𝒆 <ref type="bibr" target="#b0">(1)</ref> 𝒊 in Eq. 3 is replaced with 𝑠 ∈S 𝒆 (1) 𝒊𝒔 ). The results of two variants and IPM-GCN were reported in Table 3, in which the best results are highlighted in bold. IMP-GCN outperforms IMP-GCN 𝑠 over all the datasets, which indicates the effectiveness of employing graph structure information in subgraph generation module. It is expected that IMP-GCN 𝑠 obtains much better performance over IMP-GCN 𝑓 , because the first-order neighbors (i.e., the interaction between users and items) contributes the direct information for user and embedding in the collaborative filtering process. The results also demonstrate the reasonable design of our IPM-GCN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>As one of the most important information retrieval techniques, recommendation has made tremendous progress in past decades. Among various recommendation approaches, the model-based collaborative filtering (CF) <ref type="bibr">[5, 6, 14-16, 19, 20, 27, 32, 33]</ref> achieves a great success and becomes the mainstream recommendation technique. CF learns user and item embeddings by reconstructing the user-item interaction matrix. Earlier research efforts mainly focus on the shallow models, such as BPR <ref type="bibr" target="#b26">[27]</ref>, CML <ref type="bibr" target="#b15">[16]</ref>, matrix factorization (MF) <ref type="bibr" target="#b18">[19]</ref>. Their success motivates the development of various variants via leveraging additional information (e.g., review <ref type="bibr" target="#b24">[25]</ref>, image <ref type="bibr" target="#b11">[12]</ref>, knowledge graph <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>) to deal with different tasks (e.g., context-aware <ref type="bibr" target="#b21">[22]</ref>, session-based <ref type="bibr" target="#b22">[23]</ref>). With the rise of deep learning, it has also been widely applied in recommendation and exhibits great potential by either enhancing the user/item embedding learning or introducing non-linearity into the interaction function, promoting another peak development of recommendation technique. Many DL-based recommendation models have been proposed, such as NeuMF <ref type="bibr" target="#b14">[15]</ref>, Wide&amp;Deep <ref type="bibr" target="#b3">[4]</ref>, and achieved better performance over traditional models.</p><p>Another research line is graph-based recommendation, which can explicitly exploit high-order proximity between users and items. Early approaches infer indirect preference by random walks in the graph to provide recommendation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. The recently proposed approaches exploit the user-item bipartite graph to enrich the useritem interactions <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref> and explore other types of collaborative relations, such as user-user and item-item similar ties <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b43">44]</ref>. For example, HOP-Rec <ref type="bibr" target="#b41">[42]</ref> uses random sample positive user-item interactions to enrich the training data by using random walks. WalkRanker <ref type="bibr" target="#b43">[44]</ref> and CSE <ref type="bibr" target="#b1">[2]</ref> performs random walks to explore the high-order proximity in user-user and item-item relations. As those methods rely on random walks to sample new interactions for model training, their performance heavily depends on the quality of generated interactions by random walks. As a result, these methods need carefully selection and tuning effects.</p><p>In recent years, Graph Convolution Networks (GCNs) have attracted increasing attention in recommendation due to the powerful capability on representation learning from non-Euclidean structure <ref type="bibr">[8, 9, 14, 21, 29, 32-36, 43, 45]</ref>. And then, many GCN-based recommendation models have been developed. For example, GC-MC <ref type="bibr" target="#b28">[29]</ref> employs one convolution layer to exploit the direct connections between users and items; PinSage <ref type="bibr" target="#b42">[43]</ref> combines random walks with multiple graph convolution layers on the item-item graph for Pinterest image recommendation; MEIRec <ref type="bibr" target="#b7">[8]</ref> utilizes metapath-guided neighbors to exploit rich structure information for intent recommendation; NGCF <ref type="bibr" target="#b32">[33]</ref> exploits high-order proximity by propagating embeddings on the user-item interaction graph; instead of implicitly capturing the high-order connectivity through the propagation embedding, SMOG-CF <ref type="bibr" target="#b44">[45]</ref> is proposed to directly capture the high-order connectivity between neighboring nodes at any order. Multi-GCCF <ref type="bibr" target="#b27">[28]</ref> explicitly incorporates the user-user and item-item graphs, which is built upon the user-item bipartite graph, in the embedding learning process. Inspired by the study of simplifying GCN <ref type="bibr" target="#b36">[37]</ref>, researchers also introspect the complex design in GCN-based recommendation models. He at al. <ref type="bibr" target="#b13">[14]</ref> pointed out that the two common designs feature transformation and nonlinear activation have no positive effects on the final performance, and proposed LightGCN which substantially improves the performance over NDCG. Meanwhile, Chen et al. <ref type="bibr" target="#b2">[3]</ref> also proposed to remove the nonlinearity in the network and introduced a residual network to alleviate the over-smoothing problem in existing GCN-based recommendation models. In this paper, we move a step further on this research line. We claim that the indiscriminatively exploiting the high-order neighboring nodes is also an important reason for the over-smoothing problem for GCN-based recommendation model. A typical example is that two users with contradictory interests can be also connected via a 𝑘-order path in the user-item interaction graph. To tackle the problem, we propose an interest-aware message-passing strategy to make the embedding propagation only happened inside a subgraph with similar interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In work, we argued that exploiting high-order node indiscriminately would introduce negative information into the embedding propagation in the GCN-based recommendation models, causing the performance degradation when stacking more layers. We presented a IMP-GCN model which learns user and item embeddings by performing high-order graph convolution inside subgraphs. The subgraphs are formed by a designed subgraph generation algorithm that groups users with similar interests and their interacted items into the same graph. In IMP-GCN, the embedding of a node learned in a subgraph only contributes to the embedding learning of other nodes in this subgraph. In this way, IMP-GCN can effectively avoid taking the noisy information into the embedding learning. Experiments on large-scale real-world datasets demonstrate that IMP-GCN can gain improvements by stacking more layers to exploit information from higher-order neighbors, and achieve the state-of-the-art performance. The advantages of IMP-GCN indicate the importance of distinguishing high-order neighbors on tackling the over-smoothing problem in GCN models. We believe the insights in this study can shed light on the further development of graph-based recommendation models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The average ratio of nodes involved in different layers of graph convolution on three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results Comparison between IMP-GCN and LightGCN at different layers on Kindle Store and Gowalla. IMP-GCN 2 , IMP-GCN 3 , and IMP-GCN 4 represent IMP-GCN with 2, 3, and 4 subgraphs, respectively.</figDesc><graphic url="image-1.png" coords="6,53.80,83.69,504.38,133.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Recall on Kindle Store (b) Coverage Ratio on Kindle Store</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Statistics of Recall and Coverage Ratio on Kindle Store in three subgraphs.</figDesc><graphic url="image-2.png" coords="6,322.88,287.92,115.20,98.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Basic statistics of the experimental datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#user #item #interactions sparsity</cell></row><row><cell>Kindle Store</cell><cell>68,223 61,934</cell><cell>982,618</cell><cell>99.98%</cell></row><row><cell cols="2">Home&amp;Kitchen 66,519 28,237</cell><cell>551,681</cell><cell>99.97%</cell></row><row><cell>Gowalla</cell><cell>29,858 40,981</cell><cell>1,027,370</cell><cell>99.92%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of our model and the competitors over three datasets. Noticed that the values are reported by percentage with '%' omitted.</figDesc><table><row><cell>Datasets</cell><cell cols="2">Kindle Store</cell><cell cols="2">Home&amp;Kitchen</cell><cell cols="2">Gowalla</cell></row><row><cell>Metrics</cell><cell cols="6">Recall NDCG Recall NDCG Recall NDCG</cell></row><row><cell>NeuMF</cell><cell>4.96</cell><cell>2.06</cell><cell>1.34</cell><cell>0.62</cell><cell>12.96</cell><cell>11.21</cell></row><row><cell>CSE</cell><cell>7.65</cell><cell>4.54</cell><cell>1.93</cell><cell>0.91</cell><cell>13.85</cell><cell>11.51</cell></row><row><cell>HOP-Rec</cell><cell>7.96</cell><cell>4.58</cell><cell>1.98</cell><cell>0.94</cell><cell>14.11</cell><cell>12.70</cell></row><row><cell>GCMC</cell><cell>7.93</cell><cell>4.55</cell><cell>1.42</cell><cell>0.64</cell><cell>14.03</cell><cell>11.68</cell></row><row><cell>NGCF</cell><cell>8.25</cell><cell>5.09</cell><cell>2.14</cell><cell>0.96</cell><cell>15.62</cell><cell>13.35</cell></row><row><cell cols="2">LightGCN 10.22</cell><cell>6.24</cell><cell>3.03</cell><cell>1.39</cell><cell>17.96</cell><cell>15.29</cell></row><row><cell cols="3">IMP-GCN 10.88* 6.73*</cell><cell>3.22*</cell><cell cols="3">1.49* 18.69* 15.85*</cell></row><row><cell>Improv.</cell><cell>6.46%</cell><cell cols="3">7.85% 6.27% 7.19%</cell><cell>4.07%</cell><cell>3.66%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of our model and its variants over three datasets. Noticed that the values are reported by percentage with '%' omitted.</figDesc><table><row><cell>Datasets</cell><cell cols="2">Kindle Store</cell><cell cols="2">Home&amp;Kitchen</cell><cell cols="2">Gowalla</cell></row><row><cell>Metrics</cell><cell cols="6">Recall NDCG Recall NDCG Recall NDCG</cell></row><row><cell>IMP-GCN 𝑠</cell><cell>10.57</cell><cell>6.63</cell><cell>3.14</cell><cell>1.43</cell><cell>18.61</cell><cell>15.61</cell></row><row><cell>IMP-GCN 𝑓</cell><cell>10.19</cell><cell>6.40</cell><cell>2.97</cell><cell>1.31</cell><cell>17.84</cell><cell>15.11</cell></row><row><cell>IMP-GCN</cell><cell>10.88</cell><cell>6.73</cell><cell>3.22</cell><cell>1.49</cell><cell cols="2">18.69 15.85</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In experiments, we found that by stacking 7 layers, a user node almost reaches all the other users in three different datasets. Therefore, no more gain after stacking 7 layers.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/liufancs/IMP_GCN.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Note that although LR-GCN was inspired by a different motivation, its final formulation is almost the same as LightGCN.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">http://jmcauley.ucsd.edu/data/amazon.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://www.tensorflow.org.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">Number of users in the three groups 𝐺 1 , 𝐺 2 , 𝐺 3 are 3, 971, 3, 584, 6, 801, respectively.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://github.com/cnclabs/smore.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENTS</head><p>This work is supported by the National Natural Science Foundation of China, No.:61902223, No.:U1936203; the Innovation Teams in Colleges and Universities in Jinan, No.:2018GXRC014; the Shandong Provincial Natural Science Foundation, No.:ZR2019JQ23; Young creative team in universities of Shandong Province, No.:2020KJN012.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lessons from the Netflix prize challenge</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD Explorations</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="75" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collaborative Similarity Embedding for Recommender Systems</title>
		<author>
			<persName><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW. ACM</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2637" to="2643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Revisiting Graph Based Collaborative Filtering: A Linear Residual Graph Convolutional Network Approach</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levent</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremiah</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishi</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Ispir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
				<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MMALFM: Explainable recommendation by leveraging reviews and images</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><forename type="middle">C</forename><surname>Kanjirathinkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aspect-aware latent factor model: Rating prediction with ratings and reviews</title>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kankanhalli</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on World Wide Web</title>
				<meeting>the 27th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blockbusters and Wallflowers: Accurate, Diverse, and Scalable Recommendations with Random Walks</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Christoffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bibek</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM Conference on Recommender Systems</title>
				<meeting>the 9th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Metapath-Guided Heterogeneous Graph Neural Network for Intent Recommendation</title>
		<author>
			<persName><forename type="first">Junxiong</forename><surname>Shaohua Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linmei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongliang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2478" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph Neural Networks for Social Recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Yihong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on World Wide Web</title>
				<meeting>the 28th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random-Walk Computation of Similarities between Nodes of a Graph with Application to Collaborative Recommendation</title>
		<author>
			<persName><forename type="first">François</forename><surname>Fouss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Pirotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Michel</forename><surname>Renders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Saerens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="355" to="369" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ItemRank: A Random-Walk Based Scoring Algorithm for Recommender Engines</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augusto</forename><surname>Pucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artifical Intelligence</title>
				<meeting>the 20th International Joint Conference on Artifical Intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="2766" to="2771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="144" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TriRank: Reviewaware Explainable Recommendation by Modeling Aspects</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 24th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1661" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collaborative metric learning</title>
		<author>
			<persName><forename type="first">Cheng-Kang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="193" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
				<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno>ICLR. OpenReview.net</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="42" to="49" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">User Diverse Preference Modeling by Multimodal Attentive Metric Learning</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changchang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
				<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1526" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An Attribute-aware Attentive GCN Model for Recommendation</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning Context-Aware Latent Representations for Context-Aware Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="887" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Keywords Generation Improves E-Commerce Session-Based Recommendation</title>
		<author>
			<persName><forename type="first">Yuanxing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Nan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1604" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">Awni</forename><forename type="middle">Y</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM Conference on Recommender Systems</title>
				<meeting>the 7th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DeepInf: Social Influence Prediction with Deep Learning</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2110" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-graph convolution collaborative filtering</title>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Data Mining</title>
				<meeting>IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1306" to="1311" />
		</imprint>
	</monogr>
	<note>Ruiming Tang, and Xiuqiang He</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph Convolutional Matrix Completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD: Deep Learning Day</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring High-Order User Preference on the Knowledge Graph for Recommender Systems</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">KGAT: Knowledge Graph Attention Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Personalized Hashtag Recommendation for Micro-videos</title>
		<author>
			<persName><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuzheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
				<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1446" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph-Refined Convolutional Network for Multimedia Recommendation with Implicit Feedback</title>
		<author>
			<persName><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
				<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3541" to="3549" />
		</imprint>
	</monogr>
	<note>Xiangnan He, and Tat-Seng Chua</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video</title>
		<author>
			<persName><forename type="first">Yinwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
				<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1437" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>PMLR</publisher>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Collaborative Denoising Auto-Encoders for Top-N Recommender Systems</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM International Conference on Web Search and Data Mining</title>
				<meeting>the 9th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Glorot</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bengio</forename><surname>Yoshua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics. JMLR</title>
				<meeting>the 13th International Conference on Artificial Intelligence and Statistics. JMLR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Arapakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jose</surname></persName>
		</author>
		<idno>ArXiv abs/2004.04635</idno>
		<title level="m">Graph Highway Networks</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep matrix factorization models for recommender systems</title>
		<author>
			<persName><forename type="first">Hongjian</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26h International Joint Conference on Artificial Intelligence</title>
				<meeting>the 26h International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3203" to="3209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">HOP-rec: high-order proximity for implicit recommendation</title>
		<author>
			<persName><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan-Ju</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="140" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph Convolutional Neural Networks for Web-Scale Recommender Systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">WalkRanker: A Unified Pairwise Ranking Model With Multiple Relations for Item Recommendation</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2596" to="2603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stacked Mixed-Order Graph Convolutional Networks for Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 SIAM International Conference on Data Mining</title>
				<meeting>the 2020 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="73" to="81" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
