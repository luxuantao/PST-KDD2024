<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HuaTuo (??): Tuning LLaMA Model with Chinese Medical Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haochun</forename><surname>Wang</surname></persName>
							<email>hcwang@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chi</forename><surname>Liu</surname></persName>
							<email>cliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nuwa</forename><surname>Xi</surname></persName>
							<email>nwxi@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zewen</forename><surname>Qiang</surname></persName>
							<email>zwqiang@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sendong</forename><surname>Zhao</surname></persName>
							<email>sdzhao@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
							<email>bqin@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Social Computing and Information Retrieval</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HuaTuo (??): Tuning LLaMA Model with Chinese Medical Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain tasks due to the need for medical expertise in the responses. In response to this challenge, we propose HuaTuo (? ?), a LLaMA-based model that has been supervised-fine-tuned with generated QA (Question-Answer) instances. The experimental results demonstrate that HuaTuo generates responses that possess more reliable medical knowledge. Our proposed HuaTuo model is accessible at https://github.com/SCIR-HI/ Huatuo-Llama-Med-Chinese.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The advent of instruction-following large language models (LLMs), representative by Chat-GPT <ref type="bibr" target="#b5">(OpenAI, 2022)</ref>, has generated significant interest due to their exceptional performance in understanding instructions and generating human-like responses. Compared to smaller models, LLMs exhibit strong generalization across various natural language processing (NLP) tasks and unique emergent ability to solving unseen or complicated tasks. Despite ChatGPT's non-open source status, open-source communities have provided several alternatives, such as LLaMa <ref type="bibr">(Touvron et al., 2023)</ref>, with relatively affordable training costs. This positions LLMs as potential solutions for real-world scenarios requiring communication and reasoning.</p><p>However, despite their numerous merits, LLMs are not designed to cater specifically to the medical domain. Their general domain knowledge often falls short when addressing such specialized fields, where accurate and domain-specific expert knowledge is critical. This can lead to sub-optimal * Equal contribution. diagnostic precision, drug recommendations, and medical advice, potentially endangering patients. Few efforts have been made to address this problem, with existing approaches primarily focusing on supplying LLMs with medical information retrieved from conversations, where human errors may occur more frequently. Additionally, LLMs are typically trained in English, constraining their comprehension and response capabilities in languages that differ significantly from English, such as Chinese, rendering their direct application in Chinese contexts less than ideal.</p><p>In this paper, we present the HuaTuo (??) model, an LLM tailored for the biomedical domain, focusing on the Chinese language. By generating diverse instruction data based on medical knowledge from CMeKG, we emphasize ensuring the correctness of facts in the model's responses, which is vital in the biomedical domain. Through this process, we collect over 8,000 instruction data for supervised fine-tuning. Our model builds upon the open-source LLaMa-7B base model, integrates structured and unstructured medical knowledge from the Chinese medical knowledge graph (CMeKG), and employs knowledge-based instruction data for fine-tuning.</p><p>In summary, our contributions can be summarized as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pre-trained Models in Biomedical Domain</head><p>Although large language models (LLMs) exhibit remarkable performance in general domains, their lack of domain-specific knowledge results in suboptimal performance in fields that require specialized expertise, such as biomedicine. The biomedical field's inherent nature necessitates models to possess comprehensive knowledge bases for relevant queries, particularly when applied to real-world situations where patients seek health and medical advice. Several efforts have been made to adapt LLMs to the biomedical domain.</p><p>Existing approaches primarily employ ChatGPT for data assistance and train smaller models using its distilled or translated knowledge. Chatdoctor <ref type="bibr" target="#b3">(Li et al., 2023)</ref> represents the first attempt to adapt LLMs to the biomedical field by fine-tuning LLaMa using conversation demonstrations synthesized via ChatGPT. DoctorGLM <ref type="bibr">(Xiong et al.)</ref> leverages ChatGLM-6B as the base model and finetunes it with the Chinese translation of ChatDoctor dataset, obtained through ChatGPT. Additionally, Chen et al. develops a Chinese and Medically Enhanced Language model within their collection of LLMs. Collectively, these works illustrate the potential for LLMs to be successfully applied within the biomedical domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HuaTuo Model</head><p>In this section, we will introduce the training process of our HuaTuo (??) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Base Model</head><p>LLaMA <ref type="bibr">(Touvron et al., 2023)</ref> is a collection of multi-lingual base models with parameters ranging from 7 billion to 65 billion, which are open-source to the research community. Here, we adopted the LLaMA-7B model for more accessible training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Medical Knowledge</head><p>There are various kinds of medical knowledge, generally including (1) structured medical knowledge like medical knowledge graphs and (2) unstructured medical knowledge like medical guidelines. We utilized a Chinese medical knowledge graph, CMeKG <ref type="bibr" target="#b4">(Odmaa et al., 2019)</ref>, which also provides retrieved medical knowledge about diseases, drugs, symptoms, etc. Table <ref type="table" target="#tab_1">1</ref> shows several knowledge cases in the CMeKG knowledge base.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Knowledge-based Instruction Data</head><p>Instruct-tuning has proven to be effective to tune large language models <ref type="bibr" target="#b10">(Wei et al., 2022;</ref><ref type="bibr" target="#b6">Ouyang et al., 2022)</ref>, which helps the models perform satisfactorily under zero-shot scenarios with the cost of sufficient annotated instructions. Inspired by the automatic construction of the instruction along with the instances (inputs and outputs) <ref type="bibr" target="#b9">(Wang et al., 2022;</ref><ref type="bibr" target="#b7">Taori et al., 2023)</ref>, we generate our instruction data based on the above medical knowledge.</p><p>As demonstrated in Table <ref type="table" target="#tab_2">2</ref>, instruct-tuning involves supervised fine-tuning on the training instances and an instruction that describes the tasks in natural language. However, as for a large language model for medical dialogue, inputs are mostly stated as questions and instructions are all like "Answer the following question". Therefore, we dispose of the instructions and only preserve the inputs for our HuaTuo.</p><p>While the generated instructions are required to be diverse enough for unseen tasks <ref type="bibr">(Wang et al.</ref>,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge in Chinese</head><p>Knowledge translated to English Disease {"class": "?????", "???": "? ?", "????": ["????", "??? ?????????????", "?? ??"], "????": ["??????? ?"], "????": ["??", "HBV DNA? ?", "????", "??", "?????</p><formula xml:id="formula_0">? ? ?", "? ? ? ? ?", "? ? ? ? ? ? ? ? ?", "? ? ? ? ? ? ?", "? ? ?"</formula><p>, "HCV????", "???????", "????e??", "???"],"????":</p><p>["??"], "????": ["?????"], "??": ["????????"]} {"class": "Common Diseases", "Key Word": "Liver Cancer", "Drug Treatment": ["Regorafenib", "Antiviral drugs effective against hepatitis B or C", "Sorafenib"], "High Prevalence Regions": ["Sub-Saharan Africa"], "High Risk Factors": ["Obesity", "High HBV DNA levels", "Chronic alcoholism", "Male gender", "Chronic hepatitis B infection", "Family history of liver cancer", "Cirrhosis due to chronic hepatitis C", "Core promoter mutation", "Liver cirrhosis", "HCV co-infection", "Senile valvular heart disease", "Hepatitis B e antigen", "Diabetes"], "Affected Area": ["Liver"], "Auxiliary Examination": ["Liver function test"], "Medical History": ["Long-term history of chronic hepatitis B"]}. Drug { "class": "??", "???": "????", "??": ["???????????? ????"], "????": ["???", "? ? ?"], "? ?": ["? ? ?", "? ? ? ? ?"], "??": ["0.25g"], "OTC??": ["? ?OTC", "??OTC"], "???": ["?? ?", "??"], "???": ["???"], "? ?": ["???????", "???", "? ??0.1?", "???150??", "??? ????", "???", "???300??", "? ? ?0.15?", "? ? ? ? ? ?", "? ?"] "Class": "Western Medicine", "Key Word": "Metformin", "Appearance": ["Sugarcoated or film-coated tablets, white after removal of coating"], "English Names": ["Yifupian", "Gehuazhi"], "Classification":</p><p>["Biguanide class", "Anti-tuberculosis drug"], "Specifications": ["0.25g"], "OTC Types": ["OTC Class B", "OTC Class A"], "Indications": ["Diabetes", "Obesity"], "Generic Name": ["Yifupian"], "Ingredients": ["Isoniazid and pyrazinamide", "Pyrazinamide", "0.1g pyrazinamide", "150mg pyrazinamide", "This product is a compound preparation", "Isoniazid", "300mg isoniazid", "0.15g isoniazid", "Metformin hydrochloride", "Hydrochloride"] Symptom { "???": "????", "??": ["??</p><p>?????"], "????": ["??", "? ??????"], "????": ["??? ?????", "??????", "??? ?"], "????": ["??", "????", "??????"], "????": ["??"] "Key Word": "Hair Loss", "Examinations": ["Hair mineral analysis"], "Related Diseases": ["Alopecia areata", "Chronic Fatigue Syndrome"], "Related Symptoms":</p><p>["Hair color is light and brown", "Hair is dry and brittle", "Skin becomes hardened"], "Related Departments": ["Internal Medicine", "Dermatology and Venereology", "Radiation and Chemotherapy"], "Affected Area": ["Head"] </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instruction:</head><p>Translate the following sentence into Chinese.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>What are the possible reasons for liver cancer? Output: ??????????? 2022) in the general domain, the correctness of the fact in the responses from the large language model is of more concern in the biomedical domain <ref type="bibr" target="#b2">(Gilson et al., 2023)</ref>. Thus, we first sample knowledge instances from the knowledge graph and then generate the instances based on the specific knowledge with the OpenAI API <ref type="bibr" target="#b5">(OpenAI, 2022)</ref>. Finally, we collect over 8,000 instruction data, like examples in Table <ref type="table" target="#tab_4">3</ref> as training instances for supervised fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>In order to demonstrate the superior performance of HuaTuo, we conducted a comparative analysis with four baseline models.</p><p>? LLaMA <ref type="bibr">(Touvron et al., 2023)</ref> serves as the foundation model for our HuaTuo. In particular, we employed LLaMA-7B for its relative fairness in comparison to the other baselines and its ease of training.</p><p>? Alpaca <ref type="bibr" target="#b7">(Taori et al., 2023)</ref> is an instructedtuned version of LLaMA and boasts more than 80,000 instances generated in the general domain.</p><p>? ChatGLM <ref type="bibr" target="#b12">(Zeng et al., 2023)</ref> is an optimized dialogue model specifically designed for the Chinese chatting scenario. In our analysis, we compared HuaTuo's performance with ChatGLM-6B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>For the generation tasks in the general domain, evaluation metrics, such as Bleu and Rouge are utilized to determine whether a generative model can produce responses similar to the ground truth. However, as for the medical QA tasks, namely (1) safety, (2) usability, and (3) smoothness. Safety determines whether the response includes anything that can mislead the user into danger, such as wrong medicine recommendations. Usability reflects the medical expertise of a specific response. And, the Smoothness represents the ability as a language model. In the domain of natural language generation, various evaluation metrics are utilized to assess the efficacy of generative models. The widely used metrics in the general domain include Bleu and Rouge, which compare generated responses with the ground truth. Additionally, for medical question-answering tasks, we introduce an evaluation metric, SUS. The SUS metric consists of three dimensions: Safety, Usability, and Smoothness. The "Safety" dimension assesses whether the generated response has the potential to mislead the user and pose a danger to their health, for example, through incorrect medication recommendations. The "Usability" dimension evaluates the extent to which the generated response reflects medical expertise, while the "Smoothness" dimension measures the proficiency of the generative model as a language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>In this study, we constructed a test set of potential questions in Chinese dialogue scenarios and compared the generated responses of our HuaTuo model with three other baseline models. To evaluate the model performance, we recruited five annotators with medical backgrounds who assessed the randomly mixed responses of the models using a three-point scale for each dimension of Safety, Usability, and Smoothness (SUS). The SUS scale ranges from 1 (not acceptable) to 3 (good), with 2 indicating an acceptable response. The average SUS scores are presented in Table <ref type="table" target="#tab_5">4</ref>. Although LLaMA achieves the highest safety score, its responses are often uninformative and rephrased the questions, resulting in a low usability score. On the other hand, our HuaTuo model significantly improves knowledge usability without much compromising safety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>The HuaTuo initiative is primarily dedicated to research and is not intended to offer medical advice.  be guaranteed, and the medical knowledge utilized therein should not be construed as a substitute for professional medical advice. If one experiences any discomfort or distress, it is strongly advised to seek the guidance of a qualified medical professional.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Knowledge cases in the CMeKG.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Instance with an instruction.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The medical information utilized in this study is sourced from an open-access medical knowledge graph. It is important to note that the accuracy of responses generated by large language models cannot The methods for treating migraines include medication and non-pharmacological therapies. Medication therapy primarily consists of three categories: acute attack treatment, preventative treatment, and mixed treatment. Non-pharmacological therapies include lifestyle changes such as reducing staying up late, avoiding overwork, avoiding alcohol consumption, and quitting smoking; in addition, they also include treatments such as acupuncture, massage, and pulse discharge therapy. During treatment, a comprehensive assessment of the patient's condition, clinical manifestations, personal circumstances, and other factors should be taken into account to develop a personalized treatment plan.</figDesc><table><row><cell></cell><cell>Instance (in Chinese)</cell><cell>Instances (translated to English)</cell></row><row><cell cols="2">Question: ??XXX????????????</cell><cell>Patient XXX came to the hospital due to</cell></row><row><cell></cell><cell>?????????????????</cell><cell>pain in the head and behind the eyes, ac-</cell></row><row><cell></cell><cell>?????????????????</cell><cell>companied by continuous rumbling sounds.</cell></row><row><cell></cell><cell>??????????</cell><cell>After an examination, the doctor diagnosed</cell></row><row><cell></cell><cell></cell><cell>the patient with a migraine. What are the</cell></row><row><cell></cell><cell></cell><cell>treatment options for this condition?</cell></row><row><cell>Answer</cell><cell>????????????????</cell></row><row><cell></cell><cell>?????????????????</cell></row><row><cell></cell><cell>?????????????????</cell></row><row><cell></cell><cell>?????????????????</cell></row><row><cell></cell><cell>?????????????????</cell></row><row><cell></cell><cell>????????????????</cell></row><row><cell></cell><cell>?????????????????</cell></row><row><cell></cell><cell>?????????????????</cell></row><row><cell></cell><cell>?????????????</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Example for the supervised-fine-tuning training instances.</figDesc><table><row><cell></cell><cell cols="3">Safety Usability Smoothness</cell></row><row><cell>LLaMA</cell><cell>2.93</cell><cell>1.21</cell><cell>1.58</cell></row><row><cell>Alpaca</cell><cell>2.64</cell><cell>2.05</cell><cell>2.30</cell></row><row><cell>ChatGLM</cell><cell>2.59</cell><cell>1.93</cell><cell>2.41</cell></row><row><cell>HuaTuo (??)</cell><cell>2.88</cell><cell>2.12</cell><cell>2.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Experimental results of SUS score for the models.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiannan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://github.com/FreedomIntelligence/LLMZoo" />
		<title level="m">Llm zoo: democratizing chatgpt</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Glm: General language model pretraining with autoregressive blank infilling</title>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="320" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How does chatgpt perform on the united states medical licensing examination? the implications of large language models for medical education and knowledge assessment</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Gilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conrad</forename><forename type="middle">W</forename><surname>Safranek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vimig</forename><surname>Socrates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Andrew</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chartash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMIR Medical Education</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">45312</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge</title>
		<author>
			<persName><forename type="first">Yunxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruilong</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">You</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Preliminary study on the construction of chinese medical knowledge graph</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Byambasuren Odmaa</surname></persName>
		</author>
		<author>
			<persName><surname>Yunfei</surname></persName>
		</author>
		<author>
			<persName><surname>Sui Zhi-Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Dai Damai</surname></persName>
		</author>
		<author>
			<persName><surname>Baobao</surname></persName>
		</author>
		<author>
			<persName><surname>Li Sujian</surname></persName>
		</author>
		<author>
			<persName><surname>Hongying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chinese Information Processing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://chat.openai.com" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m">Stanford alpaca: An instruction-following llama model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Self-instruct: Aligning language model with self generated instructions</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.10560</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Doctorglm: Fine-tuning your chinese doctor is not a herculean task</title>
		<author>
			<persName><forename type="first">Honglin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">GLM-130b: An open bilingual pretrained model</title>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng</forename><surname>Lam Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>In The Eleventh International Conference on Learning Representations (ICLR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
