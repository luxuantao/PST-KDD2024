<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detection and Classification of Cancer in Whole Slide Breast Histopathology Images Using Deep Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-07-19">July 19, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Baris</forename><surname>Gecer</surname></persName>
							<email>b.gecer@imperial.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Bilkent University</orgName>
								<address>
									<postCode>06800</postCode>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Selim</forename><surname>Aksoy</surname></persName>
							<email>saksoy@cs.bilkent.edu.tr</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Bilkent University</orgName>
								<address>
									<postCode>06800</postCode>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ezgi</forename><surname>Mercan</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linda</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
							<email>shapiro@cs.washington.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Donald</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
							<email>donald.weaver@vtmednet.org</email>
							<affiliation key="aff4">
								<orgName type="department">Department of Pathology</orgName>
								<orgName type="institution">University of Vermont</orgName>
								<address>
									<postCode>05405</postCode>
									<settlement>Burlington</settlement>
									<region>VT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joann</forename><forename type="middle">G</forename><surname>Elmore</surname></persName>
							<email>jelmore@u.washington.edu</email>
							<affiliation key="aff5">
								<orgName type="department">Department of Medicine</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<postCode>98195</postCode>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Detection and Classification of Cancer in Whole Slide Breast Histopathology Images Using Deep Convolutional Networks</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Detection and Classification of Cancer in Whole Slide Breast Histopathology Images Using Deep Convolutional Networks</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Deptartment of Electrical and Electronic Engineering</orgName>
								<address>
									<settlement>Im-perial College London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department" key="dep1">Department of Medicine</orgName>
								<orgName type="department" key="dep2">David Geffen School of Medicine</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detection and Classification of Cancer in Whole Slide Breast Histopathology Images Using Deep Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-07-19">July 19, 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">0B478B812DF225E6E8441969341EBD99</idno>
					<idno type="DOI">10.1016/j.patcog.2018.07.022</idno>
					<note type="submission">Received date: 22 August 2017 Revised date: 13 May 2018 Accepted date: 16 July 2018 Preprint submitted to Pattern Recognition</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Pattern Recognition Digital pathology</term>
					<term>breast histopathology</term>
					<term>whole slide imaging</term>
					<term>region of interest detection</term>
					<term>saliency detection</term>
					<term>multi-class classification</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generalizability of algorithms for binary cancer vs. no cancer classification is unknown for clinically more significant multi-class scenarios where intermediate categories have different risk factors and treatment strategies. We present a system that classifies whole slide images (WSI) of breast biopsies into five diagnostic categories. First, a saliency detector that uses a pipeline of four fully convolutional networks, trained with samples from records of pathologists' screenings, performs multi-scale localization of diagnostically relevant regions of interest in WSI. Then, a convolutional network, trained from consensus-derived reference samples, classifies image patches as non-proliferative or proliferative changes, atypical ductal hyperplasia, ductal carcinoma in situ, and invasive carcinoma. Finally, the saliency and classification maps are fused for pixel-wise labeling and slide-level categorization. Experiments using 240 WSI showed that both saliency detector and classifier networks performed better than competing algorithms, and the five-class slide-level accuracy of 55% was not statistically different from the predictions of 45 pathologists. We also present example visualizations of the learned representations for breast cancer diagnosis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p>• Commonly studied scenario considers only binary cancer vs. no cancer classification.</p><p>• Our system classifies whole slide breast biopsies into five diagnostic categories.</p><p>• Pipeline of fully convolutional networks localizes diagnostically relevant regions.</p><p>• Convolutional neural network classi es detected regions of interest in whole slides.</p><p>• Experiments show that our method is compatible with predictions of 45 pathologists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Breast cancer is the most widespread form of cancer among women <ref type="bibr" target="#b0">[1]</ref>. There can be many types of deviations from a healthy tissue, where some are considered benign and some are indicators for cancer. The detection and categorization of these deviations are not always straightforward even for experienced pathologists. Histopathological image analysis promises to play an important role in helping the pathologists by indicating potential disease locations and by aiding their interpretation.</p><p>There is a large body of work on the classification of histopathological images. Most use generic color-or texturebased features and nuclear architectural features with classifiers such as support vector machines (SVM) or random forests (RF) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The most common scenario is to use manually cropped regions of interest (ROI) that have no ambiguity regarding their diagnoses. Even though these approaches can provide insights about which features are useful for classification, it is very difficult to design and tune them with respect to the extensive structural diversity found in whole slide images (WSI) that are obtained by digitization of entire glass slides <ref type="bibr" target="#b2">[3]</ref>. In particular for breast pathology, the variations in the tissue structure that range from non-proliferative changes to proliferative ones such as usual ductal hyperplasia (UDH), atypical ductal hyperplasia (ADH), ductal carcinoma in situ (DCIS), and invasive ductal carcinoma (IDC) provide challenges to both experienced and novice pathologists <ref type="bibr" target="#b3">[4]</ref>. Furthermore, subtle differences among these categories lead to different clinical actions, and the following treatments with different combinations of surgery, radiation, and hormonal therapy make the diagnostic errors extremely significant in terms of both financial and emotional consequences <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Unfortunately, the generalizability of the state-of-the-art image features and classifiers that have been designed and evaluated for the more restricted, often binary, settings is currently unknown for whole slides that contain multiple areas with different structural deviations that correspond to different levels of diagnostic importance. Even though the final diagnosis is decided based on the most severe one of these areas, existence of different levels of structural anomalies in the same slide often distracts pathologists as shown in eye tracking studies <ref type="bibr" target="#b5">[6]</ref>. Thus, automatic detection of diagnostically relevant ROIs can decrease the pathologists' workloads while also assuring that no critical region is overlooked during diagnosis. Such solu-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Figure <ref type="figure">1</ref>: Overview of the proposed framework. Salient regions are detected on the input WSI by feed-forward processing of FCN-1. Each connected component that has a probability of being diagnostically relevant above a threshold is zoomed in and passed to FCN-2. This process is repeated four times, and the detected salient regions are processed by the classification CNN to obtain the likelihood maps for five diagnostic classes. The detection results and the classification results are fused to obtain the final slide-level diagnosis.</p><p>tions will also benefit computer aided diagnosis by eliminating a significant amount of computation and lead to efficient use of computational resources in more detailed WSI analysis.</p><p>In this paper, we study both the detection and the multiclass classification of diagnostically relevant regions in whole slide breast histopathology images using deep networks. Our main contributions are threefold. First, we propose a saliency detection framework for automatic localization of ROIs. Our method uses four separate fully convolutional networks (FCN) trained to imitate the actions of pathologists at different magnifications. Although selecting the right magnification is a common goal in the literature, we go beyond that motivation, and use a data-driven feature learning approach that exploits the recorded viewing behaviors of pathologists where zoom actions are used to construct training samples. These networks progressively eliminate irrelevant areas from lower to higher magnifications, and the combined result provides a saliency map for the WSI. Second, we present another convolutional neural network (CNN) for the identification of five diagnostic categories of ductal proliferations (non-proliferative changes, proliferative changes, ADH, DCIS, and IDC) in whole slides. We consider saliency detection and classification of salient regions as two separate but sequential applications where the proposed modular solutions can also be used in distinct applications. Furthermore, we fuse the outputs of ROI detection and classification steps for slide-level diagnosis. Third, we visualize the resulting networks for better understanding of the learned models in differentiating cancer categories.</p><p>An overview of the proposed approach is shown in Figure <ref type="figure">1</ref>. The rest of the paper is organized as follows. Section 2 discusses the related work, Section 3 introduces the data set, Section 4 describes the methodology for both ROI detection and classification, Section 5 presents the experimental results, and Section 6 summarizes the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The related literature on WSI analysis resorted to restricted classification settings. For example, Dundar et al. <ref type="bibr" target="#b6">[7]</ref> used multiple instance learning for discrimination of benign cases from actionable (ADH+DCIS) ones by using whole slides with manually identified ROIs. Dong et al. <ref type="bibr" target="#b7">[8]</ref> built a logistic regression (LR) classifier for UDH versus DCIS classification where each WSI was modeled with manually cut ROIs. Some approaches to WSI analysis have focused on efficient applications of existing methods by using multi-resolution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> and multifield-of-view <ref type="bibr" target="#b11">[12]</ref> sliding windows. Even though exhaustive window-based processing of WSIs is an alternative to manually selected ROIs, tiling usually involves arbitrary splits of the image and has the risk of distorting the context. Balazsi et al. <ref type="bibr" target="#b12">[13]</ref> tried to overcome the effect of fixed tiling by using color, texture and gradient histogram features extracted from superpixels with RF classifiers for IDC versus normal classification. They concluded that generic features were sufficient for detecting invasive carcinoma but differentiating DCIS from IDC was still a problem. We recently introduced a multi-instance multi-label learning framework to study the uncertainty regarding the correspondence between the pathologists' slide-level annotations and the candidate ROIs extracted from their viewing records for weakly supervised learning using WSI <ref type="bibr" target="#b13">[14]</ref>.</p><p>As one of the rare studies on automatic ROI detection, Bahlmann et al. <ref type="bibr" target="#b14">[15]</ref> used color histograms of square patches with linear SVMs for classification as relevant versus irrelevant. Numerical results were given only for a small set of patches. We developed a bag-of-words model using color and texture features of image patches as well as superpixels with SVM and LR classifiers trained using samples extracted from the logs of pathologists' image screenings for ROI detection <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. The results of the proposed method are compared to the results of this model in Section 5. Bejnordi et al. <ref type="bibr" target="#b17">[18]</ref> classified superpixels at three scales with a large set of features and RF classifiers for progressive elimination of irrelevant areas, and used graphbased clustering of the resulting superpixels with a heuristic set of rules to obtain the ROIs. However, evaluation was done on manually annotated DCIS cases where ADH instances were excluded due to the difficulty of the problem.</p><p>Recent advances in computer vision have demonstrated that feature learning approaches using deep networks can be more successful than hand-crafted features. Such approaches have found applications in histopathology as well. For example, Cruz-Roa et al. <ref type="bibr" target="#b18">[19]</ref> showed that a three-layer convolutional neural network (CNN) that operated on 100 × 100 pixel patches at 2.5× magnification was more successful than color, texture, and graph-based features with an RF classifier in the detection of IDC. Litjens et al. <ref type="bibr" target="#b19">[20]</ref> used a deep network with 128 × 128 pixel patches at 5× magnification for the delineation of prostate cancer. Janowczyk and Madabhushi <ref type="bibr" target="#b2">[3]</ref> illustrated the use of deep learning for several tasks including IDC detection using 32 × 32 pixel patches at 2.5× magnification. CNN-based cell features were also shown to improve the accuracy of graph hashing for histopathology image classification and retrieval in <ref type="bibr" target="#b20">[21]</ref>. Other popular applications where deep learning methods achieved the top scores in competitions include mitosis detection <ref type="bibr" target="#b2">[3]</ref> and metastatic breast cancer detection in lymph nodes <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>. The common characteristics that lead to the success of deep learning in these applications are the suitability of finding an appropriate magnification at which the object of interest and  <ref type="bibr" target="#b22">[23]</ref> that proposed a novel structural feature for breast pathology. First, a multi-resolution network with two multi-path encoder-decoders and input-aware encoding blocks was used for pixel-based segmentation of ROIs into eight tissue types <ref type="bibr" target="#b23">[24]</ref>. Then, superpixels were used as the structural elements that aggregated the pixel labels, and the connected components of the sections marked as epithelium, secretion and necrosis were used to estimate the locations of ducts. Finally, the structural feature was extracted by computing histograms of these tissue types within several layers, defined 1-superpixel thick, towards both the inside and the outside of these ductal components. The structure feature was used to classify each ROI by using a four-class SVM (benign, ADH, DCIS and invasive) and by using a sequence of binary SVMs that eliminate one diagnosis at a time (invasive vs. not-invasive, ADH and DCIS vs. benign, and DCIS vs. ADH). The results of that method are also discussed in Section 5.</p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data set</head><p>We used 240 digital breast histopathology images that were collected as part of NIH-sponsored projects <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26]</ref>. The haematoxylin and eosin (H&amp;E) stained slides were selected from registries associated with the Breast Cancer Surveillance Consortium by using a random stratified method to include the full range of diagnostic categories from benign to cancer and to represent a typical pathology lab setting. Each slide that belonged to an independent case from a different patient was scanned at 40× magnification, resulting in an average image size of 100,000 × 64,000 pixels. The slides were divided into training and test sets, with 180 and 60 cases, respectively, by using stratified sampling based on age, breast density, original diagnosis, and experts' difficulty rating of the case so that both sets had the same class frequency distribution with cases from different patients. The distribution of classes is given in Table 1. ADH and DCIS cases were intentionally oversampled to gain statistical precision in the estimation of interpretive concordance for these diagnoses <ref type="bibr" target="#b24">[25]</ref>.</p><p>Three experienced pathologists who are internationally recognized in diagnostic breast pathology evaluated every slide both independently and in consensus meetings. The results of these meetings were accepted as the reference diagnosis for each slide including non-proliferative changes (including fibroadenoma), proliferative changes (including intraductal papilloma without atypia, usual ductal hyperplasia, columnar cell hyperplasia, sclerosing adenosis, complex sclerosing lesion, and flat epithelial atypia), atypical ductal hyperplasia (including intraductal papilloma with atypia), ductal carcinoma in situ, and invasive ductal carcinoma. Each slide in the test set also has independent interpretations from 45 other pathologists. The difficulty of the multi-class problem studied here can be observed from the evaluation in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref> where the individual pathologists' concordance rates compared with the reference diagnoses were 82% for the union of NP and P, 43% for ADH, 79% for DCIS, and 93% for IDC.</p><p>The data collection also involved tracking the experienced pathologists' actions while they were interpreting the slides using a web-based software tool for multi-resolution browsing of WSI data. In addition, the pathologists also marked an example ROI as a representative for the most severe diagnosis that was observed during their examination of each slide. Both these consensus ROIs and the individual viewing records of the three pathologists are used in the following sections. The diagnoses assigned by the other 45 pathologists are also used for comparison. The study was approved by the institutional review boards at Bilkent University, University of Washington, and University of Vermont.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ROI detection</head><p>In this section, first, we describe how the training data were constructed from the tracking records of pathologists for building fully convolutional networks (FCN) for detection of ROIs in arbitrarily sized images. Then, we present the pipeline of four FCNs that process large images at different magnifications where areas evaluated as non-salient are incrementally eliminated from lower to higher resolutions. This step uses FCNs because they can take arbitrary sized inputs and can generate similar sized predictions that are suitable for detection and segmentation problems <ref type="bibr" target="#b26">[27]</ref>. FCNs provide efficiency during both learning via end-to-end backpropagation and prediction via dense feedforward computation that is more advantageous over sliding window-based processing that involves redundant computation because of overlapping regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Data set preparation</head><p>The online software designed for the pathologists' interpretation of WSI supported pyramid structures with the original 40× magnification as well as layers successively subsampled by a factor of 2 up to 0.625×. The software also provided intermediate resolutions by on-the-fly subsampling from the closest higher magnification. The tracking procedure recorded the coordinates of the windows corresponding to the parts of the  The blue dot represents an example destination window (l j=54 ). The horizontal lines indicate the search range of zoom levels for a possible source window as defined in <ref type="bibr" target="#b2">(3)</ref>. The red dots are eliminated according to this rule. The yellow dots violate <ref type="bibr" target="#b1">(2)</ref>. The green dots satisfy all three conditions, and the earliest one (l i=47 ), marked with a blue ring, is selected as the source window. WSI visible on the screen and mouse events at a frequency of four entries per second. Each of these log entries is named a viewport, and the sequence of viewports from a particular pathologist's interpretation of a particular slide is denoted as l t , t = 1, 2, . . . , T in the analysis below.</p><p>Motivated by the visual search patterns of the pathologists <ref type="bibr" target="#b27">[28]</ref>, we designed a selection process that evaluated the possibility of pairs of windows, (l j , l i ), as being related during the pathologist's visual screening. In this process, the following rules were defined to assess whether a visited window (named the destination, l j ) was considered as salient by the pathologist at one of the earlier windows (named the source, l i ):</p><formula xml:id="formula_1">i &lt; j, (<label>1</label></formula><formula xml:id="formula_2">)</formula><formula xml:id="formula_3">zoom(l i ) &lt; zoom(l k ), ∀k ∈ {i + 1, . . . , j}, (<label>2</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">zoom(l j )/3 ≤ zoom(l i ) ≤ zoom(l j )/1.5.<label>(3)</label></formula><p>The first rule stated that the source window l i must be visited before the destination l j . The second rule ensured that the des- tination window was viewed at a higher magnification than the source window, and there was no zoom out action going to a lower magnification than the zoom level of the source window between the two windows. The third rule required that the zoom level of the source window was in a particular range so that there was sufficient context around the destination in which it was considered salient (e.g., when the zoom level of the destination l j was 30, the zoom level of the source must be in the range <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20]</ref>). Each viewport in our data set was evaluated as a potential destination, and if one or more sources that satisfied (1)-( <ref type="formula" target="#formula_5">3</ref>) were found, the earliest one was used to form the viewport pair. An example is given in Figure <ref type="figure" target="#fig_1">2</ref>. After evaluating all actions, the pairs that contained common source windows were grouped together, and each group was used to create one data sample where the input was the raw image corresponding to the common source window (l i ) and the label was a same sized pixel-level binary mask where the union of destination windows (l j s) in the group were marked as positive (salient). An example is given in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>Training samples were collected from the viewing records of the three experienced pathologists for the 180 training images. The resulting samples were split into four sets according to the zoom levels of the source windows. These sets, shown in Figure <ref type="figure" target="#fig_2">3</ref>, formed the training data for four separate deep networks where each focused on specific contextual cues in a particular range of magnifications. The four training sets consisted of a total of 64,144 images with an average size of 535×416 pixels. The total number of pixels labeled as negative was around five times as many as those that were labeled as positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Network architecture for detection</head><p>Our network architecture and the related learning parameters were influenced from the deep network presented in <ref type="bibr" target="#b28">[29]</ref> because of its success in the ImageNet challenge and simple strategies. Nevertheless, we ensured that the sizes of the receptive fields of the convolutional layers were compatible with the fundamental characteristics of the biopsies such as ductal structures.</p><p>Our fully convolutional network architecture is shown in Figure <ref type="figure" target="#fig_3">4</ref>. The inputs were arbitrary sized RGB images that were collected as in the previous section. Input images were preprocessed by subtracting the overall mean of RGB values of training images from each pixel. The image was then passed through</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T three similar convolutional layers, as in <ref type="bibr" target="#b28">[29]</ref>, where filters had a very small width and height (3 × 3) followed by a ReLU nonlinearity unit. Convolutional stride and spatial padding was set to 1 pixel such that the spatial resolution was preserved. ReLU was followed by the max pooling operation with a 3 × 3 pixel window and a stride of 3 after the first layer, and a 2×2 window and a stride of 2 after the remaining layers. These three convolutional layers were followed by another convolutional layer with a 4 × 4 window size and a convolutional stride of 4. This layer included a ReLU nonlinearity but no max pooling operation. After that, there was one fully connected layer (which was, in fact, a 1 × 1 convolutional layer in FCN) followed by a dropout operation with a rate of 0.5. The network continued with a deconvolutional layer with an upsample rate of 16 times and cropping of 32 pixels from all sides. Number of filters in all layers were 32, 32, 64, 128, 2, respectively. The final layer was connected to the 'multinomial logistic loss' (softmax log loss) objective function layer while training, but after training, we removed that layer and added a 'softmax' layer to estimate class (relevant versus irrelevant) probabilities. The hyper-parameters of the network architecture were tuned on one-fifth of the training set as validation data. Given an input image with a size of m × n pixels, the resulting map of size m/3 × n/3 that was relative to the input was an advantage of the fully convolutional design that improved the precision of detection and localization of salient regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Pipeline</head><p>We designed a pipeline that gradually eliminated diagnostically irrelevant regions efficiently in four successive stages where the ultimate output was a saliency map of the input image. A given image was processed by four networks that had the same architecture but were trained to handle images at different magnifications as shown in Figure <ref type="figure">1</ref>. Let Φ represent the input image at 40× magnification along with the constructed multiresolution pyramid. For ROI detection, we used the 0.625×, 1.25×, 2.5×, and 5× magnifications, denoted as Φ 1 , Φ 2 , Φ 3 , and Φ 4 , respectively, corresponding to the zoom level ranges shown in Figure <ref type="figure" target="#fig_2">3</ref>.</p><p>The analysis started with the smallest magnification, Φ 1 , being fed to the first network FCN 1 to produce the saliency map Θ 1 . Then, the regions with probability of being diagnostically relevant above a particular threshold were fed to the second network. The same procedure was repeated for the remaining stages. The final saliency map Θ was computed as the weighted geometric mean <ref type="bibr" target="#b29">[30]</ref> of the thresholded outputs of four networks, Θ 1 , Θ 2 , Θ 3 , and Θ 4 , as</p><formula xml:id="formula_6">Θ = 4 k=1 Θ w k / 4 r=1 w r k (4)</formula><p>where w k = ( 1 2 ) 4-k . The weighting scheme assigned larger weights to higher magnifications as their inputs included more details. The output maps were computed in such a way that the pixels below the threshold were set to the minimum value of the pixels above the threshold as</p><formula xml:id="formula_7">Θ k (x, y) =          FCN k Φ k (x, y) if (x, y) ∈ Ω k , min (x ,y )∈Ω k FCN k Φ k (x , y ) otherwise (5)</formula><p>where Ω 1 was the set of all pixels in the input image (Ω 1 = {(x, y) ∈ Φ}), and</p><formula xml:id="formula_8">Ω k = {(x, y) ∈ |Θ k-1 | τ } for k &gt; 1</formula><p>were the sets of pixels above the corresponding thresholds. |Θ k | τ denotes thresholding Θ k adaptively such that the lowest τ percentage of the values of Θ k were removed from the set of pixels to be processed in the subsequent stages. This ensured that the saliency information obtained by earlier FCNs were not lost while preserving the order of pixel values (i.e., the pixels below the threshold could not have higher values than those above it in the geometric mean). Tuning the parameter τ is discussed in Section 5. Note that, all Θ k maps were scaled to the same resolution, and the geometric mean in (4) was computed pixel-wise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ROI classification</head><p>In this section, we describe the methodology for both patchlevel and slide-level classification of WSIs into five diagnostic categories (NP, P, ADH, DCIS, IDC) using a convolutional neural network (CNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Data set preparation</head><p>A single WSI often contains multiple areas with different levels of diagnostic importance, and a small area can lead to the pathologist's finalization of the diagnosis. Our data set contained an example ROI that was marked for each WSI as a representative for the most severe diagnosis that was observed during the three experienced pathologists' consensus meetings. We used these consensus ROIs as the training examples for our deep network for classification, and sampled 100 × 100 pixel patches with 50 pixel strides to form the training data. The 10× magnification was used so that the patches had sufficient context. This combination of patch size and magnification also allowed us to fit a reasonable number of patches in the available GPU memory. The neighboring patches had 50% overlap to achieve translation invariance. The resulting training set consisted of 1,272,455 patches belonging to five categories. Note that, even though the number of samples seems to be large, many of these patches may contain irrelevant content such as empty areas, necrosis, etc., because the consensus ROIs were marked roughly using rectangular boundaries as shown in Figure <ref type="figure" target="#fig_9">10</ref>. We plan to integrate tissue segmentation as a preprocessing stage to perform contextual sampling from epithelial and stromal regions in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Network architecture for classification</head><p>Five-class classification was a more challenging task than binary saliency detection; thus, a deeper network and more training data were required. The training set of patches contained approximately ten times as many pixels as the training set used for detection. Furthermore, the design of the network was updated with more layers, filters, and neurons as shown in Figure <ref type="figure" target="#fig_4">5</ref>. The resulting network accepted 100 × 100 × 3 fixed sized inputs. Input images were normalized by subtracting the overall mean of the three channels. The network consisted of six convolutional layers with 3 × 3 filters, followed by three fully convolutional layers and a final softmax layer. Except the last layer, all layers were followed by a ReLU nonlinearity. The convolutional layers contained 64, 64, 64, 128, 128, 256 filters in respective order, and the first, second, fourth, and sixth layers were followed by a 2 × 2 max pooling operation with a stride of 2. The fully connected layers contained 512, 512, 5 neurons, and are followed by a dropout operation with 0.5 probability. The hyper-parameters of the network architecture were tuned on one-fifth of the training set as validation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Our focus was the development of the complete framework, starting from the step that extracts training data from the raw viewing logs of the pathologists to the steps that include the detection of diagnostically relevant ROIs and the ROI-level and slide-level classification of whole slide images. Thus, the network architectures used in this paper were adapted from the network in <ref type="bibr" target="#b28">[29]</ref>, which has been accepted to be one of the stateof-the-art baselines in many domains. The overall effectiveness can be improved by replacing the networks in Figure <ref type="figure">1</ref> with other suitable architectures from the literature in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Post-processing for slide-level classification</head><p>The network provided class probabilities for fixed sized input patches. In order to obtain probability maps for the whole slides, we needed to either classify patches extracted by sliding windows or fully convolutionalize the network. We chose to implement the latter as it enabled more efficient WSI classification that, in fact, implicitly implemented sliding windows with a step size of 16. Therefore, each pixel in the probability maps corresponded to a 16 × 16 pixel patch in the input space.</p><p>The probability maps produced by the above strategy were further downsampled by a factor of seven by bilinear interpolation in order to smooth out the estimates and remove the noise caused by small isolated details. The downsampled maps were then used to determine the final classification such that every pixel voted for the class that had the greatest probability for that pixel. Finally, the class with the majority of the votes was selected as the final diagnosis for the corresponding WSI.</p><p>An alternative approach is to learn a slide-level decision fusion model. This has been motivated in the literature <ref type="bibr" target="#b30">[31]</ref> for cases in which individual patches may not be discriminative and their predictions can be biased, whereas the learned fusion may model their joint appearance and correct the bias of patch-level decisions. We implemented the method in <ref type="bibr" target="#b30">[31]</ref> where a class histogram was generated by summing up all of the class probabilities assigned to all pixels by the patch-level classifier, and a multi-class SVM was trained by using these histograms to produce slide-level predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we present the experiments for the detection and classification tasks as well as the visualization of the trained networks. The training samples for both tasks were further divided into 80% training and 20% validation sets for estimating the hyper-parameters and to avoid overfitting. The implementations were derived from the MatConvNet library <ref type="bibr" target="#b31">[32]</ref> with a number of significant modifications, and ran on a system with an NVIDIA GeForce GTX-970 GPU, Intel Xeon ES-2630 2.60GHz CPU, and 64 GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">ROI detection</head><p>We trained the four FCNs for 50 epochs using the training set. For each FCN, the stochastic gradient descent algorithm was run to optimize a total of 168,290 network parameters on mini batches of 25 images with 0.0001 learning rate, 0.0005 weight decay, and 0.9 momentum. These hyper-parameters were empirically set on a subset of the validation data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Reference data</head><p>Detection of diagnostically relevant ROIs in WSI has not been a well-studied task in the literature, and there is no publicly available data set that is suitable for the evaluation of this task. Therefore, we used the viewport tracking data to generate the annotations for evaluation.</p><p>This procedure followed the same approach described in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Saliency of the viewports were evaluated by using the following set of rules:</p><p>• The pathologist zoomed into a region from the previous window and zoomed out right after. This event was named a zoom peak, and was a local maximum in the zoom level. • The pathologist slowly slid the viewports while maintaining the same zoom level. This event was named a slow panning, and was represented by the union of the consecutive group of viewports with small displacement. • The pathologist viewed the same region for more than 2 seconds. This event was named a fixation.</p><p>More details can be found in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. These rules were applied to all viewport logs from the three experienced pathologists, and the union of all windows that satisfied at least one of these rules was computed to create a binary saliency mask for each WSI in the test set. Morphological operations were also used to remove the outer white regions that corresponded to the slide background outside the tissue section because the rectangular viewports often contained such regions. Examples for the saliency masks are shown in Figure <ref type="figure" target="#fig_6">7</ref>. The training and validation labels described in Section 4.1.1 and the test labels described in this section all came from different cases belonging to different patients. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Evaluation criteria</head><p>The output of the detection pipeline for each test WSI contains pixel-wise probability estimates in the range [0, 1]. These estimates were compared to the reference binary saliency mask for computing pixel-based receiver operating characteristic (ROC) curves by averaging all results from the 60 test cases.</p><p>The resulting performance was compared with two alternative approaches. The first one was the classification framework proposed in <ref type="bibr" target="#b16">[17]</ref>. The approach in <ref type="bibr" target="#b16">[17]</ref> can be considered as a state-of-the-art method that used a bag-of-words model with color and texture features of image patches where a logistic regression classifier trained on the binary saliency masks extracted from the viewport logs of the training slides was used to produce the detection scores.</p><p>The second comparison used the U-Net architecture proposed for biomedical image segmentation <ref type="bibr" target="#b32">[33]</ref>. The U-Net network consists of 23 convolutional layers where a contracting path is followed by an expansive path. The contracting path uses the typical architecture of a convolutional network, whereas each step in the expansive path performs convolutions on the concatenation of the upsampled version of the previous step in the expansive path and the corresponding feature map from the contracting path. The same training data in four different sets of magnifications were used to train four separate networks that were combined with the same weighting scheme proposed in Section 4.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3.">Results and discussion</head><p>Figure <ref type="figure" target="#fig_5">6</ref>(a) shows the ROC curves for different τ values that were used to eliminate a certain percentage of the pixels for further processing in subsequent stages in the pipeline. The true positive rate (TPR) was considered to represent the effectiveness of the method in identifying all diagnostically relevant ROIs, and the false positive rate (FPR) was considered a suitable metric to evaluate the efficiency of the method to reduce the area to be processed in the following steps as the salient regions usually occupied a relatively small part of a WSI. According to Figure <ref type="figure" target="#fig_5">6</ref>(a), while monotonic improvements on both effectiveness and efficiency were observed until τ = 0.4, further increase in τ corresponded to a decrease in accuracy. Therefore, there is an application dependent trade-off as higher τ values continue to yield more efficiency.</p><p>Comparative results are presented in Figure <ref type="figure" target="#fig_5">6</ref>(b). The proposed method attained the best area under the curve (AUC) value for τ = 0.4 as 0.9153, whereas <ref type="bibr" target="#b16">[17]</ref> obtained 0.9124 and the network in <ref type="bibr" target="#b32">[33]</ref> obtained 0.9043. We also saw that when FPR = 0.2, TPR of <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b32">[33]</ref> were 0.8552 and 0.8902, respectively, while our method achieved 0.8947. Similarly, in the high TPR region above 0.8, our method obtained smaller FPR values compared to <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b32">[33]</ref>. Only after a TPR of 0.98, <ref type="bibr" target="#b16">[17]</ref> achieved higher TPR at the same FPR. Overall, our method achieved better effectiveness than both <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b32">[33]</ref>, even though <ref type="bibr" target="#b16">[17]</ref> used the same set of rules listed in Section 5.1.1 for generating both training and test data whereas our method used a different training set. Furthermore, our method was significantly more efficient than both <ref type="bibr" target="#b16">[17]</ref> (with a factor of 74 times for τ = 0.4) and <ref type="bibr" target="#b32">[33]</ref> (with a factor of 24 times at the same threshold setting) by operating on lower resolutions and processing only a small portion of the images at utmost 5× magnification in the proposed pipeline, whereas <ref type="bibr" target="#b16">[17]</ref> processed entire slides using sliding windows at full 40× magnification and <ref type="bibr" target="#b32">[33]</ref> used a much larger network architecture.</p><p>The fully convolutional network architecture used in this paper efficiently learned to make dense predictions for per-pixel tasks as the output was aggregated from local computations. Explicit connections from early activations to later layers as in the U-Net architecture have the potential of capturing more detailed location information in the final predictions. However, the resulting networks often need a trade-off for increased complexity in larger scale problems, such as WSI classification in this paper, via subsampling to keep the filters small and the computational requirements reasonable <ref type="bibr" target="#b26">[27]</ref>.</p><p>Figures <ref type="figure" target="#fig_6">7,</ref><ref type="figure" target="#fig_7">8</ref>, and 9 present example detection results. Both the full WSI output and the zoomed results showed that the proposed method produced detailed and more precise localization of the relevant regions whereas <ref type="bibr" target="#b16">[17]</ref> produced more blurry results because of the windowing effects.   <ref type="figure" target="#fig_6">7</ref>. From top to bottom: RGB image, the reference saliency mask, output of the proposed approach for τ = 0.4, output of <ref type="bibr" target="#b16">[17]</ref>. The roughness of the saliency masks used for training and testing can be seen. The proposed method provides more detailed pixel-wise predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">ROI classification</head><p>The CNN used for classification was trained for 50 epochs to optimize a total of 5,576,581 network parameters on mini batches of 256 patches with 0.01 learning rate, 0.0005 weight decay, and 0.9 momentum. These hyper-parameters were empirically set on a subset of the validation data. In order to evaluate the effectiveness of the trained network, we performed experiments for two tasks: classification of 100×100 pixel patches and classification of individual WSIs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Reference data</head><p>The consensus labels assigned by the three experienced pathologists were used as the slide-level reference data. We also used the individual diagnoses provided by the 45 other pathologists on the 60 test cases for comparison. These diagnoses were originally collected for evaluation of the differences between glass slides and digital slides. Therefore, 23 pathologists labeled the same cases by looking at the glass slides, and 22 evaluated the digital slides in WSI format.</p><p>For the patch classification task, 209,654 patches with 100 × 100 pixels were sampled from the consensus ROIs of the test cases. Each patch was labeled with the consensus label of the corresponding WSI. However, since the consensus ROIs were roughly drawn as rectangular shapes, some of these patches may contain irrelevant content as in the case of training data generation. The training and validation data described in Section 4.2.1 and the test data described in this section all came from different cases belonging to different patients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Results and discussion</head><p>Patch classification. The accuracy of the CNN for classification of the test patches into five categories was 39.04%. The resulting confusion matrix is shown in Table <ref type="table" target="#tab_1">2</ref>. The errors seemed mostly as underestimations of diagnostic classes as the lower triangle of the confusion matrix added up to 63.21% of the wrong classifications. However, visual inspection of the patches showed that some of them were actually not errors because the whole consensus ROIs were labeled with the same diagnosis without a precise delineation of the ductal regions, and not all patches sampled from these ROIs contained the same level of structural cues that represented the given label. For example, a patch that was sampled from an ROI labeled as ADH could easily contain usual hyperplasia or even stromal regions. Compared to the binary classification tasks of invasive cancer, mitosis, metastasis, etc., detection that have been widely studied in the literature, the labeling of ductal proliferations and hyperplastic changes was a more difficult problem with a higher uncertainty. The fusion of ROI detection and patch classification will recover some of these errors in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WSI classification.</head><p>The classification of a WSI by using the fully convolutionalized CNN produced probability maps containing the five-class likelihoods as well as a label map indicating the winning class for each pixel. The class with the highest overall frequency in the whole image (i.e., the majority voting approach described in Section 4.2.3) can be used as the slidelevel diagnosis. For robustness to the uncertainty in the output of the patch-based classifier due to the roughness of the consensus ROIs and the corresponding training samples, we also used the saliency map for each WSI, and applied an adaptive threshold so that only the top 15% of the salient pixels remained, where the final slide-level class prediction was obtained for the WSI by using majority voting only among the class labels of the pixels that achieved the highest (top 15%) probability of being salient. The threshold percentage was selected by using the validation data. Figure <ref type="figure" target="#fig_9">10</ref> shows an example classification. More examples can be found in <ref type="bibr" target="#b33">[34]</ref>.</p><p>We also used the learned decision fusion model by training two separate multi-class SVM classifiers by using the class histograms of the pixels (i.e., the learned fusion approach described in Section 4.2.3) without and with selection by the saliency detection pipeline. The same thresholding protocol was used during selection.</p><p>Quantitative evaluation was performed by comparing the final slide-level predictions with the consensus labels and the predictions of the 45 pathologists. We also trained multi-class SVM and RF classifiers with state-of-the-art hand-crafted features including 192-bin Lab histograms (64 bins per channel), 128-bin local binary pattern (LBP) histograms (64 bins for each of the H and E channels estimated via color deconvolution), and 50 nuclear architectural features (as in <ref type="bibr" target="#b11">[12]</ref>) with different feature combinations. Both the SVM and the RF classifiers are popular non-deep learning methods for histopathological image classification, and were used as representative baselines in our experiments. The features were computed within 3,600 × 3,600 pixel windows at the highest 40× magnification where the window size was decided based on the observations in <ref type="bibr" target="#b16">[17]</ref>. Sliding windows that were inside the consensus ROIs of the training set were used to build the SVM with a linear kernel and the RF  The variability in the pathologists' predictions, with a very wide range of concordance rates compared with the reference diagnoses particularly for the P, ADH, and DCIS categories, is consistent with the medical literature where inter-rater agreement has always been a known challenge <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>classifier where the cost parameter for the SVM and the number of trees and tree depths for the RF were obtained by using crossvalidation. The resulting classifiers were then used to label the sliding windows of the test WSIs, and the resulting likelihood maps were combined with the same saliency detection outputs as in our method to obtain the slide-level predictions. Table <ref type="table" target="#tab_2">3</ref> shows the confusion matrix for our method. Figure <ref type="figure" target="#fig_10">11</ref> shows the class-specific precision and recall values for our method, the best performing baselines when all features were combined (370 features), and the 45 pathologists' predictions. Table <ref type="table" target="#tab_3">4</ref> summarizes all results.  We observed that the learned fusion approach improved the results against majority voting (from 23.33% to 38.33%) when the saliency map was not used. However, considering only the set of pixels with the highest probability of being salient in the slide-level prediction resulted in the same accuracy (55%) when both the majority voting and the learned fusion approaches were used. The 55% classification accuracy achieved by the proposed framework was also 10% higher than the best performing hand-crafted feature and classifier combination as seen in Table <ref type="table" target="#tab_3">4</ref>. This shows that our saliency detection pipeline was very selective and discriminative where majority voting among the most salient pixels was sufficient for the slide-level diagnosis, with the additional benefit of incrementally eliminating most of the image regions in lower magnifications and processing only small portions of the images in higher magnifications. In particular, the average running times for a single whole slide test image (with an average size of 94,525 × 64,330 pixels) could be summarized as follows: saliency detection at 0.625×, 1.25×, 2.5×, and 5× magnifications took 0.50, 1.21, 2.90, and 6.97 seconds, respectively, for a total of 11.58 seconds for the whole pipeline when the threshold for eliminating diagnostically irrelevant regions was set to τ = 0.4, and classification of the patches that contained the top 15% of the salient pixels took 55.09 seconds using our Matlab-based implementation on a single core of the CPU.</p><p>The overall slide-level classification accuracy of 55% was also comparable to the performances of the 45 pathologists that practice breast pathology in their daily routines. As seen from Figure <ref type="figure" target="#fig_10">11</ref>, there were very mixed performances from the pathologists for the P, ADH, and DCIS classes. In the clinical setting, the pathologists usually agree in their diagnoses for the NP and IDC cases because these are at two extremes of the continuum of histologic features. Given the smaller amount of data used to train the networks, our performance for the NP and IDC classes were lower than the typical pathologist's performance. As there is little clinical difference in how the patients with non-proliferative (NP) and proliferative (P) benign biopsies are managed, we plan to merge the NP and P cases as a single class named benign without atypia in future work. However, when the other more difficult intermediate diagnostic categories with different clinical significance as risk factors for future cancer and with different subsequent surveillance and preventive treatment options were concerned, the proposed method performed better, in terms of recall, than 30 pathologists for P, 5 pathologists for ADH, and 39 pathologists for DCIS. In terms of precision, our method was better than 17 pathologists for P, 34 pathologists for ADH, and 2 pathologists for DCIS.</p><p>We also applied McNemar's test <ref type="bibr" target="#b34">[35]</ref> to compare the proposed method with the pathologists. Given the predictions of our method and the individual pathologists' for all 60 test cases, 45 tests were carried out at 5% significance level, and in 32 of these tests the null hypothesis could not be rejected, i.e., their performances were not statistically significantly different than ours. Furthermore, we performed a z-test also at 5% significance level, and again we could not reject the null hypothesis, i.e., our scores belonged to the same normal distribution estimated from the performances of the 45 pathologists.</p><p>The overall results indicated that the fusion of saliency detection for localization of diagnostically relevant regions and the classification of these regions into five diagnostic classes using deep networks provided a promising solution. The alternative approach of <ref type="bibr" target="#b22">[23]</ref> that was tested on the same data set in four-class classification (after merging non-proliferative and proliferative changes as a single category named benign) achieved an accuracy of 56% when the structure feature computed using histograms of eight tissue types within layers of superpixels both inside and around ductal objects was used. Though not directly comparable with our five-class slide-level performance as that accuracy was computed only within the consensus ROIs of the test slides, it provided additional confirmation of the difficulty of the multi-class classification problem involving the full range of histologic categories. Another important finding of that work was that, the structure feature that explicitly incorporated the highly specialized domain knowledge into the classification model was particularly powerful in discriminating ADH cases from DCIS that have not been studied in the published literature. Given the years of training and experience that the pathologists use to diagnose the biopsies, and the importance of objective and repeatable measures for interpreting the tissue samples under the multi-class classification scenario where different classes carry significantly different clinical consequences, our comparable results on this challenging data set showed the promise of deep learning where future work with larger and more precisely labeled data sets and additional computational resources will eventually be practical in a clinical setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Visualization</head><p>CNNs are often criticized as black box models. Recent work on the visualization of the inner details of CNNs can also be useful in understanding the representations learned from pathology data. We used the occlusion <ref type="bibr" target="#b35">[36]</ref> and deconvolution <ref type="bibr" target="#b36">[37]</ref> methods, with implementations from the FeatureVis library <ref type="bibr" target="#b37">[38]</ref>, to visualize the CNN learned for multi-class classification.</p><p>The occlusion method added small-sized random occluders at different locations in a patch and compared the resulting activation after each occlusion with the original one. Figure <ref type="figure" target="#fig_13">12</ref> shows the visualization results as maps of the importance of different details in example images that affected the classification of particular classes positively or negatively. For example, the first three rows show examples of ductal regions with few layers of epithelial cells around lumens. The fifth and sixth rows show examples of atypical proliferations. The seventh and eighth rows show examples of ducts filled with epithelial cells. The tenth and eleventh rows show examples of intertwined groups of cells with no apparent ductal structure. The ninth and twelfth rows contain examples that were listed as misclassifications that might actually be correct decisions but were counted as errors because of the imprecise delineation of the consensus ROIs and the difficulty of sampling from these large rectangular windows. Finally, the fourth row shows a clear example of the need of the saliency detection step because the almost empty patch confused the CNN and led to activations for multiple classes as  similar regions were included in the sample sets for all classes. Note that, it was ignored in the final fused decision because the fully convolutional multi-scale saliency detection pipeline eliminated such areas.</p><p>The deconvolution method built reconstructions by projecting the activations back to the input space so that parts of the input image that most strongly activated particular neurons were found. Figure <ref type="figure" target="#fig_15">13</ref> illustrates the top-9 responsive patches for example neurons from different layers and the visualization of the contributions of their pixels. The examples showed how the lower layers captured the fundamental features such as edges and blobs, and the higher layers developed more abstract features based on patterns representing particular arrangements of nuclei and other ductal structures. Future work includes more detailed evaluation of these visualizations in a clinical perspective together with the pathologists.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We presented a deep learning-based computer aided diagnosis system for breast histopathology. The proposed framework covered the whole workflow from an input whole slide image to its categorization into five diagnostic classes. The first step was saliency detection by using a pipeline of four sequential fully convolutional networks for multi-scale processing of the whole slide at different magnifications for localization of diagnostically relevant ROIs. Both the learning and the inference procedures imitated the way pathologists analyze the biopsies by using the pathologists' recorded actions while they were interpreting the slides. The second step was a patch-based multiclass convolutional network for diagnosis that was learned by using representative ROIs resulting from the consensus meetings of three experienced pathologists. The final step was the fusion of the saliency detector and the fully-convolutionalized classifier network for pixel-wise labeling of the whole slide, and a majority voting process to obtain the final slide-level diagnosis. The deep networks used for detection and classification performed better than competing methods that used hand-crafted features and statistical classifiers. The classification network also obtained comparable results with respect to the diagnoses provided by 45 other pathologists on the same data set. We also presented example visualizations of the learned representations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>for better understanding of the features that were determined to be discriminative for breast cancer diagnosis. Given the novelty of the five-class classification problem that is important for clinical applicability of computer aided diagnosis, the proposed solutions and the presented results by using a challenging whole slide image data set show the potential of deep learning for whole slide breast histopathology where future work with larger data sets with more detailed training labels have the promise to result in systems that are useful to pathologists in clinical applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training sample generation from the viewport log of a pathologist. The x-axis shows the log entry index and the y-axis shows the zoom level.The blue dot represents an example destination window (l j=54 ). The horizontal lines indicate the search range of zoom levels for a possible source window as defined in<ref type="bibr" target="#b2">(3)</ref>. The red dots are eliminated according to this rule. The yellow dots violate<ref type="bibr" target="#b1">(2)</ref>. The green dots satisfy all three conditions, and the earliest one (l i=47 ), marked with a blue ring, is selected as the source window.</figDesc><graphic coords="6,25.47,75.23,251.10,118.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Training sample generation from the viewport logs (cont.). Grouped viewports are shown with the same color where the filled dots are the source windows and the dots with rings represent the destination windows. A data sample is illustrated for the red group where the source window (l i=47 ) defines the input image data within the WSI, and the union of destination windows (l j=54,...,67 ) are used to construct the saliency label mask. The four ranges of zoom levels that are considered for training four different FCNs are also shown: FCN-1 with zoom(l i ) = 1 (red), FCN-2 with 2 ≤ zoom(l i ) ≤ 3 (green), FCN-3 with 4 ≤ zoom(l i ) ≤ 6 (yellow), and FCN-4 with 7 ≤ zoom(l i ) ≤ 40 (blue).</figDesc><graphic coords="6,25.47,284.11,251.10,125.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of the FCN architecture for ROI detection. The number and size of the filters at each layer are given. All convolutional layers are followed by ReLU nonlinearity. We also show the corresponding image size at each layer for an input of m × n pixels. Note the deconvolutional layer at the end.</figDesc><graphic coords="6,294.51,75.23,251.10,80.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of the CNN architecture for ROI classification. The number and size of the filters at each layer are given. All convolutional layers are followed by ReLU nonlinearity.</figDesc><graphic coords="8,25.47,75.23,251.10,70.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: ROC curves for the proposed saliency detection pipeline with different τ values (a) and comparisons with the method of Mercan et al. in [17] and the U-Net architecture in [33] (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Example saliency detection results for three WSIs. From top to bottom: RGB WSI, the reference saliency mask, output of the proposed approach for τ = 0.4, output of [17]. The image sizes, from left to right, are 77,440 × 68,608, 128,576 × 65,936, and 132,256 × 55,984, pixels, respectively, at 40× magnification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Details of the individual stages in the saliency detection pipeline for the images shown in Figure 7. From top to bottom: outputs of the four FCNs, Θ 1 , Θ 2 , Θ 3 , Θ 4 , respectively, for τ = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Zoomed examples from Figure7. From top to bottom: RGB image, the reference saliency mask, output of the proposed approach for τ = 0.4, output of<ref type="bibr" target="#b16">[17]</ref>. The roughness of the saliency masks used for training and testing can be seen. The proposed method provides more detailed pixel-wise predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Example classification result for a WSI with a consensus label of ADH. (a) Original image (65,936 × 128,576 pixels) with consensus ROIs marked with black lines. (b) Patch-level classification for five classes: NP (white), P (yellow), ADH (green), DCIS (blue), IDC (red). (c) Saliency detection (brighter values indicate higher probability). (d) Pixels (in (b)) whose labels were used in the majority voting for slide-level diagnosis after thresholding the saliency map. (e-i) Pixel-wise likelihood maps for five classes. This sample was correctly classified as ADH using the majority voting of the labels shown as overlay in (d). (Best viewed in color with zoom.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure11: Class-specific precision versus recall for the proposed method (square), the SVM baseline (diamond), the RF baseline (circle), and the 45 pathologists (dot). Colors represent: NP (gray), P (yellow), ADH (green), DCIS (blue), IDC (red). The variability in the pathologists' predictions, with a very wide range of concordance rates compared with the reference diagnoses particularly for the P, ADH, and DCIS categories, is consistent with the medical literature where inter-rater agreement has always been a known challenge<ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Visualization of the learned representations using the occlusion method. Each row represents a separate example patch. From left to right: 100 × 100 pixel input patch, importance of local details overlaid on the input image for individual diagnostic classes NP, P, ADH, DCIS, and IDC. Warmer colors indicate higher impact of that region (either positively or negatively) for the classification of that class. Reference diagnoses are marked by green boxes. The predictions of our method are shown by red bars whose heights indicate the likelihood. (Best viewed in color with zoom.)</figDesc><graphic coords="13,50.58,75.23,200.88,376.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Visualization of the network layers by using the deconvolution method. For each convolutional (CONV) and fully connected (FC) layer, the top-9 activations (as 3 × 3 groups) for four example neurons (left) and their corresponding original input patches (right) are shown. The last softmax layer consists of five neurons corresponding to five classes; from left to right and top to bottom: NP, P, ADH, DCIS, IDC. (Best viewed in color with zoom.)</figDesc><graphic coords="13,413.78,261.72,130.57,97.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Distribution of diagnostic classes among the 180 training and 60 test slides.</figDesc><table><row><cell>Class</cell><cell cols="2">Training Test</cell></row><row><cell>Non-proliferative changes only (NP)</cell><cell>8</cell><cell>5</cell></row><row><cell>Proliferative changes (P)</cell><cell>50</cell><cell>13</cell></row><row><cell>Atypical ductal hyperplasia (ADH)</cell><cell>50</cell><cell>16</cell></row><row><cell>Ductal carcinoma in situ (DCIS)</cell><cell>55</cell><cell>21</cell></row><row><cell>Invasive ductal carcinoma (IDC)</cell><cell>17</cell><cell>5</cell></row><row><cell cols="3">the relevant context can be fit within a fixed size patch and the</cell></row><row><cell cols="3">availability of millions of training examples. The large amount</cell></row><row><cell cols="3">of variation in the sizes of the structures of interest and the lack</cell></row><row><cell cols="3">of large amount of labeled data for the multi-class scenario that</cell></row><row><cell cols="3">considers both pre-invasive and invasive stages of breast can-</cell></row><row><cell cols="3">cer presents outstanding challenges to both traditional and deep</cell></row><row><cell>learning-based approaches.</cell><cell></cell><cell></cell></row><row><cell cols="3">Besides this work, the only other deep learning study that</cell></row><row><cell cols="3">considered this challenging range of histologic categories re-</cell></row><row><cell>flecting the actual clinical practice is</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Confusion matrix for patch-level classification.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Predicted</cell><cell>TPR &amp;</cell></row><row><cell></cell><cell></cell><cell>NP</cell><cell>P</cell><cell>ADH DCIS</cell><cell>IDC Recall</cell></row><row><cell></cell><cell cols="2">NP 2,477</cell><cell>945</cell><cell cols="2">725 2,027 4,227 0.2382</cell></row><row><cell></cell><cell>P</cell><cell>503</cell><cell>7,246</cell><cell>3,364 7,823</cell><cell>3,275 0.3262</cell></row><row><cell>True</cell><cell>ADH</cell><cell cols="2">4,092 9,249</cell><cell cols="2">5,727 10,572 4,511 0.1677</cell></row><row><cell></cell><cell>DCIS</cell><cell cols="4">5,003 23,074 7,068 47,412 9,550 0.5147</cell></row><row><cell></cell><cell>IDC</cell><cell cols="2">661 9,145</cell><cell cols="2">509 21,491 18,978 0.3737</cell></row><row><cell></cell><cell cols="5">FPR 0.0515 0.2263 0.0665 0.3566 0.1357</cell></row><row><cell cols="6">Precision 0.1945 0.1459 0.3293 0.5308 0.4681</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Confusion matrix for slide-level classification.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Classification accuracies for the pathologists, the proposed deep learning-based method, and the state-of-the-art hand-crafted feature representations and classifiers.</figDesc><table><row><cell>Accuracy (%)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment B. Gecer and S. Aksoy were supported in part by the Scientific and Technological Research Council of Turkey (grant 113E602) and in part by the GEBIP Award from the Turkish Academy of Sciences. E. Mercan, L. G. Shapiro, D. L. Weaver, and J. G. Elmore were supported in part by the National Cancer Institute of the National Institutes of Health (awards R01-CA172343 and R01-140560). The content is solely the responsibility of the authors and does not necessarily represent the views of the National Cancer Institute or the National Institutes of Health.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>We confirm that there are no known conflicts of interest associated with this work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T A C C E P T E D M A N U S C R I P T</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Breast cancer histopathology image analysis: A review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Veta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P W</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Van Diest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1400" to="1411" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Histopathological image analysis: A review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Gurcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yener</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Reviews in Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="147" to="171" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Janowczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pathology Informatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diagnostic concordance among pathologists interpreting breast biopsy specimens</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Longton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Geller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Onega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N A</forename><surname>Tosteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pepe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Schnitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>O'malley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of American Medical Association</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1122" to="1132" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histological features associated with diagnostic agreement in atypical ductal hyperplasia of the breast: Illustrative cases from the B-Path study</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Rendi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Histopathology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="1028" to="1046" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Eye movements as an index of pathologist visual expertise: A pilot study</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Brunye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Elmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computerized classification of intraductal breast lesions using histopathological images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Badve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bilgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sertel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Gurcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1977" to="1984" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName><forename type="first">F</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Irshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-Y</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Lerwill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Brachtel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Knoblauch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Montaser-Kouhsari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K F</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Faulkner-Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Wilbur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Schnitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational pathology to discriminate benign from malignant intraductal proliferations of the breast</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sertel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Saltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Gurcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer-aided evaluation of neuroblastoma on whole-slide histology images: Classifying grade of neuroblastic differentiation</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1080" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gurcan, Computer-aided prognosis of neuroblastoma on whole-slide images: Classification of stromal development</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sertel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Catalyurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Saltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1093" to="1103" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A boosted Bayesian multiresolution classifier for prostate cancer detection from digitized needle biopsies</title>
		<author>
			<persName><forename type="first">S</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomaszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1205" to="1218" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-field-of-view framework for distinguishing tumor grade in ER+ breast cancer from entire histopathology slides</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basavanhally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomaszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2089" to="2099" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Invasive ductal breast carcinoma detector that is robust to image magnification in whole digital slides</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balazsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blanco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zoroquiain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Burnier</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Elmore, Multi-instance multi-label learning for multi-class classification of whole slide breast histopathology images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mercan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mercan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="316" to="325" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated detection of diagnostically relevant regions in H&amp;E stained digital pathology slides</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bahlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chekkoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khurd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kamen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Krupinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE Medical Imaging Symposium</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Elmore, Localization of diagnostically relevant regions of interest in whole slide images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mercan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brunye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1179" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Elmore, Localization of diagnostically relevant regions of interest in whole slide images: A comparative study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mercan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aksoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Brunye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="496" to="506" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated detection of DCIS in wholeslide H&amp;E stained breast histopathology images</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balkenhol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A W M</forename><surname>Van Der Laak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2141" to="2150" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate and reproducible invasive breast cancer detection in whole slide images: A deep learning approach for quantifying tumor extent</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cruz-Roa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basavanhally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomaszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">46450</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Timofeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hermsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nagtegaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hulsbergen-Van De Kaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Der Laak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date>26286</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Supervised graph hashing for histopathology image retrieval and classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gadepalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohlberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Q</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02442</idno>
		<title level="m">Detecting cancer metastases on gigapixel pathology images</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Mercan</surname></persName>
		</author>
		<title level="m">Digital pathology: Diagnostic errors, viewing behavior and image characteristics</title>
		<meeting><address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning to segment breast biopsy whole slide images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mercan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Applications of Computer Vision</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Development of a diagnostic test set to assess agreement in breast pathology: practical application of the Guidelines for Reporting Reliability and Agreement Studies (GRRAS)</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Oster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Reisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Longton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Onega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pepe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Geller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N A</forename><surname>Tosteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Elmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Women&apos;s Health</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A randomized study comparing digital imaging to traditional glass slide microscopy for breast biopsy and cancer diagnosis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Elmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Longton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pepe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Geller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Onega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N A</forename><surname>Tosteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mercan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Brunye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pathology Informatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="640" to="651" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Accuracy is in the eyes of the pathologist: The visual interpretive process and diagnostic accuracy with digital whole slide images</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Brunye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mercan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Elmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="171" to="179" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Color-blob-based COSFIRE filters for object recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="165" to="174" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Patchbased convolutional neural network for whole slide tissue image classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Kurc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Saltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2424" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Matconvnet -Convolutional Neural Networks for MATLAB</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/matconvnet/" />
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Detection and classification of breast cancer in whole slide histopathology images using deep convolutional networks, Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gecer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Ankara, Turkey</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Bilkent University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Approximate statistical tests for comparing supervised learning algorithms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1895" to="1923" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene CNNs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Grun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07757</idno>
		<title level="m">A taxonomy and library for visualizing learned features in convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
