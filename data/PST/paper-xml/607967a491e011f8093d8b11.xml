<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lorentzian Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-15">15 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiding</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
							<email>shichuan@bupt.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications Beijing</orgName>
								<address>
									<settlement>Xiao Wang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Beijing University of Posts and Telecommunications Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Nian Liu</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Beijing University of Posts and Telecommunications Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Guojie Song</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lorentzian Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-15">15 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3449872</idno>
					<idno type="arXiv">arXiv:2104.07477v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph representation learning</term>
					<term>hyperbolic space</term>
					<term>deep learning</term>
					<term>representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional networks (GCNs) have received considerable research attention recently. Most GCNs learn the node representations in Euclidean geometry, but that could have a high distortion in the case of embedding graphs with scale-free or hierarchical structure. Recently, some GCNs are proposed to deal with this problem in non-Euclidean geometry, e.g., hyperbolic geometry. Although hyperbolic GCNs achieve promising performance, existing hyperbolic graph operations actually cannot rigorously follow the hyperbolic geometry, which may limit the ability of hyperbolic geometry and thus hurt the performance of hyperbolic GCNs. In this paper, we propose a novel hyperbolic GCN named Lorentzian graph convolutional network (LGCN), which rigorously guarantees the learned node features follow the hyperbolic geometry. Specifically, we rebuild the graph operations of hyperbolic GCNs with Lorentzian version, e.g., the feature transformation and non-linear activation. Also, an elegant neighborhood aggregation method is designed based on the centroid of Lorentzian distance. Moreover, we prove some proposed graph operations are equivalent in different types of hyperbolic geometry, which fundamentally indicates their correctness. Experiments on six datasets show that LGCN performs better than the state-of-the-art methods. LGCN has lower distortion to learn the representation of tree-likeness graphs compared with existing hyperbolic GCNs. We also find that the performance of some hyperbolic GCNs can be improved by simply replacing the graph operations with those we defined in this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref> are powerful deep representation learning methods for graphs. The current GCNs usually follow a message passing manner, where the key steps are feature transformation and neighborhood aggregation. Specifically, GCNs leverage feature transformation to transform the features into higher-level features, and neighborhood aggregation in GCNs averages the features of its local neighborhood for a given node. GCNs have aroused considerable attention <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref> and are widely used in many application areas, e.g., natural language processing <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b57">58]</ref>, recommendation <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b58">59]</ref> and disease prediction <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Most GCNs learn the node features in Euclidean spaces. However, some studies find that compared with Euclidean geometry, hyperbolic geometry actually can provide more powerful ability to embed graphs with scale-free or hierarchical structure <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>. As a consequence, several recent efforts begin to define graph operations in hyperbolic spaces (e.g., feature transformation, neighborhood aggregation), and propose hyperbolic GCNs in different ways <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b59">60]</ref>. For instance, HGCN <ref type="bibr" target="#b5">[6]</ref> extends the graph convolution on the hyperboloid manifold of hyperbolic spaces, while HAT <ref type="bibr" target="#b59">[60]</ref> leverages the PoincarÃ© ball manifold to design hyperbolic graph operations.</p><p>Despite the promising performance of hyperbolic GCNs, existing hyperbolic message passing rules do not rigorously follow hyperbolic geometry, which may not fully embody the ability of hyperbolic spaces. Specifically, these hyperbolic GCNs suffer from the following issues: (1) Some hyperbolic graph operations could make node features out of the hyperbolic spaces. For example, a critical step of HGCN <ref type="bibr" target="#b5">[6]</ref>, the feature transformation, is actually conducted in tangent spaces. However, it ignores the constraint of Lorentzian scalar product in tangent spaces, which leads to the node features deviate from the hyperboloid manifold. <ref type="bibr" target="#b1">(2)</ref> The current hyperbolic neighborhood aggregations do not conform to the same mathematical meanings with Euclidean one, which could cause a distortion for the learned node features. Actually, the mathematical meanings of Euclidean neighborhood aggregation can be considered as the weighted arithmetic mean or centroid of the representations of node neighbors. However, the neighborhood aggregation in hyperbolic GCN may not obey the similar rules in hyperbolic spaces.</p><p>Taking HGCN <ref type="bibr" target="#b5">[6]</ref> as an example, it aggregates the node features in tangent spaces, which can only meet the mathematical meanings in tangent spaces, rather than hyperbolic spaces. Since we aim to build a hyperbolic GCN, it is a fundamental requirement to ensure the basic graph operations rigorously follow the hyperbolic geometry and mathematical meaning, so that we can well possess the capability of preserving the graph structure and property in the hyperbolic spaces.</p><p>In this paper, we propose a novel Lorentzian Graph Convolutional Network (LGCN), which designs a unified framework of graph operations on the hyperboloid model of hyperbolic spaces. The rigorous hyperbolic graph operations, including feature transformation and non-linearity activation, are derived from this framework to ensure the transformed node features follow the hyperbolic geometry. Also, based on the centroid of Lorentzian distance, an elegant hyperbolic neighborhood aggregation is proposed to make sure the node features are aggregated to satisfy the mathematical meanings. Moreover, we theoretically prove that some proposed graph operations are equivalent to those defined in another typical hyperbolic geometry, i.e., the PoincarÃ© ball model <ref type="bibr" target="#b12">[13]</ref>, so the proposed methods elegantly bridge the relation of these graph operations in different models of hyperbolic spaces, and also indicates the proposed methods fill the gap of lacking rigorously graph operations on the hyperboloid model. We conduct extensive experiments to evaluate the performance of LGCN, well demonstrating the superiority of LGCN in link prediction and node classification tasks, and LGCN has lower distortion when learning the representation of tree-likeness graphs compared with existing hyperbolic GCNs. We also find the proposed Lorentzian graph operations can enhance the performance of existing hyperbolic GCN in molecular property prediction task, by simply replacing their operation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Graph neural networks</head><p>Graph neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b44">45]</ref>, which extend the deep neural network to deal with graph data, have achieved great success in solving machine learning problems. There are two main families of GNNs have been proposed, i.e., spectral methods and spatial methods. Spectral methods learn node representation via generalizing convolutions to graphs. Bruna et al. <ref type="bibr" target="#b4">[5]</ref> extended convolution from Euclidean data to arbitrary graph-structured data by finding the corresponding Fourier basis of the given graph. Defferrard et al. <ref type="bibr" target="#b9">[10]</ref> leveraged K-order Chebyshev polynomials to approximate the convolution filter. Kipf et al. <ref type="bibr" target="#b24">[25]</ref> proposed GCN, which utilized a first-order approximation of ChebNet to learn the node representations. Niepert et al. <ref type="bibr" target="#b36">[37]</ref> normalized each node and its neighbors, which served as the receptive field for the convolutional operation. Wu et al. <ref type="bibr" target="#b54">[55]</ref> proposed simple graph convolution by converting the graph convolution to a linear version. Moreover, some researchers defined graph convolutions in the spatial domain. Li et al. <ref type="bibr" target="#b28">[29]</ref> proposed the gated graph neural network by using the Gate Recurrent Units (GRU) in the propagation step. VeliÄkoviÄ‡ et al. <ref type="bibr" target="#b51">[52]</ref> studied the attention mechanism in GCN to incorporate the attention mechanism into the propagation step. Chen et al. <ref type="bibr" target="#b6">[7]</ref> sampled a fix number of nodes for each graph convolutional layer to improve its efficiency. Ma et al. <ref type="bibr" target="#b31">[32]</ref> obtained the sequential information of edges to model the dynamic information as graph evolving. A comprehensive review can be found in recent surveys <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b60">61]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hyperbolic graph representation learning</head><p>Recently, node representation learning in hyperbolic spaces has received increasing attention. Nickel et al. <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> embedded graph into hyperbolic spaces to learn the hierarchical node representation. Sala et al. <ref type="bibr" target="#b42">[43]</ref> proposed a novel combinatorial embedding approach as well as a approach to Multi-Dimensional Scaling in hyperbolic spaces. To better modeling hierarchical node representation, Ganea et al. <ref type="bibr" target="#b11">[12]</ref> and Suzuki et al. <ref type="bibr" target="#b47">[48]</ref> embedded the directed acyclic graphs into hyperbolic spaces to learn their hierarchical feature representations. Law et al. <ref type="bibr" target="#b26">[27]</ref> analyzed the relation between hierarchical representations and Lorentzian distance. Also, BalaÅ¾eviÄ‡ et al. <ref type="bibr" target="#b2">[3]</ref> analyzed the hierarchical structure in multi-relational graph, and embedded them in hyperbolic spaces. Moreover, some researchers began to study the deep learning in hyperbolic spaces. Ganea et al. <ref type="bibr" target="#b12">[13]</ref> generalized deep neural models in hyperbolic spaces, such as recurrent neural networks and GRU. Gulcehre et al. <ref type="bibr" target="#b17">[18]</ref> proposed the attention mechanism in hyperbolic spaces. There are some attempts in hyperbolic GCNs recently. Liu et al. <ref type="bibr" target="#b29">[30]</ref> proposed graph neural networks in hyperbolic spaces which focuses on graph classification problem. Chami et al. <ref type="bibr" target="#b5">[6]</ref> leveraged hyperbolic graph convolution to learn the node representation in hyperboloid model. Zhang et al. <ref type="bibr" target="#b59">[60]</ref> proposed graph attention network in PoincarÃ© ball model to embed some hierarchical and scale-free graphs with low distortion. Bachmann et al. <ref type="bibr" target="#b1">[2]</ref> also generalized graph convolutional in a non-Euclidean setting. Although these hyperbolic GCNs have achieved promising results, we find that some basis properties of GCNs are not well preserved. so how to design hyperbolic GCNs in a principled manner is still an open question. The detailed of existing hyperbolic GCNs will be discussed in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES 3.1 Hyperbolic geometry</head><p>Hyperbolic geometry is a non-Euclidean geometry with a constant negative curvature. The hyperboloid model, as one typical equivalent model which well describes hyperbolic geometry, has been widely used <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36]</ref>. Let x, y âˆˆ R ğ‘›+1 , then the Lorentzian scalar product is defined as:</p><formula xml:id="formula_0">âŸ¨x, yâŸ© L := âˆ’ğ‘¥ 0 ğ‘¦ 0 + ğ‘› âˆ‘ï¸ ğ‘–=1 ğ‘¥ ğ‘– ğ‘¦ ğ‘– .<label>(1)</label></formula><p>We denote H ğ‘›,ğ›½ as the ğ‘›-dimensional hyperboloid manifold with constant negative curvature âˆ’1/ğ›½ (ğ›½ &gt; 0):</p><formula xml:id="formula_1">H ğ‘›,ğ›½ := {x âˆˆ R ğ‘›+1 : âŸ¨x, xâŸ© L = âˆ’ğ›½, ğ‘¥ 0 &gt; 0}.<label>(2)</label></formula><p>Also, for x, y âˆˆ H ğ‘›,ğ›½ , Lorentzian scalar product satisfies:</p><p>âŸ¨x, yâŸ© L â‰¤ âˆ’ğ›½, and âŸ¨x, yâŸ© L = âˆ’ğ›½ iff x = y.</p><p>The tangent space at x is defined as a ğ‘›-dimensional vector space approximating H ğ‘›,ğ›½ around x,</p><formula xml:id="formula_3">T x H ğ‘›,ğ›½ := {v âˆˆ R ğ‘›+1 : âŸ¨v, xâŸ© L = 0}.<label>(4)</label></formula><p>Note that Eq. ( <ref type="formula" target="#formula_3">4</ref>) has a constraint of Lorentzian scalar product. Also, for v, w âˆˆ T x H ğ‘›,ğ›½ , a Riemannian metric tensor is given as ğ‘”</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ›½</head><p>x (v, w) := âŸ¨v, wâŸ© L . Then the hyperboloid model is defined as the hyperboloid manifold H ğ‘›,ğ›½ equipped with the Riemannian metric tensor ğ‘”</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ›½</head><p>x . The mapping between hyperbolic spaces and tangent spaces can be done by exponential map and logarithmic map. The exponential map is a map from subset of a tangent space of H ğ‘›,ğ›½ (i.e., T x H ğ‘›,ğ›½ ) to H ğ‘›,ğ›½ itself. The logarithmic map is the reverse map that maps back to the tangent space. For points x, y âˆˆ H ğ‘›,ğ›½ , v âˆˆ T x H ğ‘›,ğ›½ , such that v â‰  0 and x â‰  y, the exponential map exp ğ›½ x (â€¢) and logarithmic map log ğ›½ x (â€¢) are given as follows:</p><formula xml:id="formula_4">exp ğ›½ x (v) = cosh âˆ¥vâˆ¥ L âˆšï¸ ğ›½ x + âˆšï¸ ğ›½ sinh âˆ¥vâˆ¥ L âˆšï¸ ğ›½ v âˆ¥vâˆ¥ L ,<label>(5) log</label></formula><formula xml:id="formula_5">ğ›½ x (y) = ğ‘‘ ğ›½ H (x, y) y + 1 ğ›½ âŸ¨x, yâŸ© L x âˆ¥y + 1 ğ›½ âŸ¨x, yâŸ© L xâˆ¥ L ,<label>(6)</label></formula><p>where âˆ¥vâˆ¥ L = âˆšï¸ âŸ¨v, vâŸ© L denotes Lorentzian norm of v and ğ‘‘ ğ›½ H (â€¢, â€¢) denotes the intrinsic distance function between two points x, y âˆˆ H ğ‘‘,ğ›½ , which is given as:</p><formula xml:id="formula_6">ğ‘‘ ğ›½ H (x, y) = âˆšï¸ ğ›½ arcosh âˆ’ âŸ¨x, yâŸ© L /ğ›½ .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hyperbolic graph convolutional networks</head><p>Recently, several hyperbolic GCNs have been proposed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b59">60]</ref>.</p><p>Here we use HGCN <ref type="bibr" target="#b5">[6]</ref>, which extends Euclidean graph convolution to the hyperboloid model, as a typical example to illustrate the basic framework of hyperbolic GCN. Let h</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘˜,ğ›½ ğ‘–</head><p>âˆˆ H ğ‘˜,ğ›½ be a ğ‘˜-dimensional node feature of node ğ‘–, ğ‘ (ğ‘–) be a set of its neighborhoods with aggregation weight ğ‘¤ ğ‘– ğ‘— , and M be a (ğ‘‘ +1) Ã— (ğ‘˜ +1) weight matrix. The message passing rule of HGCN consists of feature transformation:</p><formula xml:id="formula_7">h ğ‘‘,ğ›½ ğ‘– = exp ğ›½ 0 (M log ğ›½ 0 (h ğ‘˜,ğ›½ ğ‘– )),<label>(8)</label></formula><p>and neighborhood aggregation:</p><formula xml:id="formula_8">ğ´ğºğº ğ›½ (h ğ‘‘,ğ›½ ğ‘– ) = exp ğ›½ h ğ‘– âˆ‘ï¸ ğ‘— âˆˆğ‘ (ğ‘–)âˆª{ğ‘– } ğ‘¤ ğ‘– ğ‘— log ğ›½ h ğ‘– (h ğ‘‘,ğ›½ ğ‘— ) . (<label>9</label></formula><formula xml:id="formula_9">)</formula><p>As we can see in Eq. ( <ref type="formula" target="#formula_7">8</ref>), the features are transformed from hyperbolic spaces to tangent spaces via logarithmic map log ğ›½ 0 (â€¢). However, the basic constraint of tangent spaces in Eq. (4), âŸ¨v,</p><formula xml:id="formula_10">xâŸ© L = 0, is violated, since âŸ¨M log ğ›½ 0 (h ğ‘˜,ğ›½ ğ‘– ), 0âŸ© L â‰  0, 0 = ( âˆšï¸ ğ›½, 0, â€¢ â€¢ â€¢ , 0) âˆˆ H ğ‘˜,ğ›½</formula><p>. As a consequence, the node features would be out of the hyperbolic spaces after projecting them back to hyperboloid manifold via the exponential map exp ğ›½ 0 , which do not satisfy hyperbolic geometry rigorously.</p><p>On the other hand, in Euclidean spaces, the node feature h ğ‘‘ ğ‘– âˆˆ R ğ‘‘ aggregates information from its neighborhoods via ğ‘— âˆˆğ‘ (ğ‘–)âˆª{ğ‘–} ğ‘¤ ğ‘–ğ‘— h ğ‘‘ ğ‘— , which has the following meaning in mathematics: Remark 3.1. Given a node, the neighborhood aggregation essentially is the weighted arithmetic mean for features of its local neighborhoods <ref type="bibr" target="#b54">[55]</ref>. Also, the feature of aggregation is the centroid of the neighborhood features in geometry.</p><p>Remark 3.1 indicates the mathematical meanings of neighborhood aggregation in Euclidean spaces. Therefore, the neighborhood aggregation in Eq. ( <ref type="formula" target="#formula_8">9</ref>) should also follow the same meanings with Euclidean one in hyperbolic spaces. However, we can see that the Eq. ( <ref type="formula" target="#formula_8">9</ref>) in HGCN only meets these meanings in tangent spaces rather than hyperbolic spaces, which could cause a distortion for the features. To sum up, the above issues indicate existing hyperbolic graph operations do not follow mathematic fundamentally, which may cause potential untrustworthy problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LGCN: OUR PROPOSED MODEL</head><p>In order to solve the issues of existing hyperbolic GCNs, we propose LGCN, which designs graph operations to guarantee the mathematical meanings in hyperbolic spaces. Specifically, LGCN first maps the input node features into hyperbolic spaces and then conducts feature transformation via a delicately designed Lorentzian matrix-vector multiplication. Also, the centroid based Lorentzian aggregation is proposed to aggregate features, and the aggregation weights are learned by a self attention mechanism. Moreover, Lorentzian pointwise non-linear activation is followed to obtain the output node features. Note that the curvature of a hyperbolic space (i.e., âˆ’1/ğ›½) is also a trainable parameter for LGCN. Despite the same expressive power, adjusting curvature of LGCN is important in practice due to factors of limited machine precision and normalization. The details of LGCN are introduced in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mapping feature with different curvature</head><p>The input node features of LGCN could live in the Euclidean spaces or hyperbolic spaces. For ğ‘˜-dimensional input features, we denote them as h ğ‘˜,ğ¸ âˆˆ R ğ‘˜ ( ğ¸ indicates Euclidean spaces) and h ğ‘˜,ğ›½ â€² âˆˆ H ğ‘˜,ğ›½ â€² , respectively. If original features live in Euclidean spaces, we need to map them into hyperbolic spaces. We assume that the input features h ğ‘˜,ğ¸ live in the tangent space of H ğ‘˜,ğ›½ at its origin 0</p><formula xml:id="formula_11">= ( âˆšï¸ ğ›½, 0, â€¢ â€¢ â€¢ , 0) âˆˆ H ğ‘˜,ğ›½ , i.e.</formula><p>, T 0 H ğ‘˜,ğ›½ . A "0" element is added at the first coordinate of h ğ‘˜,ğ¸ to satisfy the constraint âŸ¨(0, h ğ‘˜,ğ¸ ), 0âŸ© L = 0 in Eq. ( <ref type="formula" target="#formula_3">4</ref>). Thus, the input feature h ğ‘˜,ğ¸ âˆˆ R ğ‘˜ can be mapped to the hyperbolic spaces via exponential map:</p><formula xml:id="formula_12">h ğ‘˜,ğ›½ = exp ğ›½ 0 (0, h ğ‘˜,ğ¸ ) . (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>If the input features h ğ‘˜,ğ›½ â€² live in a hyperbolic space (e.g., the output of previous LGCN layer), whose curvature âˆ’1/ğ›½ â€² might be different with the curvature of current hyperboloid model. We can transform it into the hyperboloid model with a specific curvature âˆ’1/ğ›½:</p><formula xml:id="formula_14">h ğ‘˜,ğ›½ = exp ğ›½ 0 (log ğ›½ â€² 0 (h ğ‘˜,ğ›½ â€² )).<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lorentzian feature transformation</head><p>Hyperbolic spaces are not vector spaces, which means the operations in Euclidean spaces cannot be applied in hyperbolic spaces.</p><p>To ensure the transformed features satisfy the hyperbolic geometry, it is crucial to define some canonical transformations in the hyperboloid model, so we define:</p><formula xml:id="formula_15">Definition 4.1 (Lorentzian version). For ğ‘“ : R ğ‘› â†’ R ğ‘š (ğ‘›, ğ‘š &gt; 2) and two points x = (ğ‘¥ 0 , â€¢ â€¢ â€¢ , ğ‘¥ ğ‘› ) âˆˆ H ğ‘›,ğ›½ , v = (ğ‘£ 0 , â€¢ â€¢ â€¢ , ğ‘£ ğ‘› ) âˆˆ T 0 H ğ‘›,ğ›½</formula><p>, we define the Lorentzian version of ğ‘“ as the map H ğ‘›,ğ›½ â†’ H ğ‘š,ğ›½ by:</p><formula xml:id="formula_16">ğ‘“ âŠ— ğ›½ (x) := exp ğ›½ 0 ( f (log ğ›½ 0 (x))), f (v) := (0, ğ‘“ (ğ‘£ 1 , â€¢ â€¢ â€¢ , ğ‘£ ğ‘› )), (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>where exp</p><formula xml:id="formula_18">ğ›½ 0 : T 0 H ğ‘›,ğ›½ â†’ H ğ‘š,ğ›½ and log ğ›½ 0 : H ğ‘›,ğ›½ â†’ T 0 H ğ‘š,ğ›½ .</formula><p>Lorentzian version leverages logarithmic and exponential map to project the features between hyperbolic spaces and tangent spaces. As the tangent spaces are vector spaces and isomorphic to R ğ‘› , the Euclidean transformations can be applied to the tangent spaces. Moreover, given a point v = (ğ‘£ 0 , â€¢ â€¢ â€¢ , ğ‘£ ğ‘› ) âˆˆ T 0 H ğ‘›,ğ›½ , existing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref> directly apply the Euclidean transformations on all coordinates (ğ‘£ 0 , â€¢ â€¢ â€¢ , ğ‘£ ğ‘› ) in tangent spaces. Different from these methods, Lorentzian version only leverages the Euclidean transformations on the last ğ‘› coordinates (ğ‘£ 1 , â€¢ â€¢ â€¢ , ğ‘£ ğ‘› ) in tangent spaces, and the first coordinate (ğ‘£ 0 ) is set as "0" to satisfy the constraint in Eq. ( <ref type="formula" target="#formula_3">4</ref>). Thus, this operation can make sure the transformed features rigorously follow the hyperbolic geometry.</p><p>In order to apply linear transformation on the hyperboloid model, following Lorentzian version, the Lorentzian matrix-vector multiplication can be derived: Definition 4.2 (Lorentzian matrix-vector multiplication). If M : R ğ‘› â†’ R ğ‘š is a linear map with matrix representation, given two points</p><formula xml:id="formula_19">x = (ğ‘¥ 0 , â€¢ â€¢ â€¢ , ğ‘¥ ğ‘› ) âˆˆ H ğ‘›,ğ›½ , v = (ğ‘£ 0 , â€¢ â€¢ â€¢ , ğ‘£ ğ‘› ) âˆˆ T 0 H ğ‘›,ğ›½ , we have: M âŠ— ğ›½ (x) = exp ğ›½ 0 ( M(log ğ›½ 0 (x))), M(v) = (0, M(ğ‘£ 1 , â€¢ â€¢ â€¢ , ğ‘£ ğ‘› )). (13) Let M be a ğ‘š Ã— ğ‘› matrix, M â€² be a ğ‘™ Ã— ğ‘š matrix, x âˆˆ H ğ‘›,ğ›½ , MâŠ— ğ›½ x := M âŠ— ğ›½ (x), we have matrix associativity as: (M â€² M)âŠ— ğ›½ x = M â€² âŠ— ğ›½ (MâŠ— ğ›½ x).</formula><p>A key difference between Lorentzian matrix-vector multiplication and other matrix-vector multiplications on the hyperboloid model <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref> is the size of the matrix M. Assuming a ğ‘›-dimensional feature needs to be transformed into a ğ‘š-dimensional feature. Naturally, the size of matrix M should be ğ‘š Ã— ğ‘›, which is satisfied by Lorentzian matrix-vector multiplication. However, the size of matrix M is (ğ‘š + 1) Ã— (ğ‘› + 1) for other methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref> (as shown in Eq. ( <ref type="formula" target="#formula_7">8</ref>)), which leads to the constraint of tangent spaces cannot be satisfied, i.e., âŸ¨M log ğ›½ 0 (h ğ‘˜,ğ›½ ğ‘– ), 0âŸ© L â‰  0 in Eq. ( <ref type="formula" target="#formula_3">4</ref>), so the transformed features would be out of the hyperbolic spaces. Moreover, the Lorentzian matrix vector multiplication has the following property: Theorem 4.1. Given a point in hyperbolic space, which is represented by x ğ‘›,ğ›½ âˆˆ H ğ‘›,ğ›½ using hyperboloid model or x ğ‘›,ğ›¼ âˆˆ D ğ‘›,ğ›¼ using PoincarÃ© ball model <ref type="bibr" target="#b12">[13]</ref>, respectively. Let M be a ğ‘š Ã— ğ‘› matrix, Lorentzian matrix-vector multiplication MâŠ— ğ›½ x ğ‘›,ğ›½ used in hyperboloid model is equivalent to MÃ¶bius matrix-vector multiplication MâŠ— ğ›¼ x ğ‘›,ğ›¼ used in PoincarÃ© ball model.</p><p>The proof is in Appendix B.1. This property elegantly bridges the relation between the hyperboloid model and PoincarÃ© ball model w.r.t. matrix-vector multiplication. We use the Lorentzian matrixvector multiplication to conduct feature transformation on the hyperboloid model as:</p><formula xml:id="formula_20">h ğ‘‘,ğ›½ = MâŠ— ğ›½ h ğ‘˜,ğ›½ .<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Lorentzian neighborhood aggregation</head><p>As in Remark 3.1, in Euclidean spaces, the neighborhood aggregation is to compute the weight arithmetic mean or centroid (also called center of mass) of its neighborhood features (see Fig. <ref type="figure" target="#fig_1">1(a)</ref>). Therefore, we aim to aggregate neighborhood features in hyperbolic spaces to follow these meanings. FrÃ©chet mean <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> provides a feasible way to compute the centroid in Riemannian manifold. Also, the arithmetic mean can be interpreted as a kind of FrÃ©chet mean. Thus, FrÃ©chet mean meets the meanings of neighborhood aggregation. The main idea of FrÃ©chet mean is to minimize an expectation of (squared) distances with a set of points. However, FrÃ©chet mean does not have a closed form solution w.r.t. the intrinsic distance ğ‘‘ ğ›½ H in hyperbolic spaces, and it has to be inefficiently computed by gradient descent. Therefore, we propose an elegant neighborhood aggregation method based on the centroid of the squared Lorentzian distance, which can well balance the mathematical meanings and efficiency: Theorem 4.2 (Lorentzian aggregation via centroid of sqared Lorentzian distance). For a node feature h ğ‘‘,ğ›½ ğ‘– âˆˆ H ğ‘‘,ğ›½ , a set of its neighborhoods ğ‘ (ğ‘–) with aggregation weights ğ‘¤ ğ‘– ğ‘— &gt; 0, the neighborhood aggregation consists in the centroid c ğ‘‘,ğ›½ of nodes, which minimizes the problem:</p><formula xml:id="formula_21">arg min c ğ‘‘,ğ›½ âˆˆH ğ‘‘,ğ›½ âˆ‘ï¸ ğ‘— âˆˆğ‘ (ğ‘–)âˆª{ğ‘– } ğ‘¤ ğ‘– ğ‘— ğ‘‘ 2 L (h ğ‘‘,ğ›½ ğ‘— , c ğ‘‘,ğ›½ ),<label>(15)</label></formula><p>where ğ‘‘ 2 L (â€¢, â€¢) denotes squared Lorentzian distance, and this problem has closed form solution:</p><formula xml:id="formula_22">c ğ‘‘,ğ›½ = âˆšï¸ ğ›½ ğ‘— âˆˆğ‘ (ğ‘–)âˆª{ğ‘– } ğ‘¤ ğ‘– ğ‘— h ğ‘‘,ğ›½ ğ‘— |âˆ¥ ğ‘— âˆˆğ‘ (ğ‘–)âˆª{ğ‘– } ğ‘¤ ğ‘– ğ‘— h ğ‘‘,ğ›½ ğ‘— âˆ¥ L | . (<label>16</label></formula><formula xml:id="formula_23">)</formula><p>The proof is given in Appendix B.2. For points x ğ‘›,ğ›½ , y ğ‘›,ğ›½ âˆˆ H ğ‘›,ğ›½ , the squared Lorentzian distance is defined as <ref type="bibr" target="#b39">[40]</ref>:</p><formula xml:id="formula_24">ğ‘‘ 2 L (x ğ‘›,ğ›½ , y ğ‘›,ğ›½ ) = âˆ’2ğ›½ âˆ’ 2âŸ¨x ğ‘›,ğ›½ , y ğ‘›,ğ›½ âŸ© L .<label>(17)</label></formula><p>Fig. <ref type="figure" target="#fig_1">1</ref>(b) illustrates Lorentzian aggregation via centroid. Similar to FrÃ©chet/Karcher means, the node features computed by Lorentzian aggregation are the minimum of an expectation of squared Lorentzian distance. Also, the features of aggregation in Lorentzian neighborhood aggregation are the centroids in the hyperboloid model in geometry <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b39">40]</ref>. On the other hand, some hyperbolic GCNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b59">60]</ref> aggregate neighborhoods in tangent spaces (as shown in Fig. <ref type="figure" target="#fig_1">1(c</ref>)), that can only be regarded as centroid or arithmetic mean in the tangent spaces, rather than hyperbolic spaces. Thus Lorentzian aggregation via centroid of squared Lorentzian distance is a promising method, which satisfies more elegant mathematical meanings compared to other hyperbolic GCNs.  As shown in Eq. ( <ref type="formula" target="#formula_22">16</ref>), there is an aggregation weight ğ‘¤ ğ‘– ğ‘— indicating the importance of neighborhoods for a center node. Here we propose a self-attention mechanism to learn the aggregation weights ğ‘¤ ğ‘– ğ‘— . For two node features h ğ‘‘,ğ›½ ğ‘– , h ğ‘‘,ğ›½ ğ‘– âˆˆ H ğ‘‘,ğ›½ , the attention coefficient ğœ‡ ğ‘– ğ‘— , which indicates the importance of node ğ‘— to node ğ‘–, can be computed as:</p><formula xml:id="formula_25">ğœ‡ ğ‘– ğ‘— = ğ´ğ‘‡ğ‘‡ (h ğ‘‘,ğ›½ ğ‘– , h ğ‘‘,ğ›½ ğ‘— , M ğ‘ğ‘¡ğ‘¡ ),<label>(18)</label></formula><p>where ğ´ğ‘‡ğ‘‡ (â€¢) indicates the function of computing the attention coefficient and the ğ‘‘ Ã— ğ‘‘ matrix M ğ‘ğ‘¡ğ‘¡ is to transform the node features into attention-based ones. Considering a large attention coefficient ğœ‡ ğ‘– ğ‘— represents a high similarity of nodes ğ‘— and ğ‘–, we define ğ´ğ‘‡ğ‘‡ (â€¢) based on squared Lorentzian distance, as</p><formula xml:id="formula_26">ğœ‡ ğ‘– ğ‘— = âˆ’ğ‘‘ 2 L (M ğ‘ğ‘¡ğ‘¡ âŠ— ğ›½ h ğ‘‘,ğ›½ ğ‘– , M ğ‘ğ‘¡ğ‘¡ âŠ— ğ›½ h ğ‘‘,ğ›½ ğ‘— ).<label>(19)</label></formula><p>For all the neighbors ğ‘ (ğ‘–) of node ğ‘– (including itself), we normalize them using the softmax function to compute the aggregation weight:</p><formula xml:id="formula_27">ğ‘¤ ğ‘– ğ‘— = exp(ğœ‡ ğ‘– ğ‘— ) ğ‘¡ âˆˆğ‘ (ğ‘–)âˆª{ğ‘– } exp(ğœ‡ ğ‘–ğ‘¡ ) . (<label>20</label></formula><formula xml:id="formula_28">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Lorentzian pointwise non-linear activation</head><p>Non-linear activation is an indispensable part of GCNs. Similar to feature transformation, existing non-linear activations on the hyperboloid model <ref type="bibr" target="#b5">[6]</ref> also make features out of the hyperboloid model. Here, we derive the Lorentzian pointwise non-linear activation following the Lorentzian version: </p><formula xml:id="formula_29">ğœ âŠ— ğ›½ (x) = exp ğ›½ 0 ( Ïƒ âŠ— ğ›½ (log ğ›½ 0 (x))), Ïƒ âŠ— ğ›½ (v) = (0, ğœ (ğ‘£ 1 ), â€¢ â€¢ â€¢ , ğœ (ğ‘£ ğ‘› ))).<label>(21)</label></formula><p>The Lorentzian pointwise non-linear activation not only ensures the transformed features still live in the hyperbolic spaces, but also has the following property. Theorem 4.3. Given a point in hyperbolic space, it is modeled by x ğ‘›,ğ›½ âˆˆ H ğ‘›,ğ›½ using hyperboloid model and x ğ‘›,ğ›¼ âˆˆ D ğ‘›,ğ›¼ using PoincarÃ© ball model, respectively. Lorentzian pointwise non-linearity ğœ âŠ— ğ›½ (x ğ‘›,ğ›½ ) in the hyperboloid model is equivalent to MÃ¶bius pointwise non-linearity ğœ âŠ— ğ›¼ (x ğ‘›,ğ›¼ ) in the PoincarÃ© ball model <ref type="bibr" target="#b12">[13]</ref>, when ğœ (â€¢) indicates some specific non-linear activation, e.g., Relu, leaklyRelu.</p><p>The proof is in Appendix B.3. This property also bridges the pointwise non-linearity in the two models. Following the Lorentzian pointwise non-linear activation, the output of the LGCN layer is:</p><formula xml:id="formula_30">u ğ‘‘,ğ›½ = ğœ âŠ— ğ›½ (c ğ‘‘,ğ›½ ),<label>(22)</label></formula><p>which can be used to downstream tasks, e.g., link prediction and node classification.  <ref type="bibr" target="#b39">[40]</ref> Hypreboloid <ref type="bibr" target="#b26">[27]</ref> the features still live in the hyperbolic spaces after applying the graph operation. We analyze this property about feature transformation and pointwise non-linearity activation, denoted as inside ğ‘“ and inside ğ‘› , respectively. Also, as mentioned in Theorem 4.2, similar with FrÃ©chet means, the neighborhood aggregation to minimize an expectation of distances could better satisfy the mathematical meanings, and this property is denoted as expect-agg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion on related works</head><p>The current hyperbolic GCNs can be classified into two classes: PoincarÃ© ball GCNs, including HGNN ğ‘ƒ <ref type="bibr" target="#b29">[30]</ref>, HAT <ref type="bibr" target="#b59">[60]</ref> and ğœ…GCN <ref type="bibr" target="#b1">[2]</ref>; Hyperboloid GCNs, i.e., HGCN <ref type="bibr" target="#b5">[6]</ref>, HGNN ğ» <ref type="bibr" target="#b29">[30]</ref> and LGCN. We summarize the properties of graph operations of these hyperbolic GCNs in Table <ref type="table" target="#tab_0">1</ref>. It can be seen that: <ref type="bibr" target="#b0">(1)</ref> The existing hyperbolic GCNs do not have all of the three properties except LGCN. More importantly, none of the existing hyperbolic neighborhood aggregation satisfy expect-agg. (2) All the PoincarÃ© ball GCNs satisfy inside ğ‘“ and inside ğ‘› , while existing hyperboloid GCNs cannot make sure these properties. That is because they do not consider the constrain of tangent spaces and the transformed features will be outside of the hyperboloid. Note that because of lacking non-linear activation on the hyperboloid model, HGNN ğ» avoids this problem by conducting non-linear activation on the PoincarÃ© ball, which is implemented via projecting node representations between the PoincarÃ© ball and hyperboloid model. That brings extra computing cost, and also indicates a principle definition of graph operations is needed for the hyperboloid model. On the other hand, LGCN fills this gap of lacking rigorously graph operations on the hyperboloid model to ensure the features can be transformed following hyperbolic geometry. (3) Only LGCN satisfies expect-agg. Most hyperbolic GCNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b59">60]</ref> leverage aggregation in the tangent spaces (as shown in Fig. <ref type="figure" target="#fig_1">1(c</ref>)), which satisfies expect-agg in the tangent spaces, instead of the hyperbolic spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Hyperbolic centroids.</head><p>There are some works exploit hyperbolic centroids. Actually, the centroid in metric spaces is to find a point which minimizes the sum of squared distance w.r.t. given points <ref type="bibr" target="#b10">[11]</ref>, and we denote this property as sum-dis. Also, the efficiency of computing centroid is important, so we concern whether a centroid has a closed-form solution, and this property is denoted as closed-form.</p><p>We summarize hyperbolic centroids as well as some related works in Table <ref type="table" target="#tab_1">2</ref>. FrÃ©chet mean <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> is a generalization of centroids to metric spaces by minimizing the sum of squared distance. Some works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b53">54]</ref> use FrÃ©chet mean in hyperbolic spaces, which do not have closed-form solution, so they have to compute them via gradient descent. Moreover, Einstein <ref type="bibr" target="#b49">[50]</ref> and MÃ¶bius gyromidpoint <ref type="bibr" target="#b50">[51]</ref> are centroids with close-form solution for two different kind of hyperbolic geometry, i.e., the Klein ball and PoincarÃ© ball model, respectively. Some researchers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref> exploit Einstein/MÃ¶bius gyromidpoint in representation learning problem. One limitation of Einstein and MÃ¶bius gyromidipoint is they cannot be seen as minimizing the sum of squared distances. Furthermore, Lorentzian centroid <ref type="bibr" target="#b39">[40]</ref> is the centroid for the hyperboloid model, which can be seen as a sum of squared distance and has closed-form solution. The relations between Lorentzian centroid and hierarchical structure data are analyzed in representations learning problem <ref type="bibr" target="#b26">[27]</ref>. To sum up, only Lorentzian centroid satisfies the two properties, and we are the first one to leverage it in hyperbolic GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Experimental setup</head><p>5.1.1 Dataset. We utilize six datasets in our experiments: Cora, Citeseer, Pubmed, <ref type="bibr" target="#b56">[57]</ref>Amazon <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b45">46]</ref>,USA <ref type="bibr" target="#b41">[42]</ref>,and Disease <ref type="bibr" target="#b5">[6]</ref>. Cora, Citeseer and Pubmed are citation networks where nodes represent scientific papers, and edges are citations between them. The Amazon is a co-purchase graph, where nodes represent goods and edges indicate that two goods are frequently bought together. The USA is a air-traffic network, and the nodes corresponding to different airports. We use one-hot encoding nodes in the USA dataset as the node features. The Disease dataset is a graph with tree structure, where node features indicate the susceptibility to the disease. The details of data statistics are shown in the Table <ref type="table" target="#tab_2">3</ref>. We compute ğ›¿ ğ‘ğ‘£ğ‘” -hyperbolicity <ref type="bibr" target="#b0">[1]</ref> to quantify the tree-likeliness of these datasets. A low ğ›¿ ğ‘ğ‘£ğ‘” -hyperbolicity of a graph indicates that it has an underlying hyperbolic geometry. The details about ğ›¿ ğ‘ğ‘£ğ‘” -hyperbolicity are shown in Appendix C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Baselines.</head><p>We compare our method with the following stateof-the-art methods: (1) A Euclidean network embedding model i.e., DeepWalk <ref type="bibr" target="#b38">[39]</ref> and a hyperbolic network embedding model i.e., PoincarÃ©Emb <ref type="bibr" target="#b34">[35]</ref>; (2) Euclidean GCNs i.e., GraphSage <ref type="bibr" target="#b18">[19]</ref>, GCN [25], GAT <ref type="bibr" target="#b51">[52]</ref>; (3) Hyperbolic GCNs i.e., HGCN <ref type="bibr" target="#b5">[6]</ref>, ğœ…GCN [2] 1 , HAT <ref type="bibr" target="#b59">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Parameter setting.</head><p>We perform a hyper-parameter search on a validation set for all methods. The grid search is performed over the following search space: Learning rate: [0.01, 0.008, 0.005, 0.001]; Dropout probability: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]; ğ¿ 2 regularization strength: [0, 1e-1, 5e-2, 1e-2, 5e-3, 1e-3, 5e-4, 1e-4]. The results are reported over 10 random parameter initializations. For all the methods, we set ğ‘‘, i.e., the dimension of latent representation as 8, 16, 32, 64 in link prediction and node classification tasks for a more comprehensive comparison. In case studies, we set the dimension of latent representation as 64. Note that the experimental setting in molecular property prediction task is same with <ref type="bibr" target="#b29">[30]</ref>. We optimize DeepWalk with SGD while optimize PoincarÃ©Emb with Riemanni-anSGD <ref type="bibr" target="#b3">[4]</ref>. The GCNs are optimized via Adam <ref type="bibr" target="#b23">[24]</ref>. Also, LGCN leverages DropConnect <ref type="bibr" target="#b52">[53]</ref> which is the generalization of Dropout and can be used in the hyperbolic GCNs <ref type="bibr" target="#b5">[6]</ref>. Moreover, although</p><p>LGCN and the learned node representations are hyperbolic, the trainable parameters in LGCN live in the tangent spaces, which can be optimized via Euclidean optimization <ref type="bibr" target="#b12">[13]</ref>, e.g., Adam <ref type="bibr" target="#b23">[24]</ref>. Furthermore, LGCN uses early stopping based on validation set performance with a patience of 100 epochs. The Hardware used in our experiments is: Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz, GeForce @ GTX 1080Ti. 1 We only consider the ğœ…GCN in the hyperbolic setting since we focus on hyperbolic GCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Link prediction</head><p>We compute the probability scores for edges by leveraging the Fermi-Dirac decoder <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref>. For the output node features u , where ğ‘Ÿ and ğ‘¡ are hyper-parameters. We then minimize the cross-entropy loss to train the LGCN model. Following <ref type="bibr" target="#b5">[6]</ref>, the edges are split into 85%, 5%, 10% randomly for training, validation and test sets for all datasets, and the evaluation metric is AUC.</p><p>The results are shown in Table <ref type="table" target="#tab_3">4</ref>. We can see that LGCN performs best in all cases, and its superiority is more significant for the low dimension setting. Suggesting the graph operations of LGCN provide powerful ability to embed graphs. Moreover, hyperbolic GCNs perform better than Euclidean GCNs for datasets with lower ğ›¿ ğ‘ğ‘£ğ‘” , which further confirms the capability of hyperbolic spaces in modeling tree-likeness graph data. Furthermore, compared with network embedding methods, GCNs achieve better performance in most cases, which indicates GCNs can benefit from both structure and feature information in a graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Node classification</head><p>Here we evaluate the performance of LGCN on the node classification task. We split nodes in Disease dataset into 30/10/60% for training, validation and test sets <ref type="bibr" target="#b5">[6]</ref>. For the other datasets, we use only 20 nodes per class for training, 500 nodes for validation,  <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b56">57]</ref>. The accuracy is used to evaluate the results. Table <ref type="table" target="#tab_4">5</ref> reports the performance. We can observe similar results to Table <ref type="table" target="#tab_3">4</ref>. That is, LGCN preforms better than the baselines in most cases. Also, hyperbolic GCNs outperform Euclidean GCNs for datasets with lower ğ›¿ ğ‘ğ‘£ğ‘” , and GCNs perform better than network embedding methods. Moreover, we notice that hyperbolic GCNs do not have an obvious advantage compared with Euclidean GCNs on Citeseer dataset, which has the biggest ğ›¿ ğ‘ğ‘£ğ‘” . We think no obvious tree-likeness structure of Citeseer makes those hyperbolic GCNs do not work well on this task. In spite of this, benefiting from the well-defined Lorentzian graph operations, LGCN also achieves very competitive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Ablations study.</head><p>Here we evaluate the effectiveness of some components in LGCN, including self attention (ğ‘ğ‘¡ğ‘¡) and trainable curvature (ğ›½). We remove these two components from LGCN and obtain two variants LGCN \ğ‘ğ‘¡ğ‘¡ and LGCN \ğ›½ , respectively. To further validate the performance of the proposed centroid-based Lorentzian aggregation, we exchange the aggregation of HGCN and LGCN \ğ‘ğ‘¡ğ‘¡ , denoted as HGCN ğ‘ and LGCN \ğ‘ğ‘¡ğ‘¡ \ğ‘ , respectively. To better analyze the ability of modeling graph with underlying hyperbolic geometry, we conduct the link prediction (LP) and node classification (NC) tasks on three datasets with lower ğ›¿ ğ‘ğ‘£ğ‘” , i.e., Disease, USA, and Amazon datasets. The results are shown in Table <ref type="table" target="#tab_5">6</ref>. Comparing LGCN to its variants, we observe that LGCN always achieves best performances, indicating the effectiveness of self attention and trainable curvature. Moreover, HGCN ğ‘ achieves better results than HGCN, while LGCN \ğ‘ğ‘¡ğ‘¡ performs better than LGCN \ğ‘ğ‘¡ğ‘¡ \ğ‘ in most cases, suggesting the effectiveness of the proposed neighborhood aggregation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Extension to graph-level task: molecular property prediction.</head><p>Most existing hyperbolic GCNs focus on node-level tasks, e.g., node classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b59">60]</ref> and link prediction <ref type="bibr" target="#b5">[6]</ref> tasks. We also notice that HGNN <ref type="bibr" target="#b29">[30]</ref>, as a hyperbolic GCN, achieves good results on graph-level task, i.e., molecular property prediction. Here we provide a Lorentzian version of HGNN named HGNN ğ¿ , which keeps the model structure of HGNN and replaces its graph operations with those operations defined in this paper, i.e., feature transformation, neighborhood aggregation, and non-linear activation. Following HGNN <ref type="bibr" target="#b29">[30]</ref>, we conduct molecular property prediction task on ZINC dataset. The experiment is a regression task to predict molecular properties: the water-octanal partition coefficient (logP), qualitative estimate of drug-likeness (QED), and synthetic accessibility score (SAS). The experimental setting is same with HGNN for a fair comparison, and we reuse the metrics already reported in HGNN for state-of-the-art techniques. HGNN implemented with PoincarÃ© ball, hyperboloid model is denoted as HGNN ğ‘ƒ , HGNN ğ» , respectively. The results of mean absolute error (MAE) are shown in Table <ref type="table" target="#tab_6">7</ref>. HGNN ğ¿ achieves best performance among all the baselines. Also, HGNN ğ¿ can further improve the performance of HGNN, which verifies the effectiveness of the proposed graph operations. Moreover, as HGNN ğ¿ is obtained by simply replacing the graph operation of HGNN ğ» , the proposed Lorentzian operations provide an alternative way for hyperbolic deep learning.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Distortion analysis.</head><p>We have mentioned that some existing hyperbolic GCNs could cause a distortion to the learned graph structure, since their graph operations do not rigorously follow the hyperbolic geometry. Thus, here we evaluate the distortions of GCNs on the Disease dataset. Following <ref type="bibr" target="#b16">[17]</ref>, we define the average distortion to evaluate the ability of GCNs to preserve graph structures.</p><p>The average distortion is defined as: ,</p><p>where ğ‘‘ (u ğ‘– , u ğ‘— ) is the intrinsic distance between the output features of nodes ğ‘– and ğ‘—, and ğ‘‘ ğº (ğ‘–, ğ‘—) is their graph distance. Both of the distances are divided by their average values, i.e., ğ‘‘ ğ‘ğ‘£ğ‘” and ğ‘‘ ğº ğ‘ğ‘£ğ‘” , to satisfy the scale invariance. The results of the link prediction are shown in Fig. <ref type="figure">2</ref>, and a lower average distortion indicates a better preservation of the graph structure. We can find that LGCN has the lowest average distortion among these GCNs, which is benefited from the well-defined graph operations. Also, all the hyperbolic GCNs have lower average distortion compared with Euclidean GCNs. That is also reasonable, since hyperbolic spaces is more suitable to embed tree-likeness graph than Euclidean spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Attention analysis.</head><p>In addition, we examine the learned attention values in LGCN. Intuitively, important neighbors tend to have large attention values. We take a node in the Disease dataset for node classification task as an illustrative example. As shown in Fig. <ref type="figure" target="#fig_4">3</ref>, the nodes are marked by their indexes in the dataset, and the node with a red outline is the center node. The color of a node indicates its label, and the attention value for a node is visualized by its edge width. We observe that the center node 3 pays more attention to nodes with the same class, i.e., nodes 0, 3, 14, 15, 16, suggesting that the proposed attention mechanism can automatically distinguish the difference among neighbors and assign the higher weights to the meaningful neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.5">Efficiency comparison.</head><p>We further analyze the efficiency of some GCNs. To better analyze the aggregation in Theorem 4.2, we provide a variant of LGCN named LGCN ğ¹ , which minimizes Eq. ( <ref type="formula" target="#formula_21">15</ref>) w.r.t. the intrinsic distance, i.e., Eq. <ref type="bibr" target="#b6">(7)</ref>. Note that the aggregation of LGCN ğ¹ is a kind of FrÃ©chet mean which dose not have closed-form solutions, so we compute it via a state-of-the-art gradient descent based method <ref type="bibr" target="#b30">[31]</ref>. Here we report the link prediction performance and training time per 100 epochs of GCNs on Disease dataset in Fig. <ref type="figure" target="#fig_5">4</ref>. One can see that GCN is the fastest. Most hyperbolic GCNs, e.g., ğœ…GCN, HAT, LGCN, are on the same level with GAT. HGCN is slower than above methods, and LGCN ğ¹ is the slowest. Although hyperbolic GCNs are slower than Euclidean GCNs, they have better performance. Moreover, HGCN is significantly slower than LGCN, since HGCN aggregates different nodes in different tangent spaces, and this process cannot be computed parallelly. HAT addresses this problem by aggregating all the nodes in the same tangent space. Despite the running time of HAT and ğœ…GCN are on par with LGCN, LGCN achieves better results. Furthermore, both LGCN and LGCN ğ¹ aggregate nodes by minimizing an expectation of distance. However, the aggregation in LGCN has a closed-form solution while LGCN ğ¹ has not. Despite LGCN ğ¹ has a little improvement with LGCN, it is not cost-effective. To sum up, LGCN can learn more effective node representations with acceptable efficiency. 5.4.6 Parameter sensitivity. We further test the impact of attention matrix size of LGCN. We change the horizontal dimension of matrix from 4 to 256. The results on the link prediction task are shown in Fig. <ref type="figure" target="#fig_6">5</ref>. We can see that with the growth of the matrix size, the performance raises first and then starts to drop slowly. The results indicate that the attention matrix needs a suitable size to learn the attention coefficient. Also, LGCN has a stable perofmance when the horizontal dimension ranges from 32 to 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Existing hyperbolic GCNs cannot rigorously follow the hyperbolic geometry, which might limit the ability of hyperbolic geometry. To address this issue, we propose a novel Lorentzian graph neural network, called LGCN, which designs rigorous hyperbolic graph operations, e.g., feature transformation and non-linear activation. An elegant neighborhood aggregation method is also leveraged in LGCN, which conforms to the mathematical meanings of hyperbolic geometry. The extensive experiments demonstrate the superiority of LGCN, compared with the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Hyperbolic aggregation in tangent spaces</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three types of neighborhood aggregation. The three types of aggregation can be considered as computing centroids in Euclidean spaces, hyperbolic spaces, tangent spaces, respectively.</figDesc><graphic url="image-4.png" coords="5,80.20,220.51,135.02,95.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 4 . 3 (</head><label>43</label><figDesc>Lorentzian pointwise non-linear activation). If ğœ : R ğ‘› â†’ R ğ‘› is a pointwise non-linearity map, given two points x = (ğ‘¥ 0 , â€¢ â€¢ â€¢ , ğ‘¥ ğ‘› ) âˆˆ H ğ‘›,ğ›½ and v = (ğ‘£ 0 , â€¢ â€¢ â€¢ , ğ‘£ ğ‘› ) âˆˆ T 0 H ğ‘›,ğ›½ , the Lorentzian version ğœ âŠ— ğ›½ is:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>probability of existing the edge ğ‘’ ğ‘– ğ‘— between u ğ‘– )âˆ’ğ‘Ÿ )/ğ‘¡ + 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attention value on Disease.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance v.s. the running time on Disease.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results w.r.t. attention matrix dimension on different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Hyperbolic graph operations.</figDesc><table><row><cell>Method</cell><cell>Manifold</cell><cell cols="3">inside ğ‘“ inside ğ‘› except-agg</cell></row><row><cell>HGNN ğ‘ƒ</cell><cell></cell><cell></cell><cell></cell><cell>âœ—</cell></row><row><cell>HAT</cell><cell>PoincarÃ© ball</cell><cell></cell><cell></cell><cell>âœ—</cell></row><row><cell>ğœ…GCN</cell><cell></cell><cell></cell><cell></cell><cell>âœ—</cell></row><row><cell>HGNN ğ»</cell><cell></cell><cell>âœ—</cell><cell>-</cell><cell>âœ—</cell></row><row><cell>HGCN</cell><cell>Hyperboloid</cell><cell>âœ—</cell><cell>âœ—</cell><cell>âœ—</cell></row><row><cell>LGCN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>4.5.1 Hyperbolic graph operations. We compare LGCN with some existing hyperbolic GCNs regarding the properties of graph operations. A rigorous hyperbolic graph operation should make sure</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Hyperbolic centroids.</figDesc><table><row><cell>Centroid</cell><cell>Manifold</cell><cell cols="2">sum-dis closed-form literature</cell></row><row><cell>FrÃ©chet mean [11, 22]</cell><cell>-</cell><cell>âœ—</cell><cell>[31, 44, 54]</cell></row><row><cell>Einstein gyromidpoint [50]</cell><cell>Klein ball</cell><cell>âœ—</cell><cell>[18]</cell></row><row><cell cols="2">MÃ¶bius gyromidpoint [51] PoincarÃ© ball</cell><cell>âœ—</cell><cell>[2]</cell></row><row><cell>Lorentzian centroid</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Dataset statistic    </figDesc><table><row><cell cols="5">Dataset Nodes Edges Label Node features</cell></row><row><cell>Cora</cell><cell>2708</cell><cell>5429</cell><cell>7</cell><cell>1433</cell></row><row><cell cols="2">Citeseer 3327</cell><cell>4732</cell><cell>6</cell><cell>3703</cell></row><row><cell cols="3">Pubmed 19717 44338</cell><cell>3</cell><cell>500</cell></row><row><cell cols="3">Amazon 13381 245778</cell><cell>10</cell><cell>767</cell></row><row><cell>USA</cell><cell>1190</cell><cell>13599</cell><cell>4</cell><cell>-</cell></row><row><cell>Disease</cell><cell>1044</cell><cell>1043</cell><cell>2</cell><cell>1000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>AUC (%) for link prediction task. The best results are marked by bold numbers.</figDesc><table><row><cell>Dataset</cell><cell cols="4">dimension deepwalk poincarÃ©Emb GraphSage</cell><cell>GCN</cell><cell>GAT</cell><cell>HGCN</cell><cell>ğœ…GCN</cell><cell>HAT</cell><cell>LGCN</cell></row><row><cell></cell><cell>8</cell><cell>57.3Â±1.0</cell><cell>67.9Â±1.1</cell><cell>65.4Â±1.4</cell><cell cols="4">76.9Â±0.8 73.5Â±0.8 84.1Â±0.7 85.3Â±0.8 83.9Â±0.7 89.2Â±0.7</cell></row><row><cell>Disease</cell><cell>16</cell><cell>55.2Â±1.7</cell><cell>70.9Â±1.0</cell><cell>68.1Â±1.0</cell><cell cols="4">78.2Â±0.7 73.8Â±0.6 91.2Â±0.6 92.0Â±0.5 91.8Â±0.5 96.6Â±0.6</cell></row><row><cell>ğ›¿ ğ‘ğ‘£ğ‘” = 0.00</cell><cell>32</cell><cell>49.1Â±1.3</cell><cell>75.1Â±0.7</cell><cell>69.5Â±0.6</cell><cell cols="4">78.7Â±0.5 75.7Â±0.3 91.8Â±0.3 94.5Â±0.6 92.3Â±0.5 96.3Â±0.5</cell></row><row><cell></cell><cell>64</cell><cell>47.3Â±0.1</cell><cell>76.3Â±0.3</cell><cell>70.1Â±0.7</cell><cell cols="4">79.8Â±0.5 77.9Â±0.3 92.7Â±0.4 95.1Â±0.6 93.4Â±0.4 96.8Â±0.4</cell></row><row><cell></cell><cell>8</cell><cell>91.5Â±0.1</cell><cell>92.3Â±0.2</cell><cell>82.4Â±0.8</cell><cell cols="4">89.0Â±0.6 89.6Â±0.9 91.6Â±0.8 92.0Â±0.6 92.7Â±0.8 95.3Â±0.2</cell></row><row><cell>USA</cell><cell>16</cell><cell>92.3Â±0.0</cell><cell>93.6Â±0.2</cell><cell>84.4Â±1.0</cell><cell cols="4">90.2Â±0.5 91.1Â±0.5 93.4Â±0.3 93.3Â±0.6 93.6Â±0.6 96.3Â±0.2</cell></row><row><cell>ğ›¿ ğ‘ğ‘£ğ‘” = 0.16</cell><cell>32</cell><cell>92.5Â±0.1</cell><cell>94.5Â±0.1</cell><cell>86.6Â±0.8</cell><cell cols="4">90.7Â±0.5 91.7Â±0.5 93.9Â±0.2 93.2Â±0.3 94.2Â±0.6 96.5Â±0.1</cell></row><row><cell></cell><cell>64</cell><cell>92.5Â±0.1</cell><cell>95.5Â±0.1</cell><cell>89.3Â±0.3</cell><cell cols="4">91.2Â±0.3 93.3Â±0.4 94.2Â±0.2 94.1Â±0.5 94.6Â±0.6 96.4Â±0.2</cell></row><row><cell></cell><cell>8</cell><cell>96.1Â±0.0</cell><cell>95.1Â±0.4</cell><cell>90.4Â±0.3</cell><cell cols="4">91.1Â±0.6 91.3Â±0.6 93.5Â±0.6 92.5Â±0.7 94.8Â±0.8 96.4Â±1.1</cell></row><row><cell>Amazon</cell><cell>16</cell><cell>96.6Â±0.0</cell><cell>96.7Â±0.3</cell><cell>90.8Â±0.5</cell><cell cols="4">92.8Â±0.8 92.8Â±0.9 96.3Â±0.9 94.8Â±0.5 96.9Â±1.0 97.3Â±0.8</cell></row><row><cell>ğ›¿ ğ‘ğ‘£ğ‘” = 0.20</cell><cell>32</cell><cell>96.4Â±0.0</cell><cell>96.7Â±0.1</cell><cell>92.7Â±0.2</cell><cell cols="4">93.3Â±0.9 95.1Â±0.5 97.2Â±0.8 94.7Â±0.5 97.1Â±0.7 97.5Â±0.3</cell></row><row><cell></cell><cell>64</cell><cell>95.9Â±0.0</cell><cell>97.2Â±0.1</cell><cell>93.4Â±0.4</cell><cell cols="4">94.6Â±0.8 96.2Â±0.2 97.1Â±0.7 95.3Â±0.2 97.3Â±0.6 97.6Â±0.5</cell></row><row><cell></cell><cell>8</cell><cell>86.9Â±0.1</cell><cell>84.5Â±0.7</cell><cell>87.4Â±0.4</cell><cell cols="4">87.8Â±0.9 87.4Â±1.0 91.4Â±0.5 90.8Â±0.6 91.1Â±0.4 92.0Â±0.5</cell></row><row><cell>Cora</cell><cell>16</cell><cell>85.3Â±0.8</cell><cell>85.8Â±0.8</cell><cell>88.4Â±0.6</cell><cell cols="4">90.6Â±0.7 93.2Â±0.4 93.1Â±0.4 92.6Â±0.4 93.0Â±0.3 93.6Â±0.4</cell></row><row><cell>ğ›¿ ğ‘ğ‘£ğ‘” = 0.35</cell><cell>32</cell><cell>82.3Â±0.4</cell><cell>86.5Â±0.6</cell><cell>88.8Â±0.4</cell><cell cols="4">92.0Â±0.6 93.6Â±0.3 93.3Â±0.3 92.8Â±0.5 93.1Â±0.3 94.0Â±0.4</cell></row><row><cell></cell><cell>64</cell><cell>81.6Â±0.4</cell><cell>86.7Â±0.5</cell><cell>90.0Â±0.1</cell><cell cols="4">92.8Â±0.4 93.5Â±0.3 93.5Â±0.2 93.0Â±0.7 93.3Â±0.3 94.4Â±0.2</cell></row><row><cell></cell><cell>8</cell><cell>81.1Â±0.1</cell><cell>83.3Â±0.5</cell><cell>86.1Â±1.1</cell><cell cols="4">86.8Â±0.7 87.0Â±0.8 94.6Â±0.2 93.5Â±0.5 94.4Â±0.3 95.4Â±0.2</cell></row><row><cell>Pubmed</cell><cell>16</cell><cell>81.2Â±0.1</cell><cell>85.1Â±0.5</cell><cell>87.1Â±0.4</cell><cell cols="4">90.9Â±0.6 91.6Â±0.3 96.1Â±0.2 94.9Â±0.3 96.2Â±0.3 96.6Â±0.1</cell></row><row><cell>ğ›¿ ğ‘ğ‘£ğ‘” = 0.36</cell><cell>32</cell><cell>76.4Â±0.1</cell><cell>86.5Â±0.1</cell><cell>88.2Â±0.5</cell><cell cols="4">93.2Â±0.5 93.6Â±0.2 96.2Â±0.2 95.0Â±0.3 96.3Â±0.2 96.8Â±0.1</cell></row><row><cell></cell><cell>64</cell><cell>75.3Â±0.1</cell><cell>87.4Â±0.1</cell><cell>88.8Â±0.5</cell><cell cols="4">93.6Â±0.4 94.6Â±0.2 96.5Â±0.2 94.9Â±0.5 96.5Â±0.1 96.9Â±0.0</cell></row><row><cell></cell><cell>8</cell><cell>80.7Â±0.3</cell><cell>79.2Â±1.0</cell><cell>85.3Â±1.6</cell><cell cols="4">90.3Â±1.2 89.5Â±0.9 93.2Â±0.5 92.6Â±0.7 93.1Â±0.3 93.9Â±0.6</cell></row><row><cell>Citeseer</cell><cell>16</cell><cell>78.5Â±0.5</cell><cell>79.7Â±0.7</cell><cell>87.1Â±0.9</cell><cell cols="4">92.9Â±0.7 92.2Â±0.7 94.3Â±0.4 93.8Â±0.4 93.6Â±0.5 95.4Â±0.5</cell></row><row><cell>ğ›¿ ğ‘ğ‘£ğ‘” = 0.46</cell><cell>32</cell><cell>73.1Â±0.4</cell><cell>79.8Â±0.6</cell><cell>87.3Â±0.4</cell><cell cols="4">94.3Â±0.6 93.4Â±0.4 94.7Â±0.3 93.5Â±0.5 94.2Â±0.5 95.8Â±0.3</cell></row><row><cell></cell><cell>64</cell><cell>72.3Â±0.3</cell><cell>79.6Â±0.6</cell><cell>88.1Â±0.4</cell><cell cols="4">95.4Â±0.5 94.4Â±0.3 94.8Â±0.3 93.8Â±0.5 94.3Â±0.2 96.4Â±0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Accuracy (%) for node classification task. The best results are marked by bold numbers.</figDesc><table><row><cell>Dataset</cell><cell cols="4">dimension deepwalk poincarÃ©Emb GraphSage</cell><cell>GCN</cell><cell>GAT</cell><cell>HGCN</cell><cell>ğœ…GCN</cell><cell>HAT</cell><cell>LGCN</cell></row><row><cell></cell><cell>8</cell><cell>59.6Â±1.6</cell><cell>57.0Â±0.8</cell><cell>73.9Â±1.5</cell><cell cols="4">75.1Â±1.1 76.7Â±0.7 81.5Â±1.3 81.8Â±1.5 82.3Â±1.2 82.9Â±1.2</cell></row><row><cell>Disease</cell><cell>16</cell><cell>61.5Â±2.2</cell><cell>56.1Â±0.7</cell><cell>75.3Â±1.0</cell><cell cols="4">78.3Â±1.0 76.6Â±0.8 82.8Â±0.8 82.1Â±1.1 83.6Â±0.9 84.4Â±0.8</cell></row><row><cell>ğ›¿ ğ‘ğ‘£ğ‘” = 0.00</cell><cell>32</cell><cell>62.0Â±0.3</cell><cell>58.7Â±0.7</cell><cell>76.1Â±1.7</cell><cell cols="4">81.0Â±0.9 79.3Â±0.7 84.0Â±0.8 82.8Â±0.9 84.9Â±0.9 86.8Â±0.8</cell></row><row><cell></cell><cell>64</cell><cell>61.8Â±0.5</cell><cell>60.1Â±0.8</cell><cell>78.5Â±1.0</cell><cell cols="4">82.7Â±0.9 80.4Â±0.7 84.3Â±0.8 83.0Â±1.0 85.1Â±0.8 87.1Â±0.8</cell></row><row><cell></cell><cell>8</cell><cell>44.3Â±0.6</cell><cell>38.9Â±1.1</cell><cell>46.8Â±1.4</cell><cell cols="4">50.5Â±0.5 47.8Â±0.7 50.5Â±1.1 49.1Â±0.9 50.7Â±1.0 51.6Â±1.1</cell></row><row><cell>USA</cell><cell>16</cell><cell>42.3Â±1.3</cell><cell>38.3Â±1.0</cell><cell>47.5Â±0.8</cell><cell cols="4">50.9Â±0.6 49.5Â±0.7 51.1Â±1.0 50.5Â±1.2 51.3Â±0.9 51.9Â±0.9</cell></row><row><cell>ğ›¿ ğ‘ğ‘£ğ‘” = 0.16</cell><cell>32</cell><cell>39.0Â±1.0</cell><cell>39.0Â±0.8</cell><cell>48.0Â±0.7</cell><cell cols="4">50.6Â±0.5 49.1Â±0.6 51.2Â±0.9 50.9Â±1.0 51.5Â±0.8 52.4Â±0.9</cell></row><row><cell></cell><cell>64</cell><cell>42.7Â±0.8</cell><cell>39.2Â±0.8</cell><cell>48.2Â±1.1</cell><cell cols="4">51.1Â±0.6 49.6Â±0.6 52.4Â±0.8 51.8Â±0.8 52.5Â±0.7 52.8Â±0.8</cell></row><row><cell></cell><cell>8</cell><cell>66.7Â±1.0</cell><cell>65.3Â±1.1</cell><cell>71.3Â±1.6</cell><cell cols="4">70.9Â±1.1 70.0Â±0.9 71.7Â±1.3 70.3Â±1.2 71.0Â±1.0 72.0Â±1.3</cell></row><row><cell>Amazon</cell><cell>16</cell><cell>67.5Â±0.8</cell><cell>67.0Â±0.7</cell><cell>72.3Â±1.6</cell><cell cols="4">70.9Â±1.1 72.7Â±0.8 72.7Â±1.3 71.9Â±1.1 73.3Â±1.0 75.0Â±1.1</cell></row><row><cell>ğ›¿ ğ‘ğ‘£ğ‘” = 0.20</cell><cell>32</cell><cell>70.0Â±0.5</cell><cell>68.1Â±0.3</cell><cell>73.4Â±1.2</cell><cell cols="4">71.5Â±0.8 72.5Â±0.7 75.3Â±1.0 72.9Â±0.6 74.9Â±0.8 75.5Â±0.9</cell></row><row><cell></cell><cell>64</cell><cell>70.3Â±0.7</cell><cell>67.3Â±0.4</cell><cell>74.1Â±1.2</cell><cell cols="4">73.0Â±0.6 72.9Â±0.8 75.5Â±0.6 73.5Â±0.4 75.4Â±0.7 75.8Â±0.6</cell></row><row><cell></cell><cell>8</cell><cell>64.5Â±1.2</cell><cell>57.5Â±0.6</cell><cell>74.5Â±1.3</cell><cell cols="4">80.3Â±0.8 80.4Â±0.8 80.0Â±0.7 81.0Â±0.5 82.8Â±0.7 82.6Â±0.8</cell></row><row><cell>Cora</cell><cell>16</cell><cell>65.2Â±1.6</cell><cell>64.4Â±0.3</cell><cell>77.3Â±0.8</cell><cell cols="4">81.9Â±0.6 81.7Â±0.7 81.3Â±0.6 80.8Â±0.6 83.1Â±0.6 83.3Â±0.7</cell></row><row><cell>ğ›¿ ğ‘ğ‘£ğ‘” = 0.35</cell><cell>32</cell><cell>65.9Â±1.5</cell><cell>64.9Â±0.4</cell><cell>78.8Â±1.2</cell><cell cols="4">81.5Â±0.4 82.6Â±0.7 81.7Â±0.7 81.8Â±0.5 83.2Â±0.6 83.5Â±0.6</cell></row><row><cell></cell><cell>64</cell><cell>66.5Â±1.7</cell><cell>68.6Â±0.4</cell><cell>79.2Â±0.6</cell><cell cols="4">81.6Â±0.4 83.1Â±0.6 82.1Â±0.7 81.5Â±0.7 83.1Â±0.5 83.5Â±0.5</cell></row><row><cell></cell><cell>8</cell><cell>73.2Â±0.7</cell><cell>66.0Â±0.8</cell><cell>75.9Â±0.4</cell><cell cols="4">78.6Â±0.4 71.9Â±0.7 77.9Â±0.6 78.5Â±0.7 78.5Â±0.6 78.8Â±0.5</cell></row><row><cell>Pubmed</cell><cell>16</cell><cell>73.9Â±0.8</cell><cell>68.0Â±0.4</cell><cell>77.3Â±0.3</cell><cell cols="4">79.1Â±0.5 75.9Â±0.7 78.4Â±0.4 78.3Â±0.6 78.6Â±0.5 78.6Â±0.7</cell></row><row><cell>ğ›¿ ğ‘ğ‘£ğ‘” = 0.36</cell><cell>32</cell><cell>72.4Â±1.0</cell><cell>68.4Â±0.5</cell><cell>77.7Â±0.3</cell><cell cols="4">78.7Â±0.5 78.2Â±0.6 78.6Â±0.6 78.8Â±0.6 78.8Â±0.6 78.9Â±0.6</cell></row><row><cell></cell><cell>64</cell><cell>73.5Â±1.0</cell><cell>69.9Â±0.6</cell><cell>78.0Â±0.4</cell><cell cols="4">79.1Â±0.5 78.7Â±0.4 79.3Â±0.5 79.0Â±0.5 79.0Â±0.6 79.6Â±0.6</cell></row><row><cell></cell><cell>8</cell><cell>47.8Â±1.6</cell><cell>38.6Â±0.4</cell><cell>65.8Â±1.6</cell><cell cols="4">68.9Â±0.7 69.5Â±0.8 70.9Â±0.6 70.3Â±0.6 71.2Â±0.7 71.8Â±0.7</cell></row><row><cell>Citeseer</cell><cell>16</cell><cell>46.2Â±1.5</cell><cell>40.4Â±0.5</cell><cell>67.8Â±1.1</cell><cell cols="4">70.2Â±0.6 71.6Â±0.7 71.2Â±0.5 70.7Â±0.5 71.9Â±0.6 71.9Â±0.7</cell></row><row><cell>ğ›¿ ğ‘ğ‘£ğ‘” = 0.46</cell><cell>32</cell><cell>43.6Â±1.9</cell><cell>43.5Â±0.5</cell><cell>68.5Â±1.3</cell><cell cols="4">70.4Â±0.5 72.6Â±0.7 71.9Â±0.4 71.2Â±0.5 72.4Â±0.5 72.5Â±0.5</cell></row><row><cell></cell><cell>64</cell><cell>46.6Â±1.4</cell><cell>43.6Â±0.4</cell><cell>69.2Â±0.8</cell><cell cols="4">70.8Â±0.4 72.4Â±0.7 71.7Â±0.5 71.0Â±0.3 72.2Â±0.5 72.5Â±0.6</cell></row><row><cell cols="4">1000 nodes for test. The settings are same with</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The variants of LGCN and HGCN.</figDesc><table><row><cell>Dataset</cell><cell>Disease</cell><cell></cell><cell>USA</cell><cell></cell><cell></cell><cell>Amazon</cell></row><row><cell>Task</cell><cell>LP</cell><cell>NC</cell><cell>LP</cell><cell>NC</cell><cell>LP</cell><cell>NC</cell></row><row><cell>HGCN</cell><cell cols="6">92.7Â±0.4 84.3Â±0.8 94.2Â±0.2 52.4Â±0.8 97.1Â±0.7 75.5Â±0.6</cell></row><row><cell>HGCN ğ‘</cell><cell cols="6">94.3Â±0.5 86.2Â±1.0 95.6Â±0.1 52.5Â±0.8 97.3Â±0.3 75.8Â±0.4</cell></row><row><cell>LGCN \ğ›½</cell><cell cols="6">96.3Â±0.4 86.3Â±0.7 96.1Â±0.3 52.5Â±0.7 96.5Â±0.7 75.6Â±0.5</cell></row><row><cell>LGCN \ğ‘ğ‘¡ğ‘¡</cell><cell cols="6">95.9Â±0.3 86.6Â±0.8 95.9Â±0.2 52.2Â±0.7 97.0Â±0.6 74.6Â±0.5</cell></row><row><cell cols="7">LGCN \ğ‘ğ‘¡ğ‘¡ \ğ‘ 92.6Â±0.6 83.2Â±0.6 94.6Â±0.4 52.2Â±0.9 96.6Â±0.9 74.3Â±0.6</cell></row><row><cell>LGCN</cell><cell cols="6">96.8Â±0.4 87.1Â±0.8 96.4Â±0.2 52.8Â±0.8 97.6Â±0.5 75.8Â±0.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>MAE (scaled by 100) of predicting molecular properties logP, QED and SAS on ZINC dataset.</figDesc><table><row><cell>Property</cell><cell>logP</cell><cell>QED</cell><cell>SAS</cell></row><row><cell cols="4">MPNN[14] 4.1Â±0.02 8.4Â±0.05 9.2Â±0.07</cell></row><row><cell cols="4">GGNN[29] 3.2Â±0.20 6.4Â±0.20 9.1Â±0.10</cell></row><row><cell>HGNN ğ‘ƒ</cell><cell cols="3">3.1Â±0.01 6.0Â±0.04 8.6Â±0.02</cell></row><row><cell>HGNN ğ»</cell><cell cols="3">2.4Â±0.02 4.7Â±0.00 7.7Â±0.06</cell></row><row><cell>HGNN ğ¿</cell><cell cols="3">2.2Â±0.03 3.3Â±0.05 5.8Â±0.05</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by the National Natural Science Foundation of China (No. U20B2045, 61772082, 62002029, U1936104, 61972442), and the National Key Research and Development Program of China (2018YFB1402600). It is also supported by BUPT Excellent Ph.D. Students Foundation (No. CX2019126).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A HYPERBOLIC GEOMETRY</head><p>Hyperbolic geometry is a non-Euclidean geometry with a constant negative curvature. There are some equivalent hyperbolic models to describe hyperbolic geometry, including the hyperboloid model, the PoincarÃ© ball model, and the Klein ball model, etc. Since we have introduced the hyperboloid model in Section 3.1, here we introduce the PoincarÃ© ball model. More rigorous and in-depth introduction of differential geometry and hyperbolic geometry can be found in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>PoincarÃ© ball. We consider a specific PoincarÃ© ball model D ğ‘›,ğ›¼ <ref type="bibr" target="#b12">[13]</ref>, which is defined by an open ğ‘‘-dimensional ball of radius 1/ âˆš ğ›¼ (ğ›¼ &gt; 0): D ğ‘›,ğ›¼ := {x âˆˆ R ğ‘› : ğ›¼ âˆ¥xâˆ¥ 2 &lt; 1}, equipped with the Riemannian metric: ğ‘” D x = (ğœ† ğ›¼ x ) 2 ğ‘” R , where ğœ† ğ›¼ x = 2/(1âˆ’ğ›¼ âˆ¥xâˆ¥ 2 ), ğ‘” R = I ğ‘‘ . When x = 0 âˆˆ D ğ‘›,ğ›¼ , the exponential map exp ğ›¼ 0 : T 0 D ğ‘›,ğ›¼ â†’ D ğ‘›,ğ›¼ and the logarithmic map log ğ›¼ 0 : D ğ‘›,ğ›¼ â†’ T 0 D ğ‘›,ğ›¼ are given for vâˆˆT 0 D ğ‘›,ğ›¼ \{0} and y âˆˆ D ğ‘›,ğ›¼ \{0}:</p><p>The PoincarÃ© ball model and the hyperboloid model are isomorphic, and the diffeomorphism maps one space onto the other as shown in following <ref type="bibr" target="#b5">[6]</ref>:</p><p>and M be a ğ‘š Ã— ğ‘› matrix, Lorentzian matrix-vector multiplication is shown as following:</p><p>Let x ğ‘›,ğ›¼ âˆˆ D ğ‘›,ğ›½ , MÃ¶bius matrix-vector multiplication has the formulation as <ref type="bibr" target="#b12">[13]</ref>:</p><p>For ğ‘ H ğ‘›,ğ›½ â†’D ğ‘›,ğ›¼ (x ğ‘›,ğ›½ ) = x ğ‘›,ğ›¼ and a shared ğ‘š Ã— ğ‘› matrix M, we aim to prove ğ‘ H ğ‘›,ğ›½ â†’D ğ‘›,ğ›¼ (y ğ‘›,ğ›½ ) = y ğ‘›,ğ›¼ .</p><p>For x ğ‘›,ğ›½ = (ğ‘¥ (ğ›½)</p><p>ğ‘› ), and the logarithmic map of x at 0 ğ‘›,ğ›½ = ( âˆšï¸ ğ›½, 0, â€¢ â€¢ â€¢ , 0) âˆˆ H ğ‘›,ğ›½ , i.e., is shown as following:</p><p>The Lorentzian matrix-vector multiplication is given as following:</p><p>Then we map y ğ‘š,ğ›½ to the PoincarÃ© ball via Eq. ( <ref type="formula">25</ref>),</p><p>Note that âˆ¥mâˆ¥ L = âˆšï¸ âŸ¨m, mâŸ© L =âˆ¥ğ‘Mx (ğ›½) âˆ¥ and ğ‘¥</p><p>Moreover, the point x ğ‘›,ğ›¼ = (ğ‘¥ (ğ›¼)</p><p>ğ‘› ) âˆˆ D ğ‘›,ğ›¼ can be mapped into the hyperboloid model via Eq. ( <ref type="formula">26</ref>) as following:</p><p>Thus, the squared norm of x(ğ›½) is given as:</p><p>Moreover, the curvature of the PoincarÃ© ball model D ğ‘›,ğ›¼ is âˆ’ğ›¼, while the curvature of the hyperboloid model H ğ‘›,ğ›½ is âˆ’ğ›½. The maps between the two models in Eq. ( <ref type="formula">25</ref>) and Eq. ( <ref type="formula">26</ref>) ensure they have a same curvature, i.e., âˆ’ğ›¼ = âˆ’1/ğ›½, ğ›¼ = 1/ğ›½. Therefore, combining Eq. (32) and Eq. ( <ref type="formula">34</ref>), we have:</p><p>Therefore, Lorentzian matrix-vector multiplication is equivalence to MÃ¶bius matrix-vector multiplication. â–¡</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Theorem 3.2</head><p>Proof. Combining Eq. ( <ref type="formula">15</ref>) and Eq. ( <ref type="formula">17</ref>), we have following equality:</p><p>where ğœ‡ &gt; 0 is a scaling factor to satisfy ğœ‡ ğ‘— âˆˆğ‘ (ğ‘–)âˆª{ğ‘– } ğ‘¤ ğ‘– ğ‘— h ğ‘‘,ğ›½ ğ‘— âˆˆ H ğ‘›,ğ›½ . Also, we can infer from Eq. ( <ref type="formula">3</ref>),</p><p>ğ‘— âˆˆ H ğ‘›,ğ›½ . Thus the Lorentzian scalar product of it and itself should equal to âˆ’ğ›½, and we have:</p><p>We have</p><p>, and Proof. Here we prove the theorem in the case of leveraging Leak-lyRelu activation function, as an example. Let</p><p>, ğœ (â€¢) be the LeaklyRelu activation function, and Lorentzian non-linear activation is given as following:</p><p>where ğœ (ğ‘£ ğ‘– ) = max(ğ‘˜ğ‘£ ğ‘– , ğ‘£ ğ‘– ) for ğ‘˜ âˆˆ (0, 1). Let x ğ‘›,ğ›¼ âˆˆ D ğ‘›,ğ›½ , MÃ¶bius non-linear activation has the formulation as <ref type="bibr" target="#b12">[13]</ref>:</p><p>For ğ‘ H ğ‘›,ğ›½ â†’D ğ‘›,ğ›¼ (x ğ‘›,ğ›½ ) = x ğ‘›,ğ›¼ and the LeaklyRelu activation function ğœ (â€¢), we aim to prove ğ‘ H ğ‘›,ğ›½ â†’D ğ‘›,ğ›¼ (y ğ‘›,ğ›½ ) = y ğ‘›,ğ›¼ . For MÃ¶bius non-linear activation, we first map the features x ğ‘›,ğ›¼ âˆˆ D ğ‘›,ğ›¼ into the tangent space T 0 D ğ‘›,ğ›¼ via logarithmic map</p><p>we have log ğ›¼ 0 (x ğ‘›,ğ›¼ ) = ğ‘™x ğ‘›,ğ›¼ . Also, note that the LeaklyRelu activation function satisfies: ğœ (log ğ›¼ 0 (x ğ‘›,ğ›¼ )) = ğ‘™ğœ (x ğ‘›,ğ›¼ ). MÃ¶bius pointwise non-linear activation in Eq. ( <ref type="formula">40</ref>) is equivalent to:</p><p>Moreover, for Lorentzian pointwise non-linear activation, similar to Eq. ( <ref type="formula">29</ref>), we also map the feature x ğ‘›,ğ›½ = (ğ‘¥</p><p>ğ‘› ) to the tangent space T 0 D ğ‘›,ğ›½ via log</p><p>where x(ğ›½) = (ğ‘¥ (ğ›½)</p><p>ğ‘› ), and ğ‘ =</p><p>Thus, the results of Eq. ( <ref type="formula">39</ref>) for LeaklyRelu is given as: Ïƒ âŠ— ğ›½ log ğ›½ 0 (x ğ‘›,ğ›½ ) = ğ‘ 0, ğœ ( x(ğ›½) ) = m. Also the Lorentzian norm of m also satisfies as: âˆ¥mâˆ¥ L = âˆ¥ğ‘ğœ ( x)âˆ¥ = ğ‘âˆ¥ğœ ( x)âˆ¥. The Lorentzian non-linear activation is given as:</p><p>Furthermore, according to Eq. ( <ref type="formula">33</ref>), we project the ğœ âŠ— ğ›½ (x ğ‘›,ğ›½ ) = y ğ‘›,ğ›½ into the PoincarÃ© ball model as following: ğ‘ H ğ‘›,ğ›½ â†’D ğ‘›,ğ›¼ (y ğ‘›,ğ›½ ) =</p><p>âˆ¥ğœ (x ğ‘›,ğ›¼ ) âˆ¥ = y ğ‘›,ğ›¼ . Therefore, Lorentzian pointwise non-linear activation is equivalence to MÃ¶bius pointwise non-linear activation. â–¡</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXPERIMENTAL DETAILS C.1 ğ›¿-hyperbolicity</head><p>Here we introduce the hyperbolicity measurements originally proposed by Gromov <ref type="bibr" target="#b15">[16]</ref>. Considering a quadruple of distinct nodes ğ‘£ 1 , ğ‘£  <ref type="bibr" target="#b5">[6]</ref>, is a worst case measurement, which focuses on a local quadruple nodes, and does not reflect the hyperbolicity of the whole graph <ref type="bibr" target="#b48">[49]</ref>. Moreover, both time complexity of ğ›¿ ğ‘¤ğ‘œğ‘Ÿğ‘ ğ‘¡ and ğ›¿ ğ‘ğ‘£ğ‘” are ğ‘‚ (|ğ‘‰ | 4 ). Since ğ›¿ ğ‘ğ‘£ğ‘” is robust to adding/removing an edge from the graph, it can be approximated via sampling, while ğ›¿ ğ‘¤ğ‘œğ‘Ÿğ‘ ğ‘¡ cannot. Therefore we leverage ğ›¿ ğ‘ğ‘£ğ‘” as the measurement.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Topological implications of negative curvature for biological and social networks</title>
		<author>
			<persName><forename type="first">RÃ©ka</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasim</forename><surname>Mobasheri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">32811</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Constant Curvature Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Bachmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>BÃ©cigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-relational PoincarÃ© graph embeddings</title>
		<author>
			<persName><forename type="first">Ivana</forename><surname>BalaÅ¾eviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4463" to="4473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent on Riemannian manifolds</title>
		<author>
			<persName><forename type="first">Silvere</forename><surname>Bonnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Control</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="2217" to="2229" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Hyperbolic graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>RÃ©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4869" to="4880" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical structure and the prediction of missing links in networks</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristopher</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">Ej</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="page">98</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Power-law distributions in empirical data</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Clauset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosma</forename><forename type="middle">Rohilla</forename><surname>Shalizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">Ej</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="661" to="703" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">MichaÃ«l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Les Ã©lÃ©ments alÃ©atoires de nature quelconque dans un espace distanciÃ©</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>FrÃ©chet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Annales de l&apos;institut Henri PoincarÃ©</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="215" to="310" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hyperbolic Entailment Cones for Learning Hierarchical Embeddings</title>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>BÃ©cigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1646" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>BÃ©cigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<title level="m">Hyperbolic neural networks. In NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5350" to="5360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural message passing for Quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno>ICML. 1263-1272</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<idno>IJCNN. 729-734</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hyperbolic groups</title>
		<author>
			<persName><forename type="first">Mikhael</forename><surname>Gromov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Essays in group theory</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="75" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning mixedcurvature representations in product spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>RÃ©</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hyperbolic attention networks</title>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Differential geometry, Lie groups, and symmetric spaces</title>
		<author>
			<persName><forename type="first">Sigurdur</forename><surname>Helgason</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>Academic press</publisher>
			<biblScope unit="volume">80</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention networks for semi-supervised short text classification</title>
		<author>
			<persName><forename type="first">Linmei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianchi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4823" to="4832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Riemannian comparison constructions</title>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Karcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
	<note>SFB 256</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Karcher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.2087</idno>
		<title level="m">Riemannian center of mass and so called karcher mean</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry of complex networks</title>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">MariÃ¡n</forename><surname>BogunÃ¡</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lorentzian Distance Learning for Hyperbolic Representations</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3672" to="3681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<author>
			<persName><forename type="first">Bennett</forename><surname>Jeffrey M Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sun-Chin</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Glickenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Guenther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ivey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Knopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Manifolds and differential geometry</title>
				<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">643</biblScope>
			<biblScope unit="page">658</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Hyperbolic graph neural networks</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8228" to="8239" />
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Differentiating through the FrÃ©chet Mean</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isay</forename><surname>Katsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Streaming graph neural networks</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="719" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Image-Based Recommendations on Styles and Substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
	<note>Qinfeng Shi, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Machine learning meets complex networks via coalescent embedding in the hyperbolic space</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Muscoloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josephine</forename><forename type="middle">Maria</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Ciucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ginestra</forename><surname>Bianconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Vittorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cannistraci</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1615</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PoincarÃ© embeddings for learning hierarchical representations</title>
		<author>
			<persName><forename type="first">Maximillian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6338" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning continuous hierarchies in the lorentz model of hyperbolic geometry</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3779" to="3788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2014">2016. 2014-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spectral graph convolutions for population-based disease prediction</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofia</forename><forename type="middle">Ira</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enzo</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><forename type="middle">Guerrerro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="177" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>John G Ratcliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Axler</surname></persName>
		</author>
		<author>
			<persName><surname>Ribet</surname></persName>
		</author>
		<title level="m">Foundations of hyperbolic manifolds</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hybrid Approach of Relation Network and Localized Graph Convolutional Filtering for Breast Cancer Subtype Classification</title>
		<author>
			<persName><forename type="first">Sungmin</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokjun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sun</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05859</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">struc2vec: Learning node representations from structural identity</title>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Hp</forename><surname>Leonardo Fr Ribeiro</surname></persName>
		</author>
		<author>
			<persName><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel R Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Representation Tradeoffs for Hyperbolic Embeddings</title>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4460" to="4469" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representation tradeoffs for hyperbolic embeddings</title>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>RÃ©</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4457" to="4466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Pitfalls of Graph Neural Network Evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>GÃ¼nnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Session-based Social Recommendation via Dynamic Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="555" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hyperbolic Disk Embeddings for Directed Acyclic Graphs</title>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryusuke</forename><surname>Takahama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shun</forename><surname>Onoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6066" to="6075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">PoincarÃ© GloVe: Hyperbolic Word Embeddings</title>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Tifrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>BÃ©cigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Analytic hyperbolic geometry: Mathematical foundations and applications</title>
		<author>
			<persName><surname>Abraham A Ungar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Barycentric calculus in Euclidean and hyperbolic geometry: A comparative introduction</title>
		<author>
			<persName><surname>Abraham A Ungar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Leimeister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08207</idno>
		<title level="m">Gradient descent in hyperbolic space</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7370" to="7377" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03046</idno>
		<title level="m">Hyperbolic Graph Attention Network</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04202</idno>
		<title level="m">Deep learning on graphs: A survey</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
