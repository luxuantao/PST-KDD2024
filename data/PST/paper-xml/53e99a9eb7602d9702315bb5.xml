<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Randomization and Derandomization in Space-Bounded Computation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Saks</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics Rutgers University New Brunswick</orgName>
								<address>
									<postCode>08903</postCode>
									<region>NJ</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Randomization and Derandomization in Space-Bounded Computation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6288BFA855E98F0C2D63C77299155A84</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is a survey of space-bounded probabilistic computation, summarizing the present state of knowledge about the relationships between the various complexity classes associated with such computation. The survey especially emphasizes recent progress in the construction of pseudorandom generators that fool probabilistic space-bounded computations, and the application of such generators to obtain deterministic simulations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Inspired in part by the then-surprising use of randomness in algorithms for "non-random" problems such as primality testing ( <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b39">40]</ref>), probabilistic computation emerged as a major subfield of complexity theory during the late 1970's. Beginning with Gill's seminal paper <ref type="bibr" target="#b14">[15]</ref>, researchers defined models and built the foundations for a rigorous study of probabilistic computation and, in particular, of probabilistic time-and space-bounded complexity classes. Despite the considerable success in understanding the structure of and relationships between such classes, the two central questions, "Does randomness ever provide more than a polynomial advantage in time over determinism?" and "Does randomness provide more than a constant factor advantage in space over determinism?", remain unsolved. However, recent progress in the realm of space-bounded computation gives promise that a negative answer to (at least one form of) the second question may be in sight, or, short of that, that some less sweeping but still fundamental questions are accessible. This survey of probabilistic space-bounded computation aims at providing a thor-ough overview of the developments in this area, and at highlighting some of the many questions that remain.</p><p>Early work on space-bounded probabilistic complexity classes was based on connections between these classes and matrix computations. These connections led to two of the main results: that (unbounded error) probabilistic space s is contained in deterministic space s 2 [9], and that the power of unbounded error probabilistic space s is not diminished by imposing a time bound of 2 O(s) <ref type="bibr" target="#b21">[22]</ref>.</p><p>Much of the more recent progress in the area grew out of efforts to place the theory of pseudorandom generation on a firm theoretical foundation, which, motivated largely by considerations from cryptography, began in the mid 1980's. A fundamental insight here, originating in <ref type="bibr" target="#b49">[50]</ref> and expanded upon and systematized in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b28">29]</ref>, is the connection between "hardness" and "randomness": the existence of problems whose instances can be generated efficiently, and that are sufficiently hard to approximate within a particular computation model, can be used as the basis for a pseudorandom generator. Such generators can then be used to construct deterministic simulations of randomized computation within that model. Lacking any provable hardness results for time-bounded computation, this insight led to a number of "conditional" results of the form: if two time-bounded complexity classes A and B are distinct, then probabilistic time-bounded class C can be simulated in deterministic time-bounded class D. Now, inasmuch as space-bounded computation suffers from the same absence of provable hardness results as time-bounded computation does, one might at first expect that this approach would be limited to conditional results here as well. However, it turns out that to construct provably good pseudorandom generators for space-bounded computation, it is enough to prove hardness results for a model of space-bounded computation in which the input is accessible only by a one-way tape. This observation opened the way for a sequence of papers [2, <ref type="bibr" target="#b6">7,</ref><ref type="bibr">30,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b5">6]</ref>, presenting ingenious constructions of pseudorandom number generators that can be proved unconditionally to look random to space-bounded machines. These constructions provide the basis for some significant deterministic simulations of randomness: that any bounded error randomized log space, polynomial time computation can be simulated by a deterministic polylog space, polynomial time computation <ref type="bibr" target="#b30">[31]</ref>, and also by an O((log n) 3/2 ) space deterministic computation ( <ref type="bibr" target="#b41">[42]</ref>), and that if the randomized computation uses only polylog many random bits then it has a log space deterministic simulation <ref type="bibr" target="#b36">[37]</ref>.</p><p>In this survey, we focus on language membership problems and on complexity classes corresponding to space functions s(n) that are at least log n (thus omitting the notable body of work on very small space classes and probabilistic automata, e.g, <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23]</ref>). Within these restrictions, the aim is to be reasonably comprehensive, and apologies are offered in advance for the inevitable omissions.</p><p>Section 2 presents definitions of the relevant models and complexity classes and their connection with Markov chains and matrix computations. Section 3 summarizes the known relationships between these complexity classes and some of the main open problems. Section 4 provides a closer look at the recent developments in pseudorandom generators for space bounded computation, and their application to deterministic simulation. A final section briefly considers some other related research directions.</p><p>2 Models, complexity classes and equivalent problems</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Some Basics</head><p>In this paper, deterministic Turing machines (DTMs) have three tapes: a read-only input tape with a two-way head, a work tape with a two-way head, and a write-only output tape with a one-way head. Nondeterministic and probabilistic Turing machines (PTMs) have an auxiliary read-only tape with a oneway head that, for non-deterministic machines, contains an infinite string of non-deterministic bits, and for probabilistic machines, contains an infinite string of unbiased independent bits. Informally, we think of these bits as "flips" of a random coin, and the value of past coin flips can only be recalled if they have been written on the work tape.</p><p>A configuration of a machine consists of (1) the contents of the work tape and the position of the head (2) the position of the head on the input tape, and (3) the state of the final state control. Note that it does not include any information about the output tape or (for non-deterministic or randomized machines) the auxiliary tape.</p><p>The execution M (x) of a DTM M on an input x is the (possibly infinite) sequence of configurations through which M passes on input x. For a PTM M , the execution of M (x) is a random process, which depends on the string stored in the auxiliary tape. Under this probability distribution, all events associated with an execution, such as "the machine does not halt" are assigned a probability. For a PTM M and input x, a run of M (x) is a specific outcome of this random process, i.e., a run is obtained by specifying a particular setting of the bits on the random tape. For a DTM, a run and an execution are synonomous.</p><p>We will usually be considering Turing Machines for language membership. Without loss of generality, we will usually assume that such a machine either produces no output (and either halts or not), or outputs a single 1 and then halts immediately. If a run of M (x) produces a 1 output, then we say that the run accepts x. For a PTM M , we define p M (x) to be the probability that M (x) accepts x.</p><p>We adopt the following notions of time and space for probabilistic computations. For functions s(n) and t(n), a PTM M is said to operate in space s(n) if for every input x, every run of M (x) requires at most space s(|x|), and is said to operate in time t(n) if for every input x the expected time of M (x) is at most t(n). As explained in <ref type="bibr" target="#b14">[15]</ref>, there are some problems with these definitions, and the less restrictive definitions given there are, in general, better behaved. However, for our purposes, the definitions given above will serve just as well. Also, there are the usual technicalities that s and t should be suitably well-behaved functions (see, e.g., the discussion of "proper" complexity functions in <ref type="bibr" target="#b37">[38]</ref>), and we always assume that the functions s and t satisfy whatever conditions are necessary.</p><p>The usual definition of the space of a run is the number of distinct cells that are accessed on the work tape. We adopt the following convenient (and common) abuse of terminology: we will use the term space to denote the number of work tape cells accessed plus the number of bits to encode the state of the finite state control, the positions of the heads on the input tape and work tape. This abuse allows us to say that a run using space S has at most 2 S distinct configurations. Given our restriction to space functions s(n) that are at least log(n), this notion of space differs from the usual one by at most a multiplicative constant.</p><p>We will indulge in a few other notational abuses. We will be very casual about constant factors: when we say s, we may well mean θ(s) or O(s). Also, if machine M has space bound s, and x is an input we may say that the execution M (x) operates in space s, when properly we should say that M (x) operates in space s(|x|).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Randomized space-bounded complexity classes</head><p>Notational conventions for denoting probabilistic space-bounded complexity classes have not been completely standardized, and there are some incompatibilities in the choice of notation by various researchers. The most serious instance is the complexity class RL, which depending on the author of the paper, has one of two distinctly different meanings. This section proposes a reasonably comprehensive and consistent set of notation that does not differ too much from common usage; the conventions are inspired by and adapted from the taxonomy proposed in <ref type="bibr" target="#b7">[8]</ref> for randomized log space complexity classes, with some modifications that seem appropriate for extending these definitions to general space-bounded classes. At the end of this section we include a short discussion relating the notation here to notation elsewhere.</p><p>If M is a PTM, the language L computed by M is the set of strings x such that p M (x) &gt; 1/2. Probabilistic Turing machines can be classified according to their behavior:</p><p>• M operates in bounded error if p M (x) ≤ 1/3 for all x ∈ L.</p><p>• M operates with 1-sided error if x ∈ L implies that p M (x) = 0.</p><p>• M halts almost surely if for all x, the probability that M (x) does not halt is 0.</p><p>• M halts absolutely if for all x, M halts on every run (i.e., for every setting of the random tape). For example, a machine that reads random bits and halts the first time it reads a 1, halts almost surely, but not absolutely.</p><p>The first two properties are very familiar and arise in the study of both time-and space-bounded computation. The distinction between the third and fourth properties can be shown to be unimportant in the context of time-bounded complexity, but, as will be seen, is crucial in the context of space-bounded complexity. It turns out that the condition "halts almost surely" can be imposed without loss of generality (see Theorem 2.1). However, the condition "halts absolutely", which appears at first to be only a slightly stronger condition, seems to make a huge difference in the power of randomized machines. This will be discussed in some detail in section 3.3.</p><p>These conditions on probabilistic machines give rise to eight randomized space-bounded complexity classes, listed in figure <ref type="figure">1</ref>. The class P rSP ACE(s) (for "probabilistic space s") contains all languages computed by a space s probabilistic Turing machine. BP SP ACE(s) (for "bounded error probabilistic space s"), and RSP ACE(s) are, respectively, those languages computed by a space s machine that operate with bounded error and with one-sided error. The class ZP SP ACE(s) = RSP ACE(s) ∩ co -RSP ACE(s) is the class of languages L such that both L and its complement L are computed by a space s one-sided error PTM. The notation ZP SP ACE stands for zero-sided error and is motivated by the following alternative characterization:</p><formula xml:id="formula_0">Proposition 2.1 A language L is in ZP SP ACE(s)</formula><p>if and only if there is a PTM M satisfying: (i) on any input x, either M produces no output, or it outputs "accept", or it outputs "reject" (ii) for x ∈ L it outputs "accept" with probability greater than 1/2 and never outputs "reject", and (iii) for x ∈ L, it outputs "reject" with probability greater than 1/2 and never outputs "accept".</p><p>For each of these classes, the corresponding class with subscript H is defined by modifying the definition of the class so that the accepting machine for the language is required to halt absolutely. We refer to these classes as the halting classes and to the others as the non-halting classes.</p><p>From the definitions it is clear that the containments indicated in the figure hold.</p><p>In addition to these classes and the standard classes DSP ACE(s) and N SP ACE(s), we will also refer to the class SSP ACE(s), of languages computed by a symmetric Turing machine <ref type="bibr" target="#b24">[25]</ref> running in space s. For our purposes it suffices to know that any language in SSP ACE(s) can be reduced in DSP ACE(s) to an undirected (s, t)-connectivity (U ST CON ) problem for a graph on 2 O(s) vertices.</p><p>For the special case where s(n) = log n, we write BP L, RL, SL, BP H L, etc. for the various classes. By this convention, the class P rSP ACE(log n) would be denoted P rL, but we will use the standard name P L for this class.</p><p>We will occasionally consider complexity classes corresponding to computation that is both time- Remark: In most of the early papers on randomized computation from the late '70s and early 80s, the focus was on computations that do not necessarily halt absolutely, and the complexity class definitions were made for this case only. The definitions here of P rSP ACE(s), BP SP ACE(s) and ZP SP ACE(s) follow the usage introduced in these papers. The prefix V P appears in some of the early literature in place of the prefix R to denote one-sided error computation, but R seems to gradually have become the preferred choice.</p><p>As will be seen in Proposition 2.3, the condition of halting absolutely for space s computation is essentially equivalent to imposing a bound of 2 s on either the maximum or expected time of the computation. For the case s = log n, the corresponding time bound is thus polynomial. In this case, notation for, e.g., onesided classes has included RSP ACE poly (log n) and the more widely used RLP . The "sub H" notation proposed here seems more natural and convenient for general values of the space parameter.</p><p>The fact (see Proposition 3.1 below) that the class RSP ACE(s) coincides with N SP ACE(s) led to diminishing reference to RSP ACE(s) (and RL in particular) as a distinct class. Gradually it became clear that the "interesting" one-sided probabilistic log space class to study, was the one with a polynomial time bound. This led to the co-opting of the term RL to refer to this class, with ensuing notational inconsistencies. This notational ambiguity is particularly bothersome in the case of bounded error computation, since based on current knowledge, the classes BP H SP ACE(s) and BP SP ACE(s) are distinct from each other and from other standard classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The robustness of probabilistic classes</head><p>We have followed the convention that the language recognized by a probabilistic machine is the set of strings that are accepted with probability strictly greater than 1/2. The choice of 1/2 is, of course, arbi-trary. It is easy to show that the classes P rSP ACE(s) and P r H SP ACE(s) are unchanged if we set the acceptance threshold β to be any dyadic number in (0, 1) (dyadic means having a terminating binary representation; in fact β can be a function of the input, provided that β is computable in space s, and so, in particular is at most 2 s bits long). We'll say that the PTM M β-computes L if for x ∈ L, p M (x) &gt; β and for x ∈ L, p M (x) ≤ β. Given any such PTM, we can construct M ′ that computes L relative to acceptance threshold 1/2 by having M ′ first flip a coin. If the coin is heads, M ′ runs M on x and accepts if M does. If the coin is tails, M ′ flips a sequence of at most 2 s coins in order to simulate a single coin with bias 1 -β and accepts if that coin is heads. Conversely, it is not hard (by a similar type of construction) to convert a computation with acceptance threshold 1/2 to any dyadic threshold β with at most 2 s bits.</p><p>Similarly, for bounded error computation, we can replace both the upper and lower thresholds 1/3 and 1/2 by arbitrary dyadic rationals (possibly functions of the input) α and β provided that α ≤ β -2 -θ(s) We'll say that M (α, β)-computes L if for x ∈ L, p M (x) &gt; β and for x ∈ L, p M (x) &lt; α. Given a machine M that (α, β)-computes L, define the machine M ′ that on input x does 2 cs independent trials of M (x) for some large enough c, and accepts if the fraction of accepting trials exceeds (α + β)/2. By standard probability estimates, one shows that M ′ (2 -θ(s) , 1 -2 -θ(s) )computes L, i.e., the probability of error can be made exponentially small in s.</p><p>There is one subtle point here that is easy to miss. Based on the definition of a general probabilistic computation, the computation need not halt with probability 1. If it doesn't, then we can not reliably do repeated trials, since there is a nontrivial chance that the computation stalls during one trial. Thus, we need the following result of Simon:</p><p>Theorem 2. <ref type="bibr">1 [45]</ref> If M is a PTM running in space s with unbounded error (resp., bounded error, one-sided error) then there is a machine M ′ running in space s that computes the same language as L and runs in expected time 2 2 O(s) (and hence halts almost surely).</p><p>A simple construction of M ′ given in <ref type="bibr" target="#b40">[41]</ref> is: choose constants c and d appropriately and perform the following loop: while the computation has not halted, do (i) run M for d s steps, and if M does not halt, (ii) toss (c + d) s coins and if all come up tails then halt and reject.</p><p>The second part of the loop provides a "probabilistic clock" which will cause the computation to halt with probability 1, but whose expectation is very large, i.e., 2 (c+d) s . Thus M ′ clearly halts within the required expected time, and it suffices to show that it computes the same language as M . Clearly for any input x, p M ′ (x) ≤ p M (x), and thus the language computed by M ′ is a subset of that computed by M . On the other hand, it can be shown that the fact that M and M ′ run in space s and p M ′ (x) &lt; p M (x) imply that p M ′ (x) ≥ p M (x) -2 -c s and combined with a fact that for a space s PTM M ,</p><formula xml:id="formula_1">p M (x) &gt; 1/2 implies p M (x) ≥ 1/2 + 2 -c s ([15], Lemma 6.6), one concludes that p M (x) &gt; 1/2 implies p M ′ (x) &gt; 1/2.</formula><p>The classes R H SP ACE(s) and RSP ACE(s) are similarly robust with respect to the acceptance threshold. In discussing the various classes, we freely use whatever thresholds are convenient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Markov Chains and Matrix Computations</head><p>In understanding a computational model, two fundamental questions that are asked are: "what problems can be solved in this model?", and "are there natural mathematical structures for representing computation in this model?" For randomized space-bounded computation, answers to both of these questions can be found within the related domains of finite state Markov chains and matrix computation. We will assume familiarity with the most elementary facts about finite Markov Chains (see, e.g., <ref type="bibr" target="#b27">[28]</ref>).</p><p>Associated to every finite Markov chain C on state space S is its transition probability matrix P = P C , where for i, j ∈ S, P [i, j] is the probability, given that the chain is in state i at a particular step, of moving to state j. P is a stochastic matrix, i.e., it is nonnegative with row sums 1. Any submatrix of P (defined by eliminating rows and/or columns) is substochastic (nonnegative with row sums at most 1). Such a matrix Q is strictly substochastic if all row sums are strictly less than 1.</p><p>We will begin by formulating some related computational problems for Markov chains and for stochastic, substochastic and strictly stochastic matrices, and then will discuss the direct connection of these problems with probabilistic space-bounded computation.</p><p>For a Markov chain C with transition probability P , we define the following matrix-valued functions of C (or, equivalently P ). For an integer t ≥ 1, the t-step transition probability matrix P t is the matrix with P t [i, j] equal to the probability that, given the chain is in state i at a particular time, that the chain is in state j exactly t steps later. Of course, as the notation suggests, P t is equal to the t th power of P .</p><p>The hitting probability matrix P H , is the matrix with P H [i, j] equal to the probability that, given the chain is in state i at a particular time, that the chain will visit state j at some later time. It is a straightforward exercise to show that the j th column of this matrix can be computed as follows: let v be the j th column of P and Q be the matrix P with column j and row j replaced by 0's. Define Q * to be the matrix (with possibly infinite entries</p><formula xml:id="formula_2">) i≥0 Q i ; Q is called the com- pletion of Q [9]</formula><p>. Then it can be shown that (i) the matrix-vector product Q * v is well defined even if Q * has infinite entries since any infinite entry in Q * must coincide with a 0 entry of v, (ii) Q * v is equal to the j th column of P H and (iii) Q * is finite if and only if</p><formula xml:id="formula_3">I -Q is invertible, in which case Q * v = (I -Q) -1 ,<label>(iv)</label></formula><p>For any positive δ &lt; 1, the matrix δQ is strictly substochastic which implies that (I -δQ) is invertible and so (δQ) * = (I -δQ) -1 is finite. Furthermore Q * v is equal to the limit as δ tends to 1 of (I -δQ) -1 v.</p><p>The two functions above map matrices to matrices, and by specifying a particular entry [i, j] of the output, we may view them as mapping matrices to real numbers (and in our case the range will be [0, 1]). For any real valued function f on some domain D, we define the threshold language L f associated to f to be the set of inputs x ∈ D such that f(x) &gt; 1/2. We will consider three versions of the membership problem for this language, the exact, approximate and onesided versions. In each we require that on input x, if x ∈ L f , then x is accepted. In the exact version we require that if x ∈ L f , then x is not accepted. In the approximate version, false acceptance of x ∈ L is permitted for those x such that f(x) &gt; 1/3 and in the one-sided version we allow false acceptance of x ∈ L for those x for which f(x) &gt; 0.</p><p>Figure <ref type="figure">2</ref> shows a correspondence between spacebounded complexity classes and the threshold problem for the Markov chain functions and matrix functions described above. In each of the two rows, unbounded error computation corresponds to the exact version of the threshold problem for the function, bounded error computation corresponds to the approximate version and one-sided error computation to the one-sided version.</p><p>As will be seen below, for a given type of prob- </p><formula xml:id="formula_4">* = lim δ→1 (I -δQ) -1 , for Q stochastic (I -R) -1 , for R strictly substochastic</formula><p>Figure <ref type="figure">2</ref>: Matrix and Markov chain problems corresponding to space-bounded probabilistic computation abilistic space bounded computation, we can reduce a space s computation to the corresponding matrix problem for a 2 s × 2 s matrix whose entries are 0,1/2 or 1. On the other hand, each matrix problem of the given type (where the matrix entries can be arbitrary dyadic rationals) can be solved in the corresponding probabilistic space bounded complexity class, where the space required is logarithmic in the size of instance of the matrix problem. Various early papers in the area established that the problems listed within each row, in their various versions, play the role of complete problems for the corresponding set of complexity classes in that row. (Strictly speaking the approximate and one-sided versions of these problems are not complete for their corresponding class, since such a problem is not really a pure language membership problem, but is a "promise problem" <ref type="bibr" target="#b37">[38]</ref>. One can reformulate the definitions to make it all precise, but we won't do that here.)</p><p>We first discuss how to reduce arbitrary probabilistic space-bounded computation to Markov chain and matrix problems. As first observed in <ref type="bibr" target="#b14">[15]</ref>, the execution M (x) of a probabilistic machine M with space bound s, on a given input x, is a Markov chain whose state space is the set of 2 s possible configurations. Each step of M (x) is either a deterministic change in the configuration or involves reading the next random bit ("tossing a coin") and changing the configuration to one of two configurations depending on that bit. Thus, for a given configuration i, the i th row of the transition probability matrix either has a single nonzero entry, which is 1 (in the case that the next step from i is deterministic) or two nonzero entries which are each 1/2. By standard transformations we may assume that the Markov chain has exactly one state that corresponds to accepting x, labeled ACC, and that ACC is an absorbing state; i.e., P [ACC, ACC] = 1. We label the initial configuration of M (x) by ST ART .</p><p>With these definitions, it is clear that the probability that M (x) accepts is just the entry P H [ST ART, ACC] of the hitting probability matrix. As explained earlier, this computation boils down to the matrix computation</p><formula xml:id="formula_5">Q * = lim δ→1 (I -δQ) -1 for some substochastic Q. In [21], it is shown that if we choose δ ≥ 1 -2 -Cs for some C &gt; 0, then Q * [i, j] &gt; T if and only if (I -δQ) -1 [i, j] &gt; T . Thus we can reduce the problem Q * [i, j] &gt; T to the prob- lem (I -R) -1 [i, j] &gt; T for some strictly substochastic matrix R.</formula><p>For the halting classes, we can reduce them to the same problems as for the non-halting classes, but in fact we can reduce to the threshold problem for the t-step transition matrix. We say that a Markov chain is acyclic with respect to a designated state ST ART if the underlying directed graph of the Markov chain, when restricted to the set of states accessible from ST ART has no directed cycles except for self loops at absorbing states. Then we have: Proposition 2.2 The Markov chain associated to a computation that halts absolutely is acyclic with respect to the ST ART state. This is clear, since the existence of a cycle that uses only accessible states implies that there is a setting of the random coins on the tape that will cause the chain to traverse this cycle indefinitely so that the computation will not halt.</p><p>Acyclicity can easily be seen to imply that for any t exceeding the number of states of the chain, the chain starting from ST ART must occupy an absorbing state at step t. This means that the probability of eventually hitting ACC is equal to the probability of being in ACC after exactly t steps. From this the reductions in the top half of figure 2 follow.</p><p>The acyclicity of the Markov chain immediately implies that a space s PTM that halts absolutely runs with time bound 2 s . Formally, we have: Proposition 2.3 1. A probabilistic space s machine that halts absolutely, must halt within 2 s steps.</p><p>2. For each X ∈ {P r, BP, R, ZP },</p><formula xml:id="formula_6">X H SP ACE(s) = XT ISP (2 O(s) , s).</formula><p>The first part implies the forward inclusion of the second part; the reverse inclusion of the second part is easy but reflects a subtlety in the precise definition ( <ref type="bibr" target="#b14">[15]</ref>) of time-bounded randomized computation; i.e., time bound t(n) does not in general mean that the computation is guaranteed to halt in time t(n). However, one can easily show that for a computation in XT ISP (2 O(s) , s) if one uses an O(s)-bit clock to halt the computation after 2 O(s) steps, the resulting computation recognizes the same language.</p><p>Next, we sketch the other direction of the correspondences represented in figure <ref type="figure">2</ref>: that the various matrix computations can be solved within the corresponding complexity class.</p><p>Consider first the halting classes. It is easy to see that each variant of the transition probability problem (and hence the matrix exponentiation problem) can be solved in the corresponding complexity class. Basically, this is because we can build a probabilistic machine M that on input a 2 s × 2 s stochastic matrix and index i, simulates the run of the associated Markov chain starting from i, where the simulation runs in space s. (Notice that since we restrict to the case that all probabilities are dyadic rationals we can simulate a single transition exactly by a finite sequence of coin flips). To compute or approximate the threshold language associated to the t-step transition probability for states i and j and threshold T , we simply simulate the chain for t steps starting at i, and accept if and only if the simulation ends in state j.</p><p>Now consider the non-halting classes. We argue similarly that the various versions of the hitting time problem for Markov chains (and the corresponding completion problem for matrices) can be solved in these classes. Now the algorithm consists of running the Markov chain starting from i for an indefinite number of steps and halting and accepting when and if the chain arrives in state j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Randomness-bounded computation</head><p>Yet another way to view the restriction "halts absolutely" is as a restriction on the number of random bits used by the algorithm. By proposition 2.3 any space s halting machine runs for time at most 2 O(s) and hence uses at most this number of random bits. Conversely, any machine that satisfies a restriction on the number of random bits can be converted to one that is halting and accepts the same language, as follows: add a counter that increments with each step of the computation and is reset to 0 each time a new random bit is read, and if the counter ever reaches 2 s , halt and reject. This modification clearly guarantees that the machine halts absolutely, and it is easy to see that it accepts the same language as the original machine.</p><p>This interpretation of absolute halting suggests a further refinement of the probabilistic complexity classes, according to the number of random bits used.</p><p>If M is a PTM, we say that M operates within randomness bound r = r(n) if on any input x, M (x) uses at most r(|x|) random bits. If there is such a function, we say the machine is randomness-bounded, which by the above remarks, is the same as saying that the machine halts absolutely. For X ∈ {P r, BP, R, ZP }, and r = r(n), s = s(n), we define X r SP ACE(s) to be the subclass of XSP ACE(s) consisting of those languages that are recognizable by a PTM M that operates within randomness bound r(n) and satisfies the restrictions specified by XSP ACE(s). In analogy to Proposition 2.3 we have:</p><formula xml:id="formula_7">Proposition 2.4 For any X ∈ {P r, BP, R, ZP }, X H SP ACE(s) = X 2 O(s) SP ACE(s).</formula><p>On the other hand, the trivial deterministic simulation of a randomized machine (try all values of the random bits and count the number of accepting computations) yields: Proposition 2.5 P r r SP ACE(s) ⊆ DSP ACE(r + s), in particular P r s SP ACE(s) = DSP ACE(s). Thus, space s probabilistic computation with randomness bound θ(s) provides no advantage over determinism, while randomness bound 2 s gives the full power of absolutely halting probabilistic computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Relationships between complexity classes</head><p>As mentioned in the introduction, there has been considerable success in relating probabilistic classes to each other and to deterministic and non-deterministic classes. In figure <ref type="figure" target="#fig_0">3</ref> we summarize the present state of knowledge regarding the relationships between the various classes. In figure <ref type="figure">4</ref> we present a plausible guess of the "true" map of this part of the complexity theory world. In it there are only three distinct classes, deterministic space s, nondeterministic space s, and unbounded probabilistic space s, and all the other classes are equivalent to one of those three. The complete lack of lower bound techniques for complexity classes above N C 1 means, of course, that proving the distinctness of these three classes remains a wistful dream. However, the underlying theme of the rest of this paper is that there collapsing the picture in 3 down to the picture in 4 may indeed be possible in the foreseeable future.  In this section, we review in detail the results summarized in figure <ref type="figure" target="#fig_0">3</ref>. At the same time, we highlight various questions that point in the direction of figure <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Solving U ST CON in R H L</head><p>The most well known use of probabilism in spacebounded computation is the result of Aleliunas et al. <ref type="bibr">[3]</ref> showing how U ST CON can be decided in R H L. By our earlier remark that SSP ACE(s) can be reduced to a U ST CON problem on a graph of size 2 s , this implies that SSP ACE(s) ⊆ R H SP ACE(s).</p><p>Given the n vertex graph G and vertices s and t, the randomized algorithm for U ST CON is to start at vertex s and take a random walk on the graph G: when the walk is at vertex v choose one of the neighbors of v uniformly at random and move to that neighbor. It can be shown that if s and t are in the same component then after at most poly(n) steps the random walk will reach t with high probability. So simply walk for that number of steps and accept if and only if the vertex t is visited during the walk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Probabilism vs. non-determinism</head><p>One of the earliest observations made about probabilistic space-bounded computation is that, when there is no halting restriction, one-sided error is equivalent to non-determinism:</p><formula xml:id="formula_8">Proposition 3.1 [15] RSP ACE(s) = N SP ACE(s)</formula><p>The forward inclusion is trivial, while the reverse inclusion is obtained by noting that given a NTM M , Figure <ref type="figure">4</ref>: A conjectural view of the space-bounded complexity world we can construct a P T M M ′ that on input x, performs an infinite loop, where in each iteration it simulates M , using random bits in place of the nondeterministic ones. Such a machine will never falsely accept x ∈ L, and will accept any x ∈ L with probability 1, hence operates with one-sided error.</p><p>This equivalence, together with the trivial inclusions between probabilistic classes, imply that R H SP ACE(s) ⊆ N SP ACE(s) ⊆ BP SP ACE(s) ⊆ P rSP ACE(s). In light of the celebrated result <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b47">48]</ref> that the alternating space hierarchy collapses to N SP ACE(s), the second question represents an analog to the result <ref type="bibr" target="#b45">[46]</ref> that BP P is contained in the polynomial time hierarchy. However, a straightforward attempt to adapt the simulation for BP P to question 3.2 fails for at least two reasons: the simulation seems to require two-way access to non-deterministic bits and also to require the computation of universal hash functions in small space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Halting versus non-halting computations</head><p>Proposition 3.1 makes apparent the potentially drastic affect that the halting condition has on one-sided error computation: without the halting condition one gets the full power of non-determinism, while it seems implausible that R H SP ACE(s) = N SP ACE(s). In contrast, for unbounded error computation, Jung showed that the two classes coincide:</p><formula xml:id="formula_9">Theorem 3.1 ([22]) P rSP ACE(s) = P r H SP ACE(s).</formula><p>The proof of the non-trivial inclusion as simplified in <ref type="bibr" target="#b3">[4]</ref>, follows from (1) the fact listed in figure <ref type="figure">2</ref> that P rSP ACE(s) can be reduced to the threshold problem for the entry of some matrix inverse, (2) the fact that the threshold problem for a matrix inverse can be converted (via Kramer's rule) to the question of which of two matrices has a larger determinant, and (3) this latter problem is solvable in P r H SP ACE(s).</p><p>What about the relationship between BP SP ACE(s) and BP H SP ACE(s)? Since N SP ACE(s) ⊆ BP SP ACE(s), and we suspect that BP H SP ACE(s) equals DSP ACE(s), we certainly expect the two classes to be different. One reasonable intuition is that the power of BP SP ACE(s) is simply the combination of the power of BP H SP ACE(s) and N SP ACE(s): Question 3.3 Is BP SP ACE(s) equal to the class of languages accepted by a deterministic space s machine with access to oracles for N SP ACE(s) and BP H SP ACE(s)?</p><p>Note also that if question 3.3 is shown to have an affirmative answer, then questions 3.1 and 3.2 would be equivalent.</p><p>In an effort to refine this question, one can define a class that contains both BP H SP ACE(s) and N SP ACE(s) and is contained in BP SP ACE(s), which we will call, for lack of a better term, SBP H SP ACE(s), (for slightly bounded error). (This class arose in discussions with Eric Allender, Shiyu Zhou and David Zuckerman). A PTM M is said to accept a language L with slightly bounded error if there is an integer valued function K(n), computable in space s(n), such that for x ∈ L, p M (x) &gt; 2 -K(|x|) and for x ∈ L, p M (x) ≤ 2 -K(|x|)-1 . This class differs from BP H SP ACE(s) because the function K might be as big as 2 θ(s) and so the error probability is potentially doubly exponential in s and thus too small to amplify by a halting computation (which can only perform 2 s iterations.) However, using a probabilistic clock similar to the one that appears in the proof of Theorem 2.1, one can amplify the acceptance probability by performing a random number of trials where the expected number of trials is 2 2 K(|x|) , and show that SBP H SP ACE(s) ⊆ BP SP ACE(s). It is possible that the inclusion also goes the other way, which would be interesting on its own, and also might help to resolve question 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Complementary classes</head><p>Theorem 3.2 The following space-bounded complexity classes are closed under complementation: ZP SP ACE(s), RSP ACE(s), BP SP ACE(s), P rSP ACE(s), ZP H SP ACE(s), BP H SP ACE(s), P r H SP ACE(s), SSP ACE(s), and N SP ACE(s).</p><p>For each of the classes ZP SP ACE(s), ZP H SP ACE(s), BP H SP ACE, and P r H SP ACE(s) the result follows trivially from the definition. For BP SP ACE(s) and P rSP ACE(s) closure under complement was first shown in <ref type="bibr">[45]</ref>; it requires Theorem 2.1, which shows that we can assume that a probabilistic computation halts almost surely, and this then permits the roles of acceptance and nonacceptance to be interchanged. For P rSP ACE(s) one can also see this as a consequence of the equivalence of P rSP ACE(s) and P r H SP ACE(s).</p><p>For N SP ACE(s) (and hence for RSP ACE(s)), closure under complementation is, of course, from <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Finally for SSP ACE(s), the result follows from a very clever direct reduction <ref type="bibr" target="#b34">[35]</ref> from the problem co -U ST CON to U ST CON . This implies an earlier result of <ref type="bibr" target="#b7">[8]</ref>, which was proved using inductive counting arguments in the spirit of <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b47">48]</ref>, that SSP ACE(s) ⊆ co -R H SP ACE(s), i.e., SSP ACE(s) ⊆ ZP H SP ACE(s).</p><p>Conspicuous by its omission from the list of classes in Theorem 3.2 is the class R H SP ACE(s). Indeed, the following is a very tantalizing open question, which appears in <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_10">Question 3.4 Is R H SP ACE(s) closed under com- plement, i.e., is R H SP ACE(s) = ZP H SP ACE(s)?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Deterministic Simulations</head><p>The first deterministic simulation results for bounded-space probabilistic computation were for the most powerful class P rSP ACE(s). Gill <ref type="bibr" target="#b14">[15]</ref> showed that P rSP ACE(s) ⊆ DSP ACE(2 O(s) ). Simon <ref type="bibr" target="#b44">[44]</ref> was the first to show that P rSP ACE(s) can be simulated deterministically with only a polynomial increase in space, specifically, in DSP ACE(s 6 ). This result was improved independently by Borodin, Cook and Pippenger, and Jung:</p><formula xml:id="formula_11">Theorem 3.3 ([9, 20]) P rSP ACE(s) ⊆ DSP ACE(s 2 ).</formula><p>This strengthens Savitch's fundamental theorem <ref type="bibr" target="#b43">[43]</ref> that N SP ACE(s) ⊆ DSP ACE(s 2 ).</p><p>The proofs of Theorem 3.3 and its antecedents were based on small space solutions to one of the associated matrix problems discussed in section 2. <ref type="bibr" target="#b3">4</ref>. In particular, in [9], the problem of computing the limit as δ -→ 1, of (I -δQ) -1 is solved by computing the inverse matrix treating δ as a formal variable and then passing to the limit. These computations are shown to be performable in N C 2 ⊆ DSP ACE((log m) 2 ) (as a function of the matrix size m = 2 s ).</p><p>Any improvement in Theorem 3.3 would, in particular, imply an improvement in Savitch's theorem, and would be a remarkable achievement.</p><p>The other known deterministic simulations are for bounded error halting computations. Each approaches the conjectured collapse BP H SP ACE(s) = DSP ACE(s) from a different direction.</p><p>The matrix problems that were shown to be equivalent to P rSP ACE(s) are all solvable in polynomial time, which implies that P rSP ACE(s) ⊆ DT IM E(2 O(s) ). It is not known, however, whether problems in P rSP ACE(s) (or even N SP ACE(s)) can be solved by a DTM that runs simultaneously in polylog space and polynomial time. For BP H SP ACE(s), such a result was obtained by Nisan: Theorem 3.4 ( <ref type="bibr" target="#b30">[31]</ref>)</p><formula xml:id="formula_12">BP H SP ACE(s) ⊆ DT ISP (2 O(s) , s 2 ).</formula><p>When specialized to the case s = log n, this says that bounded error log-space poly-time probabilistic computation (BP H L) can be simulated in deterministic polylog-space poly-time (SC, or "Steve's class"). In contrast, the known deterministic simulations of N L in polylog space require n log n time.</p><p>In another direction, Ajtai, et al.</p><p>[2] looked at the problem of deterministically simulating space s PTMs that operate with some randomness bound r. As noted in Proposition 2.4, if r = θ(s), then such a simulation is trivial and for r = 2 s this the problem is equivalent to simulating any halting computation. They showed that a one-sided probabilistic computation operating with randomness bound s 2 / log s can be made deterministic. This was extended by Nisan and Zuckerman, to the case that r is any polynomial in s, and to bounded error (not just one-sided error) computation:</p><formula xml:id="formula_13">Theorem 3.5 ([37]) BP poly(s) SP ACE(s) ⊆ DSP ACE(s).</formula><p>The best result known for deterministic simulations of halting bounded error computations with no nontrivial restriction on the number of random bits was proved by Saks and Zhou: Theorem 3.6 ( <ref type="bibr" target="#b41">[42]</ref>)</p><formula xml:id="formula_14">BP H SP ACE(s) ⊆ DSP ACE(s 3/2 ).</formula><p>This result generalized the previous result of Nisan, Szemerédi and Wigderson <ref type="bibr" target="#b33">[34]</ref> that U ST CON could be solved in DSP ACE((log n) 3/2 ).</p><p>The central ingredient in the deterministic simulations that achieve Theorems 3.4, 3.5 and 3.6 are pseudorandom generators for space-bounded computation. The discussion of such generators and their use in these results is the subject of the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Pseudorandom number generators for space-bounded computation</head><p>A pseudorandom generator (PRG) for a class of probabilistic machines is, intuitively, a probabilistic machine that uses only a "small" number of random bits from it's tape and outputs a long "pseudorandom" string that "fools" any machine in the class, in the sense that no machine in the class can distinguish the output of the PRG from a truly random string. Clearly, such PRGs can be used to reduce the amount of randomness needed by a computation. In this section we discuss the construction and application of PRGs for the class of halting space-bounded PTMs (HSBP T M s).</p><p>Formally, a generator is a function G that maps {0, 1} R2 to {0, 1} R1 for some integers R 2 and R 1 . The parameter R 2 is referred to as the seed length of the generator, and the parameter R 1 is the output length of the generator. We use the notation G{R 2 , R 1 } to indicate that G is a generator with these parameters.</p><formula xml:id="formula_15">If G{R 2 , R 1 } is a generator, then G induces a prob- ability distribution D G on {0, 1} R1 called the output distribution of G: for z ∈ {0, 1} R1 , D G (z) is equal to the probability, for w selected uniformly from {0, 1} R2 , that G(w) = z.</formula><p>Now suppose that M (x) is a PTM execution that uses at most R 1 random bits. We defined p M (x) to be the probability that M accepts x, under the assumption that the R 1 random bits it uses are selected uniformly from all R 1 bit strings. If G{R 2 , R 1 } is a generator, define p G M (x) to be the probability that M accepts x under the assumption that it uses R 1 bits selected according to the output distribution of G. We come to the first key definition: Definition 4.1 Let G{R 2 , R 1 } be a generator and ǫ ∈ (0, 1).</p><formula xml:id="formula_16">1. If M (x) is a PTM execution using at most R 1 random bits, then G is said to ǫ-fool M (x) if |p G M (x) -p M (x)| ≤ ǫ.</formula><p>2. Let S be a positive integer. G is said to be an ǫ-PRG for space S if for any PTM execution M (x) that runs in space S and with randomness bound</p><formula xml:id="formula_17">R 1 , G ǫ-fools M (x).</formula><p>Intuitively, G is an ǫ-PRG if it is impossible for any space S computation to statistically distinguish the output of G from a random string, with probability more than ǫ.</p><p>As defined, generators are "non-uniform" objects: each generator is a single function mapping strings of one fixed length to another. To be useful for deterministic simulation of HSBP T M computations, we want to construct an ensemble of generators whose seed length and output length are variable and that are, in some appropriate sense, efficiently computable. This leads to the next set of definitions.</p><p>First, let us establish a notational convention: we will continue to use s = s(n) to refer to the spacebound of some HSBP T M M and r = r(s) to denote the randomness bound. We will use capital letters S and R to denote specific values that s and r take on. For instance, if x is an input to M , then when we refer to the execution M (x), we will use S = s(|x|) and = r(S).</p><p>We define HSBP T M generator ensemble Ĝ to be a HSBP T M that takes as input two parameters S and R, where S represents the space bound of the computation that the generator is intended to fool, and R is the number of pseudorandom bits being requested from the generator. The generator outputs R bits. The seed length of Ĝ on input S, R, L(S, R) = L Ĝ(S, R) is defined to be the maximum number of random bits that Ĝ reads from its random tape during it's computation. The behavior of Ĝ on input S and R can be viewed as a generator G S,R {L(S, R), R}. By definition Ĝ is itself spacebounded. The space-bound s Ĝ of Ĝ is expressed as a function of S and R (not of the input length).</p><p>Let Ĝ be a HSBP T M ensemble. If M is a PTM with space bound s and randomness bound r, we define the Ĝ simulation of M , M ′ = Ĝ • M , to be a PTM that on input x, simulates M (x), but instead of directly using random bits from its auxiliary tape, it uses the output of G S,R , where S = s(|x|) and R = r(s). (Notice that we are assuming (without loss of generality) that M ′ comes "equipped" with the functions s and r). M ′ (x) executes M (x), until the time it needs a random bit. Instead of reading a bit from its auxiliary tape, it simulates Ĝ on inputs S and R until Ĝ outputs a bit, at which point the execution of M (x) resumes, using the first output bit of G S,R in place of M (x). Each time that M (x) needs another random bit, the execution of G S,R is resumed where it left off until it produces the next output bit, which is used in M (x). The space needed for Ĝ•M (x) is the sum of the space needed for G S,R and M (x), and thus:</p><p>Proposition 4.1 Let M be a HSBP T M with space bound s randomness bound r(s), and let Ĝ be a HSBP T M generator ensemble with space bound s Ĝ(S, R). Then Ĝ • M is a PTM with space bound s + s G (s, r(s)) randomness bound L Ĝ(s, r(s)).</p><p>for any input x and for S = s(|x|), R = r(S), the probability</p><formula xml:id="formula_18">p Ĝ•M (x) that Ĝ • M accepts x is equal to p G S,R M (x).</formula><p>The next definition is the analog of definition 4.1 for HSBP T M generator ensembles. Definition 4.2 Let Ĝ be an HSBP T M generator, and let ǫ = ǫ(S, R) be a function whose range is in the interval (0, 1).</p><p>1. Let M be a HSBP T M . Let x be an input and let S and R be the space and randomness bound for M The ensemble Ĝ is said to ǫ-fool the execution M (x) if the generator G S,R ǫ(S, R)-fools M (x).</p><p>2. Let r = r(S) be a (suitably well behaved) function with r(S) ≤ 2 S . Ĝ is an ǫ-PRG for randomness bound r, or (ǫ, r)-PRGE (pseudorandom generator ensemble) if for any integer S ≥ 1, G S,r(S) is an ǫ(S, r(S))-PRG for space S. In other words, for any HSBP T M M with randomness bound r, and for any input x, Ĝ ǫ-fools M (x).</p><p>Note that since every HSBP T M has randomness bound 2 s , an (ǫ, 2 s )-PRGE Ĝ ǫ-fools every HSBP T M and so we say, simply that Ĝ is an ǫ-PRGE.</p><p>The existence of appropriate PRGEs implies the following deterministic simulation result for BP H SP ACE(s): Proposition 4.2 Suppose that Ĝ is an (1/20, r)-PRGE for some suitably well behaved function r = r(s), and that Ĝ has seed length function L and runs in space s Ĝ. Then:</p><formula xml:id="formula_19">BP r SP ACE(s) ⊆ BP L(s,r) SP ACE(s + s Ĝ(s, r)) ⊆ DSP ACE(s + s Ĝ (s, r) + L(s, r))</formula><p>Indeed, the first containment follows from Proposition 4.1 and the fact that by the definition of an ǫ-PRGE, if M is a HSBP T M that computes L with bounded error, then Ĝ • M (1/3 + ǫ, 1/2 -ǫ)-computes L, as defined in section 2.3. The second containment follows immediately from the naive derandomization of Ĝ • M as in Proposition 2. <ref type="bibr" target="#b4">5</ref>.</p><p>What do we need in this proposition to prove BP H SP ACE(s) ⊆ DSP ACE(s)? For an arbitrary HSBP T M , we may take r(s) = 2 s . If the seed length, L(s, 2 s ) is θ(s) and the space s Ĝ(s, 2 s ) required to produce 2 s bits is also θ(s), then we would have this optimal deterministic simulation of BP H SP ACE(s). Now, it is an elementary exercise in the probabilismethod to show that if we omit the requirement that the generator be computable, and take, for each S ≥ 1, G S,2 S to be a random function mapping c 1 S bits to 2 S bits, then the result would fool all probabilistic computations running in space S. This fact can be as a rough analog to Adleman's result ( <ref type="bibr" target="#b0">[1]</ref>) that BP P can be computed by non-uniform polynomial circuits.</p><p>Of course, we do not now know how to construct efficiently computable generators that achieve the optimal simulation (otherwise, this would be quite a difpaper!). The crux of the problem, then, is to find a PRGE whose seed length and space requirement are sufficiently small functions of S and R.</p><p>The literature contains six constructions of generators of various quality which we will discuss. The last of the six generators, which builds heavily on previous ones, provides the best parameters currently known: it gives an ǫ-PRG Ĝ for some ǫ = R -θ (1) , such that the seed length L(S, R) is O( S log R 2 log S-log log R ), and the space required for the generator is linear in the size of the seed. The two most interesting special cases are when R = 2 S , in which case the seed length is O(S 2 ), and the case that R = poly(S) in which case the seed length is O(S). For each of these two special cases, previous constructions had given the same bounds.</p><p>The remainder of this section consists of six subsections each devoted to one of the generator constructions, presented in historical order. These subsections summarize what the various generators accomplish, and describe their constructions. The question of why these constructions are generators is beyond the scope of this survey. As stated in the previous section, the three deterministic simulations stated in Theorems 3.4,3.5 and 3.6 were obtained using these generators. For Theorem 3.5, the simulation is constructed directly from one of the generators by using Proposition 4.2. For each of the other two, the deran-domization uses Nisan's generator inside a more intricate simulation. We will discuss these simulations together with Nisan's generator in Section 4.3.</p><p>Two final remarks on terminology and notation before proceeding to the descriptions of the generators. As described above, we are really interested in defining generator ensembles, not just generators. However, it is more convenient to define a generator ensemble Ĝ by describing a component generator G S,R for generic parameters S and R. The fact that these generators are part of an ensemble is almost always implicit. Typically, it is also more convenient to first define the generic function in terms of parameters other than S and R, and then to relate the chosen parameters to S and R.</p><p>The second remark is that, in the case that we are only trying to build an (ǫ, r)-generator for some particular randomness bound r, it suffices to assume that the parameter R is equal to r(S); since we don't care the generator does for other values of R. In this case, we can and will view Ĝ as taking a single parameter S, and, if needed for clarity, we refer to Ĝ as a single parameter HSBP T M generator ensemble. We then write L(S) for the seed length function, and G S for the function from L(S) bits to r(S) bits defined by Ĝ with input S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The AKS pseudorandom sampler</head><p>The first work in the direction of constructing PRGs for small space was by Ajtai, Komlós and Szemerédi <ref type="bibr">[2]</ref>. Although the formalism they used differs considerably from the one given above, what they presented can be interpreted as a construction of a single parameter HSBP T M generator that, on input S, produces a string of length r(S) = θ( S 2 log S ) from a seed of size θ(S) in space θ(S). Their generator satisfies the following condition, which is a relaxation of the condition of being a PRGE. Definition 4.3 Let G{R 2 , R 1 } be a generator and ǫ ∈ (0, 1).</p><formula xml:id="formula_20">1. If M (x) is a PTM execution using at most R 1 random bits, then G is said to ǫ-sample M (x) if p M (x) ≥ ǫ implies that p G M (x) &gt; 0. 2.</formula><p>Let S be a positive integer. G is said to be an ǫ-PRS (pseudorandom sampler) for space S if for any PTM execution M (x) that runs in space S, G ǫ-samples M (x).</p><p>In the same way that PRGEs were defined from PRGs, we can define a pseudorandom sampler ensemble (PRSE) from PRS. The definition of a pseudorandom sampler ensemble is exactly what is needed for use in a deterministic simulation of a one-sided error computation (provided that ǫ ≤ 1/2). If M computes L with one-sided error, then given a 1/2-PRSE, we can deterministically decide whether x ∈ L by running the computation M (x) using the output of the generator for each possible seed. If x ∈ L no computation will accept x (since the computation has one-sided error), while if x ∈ L, there will be at least one seed which causes the execution to accept.</p><p>We remark (1) that an (ǫ, r)-PRGE for space bounded computation is automatically an (ǫ, r)-PRSE (2) the definition of an (ǫ, r)-PRSE makes sense even if G S is a partial mapping from L(S) bits to rather than a total mapping.</p><p>We briefly describe the AKS construction of a pseudorandom sampler, which introduced an important technique known as "expander walks". We present a simplified version of their that was given in <ref type="bibr" target="#b25">[26]</ref> for another purpose. For α ∈ (0, 1) and positive integers N , and d &gt; 0, an undirected graph H is an (N, d, α)expander if H has N vertices, maximum degree d and for any subset A of vertices, the fraction of vertices in V (G) -A that have a neighbor in A is at least α|A|/N . We will think of a d-regular expander as a directed graph in which each edge is replaced by a directed edges in each direction, and the edges coming out of a vertex are indexed from 1 to d. The generator G S uses an with N = 2 S vertices, whose vertices all have the same degree, where both the degree d and the parameter α are constants independent of S.</p><p>expanders exist and are explicitly constructible in the following sense: there is an algorithm that given an S-bit index to a vertex v and an integer i between 1 and d, runs in space θ(S) and returns the index of the i th neighbor of v (see, e.g. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref>).</p><p>Given a d-regular expander H and arbitrary positive integers J ≤ K we define a partial mapping sending S + (log d)(K -1) + K bits to JS bits as follows. Fix a (possibly redundant) encoding of the J element subsets of {1, 2, . . ., K} by K bits. Given the input, use the first K bits to specify such a J element subset I. Use the remaining bits to specify a path (v 1 , v 2 , . . . , v K ) in H of K vertices as follows: use S bits to specify a vertex and use each of K -1 successive blocks of log d bits to specify an edge label, and construct the path following those labels. Finally, use the subset</p><formula xml:id="formula_21">I = {i 1 &lt; i 2 &lt; . . . &lt; i J } to extract a subsequence (v i1 , v i2 , . . . , v iJ ) from (v 1 , v 2 , . . . , v k ) and</formula><p>this sequence as a sequence of JS bits. The main result of [2] is that for each ǫ &gt; 0 there are constants C 1 , C 2 , for K = C 1 S and J = C 2 S/ log S, this mapping is an (ǫ, r)-PRS, for r(S) = θ(S 2 / log S).</p><p>The result is proved by a delicate induction using a carefully selected (and quite technical) induction hypothesis. Vaguely, the idea is that a randomized computation defines a random walk through the configuration space. If the machine is such that with a substantial probability (at least ǫ) the random walk accepts, then the expander property guarantees that the sequences produced by the generator are sufficiently "spread" out that they are guaranteed to include an accepting path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The BNS generator</head><p>The first formalization of the notion of a PRG for small space and the first construction of such a generator, was done by Babai, Nisan and Szegedy <ref type="bibr" target="#b6">[7]</ref>. Their generator is most easily described in terms of three parameters k,t and u. For now, view these paas fixed. The construction needs a function f = f k,u that maps ku bits to 1 bit. We write f as f(x 1 , x 2 , . . . , x k ) where each x i ∈ {0, 1} u . For the given t, if</p><formula xml:id="formula_22">I = {i 1 &lt; i 2 &lt; . . . &lt; i k } is a subset of {1, 2 . . . , t}, define the function f I : {0, 1} ut -→ {0, 1} by f I (x 1 , . . . , x t ) = f(x i1 , x i2 , . . . , x ik ). Let B = t k</formula><p>and let I 1 , I 2 , . . . , I B be the enumeration of the k element subsets of t in colex order (i.e., I &lt; J if the highest element in their symmetric difference is in J). Finally, define G(x 1 , . . . , x t ) to be the concatenation of the bits f Ij (x 1 , . . . , x t ) for 1 ≤ j ≤ B.</p><p>The function G maps ut bits to t k bits. If f is chosen to be the generalized inner product function, whose output is the sum for 1 ≤ ≤ u of the product of the i th bits from each input, then it is shown that the resulting generator is an ǫ-PRG (for any ǫ &gt; 0) for space s, provided that s ≤ u/c k for some constant c = c(ǫ). For a given space S, we can choose the as follows: u = 2 θ( √ S) , k = θ( √ S), t = 2 k , to obtain a generator that maps 2 θ( √ S) bits to 2 θ(S) bits and ǫ-fools all space S computations, for some ǫ = 2 -θ( √ S) . proof that their generator is a PRG for space bounded computation is based on a connection between small space computation and a model of multiparty communication complexity introduced in [10]. Let f(x 1 , x 2 , . . . , x k ) be a boolean function as above. In the multi-party communication model, there are k parties, and the i th party knows every input except x i . They communicate by means of a blackboard readable by all. The communication complexity of f is the minimum number of bits that must be written in worst case in order that one of them can evaluate f. The ǫ-complexity of f, C ǫ (f) is the minimum number of bits required so that f is correctly evaluated on at least a 1/2 + ǫ of the inputs. Now fix such a function (family) f = f k,u , and consider the above generator G constructed from f. What Babai, et al. show is that if there is a space S execution M (x) that is not ǫ-fooled by the generator, then one can used this machine to construct a multi-party protocol using kS bits that is correct on at least 1/2+ǫ of the inputs, and therefore C ǫ (f)/k &lt; S. Thus if we find a function f for which a lower bound on C ǫ (f) can be proved, we get a corresponding lower bound on the smallest space needed to distinguish the output of the generator from random. In the case of the generalized inner product function, they showed Ω( u 4 k + log ǫ) lower bound on C ǫ (f), which implies the results stated above for G.</p><p>What is the limit of this approach? For multiparty communication complexity there is a trivial upper bound of u (since one party can simply announce its input to the others), so we can't hope for a bound better than C ǫ (f) ≥ u. If we did get such a bound, the corresponding lower bound on the space needed to beat the generator would be u/k. Thus to produce 2 s bits that fool space s machines, the best we could do would be to take k = θ(s), t = 2k and u = θ(s 2 ) giving a generator that needs a seed of θ(s 3 ) bits. This would be a considerable advance over the above generator, but still falls short of the θ(s) size seed that we'd like in order to derandomize space s computations.</p><p>No explicit functions f are known for which such a strong lower bound, or even a bound of u/poly(k) can be proved (although a random function f can be shown to satisfy such a bound). However, as will be seen in succeeding section, other approaches to generator construction lead to PRGs produce 2 s bits from a seed of size θ(s 2 ), which is better than the theoretical limit to the above approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Nisan's generator and its application to derandomization</head><p>The next construction of a PRG for small space was Nisan's, which was a major breakthrough. First of all, this construction substantially reduced the size of the seed needed to produce 2 s bits fooling space s executions, from 2 θ( √ s) to only θ(s 2 ). This is a remarkable achievement, but unfortunately, it seems to fall just short of providing an improved deterministic simulation of BP H SP ACE(s). This is because if we simulate BP H SP ACE(s) by running over all seeds, we will need θ(s 2 ) space just to store the seed. As we have seen, there is already a θ(s 2 )-space deterministic simulation not just for BP H SP ACE(s) but for P rSP ACE(s).</p><p>However, it turns out that Nisan's generator has some subtly stronger properties than being a PRG. These properties can be carefully exploited to obtain two deterministic simulations of BP H SP ACE(s), one in DT ISP (2 O(s) , s 2 ) (Theorem 3.4) and the other in DSP ACE(s 3/2 )(Theorem 3.6).</p><p>For fixed S and R, Nisan's construction involves not one function, but a family of functions. The property of this construction is that the family of generators is not too large and has the property that every space S computation using R bits is fooled by most of them. The following Theorem is implicit in Nisan's work: 1. Each of the functions G h maps b bits to R = b2 k bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Given as input</head><formula xml:id="formula_23">h ∈ {0, 1} 2bk and x ∈ {0, 1} b , G h (x) can be computed in space O(b + k).</formula><p>3. Suppose that S = γk and R = b2 k where γ &gt; 0 a suitably small positive real number. Suppose M is a PTM and x in an input such that the computation M (x) with space bound S and randomness bound R. Then for all but an ǫ = R -Ω(1) of choices of h ∈ {0, 1} 2bk , G h ǫ-fools M (x). Moreover, given the PTM M and input x it is possible to compute such an index h {0, 1} 2bk in time O(R O (1) ) and space O(bk).</p><p>It is easy to transform such a generator family into a single "amalgamated" generator G mapping (2k + 1)b bits to b2 k bits: G = G b,k its input as (h, x) where h ∈ {0, 1} 2bk and x ∈ {0, 1} b and computes G(h, x) = G h (x). Furthermore, it is easy to see condition 3 above implies that this amalgamated generator R -Ω(1) fools every computation running in space S = γk. Restricting to a particular choice of parameters we get: Corollary 4.1 Let b, k, S be integers with b = S = γk for some suitable γ &gt; 0. The amalgamated generator built from the family G b,k is a 2 -θ(S) -PRG that for space S that produces 2 θ(S) bits from a seed of size θ(S 2 ).</p><p>Before sketching the construction of Nisan's generator, we discuss the way Theorem 4.1 is used to prove Theorems 3.4 and 3.6. Theorem 3.4 follows almost immediately. First, notice that, while the amalgamated generator requires seeds of length S , most of the bits are used only to index the generator within the family.</p><p>The generators in the underlying family each require seeds of size only O(S). We know by the last part of the Theorem that for any execution M (x) running in space S, almost all of the generators in the family will fool M (x). If we could get our hands on one such member of the family we could, by running over all seeds, derandomize the computation in space S. The second assertion in the last part of the Theorem implies that we can indeed identify such a generator in DT ISP (2 S , S 2 ). Thus, by first finding the generator and then running over all seeds to it, we obtain a deterministic simulation of (x) that runs in time polynomial in 2 S and space θ(S 2 ) . This proves Theorem 3.4.</p><p>Theorem 3.6 does not seem to follow as easily from Theorem 4.1. We now sketch the proof. Note that this proof sketch is a from the main discussion of generators and can be skipped without loss of continuity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">BP</head><formula xml:id="formula_24">H SP ACE(s) ⊆ DSP ACE(s 3/2 )</formula><p>Recall that in order to tell whether a bounded error space S computation accepts, it suffices to estimate a particular entry of the n = 2 S power of the n × n stochastic matrix for the associated Markov chain. Thus, to prove Theorem 3.6 it would suffice to find a deterministic algorithm for this approximate exponentiation problem that runs in space O((log n) 3/2 ). More generally, the algorithm given in <ref type="bibr" target="#b41">[42]</ref> approximates P t [i, j] within 1/n and in space O(log n(log t) 1/2 ), for t ≤ n where P is an n × n stochastic matrix. For purposes of the description here we'll assume the entries of P are all 0,1/2 or 1, which is the case for stochastic matrices that model probabilistic machines, and also assume that t = 2 q is a power of 2, but the algorithm can be made to work for arbitrary rational entries and integer exponents.</p><p>In the following discussion, we will consider some randomized algorithms that operate in the following way: first they flip all of their random bits, which are all written to memory and then they proceed deterministically. For such an algorithm we distinguish between the space used to store the random bits, and the rest of the space used. We refer to the latter use of space as processing space.</p><p>At the highest level, the approximation algorithm for matrix exponentiation is obtained by derandomizing a particular randomized algorithm. This randomized algorithm has the following properties: (1) On input P and t = 2 q and indices i, j, the algorithm outputs a value z ∈ (0, 1) such that with probability at least 1 -1/n, z is within 1/n of P t [i, j], (2) The algo-rithm uses O(q 1/2 log n) random bits, and (3) The processing space of the algorithm is at most O(q 1/2 log n).</p><p>Given such an algorithm, we can easily derandomize it in space O(q 1/2 log n) by running the algorithm on the given input for each of the possible random strings, and averaging the outputs obtained. Properties (2) and (3) of the randomized algorithm guarantee that the space of the deterministic algorithm is as desired, and property (1) guarantees that the output of the algorithm is within 2/n of P t [i, j].</p><p>The construction of this randomized algorithm satisfying (1),(2) and (3) is developed in several steps. The starting point is an algorithm, algorithm A, that simply simulates the Markov chain defined by P for t steps starting from state i, and outputs z = 1 if the final state is j and z = 0 otherwise. Note that property (1) is not satisfied but we do have that the expected value of z is P t [i, j]. Furthermore, the processing space used is only θ(log n), so property (3) is than satisfied. However, the number of random bits used is t (one for each step of the Markov chain), which is far more than what is allowed by property (2).</p><p>Next, we modify algorithm A using Nisan's generator family G b,k with k = q and b = θ(log n). Recall that the members G h of G b,k are indexed by h ∈ {0, 1} 2bk . For each such h, the algorithm B h as follows: Select x ∈ {0, 1} b and run algorithm A using the first t of the output of G h (x) in place the random coins. Next, define the algorithm C h to be the one obtained by derandomizing B h : run 2 b trials, one for each choice of x average the results. Finally, define the algorithm C, that chooses h at random and then performs C h . Now the third property of the generator family implies that, if we fix P, t, i, j, then for all but 1/poly(n) choices of h, the fraction of accepting trials of algorithm A is within 1/poly(n) of the number of accepting trials of B h (where we can adjust the size of the polynomial by adjusting b by a constant factor). For any such h, the algorithm C h will output a value that is within 1/poly(n) of P t [i, j]. Thus the algorithm C has the property that with probability at least 1 -1/poly(n) it outputs a value that is within 1/poly(n) of P t [i, j], which is stronger than that needed for condition <ref type="bibr" target="#b0">(1)</ref>.</p><p>number of random bits needed by algorithm C is θ(q log n) (to choose h), which exceeds the number required by property (2) by a factor of q 1/2 . On the other hand, the processing space of this algorithm, i.e., the space over that used to record h, is only θ(log n) is better than that required by property (3), by a factor q 1/2 . The goal now is to try to somehow tradeoff the excess number of random bits used against the unused processing space. The next step is to express the computation of P t [i, j] recursively. Assume without loss of generality that q = w 2 is a perfect square and define P 0 = P and P i = P 2 w i-1 . Then P w = P 2 q and we may use the recurrence for P i to get a recursive algorithm for computing P t . Now, at each level of recursion, instead of computing P i = P 2 w i-1 exactly, we can use the algorithm C to estimate it. The resulting algorithm is algorithm D. At each level of recursion, algorithm D is estimating the 2 w power of a matrix, so the call to algorithm C needs θ(w log n) = θ(q 1/2 log n) bits for each level of recursion and θ(log n) processing space for each level of recursion. Multiplying by q 1/2 , the number of levels of recursion, we get that the number of random bits use is θ(q log n) and the processing space is θ(q 1/2 log n). Comparing this to algorithm C, we see that the number of random bits has not decreased, but the processing space has increased by a factor of 1/2 . But notice that the random bits used in algorithm D consist of w strings h 1 , h 2 , . . . , h w each θ(w log n) bits, and each used at a different level of recursion. What we'd like to do is to choose one random h of θ(w log n) and use in place of all the h i . However, this introduces dependencies between various recursive levels, and invalidates the proof of correctness of the algorithm. It is possible that these dependencies are not consequential, but there is currently no way known to prove this.</p><p>The last step, which we do not describe in detail, involves introducing a small amount of additional randomness (at most θ(q 1/2 log n)) into the algorithm, in the form of small perturbations of the matrix entries at all levels of recursion. These perturbations can be shown to be too small to affect the quality of the answer, but are sufficient to, in effect, overcome the dependencies between recursive levels, and to make it possible to prove that if the same h is used at all recursive levels then the resulting algorithm still works. The number of random bits in the resulting algorithm is θ(q 1/2 log n) as required. Since this final algorithm satisfies properties (1),(2), and (3), it can be derandomized as described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">A description of Nisan's generator</head><p>The construction of the family of generators G b,k of 4.1 consists of two steps. The first step is to take X = {0, 1} b and define a family H b of maps from X to itself. Each map in H b is indexed by a 2b-bit string.</p><p>The second step is the definition of a transformation which, given a set X and a sequence f = (f 1 , f 2 , . . . , f k ) of functions from X to itself, yields a function G f mapping X to X 2 k .</p><p>Using these two parts we construct G b,h . We need to associate to each 2bk-bit sequence h a function G h mapping b bits to b2 k bits. Given h ∈ {0, 1} 2b , interpret h as indexing a sequence (h 1 , h 2 , . . . , h k ) of function from the family H b given in the first step; the function associated with h is then G h as defined in the second step.</p><p>Let us then describe these two steps. For the first step, we recall the well known fact that for every b ≥ 1, there is a field on 2 b elements and that there is a representation of the elements by b bit binary strings such that addition and multiplication can be carried out in O(b) space. Fix such a representation, and define U b to be the class of affine linear functions, i.e., functions of the form x -→ αx + β where α, β ∈ X. This family maps X to itself and each map is indexed by the 2b bit string (α, β).</p><p>for the second step. For convenience, let F (X) be the set of maps from X to itself. Let ⊕ denote the concatenation of strings. Given f = (f 1 , f 2 , . . . , f k ) where f i ∈ F (X), define the function G f1,...,fk recursively by G f1 (x) = x ⊕ f 1 (x) and for k ≥ 2, G f1,...,fk (x) = G f1,f2,...,fk-1 (x) ⊕ G f1,f2,...,fk-1 (f k (x))</p><p>For example, when k = 3, f = (f 1 , f 2 , f 3 ) we have</p><formula xml:id="formula_25">G f (x) = x ⊕ f 1 (x) ⊕ f 2 (x) ⊕ f 1 (f 2 (x)) ⊕ f 3 (x) ⊕ f 1 (f 3 (x)) ⊕ f 2 (f 3 (x)) ⊕ f 1 (f 2 (f 3 (x))).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The INW generator for distributed networks</head><p>Impagliazzo, Nisan and Wigderson <ref type="bibr" target="#b18">[19]</ref> initiated an investigation into generators that can be used for distributed computation. In this setting, processors, arranged in some network, are running a randomized protocol. The random bits they use come from a common source. Assume that each processor needs r random bits and that there are n processors altogether. The problem is to construct a generator that maps a small m bit seed to rn bits that can be distributed to the various processors, so that the resulting computation behaves as though the distributed bits were truly random. They construct such a generator whose seed length depends on two parameters: the width of the graph (which is a rough measure of the "communication bandwidth" the graph provides) and the max-imum number c of bits sent or received by any one processor.</p><p>As one of the applications of their generator they give an alternative construction of a generator that, for each S, maps a seed length of size O(S 2 ) bits to a string of 2 θ(S) bits that fools any space S computation. This generator, however, does not give a generator family, and therefore can not be substituted for Nisan's generator in the proofs of Theorems 3.4 and 3.6.</p><p>The connection between PRGs for space-bounded computation and for networks is in the same spirit, but is more straightforward than, the connection between space-bounded computation and multiparty communication. Given a HSBPTM execution M (x) with space S, we know that the computation takes at most 2 S steps. We can simulate the computation on a network consisting of 2 S nodes arranged in a line. The first processor begins by simulating first step of the computation and then sends the current configuration to the next processor who does the second step of the etc. The maximum communication per processor is θ(S).</p><p>In the special case corresponding to PRGs for space-bounded computation, the INW generator is most easily described in terms of three parameters p, m, i. The generator F i p,m takes an p + im bit seed and maps it to 2 i p bits. For any S, the generator i2 -m/2+S -fools all space S computations. Taking p = i = m = 4S yields a generator mapping θ(S 2 ) bits to 2 θ(S) bits that S2 -S fools all space S computations.</p><p>For i = 1, the generator F p,m = F 1 p,m mapping p+m to 2m bits defined in terms of two functions L p,m and R p,m , each mapping p+m bits to m bits. F 1 p,m (x) is defined to be the concatenation L p,m (x) ⊕ R p,m (x). To define L p,m and R p,m , let H p,m be an efficiently expander graph with 2 p vertices and degree 2 m . As in the description of the AKS sampler, we view each edge as a pair of oppositely directed edges, and assume that the edges out of each vertex are labeled 1 to 2 m . On input a string of p + m bits, interpret the string as a pair (v, i) where v is the vertex indexed by the first p bits and i an integer (between 1 and 2 m ) indexing one of the edges out of v. Define L p,m (v, i) = v and R p,m (v, i) = w where w is the other endpoint of the indicated edge.</p><p>For i ≥ 2, the generator F i p,m is defined recursively in a way that closely resembles the recursive definition of G f1,f2,...,fk in Nisan's generator. For i ≥ 2, F i p,m is supposed to map p + im bits to 2 i m bits. For v ∈ {0, 1} p+(i-1)m and i ∈ {0, 1} m , F i p,m (v, i) is defined to be</p><formula xml:id="formula_26">F i-1 p,m (L p+im,m (v, i)) ⊕ F i-1 p,m (R p+im,m (v, i)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Extractors and the Nisan-Zuckerman generator</head><p>Recall that the AKS construction of a PRS for space S, mapped θ(S) bits to θ(S 2 / log S) bits. Because the size of the seed was only θ(S), this led to a deterministic simulation of a non-trivial part of R H SP ACE(s) in DSP ACE(s).</p><p>If one examines the other generators described above, it can be seen that these generators only magnify a seed of size θ(S) by a constant factor. one only a non-trivial expansion of the random seed if the seed is asymptotically larger than S. This means that these generators do not provide a simulation of a nontrivial part of BP H SP ACE(s) in DSP ACE(s), at least not in any obvious way. To get such a simulation, we would need to build generators for space S that nontrivially magnify a θ(S) bit seed. Such generators were found by Nisan and Zuckerman. Their construction gave generators that mapped a θ(S) bit seed to a poly(S) bit string, for any fixed polynomial, fooling space S computations. As mentioned earlier, this construction together with the naive deterministic simulation, yields Theorem 3.5.</p><p>At the center of their construction is a combinatorial construction called an extractor. A thorough survey of extractors is given by Nisan <ref type="bibr" target="#b32">[33]</ref>; we content ourselves with a short discussion. Extractors first arose in the following context. Suppose we have a random source that outputs bits that are "faulty"; a k-bit string produced by the source may have some biased bits, or dependencies among its bits. We would like to find a mapping that maps the k-bit output of the source to an m-bit string for some m that is less, but not too much less than k. We want this mapping to have the property that when applied to a k-bit string from the faulty source, its output is uniformly (or nearly uniformly) distributed over all m-bit strings. We make the assumption that the faulty source comes from some distribution that is "not too concentrated". This assumption is formalized by requiring that the probability distribution of the source assigns probability at most 2 -δk to any k-bit string, where δ ∈ (0, 1) is some constant. Such a random source is called a δ-source. It is not hard to see that there is no single mapping that will work for all δ-sources. However, suppose that in addition to the k-bit faulty source we have access to a "short" string of t bits from a genuine source. Then it turns out that it is possible to find a function E that maps a pair (x, y) where x is a k-bit string from the faulty source and y is a perfectly random t bit string to an m-bit string E(x, y) (for some m reasonably close to k) such that the output is very close to the uniform distribution, assuming only that x comes from some δ-source. Here we measure the distance of the output from the uniform distribution by statistical distance: for two distributions D 1 and D 2 on the same set X, the distance between them is defined to be the maximum over Y</p><formula xml:id="formula_27">⊆ X of |D 1 (Y ) -D 2 (Y )|.</formula><p>Such a function E is called an extractor because it enables one to "extract" nearly uniform randomness from faulty randomness. Formally, a (k, t, m, δ, ǫ)extractor is a function E : {0, 1} k ×{0, 1} t -→ {0, 1} m such that if D k is any δ-source on k bits, then for x selected according to k and y a t-bit string chosen uniformly at random, the string y ⊕ E(x, y) (the concatenation of y and E(x, y)), is within statistical distance ǫ of the uniform distribution on t+m-bit strings.</p><p>The generator construct uses an extractor in the following way. Given a (k, t, m, δ, ǫ)-extractor E, and an integer r, we can define a generator G E,r mapping k + tr bits to tm bits as follows: interpret the input as a sequence (x, y 1 , y 2 , . . . , y r ) where x is a k-bit string and each y i is a t-bit string. The output of the generator is the concatenation E(x, y 1 ) ⊕ E(x, y 2 ) ⊕ . . . ⊕ E(x, y k ). The intuition behind why this generator fools small space computations is as follows. Suppose that the computation has used the j blocks of random bits, and we compute the next block by selecting y j+1 and computing E(x, y j+1 Then the current state of the computation encodes some information about the random string x, and conditioned on this information, x becomes a δ-source, for some δ. The string y j+1 provides "fresh" random bits, and so the output E(x, y j+1 ) of the extractor is very close to being a uniform string, even when conditioned on the current state of the computation.</p><p>To build a generator that fools space S computations, they choose some γ &gt; 0, k = θ(S), t = θ(S 1-γ ), and r = θ(S γ ). Then the input to the generator has θ(S) bits. Nisan and Zuckerman show that, for these parameters, there is an extractor for these values of k and t, δ = 1/2 and ǫ = 2 -θ(S) , whose output size is m = θ(S). Thus the output of the generator is θ(S 1+γ ) bits. It can further be shown that this is a 2 -θ(S) -PRG for space S.</p><p>Thus, we can magnify θ(S) random bits to θ(S 1+γ ) pseudorandom bits that fool space S machines. By composing generators of this form some constant number of times, we get a PRG that stretches θ(S) bits to poly(S) bits for any desired polynomial.</p><p>We make one final remark. The Nisan and INW generators for space S stretch θ(S 2 ) bits to 2 θ(S) bits, while the Nisan-Zuckerman generator stretches θ(S) bits to poly(S) bits. One might hope that one could compose these generators to get a generator mapping S bits to 2 S bits (first apply the NZ generator and then apply Nisan's generator to the output). This does not work for two reasons. First, the resulting generator is not computable in space S, because when the inner generator is evaluated on S bits, the S 2 bits it produces must be written down in order to evaluate the outer generator. The second problem, which really stems from the first, is that it is not clear that the resulting generator fools space S computations. It turns out that in general, for the composition of two generators to fool space S, one seems to need the condition that the outer generator is computable in space S, which is not the case here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">The Armoni-Wigderson generator</head><p>The Armoni-Wigderson generator <ref type="bibr" target="#b5">[6]</ref>, is a modification of the Nisan-Zuckerman generator, and provides a single construction for fooling space s computation that (1) stretches θ(S) bits to poly(S) bits (matching the Nisan Zuckerman generator), (2) stretches θ(S 2 ) bits to 2 θ(S) bits, (matching the Nisan and INW generators) and (3) for seed sizes in the intermediate range θ(S 1+γ ) for 0 &lt; γ &lt; 1, does slightly better than any of the other generators, producing S θ(S γ ) output bits as compared to the Nisan and INW generators which produces 2 θ(S γ ) bits.</p><p>generator is built recursively by composing the same extractor-based generators used by Nisan-Zuckerman.</p><p>The improvement comes from two sources: first, they use an improved extractor construction of Zuckerman <ref type="bibr" target="#b50">[51]</ref>. The more significant difference comes in how they compose the generators. In the Nisan-Zuckerman generator, the seed size grows exponentially in the number of nested levels of composition, while in this generator, the seed size only grows linearly in the number of nested levels of composition. The composition method used for this generator is related to that used in both the Nisan and INW generators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Other Directions</head><p>We conclude this paper with a brief discussion of some additional topics in space-bounded probabilistic computation not covered earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Read-once versus multi-access random bits</head><p>specifying the model of probabilistic spacebounded computation, we made the assumption that the random bits for the computation are on a separate tape that is read by a one-way head. This means that each bit, once accessed, can not be reread, unless it is written on the work tape. An alternative model is to have the random bits written on a two-way readable tape, so that each bit can be accessed arbitrarily often. Such a model was proposed in <ref type="bibr" target="#b6">[7]</ref> and studied further in <ref type="bibr" target="#b31">[32]</ref>. Following <ref type="bibr" target="#b31">[32]</ref>, for each probabilistic complexity class XSP ACE(s), we denote the corresponding class with multi-access random bits by X * SP ACE(s).</p><p>For non-halting probabilistic computation, multiaccess is extremely powerful; it can be shown, for instance, that R * SP ACE(s) contains N P . Many of the results that hold for read-once random bits are not clear in the multi-access case, for instance, in this case it is not clear that a computation that has an upper bound on the number of random bits used can be assumed halt absolutely.</p><p>There are two main results about these new classes in the literature. The first, which is obtained by plugging Nisan's generator into a simulation given in <ref type="bibr" target="#b6">[7]</ref>, says that BP H SP ACE(s) can be simulated in BP * s 2 SP ACE(s), i.e., by space s PTMs that use at most θ(s 2 ) multi-access random bits. The second <ref type="bibr" target="#b31">[32]</ref> says that BP H SP ACE(s) can be simulated in ZP * H SP ACE(s), i.e., that multi-access computation can be used to convert bounded-error machines to zero-error machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph Connectivity Universal Traversal sequences</head><p>The U ST CON problem is the most important combinatorial problem known to be in R H L but not known to be in DL, and there has been considerable effort to find space efficient deterministic algorithms for the problem. One major approach is based on of a universal traversal sequence(UTS). Universal traversal sequences can be viewed, roughly, as a type of pseudorandom generator that is specially designed to be used to derandomize the random walk algorithm for U ST CON . The goal is to explicitly construct such sequences that are as short as possible, preferably polynomial in n, the size of the graph on which the U ST CON problem is being solved. As noted in <ref type="bibr" target="#b6">[7]</ref>, a good PRG for log-space can be used to construct a short UTS, and, the shortest explicit UTS known, having size n θ(log n) , is built in this way from Nisan's generator.</p><p>A thorough overview of the complexity theoretic aspects of the graph connectivity problem, including universal traversal sequences, can be found in Wigderson's survey <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hitting sets and generators for combinatorial rectangles</head><p>Another special case of the problem of constructing a small space pseudorandom generator can be formulated as follows. Let m 2 q , d be integers and let X denote the set {0, 1} q , so |X| = m. A combinatorial rectangle in X d , or (m, d)-rectangle is a subset of X d of the form R = R 1 × R 2 × . . . × R d , where R i ⊆ X. The volume of R, vol(R) is the fraction of points of X d in R, i.e., |R|/m d . Given a generator G whose output length is qd can view the output of G as specifying a point in X d . A generator G is said to ǫ-fool combinatorial rectangles if for any such rectangle R, the probability that G(w) ∈ R is within ǫ of vol(R) and is said to ǫ-samples (m, d)-rectangles if for any such rectangle R, if vol(R) ≥ ǫ, then the probability that G(w) ∈ R is strictly positive. Note that this latter condition says that the set H which is the range of G(w) intersects or hits all (m, d)-rectangles of volume at least ǫ.</p><p>Versions of this problem were discussed in [30] and <ref type="bibr">[12]</ref>. The problem of constructing such generators can be viewed as a special case of the problem of constructing small space pseudorandom generators as follows. Suppose we have a space O(S) computation which is decomposed as the AND of a sequence of d independent subcomputations, where each subcomputation requires at most S bits. The entire computation requires Sd bits, and the set of accepting strings is a combinatorial rectangle whose volume is equal to the probability of acceptance.</p><p>Using this correspondence, it can be shown that the problem of building an efficient pseudorandom sampler for space bounded computation that maps θ(S) bits to 2 θ(S) includes as a special case the following as a subproblem: for each m, d, ǫ construct an efficient generator for that ǫ-samples (m, d)-rectangles whose seed length is at O(log m+log d+log(1/ǫ)). subproblem was solved in <ref type="bibr" target="#b25">[26]</ref>. The corresponding problem of building such a generator that ǫ-fools (m, d)-rectangles is open; the best known constructions are (1) a generator with seed length log m + O(log 2 d + log d log(1/ǫ)) which can be constructed using the INW generator, and (2) a construction in <ref type="bibr" target="#b4">[5]</ref>, which gives such a generator whose seed length is O(log m + log d + log 2 (1/ǫ)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Amplification and computing with weak random sources</head><p>Some of the most interesting recent work in timebounded computation has concerned two related problems, the problem of deterministic amplification, and the problem of computing with weak random sources.</p><p>As discussed earlier, given a probabilistic computation that runs with bounded error, it is possible, by repeated trials, to make the probability of error arbitrarily small. Suppose that the algorithm uses r random bits. If one performs k independent trials, us-ing rk random bits, the error probability is reduced to probability to 2 -θ(k) . The deterministic amplification problem is to find ways to reduce the error probability by a similar amount using fewer random bits.</p><p>The problem of computing weak random sources is as follows. Suppose we are running a BPP algorithm, but the source of bits we have is "faulty". We assume that the random source is a δ-source for some δ (see <ref type="bibr">Section 4.5)</ref>. Depending on the nature of the faultiness of the source, the algorithm may not work properly. Is it possible to convert the algorithm to one that still works in polynomial time, but is robust in the sense that it works properly with any δ-source?</p><p>These two questions been studied extensively in the context of poly-time computation and there are very strong results for both of them (see <ref type="bibr" target="#b32">[33]</ref> for a survey). One can ask the same questions in the context of space-bounded computation. Here there is essentially nothing non-trivial known. All of the methods known in the poly-time case (for either problem) involve generating a large set of random bits and using the same bits repeatedly (but in different ways) over many trials. Nothing like this seems possible in small space, because a large set of random bits can not be stored.</p><p>For space-bounded computation, these problems have added significance. It turns out that one can show, for instance, that if there was a general "black box" procedure for converting BP H SP ACE(s) algorithms so that they run with a δ-source, for some δ &lt; 1, then in fact BP H SP ACE(s) = DSP ACE(s).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Relationships among space-bounded complexity classes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 4 suggests that the following two open questions have affirmative answers. Question 3.1 Is N SP ACE(s) = BP SP ACE(s)? Question 3.2 Is BP H SP ACE(s) ⊆ N SP ACE(s)?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 4 . 1</head><label>41</label><figDesc>[30, 31] Let k, b be positive integers with b ≤ 2 k . There is a family G = G b,k = {G h |h ∈ {0, 1} 2bk satisfying for each b and k:</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Much of my understanding of area has developed as a result of working with my student and collaborator, Shiyu Zhou. Shiyu also helped in the preparation of this survey by, among other things, collecting the references and preparing the figures. I am very grateful to him for his help. I also benefited greatly from discussions with and comments from Eric Allender, Allan Borodin, Noam Nisan, Avi Wigderson, and David Zuckerman, and gratefully acknowledge their contributions to this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Two theorems on random polynomial time</title>
		<author>
			<persName><forename type="first">L</forename><surname>Adleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="75" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deterministic simulation of logspace</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ajtai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Komlós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Szemerédi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="132" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random walks, universal sequences and the complexity of maze problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Aleliunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lovasz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rackoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="218" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Relationships among pl, #l, and the determininant</title>
		<author>
			<persName><forename type="first">E</forename><surname>Allender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ogihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th Annual Structure in Complexity Theory Conference</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="267" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pseudorandom generators for combinatorial rectangles</title>
		<author>
			<persName><forename type="first">R</forename><surname>Armoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>Manuscript</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pseudorandomness for space bounded computations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Armoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiparty protocols and logspace-hard pseudorandom sequences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two applications of inductive counting for complementation problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ruzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="559" to="578" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parallel computation and well-endowed rings and spacebounded probabilistic machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borodin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pippenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="113" to="136" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiparty protocols</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Furst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A time-complexity gap for two-way probabilistic finite state automata</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stockmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Computing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1011" to="1023" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximations of general independent distributions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Even</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Velicković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th ACM Symposium Theory of Computing</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="10" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic two-way machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Freivalds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Mathematical Foundations of Computer Science</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the International Symposium on Mathematical Foundations of Computer Science</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">188</biblScope>
			<biblScope unit="page" from="33" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explicit construction of linear-sized superconcentrators</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gabber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Galil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="407" to="420" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computational complexity of probabilistic turing machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="675" to="695" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deterministic simulation of tape-bounded probabilistic turing machine transducers</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="333" to="338" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A lower bound for probabilistic algorithms for finite state machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="88" to="105" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nondeterministic space is closed under complementation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="935" to="938" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pseudorandomness for network algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Impagliazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="356" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Relationships between probabilistic and deterministic tape complexity. 10th Symon Mathematical Foundations of Computer Science</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On probabilistic tape complexity and fast circuits for matrix inversion problems. 11th International Colloquium on Automata, Languages and Programming</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="281" to="291" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On probabilistic time and space</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Colloquium on Automata, Languages and Programming</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="page" from="310" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Regularity of one-letter languages acceptable by 2-way finite probabilistic automata</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kaneps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fundamentals of Computation Theory</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>Fundamentals of Computation Theory</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Minimal nontrivial space complexity of probabilistic one-way turing machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kaneps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Freivalds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Mathematical Foundations of Computer Science</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the Conference on Mathematical Foundations of Computer Science</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">452</biblScope>
			<biblScope unit="page" from="355" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Symmetric spacebounded computation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Papadimitiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="161" to="187" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient construction of a small hitting set for combinatorial rectangles in high dimension</title>
		<author>
			<persName><forename type="first">N</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Luby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zuckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="258" to="267" />
		</imprint>
	</monogr>
	<note>Manuscript. A preliminary version of this paper appeared in the proceedings of the</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Explicit expanders and the ramanujan conjectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lubotzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sarnak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="240" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Randomized Algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Using hard problems to create pseudorandom generators</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>An ACM Distinguished Dissertation</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pseudorandom generators for space-bounded computation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="449" to="461" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RL ⊆ SC</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="619" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On read-once vs. multiple access to randomness in logspace</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="135" to="144" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Extracting randomness: How and why</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Annual Conference on Computational Complexity</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Undirected connectivity in o(log 1.5 n) space</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Szemerédi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Symmetric logspace is closed under complement</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ta-Shma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="140" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hardness vs. randomness</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="2" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">More deterministic simulation in logspace</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zuckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th ACM Symposium Theory of Computing</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
	<note>Revised version Randomness is linear in space to appear in JCSS</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Computational Complexity</title>
		<author>
			<persName><forename type="first">C</forename><surname>Papadimitriou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Addison Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Probabilistic automata</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="230" to="245" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Algorithms and Complexity: Recent Results and New Directions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rabin</surname></persName>
		</author>
		<editor>J.F.Traub</editor>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="21" to="39" />
		</imprint>
	</monogr>
	<note>Probabilistic algorithms</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spacebounded hierarchies and probabilistic computation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ruzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="216" to="230" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">ACE(s) ⊆</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Ace</forename><surname>Dsp</surname></persName>
		</author>
		<title level="m">36th IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Relationships between nondeterministic and deterministic space complexities</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Savitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="192" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">J. probabilistic turing machine complexity classes are closed under complement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="1981">1981. 1981</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="158" to="167" />
		</imprint>
	</monogr>
	<note>On tape-bounded probabilistic turing machine acceptors</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A complexity theoretic approach to randomness</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="1983">1983</date>
			<biblScope unit="page" from="330" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A fast monte-carlo test for primality</title>
		<author>
			<persName><forename type="first">R</forename><surname>Solovay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Strassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="85" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The method of forced enumeration for nondeterministic automata</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szelepcsényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Informatica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="279" to="284" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The complexity of graph connectivity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, 17th Symposium</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>17th Symposium</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">629</biblScope>
			<biblScope unit="page" from="112" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Theory and applications of trapdoor functions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="80" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Randomness-optimal sampling, extractors and constructive leader election</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zuckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th ACM Symposium on Theory of Computing</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
