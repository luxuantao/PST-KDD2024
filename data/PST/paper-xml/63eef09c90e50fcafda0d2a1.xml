<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LIGHTGCL: SIMPLE YET EFFECTIVE GRAPH CON-TRASTIVE LEARNING FOR RECOMMENDATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-16">16 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xuheng</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Xubin</forename><surname>Ren</surname></persName>
							<email>xubinrencs@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LIGHTGCL: SIMPLE YET EFFECTIVE GRAPH CON-TRASTIVE LEARNING FOR RECOMMENDATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-16">16 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2302.08191v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural network (GNN) is a powerful learning approach for graph-based recommender systems. Recently, GNNs integrated with contrastive learning have shown superior performance in recommendation with their data augmentation schemes, aiming at dealing with highly sparse data. Despite their success, most existing graph contrastive learning methods either perform stochastic augmentation (e.g., node/edge perturbation) on the user-item interaction graph, or rely on the heuristic-based augmentation techniques (e.g., user clustering) for generating contrastive views. We argue that these methods cannot well preserve the intrinsic semantic structures and are easily biased by the noise perturbation. In this paper, we propose a simple yet effective graph contrastive learning paradigm LightGCL that mitigates these issues impairing the generality and robustness of CL-based recommenders. Our model exclusively utilizes singular value decomposition for contrastive augmentation, which enables the unconstrained structural refinement with global collaborative relation modeling. Experiments conducted on several benchmark datasets demonstrate the significant improvement in performance of our model over the state-of-the-arts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) have shown effectiveness in graph-based recommender systems by extracting local collaborative signals via neighborhood representation aggregation <ref type="bibr" target="#b22">(Wang et al., 2019;</ref><ref type="bibr">Chen et al., 2020b)</ref>. In general, to learn user and item representations, GNN-based recommenders perform embedding propagation on the user-item interaction graph by stacking multiple message passing layers for exploring high-order connectivity <ref type="bibr" target="#b6">(He et al., 2020;</ref><ref type="bibr" target="#b35">Zhang et al., 2019;</ref><ref type="bibr">Liu et al., 2021a)</ref>. Most GNN-based collaborative filtering models adhere to the supervised learning paradigm, requiring sufficient quality labelled data for model training. However, many practical recommendation scenarios struggle with the data sparsity issue in learning high-quality user and item representations from limited interaction data <ref type="bibr">(Liu et al., 2021b;</ref><ref type="bibr" target="#b10">Lin et al., 2021)</ref>. To address the label scarcity issue, the benefits of contrastive learning have been brought into the recommendation for data augmentation <ref type="bibr" target="#b24">(Wu et al., 2021)</ref>. The main idea of contrastive learning in enhancing the user and item representation is to research the agreement between the generated embedding views by contrasting the defined positive pairs with negative instance counterparts <ref type="bibr" target="#b29">(Xie et al., 2022)</ref>.</p><p>While contrastive learning has been shown to be effective in improving the performance of graphbased recommendation methods, the view generators serve as the core part of data augmentation through identifying accurate contrasting samples. Most of current graph contrastive learning (GCL) approaches employ heuristic-based contrastive view generators to maximize the mutual information between the input positive pairs and push apart negative instances <ref type="bibr" target="#b24">(Wu et al., 2021;</ref><ref type="bibr">Yu et al., 2022a;</ref><ref type="bibr" target="#b27">Xia et al., 2022b)</ref>. To construct perturbed views, SGL <ref type="bibr" target="#b24">(Wu et al., 2021)</ref> has been proposed to generate node pairs of positive view by corrupting the structural information of user-item interaction graph using stochastic augmentation strategies, e.g., node dropping and edge perturbation. To improve the graph contrastive learning in recommendation, SimGCL <ref type="bibr">(Yu et al., 2022a)</ref> offers embedding augmentation with random noise perturbation. To work on identifying semantic neighbors of nodes (users and items), HCCF <ref type="bibr" target="#b27">(Xia et al., 2022b)</ref> and NCL <ref type="bibr" target="#b11">(Lin et al., 2022)</ref> are introduced to pursue consistent representations between the structurally adjacent nodes and semantic neighbors. Despite their effectiveness, state-of-the-art contrastive recommender systems suffer from several inherent limitations: i) Graph augmentation with random perturbation may lose useful structural information, which misleads the representation learning. ii) The success of heuristic-guided representation contrasting schemes is largely built upon the view generator, which limits the model generality and is vulnerable to the noisy user behaviors. iii) Most of current GNN-based contrastive recommenders are limited by the over-smoothing issue which leads to indistinguishable representations.</p><p>In light of the above limitations and challenges, we revisit the graph contrastive learning paradigm for recommendation with a proposed simple yet effective augmentation method LightGCL. In our model, the graph augmentation is guided by singular value decomposition (SVD) to not only distill the useful information of user-item interactions but also inject the global collaborative context into the representation alignment of contrastive learning. Instead of generating two handcrafted augmented views, important semantic of user-item interactions can be well preserved with our robust graph contrastive learning paradigm. This enables our self-augmented representations to be reflective of both user-specific preferences and cross-user global dependencies.</p><p>Our contributions are highlighted as follows:</p><p>? In this paper, we enhance the recommender systems by designing a lightweight and robust graph contrastive learning framework to address the identified key challenges pertaining to this task.</p><p>? We propose an effective and efficient contrastive learning paradigm LightGCL for graph augmentation. With the injection of global collaborative relations, our model can mitigate the issues brought by inaccurate contrastive signals.</p><p>? Our method exhibits improved training efficiency compared to existing GCL-based approaches.</p><p>? Extensive experiments on several real-world datasets justify the performance superiority of our LightGCL. In-depth analyzes demonstrate the rationality and robustness of LightGCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Graph Contrastive Learning for Recommendation. A promising line of recent studies has incorporated contrastive learning (CL) into graph-based recommenders, to address the label sparsity issue with self-supervision signals. Particularly, SGL <ref type="bibr" target="#b24">(Wu et al., 2021)</ref> and SimGCL <ref type="bibr">(Yu et al., 2022a)</ref> perform data augmentation over graph structure and embeddings with random dropout operations. However, such stochastic augmentation may drop important information, which may make the sparsity issue of inactive users even worse. Furthermore, some recent alternative CL-based recommenders, such as HCCF <ref type="bibr" target="#b27">(Xia et al., 2022b)</ref> and NCL <ref type="bibr" target="#b11">(Lin et al., 2022)</ref>, design heuristic-based strategies to construct view for embedding contrasting. Despite their effectiveness, their success heavily relies on their incorporated heuristics (e.g., the number of hyperedges or user clusters) for contrastive view generation, which can hardly be adaptive to different recommendation tasks.</p><p>Self-Supervised Learning on Graphs. Recently, self-supervised learning (SSL) has advanced the graph learning paradigm by enhancing node representation from unlabeled graph data <ref type="bibr">(Zhu et al., 2021a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b20">Velickovic et al., 2019;</ref><ref type="bibr" target="#b4">Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr" target="#b15">Peng et al., 2020;</ref><ref type="bibr" target="#b37">Zhu et al., 2020;</ref><ref type="bibr" target="#b25">Wu et al., 2022)</ref>. For example, to improve the predictive SSL paradigm, AutoSSL <ref type="bibr" target="#b7">(Jin et al., 2022)</ref> automatically combines multiple pretext tasks for augmentation. Towards the line of contrastive SSL over graph structures, recent efforts focus on designing various graph contrastive learning methods <ref type="bibr">(Yu et al., 2022b;</ref><ref type="bibr" target="#b30">Yin et al., 2022;</ref><ref type="bibr">Zhang et al., 2022;</ref><ref type="bibr">Xia et al., 2022a;</ref><ref type="bibr" target="#b18">Suresh et al., 2021)</ref>. For instance, SimGRACE <ref type="bibr">Xia et al. (2022a)</ref> proposes to generate contrastive views with the GNN encoder perturbations. In AutoGCL <ref type="bibr" target="#b30">Yin et al. (2022)</ref>, graph view generators are designed to be jointly trained with the graph encoder in an end-to-end way. Additionally, GCA <ref type="bibr">(Zhu et al., 2021b)</ref> performs both topology-level and attribute-level data augmentation for contrastive view generation. In this method, important edges and features will be identified for adaptive augmentation. GraphCL <ref type="bibr" target="#b31">(You et al., 2020)</ref> generates correlated graph representation views using various augmentation strategies, such as node/edge perturbation and attribute masking. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LOCAL GRAPH DEPENDENCY MODELING</head><p>As a common practice of collaborative filtering, we assign each user u i and item v j with an embedding vector e (u)</p><p>i , e</p><p>(v) j ? R d , where d is the embedding size. The collections of all user and item embeddings are defined as E (u) ? R I?d and E (v) ? R J?d , where I and J are the number of users and items, respectively. Following <ref type="bibr" target="#b27">Xia et al. (2022b)</ref>, we adopt a two-layer GCN to aggregate the neighboring information for each node. In layer l, the aggregation process is expressed as follows:</p><formula xml:id="formula_0">z (u) i,l = ?(p( ?i,: ) ? E (v) l-1 ), z (v) j,l = ?(p( ?:,j ) ? E (u) l-1 )<label>(1) where z (u)</label></formula><p>i,l and z (v) j,l denote the l-th layer aggregated embedding for user u i and item v j . ?(?) represents the LeakyReLU with a negative slope of 0.5. ? is the normalized adjacency matrix, on which we perform the edge dropout denoted as p(?), to mitigate the overfitting issue. We implement the residual connections in each layer to retain the original information of the nodes as follows:</p><formula xml:id="formula_1">e (u) i,l = z (u) i,l + e (u) i,l-1 , e (v) j,l = z (v) j,l + e (v) j,l-1 (2)</formula><p>The final embedding for a node is the sum of its embeddings across all layers, and the inner product between the final embedding of a user u i and an item v j predicts u i 's preference towards v j :</p><formula xml:id="formula_2">e (u) i = L l=0 e (u) i,l , e (v) j = L l=0 e (v) j,l , ?i,j = e (u) i e (v) j (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">EFFICIENT GLOBAL COLLABORATIVE RELATION LEARNING</head><p>To empower graph contrastive learning for recommendation with global structure learning, we equip our LightGCL with the SVD scheme <ref type="bibr" target="#b16">(Rajwade et al., 2012;</ref><ref type="bibr" target="#b17">Rangarajan, 2001)</ref> to efficiently distill important collaborative signals from the global perspective. Specifically, we first perform SVD on the adjacency matrix A as A = U SV . Here, U / V is an I ? I / J ? J orthonormal matrix with columns being the eigenvectors of A's row-row / column-column correlation matrix. S is an I ? J diagonal matrix storing the singular values of A. The largest singular values are usually associated with the principal components of the matrix. Thus, we truncate the list of singular values to keep the largest q values, and reconstruct the adjacency matrix with the truncated matrices as ? = U q S q V q , where U q ? R I?q and V q ? R J?q contain the first q columns of U and V respectively. S q ? R q?q is the diagonal matrix of the q largest singular values.</p><p>The reconstructed matrix ? is a low-rank approximation of the adjacency matrix A, for it holds that rank( ?) = q. The advantages of SVD-based graph structure learning are two-folds. Firstly, it emphasizes the principal components of the graph by identifying the user-item interactions that are important and reliable to user preference representations. Secondly, the generated new graph structures preserve the global collaborative signals by considering each user-item pair. Given the ?, we perform message propagation on the reconstructed user-item relation graph in each layer:</p><formula xml:id="formula_3">g (u) i,l = ?( ?i,: ? E (v) l-1 ), g (v) j,l = ?( ?:,j ? E (u) l-1 )<label>(4)</label></formula><p>However, performing the exact SVD on large matrices is highly expensive, making it impractical for handling large-scale user-item matrix. Therefore, we adopt the randomized SVD algorithm proposed by <ref type="bibr" target="#b3">Halko et al. (2011)</ref>, whose key idea is to first approximate the range of the input matrix with a low-rank orthonormal matrix, and then perform SVD on this smaller matrix.</p><formula xml:id="formula_4">?q , ?q , V q = ApproxSVD(A, q), ?SV D = ?q ?q V q (5)</formula><p>where q is the required rank for the decomposed matrices, and ?q ? R I?q , ?q ? R q?q , Vq ? R J?q are the approximated versions of U q , S q , V q . Thus, we rewrite the message propagation rules in Eq. 4 with the approximated matrices and the collective representations of the embeddings as follows:</p><formula xml:id="formula_5">G (u) l = ?( ?SV D E (v) l-1 ) = ?( ?q ?q V q E (v) l-1 ); G (v) l = ?( ? SV D E (u) l-1 ) = ?( Vq ?q ? q E (u) l-1 )<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">G (u) l and G (v) l</formula><p>are the collections of user and item embeddings encoded from the new generated graph structure view. Note that we do not need to compute and store the large dense matrix ?SV D . Instead, we can store ?q , ?q and Vq , which are of low dimensions. By pre-calculating ( ?q ?q ) and ( Vq ?q ) during the preprocessing stage with SVD, the model efficiency is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SIMPLIFIED LOCAL-GLOBAL CONTRASTIVE LEARNING</head><p>The conventional GCL methods such as SGL and SimGCL contrast node embeddings by constructing two extra views, while the embeddings generated from the original graph (the main-view) are not directly involved in the InfoNCE loss. The reason for adopting such a cumbersome three-view paradigm may be that the random perturbation used to augment the graph may provide misleading signals to the main-view embeddings. In our proposed method, however, the augmented graph view is created with global collaborative relations, which can enhance the main-view representations. Therefore, we simplify the CL framework by directly contrasting the SVD-augmented view embeddings g (u)</p><p>i,l with the main-view embeddings z (u) i,l in the InfoNCE loss <ref type="bibr" target="#b14">(Oord et al., 2018)</ref>:</p><formula xml:id="formula_7">L (u) s = I i=0 L l=0 -log exp(s(z (u) i,l , g<label>(u)</label></formula><p>i,l /? ))</p><formula xml:id="formula_8">I i =0 exp(s(z (u) i,l , g (u) i ,l )/? )<label>(7)</label></formula><p>where s(?) and ? stand for the cosine similarity and the temperature respectively. The InfoNCE loss</p><formula xml:id="formula_9">L (v)</formula><p>s for the items are defined in the same way. To prevent overfitting, we implement a random node dropout in each batch to exclude some nodes from participating in the contrastive learning. As shown in Eq. 8, the contrastive loss is jointly optimized with our main objective function for the recommendation task (where ?i,ps and ?i,ns denote the predicted scores for a pair of positive and negative items of user i):</p><formula xml:id="formula_10">L = L r + ? 1 ? (L (u) s + L (v) s ) + ? 2 ? ? 2 2 ; L r = I i=0 S s=1 max(0, 1 -?i,ps + ?i,ns ) (8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>To verify the superiority and effectiveness of the proposed LightGCL method, we perform extensive experiments to answer the following research questions:</p><p>? RQ1: How does LightGCL perform on different datasets compared to various SOTA baselines?</p><p>? RQ2: How does the lightweight graph contrastive learning improve the model efficiency?</p><p>? RQ3: How does our model perform against data sparsity, popularity bias and over-smoothing? In accordance with <ref type="bibr" target="#b6">He et al. (2020)</ref> and <ref type="bibr" target="#b24">Wu et al. (2021)</ref>, we split the datasets into training, validation and testing sets with a ratio of 7:2:1. We adopt the Recall@N and Normalized Discounted Cumulative Gain (NDCG)@N, where N = {20, 40}, as the evaluation metrics. ? Self-Supervised Learning Recommender Systems: GraphCL <ref type="bibr" target="#b31">(You et al., 2020)</ref>, GRACE <ref type="bibr" target="#b37">(Zhu et al., 2020)</ref>, GCA <ref type="bibr">(Zhu et al., 2021b)</ref>, MHCN <ref type="bibr" target="#b32">(Yu et al., 2021)</ref>, SAIL <ref type="bibr">(Yu et al., 2022b)</ref>, Au-toGCL <ref type="bibr" target="#b30">(Yin et al., 2022)</ref>, SimGRACE <ref type="bibr">(Xia et al., 2022a)</ref>, SGL <ref type="bibr" target="#b24">(Wu et al., 2021)</ref>, HCCF <ref type="bibr" target="#b27">(Xia et al., 2022b)</ref>, SHT <ref type="bibr">(Xia et al., 2022c)</ref>, SimGCL <ref type="bibr">(Yu et al., 2022a)</ref>.</p><p>Due to space limit, the detailed descriptions of baselines are presented in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">HYPERPARAMETER SETTINGS</head><p>To ensure a fair comparison, we tune the hyperparameters of all the baselines within the ranges suggested in the original papers, except the following fixed settings for all the models: the embedding size is set as 32; the batch size is 256; two convolutional layers are used for GCN models.</p><p>For our LightGCL, the regularization weights ? 1 and ? 2 are tuned from {1e-5, 1e-6, 1e-7} and {1e-4, 1e-5}, respectively. The temperature ? is searched from {0.3, 0.5, 1, 3 ,10}. The dropout rate is chosen from {0, 0.25}. The rank (i.e., q) for SVD, is set as 5. We use the Adam optimizer with a learning rate of 0.001 decaying at the rate of 0.98 until the rate reaches 0.0005.<ref type="foot" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PERFORMANCE VALIDATION (RQ1)</head><p>We summarize the experimental result in Table <ref type="table" target="#tab_1">1</ref> <ref type="foot" target="#foot_1">2</ref> , with the following observations and conclusions: ? Contrastive Learning Dominates. As can be seen from the table, recent methods implementing contrastive learning (SGL, HCCF, SimGCL) exhibit consistent superiority as compared to traditional graph-based (GCCF, LightGCN) or hypergraph-based (HyRec) models. They also perform better than some of other self-supervised learning approaches (MHCN). This could be attributed to the effectiveness of CL to learn evenly distributed embeddings <ref type="bibr">(Yu et al., 2022a</ref>). ? Contrastive Learning Enhancement. Our method consistently outperforms all the contrastive learning baselines. We attribute such performance improvement to the effective augmentation of graph contrastive learning via injecting global collaborative contextual signals. Other compared contrastive learning-based recommenders (e.g., SGL, SimGCL, and HCCF) are easily biased by noisy interaction information and generate misleading self-supervised signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EFFICIENCY STUDY (RQ2)</head><p>GCL models often suffer from a high computational cost due to the construction of extra views and the convolution operations performed on them during training. However, the low-rank nature of the SVD-reconstructed graph and the simplified CL structure enable the training of our LightGCL to be highly efficient. We analyze the pre-processing and per-batch training complexity of our model in comparison to three competitive baselines, as summarized in Table <ref type="table" target="#tab_2">2</ref>.<ref type="foot" target="#foot_2">3</ref>  </p><formula xml:id="formula_11">O(E) O(E) O(E) O(E) SVD - - - O(qE) Training Augmentation - O(2?E) - - Graph Convolution O(2ELd) O(2ELd + 4?ELd) O(6ELd) O[2ELd + 2q(I + J)Ld] BPR Loss O(2Bd) O(2Bd) O(2Bd) O(2Bd) InfoNCE Loss - O(Bd + BM d) O(Bd + BM d) O[(Bd + BM d)L]</formula><p>? Although our model requires performing the SVD in the pre-processing stage which takes O(qE), the computational cost is negligible compared to the training stage since it only needs to be performed once. In fact, by moving the construction of contrastive view to the pre-processing stage, we avoid the repetitive graph augmentation during training, which improves model efficiency. To evaluate the robustness of our model in alleviating data sparsity, we group the sparse users by their interaction degrees and calculate the Recall@20 of each group on Yelp and Gowalla datasets.</p><p>As can be seen from the figures, the performance of HCCF and SimGCL varies across datasets, but our LightGCL consistently outperforms them in all cases. In particular, our model performs notably well on the extremely sparse user group (&lt; 15 interactions), as the Recall@20 of these users is not much lower (and is even higher on Gowalla) than that of the whole dataset.  Additionally, we illustrate our model's ability to mitigate popularity bias compared to HCCF and SimGCL. Similar to Section 4.4, we group the long-tail items by their degree of interactions. Following <ref type="bibr" target="#b24">Wu et al. (2021)</ref>, we adopt the decomposed Recall@20 defined as</p><formula xml:id="formula_12">Recall (g) = |(V u rec ) (g) ?V u test | |V u test |</formula><p>where V u test refers to the set of test items for the user u, and (V u rec ) (g) is the set of Top-K recommended items for u that belong to group g. The results are shown in Fig. <ref type="figure" target="#fig_3">3</ref>. Similar to the results on sparse users, HCCF and SimGCL's performance fluctuates a lot with the influence of popularity bias. Our model performs better in most cases, which shows its resistance against popularity bias. Note that since the extremely sparse group (&lt; 15 interactions) is significantly larger than the other groups in Gowalla, they contribute to a large fraction of the Recall@20, resulting in a different trend from that of Yelp in the figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">BALANCING BETWEEN OVER-SMOOTHING AND OVER-UNIFORMITY (RQ3)</head><p>In this section, we illustrate the effectiveness of our model in learning a moderately dispersed embedding distribution, by preserving user unique preference pattern and inter-user collaborative dependencies. We randomly sample 2,000 nodes from Yelp and Gowalla and map their embeddings to the 2-D space with t-SNE (Van der <ref type="bibr" target="#b19">Maaten &amp; Hinton, 2008)</ref>. The visualizations of these embeddings are presented in Fig. <ref type="figure" target="#fig_4">4</ref>. We also calculate the Mean Average Distance (MAD) <ref type="bibr">(Chen et al., 2020a)</ref> of the embeddings, summarized in Table <ref type="table" target="#tab_4">3</ref>.  As can be seen from Fig. <ref type="figure" target="#fig_4">4</ref>, the embedding distributions of non-CL methods (i.e., LightGCN, MHCN) exhibit indistinguishable clusters in the embedding space, which indicates the limitation of addressing the over-smoothing issue. On the contrary, the existing CL-based methods tend to learn i) over-uniform distributions, e.g., SGL on Yelp learns a huge cloud of evenly-distanced embeddings with no clear community structure to well capture the collaborative relations between users; ii) highly dispersed small clusters with severe over-smoothing issue inside the clusters, e.g., the embeddings of SimGCL on Gowalla appear to be scattered grained clusters inside which embeddings are highly similar. Compared with them, clear community structures could be identified by our method to capture collaborative effects, while the embeddings inside each community are reasonably dispersed to be reflective of user-specific preference. The MAD of our model's learned features is also in between of the two types of baselines as shown in Table <ref type="table" target="#tab_4">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">ABLATION STUDY (RQ4)</head><p>To investigate the effectiveness of our SVD-based graph augmentation scheme, we perform the ablation study to answer the question of whether we could provide guidance to the contrastive learning with a different approach of matrix decomposition. To this end, we implement two variants of our model, replacing the approximated SVD algorithm with other matrix decomposition methods: CL-MF adopts the view generated by a pre-trained MF <ref type="bibr" target="#b9">(Koren et al., 2009)</ref>; CL-SVD++ utilizes the SVD++ <ref type="bibr" target="#b8">(Koren, 2008)</ref> which takes implicit user feedback into consideration. As shown in Table <ref type="table" target="#tab_5">4</ref>, with the information distilled from MF or SVD++, the model is able to achieve satisfactory results, indicating the effectiveness of using matrix decomposition to empower CL and the flexibility of our proposed framework. However, adopting a pre-trained CL component is not only tedious and timeconsuming but also inferior to utilizing the approximate SVD algorithm in terms of performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">HYPERPARAMETER ANALYSIS (RQ5)</head><p>In this section, we investigate our model's sensitivity in relation to several key hyperparameters: the regularization weight for InfoNCE loss ? 1 , the temperature ? , and the required rank of SVD q.</p><p>? The impact of ? 1 . As illustrated in Fig. <ref type="figure" target="#fig_6">6</ref>, for the three datasets Yelp, Gowalla and ML-10M, the model's performance reaches the peak when ? 1 = 10 -7 . It can be noticed that ? 1 with the range of [10 -6 , 10 -8 ] can often lead to performance improvement. Figure <ref type="figure">7</ref>: Impact of ?</p><p>? The impact of ? . Fig. <ref type="figure">7</ref> indicates that the model's performance is relatively stable across different selections of ? from 0.1 to 10, while the best configuration of ? value varies by datasets. ? The selection of q. q determines the rank of SVD in our model. Experiments have shown that satisfactory results can be achieved with a small q. Specifically, as in Fig. <ref type="figure" target="#fig_5">5</ref>, we observe that q = 5 is sufficient to preserve important structures of the user-item interaction graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">CASE STUDY (RQ4)</head><p>In this section, we present a case study to intuitively show the effectiveness of our model to identify useful knowledge from noisy user-item interactions and make accurate recommendations accordingly. In Fig. <ref type="figure" target="#fig_7">8</ref>, we can see that the venues visited by user #26 in Yelp mainly fall into two communities: Cleveland (where the user probably lives) and Arizona (where the user may have travelled to). In the reconstructed graph, these venues are assigned a new weight according to their potential importance. Note that item #2583, a car rental agency in Arizona, has been assigned a negative weight, which conforms to our common sense that people generally would not visit multiple car rental agencies in one trip. The SVD-augmented view also provides predictions on invisible links by assigning a large weight<ref type="foot" target="#foot_3">4</ref> to potential venues of interest, such as #2647 and #658. Note that when exploiting the graph, the augmented view does not overlook the smaller Arizona community, which enables the model to predict items of minor interests that are usually overshadowed by the majority. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a simple and effective augmentation method to the graph contrastive learning framework for recommendation. Specifically, we explore the key idea of making the singular value decomposition powerful enough to augment user-item interaction graph structures. Our key findings indicate that our graph augmentation scheme exhibits strong ability in resisting data sparsity and popularity bias. Extensive experiments show that our model achieves new state-of-the-art results on several public evaluation datasets. In future work, we plan to explore the potential of incorporating casual analysis into our lightweight graph contrastive learning model to enhance the recommender system with mitigating confounding effects for data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PERFORMANCE COMPARISON WITH BASELINES (CONTINUED)</head><p>In this appendix, we show the performance of NCF, GCCF, GraphCL, SAIL, GRACE, and Auto-GCL, which are not shown in Table <ref type="table" target="#tab_1">1</ref> due to space limit. The results are summarized in Table <ref type="table" target="#tab_6">5</ref>. As can be seen from the table, our model outperforms these baselines consistently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C THEORETICAL ANALYSIS</head><p>We conduct theoretical analyses to show that our local-global CL (Eq. 7) is augmented to maximize the similarity between embeddings of potentially related nodes, based on the SVD-based global relation learning. Specifically, for a node v j ? U, where U = {u i |A i,i = 0, ?i,i = 0}, the embeddings are not updated by s(z i,l , g i,l ) in the vanilla InfoNCE loss, as v j is not adjacent to u i . Instead, our local-global contrastive assigns the following gradients to the embeddings of v j :</p><p>?s(z i,l , g i,l )/?g i,l-1 = ?s ? ? z i,l , ?( j?U ? i,j g j,l-1 + A i,j =0 ? i,j g j ,l-1 ) ? ? /?g j,l-1 = z i,l z i,l g i,l</p><p>? ? (?)</p><formula xml:id="formula_13">? ? i,j<label>(9)</label></formula><p>where ? i,j denotes the normalization weight for node u i and v j . In this way, the embeddings of nodes in U are also pulled close to s i,l , which injects relatedness information learned by the SVD into the local-global CL optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D CALCULATION OF COMPLEXITY D.1 ADJACENCY MATRIX NORMALIZATION</head><p>For a sparse user-item matrix stored in the Coordinate Format (COO), it requires visiting every nonzero elements in the matrix to perform normalization. Thus, the computational complexity is in the order of the number of edges O(E). Note that for the baseline SGL, it requires normalizing the two augmented graph structures during the training phase, each of which contains ?E edges, so it induces a complexity of O(2?E) per batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 APPROXIMATE SVD ALGORITHM</head><p>We refer the readers to <ref type="bibr" target="#b3">Halko et al. (2011)</ref> in which the complexity of the approximate SVD algorithm is explained in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 GRAPH CONVOLUTION</head><p>Given a sparse COO matrix A with E edges and a dense matrix E with dimensions I(J) ? d, it takes O(Ed) time to calculate AE. To perform graph convolution on a graph, we need to multiply the sparse adjacency matrix with E (v)</p><p>l-1 ? R J?d and its transpose with E (u)</p><p>l-1 ? R I?d , which takes O(Ed) each, and O(2Ed) in total. For L layers, O(2ELd) is required. For traditional CL-based methods such as SGL and SimCGL, a three-view structure is adopted, resulting in a complexity of O(12ELd) (for SGL it again varies a bit depending on ?).</p><p>For the SVD-view of our model, V q E (v)</p><p>l-1 takes O(qJd), and multiplying the result with the precalculated ( ?q ?q ) takes O(qId); ? q E (v)</p><p>l-1 takes O(qId), and multiplying the result with the precalculated ( Vq ?q ) takes O(qJd). So in total it takes O(2q(I + J)d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 BPR LOSS</head><p>In each batch with B users, calculating the scores for positive and negative items both take O(Bd), so in total it takes O(2Bd).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 CL LOSS</head><p>In each batch with B users, calculating the numerator of InfoNCE loss takes O(Bd), and calculating the denominator takes O(BM d) where M denotes the total number of nodes in the batch. Since our model adopts a per layer InfoNCE loss, a factor of L is appended.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall structure of LightGCL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>model against 16 state-of-the-art baselines with different learning paradigms: ? MLP-enhanced Collaborative Filtering: NCF (He et al., 2017). ? GNN-based Collaborative Filtering: GCCF (Chen et al., 2020c), LightGCN (He et al., 2020). ? Disentangled Graph Collaborative Filtering: DGCF (Wang et al., 2020b). ? Hypergraph-based Collaborative Filtering: HyRec (Wang et al., 2020a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance on users of different sparsity degrees, in terms of Recall (histograms) and relative Recall w.r.t overall performances (charts).</figDesc><graphic url="image-15.png" coords="7,113.75,252.55,198.00,137.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: LightGCL's ability to alleviate popularity bias in comparison to SOTA CLbased methods HCCF and SimGCL.</figDesc><graphic url="image-16.png" coords="7,320.05,266.83,178.20,108.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Embedding distributions on Yelp and Gowalla visualized with t-SNE.</figDesc><graphic url="image-17.png" coords="8,108.00,81.86,396.01,135.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Recall change w.r.t. q.</figDesc><graphic url="image-18.png" coords="8,127.51,518.21,126.72,88.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Impact of ? 1 .Figure7: Impact of ?</figDesc><graphic url="image-20.png" coords="9,312.23,81.86,178.20,80.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Case study on user #26 in Yelp dataset.</figDesc><graphic url="image-21.png" coords="9,127.80,428.10,356.40,128.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>We evaluate our model and the baselines on five real-world datasets: Yelp (29,601 users, 24,734 items, 1,517,326 interactions): a dataset collected from the rating interactions on Yelp platform; Gowalla (50,821 users, 57,440 items, 1,172,425 interactions): a dataset containing users' check-in records collected from Gowalla platform; ML-10M (69,878 users, 10,195 items, 9,988,816 interactions): a well-known movie-rating dataset for collaborative filtering; Amazon-book (78,578 users, 77,801 items, 2,240,156 interactions): a dataset composed of users' ratings on books collected fromAmazon; and Tmall (47,939 users, 41,390 items, 2,357,450 interactions): a E-commerce dataset containing users' purchase records on different products in Tmall platform.</figDesc><table><row><cell>? RQ4: How does the local-global contrastive learning contribute to the performance of our model?</cell></row><row><cell>? RQ5: How do different parameter settings affect our model performance?</cell></row><row><cell>4.1 EXPERIMENTAL SETTINGS</cell></row><row><cell>4.1.1 DATASETS AND EVALUATION PROTOCOLS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison with baselines on five datasets. Data Metric DGCF HyRec LightGCN MHCN SGL SimGRACE GCA HCCF SHT SimGCL LightGCL p-val. impr.</figDesc><table><row><cell></cell><cell>R@20 0.0466 0.0472 0.0482 0.0503 0.0526</cell><cell>0.0603</cell><cell>0.0621 0.0626 0.0651 0.0718</cell><cell>0.0793</cell><cell>7e-9 10%</cell></row><row><cell>Yelp</cell><cell>N@20 0.0395 0.0395 0.0409 0.0424 0.0444 R@40 0.0774 0.0791 0.0803 0.0826 0.0869</cell><cell>0.0435 0.0989</cell><cell>0.0530 0.0527 0.0546 0.0615 0.1021 0.1040 0.1091 0.1166</cell><cell>0.0668 0.1292</cell><cell>8e-9 8% 2e-9 10%</cell></row><row><cell></cell><cell>N@40 0.0511 0.0522 0.0527 0.0544 0.0571</cell><cell>0.0656</cell><cell>0.0677 0.0681 0.0709 0.0778</cell><cell>0.0852</cell><cell>2e-9 9%</cell></row><row><cell>Gowalla</cell><cell>R@20 0.0944 0.0901 0.0985 0.0955 0.1030 N@20 0.0522 0.0498 0.0593 0.0574 0.0623 R@40 0.1401 0.1356 0.1431 0.1393 0.1500 N@40 0.0671 0.0660 0.0710 0.0689 0.0746</cell><cell>0.0869 0.0528 0.1276 0.0637</cell><cell>0.0896 0.1070 0.1232 0.1357 0.0537 0.0644 0.0731 0.0818 0.1322 0.1535 0.1804 0.1956 0.0651 0.0767 0.0881 0.0975</cell><cell>0.1578 0.0935 0.2245 0.1108</cell><cell>1e-6 16% 2e-6 14% 3e-6 14% 3e-6 13%</cell></row><row><cell>ML-10M</cell><cell>R@20 0.1763 0.1801 0.1789 0.1497 0.1833 N@20 0.2101 0.2178 0.2128 0.1814 0.2205 R@40 0.2681 0.2685 0.2650 0.2250 0.2768 N@40 0.2340 0.2340 0.2322 0.1962 0.2426</cell><cell>0.2254 0.2686 0.3295 0.2939</cell><cell>0.2145 0.2219 0.2173 0.2265 0.2613 0.2629 0.2573 0.2613 0.3231 0.3265 0.3211 0.3345 0.2871 0.2880 0.3318 0.2880</cell><cell>0.2613 0.3106 0.3799 0.3387</cell><cell>1e-9 15% 3e-9 18% 7e-10 13% 1e-9 17%</cell></row><row><cell>Amazon</cell><cell>R@20 0.0211 0.0302 0.0319 0.0296 0.0327 N@20 0.0154 0.0225 0.0236 0.0219 0.0249 R@40 0.0351 0.0432 0.0499 0.0489 0.0531 N@40 0.0201 0.0246 0.0290 0.0284 0.0312</cell><cell>0.0381 0.0291 0.0621 0.0371</cell><cell>0.0309 0.0322 0.0441 0.0474 0.0238 0.0247 0.0328 0.0360 0.0498 0.0525 0.0719 0.0750 0.0301 0.0314 0.0420 0.0451</cell><cell>0.0585 0.0436 0.0933 0.0551</cell><cell>2e-7 23% 2e-6 21% 1e-7 24% 9e-7 22%</cell></row><row><cell>Tmall</cell><cell>R@20 0.0235 0.0233 0.0225 0.0203 0.0268 N@20 0.0163 0.0160 0.0154 0.0139 0.0183 R@40 0.0394 0.0350 0.0378 0.0340 0.0446</cell><cell>0.0222 0.0152 0.0367</cell><cell>0.0373 0.0314 0.0387 0.0473 0.0252 0.0213 0.0262 0.0328 0.0616 0.0519 0.0645 0.0766</cell><cell>0.0528 0.0361 0.0852</cell><cell>3e-5 11% 1e-4 10% 1e-5 11%</cell></row><row><cell></cell><cell>N@40 0.0218 0.0199 0.0208 0.0188 0.0246</cell><cell>0.0203</cell><cell>0.0337 0.0284 0.0352 0.0429</cell><cell>0.0473</cell><cell>7e-5 10%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of computational complexity against baselines.</figDesc><table><row><cell>Stage</cell><cell>Computation</cell><cell>LightGCN</cell><cell>SGL</cell><cell>SimGCL</cell><cell>LightGCL</cell></row><row><cell>Pre-processing</cell><cell>Normalization</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>? Traditional GCN methods (e.g., LightGCN)  only perform convolution on one graph, inducing a complexity of O(2ELd) per batch. For most GCL-based methods, three contrastive views are computed per batch, leading to a complexity of roughly three times of LightGCN. In our model, instead, only two contrastive views are involved. Additionally, due to the low-rank property of SVD-based graph structure learning, our graph encoder takes only O[2q(I + J)Ld] time. For most datasets, including the five we use, 2q(I + J) &lt; E. Therefore, the training complexity of our model is less than half of that of the SOTA efficient model SimGCL. 4.4 RESISTANCE AGAINST DATA SPARSITY AND POPULARITY BIAS (RQ3)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Mean Average Distance (MAD) of the embeddings learned by different methods.</figDesc><table><row><cell>Dataset</cell><cell>MHCN</cell><cell>LightGCN</cell><cell>LightGCL</cell><cell>SGL</cell><cell>SimGCL</cell></row><row><cell>Yelp</cell><cell>0.8806</cell><cell>0.9469</cell><cell>0.9657</cell><cell>0.9962</cell><cell>0.9956</cell></row><row><cell>Gowalla</cell><cell>0.9247</cell><cell>0.9568</cell><cell>0.9721</cell><cell>0.9859</cell><cell>0.9897</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on LightGCL.</figDesc><table><row><cell>Variant</cell><cell>Recall@20</cell><cell>Yelp NDCG@20</cell><cell cols="2">Gowalla Recall@20 NDCG@20</cell></row><row><cell>CL-MF</cell><cell>0.0781</cell><cell>0.0659</cell><cell>0.1561</cell><cell>0.0929</cell></row><row><cell>CL-SVD++</cell><cell>0.0788</cell><cell>0.0666</cell><cell>0.1568</cell><cell>0.0932</cell></row><row><cell>LightGCL</cell><cell>0.0793</cell><cell>0.0668</cell><cell>0.1578</cell><cell>0.0935</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison with baselines on five datasets (continued).</figDesc><table><row><cell>Data</cell><cell cols="3">Metric NCF GCCF GraphCL SAIL GRACE AutoGCL LightGCL</cell></row><row><cell></cell><cell>R@20 0.0252 0.0462 0.0462 0.0471 0.0550</cell><cell>0.0593</cell><cell>0.0793</cell></row><row><cell>Yelp</cell><cell>N@20 0.0202 0.0398 0.0401 0.0405 0.0470 R@40 0.0487 0.0760 0.0764 0.0773 0.0917</cell><cell>0.0494 0.1009</cell><cell>0.0668 0.1292</cell></row><row><cell></cell><cell>N@40 0.0289 0.0508 0.0511 0.0516 0.0605</cell><cell>0.0650</cell><cell>0.0852</cell></row><row><cell></cell><cell>R@20 0.0171 0.0951 0.0997 0.0999 0.0744</cell><cell>0.0832</cell><cell>0.1578</cell></row><row><cell>Gowalla</cell><cell>N@20 0.0106 0.0535 0.0603 0.0602 0.0452 R@40 0.0216 0.1392 0.1473 0.1472 0.1071</cell><cell>0.0484 0.1291</cell><cell>0.0935 0.2245</cell></row><row><cell></cell><cell>N@40 0.0118 0.0684 0.0727 0.0725 0.0539</cell><cell>0.0605</cell><cell>0.1108</cell></row><row><cell></cell><cell>R@20 0.1097 0.1742 0.1659 0.1728 0.2107</cell><cell>0.2325</cell><cell>0.2613</cell></row><row><cell>ML-10M</cell><cell>N@20 0.1297 0.2109 0.2038 0.2118 0.2476 R@40 0.1634 0.2606 0.2560 0.2639 0.3075</cell><cell>0.2755 0.3415</cell><cell>0.3106 0.3799</cell></row><row><cell></cell><cell>N@40 0.1427 0.2331 0.2250 0.2332 0.2711</cell><cell>0.3023</cell><cell>0.3387</cell></row><row><cell></cell><cell>R@20 0.0142 0.0317 0.0360 0.0357 0.0360</cell><cell>0.0325</cell><cell>0.0585</cell></row><row><cell>Amazon</cell><cell>N@20 0.0085 0.0243 0.0266 0.0264 0.0271 R@40 0.0223 0.0483 0.0585 0.0581 0.0583</cell><cell>0.0241 0.0553</cell><cell>0.0436 0.0933</cell></row><row><cell></cell><cell>N@40 0.0133 0.0285 0.0340 0.0338 0.0345</cell><cell>0.0318</cell><cell>0.0551</cell></row><row><cell></cell><cell>R@20 0.0082 0.0209 0.0251 0.0254 0.0303</cell><cell>0.0312</cell><cell>0.0528</cell></row><row><cell>Tmall</cell><cell>N@20 0.0059 0.0141 0.0175 0.0177 0.0210 R@40 0.0140 0.0356 0.0416 0.0424 0.0505</cell><cell>0.0204 0.0524</cell><cell>0.0361 0.0852</cell></row><row><cell></cell><cell>N@40 0.0079 0.0196 0.0233 0.0236 0.0281</cell><cell>0.0278</cell><cell>0.0473</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>More detailed parameter settings can be found in our released source code.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Due to space limit, results of NCF, GCCF, GraphCL, SAIL, GRACE, and AutoGCL are in Appendix B.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>In the table, E, L and d denotes the edge number, the layer number and embedding size; ? ? (0, 1] is the edge keep rate; q is the required rank; I and J represents the number of users and items; B and M are the batch size and node number in a batch. Detailed calculations are shown in Appendix D</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Due to the fully connected nature of the SVD-reconstructed graph, the weights of unobserved interactions in the graph are of smaller magnitude. A weight of 0.01 is already a large weight in the graph.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A DETAILS OF THE BASELINES MLP-enhanced Collaborative Filtering:</p><p>? NCF <ref type="bibr" target="#b5">(He et al., 2017)</ref> is a collaborative filtering model that leverages neural network to exploit non-linearity. Two hidden layers are used in our evaluation.</p><p>GNN-based Collaborative Filtering:</p><p>? GCCF <ref type="bibr">(Chen et al., 2020c)</ref> strengthens the GNN-based collaborative filtering by implementing a residual network and reducing the non-linear transformation.</p><p>? LightGCN <ref type="bibr" target="#b6">(He et al., 2020)</ref> adopts a simplified GCN structure without embedding weight matrices and non-linear projection.</p><p>Disentangled Graph Collaborative Filtering:</p><p>? DGCF <ref type="bibr">(Wang et al., 2020b</ref>) learns a more sophisticated representation by segmenting the embedding vectors to represent multiple latent intentions.</p><p>Hypergraph-based Collaborative Filtering:</p><p>? HyRec <ref type="bibr">(Wang et al., 2020a</ref>) makes use of hypergraph to encode multi-order information between users and items.</p><p>Self-Supervised Learning Recommender Systems:</p><p>? GraphCL <ref type="bibr" target="#b31">(You et al., 2020)</ref> utilizes random node dropping and edge masking to generate two contrastive views, which were aligned by optimizing the SSL loss function.</p><p>? GRACE <ref type="bibr" target="#b37">(Zhu et al., 2020)</ref> proposes to corrupt the graph structure by both random edge dropout and random node feature dropping, and uses the corrupted graphs as the contrastive views.</p><p>? GCA <ref type="bibr">(Zhu et al., 2021b)</ref> adaptively dropout the nodes and edges by their importance calculated with node centrality.</p><p>? MHCN <ref type="bibr" target="#b32">(Yu et al., 2021)</ref> creates self-supervised signals for the graph representation learning by graph infomax network.</p><p>? SAIL <ref type="bibr">(Yu et al., 2022b)</ref> maximizes the neighborhood predicting probability between GNNgenerated high-level features and input node features.</p><p>? AutoGCL <ref type="bibr" target="#b30">(Yin et al., 2022)</ref> uses GNN to learn to mask nodes and edges in the augmented graph. It minimizes the similarity between the augmented and the original graph, while maximizing the similarity of the embeddings generated through them, so as to uncover the most important information in the graph.</p><p>? SimGRACE <ref type="bibr">(Xia et al., 2022a)</ref> creates augmented view by randomly perturbing the parameters of the GNN network.</p><p>? SGL <ref type="bibr" target="#b24">(Wu et al., 2021)</ref> adopts random walk sampling and probabilistic edge/node dropout to create augmented views for contrastive learning. In our experiments, we adopt the SGL-ED variant, which implements random edge dropout and exhibits the strongest performance according to the original paper.</p><p>? HCCF <ref type="bibr" target="#b27">(Xia et al., 2022b)</ref> encodes global graph information with hypergraph and contrasts it against the local information encoded with GCN. In our experiments, the number of hyper-edges are set as 128 following the original paper.</p><p>? SHT <ref type="bibr">(Xia et al., 2022c)</ref> adopts a hypergraph transformer framework to exploit global collaborative relationships and distills the global information to generate the cross-view self-supervised signals.</p><p>In our experiments, the number of hyper-edges are set as 128 following the original paper.</p><p>? SimGCL <ref type="bibr">(Yu et al., 2022a)</ref> propose to simplify the graph augmentation process of contrastive learning by directly injecting random noises into the feature representation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring and relieving the oversmoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Halko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">A</forename><surname>Per-Gunnar Martinsson</surname></persName>
		</author>
		<author>
			<persName><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="288" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on world wide web (WWW)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on research and development in Information Retrieval (SIGIR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automated selfsupervised learning for graphs</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Task-adaptive neural process for user cold-start recommendation</title>
		<author>
			<persName><forename type="first">Xixun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference</title>
		<meeting>the Web Conference</meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1306" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving graph collaborative filtering with neighborhood-enriched contrastive learning</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changxin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference (WWW)</title>
		<meeting>the ACM Web Conference (WWW)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2320" to="2329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interest-aware message-passing gcn for recommendation</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1296" to="1305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Leveraging distribution alignment via stein path for cross-domain cold-start recommendation</title>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaochao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="19223" to="19234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image denoising using the higher order singular value decomposition</title>
		<author>
			<persName><forename type="first">Ajit</forename><surname>Rajwade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arunava</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="849" to="862" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning matrix space image representations</title>
		<author>
			<persName><forename type="first">Anand</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="153" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adversarial graph augmentation to improve graph contrastive learning</title>
		<author>
			<persName><forename type="first">Susheel</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15920" to="15933" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep graph infomax. ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Next-item recommendation with sequential hypergraphs</title>
		<author>
			<persName><forename type="first">Jianling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 43rd international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1101" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disentangled graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongye</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 43rd international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1001" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selfsupervised graph learning for recommendation</title>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on research and development in information retrieval (SIGIR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="726" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust tensor graph convolutional networks via t-svd based graph augmentation</title>
		<author>
			<persName><forename type="first">Zhebin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaomin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2090" to="2099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simgrace: A simple framework for graph contrastive learning without data augmentation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bozhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the ACM Web Conference (WWW)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1070" to="1079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hypergraph contrastive collaborative filtering</title>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">Xiangji</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2022</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2022<address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">July 11-15, 2022., 2022b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-supervised hypergraph transformer for recommender systems</title>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Washington DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">August 14-18, 2022. 2022</date>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contrastive learning for sequential recommendation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiwen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiandong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1259" to="1273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Autogcl: Automated graph contrastive learning via learnable view generators</title>
		<author>
			<persName><forename type="first">Yihang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingzhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="8892" to="8900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Self-supervised multi-channel hypergraph convolutional network for social recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nguyen Quoc Viet Hung, and Xiangliang Zhang</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="413" to="424" />
		</imprint>
	</monogr>
	<note>Proceedings of the Web Conference 2021</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Are graph augmentations necessary? simple graph contrastive learning for recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sail: Self-augmented graph contrastive learning</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="8927" to="8935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Star-gcn: Stacked and reconstructed graph convolutional networks for recommender systems</title>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenglin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enhancing sequential recommendation with graph contrastive learning</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04131</idno>
		<title level="m">Deep graph contrastive representation learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An empirical study of graph contrastive learning</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01116</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference (WWW)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
