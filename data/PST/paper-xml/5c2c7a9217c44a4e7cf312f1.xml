<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning on Graphs: A Survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
							<email>zw-zhang16@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
							<email>cuip@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
							<email>wwzhu@tsinghua.edu.cn.p.cui</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Learning on Graphs: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Data</term>
					<term>Deep Learning</term>
					<term>Graph Neural Network</term>
					<term>Graph Convolutional Network</term>
					<term>Graph Autoencoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has been shown successful in a number of domains, ranging from acoustics, images to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, a significant amount of research efforts have been devoted to this area, greatly advancing graph analyzing techniques. In this survey, we comprehensively review different kinds of deep learning methods applied to graphs. We divide existing methods into five categories based on their model architectures: Graph Recurrent Neural Networks, Graph Convolutional Networks, Graph Autoencoders, Graph Reinforcement Learning, and Graph Adversarial Methods. We then provide a comprehensive overview of these methods in a systematic manner mainly following their history of developments. We also analyze the differences and compositionality of different architectures. Finally, we briefly outline their applications and discuss potential future directions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the last decade, deep learning has been a "crown jewel" in artificial intelligence and machine learning <ref type="bibr" target="#b0">[1]</ref>, showing superior performance in acoustics <ref type="bibr" target="#b1">[2]</ref>, images <ref type="bibr" target="#b2">[3]</ref>, and natural language processing <ref type="bibr" target="#b3">[4]</ref>, etc. The expressive power of deep learning to extract complex patterns underlying data has been well recognized. On the other hand, graphs 1 are ubiquitous in the real world, representing objects and their relationships such as social networks, e-commerce networks, biology networks, and traffic networks, etc. Graphs are also known to have complicated structures that contain rich underlying values <ref type="bibr" target="#b4">[5]</ref>. As a result, how to utilize deep learning methods for graph data analysis has attracted considerable research attention in the past few years. This problem is nontrivial because several challenges exist for applying traditional deep learning architectures to graphs:</p><p>• Irregular structures of graphs. Unlike images, audio, and texts which have a clear grid structure, graphs have irregular structures, making it hard to generalize some basic mathematical operations to graphs <ref type="bibr" target="#b5">[6]</ref>. For example, it is not straightforward to define convolution and pooling operation for graph data, which are the fundamental operations in Convolutional Neural Networks (CNNs). This is often referred to as the geometric deep learning problem <ref type="bibr" target="#b6">[7]</ref>.</p><p>• Heterogeneity of graphs. The graph itself can be complicated with diverse types and properties. For example, graphs can be heterogeneous or homogenous, weighted or unweighted, and signed or unsigned. In addition, the tasks for graphs also vary greatly, ranging from node-focused problems such as node classification and link prediction to graphfocused problems such as graph classification and graph generation. The diverse types, properties, and tasks require different model architectures to tackle specific problems.</p><p>1. Graphs are also called networks such as in social networks. In this paper, we use two terms interchangeably.</p><p>• Large-scale graphs. In the big-data era, real graphs can easily have millions or billions of nodes and edges, such as social networks or e-commerce networks <ref type="bibr" target="#b7">[8]</ref>. As a result, how to design scalable models, preferably having a linear time complexity with respect to the graph size, becomes a key problem.</p><p>• Incorporating interdisciplinary knowledge. Graphs are often connected with other disciplines, such as biology, chemistry or social sciences. The interdisciplinary provides both opportunities and challenges: domain knowledge can be leveraged to solve specific problems, but integrating domain knowledge could complicate the model design. For example, in generating molecular graphs, the objective function and chemical constraints are often non-differentiable, so gradientbased training methods cannot be easily applied.</p><p>To tackle these challenges, tremendous effort has been made towards this area, resulting in a rich literature of related papers and methods. The architecture adopted also varies greatly, ranging from supervised to unsupervised, convolutional to recursive. However, to the best of our knowledge, little attention has been paid to systematically summarize the differences and connections between these diverse methods.</p><p>In this paper, we try to fill this gap by comprehensive reviewing deep learning methods on graphs. Specifically, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, we divide the existing methods into five categories based on their model architectures: Graph Recurrent Neural Networks (Graph RNNs), Graph Convolutional Networks (GCNs), Graph Autoencoders (GAEs), Graph Reinforcement Learning (Graph RL), and Graph Adversarial Methods. We summarize some main characteristics of these categories in Table <ref type="table" target="#tab_0">1</ref> based on the following high-level distinctions. Graph RNNs capture recursive and sequential patterns of graphs by modeling and states in either the node-level or the graph-level. GCNs define convolution and readout operations on irregular graph structures to capture common local and global structural patterns. GAEs assume low-rank graph structures and adopt unsupervised methods for node representation learning. Graph RL defines graph-based actions and rewards to get feedbacks on graph tasks while following constraints. Graph Adversarial Methods adopt adversarial training to enhance the  The generalization ability and robustness of graph models Graph adversarial training and attacks generalization ability of graph models and test their robustness by adversarial attacks.</p><p>In the following sections, we provide a comprehensive overview of these methods in detail, mainly following their history of developments and how these methods solve the challenges of graphs. We also analyze the differences between these models and how to composite different architectures. Finally, we briefly outline their applications, introduce several open libraries, and discuss potential future directions. We also provide a collection of source codes of various methods discussed in the paper and summarize some common applications in the appendix.</p><p>Related works. There are several surveys that are related to our paper. Bronstein et al. <ref type="bibr" target="#b6">[7]</ref> summarize some early GCN methods as well as CNNs on manifolds and study them comprehensively through geometric deep learning. Battaglia et al. <ref type="bibr" target="#b8">[9]</ref> summarize how to use GNNs and GCNs for relational reasoning using a unified framework called graph networks, Lee et al. <ref type="bibr" target="#b9">[10]</ref> review the attention models for graphs, Zhang et al. <ref type="bibr" target="#b10">[11]</ref> summarize some GCNs, and Sun et al. <ref type="bibr" target="#b12">[12]</ref> briefly survey adversarial attacks on graphs. We differ from these works in that we systematically and comprehensively review different deep learning architectures on graphs rather than focusing on one specific branch. Concurrent to our work, Zhou et al. <ref type="bibr" target="#b13">[13]</ref> and Wu et al. <ref type="bibr" target="#b14">[14]</ref> also survey this field with different focuses and categorizations. Specifically, both their works focus on GCNs without considering graph reinforcement learning and graph adversarial methods, which are covered in our paper.</p><p>Another closely related topic is network embedding, trying to embed nodes into a low-dimensional vector space <ref type="bibr" target="#b15">[15]</ref>- <ref type="bibr" target="#b17">[17]</ref>. The main distinction between network embedding and our paper is that we focus on how different deep learning models can be applied to graphs, and network embedding can be recognized as a concrete example using some of these models (and they use non-deep-learning methods as well).</p><p>The rest of this paper is organized as follows. In Section 2, we introduce notations and preliminaries. Then, we review Graph RNNs, GCNs, GAEs, Graph RL, and Graph Adversarial Methods in Section 3 to Section 7, respectively. We conclude with </p><formula xml:id="formula_0">G = (V, E) A graph N, M</formula><p>The number of nodes and edges</p><formula xml:id="formula_1">V = {v 1 , ..., v N }</formula><p>The set of nodes</p><formula xml:id="formula_2">F V , F E</formula><p>Attributes/features for nodes and edges A The adjacency matrix</p><formula xml:id="formula_3">D(i, i) = j A(i, j)</formula><p>The diagonal degree matrix</p><formula xml:id="formula_4">L = D − A The Laplacian matrix QΛQ T = L</formula><p>The eigen-decomposition of L P = D −1 A</p><p>The transition matrix</p><formula xml:id="formula_5">N k (i), N (i)</formula><p>The k-step and 1-step neighbors of v i H l</p><p>The hidden representation in the l th layer f l</p><p>The number of dimensions of H l ρ(•) Some non-linear activation X 1 X 2</p><p>Element-wise multiplication Θ Learnable parameters a discussion in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">NOTATIONS AND PRELIMINARIES</head><p>Notations. In this paper, a graph 2 is represented as</p><formula xml:id="formula_6">G = (V, E) where V = {v 1 , ..., v N } is a set of N = |V | nodes and E ⊆ V × V is a set of M = |E| edges between nodes. We use A ∈ R N ×N</formula><p>to denote the adjacency matrix, where its i th row, j th column, and an element denoted as A(i, :), A(:, j), A(i, j), respectively. The graph can be directed/undirected and weighted/unweighted. We mainly consider unsigned graphs, so A(i, j) ≥ 0. Signed graphs will be discussed in future works. We use F V and F E to denote features for nodes and edges, respectively. For other variables, we use bold uppercase characters to denote matrices and bold lowercase characters to denote vectors, e.g. X and x.</p><p>The transpose of a matrix is denoted as X T and the element-wise multiplication is denoted as X 1 X 2 . Functions are marked by curlicue, e.g. F(•).</p><p>Preliminaries. For an undirected graph, its Laplacian matrix is defined as L = D − A, where D ∈ R N ×N is a diagonal degree 2. We only consider graphs without self-loops or multiple edges. matrix with D(i, i) = j A(i, j). Its eigen-decomposition is denoted as L = QΛQ T , where Λ ∈ R N ×N is a diagonal matrix of eigenvalues sorted in the ascending order and Q ∈ R N ×N are the corresponding eigenvectors. The transition matrix is defined as P = D −1 A, where P(i, j) represents the probability of a random walk starting from node v i lands at node v j . The k-step neighbors of node v i are defined as N k (i) = {j|D(i, j) ≤ k}, where D(i, j) is the shortest distance from node v i to v j , i.e. N k (i) is a set of nodes reachable from node v i within k-steps. To simplify notations, we drop the subscript for the immediate neighborhood, i.e. N (i) = N 1 (i).</p><p>For a deep learning model, we use superscripts to denote layers, e.g. H l . We use f l to denote the number of hidden dimensions in layer l. The sigmoid activation function is defined as σ(x) = 1/ (1 + e −x ) and rectifier linear unit (ReLU) is defined as ReLU(x) = max(0, x). A general element-wise nonlinear activation function is denoted as ρ(•). In this paper, unless stated otherwise, we assume all functions are differentiable so that we can learn model parameters Θ through back-propagation <ref type="bibr" target="#b18">[18]</ref> using commonly adopted optimizers, such as Adam <ref type="bibr" target="#b19">[19]</ref>, and training techniques, such as dropout <ref type="bibr" target="#b20">[20]</ref>. We summarize the notations in Table <ref type="table">2</ref>.</p><p>The tasks for learning a deep model on graphs can be broadly categorized into two domains:</p><p>• Node-focused tasks: the tasks are associated with individual nodes in the graph. Examples include node classification, link prediction, and node recommendation.</p><p>• Graph-focused tasks: the tasks are associated with the whole graph. Examples include graph classification, estimating certain properties of the graph or generating graphs. Note that such distinctions are more conceptually than mathematically rigorous. On the one hand, there exist tasks associated with mesoscopic structures such as community detection <ref type="bibr" target="#b21">[21]</ref>. In addition, node-focused problems can sometimes be studied as graphfocused problems by transforming the former into ego-centric networks <ref type="bibr" target="#b22">[22]</ref>. Nevertheless, we will explain the differences in algorithm designs for these two categories when necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAPH RECURRENT NEURAL NETWORKS</head><p>Recurrent Neural Networks (RNNs) such as Gated Recurrent Units (GRU) <ref type="bibr" target="#b31">[31]</ref> or LSTM <ref type="bibr" target="#b32">[32]</ref> are de facto standards in modeling sequential data. In this section, we first review Graph Recurrent Neural Networks which can capture recursive and sequential patterns of graphs. Graph RNNs can be broadly divided into two categories: node-level RNNs and graph-level RNNs. The main distinction lies in whether the pattern is in the node-level and modeled by states of nodes, or in the graph-level and modeled by a common state of the graph. The main characteristics of the methods surveyed are summarized in Table <ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Node-level RNNs</head><p>Node-level RNNs for graphs, which are also referred to as Graph Neural Networks (GNNs) 3 , can be dated back to the "pre-deeplearning" era <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b33">[33]</ref>. The idea of GNN is simple: to encode structural information of the graph, each node v i can be represented by a low-dimensional state vector s i , 1 ≤ i ≤ N .</p><p>3. Recently, GNNs are also used to refer to general neural networks for graph data. We follow the traditional name convention and use GNNs to refer to this specific type of Graph RNNs.</p><p>Motivated by recursive neural networks <ref type="bibr" target="#b34">[34]</ref>, a recursive definition of states is adopted <ref type="bibr" target="#b23">[23]</ref>:</p><formula xml:id="formula_7">s i = j∈N (i) F s i , s j , F V i , F V j , F E i,j ,<label>(1)</label></formula><p>where F(•) is a parametric function to be learned. After obtaining s i , another parametric function O(•) is applied to get the final outputs:</p><formula xml:id="formula_8">ŷi = O s i , F V i .<label>(2)</label></formula><p>For graph-focused tasks, the authors suggest adding a special node with unique attributes corresponding to the whole graph. To learn model parameters, the following semi-supervised 4 method is adopted: after iteratively solving Eq. ( <ref type="formula" target="#formula_7">1</ref>) to a stable point using Jacobi method <ref type="bibr" target="#b35">[35]</ref>, one step of gradient descent is performed using the Almeida-Pineda algorithm <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref> to minimize a taskspecific objective function, for example, the square loss between predicted values and the ground-truth for regression tasks; then, this process is repeated until convergence. NN4G <ref type="bibr" target="#b24">[24]</ref> takes a similar approach but differs in the definition of states. By removing s i in the right-hand side of Eq (1), the states in NN4G are not recursively defined, which are easier to compute.</p><p>With two simple equations in Eqs. (1)(2), GNN plays two important roles. In retrospect, GNN unifies some early methods in processing graph data, such as recursive neural networks and Markov chains <ref type="bibr" target="#b23">[23]</ref>. Looking to the future, the general idea in GNN has profound inspirations: as will be shown later, many state-of-the-art GCNs actually have a similar formulation as Eq. ( <ref type="formula" target="#formula_7">1</ref>), following the framework of exchanging information with immediate node neighborhoods. In fact, GNNs and GCNs can be unified into some common frameworks and GNN is equivalent to GCN using identical layers to reach a stable state. More discussion will be given in Section 4.</p><p>Though conceptually important, GNN has several drawbacks. First, to ensure that Eq. ( <ref type="formula" target="#formula_7">1</ref>) has a unique solution, F(•) has to be a "contraction map" <ref type="bibr" target="#b38">[38]</ref>, which severely limits the modeling ability. Second, since many iterations are needed to reach stable states between gradient descend steps, GNN is computationally expensive. Because of these drawbacks and perhaps the lack of computational power (e.g. Graphics Processing Unit, GPU, is not widely used for deep learning those days) and lack of research interests, GNN was not a general research focus in the first place.</p><p>A notable improvement to GNN is Gated Graph Sequence Neural Networks (GGS-NNs) <ref type="bibr" target="#b25">[25]</ref> with the following modifications. Most importantly, the authors replace the recursive definition of Eq. ( <ref type="formula" target="#formula_7">1</ref>) with GRU, thus removing the requirement of "contraction map" and supporting modern optimization techniques. Specifically, Eq. ( <ref type="formula" target="#formula_7">1</ref>) is adapted as:</p><formula xml:id="formula_9">s (t) i = (1 − z (t) i ) s (t−1) i + z (t) i s (t) i ,<label>(3)</label></formula><p>where z is calculated by update gates, s are candidates for updating and t is the pseudo time. Secondly, the authors propose using several such networks operating in sequence to produce a sequence output, which can be applied to applications such as program verification <ref type="bibr" target="#b39">[39]</ref>. SSE <ref type="bibr" target="#b26">[26]</ref> takes a similar approach as Eq. (3). Instead of using GRU in the calculation, SSE adopts stochastic fixed-point gradient descent to accelerate training, which basically alternates between  Recursive definition of node states No -NN4G <ref type="bibr" target="#b24">[24]</ref> No -GGS-NNs <ref type="bibr" target="#b25">[25]</ref> Yes Sequence output SSE <ref type="bibr" target="#b26">[26]</ref> Yes -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-level</head><p>You et al. <ref type="bibr" target="#b27">[27]</ref> Generate nodes and edges in an autoregressive manner No -DGNN <ref type="bibr" target="#b28">[28]</ref> Capture the time dynamics of the formation of nodes and edges Yes -RMGCNN <ref type="bibr" target="#b29">[29]</ref> Recursively reconstruct the graph Yes Convolutional layers Dynamic GCN <ref type="bibr" target="#b30">[30]</ref> Gather node representations in different time slices Yes Convolutional layers calculating steady states of nodes using local neighborhoods and optimizing model parameters, both in stochastic mini-batches.</p><p>GNN and its extensions have many applications. For example, CommNet <ref type="bibr" target="#b40">[40]</ref> applies GNN to multi-agent AI systems by regarding each agent as a node and updating the states of agents by communicating with others for several time steps before taking an action. Interaction Network (IN) <ref type="bibr" target="#b41">[41]</ref> uses GNN for physical reasoning by representing objects as nodes, relations as edges, and using pseudo-time as a simulation system. VAIN <ref type="bibr" target="#b42">[42]</ref> improves CommNet and IN by introducing attentions to weigh different interactions. Relation Networks (RNs) <ref type="bibr" target="#b43">[43]</ref> propose using GNN as a relational reasoning module to augment other neural networks and show promising results in visual question answering problems. <ref type="bibr" target="#b44">[44]</ref> applied GNN to model the hidden states of the graph in graph generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph-level RNNs</head><p>In this subsection, we review how to apply RNNs to capture graph-level patterns, e.g. temporal patterns of dynamic graph or sequential patterns in the graph's different levels of granularities. In graph-level RNNs, instead of learning states for each node, the states are encoded in a single RNN applied to the whole graph.</p><p>You et al. <ref type="bibr" target="#b27">[27]</ref> apply Graph RNN to the graph generation problem. Specifically, they adopt two RNNs, one for generating new nodes while the other generates edges for the newly added node in an autoregressive manner. They show that such hierarchical RNN architecture can effectively learn from input graphs compared to the traditional rule-based graph generative models while having a reasonable time complexity.</p><p>Dynamic Graph Neural Network (DGNN) <ref type="bibr" target="#b28">[28]</ref> proposes using time-aware LSTM <ref type="bibr" target="#b45">[45]</ref> to learn node representations, aiming to capture the temporal information of dynamic graphs. After a new edge is established, DGNN uses LSTM to update the representation of the two interacting nodes as well as their immediate neighbors, i.e. considering the one-step propagation effect. The authors show that time-aware LSTM can well model the establishing orders and time intervals of edge formations, which in turn benefits a range of graph applications.</p><p>It is also possible to use Graph RNN in conjunction with other architectures, such as GCNs or GAEs. For example, RMGCNN <ref type="bibr" target="#b29">[29]</ref> applies LSTM to the results of GCNs to progressively reconstruct the graph as illustrated in Figure <ref type="figure">2</ref>, aiming to tackle the graph sparsity problem. By using LSTM, the information from different parts of the graph can diffuse across long ranges without needing too many GCN layers. Dynamic GCN <ref type="bibr" target="#b30">[30]</ref> applies LSTM to gather results of GCNs of different time slices in dynamic networks, aiming to capture both spatial and temporal graph information.</p><formula xml:id="formula_10">X X (t) X(t) MGCNN RNN dX (t) X (t+1) = X (t) + dX (t)</formula><p>row+column filtering Fig. <ref type="figure">2</ref>. The framework of RMGCNN reprinted from <ref type="bibr" target="#b29">[29]</ref> with permission. RMGCNN adds LSTM into GCN to progressively reconstruct the graph. X t , Xt , and dX t represents the estimated matrix (inputs to GCNs), the outputs of GCNs, and the incremental updates produced by RNN at iteration t, respectively. MGCNN refers to Multi-Graph CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GRAPH CONVOLUTIONAL NETWORKS (GCNS)</head><p>Graph Convolutional Networks (GCNs) are inarguably the hottest topic of graph-based deep learning. Mimicking CNNs, modern GCNs learn common local and global structural patterns on graphs through designing convolution and readout functions. Since most GCNs can be trained with task-specific loss via back-propagation (with few exceptions such as unsupervised training in <ref type="bibr" target="#b80">[80]</ref>), we focus on the architectures adopted. We first discuss the convolution operations, then move to the readout operations and some other improvements. We summarize the main characteristics of GCNs surveyed in this paper in Table <ref type="table" target="#tab_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Convolution Operations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Spectral Methods</head><p>For CNNs, convolution is the most fundamental operation. However, standard convolution for image or text can not be directly applied to graphs because of lacking a grid structure <ref type="bibr" target="#b5">[6]</ref>. Bruna et al. <ref type="bibr" target="#b46">[46]</ref> first introduce convolution for graph data from the spectral domain using the graph Laplacian matrix L <ref type="bibr" target="#b81">[81]</ref>, which plays a similar role as the Fourier basis for signal processing <ref type="bibr" target="#b5">[6]</ref>. Specifically, the convolution operation on the graph * G is defined as:</p><formula xml:id="formula_11">u 1 * G u 2 = Q Q T u 1 Q T u 2 ,<label>(4)</label></formula><p>where u 1 , u 2 ∈ R N are two signals defined on nodes and Q are eigenvectors of L. Briefly speaking, Q T transforms graph signals u 1 , u 2 into the spectral domain, and Q does the inverse transform after an element-wise product. The validity of this definition is based on the convolution theorem, i.e. the Fourier transform of a convolution operation is the element-wise product of their Fourier transforms. Then, filtering a signal u can be obtained as where u is the output signal, Θ = Θ(Λ) ∈ R N ×N is a diagonal matrix of learnable filters and Λ are eigenvalues of L. Then, a convolutional layer is defined by applying different filters to different input and output signals as follows:</p><formula xml:id="formula_12">u = QΘQ T u,<label>(5)</label></formula><formula xml:id="formula_13">u l+1 j = ρ f l i=1 QΘ l i,j Q T u l i j = 1, ..., f l+1 ,<label>(6)</label></formula><p>where l is the layer, u l j ∈ R N is the j th hidden representation (i.e. signals) for nodes in the l th layer, Θ l i,j are learnable filters. The idea of Eq. ( <ref type="formula" target="#formula_13">6</ref>) is similar to conventional convolutions: passing the input signals through a set of learnable filters to aggregate the information, followed by some non-linear transformation. By using nodes features F V as the input layer and stacking multiple convolutional layers, the overall architecture is similar to CNNs. Theoretical analysis shows that such a definition of convolution operation on graphs can mimic certain geometric properties of CNNs, which we refer readers to <ref type="bibr" target="#b6">[7]</ref> for a comprehensive survey.</p><p>However, directly using Eq. ( <ref type="formula" target="#formula_13">6</ref>) requires O(N ) parameters to be learned, which may not be feasible in practice. In addition, the filters in the spectral domain may not be localized in the spatial domain, i.e. each node can be affected by all other nodes rather than only nodes in a small region. To alleviate these problems, Bruna et al. <ref type="bibr" target="#b46">[46]</ref> suggest using the following smooth filters:</p><formula xml:id="formula_14">diag Θ l i,j = K α l,i,j ,<label>(7)</label></formula><p>where K is a fixed interpolation kernel and α l,i,j are learnable interpolation coefficients. The authors also generalize this idea to the setting where the graph is not given but constructed from raw features using either a supervised or an unsupervised method <ref type="bibr" target="#b47">[47]</ref>. However, two fundamental limitations still remain unsolved. First, since the full eigenvectors of the Laplacian matrix are needed during each calculation, the time complexity is at least O(N 2 ) per forward and backward pass, not to mention the O(N 3 ) complexity in calculating the eigen-decomposition, which is not scalable to large-scale graphs. Second, since the filters depend on the eigenbasis Q of the graph, parameters can not be shared across multiple graphs with different sizes and structures.</p><p>Next, we review two lines of works trying to solve these limitations and then unify them using some common frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Efficiency Aspect</head><p>To solve the efficiency problem, ChebNet <ref type="bibr" target="#b48">[48]</ref> proposes using a polynomial filter as follows:</p><formula xml:id="formula_15">Θ(Λ) = K k=0 θ k Λ k ,<label>(8)</label></formula><p>where θ 0 , ..., θ K are learnable parameters and K is the polynomial order. Then, instead of performing the eigen-decomposition, the authors rewrite Eq. ( <ref type="formula" target="#formula_15">8</ref>) using the Chebyshev expansion <ref type="bibr" target="#b82">[82]</ref>:</p><formula xml:id="formula_16">Θ(Λ) = K k=0 θ k T k ( Λ),<label>(9)</label></formula><p>where Λ = 2Λ/λ max − I are the rescaled eigenvalues, λ max is the maximum eigenvalue, I ∈ R N ×N is the identity matrix and T k (x) is the Chebyshev polynomial of order k. The rescaling is necessary because of the orthonormal basis of Chebyshev polynomials. Using the fact that polynomial of the Laplacian matrix acts as a polynomial of its eigenvalues, i.e. L k = QΛ k Q T , the filter operation in Eq. ( <ref type="formula" target="#formula_12">5</ref>) can be rewritten as: </p><formula xml:id="formula_17">u = QΘ(Λ)Q T u = K k=0 θ k QT k ( Λ)Q T u = K k=0 θ k T k ( L)u = K k=0 θ k ūk ,<label>(10</label></formula><formula xml:id="formula_18">(x) = 2xT k−1 (x) − T k−2 (x) and T 0 (x) = 1, T 1 (x) = x, ūk can also be calculated recursively: ūk = 2 Lū k−1 − ūk−2<label>(11)</label></formula><p>with ū0 = u, ū1 = Lu. Now, since only the matrix multiplication of a sparse matrix L and some vectors needs to be calculated, the time complexity is O(KM ) by using sparse matrix multiplications, where M is the number of edges and K is the polynomial order, i.e. linear with respect to the graph size. It is also easy to see that such a polynomial filter is strictly K-localized: after one convolution, the representations of node v i will only be affected by its K-step neighborhood N K (i). Interestingly, this idea is independently used in network embedding to preserve the highorder proximity <ref type="bibr" target="#b83">[83]</ref>, of which we omit the details for brevity.</p><p>An improvement to ChebNet introduced by Kipf and Welling <ref type="bibr" target="#b49">[49]</ref> further simplifies the filtering by only using the first-order neighbors as follows:</p><formula xml:id="formula_19">h l+1 i = ρ   j∈ Ñ (i) 1 D(i, i) D(j, j) h l j Θ l   ,<label>(12)</label></formula><p>where h l i ∈ R f l is the hidden representation of node v i in the l th layer 5 , D = D + I and Ñ (i) = N (i) ∪ {i}. This can be written equivalently in the matrix form:</p><formula xml:id="formula_20">H l+1 = ρ D− 1 2 Ã D− 1 2 H l Θ l ,<label>(13)</label></formula><p>where Ã = A + I, i.e. adding a self-connection. The authors show that Eq. ( <ref type="formula" target="#formula_20">13</ref>) is a special case of Eq. ( <ref type="formula" target="#formula_15">8</ref>) by setting K = 1 with a few minor changes. Then, the authors argue that stacking an adequate number of layers as illustrated in Figure <ref type="figure" target="#fig_2">3</ref> has a similar modeling capacity as ChebNet and leads to better results. An important insight of ChebNet and its extension is that they connect the spectral graph convolution with the spatial architecture as in GNNs, i.e. defining graph convolutions by considering the neighborhoods of nodes. Specifically, they show that when the spectral convolution function is polynomial or first-order, spectral graph convolution is equivalent to the spatial convolution. In addition, the convolution in Eq. ( <ref type="formula" target="#formula_19">12</ref>) is very similar to the definition of states in GNN in Eq. ( <ref type="formula" target="#formula_7">1</ref>), except that the convolution definition replaces the recursive definition. In this aspect, GNN can be regarded as GCN using a large number of identical layers to reach stable states <ref type="bibr" target="#b6">[7]</ref>, i.e. GNN uses a fixed function with fixed parameters to iteratively update node hidden states until equilibrium, while GCN has a preset number of layers with each layer containing different parameters.</p><p>5. We use a different letter because h l ∈ R f l is the hidden representation of one node, while u l ∈ R N represents a dimension for all nodes. It is worth mentioning that some pure spectral methods have also been proposed to solve the efficiency problem. For example, instead of using Chebyshev expansion as in Eq. ( <ref type="formula" target="#formula_16">9</ref>), CayleyNet <ref type="bibr" target="#b50">[50]</ref> adopts the Cayley polynomials in defining convolutions:</p><formula xml:id="formula_21">Θ(Λ) = θ 0 + 2Re K k=1 θ k (θ h Λ − iI) j (θ h Λ + iI) j ,<label>(14)</label></formula><p>where i = √ −1 denotes the imaginary unit and θ h is another spectral zoom parameter. Besides being efficient as ChebNet, the authors demonstrate that the Cayley polynomials can detect "narrow frequency bands of importance" to achieve better results. Graph Wavelet Neural Network (GWNN) <ref type="bibr" target="#b51">[51]</ref> further proposes to replace the Fourier transform in spectral filters by the graph wavelet transform, i.e. rewrite Eq. ( <ref type="formula" target="#formula_11">4</ref>) as:</p><formula xml:id="formula_22">u 1 * G u 2 = ψ ψ −1 u 1 ψ −1 u 2 , (<label>15</label></formula><formula xml:id="formula_23">)</formula><p>where ψ is the graph wavelet bases. By using fast approximating algorithms to calculate ψ and ψ −1 , the computational complexity is also O(KM ), i.e. linear with respect to the graph size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Multiple Graphs Aspect</head><p>In the meantime, a parallel of works focuses on generalizing convolution operation to multiple graphs of arbitrary sizes. Neural FPs <ref type="bibr" target="#b52">[52]</ref> propose a spatial method also using the first-order neighbors:</p><formula xml:id="formula_24">h l+1 i = σ j∈ N (i) h l j Θ l . (<label>16</label></formula><formula xml:id="formula_25">)</formula><p>Since the parameters Θ can be shared across different graphs and are independent of graph sizes, Neural FPs can handle multiple graphs of arbitrary sizes. Note that Eq. ( <ref type="formula" target="#formula_24">16</ref>) is very similar to Eq. ( <ref type="formula" target="#formula_19">12</ref>). However, instead of considering the influence of node degrees by adding a normalization term, Neural FPs propose learning different parameters Θ for nodes with different degrees. This strategy performs well for small graphs such as the molecular graphs, i.e. atoms as nodes and bonds as edges, but may not be scalable to large-scale graphs. PATCHY-SAN <ref type="bibr" target="#b53">[53]</ref> adopts a different idea to assign a unique order of neighbors using the graph labeling procedure such as the Weisfeiler-Lehman kernel <ref type="bibr" target="#b84">[84]</ref> and arrange nodes in a line using this pre-defined order. In addition, PATCHY-SAN defines a "receptive field" for each node v i by selecting a fixed number of nodes from its k-step neighborhoods N k (i). Then standard 1-D CNN with proper normalization is adopted. Since now nodes in different graphs all have a "receptive field" with a fixed size and order, PATCHY-SAN can learn from multiple graphs like normal CNNs. The drawbacks are that the convolution depends heavily on the graph labeling procedure which is a preprocessing step that is not learned. LGCN <ref type="bibr" target="#b54">[54]</ref> further proposes to simplify the sorting process by using the lexicographical order, i.e. sort neighbors based on their hidden representations in the last layer H L . Instead of using a single order, the authors sort different channels of H L separately. SortPooling <ref type="bibr" target="#b55">[55]</ref> takes a similar idea, but rather than sorting neighbors for each node, the authors propose to sort all nodes directly, i.e. a single order for all neighborhoods. Despite the differences between these methods, enforcing a 1-D order of nodes may not be a natural choice for graphs.</p><p>DCNN <ref type="bibr" target="#b56">[56]</ref> adopts another approach to replace the eigenbasis of the convolution by a diffusion-basis, i.e. the "receptive field" of nodes is determined by the diffusion transition probability between nodes. Specifically, the convolution is defined as:</p><formula xml:id="formula_26">H l+1 = ρ P K H l Θ l ,<label>(17)</label></formula><p>where P K = (P) K is the transition probability of a length K diffusion process (i.e. random walk), K is a preset diffusion length and Θ l ∈ R f l ×f l is a diagonal matrix of learnable parameters. Since only P K depend on the graph structure, the parameters Θ l can be shared across graphs of arbitrary sizes. However, calculating P K induces the time complexity O N 2 K , thus making the method not scalable to large-scale graphs.</p><p>DGCN <ref type="bibr" target="#b57">[57]</ref> further proposes to jointly adopt diffusion and adjacency basis using a dual graph convolutional network. Specifically, DGCN uses two convolutions: one as Eq. ( <ref type="formula" target="#formula_20">13</ref>), and the other replaces the adjacency matrix with the positive pointwise mutual information (PPMI) matrix <ref type="bibr" target="#b85">[85]</ref> of the transition probability, i.e.</p><formula xml:id="formula_27">Z l+1 = ρ D − 1 2 P X P D − 1 2 P Z l Θ l , (<label>18</label></formula><formula xml:id="formula_28">)</formula><p>where X P is the PPMI matrix and D P (i, i) = j X P (i, j) is the diagonal degree matrix of X P . Then, two convolutions are ensembled by minimizing the mean square differences between H and Z. A random walk sampling procedure is also proposed to accelerate the calculation of the transition probability. Experiments demonstrate that such dual convolutions are effective even for single-graph problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Frameworks</head><p>Based on the above two lines of works, MPNNs <ref type="bibr" target="#b58">[58]</ref> propose a unified framework for the graph convolution operation in the spatial domain using the message-passing functions:</p><formula xml:id="formula_29">m l+1 i = j∈N (i) F l h l i , h l j , F E i,j h l+1 i = G l h l i , m l+1 i ,<label>(19)</label></formula><p>where F l (•) and G l (•) are message functions and vertex update functions that need to be learned, respectively, and m l are the "messages" passed between nodes. Conceptually, MPNNs propose a framework that each node sends messages based on its states and updates its states based on messages received from immediate neighbors. The authors show that the above framework includes many existing methods such as <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b47">[47]</ref>, <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b61">[61]</ref> as special cases. Besides, the authors propose adding a "master" node that is connected to all nodes to accelerate the passing of messages across long distances and split the hidden representations into different "towers" to improve the generalization ability.</p><p>The authors show that a specific variant of the MPNNs can achieve state-of-the-art performance in predicting molecular properties. Concurrently, GraphSAGE <ref type="bibr" target="#b59">[59]</ref> takes a similar idea as Eq. ( <ref type="formula" target="#formula_29">19</ref>) with multiple aggregating functions as follows:</p><formula xml:id="formula_30">m l+1 i = AGGREGATE l ({h l j , ∀j ∈ N (i)}) h l+1 i = ρ Θ l h l i , m l+1 i ,<label>(20)</label></formula><p>where [•, •] is concatenation and AGGREGATE(•) is the aggregating function. The authors suggest three aggregating functions: element-wise mean, long short-term memory (LSTM) <ref type="bibr" target="#b32">[32]</ref> and max-pooling as follows:</p><formula xml:id="formula_31">AGGREGATE l = max{ρ(Θ pool h l j + b pool ), ∀j ∈ N (i)},<label>(21)</label></formula><p>where Θ pool and b pool are parameters to be learned and max {•} is element-wise maximum. For the LSTM aggregating function, since an order of neighbors is needed, the authors adopt the simple random order.</p><p>Mixture model network (MoNet) <ref type="bibr" target="#b60">[60]</ref> also tries to unify the existing works of GCNs as well as CNN for manifolds into a common framework using "template matching":</p><formula xml:id="formula_32">h l+1 ik = j∈N (i) F l k (u(i, j))h l j , k = 1, ..., f l+1 ,<label>(22)</label></formula><p>where u(i, j) are the pseudo-coordinates of node pair v i and v j , F l k (u) is a parametric function to be learned, h l ik is the k th dimension of h l i . In other words, F l k (u) serve as the weighting kernel for combining neighborhoods. Then, MoNet suggests using the Gaussian kernel:</p><formula xml:id="formula_33">F l k (u) = exp − 1 2 (u − µ l k ) T (Σ l k ) −1 (u − µ l k ) ,<label>(23)</label></formula><p>where µ l k are mean vectors and Σ l k are diagonal covariance matrices to be learned. The pseudo-coordinates are set to be degrees as in <ref type="bibr" target="#b49">[49]</ref>, i.e.</p><formula xml:id="formula_34">u(i, j) = ( 1 D(i, i) , 1 D(j, j) ).<label>(24)</label></formula><p>Graph Networks (GNs) <ref type="bibr" target="#b8">[9]</ref> propose a more general framework for both GCNs and GNNs to learn three sets of representations: h l i , e l ij , z l as the representation for nodes, edges and the whole graph respectively. The representations are learned using three aggregation functions and three update functions:</p><formula xml:id="formula_35">m l i = G E→V ({h l j , ∀j ∈ N (i)}) m l V = G V →G ({h l i , ∀v i ∈ V }) m l E = G E→G ({h l ij , ∀(v i , v j ) ∈ E}) h l+1 i = F V (m l i , h l i , z l ) e l+1 ij = F E (e l ij , h l i , h l j , z l ) z l+1 = F G (m l E , m l V , z l ),<label>(25)</label></formula><p>where</p><formula xml:id="formula_36">F V (•), F E (•), F G (•)</formula><p>are corresponding updating functions for nodes, edges and the whole graph respectively and G(•) are message-passing functions with superscripts denoting message-passing directions. Note that the message-passing functions all take sets as inputs, thus they should take variable numbers of arguments and invariant to input permutations, e.g. elementwise summation, mean or maximum. Compared with MPNNs, GNs introduce edge representations and the whole graph representation, thus making the framework more general.</p><p>In summary, the convolution operations have evolved from the spectral domain to the spatial domain and from multi-step neighbors to the immediate neighbors. Currently, gathering information from immediate neighbors like Eq. ( <ref type="formula" target="#formula_20">13</ref>) and following the framework of Eqs. <ref type="bibr" target="#b19">(19)</ref>  <ref type="bibr" target="#b20">(20)</ref>  <ref type="bibr" target="#b25">(25)</ref> are the most common choices for the graph convolution operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Readout Operations</head><p>Using convolution operations, useful features for nodes can be learned to solve many node-focused tasks. However, to tackle graph-focused tasks, information of nodes needs to be aggregated to form a graph-level representation. In the literature, this is usually called the readout operation 6 . This problem is non-trivial because stride convolutions or pooling in standard CNNs cannot be directly used due to the lack of a grid structure.</p><p>Order invariance. A critical requirement for the graph readout operations is that the operation should be invariant to the order of nodes, i.e. if we change the indices of nodes and edges using a bijective function between two vertex sets, representation of the whole graph should not change. For example, whether a drug can treat certain diseases should be independent of how the drug is represented as a graph. Note that since this problem is related to the graph isomorphism problem which is known to be NP <ref type="bibr" target="#b86">[86]</ref>, we can only find a function that is order-invariant but not vice versa in polynomial time, i.e. even two graphs are not isomorphic, they may have the same representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Statistics</head><p>The most basic order-invariant operations are simple statistics like taking the sum, average or max-pooling <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b56">[56]</ref>, i.e.</p><formula xml:id="formula_37">h G = N i=1 h L i or h G = 1 N N i=1 h L i or h G = max h L i , ∀i ,<label>(26)</label></formula><p>where h G is the representation for graph G and h L i is the representation of node v i in the final layer L. However, such firstmoment statistics may not be representative enough to distinguish different graphs.</p><p>In <ref type="bibr" target="#b61">[61]</ref>, the authors suggest considering the distribution of node representations by using fuzzy histograms <ref type="bibr" target="#b87">[87]</ref>. The basic idea of fuzzy histograms is to construct several "histogram bins" and then calculate the memberships of h L i to these bins, i.e. regarding representations of nodes as samples and match them to some pre-defined templates, and return the concatenation of the final histograms. In this way, nodes with the same sum/average/maximum but with different distributions can be distinguished.</p><p>Another commonly used approach for gathering information is to add a fully connected (FC) layer as the final layer <ref type="bibr" target="#b46">[46]</ref>, i.e.</p><formula xml:id="formula_38">h G = ρ H L Θ F C ,<label>(27)</label></formula><p>where</p><formula xml:id="formula_39">H L ∈ R N f L is the concatenation of the final node representation H L , Θ F C ∈ R N f L ×foutput</formula><p>are parameters, and f output is the dimensionality of outputs. Eq. ( <ref type="formula" target="#formula_38">27</ref>) can be regarded as a weighted sum of combing node-level features. One advantage is that the model can learn different weights for different nodes, at the cost of being unable to guarantee order invariance.</p><p>6. This is also related to graph coarsening, i.e. reducing a large graph to a smaller graph, since the graph-level representation can be obtained by coarsening the graph to a single node. Some papers use two terms exchangeably.  Reprinted from <ref type="bibr" target="#b62">[62]</ref> with permission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Hierarchical clustering</head><p>Rather than a dichotomy between node or graph level structures, graphs are known to exhibit rich hierarchical structures <ref type="bibr" target="#b88">[88]</ref>, which can be explored by hierarchical clustering methods as shown in Figure <ref type="figure" target="#fig_3">4</ref>. For example, a density-based agglomerative clustering <ref type="bibr" target="#b89">[89]</ref> is used in Bruna et al. <ref type="bibr" target="#b46">[46]</ref> and multi-resolution spectral clustering <ref type="bibr" target="#b90">[90]</ref> is used in Henaff et al. <ref type="bibr" target="#b47">[47]</ref>. ChebNet <ref type="bibr" target="#b48">[48]</ref> and MoNet <ref type="bibr" target="#b60">[60]</ref> adopt Graclus <ref type="bibr" target="#b91">[91]</ref>, another greedy hierarchical clustering algorithm to merge two nodes at a time, together with a fast pooling method by rearranging the nodes into a balanced binary tree. ECC <ref type="bibr" target="#b69">[69]</ref> adopts another hierarchical clustering method by eigen-decomposition <ref type="bibr" target="#b92">[92]</ref>. However, these hierarchical clustering methods are all independent of the convolution operation, i.e. can be done as a pre-processing step and not trained end-to-end.</p><p>To solve that problem, DiffPool <ref type="bibr" target="#b62">[62]</ref> proposes a differentiable hierarchical clustering algorithm jointly trained with graph convolutions. Specifically, the authors propose learning a soft cluster assignment matrix in each layer using the hidden representations:</p><formula xml:id="formula_40">S l = F A l , H l ,<label>(28)</label></formula><p>where S l ∈ R N l ×N l+1 is the cluster assignment matrix, N l is the number of clusters in layer l and F(•) is a function to be learned. Then, the node representations and new adjacency matrix for this "coarsened" graph can be obtained by taking the average according to S l as follows:</p><formula xml:id="formula_41">H l+1 = (S l ) T Ĥl+1 , A l+1 = (S l ) T A l S l , (<label>29</label></formula><formula xml:id="formula_42">)</formula><p>where Ĥl+1 is obtained by applying a convolution layer to H l , i.e. coarsening the graph from N l nodes to N l+1 nodes in each layer after the convolution operation. However, since the cluster assignment is soft, the connections between clusters are not sparse and the time complexity of the method is O(N 2 ) in principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Others</head><p>Besides the aforementioned methods, there are other readout operations worthy of discussion.</p><p>In GNNs <ref type="bibr" target="#b23">[23]</ref>, the authors suggest adding a special node that is connected to all nodes to represent the whole graph. Similarly, GNs <ref type="bibr" target="#b8">[9]</ref> take the idea of directly learning the representation of the whole graph by receiving messages from all nodes and edges.</p><p>MPNNs adopt set2set <ref type="bibr" target="#b93">[93]</ref>, a modification of seq2seq model that is invariant to the order of inputs. Specifically, set2set uses a Read-Process-and-Write model that receives all inputs at once, computes internal memories using an attention mechanism and LSTM, and then writes the outputs.</p><p>As mentioned in Section 4.1.3, PATCHY-SAN <ref type="bibr" target="#b53">[53]</ref> takes the idea of imposing an order for nodes using a graph labeling procedure and then resorts to standard 1-D pooling as in CNNs. . An illustration of the multi-head attentions proposed in GAT <ref type="bibr" target="#b63">[63]</ref> where each color denotes an independent attention. Reprinted with permission.</p><p>Whether this method can preserve order invariance depends on the graph labeling procedure, which is another research field that we refer readers to <ref type="bibr" target="#b94">[94]</ref> for a survey. However, imposing an order for nodes may not be a natural choice for graphs and could hinder the performance of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Summary</head><p>In short, statistics like taking the average or sum are most simple readout operations, while hierarchical clustering algorithms jointly trained with graph convolutions are more advanced but are more sophisticated solutions. Other methods like adding a pseudo node or imposing an order exist as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Improvements and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Attention Mechanism</head><p>In aforementioned GCNs, the neighborhoods of nodes are aggregated with equal or pre-defined weights. However, the influence of neighbors can vary greatly, which should be learned during training than pre-determined. Inspired by the attention mechanism <ref type="bibr" target="#b95">[95]</ref>, Graph Attention Network (GAT) <ref type="bibr" target="#b63">[63]</ref> introduces attentions into GCNs by modifying the convolution in Eq (12) as follows:</p><formula xml:id="formula_43">h l+1 i = ρ j∈ N (i) α l ij h l j Θ l ,<label>(30)</label></formula><p>where α l ij is node v i 's attention to node v j in layer l defined as:</p><formula xml:id="formula_44">α l ij = exp LeakyReLU F h l i Θ l , h l j Θ l k∈ N (i) exp LeakyReLU F h l i Θ l , h l k Θ l ,<label>(31)</label></formula><p>where F(•, •) is another function to be learned such as a small fully connected network. The authors also suggest using multiple independent attentions and concatenating the results to improve model capacity and stability, i.e. the multi-head attention in <ref type="bibr" target="#b95">[95]</ref>, as illustrated in Figure <ref type="figure">5</ref>. GaAN <ref type="bibr" target="#b64">[64]</ref> further proposes to learn different weights for different heads, and applies their method to the traffic forecasting problem using RNN to generate the outputs. HAN <ref type="bibr" target="#b65">[65]</ref> proposes a two-level attention mechanism for heterogeneous graphs. Specifically, the node-level attention is similar to GAT, but considers the types of nodes. As a result, it can assign weights in aggregating meta-path-based neighbors for nodes. The semantic-level attention then learns the importance of different meta-paths and outputs the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Residual and Jumping Connections</head><p>Many works observe that the most suitable depth for existing GCNs is often very limited, e.g. two or three layers, potentially due to the practical difficulty in training GCNs or the over-smoothing problem, i.e. all nodes have the same representation in deeper layers <ref type="bibr" target="#b68">[68]</ref>, <ref type="bibr" target="#b76">[76]</ref>. To remedy this problem, residual connections similar to ResNet <ref type="bibr" target="#b96">[96]</ref> can be added to skip layers. For example, <ref type="bibr" target="#b49">[49]</ref> add residual connections into Eq. ( <ref type="formula" target="#formula_20">13</ref>) as follows:</p><formula xml:id="formula_45">H l+1 = ρ D− 1 2 Ã D− 1 2 H l Θ l + H l . (<label>32</label></formula><formula xml:id="formula_46">)</formula><p>They show experimentally that adding such residual connections can increase the depth of the network, which is similar to the results of ResNet.</p><p>Column Network (CLN) <ref type="bibr" target="#b66">[66]</ref> takes a similar idea using the following residual connections with learnable weights:</p><formula xml:id="formula_47">h l+1 i = α l i h l+1 i + (1 − α l i ) h l i ,<label>(33)</label></formula><p>where h l+1 i is calculated similar to Eq. ( <ref type="formula" target="#formula_20">13</ref>) and α l i are weights calculated as follows:</p><formula xml:id="formula_48">α l i = ρ b l α + Θ l α h l i + Θ l α j∈N (i) h l j ,<label>(34)</label></formula><p>where b l α , Θ l α , Θ l α are parameters. Note that Eq. ( <ref type="formula" target="#formula_47">33</ref>) is very similar to the GRU as in GGS-NNs <ref type="bibr" target="#b25">[25]</ref>, but the overall architecture is still as GCN instead of pseudo time.</p><p>Inspired by personalized PageRank, PPNP <ref type="bibr" target="#b67">[67]</ref> defines graph convolution with teleportation to the initial layer:</p><formula xml:id="formula_49">H l+1 = (1 − α) D− 1 2 Ã D− 1 2 H l + αH 0 ,<label>(35)</label></formula><p>where H 0 = F θ (F V ) and α is a hyper-parameter. Note that all parameters are in F θ (F V ) rather than in graph convolutions. Jumping Knowledge Networks (JK-Nets) <ref type="bibr" target="#b68">[68]</ref> propose another architecture to connect the last layer of the network with all lower hidden layers, i.e. "jumping" all representations to the final output as illustrated in Figure <ref type="figure">6</ref>. In this way, the model can learn to selectively exploit information from different layers. Formally, JK-Nets can be formulated as:</p><formula xml:id="formula_50">h final i = AGGREGATE(h 0 i , h 1 i , ..., h L i ),<label>(36)</label></formula><p>where h final i are the final representation for node v i , AGGREGATE(•) is the aggregating function and L is the number of hidden layers. JK-Nets use three aggregating functions similar to GraphSAGE <ref type="bibr" target="#b59">[59]</ref>: concatenation, max-pooling, and LSTM attention. Experimental results show that adding jumping connections can improve the performance of multiple GCN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Edge Features</head><p>The aforementioned GCNs mostly focus on utilizing node features and graph structures. In this section, we briefly discuss how to use another important source of information, the edge features.</p><p>For simple edge features with discrete values such as edge types, a straight-forward method is to train different parameters for different edge types and aggregate the results. For example, Neural FPs <ref type="bibr" target="#b52">[52]</ref> train different parameters for nodes with different degrees, which corresponds to the hidden edge feature of bond types in a molecular graph, and sum over the results. CLN <ref type="bibr" target="#b66">[66]</ref> trains different parameters for different types of edges in a heterogeneous graph and averages the results. Edge-Conditioned Convolution (ECC) <ref type="bibr" target="#b69">[69]</ref> also trains different parameters based on edge types and applies them to graph classification. Relational Fig. <ref type="figure">6</ref>. Jumping Knowledge Networks proposed in <ref type="bibr" target="#b68">[68]</ref> where the last layer is connected to all layers to selectively exploit different information. GC stands for graph convolutions. Reprinted with permission.</p><p>GCNs (R-GCNs) [70] take a similar idea in knowledge graphs by training different weights for different relation types and add regularizations to reduce the number of parameters. However, these methods can only handle limited discrete edge features.</p><p>DCNN <ref type="bibr" target="#b56">[56]</ref> proposes another method to convert each edge into a node connected to the head and tail node of the edge. Then, edge features can be treated as node features.</p><p>Similarly, LGCN <ref type="bibr" target="#b71">[71]</ref> constructs a line graph B ∈ R 2M ×2M to incorporate edge features as follows:</p><formula xml:id="formula_51">B i→j,i →j = 1 if j = i and j = i, 0 otherwise,<label>(37)</label></formula><p>i.e. nodes in the line graph are directed edges in the original graph and two nodes in the line graph are connected if the information can flow through their corresponding edges in the original graph. Then, LGCN adopts two GCNs on the original graph and line graph, respectively. Kearnes et al. <ref type="bibr" target="#b61">[61]</ref> propose another architecture using the "weave module". Specifically, they learn representations for both nodes and edges and exchange information between them in each weave module with four different functions: Node-to-Node (NN), Node-to-Edge (NE), Edge-to-Edge (EE) and Edge-to-Node (EN):</p><formula xml:id="formula_52">h l i = F N N (h 0 i , h 1 i , ..., h l i ) h l i = F EN ({e l ij |j ∈ N (i)}) h l+1 i = F N N (h l i , h l i ) e l ij = F EE (e 0 ij , e 1 ij , ..., e l ij ) e l ij = F N E (h l i , h l j ) e l+1 ij = F EE (e l ij , e l ij ),<label>(38)</label></formula><p>where e l ij are representations for edge (v i , v j ) in the l th layer and F(•) are learnable functions with subscripts representing message-passing directions. By stacking multiple such modules, information can propagate through alternative passing between nodes and edges representations. Note that in Node-to-Node and Edge-to-Edge functions, jumping connections similar to JK-Nets <ref type="bibr" target="#b68">[68]</ref> are implicitly added. Graph Networks <ref type="bibr" target="#b8">[9]</ref> also propose learning edge representation and update both nodes and edges representations using message-passing functions as discussed in Section 4.1.4, which contain the "weave module" as a special case by not learning the whole graph representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Sampling Methods</head><p>One critical bottleneck of training GCNs for large-scale graphs is efficiency. As shown in Section 4.1.4, many GCNs follow the framework of aggregating information from neighborhoods. However, since many real graphs follow the power-law distribution <ref type="bibr" target="#b97">[97]</ref>, i.e. few nodes have very large degrees, the expansion of neighbors can grow extremely fast. To deal with this problem, two kinds of sampling methods have been proposed: neighborhood sampling and layer-wise sampling, as illustrated in Figure <ref type="figure" target="#fig_6">7</ref>.</p><p>In neighborhood samplings, the sampling is performed for each node during the calculation. GraphSAGE <ref type="bibr" target="#b59">[59]</ref> uniformly samples a fixed number of neighbors for each node during training. PinSage <ref type="bibr" target="#b72">[72]</ref> proposes sampling neighbors using random walks on graphs together with several implementation improvements, e.g. coordination between CPU and GPU, a map-reduce inference pipeline, etc. PinSage is shown to be capable of handling a real billion-scale graph. StochasticGCN <ref type="bibr" target="#b73">[73]</ref> further proposes to reduce the sampling variances by using historical activations in the last batches as a control variate, allowing for arbitrarily small sample sizes with a theoretical guarantee.</p><p>Instead of sampling neighbors of nodes, FastGCN <ref type="bibr" target="#b74">[74]</ref> adopts a different strategy to sample nodes in each convolutional layer, i.e. layer-wise sampling, by interpreting nodes as i.i.d. samples and graph convolutions as integral transforms under probability measures. FastGCN also shows that sampling nodes via their normalized degrees can reduce variances and lead to better performance. Adapt <ref type="bibr" target="#b75">[75]</ref> further proposes to sample nodes in the lower layers conditioned on their top one, which is more adaptive and applicable for explicit variance reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5">Inductive Setting</head><p>Another important aspect of GCNs is applying to the inductive setting, i.e. training on a set of nodes/graphs and testing on another set of nodes/graphs unseen during training. In principle, this is achieved by learning a mapping function on the given features, which are not dependent on the graph basis and can be transferred across nodes/graphs. The inductive setting is verified in Graph-SAGE <ref type="bibr" target="#b59">[59]</ref>, GAT <ref type="bibr" target="#b63">[63]</ref>, GaAN <ref type="bibr" target="#b64">[64]</ref>, and FastGCN <ref type="bibr" target="#b74">[74]</ref>. However, existing inductive GCNs are only suitable for graphs with features. How to conduct inductive learning for graphs without features, usually called the out-of-sample problem <ref type="bibr" target="#b98">[98]</ref>, largely remains open in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.6">Theoretical Analysis</head><p>To understand the effectiveness of GCNs, some theoretical analysis has been proposed, which can be divided into three categories: node-focused tasks, graph-focused tasks, and general analysis.</p><p>For node-focused tasks, Li et al. <ref type="bibr" target="#b76">[76]</ref> first analyze the performance of GCNs as a special form of Laplacian smoothing, which makes the features of vertices in the same cluster similar. The original Laplacian smoothing can be formulated as:</p><formula xml:id="formula_53">h i = (1 − γ)h i + γ j∈N (i) 1 d i h j ,<label>(39)</label></formula><p>where h i and h i are the original and smoothed features of node v i , respectively. We can see that Eq. ( <ref type="formula" target="#formula_53">39</ref>) is very similar to the graph convolution proposed by Kipf and Welling in Eq. ( <ref type="formula" target="#formula_19">12</ref>). Based on this insight, Li et al. also propose a co-training and a self-training method for GCN. Recently, Wu et al. <ref type="bibr" target="#b77">[77]</ref> analyze GCNs from the signal processing perspective. By regarding node features as graph signals, they show that Eq. ( <ref type="formula" target="#formula_19">12</ref>) is basically a fixed low-pass filter. Using this insight, an extremely simplified graph convolution (SGC) is proposed by removing all non-linearities and collapsing learning parameters into one matrix:</p><formula xml:id="formula_54">H L = D− 1 2 Ã D− 1 2 L F V Θ. (<label>40</label></formula><formula xml:id="formula_55">)</formula><p>The authors show that such a "non-deep-learning" GCN variant achieves comparable performance in many tasks. <ref type="bibr" target="#b78">[78]</ref> enhances this result by showing that the low-pass filtering nature does not equip GCNs with non-linear manifold learning property, and further proposes GFNN to remedy this problem by adding an MLP after the graph convolution layer. For graph-focused tasks, <ref type="bibr" target="#b49">[49]</ref> and <ref type="bibr" target="#b55">[55]</ref> both consider the relationship between GCNs and graph kernels such as the Weisfeiler-Lehman (WL) kernel <ref type="bibr" target="#b84">[84]</ref>, which is widely used in graph isomorphism tests. They show that GCNs are conceptually generalization of the WL kernel since both methods iteratively aggregate information from node neighbors. Xu et al. <ref type="bibr" target="#b79">[79]</ref> formalize this idea by proving that WL kernel provides an upper bound for GCNs in terms of distinguishing graph structures. Based on the analysis, Xu et al. propose Graph Isomorphism Network (GIN) and show that a simple readout operation using sum and MLP can achieve provably maximum discriminative power, i.e. training accuracy in graph classification tasks.</p><p>For general analysis, Scarselli et al. <ref type="bibr" target="#b99">[99]</ref> show the Vapnik Chervonenkis dimension (VC-dim) of GCNs with different activation functions, which are comparable to standard RNNs. Chen et al. <ref type="bibr" target="#b71">[71]</ref> analyze the optimization landscape of linear GCNs and show that any local minimum is relatively close to the global minimum under certain simplifications. Verma and Zhang <ref type="bibr" target="#b100">[100]</ref> analyze the algorithmic stability and generalization bound of GCNs. They show that single-layer GCNs satisfy the strong notion of uniform stability if the largest absolute eigenvalue of graph convolution filters is independent of the graph size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">GRAPH AUTOENCODERS (GAES)</head><p>Autoencoder (AE) and its variations are widely used for unsupervised learning <ref type="bibr" target="#b101">[101]</ref>, which are suitable to learn node representations for graphs. The implicit assumption is that graphs have an inherent, potentially non-linear low-rank structure. In this section, we will first elaborate graph autoencoders and then introduce graph variational autoencoders and other improvements. The main characteristics of GAEs are summarized in Table <ref type="table" target="#tab_6">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Autoencoders</head><p>The use of AEs for graphs is originated from Sparse Autoencoder (SAE) <ref type="bibr" target="#b102">[102]</ref>. The basic idea is that, by regarding the adjacency matrix or its variations as the raw features of nodes, AEs can be leveraged as a dimension reduction technique to learn lowdimensional node representations. Specifically, SAE adopts the following L2-reconstruction loss:</p><formula xml:id="formula_56">min Θ L 2 = N i=1 P (i, :) − P (i, :) 2 P (i, :) = G (h i ) , h i = F (P (i, :)) ,<label>(41)</label></formula><p>where P is the transition matrix, P is the reconstructed matrix,</p><formula xml:id="formula_57">h i ∈ R d is the low-dimensional representation of node v i , F(•) is the encoder, G(•) is the decoder, d</formula><p>N is the dimensionality and Θ are parameters. Both the encoder and decoder are multi-layer perceptrons with many hidden layers. In other words, SAE tries to compress the information of P(i, :) into a low-dimensional vector h i and reconstruct the original feature. Another sparsity regularization term is also added. After getting the low-dimensional representation h i , k-means <ref type="bibr" target="#b112">[112]</ref> is applied for the node clustering task, which proves empirically to outperform non-deep learning baselines. However, SAE is based on an incorrect theoretical analysis 7 and the mechanism underlying such effectiveness remains unexplained.</p><p>Structure Deep Network Embedding (SDNE) <ref type="bibr" target="#b103">[103]</ref> fills in the puzzle by showing that the L2-reconstruction loss in Eq. ( <ref type="formula" target="#formula_56">41</ref>) actually corresponds to the second-order proximity between nodes, i.e. two nodes share similar latten representations if they have similar neighborhoods, which is well studied in network science such as in collaborative filtering or triangle closure <ref type="bibr" target="#b4">[5]</ref>. Motivated by network embedding methods, which show that the first-order proximity is also important <ref type="bibr" target="#b114">[114]</ref>, SDNE modifies the objective function by adding another term similar to the Laplacian Eigenmaps <ref type="bibr" target="#b81">[81]</ref>:</p><formula xml:id="formula_58">min Θ L 2 + α N i,j=1 A(i, j) h i − h j 2 ,<label>(42)</label></formula><p>i.e. two nodes also need to share similar latten representations if they are directly connected. The authors also modify the L2reconstruction loss by using the adjacency matrix and assigning different weights to zero and non-zero elements:</p><formula xml:id="formula_59">L 2 = N i=1 (A (i, :) − G (h i )) b i 2 ,<label>(43)</label></formula><p>where</p><formula xml:id="formula_60">h i = F (A (i, :)), b ij = 1 if A(i, j) = 0, b ij = β &gt; 1</formula><p>else, and β is another hyper-parameter. The overall architecture of SDNE is shown in Figure <ref type="figure">8</ref>. Motivated by another line of works, a contemporary work DNGR <ref type="bibr" target="#b104">[104]</ref> replaces the transition matrix P in Eq. ( <ref type="formula" target="#formula_56">41</ref>) with 7. The original paper <ref type="bibr" target="#b102">[102]</ref> motivates the problem by analyzing the connection between spectral clustering and Singular Value Decomposition, which is mathematically incorrect as pointed out in <ref type="bibr" target="#b113">[113]</ref>. Fig. <ref type="figure">8</ref>. The framework of SDNE reprinted from <ref type="bibr" target="#b103">[103]</ref> with permission.</p><p>Both the first and second-order proximity of nodes are preserved using deep autoencoders.</p><p>the positive pointwise mutual information (PPMI) <ref type="bibr" target="#b85">[85]</ref> matrix of a random surfing probability. In this way, the raw features can be associated with some random walk probability of the graph <ref type="bibr" target="#b115">[115]</ref>. However, constructing the input matrix takes O(N 2 ) time complexity, which is not scalable to large-scale graphs. GC-MC <ref type="bibr" target="#b105">[105]</ref> further takes a different approach by using GCN in <ref type="bibr" target="#b49">[49]</ref> as the encoder:</p><formula xml:id="formula_61">H = GCN F V , A ,<label>(44)</label></formula><p>and the decoder is a simple bilinear function:</p><formula xml:id="formula_62">Â(i, j) = H(i, :)Θ de H(j, :) T ,<label>(45)</label></formula><p>where Θ de are parameters for the encoder. In this way, node features can be naturally incorporated. For graphs without node features, a one-hot encoding of node IDs can be utilized. The authors demonstrate the effectiveness of GC-MC on the recommendation problem of bipartite graphs. Instead of reconstructing the adjacency matrix or its variations, DRNE <ref type="bibr" target="#b106">[106]</ref> proposes another modification to directly reconstruct the low-dimensional vectors of nodes by aggregating neighborhood information using LSTM. Specifically, DRNE minimizes the following objective function:</p><formula xml:id="formula_63">L = N i=1 h i − LSTM ({h j |j ∈ N (i)}) .<label>(46)</label></formula><p>Since LSTM requires the inputs to be a sequence, the authors suggest ordering the neighborhoods of nodes according to their degrees. A sampling of neighbors is also adopted for nodes with large degrees to prevent the memory from being too long. The authors prove that such a method can preserve regular equivalence and many centrality measures of nodes such as PageRank <ref type="bibr" target="#b116">[116]</ref>.</p><p>Unlike the existing works that map nodes into low-dimensional vectors, Graph2Gauss (G2G) <ref type="bibr" target="#b107">[107]</ref> proposes encoding each node as a Gaussian distribution h i = N (M(i, :), diag (Σ(i, :))) to capture the uncertainties of nodes. Specifically, the authors use a deep mapping from node attributes to the means and variances of the Gaussian distribution as the encoder:</p><formula xml:id="formula_64">M(i, :) = F M (F V (i, :)), Σ(i, :) = F Σ (F V (i, :)),<label>(47)</label></formula><p>where F M (•) and F Σ (•) are parametric functions need to be learned. Then, instead of using an explicit decoder function, they use pairwise constraints to learn the model:</p><formula xml:id="formula_65">KL (h j ||h i ) &lt; KL (h j ||h i ) ∀i, ∀j, ∀j s.t. d(i, j) &lt; d(i, j ),<label>(48)</label></formula><p>where d(i, j) is the shortest distance from node v i to v j and KL[q(•)||p(•)] is the Kullback-Leibler (KL) divergence between q(•) and p(•) <ref type="bibr" target="#b117">[117]</ref>. In other words, the constraints ensure that KL-divergence between node pairs has the relative order as the graph However, since Eq. is hard optimize, an energy-based loss <ref type="bibr" target="#b118">[118]</ref> is resorted to as relaxation:</p><formula xml:id="formula_66">L = (i,j,j )∈D E 2 ij + exp −E ij ,<label>(49)</label></formula><p>where D = {(i, j, j )|d(i, j) &lt; d(i, j )} and E ij = KL(h j ||h i ).</p><p>An unbiased sampling strategy is further proposed to accelerate the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Variational Autoencoders</head><p>Different from the aforementioned autoencoders, Variational Autoencoder (VAE) is another type of deep learning method that combines dimension reduction with generative models, potential benefits including tolerating noise and learning smooth representations <ref type="bibr" target="#b119">[119]</ref>. VAE was first introduced into modeling graph data in <ref type="bibr" target="#b108">[108]</ref>, where the decoder is a simple linear product:</p><formula xml:id="formula_67">p (A|H) = N i,j=1 σ h i h T j ,<label>(50)</label></formula><p>where h i are assumed to follow a Gaussian posterior distribution q (h i |M, Σ) = N (h i |M(i, :), diag (Σ(i, :))). For the encoder of mean and variance matrices, the authors adopt GCN in <ref type="bibr" target="#b49">[49]</ref>:</p><formula xml:id="formula_68">M = GCN M F V , A , log Σ = GCN Σ F V , A .<label>(51)</label></formula><p>Then, the model parameters can be learned by minimizing the variational lower bound <ref type="bibr" target="#b119">[119]</ref>: However, since the full graph needs to be reconstructed, the time complexity is O(N 2 ).</p><formula xml:id="formula_69">L = E q(H|F V ,A) [log p (A|H)] − KL q H|F V , A ||p(H) .<label>(52)</label></formula><p>Motivated by SDNE and G2G, DVNE <ref type="bibr" target="#b109">[109]</ref> proposes another VAE for graph data also by representing each node as a Gaussian distribution. Unlike the existing works that adopt KL-divergence as the measurement, DVNE uses Wasserstein distance <ref type="bibr" target="#b120">[120]</ref> to preserve the transitivity of nodes' similarities. Similar to SDNE and G2G, DVNE also preserves both the first and second-order proximity in the objective function:</p><formula xml:id="formula_70">min Θ (i,j,j )∈D E 2 ij + exp −E ij + αL 2 ,<label>(53)</label></formula><p>where</p><formula xml:id="formula_71">E ij = W 2 (h j ||h i )</formula><p>is the 2 nd Wasserstein distance between two Gaussian distributions h j and h i and D = {(i, j, j )|j ∈ N (i), j / ∈ N (i)} is a set of all triples corresponding to the ranking loss of the first-order proximity. The reconstruction loss is defined as: L 2 = inf q(Z|P) E p(P) E q(Z|P) P (P − G(Z)) 2 2 , (54) where P is the transition matrix and Z are samples drawn from H. The framework is shown in Figure <ref type="figure" target="#fig_7">9</ref>. Then, the objective function can be minimized as conventional VAEs using the reparameterization trick <ref type="bibr" target="#b119">[119]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Improvements and Discussions</head><p>Besides these two main categories, there are also several improvements that are worthy of discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Adversarial Training</head><p>The adversarial training scheme 8 is incorporated into GAEs as an additional regularization term in <ref type="bibr" target="#b110">[110]</ref>. The overall architecture is shown in Figure <ref type="figure" target="#fig_0">10</ref>. Specifically, the encoder of GAEs is used as the generator, and the discriminator aims to distinguish whether a latent representation comes from the generator or a prior distribution. In this way, the autoencoder is forced to match the prior distribution as regularization. The objective function is:</p><formula xml:id="formula_72">min Θ L 2 + αL GAN ,<label>(55)</label></formula><p>where L 2 is similar to the reconstruction loss defined in GAEs, and</p><formula xml:id="formula_73">L GAN is min G max D E h∼p h [log D(h)] + E z∼G(F V ,A) [log (1 − D (z))] ,<label>(56)</label></formula><p>8. We will discuss adversarial methods for graphs in general in Section 7.</p><p>where G F V , A is a generator using the graph convolutional encoder in Eq. ( <ref type="formula" target="#formula_68">51</ref>), D(•) is a discriminator with the crossentropy loss and p h is the prior distribution. In the paper, a simple Gaussian prior is adopted and experimental results demonstrate the effectiveness of the adversarial training scheme. Concurrently, NetRA <ref type="bibr" target="#b111">[111]</ref> also proposes using GAN to enhance the generalization of graph autoencoders. Specifically, the authors use the following objective</p><formula xml:id="formula_74">min Θ L 2 + α 1 L LE + α 2 L GAN ,<label>(57)</label></formula><p>where L LE is the Laplacian Eigenmaps objective function as in Eq. ( <ref type="formula" target="#formula_58">42</ref>). In addition, the authors adopt LSTM as the encoder to aggregate information from neighborhoods similar to Eq. ( <ref type="formula" target="#formula_63">46</ref>). Instead of only sampling immediate neighbors and ordering nodes using degrees as DRNE <ref type="bibr" target="#b106">[106]</ref>, the authors use random walks to generate the input sequences for LSTM. NetRA considers GAEs as the ground-truth and adopts random Gaussian noise followed by a small fully connected network as the generator, just the opposite to ARGA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Inductive Learning and GCN encoder</head><p>Similar to GCNs, GAEs can be applied to the inductive setting if node attributes are incorporated in the encoder. This can be achieved by using GCNs as the encoder such as in <ref type="bibr" target="#b105">[105]</ref>, <ref type="bibr" target="#b108">[108]</ref>, <ref type="bibr" target="#b110">[110]</ref>, or directly learning a mapping function from node features as in <ref type="bibr" target="#b107">[107]</ref>. Since the edge information is only utilized in learning the parameters, the model can be applied to nodes unseen during training. These works also show that, although GCNs and GAEs are based on different architectures, it is possible to use them jointly, which we believe is a promising future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Similarity Measures</head><p>In GAEs, many similarity measures are adopted, for example, L2-reconstruction loss, Laplacian Eigenmaps, the ranking loss for graph AEs, and KL divergence and Wasserstein distance for graph VAEs. Although these similarity measures are based on different motivations, how to choose an appropriate similarity measure for a given task and model architecture remains unstudied. More research to understand the underlying differences between these metrics is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">GRAPH REINFORCEMENT LEARNING</head><p>One aspect of deep learning that has not been discussed so far is reinforcement learning (RL), which has been shown effective in AI tasks such as game playing <ref type="bibr" target="#b127">[127]</ref>. RL is known to be good at learning from feedbacks, especially handling non-differentiable objectives and constraints. In this section, we review Graph RL methods, whose main characteristics are summarized in Table <ref type="table">6</ref>. GCPN <ref type="bibr" target="#b121">[121]</ref> utilizes RL for goal-directed molecular graph generations to deal with non-differential objectives and constraints. Specifically, the graph generation is modeled as a Markov decision process of adding nodes and edges, and the generative model is regarded as an RL agent operating in the graph generation environment. By resembling agent actions as link predictions, using domain-specific as well as adversarial rewards and using GCNs to learn node representations, GCPN can be trained end-toend using policy gradient <ref type="bibr" target="#b128">[128]</ref>.</p><p>A concurrent work, MolGAN <ref type="bibr" target="#b122">[122]</ref>, takes a similar idea of using RL for generating molecular graphs. Instead of generating Fig. <ref type="figure" target="#fig_0">10</ref>. The framework of ARGA/ARVGA reprinted from <ref type="bibr" target="#b110">[110]</ref> with permission. The adversarial training scheme is incorporated into GAEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 6</head><p>The Main Characteristics of Graph Reinforcement Learning</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Task Actions Rewards Scalability GCPN <ref type="bibr" target="#b121">[121]</ref> Graph generation Link prediction GAN + domain knowledge No MolGAN <ref type="bibr" target="#b122">[122]</ref> Graph generation Generate the whole graph GAN + domain knowledge No GTPN <ref type="bibr" target="#b123">[123]</ref> Chemical reaction prediction Predict node pairs and new bonds Prediction results No GAM <ref type="bibr" target="#b124">[124]</ref> Graph classification Predict graph labels and select the next node Classification results Yes DeepPath <ref type="bibr" target="#b125">[125]</ref> Knowledge graph reasoning Predict the next node of the reasoning path Reasoning results + diversity Yes MINERVA <ref type="bibr" target="#b126">[126]</ref> Knowledge graph reasoning Predict the next node of the reasoning path Reasoning results Yes the graph by a sequence of actions, MolGAN proposes to directly generate the full graph, working well for small molecules. GTPN <ref type="bibr" target="#b123">[123]</ref> adopts RL for predicting chemical reaction products. Specifically, the agent acts to select node pairs in the molecule graph and predict their new bond types, while the reward is given both immediately and in the end based on whether the predictions are correct. GTPN also adopts a GCN to learn node representations and an RNN to memorize the prediction sequence.</p><p>GAM <ref type="bibr" target="#b124">[124]</ref> applies RL into graph classification by using random walks to classify graphs and modeling the generation of random walks as a Partially Observable Markov Decision Process. The agent performs two actions: predicting the label of the graph and selecting the next node in the random walk. The reward simply is whether the agent correctly classifies the graph, i.e.</p><formula xml:id="formula_75">J (θ) = E P (S 1:T ;θ) T t=1 r t ,<label>(58)</label></formula><p>where r t = 1 stands for a correct prediction, r t = −1 otherwise, T is the total time steps, and S t is the environment. DeepPath <ref type="bibr" target="#b125">[125]</ref> and MINERVA <ref type="bibr" target="#b126">[126]</ref> both adopt RL for knowledge graph (KG) reasoning. Specifically, DeepPath aims at pathfinding, i.e. find the most informative path between two target nodes, while MINERVA tackles question answering, i.e. find the correct answer node given a question node and a relation. RL agents in both methods need to output a reasoning path in the KG by predicting the next node in the path at each step and receive rewards if the paths reach the correct destinations. DeepPath also adds a regularization term to encourage the diversity of the paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">GRAPH ADVERSARIAL METHODS</head><p>Adversarial methods such as Generative Adversarial Networks (GANs) <ref type="bibr" target="#b137">[136]</ref> or adversarial attacks have drawn increasing attention in the machine learning community in the past few years. In this section, we review how to apply adversarial methods to graphs, whose main characteristics are summarized in Table <ref type="table" target="#tab_8">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Adversarial training</head><p>The basic idea of GAN is to build two linked models: a discriminator and a generator. The goal of the generator is to "fool" the discriminator by generating fake data, while the discriminator aims to distinguish whether a sample comes from real data or is generated by the generator. Then, both models can benefit from each other by joint training using a minimax game. Adversarial training is shown effective in generative models and enhancing the generalization ability of discriminative models. In Section 5.3.1 and Section 6, we have reviewed how to use adversarial training schemes in GAEs and Graph RL, respectively. Here, we review several other adversarial training methods on graphs in detail.</p><p>GraphGAN <ref type="bibr" target="#b129">[129]</ref> proposes using GAN to enhance graph embedding methods <ref type="bibr" target="#b17">[17]</ref> with the following objective function:</p><formula xml:id="formula_76">min G max D N i=1 E v∼p graph (•|vi) [log D(v, v i )] + E v∼G(•|vi) [log (1 − D (v, v i ))] .<label>(59)</label></formula><p>The discriminator D(•) the generator G(•)</p><formula xml:id="formula_77">D(v, v i ) = σ(d v d T vi ), G(v|v i ) = exp(g v g T vi ) v =vi exp(g v g T vi ) ,<label>(60)</label></formula><p>where d v and g v are low-dimensional embedding vectors for node v in the discriminator and the generator, respectively. Combining the above equations, the discriminator actually has two objectives: node pairs in the original graph should have large similarities and node pairs generated by the generator should have small similarities. Such architecture is similar to network embedding methods such as LINE <ref type="bibr" target="#b114">[114]</ref>, except that we use the generator G(  Regularization for GAEs Yes Yes NetRA <ref type="bibr" target="#b111">[111]</ref> Regularization for GAEs Yes No GCPN <ref type="bibr" target="#b121">[121]</ref> Rewards for Graph RL No Yes MolGAN <ref type="bibr" target="#b122">[122]</ref> Rewards for Graph RL No Yes GraphGAN <ref type="bibr" target="#b129">[129]</ref> Generate negative samples (node pairs) Yes No ANE <ref type="bibr" target="#b130">[130]</ref> Regularization for network embedding No No GraphSGAN <ref type="bibr" target="#b131">[131]</ref> Enhancing semi-supervised learning on graphs Yes Yes NetGAN <ref type="bibr" target="#b132">[132]</ref> Generate graphs via random walks No No Adversarial Attack</p><p>Nettack <ref type="bibr" target="#b133">[133]</ref> Targeted attacks of graph structures and node attributes Yes Yes Dai et al. <ref type="bibr" target="#b135">[134]</ref> Targeted attacks of graph structures Yes No Zugner and Gunnemann <ref type="bibr" target="#b136">[135]</ref> Non-targeted attacks of graph structures No Yes additional regularization to existing network embedding methods, such as DeepWalk <ref type="bibr" target="#b138">[137]</ref> by imposing a prior distribution as real data and regarding embedding vectors as generated samples.</p><p>GraphSGAN <ref type="bibr" target="#b131">[131]</ref> proposes using GAN to enhance semisupervised learning on graphs. Specifically, GraphSGAN observes that fake nodes should be generated in the density gap between subgraphs so that the propagation effect of existing models across different clusters in the graph is weakened. To achieve that, the authors design a new optimization objective with several sophisticated loss terms to ensure that the generator generates samples in density gaps at equilibrium.</p><p>NetGAN <ref type="bibr" target="#b132">[132]</ref> adopts GAN for generating graphs. Specifically, the authors regard graph generation as learning the distribution of biased random walks and propose a GAN framework for generating and discriminating random walks using LSTM. Experiments show that such a generative model through random walks can also learn global network patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Adversarial attacks</head><p>Adversarial attacks are another class of adversarial methods to deliberately "fool" targeted methods by adding small perturbations to data. Studying adversarial attacks can subsequently deepen our understanding of existing models and inspire more robust architectures. Next, we review the graph-based adversarial attacks.</p><p>Nettack <ref type="bibr" target="#b133">[133]</ref> first proposes attacking node classification models such as GCNs by modifying graph structures and node attributes. Specifically, denoting the targeted node as v 0 , its true class as c true , the targeted model as F(A, F V ), and its loss function as L F (A, F V ), the following objective function is adopted:</p><formula xml:id="formula_78">argmax (A ,F V )∈P max c =ctrue log Z * v0,c − log Z * v0,ctrue s.t. Z * = F θ * (A , F V ), θ * = argmin θ L F (A , F V ),<label>(61)</label></formula><p>where A , F V are the modified adjacency matrix and node feature matrix, Z are classification probabilities predicted by F(•) and P is the space determined by the attack constrains. Simply speaking, the optimization aims to find the best legit changes in graph structures and node attributes so that v 0 is misclassified and θ * indicates that the attack is causative, i.e. the attack is before training the targeted model. The authors propose several constraints for the attacks. The most important constraint is that the attack should be "unnoticeable", i.e. only small changes should be added. Specifically, the authors propose to preserve data characteristics including node degree distributions and feature co-occurrences. Two attacking scenarios, namely direct attack (directly attacking v 0 ) and influence attack (only attacking other nodes), and several relaxations to make the optimization tractable are also proposed. Concurrently, Dai et al. <ref type="bibr" target="#b135">[134]</ref> also study adversarial attacks for graphs with a similar objective function as Eq. ( <ref type="formula" target="#formula_78">61</ref>), but focus on the case of only changing graph structures. Instead of assuming that the attacker has all the information, the authors consider several settings with different amounts of information available. The most effective strategy, RL-S2V, adopts structure2vec <ref type="bibr" target="#b139">[138]</ref> to learn node and graph representations and uses reinforcement learning to solve the optimization. Experimental results show that the attack is effective for both node and graph classification tasks.</p><p>The aforementioned two attacks are targeted, i.e. to misclassify some targeted node v 0 . Zugner and Gunnemann <ref type="bibr" target="#b136">[135]</ref> first study non-targeted attacks, i.e. aiming to reduce the global performance of the model, by treating the graph structure as hyper-parameters to optimize and using meta-gradients in the optimization. Several techniques are utilized to approximate the meta-gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">DISCUSSION AND CONCLUSION</head><p>So far, we have reviewed the different architectures of graph-based deep learning methods as well as their differences and connections. Next, we briefly discuss their applications, implementations, and future directions before we summarize the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Applications</head><p>Besides standard graph inference tasks such as node or graph classification 9 , graph-based deep learning methods have also been applied to a wide range of disciplines, such as modeling social influence <ref type="bibr" target="#b140">[139]</ref>, recommendation <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b72">[72]</ref>, <ref type="bibr" target="#b105">[105]</ref>, <ref type="bibr" target="#b141">[140]</ref>, chemistry and biology <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b61">[61]</ref>, <ref type="bibr" target="#b121">[121]</ref>, <ref type="bibr" target="#b122">[122]</ref>, physics <ref type="bibr" target="#b142">[141]</ref>, <ref type="bibr" target="#b143">[142]</ref>, disease or drug prediction <ref type="bibr" target="#b144">[143]</ref>- <ref type="bibr" target="#b146">[145]</ref>, gene expression <ref type="bibr" target="#b147">[146]</ref>, natural language processing (NLP) <ref type="bibr" target="#b148">[147]</ref>, <ref type="bibr" target="#b149">[148]</ref>, computer vision <ref type="bibr" target="#b150">[149]</ref>- <ref type="bibr" target="#b154">[153]</ref>, traffic forecasting <ref type="bibr" target="#b155">[154]</ref>, <ref type="bibr" target="#b156">[155]</ref>, program induction <ref type="bibr" target="#b157">[156]</ref> and solving graph-based NP problems <ref type="bibr" target="#b158">[157]</ref>, <ref type="bibr" target="#b159">[158]</ref>.</p><p>Though a thorough review of these methods is beyond the scope of this paper due to the diversity of these applications, we list several key inspirations. First, it is important to incorporate domain knowledge into the model, e.g. in constructing the graph or choosing architectures. For example, building a graph based on relative distance may be suitable for traffic forecasting problems, but may not work well for a weather prediction problem because the geographical location is also important. Second, the graphbased model can usually be built on top of other architectures rather than working alone. For example, the computer vision 9. We have collected a list of methods for common tasks in the appendix. community usually adopts CNNs to detect objects and then uses graph-based deep learning as a reasoning module <ref type="bibr" target="#b43">[43]</ref>. On the other hand, GCNs can be adopted as syntactic constraints for NLP problems <ref type="bibr" target="#b148">[147]</ref>, <ref type="bibr" target="#b149">[148]</ref>. As a result, how to integrate different models is usually the key challenge. These applications also show that graph-based deep learning not only empowers us to mine the rich value underlying existing graph data but also helps to advance other disciplines by naturally modeling relational data as graphs, greatly widening the applicability of graph-based deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Implementations</head><p>Recently, there are several open libraries for deep learning on graphs, which we list in Table <ref type="table">8</ref>. We also collect a list of the source codes for papers discussed in this paper, mostly by their original authors, in the appendix. These open implementations make it easy to learn, compare, and improve different methods. Some implementations also address the problem of distributed computing, which we do not discuss in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Future Directions</head><p>There are also several on-going or future directions which are worthy of discussions:</p><p>• New models for unstudied graph structures. Due to the extremely diverse structures of graph data, the existing methods cannot handle all of them. For example, most methods focus on homogeneous graphs, while heterogeneous graphs are seldom studied, especially those containing different modalities like in <ref type="bibr" target="#b163">[162]</ref>. Signed networks, where negative edges represent conflicts between nodes, also have unique structures and pose additional challenges to the existing methods <ref type="bibr" target="#b164">[163]</ref>. Hyper-graphs, representing complex relations between more than two objects <ref type="bibr" target="#b165">[164]</ref>, are also understudied. An important next step is to design specific deep learning models to handle these different types of graphs.</p><p>• Compositionality of existing models. As shown in many sections, many existing architectures can work together, for example using GCN as a layer in GAEs or Graph RL. Besides designing new building blocks, how to systematically composite these architectures is an interesting future direction. In this process, how to incorporate interdisciplinary knowledge in a principled way rather than case by case is also an open problem. A recent work, Graph Networks <ref type="bibr" target="#b8">[9]</ref>, takes the first step and focuses on using a general framework of GNNs and GCNs for relational reasoning problems. AutoML may also be helpful by lighting human burdens in assembling different components and choosing hyper-parameters <ref type="bibr" target="#b166">[165]</ref>.</p><p>• Dynamic graphs. Most existing methods focus on static graphs. However, many real graphs are dynamic in nature, where nodes, edges and their features can change over time. For example, in social networks, people may establish new social relations, remove old edges and their features like hobbies and occupations can change over time. New users may join the network while old users can leave. How to model the evolving characteristics of dynamic graphs and support incrementally updating model parameters largely remains open in the literature. Some preliminary works try to tackle this problem using Graph RNN architectures with encouraging results <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b30">[30]</ref>.</p><p>• Interpretability and Robustness. Since graphs are often related to other risk-sensitive scenarios, interpreting deep learning models on graphs is critical towards decision-making problems. For example, in medicine or disease-related problems, interpretability is essential in transforming computer experiments into clinical usages. However, interpretability for graph-based deep learning is even more challenging than other black-box models since nodes and edges in the graph are heavily interconnected. In addition, since many existing deep learning models on graphs are sensitive to adversarial attacks as shown in Section 7.2, how to enhance the robustness of existing methods is another important issue. Some pioneering works for interpretability and robustness can be found in <ref type="bibr" target="#b167">[166]</ref> and <ref type="bibr" target="#b168">[167]</ref>, <ref type="bibr" target="#b169">[168]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Summary</head><p>Our above survey shows that deep learning on graphs is a promising and fast-developing research field, containing exciting opportunities as well as challenges. Studying deep learning on graphs provides a critical building block in modeling relational data, and is an important step towards better machine learning and artificial intelligence eras.</p><p>As shown in Appendix Section B, node classification is the most common task for graph-based deep learning models. Here, we report the results of different methods on five node classification benchmark datasets 10 :</p><p>• Cora, Citeseer, PubMed <ref type="bibr" target="#b171">[170]</ref>: These are citation graphs with nodes representing papers, edges representing citations between papers, and papers associated with bag-of-words features and ground-truth topics as labels.</p><p>• Reddit <ref type="bibr" target="#b59">[59]</ref>: Reddit is an online discussion forum where nodes stand for posts, two nodes are connected if they are commented by the same user, and each post contains a lowdimensional word vector as features and a label indicating its community.</p><p>• PPI <ref type="bibr" target="#b59">[59]</ref>: PPI is a collection of protein-protein interaction graphs of different human tissues with features representing biological signatures and labels representing the roles of proteins. In Cora, Citeseer, Pubmed, and Reddit, there exists one graph and the same graph structure is used in both training and testing, thus the tasks are considered transductive. In PPI, since training and testing nodes are in different graphs, it is considered as an inductive node classification benchmark.</p><p>In Table <ref type="table" target="#tab_0">11</ref>, we report the results of different models on these benchmark datasets. The results are extracted from their original papers when a fixed dataset split is adopted. The table shows that many state-of-the-art methods achieve roughly comparable performance in these benchmarks, with differences smaller than one percent. Shchur et al. also find that a fixed dataset split can easily result in spurious comparisons <ref type="bibr" target="#b170">[169]</ref>. As a result, though these benchmarks are widely adopted to compare different models, more comprehensive evaluation setups are critically needed. <ref type="bibr" target="#b9">10</ref>. Publicly available at https://github.com/tkipf/gcn or http://snap.stanford. edu/graphsage/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The categorization of deep learning methods on graphs. We divide existing methods into five categories: Graph Recurrent Neural Networks, Graph Convolutional Networks, Graph Autoencoders, Graph Reinforcement Learning, and Graph Adversarial Methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) where ūk = T k ( L)u and L = 2L/λ max − I. Using the recurrence relation of Chebyshev polynomial T k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An illustrating example of the spatial convolution operation proposed by Kipf and Welling [49]. Nodes are only affected by their immediate neighbors in each convolutional layer. Reprinted with permission.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. An example of hierarchical clustering results of the nodes in the graph. Reprinted from<ref type="bibr" target="#b62">[62]</ref> with permission.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1 Fig. 5</head><label>15</label><figDesc>Fig.5. An illustration of the multi-head attentions proposed in GAT<ref type="bibr" target="#b63">[63]</ref> where each color denotes an independent attention. Reprinted with permission.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Node sampling method of (A) GraphSAGE [59], (B) Stochas-ticGCN [73], (C) FastGCN [74], (D) Adapt [75], where blue nodes indicate samples from one batch and arrows indicate sampling directions. Red nodes in figure (B) represent historical samples. Reprinted with permission.</figDesc><graphic url="image-48.png" coords="10,333.12,43.70,209.73,173.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. The framework of DVNE reprinted from<ref type="bibr" target="#b109">[109]</ref> with permission. DVNE represents nodes as distributions using VAE and adopts Wasserstein distance to preserve the transitivity of nodes similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 Some</head><label>1</label><figDesc>Main Distinctions of Deep Learning Methods on Graphs</figDesc><table><row><cell>Category Graph Recurrent Neural Networks Graph Convolutional Networks Graph Autoencoders Graph Reinforcement Learning Graph Adversarial Methods</cell><cell>Basic Assumptions/Aims Recursive and sequential patterns of graphs Common local and global structural patterns of graphs Low-rank structures of graphs Feedbacks and constraints of graph tasks</cell><cell>Main Functions Definition of states for nodes or graphs Graph convolution and readout operations Unsupervised node representation learning Graph-based actions and rewards</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Table of Commonly</head><label>2of</label><figDesc>Used Notations</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>4. It is called semi-supervised because all the graph structures and a part of node or graph labels are used during training.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>The Main Characteristics of Graph RNNs</cell><cell></cell><cell></cell></row><row><cell>Category</cell><cell>Method GNN [23]</cell><cell>Recursive/sequential patterns of graphs</cell><cell>Scalability</cell><cell>Other Improvements</cell></row><row><cell>Node-level</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4 A</head><label>4</label><figDesc>Comparison of Different Graph Convolutional Networks (GCNs). M.G. = Multiple Graphs</figDesc><table><row><cell>Method Bruna et al. [46] Henaff et al. [47] ChebNet [48] Kipf&amp;Welling [49] CayletNet [50] GWNN [51] Neural FPs [52] PATCHY-SAN [53] LGCN [54] SortPooling [55] DCNN [56] DGCN [57] MPNNs [58] GraphSAGE [59] MoNet [60] GNs [9] Kearnes et al. [61] DiffPool [62] GAT [63] GaAN [64] HAN [65] CLN [66] PPNP [67] JK-Nets [68] ECC [69] R-GCNs [70] LGNN [71] PinSage [72] StochasticGCN [73] FastGCN [74] Adapt [75] Li et al. [76] SGC [77] GFNN [78] GIN [79] DGI [80]</cell><cell>Type Spectral Spectral Spectral/Spatial Spectral/Spatial Spectral Spectral Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial Spatial</cell><cell>Convolution Interpolation Kernel Interpolation Kernel Polynomial First-order Polynomial Wavelet Transform First-order Polynomial + Order First-order + Order First-order Polynomial Diffusion First-order + Diffusion First-order First-order + Sampling First-order First-order Weave module Various First-order First-order Meta-path Neighbors First-order First-order Various First-order First-order First-order + LINE graph Random Walk First-order + Sampling First-order + Sampling First-order + Sampling First-order Polynomial Polynomial First-order First-order</cell><cell>Readout Hierarchical Clustering + FC Hierarchical Clustering + FC Hierarchical Clustering ---Sum Order + Pooling -Order + Pooling Mean -Set2set -Hierarchical Clustering Whole Graph Representation Fuzzy Histogram Hierarchical Clustering ------Hierarchical Clustering ---------Sum + MLP -</cell><cell>Scalability No No Yes Yes Yes Yes No Yes Yes Yes No No Yes Yes Yes Yes Yes No Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes</cell><cell>M.G. No No Yes -No No Yes Yes Yes Yes Yes -Yes -Yes Yes Yes Yes Yes Yes Yes --Yes Yes ----Yes Yes -Yes Yes Yes Yes</cell><cell>Other Characteristics -Constructing the Graph -----An Order for Neighbors An Order for Neighbors An Order for Nodes Edge Features -General Framework General Framework General Framework General Framework Edge Features Differentiable Pooling Attention Attention Attention -Teleportation Connection Jumping Connection Edge Features Edge Features Edge Features Neighborhood Sampling Neighborhood Sampling Layer-wise Sampling Layer-wise Sampling Theoretical analysis Theoretical analysis Theoretical analysis Theoretical analysis Unsupervised training</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5 A</head><label>5</label><figDesc>Comparison of Different Graph Autoencoders (GAEs)</figDesc><table><row><cell>Method SAE [102] SDNE [103] DNGR [104] GC-MC [105] DRNE [106] G2G [107] VGAE [108] DVNE [109] ARGA/ARVGA [110] NetRA [111]</cell><cell>Type AE AE AE AE AE AE VAE VAE AE/VAE AE</cell><cell cols="2">Objective L2-Reconstruction L2-Reconstruction + Laplacian Eigenmaps L2-Reconstruction L2-Reconstruction Recursive Reconstruction KL + Ranking Pairwise Probability of Reconstruction Wasserstein + Ranking L2-Reconstruction + GAN Recursive Reconstruction + Laplacian Eigenmaps + GAN</cell><cell>Scalability Yes Yes No Yes Yes Yes No Yes Yes Yes</cell><cell>Node Features No No No Yes No Yes Yes No Yes No</cell><cell>Other Characteristics ---GCN Encoder LSTM Encoder Nodes as distributions GCN Encoder Nodes as distributions GCN Encoder LSTM Encoder</cell></row><row><cell cols="2">Local structure preserved cost</cell><cell cols="2">Local structure preserved cost</cell><cell></cell><cell></cell></row><row><cell>…</cell><cell></cell><cell></cell><cell>…</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">parameter sharing</cell><cell></cell><cell></cell><cell></cell></row><row><cell>…</cell><cell></cell><cell></cell><cell>…</cell><cell></cell><cell></cell></row><row><cell>…</cell><cell cols="2">Global structure preserved cost</cell><cell>…</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Laplacian</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Eigenmaps</cell><cell></cell><cell></cell><cell></cell></row><row><cell>…</cell><cell></cell><cell></cell><cell>…</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">parameter sharing</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vertex i</cell><cell></cell><cell></cell><cell>Vertex j</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 7 The</head><label>7</label><figDesc>Main Characteristics of Graph Adversarial Methods</figDesc><table><row><cell>Category</cell><cell>Method ARGA/ARVGA [110]</cell><cell>Adversarial Methods</cell><cell>Scalability</cell><cell>Node Features</cell></row><row><cell>Adversarial Training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We thank Jianfei Chen, Jie Chen, William L. Hamilton, Wenbing Huang, Thomas Kipf, Federico Monti, Shirui Pan, Petar Velickovic, Keyulu Xu, Rex Ying for providing their figures.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLACE PHOTO HERE</head><p>Ziwei Zhang received his B.S. from the Department of Physics, Tsinghua University in 2016. He is currently pursuing the Ph.D. Degree in the Department of Computer Science and Technology at Tsinghua University. His research interests focus on network embedding and machine learning on graph data, especially developing scalable algorithms for large-scale networks. He has published several papers in prestigious conferences and journals, including KDD, AAAI, IJ-CAI, and TKDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLACE PHOTO HERE</head><p>Peng Cui received the PhD degree from Tsinghua University, in 2010. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A SOURCE CODES</head><p>We collect and summarize a list of the source codes for papers discussed in this paper, as shown in Table <ref type="table">9</ref>. Besides the method name and link, we also list the programming language and frameworks adopted as well as whether they are published by the original authors of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B APPLICABILITY IN COMMON TASKS</head><p>We summarize the applicability of different models in six common tasks on graphs including node clustering, node classification, network reconstruction, link prediction, graph classification, and graph generation, as shown in Table <ref type="table">10</ref>. Note that our results are based on whether the experiments are conducted in the original papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C NODE CLASSIFICATION RESULTS ON BENCHMARK DATASETS TABLE 10 A Table for Methods of Six Common Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type</head><p>Task Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node-focused Tasks</head><p>Node Clustering <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b71">[71]</ref>, <ref type="bibr" target="#b102">[102]</ref>- <ref type="bibr" target="#b104">[104]</ref>, <ref type="bibr" target="#b107">[107]</ref>, <ref type="bibr" target="#b109">[109]</ref>, <ref type="bibr" target="#b110">[110]</ref>, <ref type="bibr" target="#b129">[129]</ref>, <ref type="bibr" target="#b130">[130]</ref>, <ref type="bibr" target="#b163">[162]</ref> Node Classification Transductive <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b48">[48]</ref>- <ref type="bibr" target="#b51">[51]</ref>, <ref type="bibr" target="#b54">[54]</ref>, <ref type="bibr" target="#b56">[56]</ref>, <ref type="bibr" target="#b57">[57]</ref>, <ref type="bibr" target="#b59">[59]</ref>, <ref type="bibr" target="#b60">[60]</ref> [63]- <ref type="bibr" target="#b68">[68]</ref>, <ref type="bibr" target="#b70">[70]</ref>, <ref type="bibr" target="#b71">[71]</ref>, <ref type="bibr" target="#b73">[73]</ref>- <ref type="bibr" target="#b78">[78]</ref> [80], <ref type="bibr" target="#b103">[103]</ref>, <ref type="bibr" target="#b106">[106]</ref>, <ref type="bibr" target="#b107">[107]</ref>, <ref type="bibr" target="#b109">[109]</ref>, <ref type="bibr" target="#b111">[111]</ref>, <ref type="bibr" target="#b129">[129]</ref>- <ref type="bibr" target="#b131">[131]</ref>, <ref type="bibr" target="#b163">[162]</ref>, <ref type="bibr" target="#b168">[167]</ref> Inductive <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b54">[54]</ref>, <ref type="bibr" target="#b59">[59]</ref>, <ref type="bibr" target="#b63">[63]</ref>, <ref type="bibr" target="#b64">[64]</ref>, <ref type="bibr" target="#b68">[68]</ref>, <ref type="bibr" target="#b73">[73]</ref>, <ref type="bibr" target="#b77">[77]</ref>, <ref type="bibr" target="#b78">[78]</ref>, <ref type="bibr" target="#b80">[80]</ref> Network Reconstruction <ref type="bibr" target="#b103">[103]</ref>, <ref type="bibr" target="#b109">[109]</ref>, <ref type="bibr" target="#b111">[111]</ref>, <ref type="bibr" target="#b163">[162]</ref> Link Prediction <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b70">[70]</ref>, <ref type="bibr" target="#b72">[72]</ref>, <ref type="bibr" target="#b103">[103]</ref>, <ref type="bibr" target="#b105">[105]</ref>, <ref type="bibr" target="#b107">[107]</ref>- <ref type="bibr" target="#b111">[111]</ref>, <ref type="bibr" target="#b129">[129]</ref> Graph-focused Tasks</p><p>Graph Classification <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b52">[52]</ref>, <ref type="bibr" target="#b53">[53]</ref>, <ref type="bibr" target="#b55">[55]</ref>, <ref type="bibr" target="#b56">[56]</ref>, <ref type="bibr" target="#b61">[61]</ref>, <ref type="bibr" target="#b62">[62]</ref>, <ref type="bibr" target="#b69">[69]</ref>, <ref type="bibr" target="#b77">[77]</ref>, <ref type="bibr" target="#b79">[79]</ref>, <ref type="bibr" target="#b124">[124]</ref>, <ref type="bibr" target="#b139">[138]</ref> Graph Generation Structure-only <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b132">[132]</ref> Structure+features <ref type="bibr" target="#b44">[44]</ref>, <ref type="bibr" target="#b121">[121]</ref>, <ref type="bibr" target="#b122">[122]</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Learning Representations</title>
				<meeting>the 4th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Network science</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond sigmoids: The nettide model for social network growth, and its applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2015" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Langston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Attention models in graphs: A survey</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07984</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">TABLE 8 Libraries of Deep Learning on Graphs Name URL Language/Framework Key Characteristics PyTorch Geometric</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
		<ptr target="https://github.com/alibaba/aligraphUnknownDistributed,large-scale,in-housealgorithms" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Social Networks</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="79" to="91" />
		</imprint>
	</monogr>
	<note>Graph convolutional networks: Algorithms, applications and open challenges. unified operations, comprehensive existing methods Deep Graph Library [160. unified operations, large-scale AliGraph [161</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<ptr target="https://github.com/alibaba/eulerC++/TensorflowDistributed,large-scale" />
	</analytic>
	<monogr>
		<title level="j">Euler</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adversarial attack and defense on graph data: A survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.10528</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: A general framework for dimensionality reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
				<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Community preserving network embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
				<meeting>the 31st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to discover social circles in ego networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural network for graphs: A contextual constructive approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="498" to="511" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
				<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning steadystates of iterative algorithms over graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1114" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graphrnn: Generating realistic graphs with deep auto-regressive models</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5694" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Dynamic graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10627</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dynamic graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Manessi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manzo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06199</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks Proceedings</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A general framework for adaptive processing of data structures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="768" to="786" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An efficient method for finding the minimum of a function of several variables without calculating derivatives</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The computer journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="162" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A learning rule for asynchronous perceptrons with feedback in a combinatorial environment</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings, 1st First International Conference on Neural Networks</title>
				<meeting>1st First International Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalization of back-propagation to recurrent neural networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Pineda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page">2229</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An introduction to metric spaces and fixed point theory</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Khamsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Kirk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning to decipher the heap for program verification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Constructive Machine Learning at the International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Vain: Attentional multi-agent predictive modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G T</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Patient subtyping via time-aware lstm networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Baytas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
				<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep convolutional networks on graph-structured data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
				<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph wavelet neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
				<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dual graph convolutional networks for graphbased semi-supervised classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
				<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition</title>
				<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer-Aided Molecular Design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Gaan: Gated attention networks for learning on large and spatiotemporal graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Column networks for collective classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st AAAI Conference on Artificial Intelligence</title>
				<meeting>the 31st AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2485" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
				<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>-I. Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Supervised community detection with line graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
				<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="941" to="949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4563" to="4572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
				<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
				<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Arbitrary-order proximity preserved network embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2778" to="2786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J V</forename><surname>Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">A (sub) graph isomorphism algorithm for matching large graphs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Cordella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Foggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sansone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1367" to="1372" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Fuzzy sets and fuzzy logic</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Prentice hall New Jersey</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Hierarchical taxonomy aware network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1920" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">The elements of statistical learning: Data mining, inference, and prediction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ruppert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">466</biblScope>
			<biblScope unit="page" from="567" to="567" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Weighted graph cuts without eigenvectors a multilevel approach</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A multiscale pyramid transform for graph signals</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Faraji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2119" to="2134" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
				<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Practical graph isomorphism, II</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piperno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Academic Press, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Depthlgp: Learning embeddings of out-ofsample nodes in dynamic networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
				<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">The vapnikchervonenkis dimension of graph and recursive neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="248" to="259" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Stability and generalization of graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1539" to="1548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Structural deep network embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1225" to="1234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Deep recursive network embedding with regular equivalence</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2357" to="2366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Deep variational network embedding in wasserstein space</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2827" to="2836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2609" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Learning deep network representations with adversarially regularized autoencoders</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2663" to="2671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
				<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">A note on spectral clustering and svd of graph data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11029</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
				<meeting>the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Random walks on graphs: A survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Stanford InfoLab, Tech. Rep</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The annals of mathematical statistics</title>
				<imprint>
			<date type="published" when="1951">1951</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Predicting structured data</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
				<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Calculation of the wasserstein distance between probability distributions on the line</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vallender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability &amp; Its Applications</title>
				<imprint>
			<date type="published" when="1974">1974</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="784" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Molgan: An implicit generative model for small molecular graphs</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Graph transformation policy network for chemical reaction prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="750" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Graph classification using structural attention</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Deeppath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing systems</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Graphgan: Graph representation learning with generative adversarial nets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Adversarial network embedding</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on graphs with generative adversarial nets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
				<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="913" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Netgan: Generating graphs via random walks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Data Mining. ACM</title>
		<imprint>
			<biblScope unit="page" from="2847" to="2856" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Adversarial attack on graph structured data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1115" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations</title>
				<meeting>the 8th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Deepinf: Modeling influence locality in large social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Learning disentangled representations for recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Convolutional embedding of attributed molecular graphs for physical property prediction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1757" to="1772" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">145301</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Distance metric learning using graph convolutional networks: Application to functional brain networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00543</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Spectral graph convolutions for population-based disease prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Ktena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Towards gene expression convolutions using gene interaction graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dutil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Derevyanko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning Workshop on Computational Biology</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Graph convolutional encoders for syntax-aware neural machine translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1957" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Encoding sentences with graph convolutional networks for semantic role labeling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1506" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">The more you know: Using knowledge graphs for image classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
				<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Learning to represent programs with graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khademi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
				<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Combinatorial optimization with graph convolutional networks and guided tree search</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="536" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title level="m" type="main">Learning to np-complete problems-a graph neural network for the decision tsp</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Prates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Avelar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lemos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vardi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02721</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<monogr>
		<title level="m" type="main">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>ICLR Workshop on Representation Learning on Graphs and Manifolds</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Aligraph: A comprehensive graph neural network platform</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International Conference on Very Large Data Bases</title>
				<meeting>the 45th International Conference on Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Heterogeneous network embedding via deep architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Signed graph convolutional network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="559" to="568" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Structural deep embedding for hyper-networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Autone: Hyperparameter optimization for massive network embedding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330848</idno>
		<ptr target="http://doi.acm.org/10.1145/3292500.3330848" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, ser. KDD &apos;19</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, ser. KDD &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Gnn explainer: A tool for post-hoc explanation of graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Robust graph convolutional networks against adversarial attacks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1399" to="1407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<title level="m" type="main">Power up! robust graph convolutional network against evasion attacks based on graph powering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sojoudi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10029</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Relational Representation Learning Workshop</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">TABLE 9 A collection of published source codes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Language/Framework</note>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">steady state embedding Yes C CommNet</title>
		<author>
			<persName><forename type="first">Graph</forename><surname>Rnns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ggs-Nns</forename></persName>
		</author>
		<ptr target="https://github.com/JiaxuanYou/graph-generationYesPython/PytorchRMGCNN[29]https://github.com/fmonti/mgcnnYesPython/Tensorflow" />
		<imprint/>
	</monogr>
	<note>Relation Networks [43] https://github.com/kimhc6028/relational-networks No Python/Pytorch You et al. [27</note>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gcns</forename><surname>Chebnet</surname></persName>
		</author>
		<ptr target="https://github.com/divelab/lgcn/YesPython/TensorflowDCNN[56]https://github.com/jcatw/dcnnYesPython/TheanoDGCN[57]https://github.com/ZhuangCY/Coding-NNYesPython/TheanoMPNNs[58]https://github.com/brain-research/mpnnYesPython/Tensorflow" />
		<imprint>
			<biblScope unit="volume">52</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sae</forename><surname>Gaes</surname></persName>
		</author>
		<ptr target="https://github.com/quinngroup/deep-representations-clusteringNoPython/KerasSDNE[103]https://github.com/suanrong/SDNEYesPython/TensorflowDNGR[104]https://github.com/ShelsonCao/DNGRYesMatlabGC-MC[105]https://github.com/riannevdberg/gc-mcYesPython/TensorflowDRNE[106]https://github.com/tadpole/DRNEYesPython/TensorflowG2G[107]https://github.com/abojchevski/graph2gauss" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<title level="m" type="main">graph generation Yes Python/Tensorflow MolGAN</title>
		<ptr target="https://github.com/xwhan/DeepPathYesPython/TensorflowMINERVA[126]https://github.com/shehzaadzd/MINERVAYesPython/Tensorflow" />
		<imprint>
			<biblScope unit="volume">122</biblScope>
		</imprint>
	</monogr>
	<note>Graph RLs GCPN [121</note>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<title level="m" type="main">graph adversarial attack Yes Python/Pytorch Zugner&amp;Gunnemann</title>
		<ptr target="https://github.com/mila-iqia/gene-graph-convYesPython/PytorchRGCN[167]https://github.com/thumanlab/nrlwebYesPython/TensorflowGNN-benchmark[169]https://github.com/shchur/gnn-benchmarkYesPython/Tensorflow" />
		<imprint/>
	</monogr>
	<note>Graph Adversarial Methods GraphGAN [129. pytorch structure2vec Yes Python/Pytorch SGCN [163] http://www.cse.msu.edu/ ∼ derrtyle/ Yes Python/Pytorch Dutil et al. [146</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
