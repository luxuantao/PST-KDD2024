<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPANET: SPATIAL PYRAMID ATTENTION NETWORK FOR ENHANCED IMAGE RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jingda</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<postCode>76203</postCode>
									<settlement>Denton</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<postCode>76203</postCode>
									<settlement>Denton</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Sansom</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<postCode>76203</postCode>
									<settlement>Denton</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mara</forename><surname>Mcguire</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Texas A&amp;M University -Corpus Christi, Corpus Christi</orgName>
								<address>
									<postCode>78412</postCode>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Kalaani</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Southern University</orgName>
								<address>
									<postCode>30458</postCode>
									<settlement>Statesboro</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<postCode>76203</postCode>
									<settlement>Denton</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sihai</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<postCode>76203</postCode>
									<settlement>Denton</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qing</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<postCode>76203</postCode>
									<settlement>Denton</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Texas</orgName>
								<address>
									<postCode>76203</postCode>
									<settlement>Denton</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SPANET: SPATIAL PYRAMID ATTENTION NETWORK FOR ENHANCED IMAGE RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention mechanism has shown great success in computer vision. In this paper, we introduce Spatial Pyramid Attention Network (SPANet) to investigate the role of attention block for image recognition. Our SPANet is conceptually simple but practically powerful. It enhances the base network by adding Spatial Pyramid Attention (SPA) Blocks laterally. In contrast to other attention based networks that leverage global average pooling, our proposed SPANet considers both structural regularization and structural information. Furthermore, we investigate the topology structure of attention path connection and present three SPANet structures. SPA block is flexible to be deployed to various convolutional neural network (CNN) architectures. The experimental results show that our SPANet significantly improves the recognition accuracy without introducing much computation overhead compared with other CNN models. Codes are made publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Convolutional neural networks have shown profound influence on a variety of visual processing applications. Hence, there are ever-increasing interests in CNN improvements. To enhance the performance of CNNs, recent works add more and more convolutional layers to the CNN architecture. For example, from 8-layer AlexNet <ref type="bibr" target="#b1">[1]</ref> to 1000-layer ResNet <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>, they aim to improve the accuracy of image recognition. Inevitably, more learnable layers introduce more parameters and prolong inference time.</p><p>In addition to making neural networks deeper, other efforts focus on investigating attention mechanisms <ref type="bibr" target="#b4">[4]</ref> in CNNs. By informing a CNN network where to look and what to pay attention to, attention networks achieve a better performance with fewer layers. As an example, SENet <ref type="bibr" target="#b5">[5]</ref> introduces Squeeze-and-Excitation (SE) blocks to study the channel dependencies in a CNN architecture. Although aforementioned CNN architectures achieve better performance for Fig. <ref type="figure">1</ref>. Architecture of our SPANet. We design a Spatial Pyramid Structure to replace the traditional global average pooling. SPANet-A learns attention from current feature maps. SPANet-B learns from previous feature maps. SPANet-C adds an optional point-wise convolution to the attention path. image recognition, the use of global average pooling (GAP) layers that aggregate a 3D feature map to a 1D attention map, would certainly cause loss of structural information in intermediate feature maps. To mitigate this problem, Convolutional block attention module (CBAM) <ref type="bibr">[6]</ref> considers both channel-wise attention and spatial attention, which focuses on channel dependencies and structural information respectively.</p><p>In this paper, we innovatively incorporate structural information to channel-wise attention blocks. We argue that the limitation originating from the global average pooling makes the shallow layers (which output big-size feature maps) unable to fully leverage the advantages of attention mechanism <ref type="bibr" target="#b7">[7]</ref>. Following this argument, we present Spatial Pyramid Attention (SPA), which introduces a spatial pyramid structure to encode the intermediate features instead of using the simple global average pooling. Our proposed SPA is composed of two parts: one is a spatial pyramid structure which aggregates a 3D feature map into a 1D attention map, and the other is a combination of two fully-connected layers and a sigmoidbased activation layer, which sequentially encodes and decodes attention weights. These two parts are light-weight.</p><p>In terms of pooling schema, our spatial pyramid struc-978-1-7281-1331-9/20/$31.00 ?2020 IEEE ture could be considered similar to SPPNet <ref type="bibr" target="#b8">[8]</ref> and Region of Interesting Pooling <ref type="bibr" target="#b9">[9]</ref>. In contrast, our spatial pyramid structure encodes a feature map with more structural information while SPPNet and Region of Interesting Pooling aim to obtain a fixed-length feature vector. In addition to being capable of retaining the spatial information in each channel, a major advantage of the proposed spatial pyramid structure is that it does not introduce any additional parameter. All layers in the spatial pyramid structure are not learnable, which is nearly cost-free. Compared to SENet <ref type="bibr" target="#b5">[5]</ref>, our structure only modifies the first fully-connected layer to tackle the large input size. The small computation overhead contributes to its enhanced performance.</p><p>Inspired by self-attention, we explore three topology structures of the Spatial Pyramid Attention module in our proposed SPANet, referred to as SPANet-A, SPANet-B and SPANet-C. SPANet-A learns attention from current feature maps, which follows a traditional self-attention path connection schema. SPANet-B learns from previous feature maps. SPANet-C adds an optional point-wise convolution to the attention path. Fig. <ref type="figure">1</ref> depicts the schemas of SPANet.</p><p>We comprehensively evaluate the performance of SPANet using CIFAR-100 and a down-sampled ImageNet dataset. Without bells and whistles, SPANet outperforms related stateof-art work <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11]</ref>. Experimental results show that structural information in the attention mechanism, which we focus on, is a crucial factor for model performance. Compared to SENet that only considers the structural regularization in attention mechanism, our SPANet obtains 1.88% accuracy improvement on downsampled ImageNet <ref type="bibr" target="#b12">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Multi-Path Connection. Multi-path connection in deep learning was first used in Highway Networks <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref>. By allowing an unimpeded information flowed across several layers, a Highway Network is capable of reusing the information from previous layers, which facilities the training of deep networks. Moreover, gating units are employed to regulate the information flow. Subsequently, He et al. proposed Residual Networks (ResNet) <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>, which learn the residual functions by adding skip-connections. The ResNet shows that an identity mapping shortcut is crucial to ease the optimization <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>. Hence, ResNet discards the gating units used in Highway Networks and keeps the information passed though shortcuts. The better performance achieved by ResNet has made shortcut connections attractive. As a more dense reformulation, the work in <ref type="bibr" target="#b10">[10]</ref> connects every convolutional layer in a deep convolutional network. Without introducing more parameters, it effectively alleviates the vanishing gradient problem and improves feature reuse.</p><p>In addition to shortcut connections, there are works studying the internal multi-path connections in convolutional blocks <ref type="bibr" target="#b15">[15]</ref>. The InceptionV4 Network <ref type="bibr" target="#b15">[15]</ref> is one of this kind. Besides a shortcut connection, each inception block in InceptionV4 contains 3-6 carefully designed paths. All these paths are integrated together using filter concatenation as input to the next block. More recently, attention based networks such as SENet <ref type="bibr" target="#b5">[5]</ref> and CBAM <ref type="bibr">[6]</ref> provide an independent attention path to learn the weight of each channel and achieve state-of-the-art performance.</p><p>Attention Mechanism. Attention Mechanism <ref type="bibr" target="#b4">[4]</ref> has been prevailed in computer vision for years <ref type="bibr" target="#b16">[16]</ref>. By adopting a gating function such as soft-max and Sigmoid, attention mechanism is able to selectively emphasize salient features as well as suppress insignificant features. Thus, visual features could be better captured and exploited. In <ref type="bibr" target="#b5">[5]</ref>, a Squeeze-and-Extraction block was proposed to learn the channel-wise attention for each convolutional layer, which provides an end-to-end training paradigm for attention learning. Inspired by SENet, Competitive-SENet <ref type="bibr" target="#b17">[17]</ref> studies attention from both the residual path and the shortcut path. Although Competitive-SENet achieves promising performance, it is tailored particularly for Residual Networks <ref type="bibr" target="#b2">[2]</ref>, which limits its generalization to other models. Without being limited to channel-wise attention, Sanghyun Woo et al. <ref type="bibr">[6]</ref> exploited the relation between channel-wise attention and spatial attention and proposed a Convolutional Block Attention Module (CBAM). CBAM is composed of two parts, i.e., a channel-wise attention part and a spatial attention part. The two attention parts in CBAM is able to tell what (channel) to look and where (spatial) to focus on. Unlike CBAM that learns channel-wise attention and spatial attention separately, our proposed SPANet learns channel-wise and spatial attention in an integrated fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SPATIAL PYRAMID ATTENTION MODULE</head><p>Convolutional neural networks achieve great success in computer vision. Meanwhile, it ignores the weights of channels, which affects CNN's ability of discrimination. Attention mechanism, on the other hand, is capable of capturing channels' dependency, but ignores the structural information of channels. To enhance the representation power of CNNs, we introduce a spatial pyramid attention module. The proposed module considers the spatial pyramid structure which integrates average pooling of different sizes and explores the connection schema of attention paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Design Overview</head><p>Fig. <ref type="figure">2</ref> depicts the paradigm of our spatial pyramid attention module. The module learns a 1D attention map in an attention path, which is laterally connected to the original convolutional flow. The learned attention map is fed to each convolutional block in the original path. Such a design makes it possible to apply SPA module to various base models easily. Fig. <ref type="figure">2</ref>. Architecture of the spatial pyramid attention module. It is composed of three components, i.e., point-wise convolution, spatial pyramid structure, and multi-layer perception. Point-wise convolution is particularly designed for SPANet-C to match the channel number and integrate channel information. Spatial pyramid structure includes adaptive average pooling of three different sizes to integrate structural regularization and structural information in an attention path. Multi-layer perception learns an attention map from the output of the spatial pyramid structure. In SPANet-A, the input feature map is the current output of a block. In SPANet-B and SPANet-C, the input feature map is the previous output of a block with SPANet-C performing an optional point-wise convolution.</p><p>Suppose a CNN is composed of L layers, each of which outputs a feature map. We use x l to denote the output of the l-th layer, where l ? [1, L] is the index of a layer. We denoted adaptive average pooling and fully-connected layer as P (?, ?), F f c (?) respectively. C (?) represents a concatenation operation, ? (?) is a Sigmoid activation function, and R (?) is referred to as re-sizing a tensor to a vector.</p><p>Given an intermediate feature map x l ? R C?W ?H , an attention mechanism based CNN model learns attention weights from the input x l and multiplies each channel in x l by learnable weights to produce an output. The output of Spatial Pyramid Structure S(x l ) can be presented as:</p><formula xml:id="formula_0">S(x l ) = C (R (P (x l , 4)) , R (P (x l , 2)) , R (P (x l , 1))) . (1)</formula><p>Omitting the batch normalization and activation layer for clarity, SPA module performs transformation F as</p><formula xml:id="formula_1">F (x l ) = ? (F f c (F f c (S(x l )))) .<label>(2)</label></formula><p>Equation ( <ref type="formula" target="#formula_1">2</ref>) presents the essence of transformation by the SPA module. The batch normalization layer, activation layers and pointwise convolution omitted in Equation ( <ref type="formula" target="#formula_1">2</ref>) are included in the implementation and performance evaluation of the SPA module (Section 4). Following <ref type="bibr" target="#b8">[8]</ref>, we propose 3-level pyramid average pooling: 4 ? 4, 2 ? 2 and 1 ? 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention Path Connection</head><p>Most of the existing self-attention based networks follow a path design pattern: they learn an attention map from a feature map and then apply the learned attention map to the original feature map <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b18">18]</ref> . However, being confined to aforementioned schema compromises the exploration of attention path connections. For SPANet, we study the topology of attention path connections and explore three variations: SPANet-A, SPANet-B, and SPANet-C, as shown in Fig. <ref type="figure">1</ref>.</p><p>SPANet-A feeds the current feature map x l to the attention path to generate a 1D attention map. Accordingly, the output of a block in SPANet-A can be expressed as</p><formula xml:id="formula_2">x l = F (x l ) ? x l ,<label>(3)</label></formula><p>where ? denotes element-wise multiplication. SPANet-A uses a similar schema as traditional self-attention path connections.</p><p>SPANet-B learns an attention map directly from x l-1 (where</p><formula xml:id="formula_3">x l-1 ? R C ?W ?H ) instead of the processed x l . The output of a block in SPANet-B is x = F (x l-1 ) ? x l .<label>(4)</label></formula><p>This design in SPANet-B is to assure that the attention path is independent of the original convolutional block path, enabling the attention path to learn more generalized weights. Note that although the two paths are independent of each other, they are not completely irrelevant because the attention path and the convolutional block path are trained jointly. SPANet-C. Considering the channel number in x l-1 may not be equal to the channel number in x l , the attention path might not produce the most accurate weights for x l . Thus, we modify SPANet-B by adding a point-wise convolutional layer <ref type="bibr" target="#b19">[19]</ref> at the beginning of the attention path if C = C. We compute the output of SPANet-C as follows.</p><p>x = F (C (x l-1 )) ? x l ,</p><p>where C (?) denotes the point-wise convolution operation. The pointwise convolution, which consists of a convolutional layer with a 1 ? 1 filter and a batch normalization layer, aims to integrate channel information and match channel numbers of output feature maps. It makes the attention path further independent of x.</p><p>We focus on the topology structure of attention path connections in the preceding discussion. The implementation details of the attention mechanism are provided in Section 4. All of the three SPANets can be integrated with other CNN architectures. In the following discussion, SPANet refers to SPANet-C unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Spatial Pyramid Attention</head><p>Many existing attention based networks <ref type="bibr" target="#b5">[5,</ref><ref type="bibr">6,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b17">17]</ref> aggregate input feature maps into a 1D vector using global average pooling. They achieve structural regularization <ref type="bibr" target="#b20">[20]</ref>, but miss the structural information. In contrast, the spatial pyramid structure in our proposed attention module utilizes average pooling of three different sizes to both achieve structural regularization and explore structural information (as shown in Fig. <ref type="figure">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Spatial Pyramid Structure</head><p>Global average pooling (GAP), which aggregates the global information in each channel, was introduced in <ref type="bibr" target="#b21">[21]</ref> to replace the conventional fully-connected layers in CNNs. Since then, it has prevailed in computer vision for recognition <ref type="bibr" target="#b2">[2]</ref>, detection <ref type="bibr" target="#b22">[22]</ref>, segmentation <ref type="bibr" target="#b23">[23]</ref>, and more.</p><p>We note that existing work on global average pooling used the last feature map which is small in size (7 ? 7 for example). However, attention based CNNs (e.g., <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref>, etc.) apply global average pooling on each feature map. As presented in <ref type="bibr" target="#b20">[20]</ref>, GAP behaves similarly to a structural regularizer and is capable of preventing over-fitting. However, applying GAP to every feature map overemphasizes the effect of regularization and misses the original feature representation and structural information, especially when a feature map is large. For example, aggregating a 112 ? 112 feature map to a mean value causes significant loss of a features' representation capability, which affects feature learning.</p><p>To address this problem, we propose a spatial pyramid structure used in attention blocks. The spatial pyramid structure adaptively and averagely pools an input feature map to three scales: 4 ? 4, 2 ? 2, and 1 ? 1. The spatial pyramid structure provides a combination of three regularization terms, i.e., the 4 ? 4 average pooling captures more feature representation and structural information, the 1 ? 1 average pooling is the traditional GAP with a strong structural regularization, and the 2 ? 2 average pooling aims at a trade-off between structural information and structural regularization. Then we re-size the three outputs to three 1D vectors and combine all together to generate a 1D attention map. Our spatial pyramid structure is capable of both preserving the feature representation and inheriting the advantages of the global average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Fully-Connected Layers</head><p>The 1D attention map v extracted from the spatial pyramid structure is a concatenation of the outputs from three pooling layers. However, it cannot be used to learn channel dependency and its non-linear expression affects the effectiveness of the attention mechanism. To address this problem, we leverage the excitation block <ref type="bibr" target="#b5">[5]</ref> to encode v and generate a 1D attention map ?. The excitation block employs two fully-connected layers. Then a sigmoid layer is employed to normalize the output to a range of (0, 1).</p><p>We use W1 and W2 to denote the first and second fullyconnected layers respectively, where we set the reduction rate to r. Thus, the generated attention map is</p><formula xml:id="formula_5">? = sig (W2? (W1v)) , (<label>6</label></formula><formula xml:id="formula_6">)</formula><p>where ? is a rectified linear unit (ReLU) function and sig denotes the sigmoid function. Like in SENet <ref type="bibr" target="#b5">[5]</ref>, we set r to 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Point-wise Convolution</head><p>The attention block in our proposed spatial pyramid attention module produces attention maps to analyze channel dependencies. In SPANet-B, the attention path learns a vector converted from a feature map with C channels to multiply a feature map with C channels. However, the dis-match of channels may cause discrepancy in attention learning and decrease the performance of SPANet. SPANet-C addresses this issue by adding a point-wise convolutional layer when C = C. Specifically, the point-wise convolution is a convolutional layer with a filter in size of 1 ? 1. By setting the input channel as C and the output channel as C in the point-wise convolutional layer, we are able to match the number of channels and integrate channel information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PERFORMANCE EVALUATION</head><p>We comprehensively evaluate our SPANet on CIFAR-100 <ref type="bibr" target="#b24">[24]</ref> and ImageNet <ref type="bibr" target="#b12">[12]</ref>. Due to a lack of sufficient computing resources, we experiment on a downsampled ImageNet with 32 ? 32 images. We compare ResNet + SPANet with SENet and ResNet. We also apply SPANet and SENet to several other base CNN architectures, including VGG <ref type="bibr" target="#b25">[25]</ref>, MobileNetV2 <ref type="bibr" target="#b11">[11]</ref>, DenseNet <ref type="bibr" target="#b10">[10]</ref>, and ResNext <ref type="bibr" target="#b26">[26]</ref>, to study the generalizability of SPANet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Settings and Datasets</head><p>We implement SPANet using Pytorch. We train all models using a stochastic gradient descent method, with a 0.9 Nesterov momentum and a 5e -4 weight decay. The batch size is 512 and the learning rate is initialized as 0.1. We experiment on two common datasets: CIFAR-100 <ref type="bibr" target="#b24">[24]</ref> and Downsampled ImageNet <ref type="bibr" target="#b12">[12]</ref> (a downsampled version of the original ImageNet dataset). For training, we adopt a data augmentation scheme used in <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>. We pad an original image by 4 pixels with value zero on each side and then randomly crop the padded image back to a size of 32 ? 32 pixels. In addition, we horizontally flip 50% of images in random. To facilitate model training, we normalize the image data by using channels' means and standard deviations. On CIFAR-100, the epoch size is set to 300 and the learning rate is decreased by a factor of 10 every 70 epochs. On ImageNet, we set the epoch size to 100 and divide it by 10 at the 30th, 60th, and 90th epochs. All experiments are conducted on a server with 4 TESLA K80 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>We compare the performance of our SPANet with SENet and the base networks. We employ four base networks, i.e., light-weight model MobileNetV2 <ref type="bibr" target="#b11">[11]</ref>, heavy-weight model DenseNet <ref type="bibr" target="#b10">[10]</ref>, ResNeXt <ref type="bibr" target="#b26">[26]</ref>, and VGG16 <ref type="bibr" target="#b25">[25]</ref>.</p><p>Recognition accuracy.  the base networks in all cases. This is different from our intuition. As aforementioned, the global average pooling is for structural regularization, which mitigates over-fitting. Our spatial pyramid structure, on the other hand, uses both structural regularization and structural information to achieve a better learning capability. However, it may cause over-fitting on small datasets. The performance of SPANet on CIFAR-100 becomes stable, i.e., the training loss approaches zero. This indicates a larger training dataset can contribute to a better performance.</p><p>We further test SPANet on the downsampled ImageNet dataset. The results are presented in Table <ref type="table" target="#tab_2">2</ref>. Our major findings are 1) SPANet achieves the best performance over the base models. SPANet surpasses the base models, i.e., MobileNetV2, DenseNet, ResNeXt, and VGG16, by 2.290%, 2.696%, 1.864%, and 0.88% respectively in the Top-1 accuracy. Moreover, all three types of SPANet outperform the base models and SENet. These results show the effectiveness of SPANet and prove that using both structural regularization and structural information is imperative for the attention mechanism. 2) The best performance is not achieved by one particular type of SPANet  Training Loss. Next, we plot the training loss as shown in Fig. <ref type="figure" target="#fig_0">3</ref>. Due to space limitations, we present the results on ResNet18 and MobileNetV2. In the figure, we can see SPANet achieves the least loss compared with other models. Among the three types of SPANet, SPANet-A, SPANet-B, and SPANet-C perform the best on ResNet18, DenseNet, and MobileNetV2 respectively. They employ different connection schemas, indicating that nuances in the different topology structures of attention path connections influence the performance of SPANet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Analysis</head><p>In this set of experiments, we run a number of ablations to analyze SPANet. Tables <ref type="table" target="#tab_4">3</ref> and<ref type="table">4</ref> present the results.</p><p>Attention Connection. Unlike SPANet-A, SPANet-B uses an attention connection schema that learns attention from a previous feature map. We compare SENet with SENet+ and SPANet-A with SPANet-B because each pair only differs in the topology structure of their attention path connections. In the tables, we can see that SENet always performs better than SENet+ while the results of SPANet-A are mixed compared with SPANet-B. We observe similar results in Tables <ref type="table" target="#tab_1">1</ref> and<ref type="table" target="#tab_2">2</ref> where more backbone models are tested. Results on the downsampled ImageNet (shown in Table <ref type="table">4</ref>) indicate that the two attention path connection schemas on SENet and SPANet achieve comparable performance. Thus, we can conclude that the topology structure of an attention path connection should not be fixed to a</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Training loss on the downsampled ImageNet (Best viewed in color). The three SPANets consistently produce less loss than SENet and the base networks. Similar results are also found on CIFAR-100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Performance on CIFAR-100. SPANet and SENet achieve the best accuracy on four backbone CNN models. SPANet outperforms three backbone models.</figDesc><table><row><cell></cell><cell cols="5">Base SENet SPA-A SPA-B SPA-C</cell></row><row><cell cols="2">MobileNetV2 75.18</cell><cell>75.75</cell><cell>75.81</cell><cell>75.44</cell><cell>75.75</cell></row><row><cell>DenseNet</cell><cell>74.51</cell><cell>74.77</cell><cell>75.01</cell><cell>75.22</cell><cell>75.13</cell></row><row><cell>ResNeXt</cell><cell>77.93</cell><cell>78.96</cell><cell>78.76</cell><cell>78.56</cell><cell>78.63</cell></row><row><cell>VGG16</cell><cell>72.92</cell><cell>73.0</cell><cell>72.68</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Table1shows the results on CIFAR-100. From the table, we can see that SPANet achieves the best performance in several scenarios but not all, while SENet outperforms Performance on downsampled ImageNet. SPANet outperforms all four backbone CNN models and SENet.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Base</cell><cell></cell><cell></cell><cell></cell><cell cols="2">SENet</cell><cell>SPANet-A</cell><cell>SPANet-B</cell><cell>SPANet-C</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="8">Top1 acc. Top5 acc. Top1 acc. Top5 acc. Top1 acc. Top5 acc. Top1 acc. Top5 acc. Top1 acc. Top5 acc.</cell></row><row><cell cols="7">MobileNetV2 [11] 44.306</cell><cell cols="3">69.496</cell><cell cols="3">44.556</cell><cell>70.264</cell><cell>46.370</cell><cell>71.678</cell><cell>46.242</cell><cell>71.298</cell><cell>46.596</cell><cell>71.812</cell></row><row><cell cols="4">DenseNet [10]</cell><cell></cell><cell cols="2">49.190</cell><cell cols="3">74.454</cell><cell cols="3">50.198</cell><cell>75.078</cell><cell>50.692</cell><cell>75.476</cell><cell>51.886</cell><cell>75.714</cell><cell>50.856</cell><cell>75.872</cell></row><row><cell cols="4">ResNeXt [26]</cell><cell></cell><cell cols="2">59.346</cell><cell cols="3">81.984</cell><cell cols="3">59.784</cell><cell>82.534</cell><cell>61.210</cell><cell>83.550</cell><cell>60.130</cell><cell>82.842</cell><cell>60.106</cell><cell>82.914</cell></row><row><cell cols="4">VGG16 [25]</cell><cell></cell><cell cols="2">49.214</cell><cell cols="3">73.698</cell><cell cols="3">49.612</cell><cell>73.512</cell><cell>50.094</cell><cell>74.144</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>5.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Base</cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SENet SPANet-A</cell></row><row><cell></cell><cell>4.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SPANet-B SPANet-C</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Loss</cell><cell>3 3.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20 3</cell><cell>22</cell><cell>24</cell><cell>26</cell><cell>28</cell><cell>30</cell></row><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Epochs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(a) ResNet18</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">5.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Base</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SENet</cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SPANet-A</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SPANet-B</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SPANet-C</cell></row><row><cell cols="2">4.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Loss</cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">3.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell>35</cell><cell>40</cell><cell></cell><cell>45</cell></row><row><cell cols="2">2.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell><cell>90</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Epochs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) MobileNetV2</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. For example, SPANet-A has two greatest accuracy improvements, and SPANet-B and SPANet-C each has</figDesc><table><row><cell>Depth</cell><cell>SE</cell><cell>SE+</cell><cell cols="3">SE++ SPA-A SPA-B SPA-C</cell></row><row><cell>18</cell><cell cols="3">75.19 74.97 75.25</cell><cell>75.41</cell><cell>75.01</cell><cell>75.56</cell></row><row><cell>50</cell><cell cols="3">77.91 77.45 77.43</cell><cell>78.21</cell><cell>78.11</cell><cell>77.95</cell></row><row><cell>101</cell><cell cols="3">78.03 77.88 77.61</cell><cell>78.11</cell><cell>78.35</cell><cell>79.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation results on CIFAR-100 based on ResNet. '+' means an SE block is connected to a previous feature map. '++' means an additional point-wise convolution is added to SENet+.one. They deliver the best performance on different backbones. This result verifies it is necessary to investigate the topology structure of attention path connections. 3) The performance enhancement varies among the backbone models. SPANet achieves an improvement of 2.696% (2.040% over SENet) on DenseNet, but a small improvement of 0.88% (0.482% over SENet) on VGG16. This indicates the architecture of a base network may affect the effectiveness of SPANet. Note that the four networks represent different network architectures. MobileNetV2 is typically designed for lightweight models like<ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28]</ref>. DenseNet includes shortcut connections. ResNeXt is the first one that exposes "cardinality" dimension. VGG is a popular plane-structure network. Our experimental results demonstrate that SPANet performs well on different types of CNN architectures.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: IEEE Xplore. Downloaded on July 13,2020 at 02:18:08 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="5.">CONCLUSIONS</head><p>We present the <rs type="institution">Spatial Pyramid Attention Network (SPANet)</rs>, a new design to enhance the performance of CNN. SPANet introduces the spatial pyramid structure to the attention path, which integrates the structural information and structural regularization. We explore the topology structure of attention path connections and develop three types of SPANet using different connection schemes. Experimental results on two datasets demonstrate both the efficiency and effectiveness of SPANet.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point-wise Convolution. We also evaluate the impact of pointwise convolution on recognition accuracy. We compare SENet+ with SENet++ and SPANet-B with SPANet-C on both CIFAR-100 and downsampled ImageNet datasets. Experimental results show SENet+ consistently outperforms SENet++, while SPANet-C achieves a better performance than SPANet-B in four of six cases. This indicates that point-wise convolution improves the performance of SPANet-B, but not always among the three types of SPANet.</p><p>Spatial Pyramid Structure. We evaluate the spatial pyramid structure which is a substitute for global average pooling. We compare the performance of the base networks with that of SPANet-A based models. Tables <ref type="table">1,</ref><ref type="table">2</ref>, 3, and 4 present the results. From the tables, we can see SPANet consistently outperforms the base and SENet based networks. Specifically, SPANet-MobileNetV2 surpasses MobileNetV2 by 2.064% and SENet based network by 1.814% on the ImageNet dataset. Compared to a 0.25% improvement made by SENet, SPANet-A significantly enhances the accuracy. These results show the importance of combining structural information and structural regularization in attention paths as discussed in Section 3.3.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">In</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A downsampled variant of imagenet as an alternative to the cifar datasets</title>
		<author>
			<persName><forename type="first">Patryk</forename><surname>Chrabaszcz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08819</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Highway networks</title>
		<author>
			<persName><forename type="first">Rupesh</forename><surname>Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2377" to="2385" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attentive systems: A survey</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tam V Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="86" to="110" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Competitive inner-imaging squeeze and excitation for residual network</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihua</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08920</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09925</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Parsenet: Looking wider to see better</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
