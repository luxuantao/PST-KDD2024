<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diverge-Merge Processor (DMP): Dynamic Predicated Execution of Complex Control-Flow Graphs Based on Frequently Executed Paths</title>
				<funder>
					<orgName type="full">Cockrell Foundation</orgName>
				</funder>
				<funder ref="#_JtEhDAv">
					<orgName type="full">Intel Corporation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
							<email>hyesoon@ece.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jos?</forename><forename type="middle">A Joao</forename><surname>Onur Mutlu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
							<email>patt@ece.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Diverge-Merge Processor (DMP): Dynamic Predicated Execution of Complex Control-Flow Graphs Based on Frequently Executed Paths</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a new processor architecture for handling hard-to-predict branches, the diverge-merge processor (DMP). The goal of this paradigm is to eliminate branch mispredictions due to hard-to-predict dynamic branches by dynamically predicating them without requiring ISA support for predicate registers and predicated instructions. To achieve this without incurring large hardware cost and complexity, the compiler provides control-flow information by hints and the processor dynamically predicates instructions only on frequently executed program paths. The key insight behind DMP is that most control-flow graphs look and behave like simple hammock (if-else) structures when only frequently executed paths in the graphs are considered. Therefore, DMP can dynamically predicate a much larger set of branches than simple hammock branches.</p><p>Our evaluations show that DMP outperforms a baseline processor with an aggressive branch predictor by 19.3% on average over SPEC integer 95 and 2000 benchmarks, through a reduction of 38% in pipeline flushes due to branch mispredictions, while consuming 9.0% less energy. We also compare DMP with previously proposed predication and dual-path/multipath execution paradigms in terms of performance, complexity, and energy consumption, and find that DMP is the highest performance and also the most energy-efficient design.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>State-of-the-art high performance processors employ deep pipelines to extract instruction level parallelism (ILP) and to support high clock frequencies. In the near future, processors are expected to support a large number of in-flight instructions <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13]</ref> to extract both ILP and memory-level parallelism (MLP). As shown by previous research <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b41">42]</ref>, the performance improvement provided by both pipelining and large instruction windows critically depends on the accuracy of the processor's branch predictor. Branch predictors still remain imperfect despite decades of intensive research in branch prediction. Hard-to-predict branches not only limit processor performance but also result in wasted energy consumption.</p><p>Predication has been used to avoid pipeline flushes due to branch mispredictions by converting control dependencies into data dependencies <ref type="bibr" target="#b1">[2]</ref>. With predication, the processor fetches instructions from both paths of a branch but commits only results from the correct path, effectively avoiding the pipeline flush associated with a branch misprediction. However, predication has the following problems/limitations: 1. It requires significant support (i.e. predicate registers and predicated instructions) in the instruction set architecture (ISA). 2. Statically predicated code incurs the performance overhead of predicated execution regardless of whether a branch is easy to predict or hard to predict at run-time. The overhead of predicated code is twofolds: (i) the processor always has to fetch instructions from both paths of an if-converted branch, (ii) the processor cannot execute predicated instructions or instructions that are dependent on them until the predicate value is resolved, causing ad-ditional delay in execution. Previous research showed that predicated execution sometimes hurts processor performance due to this overhead <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>. 3. A large subset of control-flow graphs are usually not converted to predicated code because either the compiler cannot if-convert (i.e. predicate) them or the overhead of predicated execution is high. A control-flow graph that has a function call, a loop, too many exit points, or too many instructions between an entry point and an exit point are examples <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Several approaches were proposed to solve these problems/limitations. Dynamic-hammock-predication <ref type="bibr" target="#b22">[23]</ref> was proposed to predicate branches without ISA support. However, dynamichammock-predication can predicate only simple hammock branches (simple if-else structures with no nested branches), which account for only a small subset of the mispredicted branches <ref type="bibr" target="#b22">[23]</ref>. Wish branches <ref type="bibr" target="#b21">[22]</ref> were proposed to reduce the overhead of predicated execution. However, wish branches inherit the limitations of software predication (1 and 3 above) with the exception that they can be applied to loop branches.</p><p>Our goal in this paper is to devise a comprehensive technique that overcomes the three problems/limitations of predication so that more processors can employ predicated execution to reduce the misprediction penalty due to hard-to-predict branches.</p><p>We propose a new processor architecture, called the Diverge-Merge Processor (DMP). DMP dynamically predicates not only simple but also complex control-flow graphs without requiring predicate registers and predicated instructions in the ISA and without incurring large hardware/energy cost and complexity. The key mechanism of DMP is that it dynamically predicates instructions only on frequently executed control-flow paths and only if a branch is hard-to-predict at run-time. Dynamically predicating only the frequently executed paths allows DMP to achieve two benefits at the same time: 1) the processor can reduce the overhead of predicated execution since it does not need to fetch/execute all instructions that are control-dependent on the predicated branch, 2) the processor can dynamically predicate a large set of control-flow graphs because a complex control-flow graph can look and behave like a simple hammock structure when only frequently executed paths are considered.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows a control-flow graph example to illustrate the key insight behind DMP. In software predication, if the compiler estimates that the branch at block A is hard-to-predict, it would convert blocks B, C, D, E, F, and G to predicated code and all these blocks would be executed together even though blocks D, F, and G are not frequently executed at run-time <ref type="bibr" target="#b30">[31]</ref>. <ref type="foot" target="#foot_0">1</ref> In contrast, DMP considers frequently executed paths at run-time, so it can dynamically predicate only blocks B, C, and E. To simplify the hardware, DMP uses some control-flow information provided by the compiler. The compiler identifies and marks suitable branches as candidates for dynamic predication. These branches are called diverge branches. The compiler also selects a control-flow merge (or reconvergence) point corresponding to each diverge branch. In this example, the compiler marks the branch at block A as a diverge branch and the entry of block H as a controlflow merge (CFM) point. Instead of the compiler specifying which blocks are predicated (and thus fetched), the processor decides what to fetch/predicate at run-time. If a diverge branch is estimated to be lowconfidence at run-time, the processor follows and dynamically predicates both paths after the branch until the CFM point. The processor follows the branch predictor outcomes on the two paths to fetch only the frequently executed blocks between a diverge branch and a CFM point. The compiler could predicate only blocks B, C, and E based on profiling <ref type="bibr" target="#b28">[29]</ref> rather than predicating all control-dependent blocks. Unfortunately, frequently executed paths change at run-time (depending on the input data set and program phase), and code predicated for only a few paths can hurt performance if other paths turn out to be frequently executed. In contrast, DMP determines and follows frequently executed paths at run-time and therefore it can flexibly adapt its dynamic predication to run-time changes (Figure <ref type="figure" target="#fig_0">1c</ref> shows the possible hammock-shaped paths that can be predicated by DMP for the example control-flow graph). Thus, DMP can dynamically predicate hard-to-predict instances of a branch with less overhead than static predication and with minimal support from the compiler. Furthermore, DMP can predicate a much wider range of control-flow graphs than dynamic-hammock-predication <ref type="bibr" target="#b22">[23]</ref> because a control-flow graph does not have to be a simple if-else structure to be dynamically predicated; it just needs to look like a simple hammock when only frequently executed paths are considered.</p><p>Our evaluation shows that DMP improves performance by 19.3% over a baseline processor that uses an aggressive 64KB branch predictor, without significantly increasing maximum power requirements. DMP reduces the number of pipeline flushes by 38%, which results in a 23% reduction in the number of fetched instructions and a 9.0% reduction in dynamic energy consumption. This paper provides a detailed description and analysis of DMP as well as a comparison of its performance, hardware complexity, and power/energy consumption with several previously published branch processing paradigms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The Diverge-Merge Concept</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Basic Idea</head><p>The compiler identifies conditional branches with control flow suitable for dynamic predication as diverge branches. A diverge branch is a branch instruction after which the execution of the program usually reconverges at a control-independent point in the control-flow graph, a point we call the control-flow merge (CFM) point. In other words, diverge branches result in hammock-shaped control flow based on frequently executed paths in the control-flow graph of the program but they are not necessarily simple hammock branches that require the control-flow graph to be hammock-shaped. The compiler also identifies a CFM point associated with the diverge branch. Diverge branches and CFM points are conveyed to the microarchitecture through modifications in the ISA, which are described in Section 3.11.</p><p>When the processor fetches a diverge branch, it estimates whether or not the branch is hard to predict using a branch confidence estimator. If the diverge branch has low confidence, the processor enters dynamic predication mode (dpred-mode). In this mode, the processor fetches both paths after the diverge branch and dynamically predicates instructions between the diverge branch and the CFM point. On each path, the processor follows the branch predictor outcomes until it reaches the CFM point. After the processor reaches the CFM point on both paths, it exits dpred-mode and starts to fetch from only one path. If the diverge branch is actually mispredicted, then the processor does not need to flush its pipeline since instructions on both paths of the branch are already fetched and the instructions on the wrong path will become NOPs through dynamic predication.</p><p>In this section, we describe the basic concepts of the three major mechanisms to support diverge-merge processing: instruction fetch support, select-?ops, and loop branches. A detailed implementation of DMP is described in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Instruction Fetch Support</head><p>In dpred-mode, the processor fetches instructions from both directions (taken and not-taken paths) of a diverge branch using two program counter (PC) registers and a round-robin scheme to fetch from the two paths in alternate cycles. On each path, the processor follows the outcomes of the branch predictor. Note that the outcomes of the branch predictor favor the frequently executed basic blocks in the control flow graph. The processor uses a separate global branch history register (GHR) to predict the next fetch address on each path, and it checks whether the predicted next fetch address is the CFM point of the diverge branch. <ref type="foot" target="#foot_1">2</ref> If the processor reaches the CFM point on one path, it stops fetching from that path and fetches from only the other path. When the processor reaches the CFM point on both paths, it exits dpred-mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Select-?ops</head><p>Instructions after the CFM point should have data dependencies on instructions from only the correct path of a diverge branch. Before the diverge branch is executed, the processor does not know which path is correct. Instead of waiting for the resolution of the diverge branch, the processor inserts select-?ops to continue renaming/execution after exiting dpred-mode. Select-?ops are similar to the ?-functions in the static single-assignment (SSA) form <ref type="bibr" target="#b13">[14]</ref> in that they "merge" the register values produced on both sides of the hammock. <ref type="foot" target="#foot_2">3</ref> Select-?ops ensure that instructions dependent on the register values produced on either side of the hammock are supplied with the correct data values that depend on the correct direction of the diverge branch. After inserting select-?ops, the processor can continue fetching and renaming instructions. If an instruction fetched after the CFM point is dependent on a register produced on either side of the hammock, it sources (i.e. depends on) the output of a select-?op. Such an instruction will be executed after the diverge branch is resolved. However, instructions that are not dependent on select-?ops are executed as soon as their sources are ready without waiting for the resolution of the diverge branch. Figure <ref type="figure" target="#fig_2">2</ref> illustrates the dynamic predication process. Note that instructions in blocks C, B, and E, which are fetched during dpred-mode, are also executed before the resolution of the diverge branch.  branches. The benefit of dynamically predicating loop branches using DMP is very similar to the benefit of wish loops <ref type="bibr" target="#b21">[22]</ref>. The key mechanism to predicate a loop-type diverge branch is that the processor needs to predicate each loop iteration separately. This is accomplished by using a different predicate register for each iteration and inserting select-?ops after each iteration. Select-?ops choose between live-out register values before and after the execution of a loop iteration, based on the outcome of each dynamic instance of the loop branch. Instructions that are executed in later iterations and that are dependent on live-outs of previous predicated iterations source the outputs of select?ops. Similarly, instructions that are fetched after the processor exits the loop and that are dependent on registers produced within the loop source the outputs of select-?ops so that they receive the correct source values even though the loop branch may be mispredicted. The pipeline does not need to be flushed if a predicated loop is iterated more times than it should be because the predicated instructions in the extra loop iterations will become NOPs and the live-out values from the correct last iteration will be propagated to dependent instructions via select-?ops. Figure <ref type="figure" target="#fig_3">3</ref> illustrates the dynamic predication process of a loop-type diverge branch (The processor enters dpred-mode after pr20 = (cond1) add pr21 &lt;-pr11, #1 p2= pr20 branch A, pr20 select-uop pr22 = p1? pr21 : pr11 select-uop pr23 = p1? pr20 : pr10 A add pr31 &lt;-pr22, #1 pr30 = (cond1) select-uop pr32 = p2? pr31 : pr22 select-uop pr33 = p2? pr30 : pr23 the first iteration and exits after the third iteration).</p><formula xml:id="formula_0">(!p1) (!p1) (!p1) pr30 = (cond3) (!p1) (!p1) (p1) (p1) (p1) branch pr30, E (b) (a) (c) add r1 &lt;-r3, #1 C r0 = (cond2) branch r0, G branch r0, E r0 = (cond3) add r1 &lt;-r2, #-1 B sub r3 &lt;-r1, r2 branch.uncond H E add r4 &lt;-r1, r3 H branch r0, C r0 = (cond1) A</formula><formula xml:id="formula_1">add pr17 &lt;-pr32, #10 B (c) branch A, pr10 A pr10 = (cond1) add pr11 &lt;-pr1, #1 A (b) (a) A B p1= pr10 NT T add r1 &lt;-r1, #1 r0 = (cond1) branch A, r0 A add r1 &lt;-r1, #1 r0 = (cond1) branch A, r0 A add r1 &lt;-r1, #1 r0 = (cond1) branch A, r0 A add r7 &lt;-r1, #10 B (p1) (p1) (p1) (p2) (p2) (p2) branch A, pr30</formula><p>There is a negative effect of predicating loops: instructions that source the results of a previous loop iteration (i.e. loop-carried dependencies) cannot be executed until the loop-type diverge branch is resolved because such instructions are dependent on select-?ops. However, we found that the negative effect of this execution delay is much less than the benefit of reducing pipeline flushes due to loop branch mispredictions. Note that the dynamic predication of a loop does not provide any performance benefit if the branch predictor iterates the loop fewer times than required by correct execution, or if the predictor has not exited the loop by the time the loop branch is resolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DMP vs. Other Branch Processing Paradigms</head><p>We compare DMP with five previously proposed mechanisms in predication and multipath execution paradigms: dynamic-hammockpredication <ref type="bibr" target="#b22">[23]</ref>, software predication <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32]</ref>, wish branches <ref type="bibr" target="#b21">[22]</ref>, selective/limited dual-path execution (dual-path) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b14">15]</ref>, and multipath/PolyPath execution (multipath) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b24">25]</ref>. First, we classify control-flow graphs (CFGs) into five different categories to illustrate the differences between these mechanisms more clearly.</p><p>Figure <ref type="figure" target="#fig_5">4</ref> shows examples of the five different CFG types. Simple hammock (Figure <ref type="figure" target="#fig_5">4a</ref>) is an if or if-else structure that does not have any nested branches inside the hammock. Nested hammock (Figure <ref type="figure" target="#fig_5">4b</ref>) is an if-else structure that has multiple levels of nested branches. Frequently-hammock (Figure <ref type="figure" target="#fig_5">4c</ref>) is a CFG that becomes a simple hammock if we consider only frequently executed paths. Loop (Figure <ref type="figure" target="#fig_5">4d</ref>) is a cyclic CFG (for, do-while, or while structure). Non-merging control-flow (Figure <ref type="figure" target="#fig_5">4e</ref>) is a CFG that does not have a control-flow merge point even if we consider only frequently executed paths. <ref type="foot" target="#foot_3">4</ref> Figure <ref type="figure" target="#fig_6">5</ref> shows the frequency of branch mispredictions due to each CFG type. Table <ref type="table">1</ref> summarizes which blocks are fetched/predicated in different processing models for each CFG type, assuming that the branch in block A is hard to predict.  Dynamic-hammock-predication can predicate only simple hammocks which account for 12% of all mispredicted branches. Simple hammocks by themselves account for a significant percentage of mispredictions in only two benchmarks: vpr (40%) and twolf (36%). We expect dynamic-hammock-predication will improve the performance of these two benchmarks.</p><p>Software predication can predicate both simple and nested hammocks, which in total account for 16% of all mispredicted branches. Software predication fetches all basic blocks between an if-converted branch and the corresponding control-flow merge point. For example, in the nested hammock case (Figure <ref type="figure" target="#fig_5">4b</ref>), software predication fetches blocks B, C, D, E, F, G, H, and I, whereas DMP fetches blocks B, C, D, G, H, and I. Current compilers usually do not predicate frequentlyhammocks since the overhead of predicated code would be too high if Table <ref type="table">1</ref>. Fetched instructions in different processing models (after the branch at A is estimated to be low-confidence) We assume that the loop branch in block A (Figure <ref type="figure" target="#fig_5">4d</ref>) is predicted taken twice after it is estimated to be low-confidence.  these CFGs include function calls, cyclic control-flow, too many exit points, or too many instructions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31]</ref>. Note that hyperblock formation <ref type="bibr" target="#b28">[29]</ref> can predicate frequently-hammocks at the cost of increased code size, but it is not an adaptive technique because frequently executed basic blocks change at run-time. Even if we assume that software predication can predicate all frequently-hammocks, it could predicate up to 56% of all mispredicted branches. Wish branches can predicate even loops, which account for 10% of all mispredicted branches, in addition to what software predication can do. The main difference between wish branches and software predication is that the wish branch mechanism can selectively predicate each dynamic instance of a branch. With wish branches, a branch is predicated only if it is hard to predict at run-time, whereas with software predication a branch is predicated for all its dynamic instances. Thus, wish branches reduce the overhead of software predication. However, even with wish branches, all basic blocks between an if-converted branch and the corresponding CFM point are fetched/predicated. Therefore, wish branches also have higher performance overhead for nested hammocks than DMP.</p><p>Note that software predication (and wish branches) can eliminate a branch misprediction due to a branch that is control-dependent on another hard-to-predict branch (e.g. branch at B is control-dependent on branch at A in Figure <ref type="figure" target="#fig_5">4b</ref>), since it predicates all the basic blocks within a nested hammock. This benefit is not possible with any of the other paradigms except multipath, but we found that it provides significant performance benefit only in two benchmarks (3% in twolf, 2% in go).</p><p>Selective/limited dual-path execution fetches from two paths after a hard-to-predict branch. The instructions on the wrong path are selectively flushed when the branch is resolved. Dual-path execution is applicable to any kind of CFG because the control-flow does not have to reconverge. Hence, dual-path can potentially eliminate the branch misprediction penalty for all five CFG types. However, the dual-path mechanism needs to fetch a larger number of instructions than any of the other mechanisms (except multipath) because it continues fetching from two paths until the hard-to-predict branch is resolved even though the processor may have already reached a control-independent point in the CFG. For example, in the simple hammock case (Figure <ref type="figure" target="#fig_5">4a</ref>), DMP fetches blocks D, E, and F only once, but dual-path fetches D, E, and F twice (once for each path). Therefore, the overhead of dual-path is much higher than that of DMP. Detailed comparisons of the overhead and performance of different processing models are provided in Section 5.</p><p>Multipath execution is a generalized form of dual-path execution in that it fetches both paths after every low-confidence branch and therefore it can execute along many (more than two) different paths at the same time. This increases the probability of having the correct path in the processor's instruction window. However, only one of the outstanding paths is the correct path and instructions on every other path have to be flushed. Furthermore, instructions after a controlflow independent point have to be fetched/executed separately for each path (like dual-path but unlike DMP), which causes the processing resources to be wasted for instructions on all paths but one. For example, if the number of outstanding paths is 8, then a multipath processor wastes 87.5% of its fetch/execution resources for wrong-path/useless instructions even after a control-independent point. Hence, the overhead of multipath is much higher than that of DMP. In the example of Table <ref type="table">1</ref> the behavior of multipath is the same as that of dual-path because the example assumes there is only one hard-to-predict branch to simplify the explanation.</p><p>DMP can predicate simple hammocks, nested hammocks, frequently-hammocks, and loops. On average, these four CFG types account for 66% of all branch mispredictions. The number of fetched instructions in DMP is less than or equal to other mechanisms for all CFG types, as shown in Table <ref type="table">1</ref>. Hence, we expect DMP to eliminate branch mispredictions more efficiently (i.e. with less overhead) than the other processing paradigms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation of DMP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Entering Dynamic Predication Mode</head><p>The diverge-merge processor enters dynamic predication mode (dpred-mode) if a diverge branch is estimated to be low-confidence at run-time. <ref type="foot" target="#foot_4">5</ref> When the processor enters dpred-mode, it needs to do the following:</p><p>1. The front-end stores the address of the CFM point associated with the diverge branch into a buffer called CFM register. The processor also marks the diverge branch as the branch that caused entry into dpred-mode. 2. The front-end forks (i.e. creates a copy of) the return address stack (RAS) and the GHR when the processor enters dpredmode. In dpred-mode, the processor accesses the same branch predictor table with two different GHRs (one for each path) but only correct path instructions update the table after they commit.</p><p>A separate RAS is needed for each path. The processor forks the register alias table (RAT) when the diverge branch is renamed so that each path uses a separate RAT for register renaming in dpred-mode. This hardware support is similar to the dual-path execution mechanisms <ref type="bibr" target="#b0">[1]</ref>. 3. The front-end allocates a predicate register for the initiated dpred-mode. An instruction fetched in dpred-mode carries the predicate register identifier (id) with an extra bit indicating whether the instruction is on the taken or the not-taken path of the diverge branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multiple CFM points</head><p>DMP can support more than one CFM point for a diverge branch to enable the predication of dynamic hammocks that start from the same branch but end at different control-independent points. The compiler provides multiple CFM points. At run-time, the processor chooses the CFM point reached first on any path of the diverge branch and uses it to end dpred-mode. To support multiple CFM points, the CFM register is extended to hold multiple CFM-point addresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Exiting Dynamic Predication Mode</head><p>DMP exits dpred-mode when either (1) both paths of a diverge branch have reached the corresponding CFM point or (2) a diverge branch is resolved. The processor marks the last instruction fetched in dpred-mode (i.e. the last predicated instruction). The last predicated instruction triggers the insertion of select-?ops after it is renamed.</p><p>DMP employs two policies to exit dpred-mode early to increase the benefit and reduce the overhead of dynamic predication:</p><p>1. Counter Policy: CFM points are chosen based on frequently executed paths determined through compile-time profiling. At runtime, the processor might not reach a CFM point if the branch predictor predicts that a different path should be executed. For example, in Figure <ref type="figure" target="#fig_5">4c</ref>, the processor could fetch blocks C and F. In that case, the processor never reaches the CFM point and hence continuing dynamic predication is less likely to provide benefit. To stop dynamic predication early (before the diverge branch is resolved) in such cases, we use a heuristic. If the processor does not reach the CFM point until a certain number of instructions (N) are fetched on any of the two paths, it exits dpred-mode. N can be a single global threshold or it can be chosen by the compiler for each diverge branch. We found that a per-branch threshold provides 2.3% higher performance than a global threshold because the number of instructions executed to reach the CFM point varies across diverge branches. After exiting dpredmode early, the processor continues to fetch from only the predicted direction of the diverge branch.</p><p>2. Yield Policy: DMP fetches only two paths at the same time. If the processor encounters another low-confidence diverge branch during dpred-mode, it has two choices: it either treats the branch as a normal (non-diverge) branch or exits dpred-mode for the earlier diverge branch and enters dpred-mode for the later branch. We found that a low-confidence diverge branch seen on the predicted path of a dpred-mode-causing diverge branch usually has a higher probability to be mispredicted than the dpred-mode-causing diverge branch. Moreover, dynamically predicating the later control-flow dependent diverge branch usually has less overhead than predicating the earlier diverge branch because the number of instructions inside the CFG of the later branch is smaller (since the later branch is usually a nested branch of the previous diverge branch). Therefore, our DMP implementation exits dpred-mode for the earlier diverge branch and enters dpred-mode for the later diverge branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Select-?op Mechanism</head><p>Select-?ops are inserted when the processor reaches the CFM point on both paths. Select-?ops choose data values that were produced from the two paths of a diverge branch so that instructions after the CFM point receive correct data values from select-?ops. Our select?op generation mechanism is similar to Wang et al.'s <ref type="bibr" target="#b44">[45]</ref>. However, our scheme is simpler than theirs because it needs to compare only two RATs to generate the select-?ops. A possible implementation of our scheme is explained below.</p><p>When a diverge branch that caused entry into dpred-mode reaches the renaming stage, the processor forks the RAT. The processor uses two different RATs, one for each path of the diverge branch. We extend the RAT with one extra bit (M -modified-) per entry to indicate that the corresponding architectural register has been renamed in dpred-mode. Upon entering dpred-mode, all M bits are cleared. When an architectural register is renamed in dpred-mode, its M bit is set.</p><p>When the last predicated instruction reaches the register renaming stage, the select-?op insertion logic compares the two RATs. <ref type="foot" target="#foot_5">6</ref>If the M bit is set for an architectural register in either of the two RATs, a select-?op is inserted to choose, according to the predicate register value, between the two physical registers assigned to that architectural register in the two RATs. A select-?op allocates a new physical register (PR new ) for the architectural register. Conceptually, the operation of a select-?op can be summarized as PRnew=(predicate register value)?PRT :PRNT , where PR T (PR N T ) is the physical register assigned to the architectural register in the RAT of the taken (not-taken) path.</p><p>A select-?op is executed when the predicate value and the selected source operand are ready. As a performance optimization, a select-?op does not wait for a source register that will not be selected. Note that the select-?op generation logic operates in parallel with work done in other pipeline stages and its implementation does not increase the pipeline depth of the processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Handling Loop Branches</head><p>Loop branches are treated differently from non-loop branches. One direction of a loop branch is the exit of the loop and the other direction is one more iteration of the loop. When the processor enters dpredmode for a loop branch, only one path (the loop iteration direction) is executed and the processor will fetch the same static loop branch again. Entering dpred-mode for a loop branch always implies the execution of one more loop iteration.</p><p>The processor enters dpred-mode for a loop if the loop-type diverge branch is low confidence. When the processor fetches the same static loop branch again during dpred-mode, it exits dpred-mode and inserts select-?ops. If the branch is predicted to iterate the loop once more, the processor enters dpred-mode again with a different predicate register id <ref type="foot" target="#foot_6">7</ref> , regardless of the confidence of the branch prediction. In other words, once the processor dynamically predicates one iteration of the loop, it continues to dynamically predicate the iterations until the loop is exited by the branch predictor. The processor stores the predicate register ids associated with the same static loop branch in a small buffer and these are later used when the branch is resolved as we will describe in Section 3.6. If the branch is predicted to exit the loop, the processor does not enter dpred-mode again but it starts to fetch from the exit of the loop after inserting select-?ops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Resolution of Diverge Branches</head><p>When a diverge branch that caused entry into dpred-mode is resolved, the processor does the following:</p><p>1. It broadcasts the predicate register id of the diverge branch with the correct branch direction (taken or not-taken). Instructions with the same predicate id and the same direction are said to be predicated-TRUE and those with the same predicate id but different direction are said to be predicated-FALSE. 2. If the processor is still in dpred-mode for that predicate register id, it simply exits dpred-mode and continues fetching only from the correct path as determined by the resolved branch. If the processor has already exited dpred-mode, it does not need to take any special action. In either case, the pipeline is not flushed. 3. If a loop-type diverge branch exits the loop (i.e. resolved as not-taken in a backward loop), the processor also broadcasts the predicate id's that were assigned for later loop iterations along with the correct branch direction in consecutive cycles. <ref type="foot" target="#foot_7">8</ref> This ensures that the select-?ops after each later loop iteration choose the correct live-out values.</p><p>DMP flushes its pipeline for any mispredicted branch that did not cause entry into dpred-mode, such as a mispredicted branch that was fetched in dpred-mode and turned out to be predicated-TRUE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Instruction Execution and Retirement</head><p>Dynamically predicated instructions are executed just like other instructions (except for store-load forwarding described in Section 3.8). Since these instructions depend on the predicate value only for retirement purposes, they can be executed before the predicate value (i.e. the diverge branch) is resolved. If the predicate value is known to be FALSE, the processor does not need to execute the instructions or allocate resources for them. Nonetheless, all predicated instructions consume retirement bandwidth. When a predicated-FALSE instruction is ready to be retired, the processor simply frees the physical register (along with other resources) allocated for that instruction and does not update the architectural state with its results. 9 The predicate register associated with dpred-mode is released when the last predicated instruction is retired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Load and Store Instructions</head><p>Dynamically predicated load instructions are executed like normal load instructions. Dynamically predicated store instructions are sent to the store buffer with their predicate register id. However, a predicated store instruction is not sent further down the memory system (i.e. into the caches) until it is known to be predicated-TRUE. The processor drops all predicated-FALSE store requests. Thus, DMP requires the store buffer logic to check the predicate register value before sending a store request to the memory system.</p><p>DMP requires support in the store-load forwarding logic. The forwarding logic should check not only the addresses but also the predicate register ids. The logic can forward from: (1) a non-predicated store to any later load, (2) a predicated store whose predicate register value is known to be TRUE to any later load, or (3) a predicated store whose predicate register is not ready to a later load with the same predicate register id (i.e. on the same dynamically predicated path).</p><p>?ops from the previous iteration are executed (since select-?ops of the later iteration are dependent on the select-?ops of the previous iteration). 9 In a current out-of-order processor, when an instruction is ready to be retired, the processor frees the physical register allocated by the previous instruction that wrote to the same architectural register. This is exactly how physical registers are freed in DMP for non-predicated and predicated-TRUE instructions. The only difference is that a predicated-FALSE instruction frees the physical register allocated by itself (since that physical register will not be part of the architectural state) rather than the physical register allocated by the previous instruction that wrote to the same architectural register.</p><p>If forwarding is not possible, the load waits. Note that this mechanism and structures to support it are the same as the store-load forwarding mechanism in dynamic-hammock-predication <ref type="bibr" target="#b22">[23]</ref>. An outof-order execution processor that implements software predication or wish branches also requires the same support in the store buffer and store-load forwarding logic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9.">Interrupts and Exceptions</head><p>DMP does not require any special support for handling interrupts or exceptions. When the pipeline is flushed before servicing the interrupt or exception, any speculative state, including DMP-specific state is also flushed. There is no need to save and restore predicate registers, unlike software predication. The processor restarts in normal mode right after the last architectural retired instruction after coming back from the interrupt/exception service. Exceptions generated by predicated-FALSE instructions are simply dropped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.10.">Hardware Complexity Analysis</head><p>DMP increases hardware complexity compared to current processors but it is an energy efficient design as we will show in Section 5.5. Some of the hardware required for DMP is already present in current processors. For example, select-?ops are similar to CMOV operations and complex ?op generation and insertion schemes are already implemented in x86 processors. Table <ref type="table" target="#tab_1">2</ref> summarizes the additional hardware support required for DMP and the other processing models. DMP requires slightly more hardware support than dynamichammock-predication and dual-path but much less than multipath.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.11.">ISA Support for Diverge Branches</head><p>We present an example of how the compiler can transfer diverge branch and CFM point information to the hardware through simple modifications in the ISA. Diverge branches are distinguished with two bits in the ISA's branch instruction format. The first bit indicates whether or not the branch is a diverge branch and the second bit indicates whether or not a branch is of loop-type. If a branch is a diverge branch, the following N bits in the program code are interpreted as the encoding for the associated CFM points. A CFM point address can be encoded as a relative address from the diverge branch address or as an absolute address without the most significant bits. Since CFM points are located close to a diverge branch we found that 10 bits are enough to encode each CFM point selected by our compiler algorithm. The ISA could dedicate a fixed number of bytes to encode CFM points or the number of bytes can vary depending on the number of CFM points for each diverge branch. We allow maximum 3 CFM points per diverge branch. To support early exit (Section 3.3), the compiler also uses L extra bits to encode the maximum distance between a branch and its CFM point (L is a scaled 4-bit value in our implementation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Simulation Methodology</head><p>We use an execution-driven simulator of a processor that implements the Alpha ISA. An aggressive, 64KB branch predictor is used in the baseline processor. The parameters of the baseline processor are shown in Table <ref type="table">3</ref>.</p><p>We also model a less aggressive (base2) processor to evaluate the DMP concept in a configuration similar to today's processors. Table <ref type="table" target="#tab_2">4</ref> shows the parameters of the less aggressive processor that are different from the baseline processor.</p><p>The experiments are run using the 12 SPEC CPU 2000 integer benchmarks and 5 SPEC 95 integer benchmarks. <ref type="foot" target="#foot_8">10</ref> Table <ref type="table" target="#tab_3">5</ref> shows the characteristics of the benchmarks on the baseline processor. All binaries are compiled for the Alpha ISA with the -fast optimizations. We use a binary instrumentation tool that marks diverge branches and their respective CFM points after profiling. The benchmarks are run to completion with a reduced input set <ref type="bibr" target="#b25">[26]</ref> to reduce simulation time. In all the IPC (retired Instructions Per Cycle) performance results shown in the rest of the paper for DMP, instructions whose predicate values are FALSE and select-?ops inserted to support dynamic predication do not contribute to the instruction count. A detailed description of how we model different branch processing paradigms in our simulations is provided in an extended version of this paper <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Power Model</head><p>We incorporated the Wattch infrastructure <ref type="bibr" target="#b4">[5]</ref> into our cycleaccurate simulator. The power model is based on 100nm technology. The frequency we assume is 4GHz for the baseline processor and 1.5GHz for the less aggressive processor. We use the aggressive CC3 clock-gating model in Wattch: unused units dissipate only 10% of their maximum power when they are not accessed <ref type="bibr" target="#b4">[5]</ref>. All additional structures and instructions required by DMP are faithfully accounted for in the power model: the confidence estimator, one more RAT/RAS/GHR, select-?op generation/execution logic, additional microcode fields to support select-?ops, additional fields in the BTB to mark diverge branches and to cache CFM points, predicate and CFM registers, and modifications to handle load-store forwarding and instruction retirement. Forking of tables and insertion of select-?ops are modeled by increasing the dynamic access counters for every relevant structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Compiler Support for Diverge Branch and CFM Point Selection</head><p>Diverge branch and CFM point candidates are determined based on a combination of CFG analysis and profiling. Simple hammocks, nested hammocks, and loops are found by the compiler using CFG analysis. To determine frequently-hammocks, the compiler finds CFM point candidates (i.e. post-dominators) considering the portions of a program's control-flow graph that are executed during the profiling run. A branch in a suitable CFG is marked as a possible diverge branch if it is responsible for at least 0.1% of the total number of mispredictions during profiling. A CFM point candidate is selected as a CFM point if it is reached from a diverge branch for at least 30% of the dynamic instances of the branch during the profiling run and if it is within 120 static instructions from the diverge branch. The thresholds used in compiler heuristics are determined experimentally. We used the train input sets to collect profiling information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Performance of the Diverge-Merge Processor</head><p>Figure <ref type="figure" target="#fig_7">6</ref> shows the performance improvement of dynamichammock-predication, dual-path, multipath, and DMP over the baseline processor. The average IPC improvement over all benchmarks is 3.5% for dynamic-hammock-predication, 4.8% for dual-path, 8.8% for multipath, <ref type="foot" target="#foot_9">11</ref> and 19.3% for DMP. DMP improves the IPC by more than 20% on vpr (58%), mcf (47%), parser (26%), twolf (31%), compress (23%), and ijpeg (25%). A significant portion (more than 60%) of branch mispredictions in these benchmarks is due to branches that can be dynamically predicated by DMP as was shown in Figure <ref type="figure" target="#fig_6">5</ref>.</p><p>Mcf shows additional performance benefit due to the prefetching effect caused by predicated-FALSE instructions. In bzip2, even though 87% of mispredictions are due to frequently-hammocks, DMP improves IPC by only 12.2% over the baseline. Most frequently-hammocks in bzip2 have more than one CFM point and the run-time heuristic used by DMP to decide which CFM point to use for dynamic predication (Section 3.2) does not work well for bzip2. Dynamic-hammock-predication provides over 10% performance improvement on vpr and twolf because a relatively large portion of mispredictions is due to simple hammocks. The performance benefit of dual-path is higher than that of dynamic-hammock-predication but much less than that of DMP, even though dual-path is applicable to any kind of CFG. This is due to two reasons. First, dual-path fetches a larger number of instructions from the wrong path compared to dynamic-hammock-predication and DMP, as was shown in Table <ref type="table">1</ref>. Figure <ref type="figure" target="#fig_8">7</ref> shows the average number of fetched wrong-path instructions per each entry into dynamic-predication/dual-path mode in the different processors. On average, dual-path fetches 134 wrong-path instructions, which is much higher than 4 for dynamic-hammock-predication, and 20 for DMP (note that this overhead is incurred even if the lowconfidence branch turns out to be correctly predicted). Second, dualpath is applicable to one low-confidence branch at a time. While a formance improvement for dual-path (with extra execution resources to support dual-path), and Klauser and Grunwald <ref type="bibr" target="#b23">[24]</ref> reported average 9.3% performance improvement for PolyPath (multipath) with a round-robin fetch scheme. The differences between their and our results are due to different branch predictors, machine configurations, and benchmarks. Our baseline branch predictor is much more accurate than those in previous work. dual-path processor is fetching from two paths, it cannot perform dualpath execution for another low-confidence branch. However, DMP can diverge again if another low confidence diverge branch is encountered after the processor has reached the CFM point of a previous diverge branch and exited dpred-mode. For this reason, we found that dualpath cannot reduce as many pipeline flushes due to branch mispredictions as DMP. As Figure <ref type="figure" target="#fig_9">8</ref> shows, dual-path reduces pipeline flushes by 18% whereas DMP reduces them by 38%. Multipath performs better than or similarly to DMP on gzip, gcc, and go. In these benchmarks more than 40% of branch mispredictions are due to non-merging control flow that cannot be predicated by DMP but can be eliminated by multipath. Multipath also performs better than dual-path execution on average because it is applicable to multiple outstanding low-confidence branches. On average, multipath reduces pipeline flushes by 40%, similarly to DMP. However, because multipath has very high overhead (200 wrong-path instructions per low-confidence branch, as shown in Figure <ref type="figure" target="#fig_8">7</ref>), its average performance improvement is much less than that of DMP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparisons with Software Predication and Wish Branches</head><p>Figure <ref type="figure" target="#fig_10">9</ref> shows the execution time reduction over the baseline for limited software predication 12 and wish branches. Since the number of executed instructions is different in limited software predication and wish branches, we use the execution time metric for performance comparisons. Overall, limited software predication reduces execution time by 3.8%,wish branches by 6.4%, and DMP by 13.0%. In most benchmarks, wish branches perform better than predication because they can selectively enable predicated execution at run-time, thereby reducing the overhead of predication. Wish branches perform significantly better than limited software predication on vpr, parser, and ijpeg because they can be applied to loop branches. There are some differences between previous results <ref type="bibr" target="#b21">[22]</ref> and our results in the benefit of software predication and wish branches. The differences are due to the following: (1) our baseline processor already employs CMOVs which provide the performance benefit of predication for very small basic blocks, (2) ISA differences (Alpha vs. IA-64), (3) in our model of software predication, there is no benefit due to compiler optimizations that can be enabled with larger basic blocks in predicated code, (4) since wish branches dynamically reduce the overhead of software predication, they allow larger code blocks to be predicated, but we could not model this effect because Alpha ISA/compiler does not support predication.</p><p>Even though wish branches perform better than limited software predication, there is a large performance difference between wish branches and DMP. The main reason is that DMP can predicate frequently-hammocks, the majority of mispredicted branches in many benchmarks as shown in Figure <ref type="figure" target="#fig_6">5</ref>. Only parser does not have many frequently-hammocks, so wish branches and DMP perform similarly for this benchmark. Figure <ref type="figure" target="#fig_11">10</ref> shows the performance improvement of DMP over the baseline if DMP is allowed to dynamically predicate: (1) only simple hammocks, (2) simple and nested hammocks, (3) simple, nested, frequently-hammocks, and (4) simple, nested, frequentlyhammocks and loops. There is a large performance provided by the predication of frequently-hammocks as they are the single largest cause of branch mispredictions. Hence, DMP provides large performance improvements by enabling the predication of a wider range of CFGs than limited software predication and wish branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis of the Performance Impact of Enhanced DMP Mechanisms</head><p>Figure <ref type="figure" target="#fig_0">11</ref> shows the performance improvement provided by the enhanced mechanisms in DMP. Single-cfm supports only a single CFM point for each diverge branch without any enhancements. Single-cfm by itself provides 11.4% IPC improvement over the baseline processor. Multiple-cfm supports more than one CFM point for each diverge 12 We call our software predication model "limited software predication" because we do not model compiler optimization effects enabled via if-conversion.   shows the execution time reduction over the less aggressive baseline for limited software predication, wish branches, and DMP. Since the less aggressive processor incurs a smaller penalty for a branch misprediction, improved branch handling has less performance potential than in the baseline processor. However, DMP still provides 7.8% IPC improvement by reducing pipeline flushes by 30%, whereas dynamic-hammock-predication, dual-path and multipath improve IPC by 1.6%, 1.5%, and 1.3% respectively. Limited software predication reduces execution time by 1.0%, wish branches by 2.9%, and DMP by 5.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">Effect of a Different Branch Predictor</head><p>We also evaluate DMP with a recently developed branch predictor, O-GEHL <ref type="bibr" target="#b36">[37]</ref>. The O-GEHL predictor requires a complex hashing mechanism to index the branch predictor tables, but it effectively increases the global branch history length. As Figure <ref type="figure" target="#fig_14">13</ref> shows, replacing the baseline processor's perceptron predictor with a more complex, 64KB O-GEHL branch predictor (OGEHL-base) provides 13.8% performance improvement, which is smaller than the 19.3% performance improvement provided by implementing diverge-merge processing (perceptron-DMP). Furthermore, using DMP with an O-GEHL predictor (OGEHL-DMP) improves the average IPC by 13.3% over OGEHL-base and by 29% over our baseline processor. Hence, DMP still provides large performance benefits when the baseline processor's branch predictor is more complex and more accurate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3.">Effect of Confidence Estimator Size</head><p>Figure <ref type="figure" target="#fig_15">14</ref> shows the performance of dynamic-hammock-predication, dual-path, multipath and DMP with 512B, 2KB, 4KB, and 16KB confidence estimators and a perfect confidence estimator. Our baseline employs a 2KB enhanced JRS confidence estimator <ref type="bibr" target="#b18">[19]</ref>, which has 14% PVN ( accuracy) and 70% SPEC ( coverage) <ref type="bibr" target="#b16">[17]</ref>. 13 Even with a 512byte estimator, DMP still provides 18.4% performance improvement. The benefit of dual-path/multipath increases significantly with a perfect estimator because dual-path/multipath has very high overhead as shown in Figure <ref type="figure" target="#fig_8">7</ref>, and a perfect confidence estimator eliminates the incurrence of this large overhead for correctly-predicted branches. However, even with a perfect estimator, dual-path/multipath has less potential than DMP because (1) dual-path is applicable to one lowconfidence branch at a time (as explained previously in Section 5.1), (2) the overhead of dual-path/multipath is still much higher than that of DMP for a low-confidence branch because dual-path/multipath executes the same instructions twice/multiple times after a controlindependent point in the program. 13 These numbers are actually lower than what was previously published <ref type="bibr" target="#b16">[17]</ref> because our baseline branch predictor uses a different algorithm and has a much higher prediction accuracy than that of <ref type="bibr" target="#b16">[17]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Power Analysis</head><p>Figure <ref type="figure" target="#fig_16">15</ref> (left) shows the average increase/reduction due to DMP in the number of fetched/executed instructions, maximum power, energy, and energy-delay product compared to the baseline. Even though DMP has to fetch instructions from both paths of every dynamically predicated branch, the total number of fetched instructions decreases by 23% because DMP reduces pipeline flushes and thus eliminates the fetch of many wrong-path instructions. DMP executes 1% more instructions than the baseline due to the overhead of select-?ops and predicated-FALSE instructions. Due to the extra hardware required to support DMP, maximum power consumption increases by 1.4%. However, because of the reduction in fetched instructions, energy consumption is reduced by 9.0%. Moreover, energy-delay product decreases by 22.3% because of both the performance improvement and energy reduction. Hence, although DMP increases hardware complexity, it actually increases energy-efficiency by reducing pipeline flushes due to branch mispredictions. DMP is an energy-efficient design even in the less aggressive processor configuration as Figure <ref type="figure" target="#fig_16">15</ref> (right) shows.</p><p>Table <ref type="table" target="#tab_5">6</ref> provides a power/energy comparison of the branch pro- cessing paradigms. DMP reduces energy consumption and energydelay product much more than other approaches while it increases the maximum power requirements slightly more than the most relevant hardware techniques (dynamic-hammock-predication and dual-path). Note that multipath significantly increases both maximum power and energy consumption due to the extra hardware to support many outstanding paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Related Work on Predication</head><p>Software predication has been studied intensively to reduce the branch misprediction penalty <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b5">6]</ref> and to increase instruction-level parallelism <ref type="bibr" target="#b1">[2]</ref>. However, in a real IA-64 implementation, predicated execution was found to provide a small (2%) performance improvement <ref type="bibr" target="#b8">[9]</ref>. This small performance gain is due to the overhead and limitations of compile-time predication (described in Section 1), which sometimes offset the benefit of reducing the pipeline flushes due to branch mispredictions. Kim et al. <ref type="bibr" target="#b21">[22]</ref> proposed wish branches to reduce the overhead of software predication by combining conditional branching and predication. DMP can predicate a larger set of CFGs than wish branches and it overcomes the major disadvantage of wish-branches: the requirement for a predicated ISA. Klauser et al. <ref type="bibr" target="#b22">[23]</ref> proposed dynamic-hammock-predication for predicating only simple hammocks without support for predicated instructions in the ISA. DMP builds on dynamic-hammock-predication, but can predicate a much larger set of CFGs. Hence, as we showed in Section 5, DMP provides better performance and better energy efficiency.</p><p>Hyperblock formation <ref type="bibr" target="#b28">[29]</ref> predicates frequently executed basic blocks based on profiling data, and it can predicate more complex CFGs than nested hammocks by tail duplication and loop peeling. The benefits of hyperblocks are that they increase the compiler's scope for code optimization and instruction scheduling (by enlarging basic blocks) in VLIW processors and they reduce branch mispredictions <ref type="bibr" target="#b27">[28]</ref>. Unlike DMP, hyperblocks still require a predicated ISA, incur the overhead of software predication, are not adaptive to runtime changes in frequently executed control flow paths, and increase the code size <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Related Work on Dual-/Multi-path Execution</head><p>Heil and Smith <ref type="bibr" target="#b17">[18]</ref> and Farrens et al. <ref type="bibr" target="#b14">[15]</ref> proposed selective/limited dual path execution mechanisms. As we showed in Section 5, dual-path execution does not provide a performance improvement as significant as that of DMP because dual-path execution always wastes half of the fetch/execution resources even after a controlindependent point in the program.</p><p>Selective eager execution (PolyPath) was proposed by Klauser et al. <ref type="bibr" target="#b24">[25]</ref> as an implementation of multipath execution <ref type="bibr" target="#b33">[34]</ref>. Multipath execution requires more hardware cost and complexity (e.g. multiple RATs/PCs/GHRs/RASs, logic to generate/manage path IDs/tags for multiple paths, logic to selectively flush the wrong paths, and more complex store-load forwarding logic that can support multiple outstanding paths) than DMP to keep multiple paths in the instruction window. As we have shown in Section 5.5, multipath execution significantly increases maximum power and energy consumption without providing as large performance improvements as that of DMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Related Work on Control Flow Independence</head><p>Several hardware mechanisms were proposed to exploit control flow independence <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref>. These techniques aim to avoid flushing the processor pipeline if the processor is known to be at a control-independent point in the program when a mispredicted branch is resolved. In contrast to DMP, they require complex hardware to remove the control-dependent wrong-path instructions from the processor and to insert the control-dependent correct-path instructions into the pipeline after a branch misprediction. Hardware is also required to form correct data dependencies for the inserted correct path instructions. Furthermore, control-independent instructions that are data-dependent on the inserted or removed instructions have to be rescheduled and re-executed with the correct data dependencies and after the processor finishes fetching and renaming the new inserted instructions. The logic required for ensuring correct data dependencies for both control-dependent and control-independent instructions is complicated as Rotenberg et al. pointed out <ref type="bibr" target="#b34">[35]</ref>.</p><p>Collins et al. <ref type="bibr" target="#b11">[12]</ref> introduced dynamic reconvergence prediction, a hardware-based technique to identify control reconvergence points (i.e. our CFM points) without compiler support. This technique can be combined with DMP (so that CFM points are discovered at runtime rather than compile-time) and any of the mechanisms that exploit control-flow independence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Work</head><p>This paper proposed the diverge-merge processor (DMP) as an efficient architecture for compiler-assisted dynamic predicated execution. DMP dynamically predicates hard-to-predict instances of staticallyselected diverge branches. The major contributions of the divergemerge processing concept are:</p><p>1. DMP enables the dynamic predication of branches that result in complex control-flow graphs rather than limiting dynamic predication to simple hammock branches. The key insight is that most control-flow graphs look and behave like simple hammock (if-else) structures when only frequently executed paths in the graphs are considered. Therefore, DMP can eliminate branch mispredictions due to a much larger set of branches than previous predication techniques such as software predication and dynamic hammock predication. 2. DMP concurrently overcomes the three major limitations of software predication (described in Section 1). 3. DMP eliminates branch misprediction flushes much more efficiently (i.e. with less instruction execution overhead) than alternative approaches, especially dual-path and multipath execution (as shown in Table <ref type="table">1</ref> and Figure <ref type="figure" target="#fig_8">7</ref>).</p><p>Our results show that DMP outperforms an aggressive baseline processor with a very large branch predictor by 19.3% while consuming 9.0% less energy. Furthermore, DMP provides higher performance and better energy-efficiency than dynamic hammock predication, dual-path/multipath execution, software predication, and wish branches.</p><p>The proposed DMP mechanism still requires some ISA support. A cost-efficient hardware mechanism to detect diverge branches and CFM points at run-time would eliminate the need to change the ISA. Developing such mechanisms is part of our future work. The results presented in this paper are based on our initial implementation of DMP using relatively simple compiler and hardware heuristics/algorithms. The performance improvement provided by DMP can be increased further by future research aimed at improving these techniques. On the compiler side, better heuristics and profiling techniques can be developed to select diverge branches and CFM points. On the hardware side, better confidence estimators are worthy of research since they critically affect the performance benefit of dynamic predication.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Control-flow graph (CFG) example: (a) source code (b) CFG (c) possible paths (hammocks) that can be predicated by DMP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An example of how the instruction stream in Figure 1b is dynamically predicated: (a) fetched blocks (b) fetched assembly instructions (c) instructions after register renaming</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. An example of how a loop-type diverge branch is dynamically predicated: (a) CFG (b) fetched assembly instructions (c) instructions after register renaming</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Control-flow graphs: (a) simple hammock (b) nested hammock (c) frequently-hammock (d) loop (e) non-merging control flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Distribution of mispredicted branches based on CFG type</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Performance improvement provided by DMP vs. dynamichammock-predication, dual-path, and multipath execution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Fetched wrong-path instructions per entry into dynamicpredication/dual-path mode (i.e. per low-confidence branch)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. % reduction in pipeline flushes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. DMP vs. limited software predication and wish branches</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. DMP performance when different CFG types are dynamically predicated</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 . 5 . 4 . Sensitivity to Microarchitecture Parameters 5 . 4 . 1 .</head><label>1154541</label><figDesc>Figure 11. Performance impact of enhanced DMP mechanisms 5.4. Sensitivity to Microarchitecture Parameters 5.4.1. Evaluation on the Less Aggressive Processor Figure 12 (left)shows the performance benefit for dynamic-hammockpredication, dual-path, multipath, and DMP on the less aggressive baseline processor and Figure12(right) shows the execution time reduction over the less aggressive baseline for limited software predication, wish branches, and DMP. Since the less aggressive processor incurs a smaller penalty for a branch misprediction, improved branch handling has less performance potential than in the baseline processor. However, DMP still provides 7.8% IPC improvement by reducing pipeline flushes by 30%, whereas dynamic-hammock-predication, dual-path and multipath improve IPC by 1.6%, 1.5%, and 1.3% respectively. Limited software predication reduces execution time by 1.0%, wish branches by 2.9%, and DMP by 5.7%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Performance comparison of DMP versus other paradigms on the less aggressive processor</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. DMP performance with different branch predictors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Effect of confidence estimator size on performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Power consumption comparison of DMP with the baseline processor (left) and less aggressive baseline processor (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . Hardware support required for different branch processing paradigms</head><label>2</label><figDesc>. (m+1) is the maximum number of outstanding paths in multipath.</figDesc><table><row><cell>Hardware</cell><cell></cell><cell>DMP</cell><cell>Dynamic-hammock</cell><cell>Dual-path/Multipath</cell><cell>Software predication</cell><cell>Wish branches</cell></row><row><cell>Fetch support</cell><cell></cell><cell cols="2">CFM registers, +1 PC fetch both paths round-robin fetch in simple hammock</cell><cell>+1/m PC round-robin fetch</cell><cell>-</cell><cell>selection between branch/predicated code</cell></row><row><cell cols="2">Hardware-generated predicate/path IDs</cell><cell>required</cell><cell>required</cell><cell>required (path IDs)</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Branch pred. support +1 GHR, +1 RAS</cell><cell>-</cell><cell cols="2">+1/m GHR, +1/m RAS -</cell><cell>-</cell></row><row><cell>BTB support</cell><cell></cell><cell cols="2">mark diverge br./CFM mark hammock br.</cell><cell>-</cell><cell>-</cell><cell>mark wish branches</cell></row><row><cell cols="3">Confidence estimator required</cell><cell cols="2">optional (performance) required</cell><cell>-</cell><cell>required</cell></row><row><cell cols="2">Decode support</cell><cell>CFM point info</cell><cell>-</cell><cell>-</cell><cell cols="2">predicated instructions predicated instructions</cell></row><row><cell cols="2">Rename support</cell><cell>+1 RAT</cell><cell>+1 RAT</cell><cell>+1/m RAT</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Predicate registers</cell><cell>required</cell><cell>required</cell><cell>-</cell><cell>required</cell><cell>required</cell></row><row><cell cols="3">Select-?op generation required</cell><cell>required</cell><cell>-</cell><cell cols="2">optional (performance) optional (performance)</cell></row><row><cell cols="2">LD-ST forwarding</cell><cell>check predicate</cell><cell>check predicate</cell><cell>check path IDs</cell><cell>check predicate</cell><cell>check predicate</cell></row><row><cell cols="2">Branch resolution</cell><cell>check flush/no flush predicate id broadcast</cell><cell>check flush/no flush</cell><cell>check flush/no flush</cell><cell>-</cell><cell>check flush/no flush</cell></row><row><cell>Retirement</cell><cell></cell><cell>check predicate</cell><cell>check predicate</cell><cell>selective flush</cell><cell>check predicate</cell><cell>check predicate</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Table 3. Baseline processor configuration</cell></row><row><cell>Front End</cell><cell cols="6">64KB, 2-way, 2-cycle I-cache; fetches up to 3 conditional branches but fetch ends at the first predicted-taken branch; 8 RAT ports</cell></row><row><cell cols="6">64KB (64-bit history, 1021-entry) perceptron branch predictor [20]; 4K-entry BTB Branch Predictors 64-entry return address stack; minimum branch misprediction penalty is 30 cycles</cell></row><row><cell>Execution Core</cell><cell cols="6">8-wide fetch/issue/execute/retire; 512-entry reorder buffer; 128-entry load-store queue; 512 physical registers scheduling window is partitioned into 8 sub-windows of 64 entries each; 4-cycle pipelined wake-up and selection logic</cell></row><row><cell>On-chip Caches</cell><cell cols="6">L1 D-cache: 64KB, 4-way, 2-cycle, 2 ld/st ports; L2 cache: 1MB, 8-way, 8 banks, 10-cycle, 1 port; LRU replacement and 64B line size</cell></row><row><cell cols="7">Buses and Memory 300-cycle minimum memory latency; 32 banks; 32B-wide core-to-memory bus at 4:1 frequency ratio; bus latency: 40-cycle round-trip</cell></row><row><cell>Prefetcher</cell><cell cols="5">Stream prefetcher with 32 streams and 16 cache line prefetch distance (lookahead) [43]</cell></row><row><cell>DMP Support</cell><cell cols="6">2KB (12-bit history, threshold 14) enhanced JRS confidence estimator [19, 17]; 32 predicate registers; 3 CFM registers (also see Table 2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 . Less aggressive baseline processor (base2) configuration Front</head><label>4</label><figDesc></figDesc><table /><note><p><p><p><p>End</p>Fetches up to 2 conditional branches but fetch ends at the first predicted-taken branch; 4 RAT ports 16KB (31-bit history, 511-entry) perceptron branch predictor</p><ref type="bibr" target="#b19">[20]</ref></p>; 1K-entry BTB Branch Predictors 32-entry return address stack; minimum branch misprediction penalty is 20 cycles Execution Core 4-wide fetch/issue/execute/retire; 128-entry reorder buffer; 64-entry scheduling window; 48-entry load-store queue 128 physical registers; 3-cycle pipelined wake-up and selection logic Buses and Memory 200-cycle minimum memory latency; bus latency: 20-cycle round-trip</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Characteristics of the benchmarks: baseline IPC, potential IPC improvement with perfect branch prediction (PBP IPC ?), total number of retired instructions (Insts), number of static diverge branches (Diverge Br.), number of all static branches (All br.), increase in code size with diverge branch and CFM information (Code size ?), base2 processor IPC (IPC base2), potential IPC improvement with perfect branch prediction on the base2 processor (PBP IPC ? base2). perl, comp, m88 are the abbreviations for perlbmk, compress, and m88ksim respectively.</figDesc><table><row><cell></cell><cell cols="2">gzip vpr</cell><cell>gcc</cell><cell cols="9">mcf crafty parser eon perl gap vortex bzip2 twolf comp</cell><cell>go</cell><cell>ijpeg</cell><cell>li</cell><cell>m88</cell></row><row><cell>Base IPC</cell><cell cols="5">2.02 1.50 1.25 0.45 2.54</cell><cell cols="4">1.50 3.26 2.27 2.88 3.37</cell><cell cols="6">1.48 2.18 2.18 0.97 2.73 2.15 3.27</cell></row><row><cell>PBP IPC ?</cell><cell cols="15">90% 229% 96% 113% 60% 137% 21% 15% 15% 16% 94% 112% 139% 227% 93% 60% 24%</cell></row><row><cell>Insts (M)</cell><cell>249</cell><cell>76</cell><cell>83</cell><cell>111</cell><cell>190</cell><cell>255</cell><cell cols="2">129 99 404</cell><cell>284</cell><cell>316</cell><cell>101</cell><cell>150</cell><cell>137</cell><cell cols="2">346 248 145</cell></row><row><cell>Diverge br.</cell><cell>84</cell><cell cols="2">434 1245</cell><cell>62</cell><cell>192</cell><cell>37</cell><cell>116 92</cell><cell>79</cell><cell>250</cell><cell>74</cell><cell>235</cell><cell>16</cell><cell>117</cell><cell>48</cell><cell>18 158</cell></row><row><cell>All br. (K)</cell><cell>1.6</cell><cell>4.2</cell><cell>29.5</cell><cell>1.4</cell><cell>5.1</cell><cell>3.7</cell><cell cols="2">4.9 9.4 4.6</cell><cell>13</cell><cell>1.4</cell><cell>4.7</cell><cell>0.6</cell><cell>7.7</cell><cell>2</cell><cell>1.2 1.7</cell></row><row><cell>Code size ?(%)</cell><cell cols="3">0.12 0.35 0.23</cell><cell>0.1</cell><cell>0.13</cell><cell cols="4">0.03 0.01 0.03 0.03 0.09</cell><cell cols="6">0.11 0.16 0.02 0.08 0.04 0.02 0.13</cell></row><row><cell>IPC base2</cell><cell cols="5">1.77 1.39 0.98 0.52 1.76</cell><cell cols="4">1.36 2.05 1.36 2.03 1.73</cell><cell cols="6">1.39 1.71 1.79 0.86 2.05 1.69 2.10</cell></row><row><cell cols="9">PBP IPC ? base2 39% 84% 46% 58% 27% 65% 9% 7% 9%</cell><cell>8%</cell><cell cols="6">46% 46% 50% 101% 37% 34% 12%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>2. Multiple-cfm increases the performance benefit of DMP for most benchmarks because it increases the probability of reaching a CFM point in dpred-mode and, hence, the likelihood of success of dynamic predication. Mcfm-counter supports multiple CFM points and also adopts the Counter Policy (Section 3.3). Counter Policy improves performance significantly in twolf, compress, and go; three benchmarks that have a high fraction of large frequently-hammock CFGs where the branch predictor sometimes deviates from the frequently executed paths. Mcfm-counter-yield also adopts the Yield Policy (Section 3.3) to exit dpred-mode early, increasing the performance benefit of DMP to 19.3%. Yield Policy is beneficial for vpr, mcf, twolf, compress, and go benchmarks. In these benchmarks, many diverge branches are control-flow dependent (i.e. nested) on other diverge branches, and control-flow dependent diverge branches are more likely to be mispredicted.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 . Power and energy comparison of different branch processing paradigms Baseline processor Less aggressive baseline processor</head><label>6</label><figDesc>DMP dyn-ham. dual-path multipath SW-pred wish br. DMP dyn-ham. dual-path multipath SW-pred wish br.</figDesc><table><row><cell>Max power ?</cell><cell>1.4%</cell><cell>1.1%</cell><cell>1.2%</cell><cell>6.5%</cell><cell>0.1%</cell><cell>0.4% 0.9%</cell><cell>0.8%</cell><cell>0.8%</cell><cell>4.3%</cell><cell>0.1%</cell><cell>0.4%</cell></row><row><cell>Energy ?</cell><cell cols="2">-9.0% -0.7%</cell><cell>-2.2%</cell><cell>4.7%</cell><cell cols="3">-1.5% -2.9% -5.6% -0.8%</cell><cell>1.1%</cell><cell>3.7%</cell><cell cols="2">-0.1% -1.5%</cell></row><row><cell cols="3">Energy ? Delay ? -22.3% -0.9%</cell><cell>-7.0%</cell><cell>-4.3%</cell><cell cols="3">-1.8% -6.1% -9.7% -0.5%</cell><cell>0.5%</cell><cell>2.2%</cell><cell>1.2%</cell><cell>-2.1%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>If the compiler does not predicate all basic blocks between A and H because one of the branches is easy-to-predict, then the remaining easy-to-predict branch is likely to become a hard-to-predict branch after if-conversion. This problem is called misprediction migration<ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b38">39]</ref>. Therefore, the compiler (e.g. ORC<ref type="bibr" target="#b30">[31]</ref>) usually predicates all control-flow dependent basic blocks inside a region (the region is A,B,C,D,E,F,G and H in this example.). This problem can be mitigated with reverse if-conversion<ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b3">4]</ref> or by incorporating predicate information into the branch history register<ref type="bibr" target="#b2">[3]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>When the predicted next fetch address is the CFM point of the diverge branch, the processor considers that it has reached the CFM point.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Select-?ops handle the merging of only register values. We explain how memory values are handled in Section 3.8.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>If the number of static instructions between a branch and the closest control-flow merge point exceeds a certain number (T), we consider that the CFG does not have a control-flow merge point. T=200 in our experiments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>The compiler could also provide a hint bit to indicate that it is better to enter dpred-mode regardless of the confidence estimation. This additional mechanism is called short-hammocks<ref type="bibr" target="#b20">[21]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>This comparison is actually performed incrementally every time a register is renamed in dpred-mode so that no extra cycles are wasted for select-?op generation. We simplify the explanation by describing it as if it happens at once at the end of dpred-mode.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>DMP has a limited number of predicate registers (32 in our model). Note that these registers are not architecturally visible.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>Note that only one predicate id needs to be broadcast per cycle because select-?ops from a later iteration cannot anyway be executed before the select-</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>Gcc, vortex, and perl in SPEC 95 are not included because later versions of these benchmarks are included in SPEC CPU 2000.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>Klauser et al. [23]  reported average 5% performance improvement for dynamic-hammock-predication, Farrens et al.<ref type="bibr" target="#b14">[15]</ref> reported average 7% per-</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Special thanks to <rs type="person">Chang Joo Lee</rs> for the support he provided in power modeling. We thank <rs type="person">Paul Racunas</rs>, <rs type="person">Veynu Narasiman</rs>, <rs type="person">Nhon Quach</rs>, <rs type="person">Derek Chiou</rs>, <rs type="person">Eric Sprangle</rs>, <rs type="person">Jared Stark</rs>, other members of the <rs type="institution">HPS research group</rs>, and the anonymous reviewers for their comments and suggestions. We gratefully acknowledge the support of the <rs type="funder">Cockrell Foundation</rs>, <rs type="funder">Intel Corporation</rs> and the <rs type="programName">Advanced Technology Program</rs> of the <rs type="institution">Texas Higher Education Coordinating Board</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JtEhDAv">
					<orgName type="program" subtype="full">Advanced Technology Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multipath execution: opportunities and limits</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS-12</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Conversion of control dependence to data dependence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Porterfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POPL-10</title>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Architectural support for compiler-synthesized dynamic branch prediction strategies: Rationale and initial results</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Connors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gyllenhaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-3</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A framework for balancing control flow and predication</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mahlke</surname></persName>
		</author>
		<idno>MICRO-30</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wattch: a framework for architectural-level power analysis and optimizations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-27</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using predicated execution to improve the performance of a dynamically scheduled machine with speculative execution</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-performance throughput computing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Caprioli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tremblay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="32" to="45" />
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Skipper: a microarchitecture for exploiting control-flow independence</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Cher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The impact of if-conversion and branch prediction on program execution on the Intel Itanium processor</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Ngai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>In MICRO-34</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microarchitecture optimizations for exploiting memory-level parallelism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fahs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-31</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reducing branch misprediction penalties via dynamic control independence detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS-13</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Control flow optimization via dynamic reconvergence prediction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>In MICRO-37</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kilo-instruction processors: Overcoming the memory wall</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galluzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pericas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="48" to="57" />
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficiently computing static single assignment form and the control dependence graph</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cytron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Wegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Zadeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="451" to="490" />
			<date type="published" when="1991-10">Oct. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Restricted dual path execution</title>
		<author>
			<persName><forename type="first">M</forename><surname>Farrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tyson</surname></persName>
		</author>
		<idno>CSE-97-18</idno>
		<imprint>
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
		<respStmt>
			<orgName>University of California at Davis</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reducing branch misprediction penalty via selective recovery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akkary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-10</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Confidence estimation for speculation control</title>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pleszkun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-25</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Selective dual path execution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-11">Nov. 1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Assigning confidence to conditional branch predictions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>In MICRO-29</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic branch prediction with perceptrons</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-7</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Diverge-merge processor (DMP): Dynamic predicated execution of complex control-flow graphs based on frequently executed paths</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Joao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<idno>TR-HPS-2006-008</idno>
		<imprint>
			<date type="published" when="2006-09">Sept. 2006</date>
		</imprint>
		<respStmt>
			<orgName>The University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Wish branches: Combining conditional branching and predication for adaptive predicated execution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<idno>MICRO-38</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic hammock predication for non-predicated instruction set architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Instruction fetch mechanisms for multipath execution processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-32</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selective eager execution on the polypath architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paithankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-25</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MinneSPEC: A new SPEC benchmark workload for simulation-based computer architecture research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinosowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2002-06">June 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Limits of control flow on parallelism</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-19</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Characterizing the impact of predicated execution on branch prediction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Hank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gyllenhaal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>In MICRO-27</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Effective compiler support for predicated execution using the hyperblock</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Hank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bringmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>In MICRO-25</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Runahead execution: An alternative to very large instruction windows for out-of-order processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-9</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Open research compiler for Itanium processor family</title>
		<author>
			<persName><surname>Orc</surname></persName>
		</author>
		<ptr target="http://ipf-orc.sourceforge.net/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On predicated execution</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlansker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<pubPlace>Hewlett-Packard Labs, Palo Alto CA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report HPL-91-58</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Guarded execution and dynamic branch prediction in dynamic ILP processors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Pnevmatikatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-21</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The inhibition of potential parallelism by conditional jumps</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1405" to="1411" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A study of control independence in superscalar processors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-5</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Control independence in trace processors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-32</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Analysis of the O-GEometric History Length branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-32</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Sias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ueng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
		<idno>ISCA- 31</idno>
		<title level="m">Field-testing IMPACT EPIC research results in Itanium 2</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Incorporating predicate information into branch predictors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-9</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Branch prediction, instruction-window size, and cache size: Performance tradeoffs and simulation techniques</title>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1260" to="1281" />
			<date type="published" when="1999-11">Nov. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Increasing processor performance by implementing deeper pipelines</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sprangle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carmean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-29</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Continual flow pipelines</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akkary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Upton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS-XI</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">POWER4 system microarchitecture. IBM Technical White Paper</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Tendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Dodson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sinharoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-10">Oct. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The effects of predication on branch prediction</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-27</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Register renaming and scheduling for dynamic execution of predicated code</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Kling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-7</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reverse ifconversion</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Warter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Rau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
