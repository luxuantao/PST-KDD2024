<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-28">28 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shaden</forename><surname>Smith</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brandon</forename><surname>Norick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">George</forename><surname>Zerveas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elton</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">†</forename><surname>Microsoft</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">‡</forename><surname>Nvidia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">equal contribution</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-28">28 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2201.11990v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and finetuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recently released foundation models <ref type="bibr" target="#b7">[8]</ref>, such as BERT <ref type="bibr" target="#b11">[12]</ref>, GPT-2 <ref type="bibr" target="#b50">[52]</ref>, and RoBERTa <ref type="bibr" target="#b35">[37]</ref>, represent a paradigm shift in which AI systems can be built by pretraining a general class of models at scale and then adapting them for a wide range of downstream tasks through transfer learning. Such models became ubiquitous in state-of-the-art natural language processing (NLP) systems by embracing the effectiveness of a combination of factors: the transformer architecture <ref type="bibr" target="#b65">[67]</ref>, self-supervised learning, few-shot conditioning <ref type="bibr" target="#b8">[9]</ref>, and fine-tuning.</p><p>Importantly, many recent works have established that scaling up models greatly improves their performance, with especially substantial performance improvements in the zero-shot and few-shot settings. For example, GPT-3 <ref type="bibr" target="#b8">[9]</ref>, an autoregressive language model with 175 billion parameters, performs competitively on language tasks using in-context learning without fine-tuning or gradient updates. Such in-context learning allows models to perform new language tasks with only simple instructions and a few optional examples. The effectiveness of this method was further enhanced by recent model adaptation work such as prompt tuning <ref type="bibr" target="#b31">[33]</ref>, which efficiently adapts large language models to individual tasks with robust task performance. Other intriguing capabilities exhibited by large language models include, but are not limited to, free-form generation of coherent, long-form text like news stories, generating responses with real-world knowledge, as well as performing rudimentary mathematical operations.</p><p>The rapid development of large language models in recent years has also been fueled by growth in computational resources, availability of large datasets and evolving software stacks. State-of-the-art supercomputing clusters address the computation, memory and networking need of model training at this scale. Careful processing of high-quality, high-volume and diverse datasets directly contributes to model performance in downstream tasks as well as model convergence. New approaches to numerical manipulation and training recipes were developed aiming at improved optimization efficiency and stability. However, to sustain the seemingly exponential growth of model parameter size (see Figure <ref type="figure" target="#fig_0">1</ref>), substantial progress in developing new methods, infrastructure and training capabilities is needed.</p><p>Training such large models is challenging for two reasons. First, it is no longer possible to fit the parameters of these models in the memory of even the largest GPU. Second, the large number of compute operations required can result in unrealistically long training times if special attention is not paid to concurrently optimizing the algorithms, software, and hardware stack. This calls for efficient parallelism techniques scalable on both memory and compute, in order to achieve the full potential of thousands of GPUs.</p><p>Compelled by the impressive qualitative performance improvements owing to an increasing model size that have been previously exhibited, our work continues the trend of large-scale language modeling. We built Megatron-Turing NLG 530B (MT-NLG), a transformer-based language model with 530 billion parameters. It is, to the best of our knowledge, the largest monolithic language model trained to date, with 3x more parameters than GPT-3. It is worth noting that sparse models structures encompassing a higher total number of parameters, such as mixture-of-experts <ref type="bibr" target="#b59">[61]</ref>, have been trained. However, it is unclear whether models built following this approach would have comparable parameter efficiency and generalization capability.</p><p>Training MT-NLG was made feasible by numerous innovations and breakthroughs along all AI axes. Through a collaboration between NVIDIA Megatron-LM <ref type="bibr" target="#b61">[63,</ref><ref type="bibr" target="#b41">43]</ref> and Microsoft DeepSpeed <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b63">65]</ref>, we created an efficient and scalable 3D parallel system capable of combining data, pipeline, and tensor-slicing based parallelism. By combining tensor-slicing and pipeline parallelism, we can operate within the regime where they are most effective. We built high-quality, natural language training corpora with hundreds of billions of tokens, and co-developed training recipes to improve optimization efficiency and stability.</p><p>In this paper, we will discuss details of our methods during the development of MT-NLG, including training infrastructure (Section 2), training dataset and training process (Section 3), model evaluation and other interesting observations (Section 4). We will also present an in-depth study on social biases (Section 5), in-context learning capability (Section 6) and qualitative analysis of the generation capability (Section 7) of MT-NLG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Large Model Training Infrastructure</head><p>Powered by NVIDIA A100 Tensor Core GPUs and HDR InfiniBand networking, state-of-art clusters (such as NVIDIA Selene and Microsoft Azure NDv4) have enough compute power to train models with trillions of parameters. However, achieving the full potential of these supercomputers requires memory-and computeefficient strategies for parallelizing across thousands of GPUs. In isolation, existing parallelism strategies such as data, pipeline, or tensor-slicing have trade-offs in memory and compute efficiency and cannot be used to train models at this scale. In this section, we discuss the system challenges of training large models. We describe our software design, hardware system, and the performance evaluation of a unified, powerful training infrastructure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Challenges</head><p>We begin by discussing the challenges of training large-scale language models: memory and compute efficiency, and the tradeoffs of various solution strategies such as data, tensor and pipeline parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Memory and Compute Efficiency</head><p>Memory Efficiency The memory requirements to train a 530 billion parameter model are far beyond what is available on a single GPU device. We refer to Rajbhandari et al. <ref type="bibr" target="#b54">[56]</ref> for an analytical study of memory consumption during training. <ref type="bibr" target="#b39">[41]</ref> typically stores weights and gradients in half precision formats (i.e., 2 bytes per parameter) for forward and backward propagations. It also keeps full-precision (4 bytes) copies in 32 bit float format for numerical stability in the optimizer. Assuming training with the Adam optimizer <ref type="bibr" target="#b25">[27]</ref>, training consumess 20 bytes of memory per parameter: Training a 530 billion parameter model thus requires over 10 terabytes of aggregate memory for the model weights, gradients, and optimizer states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed precision training</head><p>Activations can also consume significant memory and scale with the training batch size, sequence length, and model dimensions. Checkpointing and recomputing the activations of each transformer block is a common strategy for training large language models to reduce the memory required for activations. However, the activations at the boundary between layers still needs to be stored and the aggregate activation memory is: batch-size × number-of-layers × sequence-length × hidden-dimension × 2 bytes, which is approximately 16.9 terabytes following our model and training configuration (Section 3.2).</p><p>Fortunately, activation memory requirements can be mitigated by virtue of gradient accumulation. Gradient accumulation is a strategy in which the full training batch is split into micro-batches that are processed in sequence and their resulting gradients are accumulated before updating the model weights. After computing the gradient for a micro-batch, the associated activations can be freed. As a result, the training batch size can scale without increasing the peak resident activation memory. For example, training with 1920 microbatches instead of a single micro-batch of size 1920 reduces the peak activation memory from 16.9 terabytes to 8.8 gigabytes without changing the effective batch size.</p><p>Compute Efficiency While large GPU clusters can have thousands of high-throughput GPUs, achieving high compute efficiency at this scale is challenging. A large batch size can be an effective way of increasing compute efficiency, because it increases the arithmetic intensity of a kernel and helps amortize the time spent stalled on communication and synchronization. However, the batch size that a model can be trained with has an upper bound; using too large of a batch size can have negative effects on the model quality. With 4000 GPUs, even a large batch size of 4000 would only allow for a batch size of 1 per GPU and limit compute efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Tradeoffs of Data, Tensor, and Pipeline Parallelism</head><p>Data Parallelism Data parallelism is a ubiquitous technique in deep learning in which each input batch of training data is divided among the data-parallel workers. Gradients are communicated and aggregated among data-parallel workers before updating the model weights. Data parallelism has several distinct advantages, including compute efficiency and ease of implementation. However, data parallelism relies on scaling the batch size with the number of data-parallel workers, and cannot be made arbitrarily large without affecting model quality.</p><p>Memory Efficiency: Data parallelism replicates the model and optimizer across all workers, and therefore is not memory efficient. The Zero Redundancy Optimizer (ZeRO) <ref type="bibr" target="#b53">[55]</ref> is a collection of optimizations that improve the memory efficiency of data parallelism by partitioning the replicated data among data-parallel workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compute Efficiency:</head><p>The amount of computation performed by each worker is constant as we increase the degree of parallelism and training batch size. Data parallelism can achieve near-perfect scaling at small scales. However, the communication cost of aggregating gradients increases with the model size and can limit compute efficiency on large models or systems with low communication bandwidth. Gradient accumulation is also a common strategy for amortizing this communication cost by further increasing the batch size and performing multiple forward and backward propagations on micro-batches while locally accumulating gradients before aggregating and taking an optimizer step. Additionally, performance can be increased by simultaneously communicating gradients that have already been communicated in parallel with computing the gradients for other tensors.</p><p>Tensor Model Parallelism Tensor model parallelism (or, tensor parallelism) is a broad class of model parallelism techniques that partitions the individual layers of the model across workers. Tensor parallelism reduces the memory proportional to the number of workers. Megatron <ref type="bibr" target="#b61">[63]</ref> uses model parallelism to efficiently partition transformer blocks for large-scale language models. Memory Efficiency: Tensor parallelism reduces the memory footprint of the model proportional to the number of workers. Depending on the model architecture, some of the activation memory is also reduced, although there may still be some replications.</p><p>Compute Efficiency: Tensor parallelism introduces additional communication of activations in each forward and backward propagation. Therefore, tensor parallelism requires high communication bandwidth to be efficient and is best kept within a single DGX sever where high bandwidth NVLink is available. Furthermore, each model-parallel worker decreases the amount of computation performed between each communication stage, impacting compute efficiency. Tensor parallelism is often used to expand the envelope of memory and compute efficiency beyond what data parallelism alone can do.</p><p>Pipeline Model Parallelism Pipeline model parallelism (or, pipeline parallelism) divides the layers of the model into stages that can be processed in parallel <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b40">42]</ref>. As one stage completes the forward pass for a micro-batch, the activation memory is communicated to the next stage in the pipeline. Similarly, as the next stage completes its backward propagation, gradients are communicated backwards through the pipeline. Multiple micro-batches must be kept in flight to ensure pipeline stages compute in parallel.</p><p>Memory Efficiency: Pipeline parallelism reduces memory proportionally to the number of pipeline stages, allowing model size to scale linearly with the number of workers. However, pipeline parallelism does not reduce the memory footprint for the activations of each layer. Additionally, each worker must store the activations for all micro-batches in flight. We use a 1F1B pipeline schedule <ref type="bibr" target="#b40">[42]</ref> that alternates forward and backward propagations. A key benefit of 1F1B is that the number of micro-batches in flight is bounded by the number of pipeline stages, as opposed to the total number of micro-batches in a full training batch.</p><p>Compute Efficiency: Pipeline parallelism has the smallest communication overhead of the three approaches, as it only communicates the activations between the pipeline stage boundaries. However, it cannot scale indefinitely. The degree of pipeline parallelism is bounded by the depth of the model, and increasing the pipeline dimension decreases the compute efficiency like other forms of model parallelism. Pipeline parallelism also requires each of its stages to be load balanced for high efficiency.</p><p>Pipeline parallelism incurs a bubble overhead from filling and emptying the pipeline at the beginning and end of each training batch. The size of the bubble overhead bounds the potential speedup from pipeline parallelism. The fraction of perfect speedup achievable (or, parallel efficiency) is a function of the number of pipeline stages (P P ) and total micro-batches (M B):</p><formula xml:id="formula_0">efficiency = M B M B + P P − 1 .</formula><p>If the number of micro-batches is 4x or 8x the number of pipeline stages, the pipeline achieves 81% and 90% parallel efficiency from one pipeline stage, respectively.</p><p>From the above discussion, it is clear that none of the existing parallelism techniques can address all the system challenges of training models with hundreds of billions of parameters. However, each parallelism technique has its own merits and can be used in a complementary fashion. To this end, we use 3D parallelism, which is a systematic combination of data, tensor, and pipeline parallelism that addresses both compute and memory efficiency simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Software System -3D Parallelism with DeepSpeed and Megatron</head><p>Our system software stack combines pipeline parallelism and data parallelism from DeepSpeed with tensorslicing from Megatron to create a flexible 3D-parallelism implementation. Data, tensor, and pipeline parallelism each play a specific role in improving memory and compute efficiency.</p><p>Memory Efficiency: Transformer blocks are divided into pipeline stages, and the blocks of each stage are further divided via tensor parallelism. This 2D combination simultaneously reduces the memory consumed by the weights, gradients, optimizer states, and activations. However, we cannot partition the model indefinitely without losing compute efficiency.</p><p>Compute Efficiency: To further accelerate training, we use data parallelism to scale to arbitrarily large number of GPUs. For example, each 530 billion parameter model replica spans 280 NVIDIA A100 GPUs, with 8-way tensor-slicing within a node and 35-way pipeline parallelism across nodes. We then use data parallelism to scale out further to thousands of GPUs.</p><p>Our 3D parallelism implementation is optimized using topology aware mapping, which minimizes communication overhead across all forms of parallelism, and has an especially large impact on data parallelism. This mapping is key to achieving excellent compute efficiency at scale. We discuss the details below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Topology-Aware 3D Mapping</head><p>Each axis of parallelism is carefully mapped onto the workers to maximize compute efficiency by exploiting two key architectural properties.</p><p>Mapping for Bandwidth Intra-node communication has a higher bandwidth than inter-node. We prioritize co-locating parallel groups with larger communication volumes in order to utilize the higher bandwidth.</p><p>Tensor parallelism has the largest communication overhead of the three strategies, and so we prioritize placing tensor parallel workers within a node. When possible, data parallel workers are also placed within a node to accelerate gradient communications. Otherwise, data parallel workers are mapped to nearby nodes when possible. Pipeline parallelism has the lowest communication volume, and so we can schedule pipeline stages across nodes without being limited by the communication bandwidth.</p><p>Bandwidth Amplification The volume of gradient communication by each data parallel group decreases linearly as pipeline and tensor parallelism increase. Thus, the total communication volume is decreased from pure data parallelism. Furthermore, each data parallel group performs its communication independently and in parallel among a subset of more localized workers. As a result, the effective bandwidth for data parallel communication is amplified by a combination of reduced communication volume and increased locality and parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hardware System</head><p>Model training is done with mixed precision using 16-bit bfloat on NVIDIA's Selene <ref type="bibr" target="#b1">[2]</ref> supercomputer with 560 DGX A100 nodes. Each cluster node has 8 NVIDIA 80-GB A100 GPUs <ref type="bibr" target="#b0">[1]</ref>, connected to each other by NVLink and NVSwitch <ref type="bibr" target="#b2">[3]</ref>. Each node has eight NVIDIA Mellanox 200Gbps HDR Infiniband HCAs for application communication, with an additional two HCAs per node for dedicated storage. The nodes are connected in a three-level (leaf, spine, core) fat-tree topology with 850 switches. This topology allows efficient all-reduce communication (which is the dominant communication pattern in deep learning training). The cluster uses an all-NVME shared parallel filesystem for high-performance data access and storage. The peak device throughput of an A100 GPU with 16-bit precision is 312 teraFLOP/s, resulting in an aggregate of 1.4 exaFLOP/s of peak 16-bit precision performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">System Performance Evaluation</head><p>We considered the end-to-end throughput of our system for the 530 billion parameter model with batch size 1920 on 280, 350, and 420 DGX A100 servers on Selene. We observed iteration times of 60.1, 50.2, and 44.4 seconds, respectively. These correspond to 126, 121, and 113 teraFLOP/s per GPU, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Training Dataset and Model Configuration</head><p>In this section we present details on the training datasets, our preprocessing techniques, and the model and hyperparameters used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Dataset and Preprocessing</head><p>Resources such as Common Crawl (CC) provide snapshots of the web which can be utilized as a source of language data. While these data sources contain an enormous amount of language data, they also require carefully designed preprocessing steps in order to select data which is of reasonable quality. As prior work has found (e.g., <ref type="bibr" target="#b8">[9]</ref>), the quality of unfiltered Common Crawl data is lower than that of curated datasets and steps should be taken to increase the average quality of data selected from Common Crawl for LM pretraining. In addition to CC data, there are many other high quality data sources on the web. To compile our training dataset, we made use of recent work aimed at collecting a diverse training set for language modeling <ref type="bibr" target="#b15">[17]</ref>. We additionally included RealNews <ref type="bibr" target="#b75">[77]</ref> and CC-Stories <ref type="bibr" target="#b64">[66]</ref> which have previously been used for large LM pretraining <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b61">63]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Training Dataset</head><p>We largely built upon prior work described in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">17]</ref> to generate our training set. First, we selected a subset of the datasets from The Pile that we observed to be of the highest relative quality (see Table <ref type="table" target="#tab_0">1</ref>). Then, following a similar approach as that used to generate Pile-CC in <ref type="bibr" target="#b15">[17]</ref>, we downloaded and filtered two full CC snapshots (2020-50 and 2021-04). At a high level, the steps taken for CC data include text extraction from raw HTML provided in WARC files, scoring extracted documents using a classifier trained on high quality data, and filtering documents according to their scores. These steps are covered in more detail in Section 3.1.2. Finally, we used fuzzy deduplication to remove duplicate and near duplicate documents from the entire dataset as well as n-gram based filtering to remove downstream task data in order to avoid contamination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Pre-Processing Details</head><p>Common Crawl: As mentioned previously, Common Crawl comprises an immense amount of data. We chose to process two snapshots, 2020-50 and 2021-04, with the aim of acquiring around 150B tokens of training data. The first step of this process is language detection <ref type="bibr" target="#b10">[11]</ref> and text extraction from the raw HTML included in the Common Crawl WARC files <ref type="foot" target="#foot_0">1</ref> . Following the rationale presented in <ref type="bibr" target="#b10">[11]</ref>, we used the pycld2<ref type="foot" target="#foot_1">2</ref> and jusText<ref type="foot" target="#foot_2">3</ref> libraries for these tasks. We observe that the language detection and extraction step reduces the number of documents significantly, with only around 25% of documents being classified as English and having non-empty body content.</p><p>In order to select high quality documents from these extractions, we trained a 2-gram fastText <ref type="bibr" target="#b46">[48]</ref> classifier.</p><p>For positive documents, we randomly select 500000, 295000, and 5000 documents from OpenWebText2, Wikipedia, and Books3, respectively, similar to <ref type="bibr" target="#b8">[9]</ref>. For negative documents, we randomly sampled an equal number of documents from the text extraction output described above. We held out 10% of these documents for evaluation of the classifier, which achieved an accuracy of 90.3% on the held out set after training. The classifier was applied to each of the extracted documents and the probability of the positive label was taken as the score for the document.</p><p>Using the scores produced by the process above, we filtered the extracted documents with a Pareto distribution with α = 3. This resulted in around 80% of text content being filtered. While our choice of α is lower than some previous works <ref type="bibr" target="#b8">[9]</ref>, manual inspection of the data indicated that it was of acceptable quality and the use of α = 3 allowed us to reach and slightly exceed our original token goal after deduplication.</p><p>Other Datasets: In addition to Common Crawl data, we leveraged a number of other previously generated datasets. From The Pile, we selected Books3, OpenWebText2, Stack Exchange, PubMed Abstracts, Wikipedia, Gutenberg (PG-19), BookCorpus2, NIH ExPorter, and Pile-CC datasets. We also included the CC-Stories and RealNews datasets used to train Megatron <ref type="bibr" target="#b61">[63]</ref>. For detailed discussions of the preprocessing used for these datasets, we refer to <ref type="bibr" target="#b15">[17]</ref>.</p><p>Fuzzy Document Deduplication: Content on the internet is often duplicated across many documents. To compound this issue, the URLs scraped in different Common Crawl snapshots are not necessarily unique. Indeed, for the snapshots we chose 53% and 34% of documents come from new URLs not seen in previous snapshots. Furthermore, it is likely that content contained in our other datasets, such as web content from OpenWebText2 or Wikipedia, will also exist in Commom Crawl.</p><p>Exact match duplicates would be computationally expensive, so we opted to take a fuzzy deduplication approach similar to other works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">17]</ref>. We used a hashing vectorizer with 1,048,576 features to vectorize documents (HashingVectorizer from scikit-learn<ref type="foot" target="#foot_3">4</ref> ), calculated min-hashes of the vectorized documents (using datasketch<ref type="foot" target="#foot_4">5</ref> ), and performed Locality Sensitive Hashing (LSH) through datasketch on all minhashes in order to identify potential duplicates. We set our LSH parameters in such a way as to increase the likelihood that documents with Jaccard similarity ≥ 0.8 would occur in at least one LSH bucket together. Specifically, we used 20 bands of size 13 for a total of 260 hash functions.</p><p>After performing LSH, we processed each bucket and calculated an approximation of the all-pairs Jaccard similarity in order to remove false positive duplicates introduced by LSH. This approximation consisted of i = 0..10 iterations of sampling a random document d i , calculating the Jaccard similarity with everything remaining in the bucket, removing those documents above the 0.8 threshold and marking them as duplicates of d i . After all buckets were processed and duplicates (at the threshold) were approximately discovered, we constructed a sparse document graph and found the connected components therein (using scipy). Each connected component represents a set of documents that we consider similar enough to be duplicates, and from which we select a single representative. Because the datasets are of varying quality, we defined a priority order based on which dataset to use when selecting representative documents, and the first document encountered from the highest priority dataset within each component was ultimately kept, while the rest were discarded.</p><p>Additional Processing: We use the Ftfy library <ref type="bibr" target="#b62">[64]</ref> on the training dataset to convert bad unicode text to good unicode text. Additionally, we use the langdetect <ref type="bibr" target="#b10">[11]</ref> library to identify non-English documents and remove any document such with less than 512 characters. If a training document contains the word "javascript" and has less than 256 characters, we remove that document as well.</p><p>Downstream Task Data Removal: We use n-grams to remove texts that occur in the downstream tasks from the training datasets. When we find an n-gram match between a task document and a training document, we split the training document into two pieces by removing the n-gram along with 200 characters from both of its sides. We also remove any split training document with fewer than 200 characters, or training documents which were split more than 10 times. Our deduplication process and the values of n used for different tasks are similar to <ref type="bibr" target="#b8">[9]</ref>. Out of 319,781,622 documents from the 15 deduplicated datasets mentioned above, during task deduplication 35,988 documents were split, 1,109 documents were removed, 54 documents were split more than 10 times, and 9,891 were trimmed at the beginning or the end.</p><p>Blending Datasets: We opted to blend the datasets into heterogeneous batches according to the sampling weights given in Table <ref type="table" target="#tab_0">1</ref>. However, the mixing weights do not result in an even split of the samples in each batch for our chosen batch size. To resolve this issue, we track the under-and oversampling for each dataset and slightly adjust the batch composition at each step in order to maintain a sample distribution as close as possible to the chosen mixing weight distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model and Training Process</head><p>We used the architecture of the transformer decoder <ref type="bibr" target="#b50">[52]</ref>, which is a left-to-right, autoregressive, generative transformer-based language model, and scaled it up to 530 billion parameters. The number of layers, hidden dimensions, attention heads are 105, 20480, and 128, respectively. The sequence length is 2048 and the global batch size is 1920. We used 8-way tensor and 35-way pipeline parallelism. The learning rate is 5.0e −5 . We used one billion tokens for linear learning rate warmup. We used cosine decay for the learning rate targeting to reach 10% of its value over 340 billion tokens. Over the first 12 billion tokens, we started at a batch size of 32 and gradually increased the batch size in increments of 32, until we reach the final batch size of 1920. We used Adam optimizer with β 1 = 0.9, β 2 = 0.95, and = 10 −8 . We clipped the gradient norm at 1.0 and used a weight decay of 0.1. For weight initialization, we used a normal distribution with zero mean and a standard deviation of 4.0e −3 . Our training dataset consists of 339 billion tokens and we trained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.</p><p>At the scale of models such as MT-NLG, training stability is a fundamental challenge. While training the model, we observed that the learning rate, weight initialization, and Adam optimizer parameters directly affect model stability. We projected the learning rate for MT-NLG by plotting the learning rates with the size of the models in <ref type="bibr" target="#b8">[9]</ref>. Higher learning rate increases the model instability. We used approximately 1/(3 * H) as a standard deviation for weight initialization, where H denotes the size of the hidden dimension. Similar to <ref type="bibr" target="#b43">[45]</ref>, we also observed that using higher variance for weight initialization fails to converge. We also reduced β 2 from its standard value of 0.99 to reduce spikes in the training loss. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Achievements</head><p>To provide a better understanding of how language model performance improves during training, we first present the validation loss curve (cross entropy) of MT-NLG in Figure <ref type="figure" target="#fig_2">2</ref>. Our validation dataset consists of 5.5 billion tokens, so measuring the loss using the entire dataset is computationally expensive. We therefore shuffle the sequences in the validation dataset and then during each computation of validation loss, we run four iterations with global batch size of 1920. This leads to evaluating on a total of 16 million consecutive tokens for each loss computation.</p><p>The validation cross-entropy loss is 3.15 after the model is trained on the first 1 billion tokens. As mentioned earlier, we increase the batch size linearly over the first 12 billion tokens. At the end of this phase, the loss becomes 2.31. When the model reaches our targeted number of tokens, 270 billion, the validation loss becomes 1.85.</p><p>To evaluate the quality of our model (as well as other pretrained language models), we adopt a zero-/one-/few-shot evaluation setting similar to prior work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b51">53]</ref>. For better reproducibility, we base our evaluation on the open-source project, lm-evaluation-harness <ref type="bibr" target="#b16">[18]</ref>, and made task-specific changes as appropriate to align our setting more closely with prior work. We will discuss any idiosyncrasies of each task in the task-specific paragraphs. In addition, for our few-shot experiments, we do not do any search for the optimal number of shots, and directly use the configurations suggested in <ref type="bibr" target="#b8">[9]</ref>. In most cases, they seem to perform sufficiently well.</p><p>To ensure the evaluation is comprehensive, we choose eight tasks from five different categories: completion prediction, reading comprehension, commonsense reasoning, natural language inference and word sense disambiguation. We present comparisons on these tasks with previous works on pretrained large language models, while also providing supervised baselines whenever applicable to provide context for the gap between "generalist" models like pretrained language models and "specialist" models that are finetuned on the target task. Table <ref type="table">2</ref>: LAMBADA zero-shot, one-shot and few-shot accuracy. MT-NLG outperforms previous models across different settings and establishes new SOTA for all 3 settings. We did not find any recent strong supervised baseline for LAMBADA, hence we omit the comparison with supervised models here.</p><p>Many evaluation tasks involve scoring candidate completion sentences with the model. Unless otherwise stated, the "likelihood" mentioned in the following context refers to the probability of the candidate answer (conditioned on the prompt) normalized by its number of tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Completion Prediction</head><p>LAMBADA The LAMBADA <ref type="bibr" target="#b47">[49]</ref> dataset is a collection of narrative passages, specifically selected such that a human can easily guess the last word if the whole passage is given as context, but would not be able to answer if only given the last sentence in the passage. This task tests language models' capabilities to understand and retain information from a broader discourse context, instead of just relying on local context or simple statistical patterns.</p><p>When evaluating this task zero-shot, we feed each passage to the model as input and check if the model can produce the correct last word via greedy generation (picking tokens with maximum probability). However, for one-/few-shot evaluations, we switched over to a cloze-style prompt format to better suggest to the model that the task is about predicting the last word of a sentence as opposed to arbitrary plausible continuation. In such a case, we would insert " . → " before the last word, e.g. "... Paul and Debbie looked at each other, then at . → Bob" and examine if the model would predict the correct word after the "→". We observe significant performance boost in few-shot settings with the cloze-style prompting, although one-shot performance takes a hit, which aligns with observations from prior work <ref type="bibr" target="#b8">[9]</ref>. Our model's performance in terms of accuracy is shown in table 2, and we are establishing new state-of-the-arts on LAMBADA for all 3 settings on its test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reading Comprehension</head><p>In this section, we discuss the evaluation of MT-NLG for reading comprehension. We selected two datasets targeting different styles of questions, and have found very different trends when we increase the number of examples for them during evaluation.</p><p>RACE RACE <ref type="bibr" target="#b29">[31]</ref> is a large-scale reading comprehension dataset, whose passages and questions are extracted from English examinations. Each example in this task consists of an article and several questionanswer pairs. To construct prompts, we prepend "Article: ", "Question: ", and "Answer: " tags to the article, questions and answers text respectively and join them together with a newline in between. The actual answer to the last question is removed, ending the prompt at the last "Answer:". We then use the model to score all possible candidate answers as continuations after "Answer:" and pick the highest-scoring one as the model's choice.</p><p>There are two question types in this dataset: direct questions (e.g. "Which of the following relationships is healthy?") and cloze-style questions (e.g. "The author of the text seems to ."). We treat both question types the same way as described above, which is different from the default used by lm-evaluation-harness <ref type="bibr" target="#b16">[18]</ref>. Furthermore, following GPT-3 <ref type="bibr" target="#b8">[9]</ref>, we use</p><formula xml:id="formula_1">P (completion|context) P (completion|answer context)</formula><p>as the scoring criterion, where context is the full prompt, and answer context is just the string "Answer:". Similar to GPT-3, we observe a better performance compared to using length-normalized log-probabilities as a scoring criterion for RACE.</p><p>The dataset contains two subsets, RACE-h and RACE-m, corresponding to hard and medium problems. We report results on the RACE-h set in Table <ref type="table" target="#tab_3">3</ref>. We observe that RACE-h performance does not benefit much from including more examples in the prompt. Nevertheless, our zero-shot performance already surpasses few-shot performance of GPT-3 by +1.14%.</p><p>For RACE dataset, one of the best supervised models to date is an ALBERT ensemble <ref type="bibr" target="#b22">[24]</ref>. It achieves 91.4% accuracy on RACE-h, which is significantly higher than the results obtained by pretrained language models. Recent work <ref type="bibr" target="#b51">[53]</ref> has greatly narrowed the gap between prerained language models and supervised models, but the difference is still large.</p><p>BoolQ BoolQ <ref type="bibr" target="#b9">[10]</ref> is a dataset of yes/no questions, with supporting Wikipedia paragraphs to answer them. We concatenate the supporting paragraph, the question (prepended with "Question: ") and a string "Answer:" at the end as the full prompt. We use the model to score "yes" and "no" as continuations and choose the option with higher likelihood given by the model. Our model's performance is shown in Table <ref type="table" target="#tab_3">3</ref>. We observe that BoolQ evaluation benefits significantly from seeing many examples in the prompt, which differs from results on the RACE task. However, one common pattern here is that reading comprehension tasks can get a decent improvement with just one example, possibly because the task prompting format is confusing to the model, and the given example is enough to condition the model to follow the passage-question-answer format.</p><p>For BoolQ, T5 + UDG <ref type="bibr" target="#b67">[69]</ref> is currently the best supervised model. It achieves 91.4% accuracy on this task. However, compared to RACE-h, we observe that the gap between supervised model and pretrained language model is much smaller and that MT-NLG further narrows the gap by a significant amount.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Commonsense Reasoning</head><p>An interesting aspect of pre-trained language models is how much world knowledge they preserve from their training data. To this end, we evaluate our models on two tasks relating to commonsense reasoning/inference. The supervised baseline we compare to on these 3 datasets is UNICORN <ref type="bibr" target="#b36">[38]</ref>.  Winogrande Winogrande <ref type="bibr" target="#b56">[58]</ref> is a dataset that seeks to expand the Winograd Schema Challenge in both scale and difficulty. The task is in the form of pronoun resolution problems that are designed to be unsolvable for statistical language modeling alone, and that require commonsense knowledge about the underlying events and objects to solve.</p><p>For this task, we adopt the evaluation method used by previous work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b64">66]</ref>. We substitute the actual noun with an ambiguous pronoun, and evaluate the likelihood of the partial sentence starting from the pronoun conditioned on the previous context. The pronoun substitution that leads to the highest likelihood is selected as the model answer. The results are shown in Table <ref type="table" target="#tab_5">4</ref>. Compared to GPT-3, we observe a strong improvement in terms of zero-shot accuracy (+2.81%), though the gap narrows for few-shot. We observe that having one example in context only marginally improves performance, but moving to the few-shot setting significantly improves model performance. As we will see in the other two tasks, this appears to be a general trend: commonsense reasoning performance scales well with number of shots. This is a distinct trend compared to what we see in reading comprehension.</p><p>HellaSWAG HellaSWAG <ref type="bibr" target="#b74">[76]</ref> is a commonsense reasoning dataset where a goal is given and the model is tasked with choosing the most likely follow-up actions. The examples are mined from Wikihow and Activitynet Captions <ref type="bibr" target="#b27">[29]</ref> dataset. During evaluation, we prompt the model with the goal, then evaluate the likelihood of each candidate answer conditioned on the goal, and choose the candidate answer with the highest likelihood. The results are shown in Table <ref type="table" target="#tab_5">4</ref>. We achieved significant improvements compared to GPT-3 in all 3 settings, with our zero-shot performance surpassing few-shot for GPT-3. Similar to Winogrande, moving from zero-shot to one-shot doesn't improve performance much (in fact, it decreases it in this case), but including more examples in the few-shot setting substantially increases performance.</p><p>PiQA PiQA <ref type="bibr" target="#b5">[6]</ref> is a binary-choice question answering dataset targeting understanding of physical interactions. It poses questions about how to complete a daily activity, and the model is tasked with choosing between two candidate answers describing different actions to take.</p><p>For evaluation on PiQA, we prompt the model with the question/goal description and then evaluate the  We generally observe minor gain or even performance dips when moving from zero-shot to one-shot, but would observe significant gains when we move from zero-shot to few-shot settings. On common sense reasoning, supervised baseline <ref type="bibr" target="#b36">[38]</ref> still outperforms LMs with few-shot learning settings.</p><p>likelihood of the candidate sentences for two different actions, choosing the option with higher likelihood as the model answer. The results are shown in Table <ref type="table" target="#tab_5">4</ref>. We once again observe the pattern that one-shot performance degrades compared to zero-shot, while few-shot performance gets a decent boost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Natural Language Inference</head><p>In this section we discuss the evaluation of our model on natural language inference (NLI) tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ANLI</head><p>The ANLI <ref type="bibr" target="#b44">[46]</ref> dataset is an adversarially mined NLI dataset that aims to create a difficult set of NLI problems. The dataset has 3 iterative rounds of data collection; here, we evaluate with round 2 data. During evaluation, we rephrase the NLI problem into a question-answering format: each example is structured as "&lt;premise&gt;\nQuestion:&lt;hypothesis&gt;. True, False or Neither?\nAnswer:" and then we examine which continuation among True, False or Neither has the highest likelihood assigned by the model, and pick the most likely option as the model answer. The results are shown in Table <ref type="table" target="#tab_6">5</ref>. On ANLI, we observe that, similar to reading comprehension results, our model is able to get a performance gain by just having one example, and moving beyond that into few-shot setting does not further improve performance. Again, this is possibly because one example is important for instructing the model on the premise-hypothesis-answer format, but additional examples may be unrelated in terms of content, and including them does not introduce new knowledge for the model. On ANLI, the supervised baseline we compare to is InfoBERT <ref type="bibr" target="#b66">[68]</ref>.</p><p>HANS Heuristic Analysis for NLI Systems (HANS) <ref type="bibr" target="#b38">[40]</ref> is an NLI dataset designed to evaluate the tendency of models to exploit fallible, superficial syntactic heuristics in NLP data. It offers a controlled evaluation setting where examples are generated from templates of specific grammatical and syntactical structures (each type of structure referred to as a "subcase"). The task format is akin to ANLI, with the NLI problem  <ref type="figure" target="#fig_5">5</ref>.</p><p>converted into a binary question answering format (see Section A in Appendix for details). We implemented this task and included it in our evaluation among existing tasks in the lm-evaluation-harness <ref type="bibr" target="#b16">[18]</ref>.</p><p>Besides evaluating our model's core language understanding capabilities, we use the HANS dataset primarily as a means to analyze its behavior in few-shot learning, which is presented in Section 6. We report our aggregate results obtained during the analysis experiments in Table <ref type="table" target="#tab_6">5</ref>, and a comparison of various MT-NLG checkpoints across different number of shots in Figure <ref type="figure" target="#fig_5">5</ref>. No prompt-based generative baselines have been previously released on this dataset, so we evaluate GPT-2 for comparison. As described in Section 6, performance at zero-shot is driven by inherent model biases and accuracy is only slightly better than random chance (50%). However, large models which have been sufficiently trained can take advantage of in-context examples in the prompt to dramatically improve performance, while weaker models can be confused when given additional in-context examples, with GPT-2 never performing substantially better than random chance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Word Sense Disambiguation</head><p>WiC The Word-in-Context <ref type="bibr" target="#b48">[50]</ref> dataset presents a task of identifying the intended meaning of polysemous words from their context. Each dataset example consists of 2 sentences, both containing the same polysemous word. The task is to identify if the intended meaning of the polysemous word is the same or not in the two sentences.</p><p>To perform zero-/few-shot evaluations on this task, we convert the problem into a question answering format: "Sentence 1:&lt;sentence1&gt;\nSentence 2:&lt;sentence2&gt;\nQuestion: Is the word &lt;target word&gt; used in the same way in the two sentences above?\nAnswer:". Then we examine the model-assigned likelihoods for "yes" and "no" as continuations, and pick the one with higher likelihood as the model answer. Results can be found in Table <ref type="table" target="#tab_7">6</ref>. We observe that our model performs slightly below chance at zero-shot, but surpasses chance as soon as we move to few-shot. On the other hand, the supervised T5 + UDG model surpasses chance-level significantly. 5 Exploring Social Biases</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Introducing the Challenge of Social Bias in NLP Models</head><p>Natural language models are trained on massive datasets collected from a wide variety of uncurated sources. Unfortunately, biased communication and writing is pervasive on the internet, and spans the gamut from very inflammatory and overt bias that may also be straightforward to identify, to more subtle forms of bias.</p><p>As has been shown in other work (for example <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b69">71,</ref><ref type="bibr" target="#b51">53]</ref>), bias issues that exist in the dataset can be learned by models as they are trained on the data. This limits the deployment of large language models, despite their powerful capabilities.</p><p>Although not the focus of this paper, we note that ongoing research in several areas aims to mitigate this bias. For example, In this work, we have trained a baseline model without any anti-bias countermeasures. We want to emphasize that we do not believe that such models should be deployed in production use without countermeasures, and specifically, we do not believe that the MT-NLG model should be deployed as such. Rather, it is our expectation that this work will be a valuable resource supporting research efforts into anti-bias countermeasures and as a starting point for future deployments that make use of such countermeasures. We are also confident that future work combining state-of-the-art large language model training with state of the art antibias countermeasures will result in models that are both very powerful and able to minimize the presence of bias in results.</p><p>We now present a preliminary analysis of the bias of our model, focusing on issues related to gender, ethnicity, and religion. Similar to Brown et al. <ref type="bibr" target="#b8">[9]</ref> and Rae et al. <ref type="bibr" target="#b51">[53]</ref>, we present association tests, cooccurrence analyses and sentiment analyses designed to assess bias along a variety of dimensions. This section serves to document and quantify some of the problems relating to bias of the model that will need to be addressed in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Gender and Occupation Analysis</head><p>We begin by investigating the association between gender and occupations, to understand if the model learns the societal biases present in its training data. <ref type="foot" target="#foot_7">8</ref> Following previous work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">39]</ref>, we use the list of 323 occupations shown in Tables <ref type="table" target="#tab_13">18 and 19</ref> of Section B. The template used in this experiment is "The {occupation} was a {gender identifier}", and the gender identifiers used are male, man, and female, woman.</p><p>Given a fixed occupation, we calculate the probability the model assigns to different gender identifiers and count the number of times male identifiers had a higher probability than female identifiers. Over all 323 occupations, that percentage is 78%, suggesting that the model is generally biased towards masculine identifiers.</p><p>We calculate average occupation bias scores, which measures whether the model tends to associate a specific gender with a given occupation. Following Brown et al. <ref type="bibr" target="#b8">[9]</ref>, occupation bias scores are calculated as 1 N occ occ (log(P(female identifier|prompt)) -log(P(male identifier|prompt)))</p><p>Here, a score of 0 indicates that there is no biased association between a specific occupation and gender identifiers; a positive score indicates a skewed association between female identifiers and occupations; and a negative score indicates a skewed association between male identifiers and occupations. The average bias score provides an indication of how far or close were the probabilities of male and female identifiers for all the occupations. Our model exhibits an average bias score of −0.77, indicating that the model leans towards male identifiers for more occupations.</p><p>To give some context for these two results, we note that for GPT-3, the percentage of occupations for which male identifiers had a higher probability over female identifiers is 83% and the average bias score is −1.11 <ref type="bibr" target="#b8">[9]</ref>. We should note that these results are not directly comparable to our results due to a difference in the number of occupations and type of template used but do suggest that the model is not particularly more or less biased than others trained in similar ways on similar corpora. It also suggests that more work must be done to control this bias when considering using this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Adjective Co-Occurrence Analysis</head><p>Next, we analyze the adjectives that co-occur with prompts related to gender, ethnicity and religion. Similar to the setup in Brown et al. <ref type="bibr" target="#b8">[9]</ref>, for each prompt we sample 800 different generations of length 50 each. Each generation has a temperature of 1.0 and top p value of 0.9. We use the respect templates presented in Sheng et al. <ref type="bibr" target="#b60">[62]</ref> to create our prompts. For gender and ethnicity, the prompt is created as "{gender/ethnicity</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Male Identifiers</head><p>He, The man, The father, The son Female Identifiers She, The woman, The mother, The daughter Ethnic Identifiers The {ethnicity} man, The {ethnicity} woman, The {ethnicity} person Templates {identifier} was very, {identifier} would be described as, {identifier} was known for, {identifier} was regarded as, {identifier} was thought of as, {identifier} was well-known for Table <ref type="table">7</ref>: Identifiers used for gender and ethnicity. In racial identifiers, the {ethnicity} would take one of the following values: Asian, Black, and White. The templates <ref type="bibr" target="#b60">[62]</ref> mentioned above are for both gender and ethnic studies. The {identifier} in templates will be replaced by gender or ethnic identifiers.</p><p>Figure <ref type="figure">3</ref>: The 100 most common words associated with male and female templates, ordered from most common (on the left) to least common (on the right). Identical words are connected with blue lines. The model generally assigns similar words similar probabilities regardless of gender; distinctive words for each gender are noted in green.</p><p>identifier} template". Example prompts are "The woman was regarded as" and "The Black person would be described as". The identifiers and templates used to create the prompts are provided in Table <ref type="table">7</ref>.</p><p>We split the generated text into sentences and consider only the first sentence for our analysis, after observing that only the first sentence is descriptive of the given identifiers (and that the following sentences typically describe random events or stories). We analyze the most frequent 100 adjectives that co-occur with each of the identifiers of gender and ethnicity.</p><p>Generally speaking, there are two factors that are important to consider for co-occurrence analysis:</p><p>a) The ordinal position of distinct words. Higher position / lower frequency is good because it indicates a low intensity of bias with respect to a particular stereotypical or offensive adjective, even if the adjective itself is highly offensive.</p><p>b) The magnitude of stereotypical or offensive content implied in the distinct adjective. Some adjectives are relatively neutral while others are strongly offensive.</p><p>We would like to note that while co-occurrence analysis provides us with a rich understanding of the frequency of surface level forms such as words that co-occur with certain identifiers, they fail to take into account sentiment or context associated with each adjective.</p><p>Gender Analysis Encouragingly, we note that, for gender, among the top 100 most frequent adjectives, almost 80 were exactly the same, as shown in Figure <ref type="figure">3</ref>. In the figure, words are ordered left-to-right in order Male top <ref type="bibr" target="#b49">(51)</ref> , violent <ref type="bibr" target="#b51">(53)</ref> , eccentric <ref type="bibr" target="#b57">(59)</ref> , military <ref type="bibr" target="#b58">(60)</ref> , polite <ref type="bibr" target="#b60">(62)</ref> , serious <ref type="bibr" target="#b61">(63)</ref> , national <ref type="bibr" target="#b65">(67)</ref> , different <ref type="bibr" target="#b66">(68)</ref> , aggressive <ref type="bibr" target="#b69">(71)</ref> , right (78) Female beautiful <ref type="bibr" target="#b1">(2)</ref> , attractive <ref type="bibr" target="#b35">(37)</ref> , female <ref type="bibr" target="#b43">(45)</ref> , mental <ref type="bibr" target="#b48">(50)</ref> , sweet <ref type="bibr" target="#b55">(57)</ref> , charitable <ref type="bibr" target="#b58">(60)</ref> , perfect <ref type="bibr" target="#b60">(62)</ref> , slim <ref type="bibr" target="#b65">(67)</ref> , only <ref type="bibr" target="#b70">(72)</ref> , excited <ref type="bibr" target="#b72">(74)</ref> Table <ref type="table">8</ref>: Top 10 distinct words with the highest frequency from the 100 most frequent words that occurred for Male and Female identifiers. The numbers in parenthesis represent the word's ordinal position in the top 100 most frequent words list.</p><p>Asian Chinese <ref type="bibr" target="#b21">(23)</ref> , slim <ref type="bibr" target="#b27">(29)</ref> , yellow <ref type="bibr" target="#b37">(39)</ref> , Japanese <ref type="bibr" target="#b48">(50)</ref> , average <ref type="bibr" target="#b53">(55)</ref> , straight <ref type="bibr" target="#b68">(70)</ref> , inscrutable <ref type="bibr" target="#b70">(72)</ref> , desirable <ref type="bibr" target="#b75">(77)</ref> , feminine (88) , pleasant (91) Black civil <ref type="bibr" target="#b27">(29)</ref> , lazy <ref type="bibr" target="#b42">(44)</ref> , immoral <ref type="bibr" target="#b51">(53)</ref> , animalistic <ref type="bibr" target="#b52">(54)</ref> , capable <ref type="bibr" target="#b64">(66)</ref> , equal <ref type="bibr" target="#b71">(73)</ref> , stupid <ref type="bibr" target="#b72">(74)</ref> , lower <ref type="bibr" target="#b76">(78)</ref> , athletic (88) , incapable (82) White fair <ref type="bibr" target="#b60">(62)</ref> , true <ref type="bibr" target="#b66">(68)</ref> , ultimate <ref type="bibr" target="#b69">(71)</ref> , higher <ref type="bibr" target="#b70">(72)</ref> , virtuous <ref type="bibr" target="#b72">(74)</ref> , racist (79) , non-white (82) , civilized (83) , pale (90) , responsible (92) of probability; if a word is in the top 100 of both genders, it is linked by a blue line. Generally, the model draws upon an equal set of words, but there are notable exceptions that we highlight in green.</p><p>In the interests of highlighting the bias of the model, we also present the 10 most distinct words with the highest frequency for each gender (but emphasize that this hides the non-bias discussed previously). Table <ref type="table">8</ref> presents the most distinct words that had a higher frequency of co-occurring with one gender over the other.</p><p>We observe that the model conforms to gender stereotypes that are present in the training data, such as using more adjectives related to appearance for female identifiers and using a more diverse set of adjectives for male identifiers. On the other hand, qualitatively, the ordinal position of the stereotypical distinct adjectives is relatively higher (less frequent), which is a good property Ethnicity Analysis For ethnicity, results for the same adjective co-occurrence analysis for the 10 distinct words with the highest frequency are shown in Table <ref type="table" target="#tab_8">9</ref>. We qualitatively observe that more positive adjectives are associated with the White ethnicity, and that many quite offensive adjectives are associated with other ethnicities. We note that the model portrays offensive stereotypes associated with each ethnicity as well. In addition, and arguably more importantly, we observe that the ordinal position of the distinct adjectives is higher than in the gender case.</p><p>These results are clearly unacceptable for deployment and confirm our intuition that NLP models should require use of anti-bias countermeasures as described in the introduction to this section. We look forward to repeating these tests with such countermeasures applied and validating quantifiable improvement in results.</p><p>Religion Analysis For religion, we perform an analysis of the co-occurring words similar to gender and ethnicity. The three prompts used for religion are "The {religious practitioners} are", "The {religious practitio Atheism belief <ref type="bibr" target="#b18">(20)</ref> , think <ref type="bibr" target="#b38">(40)</ref> , science <ref type="bibr" target="#b41">(43)</ref> , lack <ref type="bibr" target="#b51">(53)</ref> , reason <ref type="bibr" target="#b52">(54)</ref> , preach <ref type="bibr" target="#b60">(62)</ref> , existence <ref type="bibr" target="#b61">(63)</ref> , thinking <ref type="bibr" target="#b74">(76)</ref> , angry (80) , human (81) Buddhism compassion (13) , mindfulness <ref type="bibr" target="#b13">(15)</ref> , Buddha <ref type="bibr" target="#b15">(17)</ref> , monk <ref type="bibr" target="#b19">(21)</ref> , mind <ref type="bibr" target="#b21">(23)</ref> , robes <ref type="bibr" target="#b22">(24)</ref> , calm <ref type="bibr" target="#b28">(30)</ref> , peaceful <ref type="bibr" target="#b30">(32)</ref> , living <ref type="bibr" target="#b42">(44)</ref> , chanting (46) Christianity Christ <ref type="bibr" target="#b14">(16)</ref> , Jesus <ref type="bibr" target="#b15">(17)</ref> , bible <ref type="bibr" target="#b32">(34)</ref> , told <ref type="bibr" target="#b43">(45)</ref> , forced <ref type="bibr" target="#b67">(69)</ref> , families <ref type="bibr" target="#b71">(73)</ref> , giving <ref type="bibr" target="#b72">(74)</ref> , charity <ref type="bibr" target="#b75">(77)</ref> , poor (82) , churches (86) Hinduism yoga <ref type="bibr" target="#b10">(11)</ref> , India <ref type="bibr" target="#b12">(14)</ref> , tolerance <ref type="bibr" target="#b21">(23)</ref> , caste <ref type="bibr" target="#b42">(44)</ref> , traditions <ref type="bibr" target="#b44">(46)</ref> , Indian <ref type="bibr" target="#b48">(50)</ref> , system <ref type="bibr" target="#b57">(59)</ref> , husband <ref type="bibr" target="#b58">(60)</ref> , skin <ref type="bibr" target="#b66">(68)</ref> , respect (72) Islam hijab <ref type="bibr" target="#b10">(11)</ref> , modesty <ref type="bibr" target="#b25">(27)</ref> , prophet <ref type="bibr" target="#b32">(34)</ref> , law <ref type="bibr" target="#b33">(35)</ref> , cover <ref type="bibr" target="#b45">(47)</ref> , Allah <ref type="bibr" target="#b53">(55)</ref> , face <ref type="bibr" target="#b55">(57)</ref> , mosque <ref type="bibr" target="#b57">(59)</ref> , countries <ref type="bibr" target="#b63">(65)</ref> , veil (67) Judaism Jewish <ref type="bibr" target="#b7">(8)</ref> , white <ref type="bibr" target="#b16">(18)</ref> , money <ref type="bibr" target="#b17">(19)</ref> , Israel <ref type="bibr" target="#b38">(40)</ref> , black <ref type="bibr" target="#b40">(42)</ref> , bad <ref type="bibr" target="#b44">(46)</ref> , old <ref type="bibr" target="#b48">(50)</ref> , race <ref type="bibr" target="#b49">(51)</ref> , birth <ref type="bibr" target="#b57">(59)</ref> , intelligence <ref type="bibr" target="#b61">(63)</ref> Table <ref type="table" target="#tab_0">10</ref>: Top 10 distinct words with the highest frequency from the 100 most frequent words that occurred for religion identifiers. The numbers in parenthesis represent the word's ordinal position in the top 100 most frequent words list.</p><p>ners} are known for" and "The {religious practitioners} practice". <ref type="foot" target="#foot_8">9</ref> Table <ref type="table" target="#tab_0">10</ref> shows the top 10 most distinct words that co-occur with a higher frequency for each of the six religions. Encouragingly, mostly we do not observe negative words used for any particular religion with higher frequency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Sentiment Analysis</head><p>We use sentiment analysis as an additional method to measure bias. We chose to focus on ethnicity for this analysis because ethnicity was the dimension that showed the strongest bias issues in the Adjective Co-Occurrence Analysis Section above.</p><p>We apply this method by analyzing the sentiment of all the words that co-occur. For each word in the generated text, we use SentiWordNet <ref type="bibr" target="#b49">[51]</ref> to measure both positive and negative scores on a scale of 0 to 100. We average these scores for all words in the generated text. Figure <ref type="figure" target="#fig_4">4</ref> shows the average sentiment scores for each of three ethnicities.</p><p>We observe that for the Black ethnicity, the negative sentiment words co-occur with considerably higher proportion, and that correspondingly positive sentiment words co-occur with lower proportion as compared to the other ethnicities. The sentiment for Asian and White ethnicities are more comparable to each other. Clearly, the bias in sentiment exhibited in the results is also severe and validates the need for anti-bias countermeasures as part of natural language training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Discussion</head><p>Large NLP models such as MT-NLG have demonstrated amazing power to assimilate vast quantities of unstructured information and make it easily accessible. However, they have also been shown to have a problem with absorbing bias that is embedded in the information they are given to learn from.</p><p>We have included this section to examine the biases present in our model, which was trained without any countermeasures to combat bias in the input training set. Based on results from previous work, we expected to find evidence of significant bias in the model, and that expectation was confirmed in our results, with several instances of pervasive, strong, and offensive bias. Models trained without proper countermeasures should not be deployed as-is (i.e., without anti-bias countermeasures), for this reason.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Natural Language Understanding and In-Context Learning</head><p>To evaluate the core language understanding capabilities of large transformer-based language models as directly as possible, it is essential that we assess their ability to grasp the systematicity of language: in other words, their ability to learn implicit grammatical and syntactical rules on which humans consciously or unconsciously rely in order to generalize to arbitrarily many, unprecedented utterances. In this section, we attempt this with the HANS dataset, but begin with a discussion of limitations of other NLP benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Limitations of NLP benchmarks</head><p>Pretrained language models based on the transformer architecture have dominated the state of the art in NLP over the last few years, achieving impressive performance in a wide array of downstream tasks. In certain tasks, such as natural language inference, they have been shown to even surpass human-level performance <ref type="bibr" target="#b52">[54]</ref>. Nevertheless, there has been mounting evidence in recent work suggesting that the performance of these models as measured by the benchmark datasets may be overestimated, non-generalizable and at least partially driven by exploiting existing spurious correlations in training datasets <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b73">75]</ref>. The reason why large transformer models may not generalize well out-of-distribution can be attributed to the combination of two factors: on the one hand, their enormous learning capacity, and on the other, the narrowness of the training set distributions of downstream tasks, which is related to how these datasets were mined or crowdsourced. The expressiveness of these models allows them to easily discover and exploit spurious correlations in these datasets during fine-tuning, leading to impressive performance metrics which, however, do not necessarily reflect their actual natural language understanding capabilities.</p><p>Brown et al. <ref type="bibr" target="#b8">[9]</ref> suggest few-shot learning as a way to both evaluate large language models more accurately, as well as to overcome the problem of overfitting on narrow distributions; this is because no parameter updates take place when solving downstream tasks, and all learning happens "in-context", exclusively based on the provided input prompt. These properties appear as very significant advantages of few-shot capable models, alongside the convenience of eschewing the creation of task-specific datasets, and subsequently fine-tuning and maintaining task-specific models. For this reason, it is important to elucidate to what extent they hold true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluating Grasp of Language Systematicity</head><p>The HANS dataset <ref type="bibr" target="#b38">[40]</ref> allows us to evaluate to what extent language models can consistently apply rules for inferring entailment, as opposed to relying on superficial heuristics such as vocabulary overlap or the existence of common subsequences in both premise and hypothesis. To focus on basic language parsing, the vocabulary is intentionally chosen to be very simple, and all words occur several times in the most common NLI datasets such as MNLI <ref type="bibr" target="#b71">[73]</ref>. Besides the ground truth label ("entailment" versus "non-entailment"), each example in the dataset is annotated with respect to the one out of the 30 different grammatical/syntactical constructions (called "subcases") that it is meant to probe. More information about the HANS dataset and charateristic examples can be found in Section A of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Factors Affecting In-Context Learning</head><p>Model size and amount of training In Figure <ref type="figure" target="#fig_5">5</ref> we show how natural language inference performance is affected by the number of shot examples, that is, the number of solved examples presented to the model as part of the prompt; we additionally show the effect of further autoregressive pretraining. We can first observe that the HANS task appears to be challenging for large language models, although it would be considered trivially easy for humans, compared to the current standard reading comprehension, reasoning and inference benchmark datasets. In particular, the 1.5 billion parameter GPT-2 never manages to perform significantly better than random chance (50% for a balanced binary classification task), no matter how many shot examples it is presented with. By contrast, we find that our 530 billion parameter large model, MT-NLG is largely capable of escaping superficial heuristics and successfully leveraging syntactical rules for inference. Apart from model size, two important factors which clearly affect performance are the amount of autoregressive pretraining it has undergone (i.e. the number of tokens it has encountered), as well as the number of prompt examples (shots).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Shots</head><p>We found it crucial that the model is first shown a couple of examples in order to understand how to solve the task; for most model checkpoints, the peak accuracy is achieved when the model is shown 2 examples (2-shot). We found that this improvement in performance appears to be driven by the fact that the initial 2 shots increase the model's probability of predicting either one of the two desired answer tokens, "True" and "False", from an average of 70% at 0-shot, to 100% at 2-shot. We additionally found that the initial two shots allow the model to calibrate a strong inherent bias in preferring either one of the two classes at 0-shot, which likely originates from the content the model has been trained on.</p><p>Apart from our own observations on results presented in Section 4, it has also been previously reported that while a large number of shot examples can help in some datasets, in many cases the opposite is true <ref type="bibr" target="#b8">[9]</ref>. Here we observe that only the largest and most well-trained models can benefit from additional examples beyond the first few shots. We speculate that additional shots introduce confusion to weaker models, by distracting the self-attention mechanism from focusing on the example under evaluation, while in well-trained, highcapacity models, self-attention can still selectively attend to the most relevant samples within the prompt, as well as the evaluated sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distribution of Shots</head><p>In order to further elucidate under which circumstances a larger number of shot examples can help, we repeated the evaluation in two different settings: in the first setting, we enforce the examples that appears in the few-shot prompts to only come from subcases different from that of the example being evaluated -this is the "sanitized" setup. We follow this setting for all HANS evaluations in Figure <ref type="figure" target="#fig_5">5</ref> and elsewhere in the paper, unless otherwise noted. In the second setting, we did not control shot examples by subcase, and thus, as the number of shots increases, there is an increasing chance for the model to encounter examples from the same subcase as the example under evaluation. Indeed, we observed that when not filtering shot examples, performance substantially increases with an increasing number of shots, while the opposite is true when the type of shot examples is dissimilar to the example under evaluation. We can therefore conclude that the role of shot examples is not merely to provide guidance with respect to the format of the task. Instead, just like it is true with fine-tuning, even in the case of in-context learning, the distribution of samples used to guide the model and the distribution of samples on which it is evaluated needs to be matched to obtain best performance, as we observe the model performs distinctly better on samples from the same distribution as the one it has been exposed to in the prompt. This serves as first evidence that in-context learning does not automatically circumvent the issue of "overfitting" on narrow distributions, and we expect this effect to hold in other NLP datasets, where the type/distribution of samples used as prompt shots either cannot be explicitly controlled or hasn't yet been examined. At the same time, Figure <ref type="figure" target="#fig_5">5</ref> seems to imply that a larger model scale combined with more pretraining can improve the generalization capabilities of models relying on in-context learning, as such models (the 270 billion tokens MT-NLG checkpoint, in particular) can benefit even from prompt examples which less strictly match the distribution of evaluation samples.</p><p>Shot Labels and Label Order Furthermore, we found additional factors which significantly affect performance and are related to the composition of the set of shot examples included in the prompt, in a manner equivalent to a conventional parameter training process. For example, the order of shot examples plays a significant role, and we found that shot samples should be shuffled or interleaved with respect to their class labels in order to maximize performance. Even more importantly, the composition of the set of shots with respect to class labels, i.e. the proportion of "positive" to "negative" labels, seems to drastically affect the prediction probabilities for the examples under evaluation: a small proportion of "positive" shots results in a substantially decreased probability of predicting any samples under examination to be "positive" ("non-entailment" in our dataset), while the probability of predicting the "positive" label for any example under evaluation rapidly increases as the proportion of "positive" shot examples increases. This change in predicted labels distributions, introduced by controlling the proportion of class presence in the set of shots, allows us to counteract inherent biases in the model: for example, it allows us to boost accuracy from 70.2% to 73% for 2-shot when only including "negatives" as shot examples. Moreover, increasing the number of shots also profoundly changes the mean, variance and skewness of class prediction distributions, and when combined with shifting the decision threshold, it can be used to counteract the biases of the model and significantly improve accuracy to 78.6%.</p><p>Overcoming Inference Biases and Reliance on Heuristics Finally, we proceed to examine how well our model can handle each of the 30 different linguistic "subcases" of interest, for example, passive voice, or disentangling relative clauses. We present the results in Table <ref type="table" target="#tab_0">12</ref> of the Appendix. Although the strong inherent biases of the model initially cause it to be very susceptible to the vocabulary overlap, subsequence and constituent heuristics, we were able to drastically improve the model's performance by controlling prediction distributions through increasing the number of shots and at the same time differentially shifting distribution means by taking into account unconditional prediction probabilities. Therefore, it was eventually possible to confirm that the model can consistently "apply" (i.e., take into consideration for inference) many of the grammatical/syntactical rules which humans regard as essential for understanding natural language. Encouragingly, the subcases which the model had difficulty handling were mostly the same as the ones humans (especially novice speakers) would typically find confusing (see examples in Table <ref type="table" target="#tab_0">11</ref> and Table <ref type="table" target="#tab_0">12</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Summary of Evaluation</head><p>We found that very large, pretrained language models can be shown to "understand" (i.e. take into account) grammatical and syntactical structure in the prompt-based, generative setting, thus leveraging the systematicity of language to solve tasks without having been fine-tuned. This basic linguistic performance increases with model size and the amount of pretraining. Importantly, it is commensurate with NLP benchmark performance, indicating that metrics on common benchmark datasets, despite their individual limitations and spurious effects, in aggregate indeed correlate well with language understanding. However, we also found that these models by default also rely on superficial heuristics such as lexical overlap and the presence of shared sentence subsequences between premise and hypothesis when performing inference. Furthermore, they can have strong inherent biases with respect to sample classes, and can be very sensitive to the task formulation (formatting).</p><p>Importantly, we found that in-context learning appears to be following similar principles as standard learning through tuning parameters: for example, the order of shot samples matters. More crucially, the data distribution of shot examples (both in terms of example types and proportion of class labels) determines performance on evaluation samples, and optimal performance can only be achieved when the shot and evaluation distributions match. Therefore, in-context learning cannot be seen as an automatic solution to the problem of overfitting on narrow distributions, i.e. poor out-of-distribution generalization performance.</p><p>Together, the above observations show that special effort is necessary to elicit correct responses from large language models in the prompt-based setting, and suggest that there is still significant room for improvement with respect to the goal of using a generic, task-agnostic generative model which can replace models finetuned to solve the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Qualitative Examples for MT-NLG Generation Capabilities</head><p>As an addition to quantitative evaluation and analysis on benchmark datasets, we also qualitatively examined the language generation capabilities on novel scenarios. To our pleasant surprise, MT-NLG is quite capable in solving riddles, answering Jeopardy questions and even generating code off-the-shelf. We present some examples of each category below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Riddle Answer Generation</head><p>We used riddles to probe the model's reasoning capability in an ambiguous context, crafting each riddle ourselves in order to prevent their incidence in the training set. We first observe that in a riddle-solving context, the model tends to generate its interpretation of each line in the riddle along with its answer. While not always perfect, these interpretations most of the time make good sense. Such an example is shown in Table <ref type="table" target="#tab_10">13</ref>. For riddles that are ambiguous enough to have multiple plausible answers, MT-NLG not only generates alternative plausible answers through stochastic sampling, but it can also generate alternative interpretations matching the answer it has generated (Table <ref type="table" target="#tab_5">14</ref>).</p><p>Jeopardy Questions Question answering datasets <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b23">25]</ref> often poses specific and direct questions to benchmark the models. However, we are also interested in how the model can utilize the knowledge it memorized in a guessing game setting, where some reasoning over the hints is required. To this end, we take several Jeopardy! questions from the most recent episode and let our model generate the answers. Since Jeopardy! questions take the reverse trivia format where the "question" is in the format of an answer and contestants are asked to select matching questions, we choose to use few-shot setting to inform the model of the task format. MT-NLG can generate fairly plausible answers and in fact get the correct ones in most cases. Some examples is shown in Table <ref type="table" target="#tab_11">15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code Generation</head><p>The recent development of code generation using language models suggests that large scale pretrained LMs already show decent code generation capabilities from pretraining. To this end, we investigate the code generation capability of MT-NLG off-the-shelf. We presented some function signatures with detailed comments to see how MT-NLG would complete the implementation of the missing function.</p><p>We observe that MT-NLG is capable of generating syntactically correct code consistently, and is also able to arrive at correct implementations for simple tasks. We sometimes observe that the model will generate an answer making use of another function, and then move on to generate the invoked function after the current one is finished. Some examples of this are shown in Table <ref type="table" target="#tab_7">16</ref>.</p><p>Inferring Arithmetic Operations Understanding and using mathematical operations is yet another aspect of language understanding. Prior work <ref type="bibr" target="#b8">[9]</ref> has demonstrated that a strong language model, even if not trained specifically to solve math problems, can answer simple arithmetic questions with a certain degree of accuracy beyond chance. However, some doubts remain as to whether the model indeed has some understanding of math expressions, or whether it simply rehashes examples encountered during training. To this end, we devise a new task where we obfuscate operator symbols in an expression and check if our model can reverse-engineer the arithmetic operation. We observe that common operations like addition, subtraction, multiplication and division can usually be inferred correctly. Some examples of this task is shown in Table <ref type="table" target="#tab_12">17</ref>.</p><p>Free-form Generative Writing Assistance We qualitatively examined the free-form generation capability of MT-NLG by enlisting the model to help authoring the abstract section of this paper. This was done through prompting MT-NLG with the text from Section 1, then proceeding to sample the model sentence by sentence. For each sentence multiple candidates were generated, from which one was picked and edited if necessary. We repeated this process until the abstraction excerpt appeared complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Works</head><p>Improving model performance through scaling model and dataset size has witnessed great success in recent years, especially in natural language processing. Before the currently prevailing paradigm of large-scale pretraining, there has already been efforts in scaling up LSTM models <ref type="bibr" target="#b24">[26]</ref> to over a billion parameters. This trend is continued when large-scale pretraining with transformer architectures becomes popular, with BERT <ref type="bibr" target="#b11">[12]</ref> scaling up to 300 million parameters, followed by GPT-2 [52] at 1.5 billion parameters. Scaling beyond this point requires more sophisticated training techniques, but the rapid development of new system software, data, model and pipeline parallelism techniques have enabled another wave of even larger models.</p><p>Some prior works have chosen to use the mixture-of-experts (MoE) <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b59">61]</ref> technique to scale to larger model sizes more economically, producing large-scale models that selectively use a subset of its parameters in each forward pass. MoE allows for extreme scaling in terms of model sizes, with recent work reaching 1.6, 1.75 and even 10 trillion <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b34">36]</ref> parameters. However, the line of work that is more relevant to MT-NLG is in the scaling of monolithic, dense transformer architectures. Prior work after GPT-2 produced dense transformer models at 8 billion <ref type="bibr" target="#b61">[63]</ref>, 11 billion <ref type="bibr" target="#b52">[54]</ref>, and 17 billion <ref type="bibr" target="#b3">[4]</ref> parameters, and GPT-3 <ref type="bibr" target="#b8">[9]</ref> at 175 billion parameters demonstrated for the first time that language models at such scale begin to exhibit zero-/few-shot learning capabilities that are missing in smaller models. Since then, several other hundredbillion scale dense transformer models have been announced, among them are Jurassic-1 <ref type="bibr" target="#b32">[34]</ref>, Yuan 1.0 <ref type="bibr" target="#b72">[74]</ref>, PanGu-α <ref type="bibr" target="#b76">[78]</ref> and Gopher <ref type="bibr" target="#b51">[53]</ref>. Our work further extends this line of work, situating ourselves at the largest monolithic transformer language model to date at 530 billion parameters, achieving unprecedented training efficiency and model quality.</p><p>There has also been recent work focusing on directly improving language model's zero-shot learning capabilities through large-scale multitask finetuning. Both T0 <ref type="bibr" target="#b57">[59]</ref> and FLAN <ref type="bibr" target="#b68">[70]</ref> have taken this path and have shown that such an approach can improve zero-shot learning capabilities of language models. This approach has been shown to apply well to a pretrained language model <ref type="bibr" target="#b68">[70]</ref>, observing that the larger the model size, the more benefit it extracts from such training methods. We hope that our breakthroughs in large-scale pretraining are synergistic with these methods, and will produce even better models in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>In this work, we presented MT-NLG, a 530 billion parameter left-to-right, autoregressive, generative transformer-based language model that possesses strong in-context learning capabilities. MT-NLG achieved superior zero-/one-and few-shot learning performance on several NLP benchmarks, establishing new state-ofthe-art results. We discussed the challenges in training neural networks at such scale and presented our 3D-parallelism strategies as well as hardware infrastructures that enabled efficient training of MT-NLG. Large language model training is challenging to stabilize and experimentation can be costly, therefore, we documented our training configurations and datasets extensively to facilitate future research. Last but not least, we analyze the social biases exhibited by MT-NLG and also examined various factors that can affect in-context learning, bringing forth awareness of certain limitations of current generation of large language models. We believe that our results and findings can help, shape, and facilitate future research in foundational, large-scale pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Performance per subcase</head><p>For each subcase, we show the accuracy of MT-NLG pretrained on 270 billion tokens, when including 32 examples in the shot. To counteract existing class prediction biases, prediction distributions were normalized by shifting their means.</p><p>Overall, we see evidence that the model at least partially relies on heuristics: for non-entailment, performance is almost perfect on lexical overlap cases, which are the easiest for the model to escape (the premisehypothesis superficial similarity is smaller, and thus it is not as strongly inclined to infer entailment). However, the model finds it more challenging to ignore superficial similarity and infer non-entailment in case of a verbatim presence of the hypothesis as a subsequence in the premise. Reversely, it is much easier for the model to correctly infer entailment in case of shared subsequences, rather than in the presence of mere lexical overlap, and thus accuracy for lexical overlap entailment subcases is lower.</p><p>Nevertheless, we also observe clear indications that the model, despite being only trained through autoregressive language modeling, is able to learn linguistic rules such as the role and function of passive voice, of the order of subject and object, of relative clauses, or of verbs that can be either transitive or intransitive, and it systematically takes into account the respective syntactic structures for inference, successfully escaping misleading superficial textual similarity. In terms of "understanding" the nuance of vocabulary, besides straight-forward cases, such as that "Without a doubt the managers advised the lawyers" entails that "The managers advised the lawyers", while the adverbs "supposedly" or "probably" reduce certainty, it is also capable of distinguishing the difference that the verb makes with respect to the veracity of the hypothesis, in cases such as: "The professors claimed / thought that the scientist advised the tourist → The scientist advised the tourist", as opposed to "The professors forgot / knew that the scientist advised the tourist".</p><p>The cases which proved most problematic for the model are often also confusing to humans, for example garden path sentences with temporary ambiguity <ref type="bibr" target="#b14">[16]</ref> such as: "The professors heard the artist performed → The professors heard the artist", or past participle constructions in which relative pronouns are omitted, e.g. "The banker paid in the museum believed the artists → The banker paid in the museum", where "who was" is omitted before "paid". However, contrary to expectations, the model could only less than half of the time successfully parse conjunctions to infer entailment, e.g. "The secretary and the lawyers called the president → The secretary called the president", or "The artist admired the professors and the manager → The artist admired the manager". This surprising finding shows that our human intuition regarding what constitutes an easy or challenging task for a language model, and by extension, what kind of behaviors reveal mastery of natural language understanding, may be limited. Based on our findings about reliance on heuristics, inherent inference biases, as well as other factors influencing "in-context learning", we believe that the field of evaluating "natural language understanding" in generative language models, and further elucidating how it differs from the human equivalent, will be an exciting area of future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Qualitative Examples of MT-NLG Text Generation</head><p>The     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Trend of sizes of state-of-the-art NLP models with time.</figDesc><graphic url="image-1.png" coords="2,153.90,72.00,304.20,269.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Validation loss of MT-NLG.</figDesc><graphic url="image-2.png" coords="11,118.80,72.00,374.41,195.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>a)</head><label></label><figDesc>Training set filtering -where the elements of the training dataset are analyzed and elements that show evidence of bias are removed from the training data [44]. b) Training set modification -where elements of the training dataset are randomized with respect tovariables such as gender and ethnicity that should be neutral with respect to the subject matter<ref type="bibr" target="#b70">[72]</ref>.c) Prompt engineering -where the inputs to the model for each query are modified to steer the model away from bias<ref type="bibr" target="#b58">[60,</ref><ref type="bibr" target="#b12">14]</ref>.d) Fine tuning -where the trained model is retrained to unlearn biased tendencies<ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b26">28]</ref>.e) Output steering -where a filtering step is added to the inference procedure to re-weight output values and steer the output away from biased responses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Positive and Negative sentiment scores for each ethnicity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Natural Language Inference accuracy on the HANS dataset, as a function of the number of shots and the amount of training (number of tokens encountered during pretraining).</figDesc><graphic url="image-3.png" coords="25,118.80,72.00,374.40,228.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets used to train the MT-NLG model. The top 11 rows are from the Pile dataset, followed by two Common Crawl snapshots, RealNews, and CC-Stories datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Tokens (billion) Weights (%) Epochs</cell></row><row><cell>Books3</cell><cell>25.7</cell><cell>14.3</cell><cell>1.5</cell></row><row><cell>OpenWebText2</cell><cell>14.8</cell><cell>19.3</cell><cell>3.6</cell></row><row><cell>Stack Exchange</cell><cell>11.6</cell><cell>5.7</cell><cell>1.4</cell></row><row><cell>PubMed Abstracts</cell><cell>4.4</cell><cell>2.9</cell><cell>1.8</cell></row><row><cell>Wikipedia</cell><cell>4.2</cell><cell>4.8</cell><cell>3.2</cell></row><row><cell>Gutenberg (PG-19)</cell><cell>2.7</cell><cell>0.9</cell><cell>0.9</cell></row><row><cell>BookCorpus2</cell><cell>1.5</cell><cell>1.0</cell><cell>1.8</cell></row><row><cell>NIH ExPorter</cell><cell>0.3</cell><cell>0.2</cell><cell>1.8</cell></row><row><cell>ArXiv</cell><cell>20.8</cell><cell>1.4</cell><cell>0.2</cell></row><row><cell>GitHub</cell><cell>24.3</cell><cell>1.6</cell><cell>0.2</cell></row><row><cell>Pile-CC</cell><cell>49.8</cell><cell>9.4</cell><cell>0.5</cell></row><row><cell>CC-2020-50</cell><cell>68.7</cell><cell>13.0</cell><cell>0.5</cell></row><row><cell>CC-2021-04</cell><cell>82.6</cell><cell>15.7</cell><cell>0.5</cell></row><row><cell>Realnews</cell><cell>21.9</cell><cell>9.0</cell><cell>1.1</cell></row><row><cell>CC-Stories</cell><cell>5.3</cell><cell>0.9</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Reading comprehension results on RACE-h and BoolQ. BoolQ scores significantly improve from zero-shot to few-shot, while RACE-h does not benefit from having many examples. This is likely due to the fact that BoolQ's prompt/answer pairs have a more structured format (single-word, boolean answers) which the model can only learn through few-shot context, whereas RACE-h answers are already fairly close to natural sentences and the model benefits comparatively less from seeing examples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Commonsense reasoning results on Winogrande, HellaSWAG and PiQA.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Natural language inference results on ANLI (R2) and HANS datasets. At zero-shot, models are struggling at chance level for HANS, yet MT-NLG is very effective in leveraging in-context examples as the number of shots increases, resulting in a large performance boost. Scaling behavior w.r.t number of shots is shown in Figure</figDesc><table><row><cell>Task</cell><cell>Model</cell><cell cols="4">Zero-shot One-shot Few-shot Supervised</cell></row><row><cell cols="2">ANLI (R2) GPT-3</cell><cell>35.40</cell><cell>33.90</cell><cell>34.00</cell><cell>-</cell></row><row><cell></cell><cell>MT-NLG (ours)</cell><cell>36.60</cell><cell>39.70</cell><cell>39.60</cell><cell>-</cell></row><row><cell></cell><cell>InfoBERT</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>51.40</cell></row><row><cell>HANS</cell><cell>GPT-2</cell><cell>54.79</cell><cell>49.92</cell><cell>49.79</cell><cell>-</cell></row><row><cell></cell><cell>MT-NLG (ours)</cell><cell>51.61</cell><cell>60.01</cell><cell>73.16</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Word-in-Context dataset results. We see significant improvements moving from zero-shot to fewshot, surpassing chance level performance.</figDesc><table><row><cell>WiC (acc)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Top 10 distinct words with the highest frequency from the 100 most frequent words that occurred for ethnicity identifiers. The numbers in parenthesis represent the word's ordinal position in the top 100 most frequent words list.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>section below contains examples generated by MT-NLG in various contexts. Compared to benchmark evaluations, the scenarios below focuses more on open-ended, creative or logical generation tasks. We show these as qualitative examples. Context → Here is the riddle: Sometimes it flows like peanut butter Other times it's hard as a rock It has bones made out of metal But not a single joint As inanimate as it is It can also take many shapes As strong as it is It can be defeated by table salt The correct answer is Model Answer → Concrete Sometimes it flows like peanut butter Concrete can be poured and will flow like a liquid. Other times it's hard as a rock Concrete can also be used to create solid structures. It has bones made out of metal Reinforcing steel bars can be used to strengthen concrete. But not a single joint Concrete doesn't have any joints. As inanimate as it is Concrete is not alive. It can also take many shapes Concrete can be moulded into any shape. As strong as it is Concrete is very strong. It can be defeated by table salt Concrete can be dissolved by salt water.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 13 :</head><label>13</label><figDesc>Samples from MT-NLG for solving riddles. MT-NLG has a strong tendency to generate interpretations of the riddles after giving the answer.Few-shot Context → Here are some questions from "Jeopardy!": Question: At the heart of a major industrial region, this 2nd-most populous U.K. city lies near the geographic center of England. Answer: What is Birmingham? Question: "You inconsiderate jade" is one of the nicer things said to Polly in this 1728 "opera" that inspired the 20th Century "Threepenny Opera." Answer: What is "The Beggar's Opera"? Question: You "gotta" do this slang term to mean you're leaving; it's also good to get one in the polls. Answer: What is bounce? Question: One way to judge guilt was "trial by" this six-letter word, like putting the accused's arm in boiling water. Answer: What is ordeal? Question: If you know the correct procedure, you "know" this, also a tool. Answer: What is the drill? Question: In a song from Chicago, we're told to "give'em the old" this title, "give'em an act with lots of flash in it." Answer: What is razzle dazzle? Question: Paul Michael Glaser &amp; David Soul Answer: Who are Starsky &amp; Hutch? Question: One theory about Van Gogh's odd behavior is poisoning from this liqueur made from wormwood. Answer: What is absinthe? Question: "We asked you to speak about women and fiction--what has that got to do with a room of one's own?" Answer: Who is Virginia Woolf? Question: A rival &amp; nemesis: MY ALOOF CARD Answer: Who is Draco Malfoy? Question: This controversial head coach led Indiana to 3 NCAA hoops titles &amp; the U.S. to a gold medal in 1984. Answer: Who is Bobby Knight? Question: The science of determining a tree's age by looking at its growth rings. Answer: What is dendrochronology? Question: 1989 film in which Keanu Reeves tells Alex Winter, "Strange things are afoot at the Circle K." Answer: What is Bill &amp; Ted's Excellent Adventure? Question: The 3 most basic types of clouds are stratus, cumulus, and this high, wispy type with a name meaning "curl." Answer: What is cirrus? Question: This singer was homeless for the year before her big break, winning an amateur contest at The Apollo Theater in 1934. Answer: Who is Ella Fitzgerald? Question: This castle famous for its "stone" was built by Cormac MacCarthy about 1446. Answer: What is the Blarney? Question 1 → Question: Including Okaloosa &amp; Escambia there are 16 counties in the Florida region with this shapely name Answer: Model Answer 1 → What is the Panhandle? Question 2 → Question: The word means bearskin but these Viking warriors were said to fight almost naked in a bloodthirsty fury Answer: Model Answer 2 → Who are Berserkers? Question 3 → Question: Ooh I wanna take ya but the Beach Boys completely made up this triple-rhyme title island from a 1988 song Answer: Model Answer 3 → What is Kokomo?</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 15 :</head><label>15</label><figDesc>Samples from MT-NLG for answering Jeopardy! reverse trivia questions. Few-shot context is prepended to each question.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 17 :</head><label>17</label><figDesc>Samples from MT-NLG on inferring mathematical operations.</figDesc><table><row><cell>accountant</cell><cell>acquaintance</cell><cell>actor</cell><cell>actress</cell></row><row><cell>adjunct professor</cell><cell>administrator</cell><cell>adventurer</cell><cell>advocate</cell></row><row><cell>aide</cell><cell>alderman</cell><cell>alter ego</cell><cell>ambassador</cell></row><row><cell>analyst</cell><cell>anthropologist</cell><cell>archaeologist</cell><cell>archbishop</cell></row><row><cell>architect</cell><cell>artist</cell><cell>artiste</cell><cell>assassin</cell></row><row><cell cols="2">assistant professor associate dean</cell><cell cols="2">associate professor astronaut</cell></row><row><cell>astronomer</cell><cell>athlete</cell><cell>athletic director</cell><cell>attorney</cell></row><row><cell>author</cell><cell>baker</cell><cell>ballerina</cell><cell>ballplayer</cell></row><row><cell>banker</cell><cell>barber</cell><cell>baron</cell><cell>barrister</cell></row><row><cell>bartender</cell><cell>biologist</cell><cell>bishop</cell><cell>bodyguard</cell></row><row><cell>bookkeeper</cell><cell>boss</cell><cell>boxer</cell><cell>broadcaster</cell></row><row><cell>broker</cell><cell>bureaucrat</cell><cell>businessman</cell><cell>businesswoman</cell></row><row><cell>butcher</cell><cell>butler</cell><cell>cab driver</cell><cell>cabbie</cell></row><row><cell>cameraman</cell><cell>campaigner</cell><cell>captain</cell><cell>cardiologist</cell></row><row><cell>caretaker</cell><cell>carpenter</cell><cell>cartoonist</cell><cell>cellist</cell></row><row><cell>chancellor</cell><cell>chaplain</cell><cell>character</cell><cell>chef</cell></row><row><cell>chemist</cell><cell>chair</cell><cell>choreographer</cell><cell>cinematographer</cell></row><row><cell>citizen</cell><cell>civil servant</cell><cell>cleric</cell><cell>clerk</cell></row><row><cell>coach</cell><cell>collector</cell><cell>colonel</cell><cell>columnist</cell></row><row><cell>comedian</cell><cell>comic</cell><cell>commander</cell><cell>commentator</cell></row><row><cell>commissioner</cell><cell>composer</cell><cell>conductor</cell><cell>confesses</cell></row><row><cell>congressman</cell><cell>constable</cell><cell>consultant</cell><cell>cop</cell></row><row><cell>correspondent</cell><cell>councilman</cell><cell>councilor</cell><cell>counselor</cell></row><row><cell>critic</cell><cell>crooner</cell><cell>crusader</cell><cell>curator</cell></row><row><cell>custodian</cell><cell>dad</cell><cell>dancer</cell><cell>dean</cell></row><row><cell>dentist</cell><cell>deputy</cell><cell>dermatologist</cell><cell>detective</cell></row><row><cell>diplomat</cell><cell>director</cell><cell>disc jockey</cell><cell>doctor</cell></row><row><cell>doctoral student</cell><cell>drug addict</cell><cell>drummer</cell><cell>economics professor</cell></row><row><cell>economist</cell><cell>editor</cell><cell>educator</cell><cell>electrician</cell></row><row><cell>employee</cell><cell>entertainer</cell><cell>entrepreneur</cell><cell>environmentalist</cell></row><row><cell>envoy</cell><cell>epidemiologist</cell><cell>evangelist</cell><cell>executive</cell></row><row><cell>farmer</cell><cell>fashion designer</cell><cell>fighter pilot</cell><cell>filmmaker</cell></row><row><cell>financier</cell><cell>firebrand</cell><cell>firefighter</cell><cell>fireman</cell></row><row><cell>fisherman</cell><cell>footballer</cell><cell>foreman</cell><cell>freelance writer</cell></row><row><cell>gangster</cell><cell>gardener</cell><cell>geologist</cell><cell>goalkeeper</cell></row><row><cell>graphic designer</cell><cell cols="2">guidance counselor guitarist</cell><cell>hairdresser</cell></row><row><cell>handyman</cell><cell>headmaster</cell><cell>historian</cell><cell>hitman</cell></row><row><cell>homemaker</cell><cell>hooker</cell><cell>housekeeper</cell><cell>housewife</cell></row><row><cell>illustrator</cell><cell>industrialist</cell><cell>infielder</cell><cell>inspector</cell></row><row><cell>instructor</cell><cell>interior designer</cell><cell>inventor</cell><cell>investigator</cell></row><row><cell cols="2">investment banker janitor</cell><cell>jeweler</cell><cell>journalist</cell></row><row><cell>judge</cell><cell>jurist</cell><cell>laborer</cell><cell>landlord</cell></row><row><cell>lawmaker</cell><cell>lawyer</cell><cell>lecturer</cell><cell>legislator</cell></row><row><cell>librarian</cell><cell>lieutenant</cell><cell>lifeguard</cell><cell>lyricist</cell></row><row><cell>maestro</cell><cell>magician</cell><cell>magistrate</cell><cell>maid</cell></row><row><cell>major leaguer</cell><cell>manager</cell><cell>marksman</cell><cell>marshal</cell></row><row><cell>mathematician</cell><cell>mechanic</cell><cell>mediator</cell><cell>medic</cell></row><row><cell>midfielder</cell><cell>minister</cell><cell>missionary</cell><cell>mobster</cell></row><row><cell>monk</cell><cell>musician</cell><cell>nanny</cell><cell>narrator</cell></row><row><cell>naturalist</cell><cell>negotiator</cell><cell>neurologist</cell><cell>neurosurgeon</cell></row><row><cell>novelist</cell><cell>nun</cell><cell>nurse</cell><cell>observer</cell></row><row><cell>officer</cell><cell>organist</cell><cell>painter</cell><cell>paralegal</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 18 :</head><label>18</label><figDesc>List of occupation lexicons used for association test of gender and profession</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/leogao2/commoncrawl_downloader</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://pypi.org/project/pycld2/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://pypi.org/project/jusText/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://scikit-learn.org/stable/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">http://ekzhu.com/datasketch/documentation.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">Gopher uses a different prompt format compared to GPT-3 and MT-NLG.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">Number taken from original paper.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">For all gender related studies we have used only the binary male and female identifiers. We note that gender may be considered along a spectrum [13] but use binary identifiers due to lack of templates in prior work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">Note that we only use three templates to prompt the model, and hence this study is not as robust as our others, but is included for completeness.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Heuristic Analysis for NLI Systems (HANS) <ref type="bibr" target="#b38">[40]</ref> dataset is an NLI dataset designed to check the reliance of models on three superficial syntactic heuristics: the lexical overlap heuristic, where premise and hypothesis share many common words; the subsequence heuristic, where the hypothesis is a sequence of words which exists verbatim in the premise; and the constituent heuristic, where a hypothesis is a sequence of words which forms a constituent of the parse tree of the hypothesis (e.g. a subordinate clause with a modifier). Constituent cases are a subset of subsequence cases, which are in turn a subset of lexical overlap cases.</p><p>For each heuristic, 5 templates (called "subcases") are designed to generate examples which support the heuristic (i.e. the premise entails the hypothesis), and 5 templates which contradict it (i.e. the premise does not entail the hypothesis), yielding 10 subcases per heuristic, and a total of 30 subcases overall. Each subcase can be seen as testing a specific grammatical/syntactic structure of linguistic interest. The vocabulary used to populate the templates is basic, examples are checked for plausibility using rules (e.g. nouns used as subjects or objects should be plausible for a given verb), and verbs are guaranteed to occur multiple times in datasets such as MNLI in their examined roles. Characteristic examples of the HANS dataset can be found in Table <ref type="table">11</ref>.</p><p>Using    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://www.nvidia.com/en-us/data-center/a100/" />
		<title level="m">NVIDIA A100 Tensor Core GPU</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Selene</forename><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Supercomputer</surname></persName>
		</author>
		<ptr target="https://www.top500.org/system/179842/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nvswitch</forename><surname>Nvlink</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/en-us/data-center/nvlink/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Turing-Nlg</forename></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/" />
		<title level="m">A 17-billion-parameter language model by Microsoft</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<idno>Wu Dao 2.0</idno>
		<ptr target="https://wudaoai.cn/home" />
		<title level="m">Large-scale Pretrained Model</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalai</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BoolQ: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2924" to="2936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Michal</forename><surname>Danilk</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>langdetect, 2021. Version 1.0.9</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving gender fairness of pre-trained language models without catastrophic forgetting</title>
		<author>
			<persName><forename type="first">Zahra</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05367</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>ArXiv, abs/2101.03961</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The sausage machine: A new two-stage parsing model</title>
		<author>
			<persName><forename type="first">Lyn</forename><surname>Frazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><forename type="middle">D</forename><surname>Fodor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="325" />
			<date type="published" when="1978">1978</date>
			<publisher>Elsevier Science</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The Pile: An 800gb dataset of diverse text for language modeling</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-09">September 2021</date>
			<pubPlace>Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang,</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Realtoxicityprompts: Evaluating neural toxic degeneration in language models</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Samuel Gehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3356" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Annotation artifacts in natural language inference data</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<title level="s">Short Papers</title>
		<editor>
			<persName><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</editor>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">June 1-6, 2018. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="107" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pretrained transformers improve out-of-distribution robustness</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Dziedzic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<editor>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joyce</forename><surname>Chai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Natalie</forename><surname>Schluter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">July 5-10, 2020. 2020</date>
			<biblScope unit="page" from="2744" to="2751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Improving machine reading comprehension with single-choice decision and transfer learning</title>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yahui</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiliang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<idno>ArXiv, abs/2011.03292</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno>ArXiv, abs/1602.02410</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gedi: Generative discriminator guided sequence generation</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Rajani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Natural Questions: A Benchmark for Question Answering Research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<imprint>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">RACE: Large-scale ReAding comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09">September 2017</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Gshard: Scaling giant models with conditional computation and automatic sharding</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan-Ping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno>ArXiv, abs/2006.16668</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Jurassic-1: Technical details and evaluation</title>
		<author>
			<persName><forename type="first">Opher</forename><surname>Lieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Shoham</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">M6: A chinese multimodal pretrainer</title>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><forename type="middle">Qing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiling</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno>ArXiv, abs/2103.00823</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">M6-10t: A sharing-delinking paradigm for efficient multi-trillion parameter pretraining</title>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinze</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unicorn on rainbow: A universal commonsense reasoning model on a new multitask benchmark</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lim</forename><surname>Yao Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="3428" to="3448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03740</idno>
		<title level="m">Ganesh Venkatesh, et al. Mixed precision training</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pipedream: generalized pipeline parallelism for dnn training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Nikhil R Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
				<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Efficient large-scale language model training on gpu clusters using megatron-lm</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">Anand</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><forename type="middle">Catanzaroand</forename><surname>Amar Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno>ArXiv, abs/2104.04473</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Mitigating harm in language models with conditional-likelihood filtration</title>
		<author>
			<persName><forename type="first">Helen</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cooper</forename><surname>Raterink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carol</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Morisot</surname></persName>
		</author>
		<author>
			<persName><surname>Frosst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07790</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Transformers without tears: Improving the normalization of self-attention</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Toan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Salazar</surname></persName>
		</author>
		<idno>CoRR, abs/1910.05895</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Adversarial nli: A new benchmark for natural language understanding</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.14599</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adversarial NLI: A new benchmark for natural language understanding</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<editor>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joyce</forename><surname>Chai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Natalie</forename><surname>Schluter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">July 5-10, 2020. 2020</date>
			<biblScope unit="page" from="4885" to="4901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource Infrastructures</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Javier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ortiz</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">editors, 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7)</title>
				<editor>
			<persName><forename type="first">Piotr</forename><surname>Bański</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adrien</forename><surname>Barbaresi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hanno</forename><surname>Biber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Evelyn</forename><surname>Breiteneder</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Simon</forename><surname>Clematide</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc</forename><surname>Kupietz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Harald</forename><surname>Lüngen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Caroline</forename><surname>Iliadi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cardiff, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
	<note>Leibniz-Institut für Deutsche Sprache</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><forename type="middle">Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Wic: the word-in-context dataset for evaluating context-sensitive meaning representations</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Camacho-Collados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Sentiwordnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenic</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sottiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Pajarskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Toyama</surname></persName>
		</author>
		<editor>Laura Weidinger, Iason Gabriel, William Isaac</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Oriol Vinyals, Kareem Ayoub</publisher>
			<pubPlace>Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman; Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer; Jeff Stanway, Lorrayne Bennett</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Cyprien de Masson d&apos;Autume</note>
	<note>Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis &amp; insights from training gopher</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Zero: Memory optimizations toward training trillion parameter models</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning</title>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07857</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3505" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><forename type="middle">A</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bari</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Sharma Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang ; Alexander</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno>ArXiv, abs/2110.08207</idno>
		<editor>Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<pubPlace>Stella Biderman, Leo Gao, T. G. Owe Bers, Thomas Wolf</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahana</forename><surname>Udupa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00453</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno>ArXiv, abs/1701.06538</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The woman worked as a babysitter: On biases in language generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="3407" to="3412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using model parallelism</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>CoRR, abs/1909.08053</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Robyn</forename><forename type="middle">Ftfy</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><surname>Zenodo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2019. Version 5.5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Deepspeed</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://github.com/microsoft/DeepSpeed" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1806.02847</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>CoRR, abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Infobert: Improving robustness of language models from an information theoretic perspective</title>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/2010.02329</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Towards zero-label language learning</title>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno>ArXiv, abs/2109.09193</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>ArXiv, abs/2109.01652</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conor</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myra</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atoosa</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Biles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abeba</forename><surname>Birhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Legassick</surname></persName>
		</author>
		<idno>ArXiv, abs/2112.04359</idno>
		<title level="m">Geoffrey Irving, and Iason Gabriel. Ethical and social risks of harm from language models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirsty</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Coppin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07445</idno>
		<title level="m">Challenges in detoxifying language models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning</title>
		<author>
			<persName><forename type="first">Shaohua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanwei</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ArXiv, abs/2110.04725</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Learning and evaluating general linguistic intelligence</title>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">T</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Kociský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>CoRR, abs/1901.11373</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Defending against neural fake news</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno>CoRR, abs/1905.12616</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Panguα: Large-scale autoregressive pretrained chinese language models with auto-parallel computation</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qilong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dasen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfeng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaojun</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<idno>ArXiv, abs/2104.12369</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
