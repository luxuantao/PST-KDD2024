<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Investigating Managed Language Runtime Performance Why JavaScript and Python are 8x and 29x slower than C++, yet Java and Go can be faster?</title>
				<funder>
					<orgName type="full">Canada Research Chair fund</orgName>
				</funder>
				<funder ref="#_gWrTuVS">
					<orgName type="full">NSERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Lion</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>?YScope</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adrian</forename><surname>Chiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>?YScope</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Stumm</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>?YScope</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ding</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>?YScope</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Investigating Managed Language Runtime Performance Why JavaScript and Python are 8x and 29x slower than C++, yet Java and Go can be faster?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The most widely used programming languages today are managed languages. They are popular because their vast features improve many aspects of code development, including increased productivity and safety. However, as a product or service scales in usage, performance issues become a problem. Developers are then often faced with complex choices as they must decide whether the desired performance can be squeezed from existing code, or whether their language has reached its performance limits, requiring years of code to be ported to a new more-performant language. To make matters worse, runtime performance is shrouded in mystery as it involves complex interactions of different components, such as interpreter, just-in-time (JIT) compiler, thread library, and Garbage Collection (GC) system.</p><p>We present an in-depth performance analysis and comparison of Java, Go, JavaScript, and Python, using C++ as a baseline. We carefully instrumented the different language runtimes, so that developers can precisely measure the number of cycles taken to execute any bytecode instruction, or the overhead of dynamic type checking in JavaScript. This allows us to accurately identify sources of overhead. We further created 6 applications from the ground up to establish the LangBench benchmark; the applications cover a range of complexity, and they cover a variety of application scenarios differing in compute intensity, memory usage, network and disk I/O intensity, and available concurrency. We comprehensively analyze their completion times, resource usage, and scalability.</p><p>Overall, we found that V8/Node.js and CPython exhibit excessive overheads, executing applications 8.01x and 29.50x slower on average than their C++ counterparts, respectively. Making matters worse, applications on these two runtimes scale poorly in that they cannot effectively utilize more than one core. In contrast, OpenJDK and Go applications are performance competitive to C++, running only 1.43x and 1.30x slower, respectively, and they can easily scale to multiple cores. There are applications where OpenJDK and Go outperform their C++ counterparts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Programming languages with integrated run-time environments have continuously grown in popularity. The three most popular languages on GitHub since 2015 are JavaScript, Java, and Python <ref type="bibr" target="#b33">[38]</ref>. These languages offer the promise of improved developer productivity and thus faster product creation and adaptation because of a variety of features they offer, including easier readability and usability, dynamic type checking, memory management with garbage collection, and dynamic memory safety checks. We use the term "managed languages" to refer to these type of programming languages.</p><p>Managed languages are increasingly being used to implement systems software where performance is critical. Both Hadoop <ref type="bibr" target="#b47">[54]</ref> and Spark <ref type="bibr" target="#b69">[76]</ref> run on a Java Virtual Machine (JVM) <ref type="bibr" target="#b54">[61]</ref> as they are implemented in Java and Scala respectively. Kubernetes <ref type="bibr" target="#b18">[21]</ref>, etcd (a distributed key-value store <ref type="bibr" target="#b3">[4]</ref>), and M3 (a distributed time series database and query engine built by Uber <ref type="bibr" target="#b12">[14]</ref>) are all implemented in Go. Recently, even an operating system (OS) kernel, Biscuit <ref type="bibr" target="#b45">[52]</ref>, was implemented in Go <ref type="bibr" target="#b7">[8]</ref>. Openstack <ref type="bibr" target="#b15">[18]</ref>, Paypal <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr">Instagram [22]</ref>, and Dropbox all heavily utilize Python; Python is Dropbox's "most widely used language both for backend services and the desktop client app" with almost 4 million lines of Python code in one repository <ref type="bibr" target="#b17">[20]</ref>. As a final example, JavaScript is used in the performance critical path for the Bladerunner pub/sub system at Facebook <ref type="bibr" target="#b42">[49]</ref>.</p><p>Several factors come into play when selecting a programming language for a new service, including current developer expertise and experience, constraints imposed by existing ecosystems (e.g., home-grown libraries, development, debugging tools, performance monitoring and logging systems, etc.), and developer productivity. Managed languages are an attractive proposition, precisely because they offer the promise of higher developer productivity, leading to faster project completion times. The performance of the language is rarely a consideration at the outset, in part because of the belief that performance issues can be addressed later, perhaps through horizontal scaling by simply adding hardware. Some go as far as to claim "Choosing a language for your application simply because it's 'fast' is the ultimate form of premature optimization" <ref type="bibr" target="#b40">[47]</ref>.</p><p>However, performance will ultimately become a priority as the usage of the service begins to scale and the service becomes too slow or the cost of hardware becomes too high. Developers then begin a large sequence of performance optimizations that can grow into herculean efforts. For example, Twitch.tv and others use tricks and tweaked parameters to meet desired GC performance in Go <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">19]</ref>; project Tungsten in Spark goes as far as to bypass the JVM, to squeeze out performance <ref type="bibr" target="#b19">[23]</ref>.</p><p>But there can come a point where incremental optimizations (requiring much time and effort) no longer suffice and a more radical solution must be considered, namely switching to a "better performing" language. A few examples from industry: Stream abandoned Python for Go, as Python would spend 10ms creating objects from data that Cassandra took 1ms to fetch, noting that "We've been optimizing Cassandra, PostgreSQL, Redis, etc. for years, but eventually, you reach the limits of the language you're using." Discord switched from Go to Rust claiming that "Rust was able to outperform the hyper hand-tuned Go version." <ref type="bibr" target="#b36">[43]</ref>. Performance issues are also cited as the main reason Twitter was forced to switch from Ruby on Rails to Scala and Java <ref type="bibr" target="#b35">[40,</ref><ref type="bibr">42]</ref>.</p><p>When selecting a new language for performance reasons, the question is: what language? Understanding the performance and scalability implications of a (new) language today is non-trivial, especially for managed languages. This is for several reasons.</p><p>First, no empirical studies exist that scientifically compare the different managed languages. The primary source of information available today is the blogosphere containing heated "religious" debates that include tunnel-visioned anecdotes with few rigorous measurements to back up stated claims. For example, while many believe programs written in Java run slower than when written in C/C++ <ref type="bibr" target="#b23">[27]</ref>, others suggest that Java programs can be faster than C, because the JIT compiler produces faster machine code by leveraging a runtime profile <ref type="bibr" target="#b22">[26]</ref>. Similarly, there have been polarized debates with respect to the performance of JavaScript <ref type="bibr">[11,</ref><ref type="bibr" target="#b34">39]</ref>, Go <ref type="bibr" target="#b36">[43,</ref><ref type="bibr" target="#b39">46]</ref> and even Python -for example, sources from Paypal claimed that Python offered superior performance over other languages and reported multiple cases where Python outperformed their C++ and Java counterparts while requiring less code <ref type="bibr" target="#b0">[1]</ref>. Similarly, developers reported that Python could outperform both C/C++ and Java when using regular expressions or strings <ref type="bibr" target="#b28">[33]</ref><ref type="bibr" target="#b29">[34]</ref><ref type="bibr" target="#b30">[35]</ref>.</p><p>Discussions on scalability are even muddier. For example, in a popular blog by the official Node.js Medium account, developers conclude that by being event-driven and asynchronous, JavaScript is ideal for scaling to millions of concurrent connections, despite its event loop only executing on a single thread <ref type="bibr" target="#b38">[45]</ref>. As another example, while it should be well-known that CPython, the de facto runtime for Python today, uses a global interpreter lock (GIL) that will serialize all concurrent thread executions, Paypal's engineering blog claims that it scales well, and noted that "Dropbox, Disqus, Eventbrite, Reddit, Twilio, Instagram, Yelp, EVE Online, Second Life, and, yes, eBay and PayPal all have Python scaling stories that prove scale is more than just possible: it's a pattern" <ref type="bibr" target="#b0">[1]</ref>.</p><p>Second, no benchmark suite is publicly available today that enables a meaningful comparison between different managed languages (and their implementations). Existing benchmark suites target specific languages or applications. Extending these benchmarks to other languages is often impossible; for example, the DaCapo benchmark for Java contains applications such as Eclipse, a full-featured IDE <ref type="bibr" target="#b43">[50]</ref>. As a result, any comparison on language runtime performance often compares apples to oranges.</p><p>Third, language runtime systems are extremely complex software systems, providing multiple abstractions that all affect performance. For example, a developer must understand the interpreter, possibly multiple JIT compilers (e.g., the OpenJDK JVM contains 4 levels of JIT compilation), a memory management subsystem that performs garbage collect, the behavior of thread libraries, etc.</p><p>Finally, there are no helpful, publicly available profiling tools for understanding the overheads of language runtime systems. The language subsystems themselves expose little profiling information on their internals. For example, while it is widely speculated that dynamic type checking adds significant overhead <ref type="bibr" target="#b37">[44]</ref>, V8/Node.js does not expose any performance counters to report this overhead.</p><p>In this paper, we present an in-depth quantitative performance analysis of four of the popular managed languages with their most widely used runtime systems: CPython, OpenJDK, Node.js with the V8 engine for JavaScript, and the reference Go compiler <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b20">24,</ref><ref type="bibr" target="#b31">36,</ref><ref type="bibr" target="#b33">38]</ref>. We compare their performance characteristics with that of C++ on GCC as the baseline. Our focus is primarily on understanding their differences with respect to speed and scalability. <ref type="foot" target="#foot_0">1</ref> We chose these languages not just because of their popularity, but because they represent different designs along the following three dimensions:</p><p>? Typing. JavaScript and Python are dynamically typed, meaning the runtime must determine the type of objects at run time, whereas others are statically typed. This allows us to understand the performance impact of dynamic type checking.</p><p>? Execution modes. Only Go is ahead-of-time compiled. The other runtimes first interpret bytecode, and compile hot functions Just-In-Time (JIT). The exception is CPython that only has an interpreter and no JIT compiler. This enables us to compare three execution modes: native-only (Go and C++), a combination of interpreter and JIT-compiled native (OpenJDK and V8), and interpreter only (CPython).</p><p>? Concurrency models. V8/Node.js is event driven where event handlers are executed sequentially on a single kernel thread. Similarly, CPython has a global interpreter lock (GIL) so only one thread can execute Python code at a time. Go has its own scheduler and provides user threads as "goroutines." Its scheduler decides how many kernel threads to use for the developer's goroutines. OpenJDK's Thread is simply a kernel thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>The contributions of the paper are as follows:</p><p>Runtime instrumentations. We are the first to make publicly available (as artifacts) instrumentations for the three runtime systems of popular managed languages that are not statically compiled, namely OpenJDK, V8, and CPython. Implementing such instrumentations is challenging given the complexity of these runtimes and the fact that two of them are implemented in assembly and IR. Our instrumentations enable bytecodelevel profiling of (1) the execution overhead of any target bytecode in the interpreter and (2) the dynamic type-checking overhead in Node.js/V8. The profiling information generated, in turn, can be used to guide optimization efforts at the application level and can enable effectual optimizations. The instrumentations are described in ?2.</p><p>Benchmark suite. We are the first to make publicly available (as artifacts) six applications suitable for evaluating managed languages; these applications were used to create twelve benchmarks. The applications, which range from micro-benchmarks to real applications, cover a variety of scenarios, differing in compute intensity, memory usage, I/O intensity, relative startup time, and the degree of available concurrency. In particular, we took care to expose the differences in the three major design dimensions mentioned above. Three of the six applications are parallel, and we parallelize them using both multithreading and multiprocessing where applicable. The benchmark suite is called LangBench and is described in ?3. The source code of our instrumented runtimes and LangBench can be found at https://github.com/topics/langbench. Comparative analysis. We quantitatively analyse the performance of the benchmarks in our suite and identify how the individual runtimes improve or hinder performance relative to the respective C++ implementations. Our objective was to compare the runtimes of the target managed languages in an objective, scientific way. Many of our results are not particularly surprising (even if they contradict some views held in the blogosphere). For example, Go and OpenJDK perform significantly better than V8/Node.js and CPython, with CPython performing worst by far, even when compared against V8 and OpenJDK's interpreter-only execution ( ?6). CPython and V8/Node.js do not benefit from parallelism; in fact, increasing the number of threads systematically decreases performance ( ?7). A major source of V8's relatively poor performance is its dynamic type checking, even when the JS code only uses primitive types that never change ( ?6.1). Perhaps more surprising is the fact that in many cases, the abstractions offered by runtimes can actually lead to speedups over GCC ( ?8). This contradicts the conventional wisdom that abstraction comes at the expense of performance <ref type="bibr" target="#b67">[74]</ref>. Open-JDK outperformed GCC in three of the benchmarks, because the moving garbage collector actually improves cache locality. This leads to the unintuitive behavior that the more frequently GC is performed, the better the overall performance. Go abstracts away the usage of kernel threads, reducing the number of context switches and kernel threads. Finally, abstracting away low-level I/O operations allows runtimes to use optimal I/O system call configurations, outperforming the idiomatic approach in C++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The main limitation of our work is that it does not, and cannot, comprehensively answer every question one might have related to the performance of a language runtime. We only evaluated the runtimes of four languages, and for each language we only evaluated the implementation that is the most widely used. In addition, we only ran our workloads on a single OS/hardware stack. Our findings pertain to our benchmarks, which model real-life applications, but may not be representative of a vast range of applications. Accordingly, our study is not meant to determine the best or most performant programming language for any particular application. Furthermore, our benchmark and analysis do not focus on some performance aspects. Notably, we do not study the overhead of garbage collection when under memory pressure (there is a gap between the working set size of our benchmarks and the maximum heap size setting). There is a large body of prior work focusing on this aspect already <ref type="bibr" target="#b44">[51,</ref><ref type="bibr" target="#b56">63,</ref><ref type="bibr" target="#b72">79]</ref>. Similarly, our benchmark is not meant to measure the various JIT compiler's optimizations, as there are also a large number of existing benchmarks meant to do exactly that <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">32,</ref><ref type="bibr" target="#b46">53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Language Runtime Instrumentation</head><p>In this section we describe how we instrumented the three runtimes: OpenJDK's HotSpot JVM, Node.js/V8, and CPython. Our instrumentations measure two types of information:</p><p>(1) the performance of the execution of any bytecode instruction in the interpreter, and (2) the dynamic type and bounds checking overhead in V8's JIT compiled code. Users can specify a bytecode instruction to measure its overhead, or any JavaScript (JS) function to measure the type and bounds checking overhead when executing that function.</p><p>Why profile interpreter performance? Some have the view that interpreter performance is not important as it mostly affects the startup time, which will be amortized by "warm execution." We do not share this view. While interpreter performance may have been irrelevant over a decade ago when workloads ran in large, long-running monolithic applications that handle all requests <ref type="bibr" target="#b68">[75]</ref>, the paradigm shift to the cloud <ref type="bibr" target="#b58">[65,</ref><ref type="bibr" target="#b62">69,</ref><ref type="bibr" target="#b70">77]</ref> and data analytics <ref type="bibr" target="#b55">[62]</ref> expose the runtime's startup performance as being significant. For example, auto-scaling in the cloud often results in the bringing up of additional instances in the face of a load spike <ref type="bibr" target="#b58">[65,</ref><ref type="bibr" target="#b62">69]</ref>; the problem is also exemplified by short-running instances in Function-as-a-Service platforms <ref type="bibr" target="#b58">[65,</ref><ref type="bibr" target="#b70">77]</ref>. In 2020, the median AWS Lambda invocation ran for only 60 milliseconds <ref type="bibr" target="#b32">[37]</ref>, while startup times for the JVM and V8 are on the order of hundreds of milliseconds or even seconds <ref type="bibr" target="#b55">[62,</ref><ref type="bibr" target="#b58">65,</ref><ref type="bibr" target="#b70">77,</ref><ref type="bibr" target="#b73">80]</ref>. Similarly, data analytics systems face a fundamental tension between parallelizing long running jobs into shorter tasks and the runtime's start-up overhead <ref type="bibr" target="#b55">[62]</ref>.</p><p>In practice, instead of ignoring the performance of the interpreter, implementers spend huge efforts in optimizing the interpreter. For example, OpenJDK has two interpreter implementations: one in C++ and the other entirely in hand-crafted x86 assembly; in one benchmark ( ?6.2), we found that the C++ interpreter to be 1.93x slower than the assembly one, which is perhaps why the C++ interpreter is only used on non-x86 platforms. Similarly, V8's interpreter is written in hand-crafted IR, and IBM's OpenJ9 Java runtime has significant optimizations targetting startup time which is featured as a major advantage over OpenJDK in the cloud <ref type="bibr" target="#b48">[55,</ref><ref type="bibr" target="#b62">69,</ref><ref type="bibr" target="#b68">75]</ref>.</p><p>Bytecode-level profiling can guide optimization efforts and can enable effectual optimizations. For instance, developers can optimize their programs to avoid the use of bytecode instructions with high overheads. Instagram engineers did just this by instrumenting CPython to identify the bytecode instructions with high overheads, and then optimizing their code to avoid using these expensive instructions <ref type="bibr">[22]</ref>. Bytecodelevel profiling also allows us to understand the performance difference between different runtimes. Why profile type and bounds checking? As we will show in ?6.1, dynamic type and bounds checking is a major source of V8's overhead. Similar to bytecode profiling, programmers can optimize their JS programs to avoid such overhead once the source is identified ( ?6.1). Our instrumentation also enables eliminating type and bounds checking overhead entirely for those functions where developers know that they are safe. For instance, say a JS function accesses a[i], the element at index i of array a, and their types never change (known as "monotype"). V8 detects that a and i are monotype, and it speculatively compiles the function: it checks a against the array type (instead of other types) and i against integer, before accessing a[i]. 2 But to ensure safety, it cannot remove the 2 It also performs other checks as described in ?6.1. checks because their types could dynamically change in the future. In that case, the check will fail, forcing the JIT-compiled function to exit and be destroyed, and V8 will re-execute the function in its interpreter before recompiling it.</p><p>By disabling the checking logic in V8's JIT compiler for any user-specified JS function, we effectively create a significantly more efficient, albeit unsafe, version of the function. In the above example, developers could enable this feature to turn off the checks when they know a and i are monotype, so the JIT-compiled code will directly access a[i] by indexing into a without any checks (effectively turning the JS function into a C function). The difference in execution time of applications with and without checked functions can be significant: e.g., in LangBench's sort benchmark, disabling the checking in V8 results in 8x speedup ( ?6.1).</p><p>Note that we only instrumented V8's JIT compiler for identifying type and bounds checking overheads, but not its interpreter. This is because unlike the JIT compiler that independently compiles the different functions, the interpreter's checking logic is applied to all functions equally, leaving us only with the option of either performing checks in all of an application's functions or none of them. The latter option would likely to be too risky to be useful in practice. We further note that the checking overhead in the interpreter also becomes negligible when compared to the other overhead from the interpreter, whereas their proportion become much more significant in JIT-compiled code. Instrumentation Implementation. Conceptually the instrumentations we use to profile bytecode execution in the interpreters are simple. We locate the code block in each interpreter that processes a bytecode instruction, and inject instrumentation around it to collect metrics from the x86 CPU performance counters. In practice, however, adding instrumentation is challenging. One challenge is the complexity of the runtimes: JVM, V8, and CPython consist of approximately 1.2M, 1.0M, and 0.9M lines of code respectively, with little documentation. Instrumenting JVM and V8 is even more challenging as their interpreters are not programmed in a highlevel language (e.g., C++) as the other runtimes are, but are generated dynamically at startup time.</p><p>The HotSpot JVM has two interpreters. Its default interpreter for x86 is written in hand-crafted assembly (known as the "assembly interpreter"). It also has a interpreter written in C++. We instrumented both. Instrumenting the assembly interpreter brings three challenges. First, one needs to locate the code blocks that process the different bytecodes by searching the assembly code. Second, one has to carefully ensure that the instrumented code does not clobber any registers that are used by the interpreter's logic. Finally, HotSpot writes the assembly instructions of the interpreter into memory when it starts up and then jumps to the memory location of the beginning of the interpreter. Hence, we need to use the same mechanism in order to be able to embed our instrumentation logic (written in assembly) into memory.  Figure <ref type="figure" target="#fig_0">1</ref> shows the instruction sequence we inject as part of our instrumentation to obtain the CPU's timestamp counter (tsc). Line 1-3 saves the registers values. <ref type="foot" target="#foot_1">3</ref> The rdtscp instruction saves the higher and lower 32 bits of tsc into EDX and EAX respectively, i.e., the lower 32 bits of RDX and RAX <ref type="bibr" target="#b61">[68]</ref>. It also clears the higher 32 bits of RDX and RAX. Line 5 shifts the higher 32 bits of tsc, stored in EDX, to the higher 32 bits of RDX, and line 6 effectively concatenates the higher and lower 32-bits of tsc, and stores it into RAX. We embed this instruction sequence at both the beginning and the end of the processing of the target bytecode instruction, so that we can measure the latency by computing the difference. <ref type="foot" target="#foot_2">4</ref>Similarly, we use rdpmc to read other performance counters, including those for cycle and instruction counts.</p><p>Instrumenting V8 is even more challenging. V8's interpreter is written in hand-crafted intermediate representation (IR). When the runtime starts up, the interpreter's binary is generated dynamically from this IR by the same JIT compiler used at run time in V8. This required us to instrument both the IR code of the interpreter and the JIT compiler so that native instrumentation code is injected correctly.</p><p>Specifically, for each target bytecode, we had to locate its processing logic in the interpreter's IR and then add a new type of IR node we introduced. We further had to modify the JIT compiler, so that when it encounters this new IR node, it produces the correct assembly instructions that collects the CPU performance counters. This was challenging because there is little documentation describing the internals of V8's interpreter IR or JIT compiler.</p><p>One advantage with JVM and V8 is that developers do not need to recompile the runtime when they wish to profile a different bytecode instruction, but only need to restart the runtime. This is because the interpreters are generated dynamically at startup time. Accordingly, we identify which bytecode instruction is to be instrumented at startup time, generate the appropriate instrumentation code so that it is embedded in the interpreter when written to memory (as with JVM) or generated by the JIT compiler (as with V8).</p><p>Instrumenting CPython is far more straightforward because it is written in C++. However, the CPython runtime will have to be recompiled whenever profiling is to be enabled or the target bytecode instruction that should be instrumented is changed. In theory, one could, before the execution of each bytecode, check whether the bytecode is one of the specified target bytecodes, and conditionally execute the instrumentation, but this would add too much overhead. Instrumentation Overhead. Although our instrumentation could incur noteworthy overhead when enabled on frequently executed bytecode instructions, we only used them to measure a specific bytecode instruction, instead of end-to-end runtime. We only count the number of instructions inside of the measurement instructions, not including our instrumented instructions. This is possible as we control the exact assembly instructions generated, and we verify said assembly using objdump, gdb, and outputting the JIT-compiled assembly. <ref type="foot" target="#foot_3">5</ref>However, our cycle measurements could be skewed by the measuring instructions limiting the processor's out-of-order execution and pipelining capabilities. This is a limitation, and it is extremely difficult to accurately measure this overhead due to the fact that the very act of measuring the cycle count disrupts pipelining (similar to the observer effect in physics).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LangBench</head><p>Our goal is to compare realistic applications across different languages. We cannot reuse existing benchmarks as they target specific languages, and extending these benchmarks to other languages is infeasible. For example, porting the DaCapo benchmark requires us to implement Eclipse, a fullfeatured IDE, in four other languages <ref type="bibr" target="#b43">[50]</ref>. Therefore, we chose to build 6 applications from the ground up to cover a variety of workloads. We implemented these applications in each of the 5 languages: C++, Go, Java, JavaScript, and Python. From the six applications, we created twelve benchmarks by varying degrees of concurrency, and exploring alternative implementations of the applications.</p><p>We made a best-effort attempt at covering a variety of different types of workloads. Our applications range from micro-benchmarks to real world applications, and they stress three major resource usage categories in different ways, being one or more CPU-intensive, memory bound, and I/O bound. Additionally, we implemented parallel versions of the applications where applicable. They also vary from short running to long running ones. The applications and their categories are shown in Table <ref type="table" target="#tab_1">1</ref>.</p><p>It was important to ensure that the applications were implemented in a similar and fair manner in each of the five languages. The design of every implementation of an application is conceptually identical: they use the same algorithms and control flow. Each application is relatively small, so that it could be built in all languages using the same algorithms and data structures. This kept the complexity of the application similar across the languages. Yet, we fully rewrote the applications in each language, providing our best effort to make the code idiomatic. We referred to official language documentation (e.g. <ref type="bibr" target="#b6">[7]</ref>). In certain cases we also implemented different versions of the code. For example, in the JavaScript Sudoku implementation we re-wrote the code multiple times to change the storage of the arrays (see ?6.1 for details). For Python and JavaScript we also tried versions that create parallelism (e.g. multiprocessing in Python) and versions that only provide concurrency (e.g. threading in Python). In general, as we analysed each bottleneck, we also tried to find any more performant implementations.</p><p>The six applications are: Sudoku Solver. We implemented an exhaustive search sudoku solver, borrowing from the Spec CPU 2017 benchmark <ref type="bibr" target="#b26">[30]</ref>. The algorithm recursively labels all empty cells. At each cell, it verifies the grid state, using the next digit for the cell if verification fails. If all digits are exhausted for a cell, it backtracks to the previous cell. String Sorting. We implemented the in-place merge sort algorithm described by Katajainen et al. <ref type="bibr" target="#b52">[59]</ref> and use it to sort strings. First, we permute every possible string of length 6 with 18 possible letters, creating 18 6 strings. These strings are stored in an array, which are then sorted. Graph Coloring. Graph coloring labels each vertex in a graph, such that no two vertices with an edge between them have the same label. We implemented the algorithm presented by <ref type="bibr">Wigderson [78]</ref> which uses a bounded number of colors with run time complexity polynomial in the number of vertices, edges, and the chromatic number. The benchmark colors the YouTube social network and ground-truth communities graph from the Stanford Large Network Dataset Collection <ref type="bibr" target="#b53">[60]</ref>. We implemented both a recursive and an iterative version of the algorithm. Key-Value Store. We implemented an in-memory key-value store based on the general architecture of Redis <ref type="bibr" target="#b24">[28]</ref>. We stress the server with Redis' packaged benchmark by running a SET test followed by a GET test. Each test makes 2 million requests, randomly selecting a key from a space of 500 thousand keys, using a value size of 64 bytes. The Redis benchmark opens a configured number of client connections to the key-value store. Each connection performs an equal number of requests and is treated as a unit of concurrency (such as a thread). The clients are run on a machine separate from the one running the key-value store.</p><p>Log Analysis. We implemented the algorithm of CLP that parses logs by separating highly repetitive static text from variable values, and stores them in two different indexes <ref type="bibr" target="#b66">[73]</ref>. Logs are queried, returning the matching log messages by searching the index and raw log. We separate the searching into two separate tests. "Regex" searches the raw logs using regular expressions, whereas "Indexed" searches using indexes. Both tests can be run with parallelism, where the files to be searched are partitioned equally. We process 7000 log files totalling 1.21 GB on disk with an average size of 181 KB. The logs were generated by running various jobs from HiBench <ref type="bibr" target="#b49">[56,</ref><ref type="bibr" target="#b50">57]</ref>. Each test first indexes the logs and then performs 7 queries.</p><p>File Server. We implemented an HTTP server that serves static files from a directory. A single C++ client is always used that spawns a configured number of threads; each thread connects to the server and requests an equal partition of 1000 real log files with an average size of 16.8 MB. The server implementations handle these connections concurrently, treating each connection as a unit of concurrency. The client and server run on different machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>We ran our experiments on two in-house servers, each having 2 Xeon E5-2630V3, 16 virtual cores, 2.4 GHz CPUs, 256 GB DDR4 RAM and two 7200 RPM hard drives. They are running Linux 4.15.0 and connected by a 10 Gbps interconnect. For C++ programs we used GCC 9.3.0 compiling with -O3 against the C++17 standard. For OpenJDK 13 <ref type="bibr" target="#b14">[17]</ref>, CPython 3.8.1 <ref type="bibr" target="#b21">[25]</ref>, and Go 1.14.1 <ref type="bibr" target="#b5">[6]</ref>, we used the reference implementations for each respective language. We use Node.js 13.12.0 [16] which uses V8 7.9.317. <ref type="bibr">25 [41]</ref>.</p><p>We ran each benchmark 5 times and report the average. The key-value store, log parser, and file server benchmarks were run with the number of both client and worker threads ranging from 1 to 1024. For OpenJDK and V8 the minimum amount of memory was set by determining the first heap configuration that did not cause a crash; for Go, GOGC was set to 5%. We then continuously increased the heap settings until performance no longer improved. We used the results from the first setting (i.e., the smallest heap size) that resulted in optimal performance. For the log parser and file server benchmarks, the used log files were stored on a distributed file system with a replication factor of 2. We cleared Linux's page cache before running each benchmark. Relative completion times for various language implementations normalized to optimized code under GCC. Note the logarithmic scale of the y axis. "LA" refers to the log analysis application. The numbers at the bottom shows the benchmark's absolute execution time in the C++ implementation. For benchmarks with concurrency, the "Best" bars are annotated with the thread count that results in best completion time.</p><p>For key-value store and file server it is the number of client threads, not the number of threads used server side. For GCC and OpenJDK, the server creates 1 (kernel) thread to handle each client thread, so the number of server-side threads is the same as the client. For both Node.js and CPython, their best completion time in key-value store is achieved when using a single server-side thread (due to their scalability characterstic described in ?7). As for the file server benchmark, both Node.js and CPython's best performance is achieved when using 64 server-side threads ( ?7). The number of server-side threads in Go is automatically determined by the runtime as described in ?8.2. The number of threads for log analysis is the number of worker threads (as there is no client).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Overview of Results</head><p>Figure <ref type="figure">2</ref> shows the run times for the benchmarks in Lang-Bench. Unsurprisingly, optimized GCC was the fastest on average, with Go and OpenJDK close behind, being 1.30x and 1.43x slower than GCC. Impressively, Go and Open-JDK outperform optimized GCC for 3 out of the 12 benchmarks. V8/Node.js and CPython performed the worst with run times 8.01x and 29.50x slower than GCC. At the extreme, CPython was 129.66x slower than GCC (for the sort benchmark). V8/Node.js and CPython were competitive with GCC only when the workload is bottlenecked by disk I/O, i.e., in the file server benchmark.</p><p>We also found that V8/Node.js and CPython are limited with respect to achievable parallelism. Their design serializes the threads' computation, and requires expensive serialization for different threads (V8) or processes (CPython) to communicate. This leads to the unintuitive result that adding additional threads actually slows down parallel applications as more serialization is required. In fact, for both the key-value store and parallel log analysis benchmark, the best performance is achieved using only a single thread. In contrast, both Go and OpenJDK scale to multicores. Go achieves a 1.02x speedup over GCC in the multithreaded key-value store benchmark, despite being slower in the single threaded version.</p><p>In the subsequent sections, we use our instrumented runtimes to provide detailed analyses that explain these results. Specifically, for each runtime, we analyze (at minimum) the two worst performing benchmarks, considering both singlethreaded ( ?6) and multi-threaded ( ?7) variants for those with parallelism. We further analyzed every case where the runtime outperforms GCC ( ?8).  6 Runtime Overhead (Single-thread)</p><p>This section investigates the source of runtime overheads on the LangBench single-threaded applications that performed poorly. Specifically, we found (1) type and bounds checking ( ?6.1) is the bottleneck for V8 in its slowest benchmarks (Sudoku and Sort); (2) interpreter performance ( ?6.2) is the major cause of CPython's overhead -despite lacking a JIT compiler, its interpreter performs much worse compared to OpenJDK and V8; (3) GC write barrier ( ?6.3) is the bottleneck in the Sort benchmark for both OpenJDK and Go, which is their slowest workload, even when heap usage is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Type and Bounds Checking Overhead</head><p>We found that type checking and bounds checking made up 41.83% and 87.43% of V8's execution time in the default su-  <ref type="figure" target="#fig_2">3</ref>. We measured the resulting execution time, and compare it against the default execution time with all checks. We also show the execution time when all checks are removed.</p><p>doku and sort benchmarks, which are the two single-threaded benchmarks where V8 showed the worst performance compared to GCC. Note that V8 has other sources of overhead, such as execution being partially interpreted, when compared with GCC. For the numbers in this sub-section, we compare against the default execution time when the runtime binary is in Linux's page cache, unlike the results in Figure <ref type="figure">2</ref>, where we clear the page cache before each test. Next we zoom into the Sudoku benchmark to explain this overhead, and how we can leverage our bytecode profiling information to optimize our JavaScript code. For V8/Node.js, Sudoku spends 93% of its time primarily comparing 2D array elements of the sudoku board. The majority of this time is spent performing 11 type and bound checks for each 2D array access, as shown in Figure <ref type="figure" target="#fig_2">3</ref>. Each dimension requires 5 checks, and the 11th check is used for the final value. Table <ref type="table" target="#tab_3">2</ref> shows the overhead for these checks.</p><p>The first check ensures that board is an object pointer, by checking for a tagged bit to distinguish between an object pointer and a primitive integer value. (V8 stores both object pointers and primitive integers in a 8 byte word, so that integers can be stored inline instead of being allocated on the heap.) Second, V8 must similarly check that x is an integer, rather than an object. Omitting these checks made it 8.1% faster (shown in Table <ref type="table" target="#tab_3">2</ref>) -removing them is safe, as we know that no incorrect type will be used.</p><p>After V8 confirms that board is an object, it checks that the internal type of board, called a shape, is an array. Fourth, V8 performs a bounds check for the access to board[x]. Finally, V8 checks if the value accessed is a "hole". In JavaScript, arrays may be sparse, meaning not every index has a value. Indexes without values are called holes, which V8 must convert to undefined if accessed. The same checks must be repeated to access the second dimension of board. To use board[x][y], a last check is necessary to verify it is an integer. Profiling enabled optimization. Initially, we preallocated the fix-size sudoku board. In V8, preallocated arrays are created sparse as their values are uninitialized, requiring the hole checks. Even though the array was filled with integers before being used, sparse arrays never lose their status. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bytecode</head><p>Insn  We implemented an optimized version which would create arrays without holes, known as "packed" arrays. This optimized version was 1.48x faster (and is what is shown in Fig. <ref type="figure">2</ref>). Our optimized sudoku benchmark for V8/Node.js starts with an empty array, then appends 9 Int8Arrays to create the 2D sudoku board. This allows V8 to recognize that there are no holes. Using the built in Int8Array, preallocation initialized it with the default value of 0, rather than a hole.</p><p>Unfortunately, these optimizations cannot be applied universally. First, it presents a trade-off that can only be determined via profiling: while sparse arrays require hole checking, building a large packed array requires many internal resizing operations to grow the array. In addition, typed arrays such as Int8Array only exist for certain integral types. For example, it is not possible to preallocate a packed array of strings or any user defined type. GCC, Go, and OpenJDK. Compared to GCC, Go and Open-JDK must also perform similar bounds checks. However, they avoid the type checking as they are statically typed.</p><p>OpenJDK successfully lifts all loop-invariant computations to outside the loop. In the code of Figure <ref type="figure" target="#fig_3">4</ref>, Open-JDK determines that the maximum value of i used to access board[x][i] is 8, and checks if the length of board[x] is greater than 8 outside the loop. Go performs the bounds check in each iteration. Further, 2D arrays in OpenJDK contain pointers to 1D arrays, which may be null. However, Open-JDK has an optimization which eliminates null checks, and instead catches them using a signal handler for SIGSEGV. On the other hand, Go does not need to perform null checks as its 2D arrays are laid out contiguously in memory like in C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Interpreter Overhead</head><p>CPython is slower than the other runtimes because it lacks a JIT compiler and so programs are strictly interpreted. There-fore we further compare the three runtimes by running sudoku on each of them only in interpreter mode. OpenJDK's (assembly) interpreter outperforms both V8 and CPython by 2.59x and 5.34x respectively. This is because static typing allows OpenJDK to avoid the type checks that V8 and CPython must perform. OpenJDK has dedicated bytecodes for accessing different types of arrays (aaload for an array of arrays, iaload for an integer array). In contrast, V8 and CPython both have a single bytecode (LdaKeyedProperty and BINARY_SUBSCR, respectively) which must accommodate for any array or dictionary type. Table <ref type="table" target="#tab_5">3</ref> shows the performance profiling results of different bytecode executions, using our instrumentations.</p><p>CPython is still 2.07x slower than V8, even though both of them do dynamic typechecking. As shown in Table <ref type="table" target="#tab_5">3</ref>, CPython uses 138 instructions and 41.8 cycles to execute each byte code instruction (BINARY_SUBSCR), whereas Node.js only spends 90 instructions/26.3 cycles to process each byte code instruction (LdaKeyedProperty). This is due to the optimizations of V8's interpreter: it is hand-crafted in IR, whereas CPython is implemented in C. Similarly, we found that Open-JDK's assembly interpreter is 1.93x faster than the one implemented in C++.</p><p>We found the hand-crafted interpreter implementation made a few notable optimzations. First, it aggressively inlined functions. The CPython bytecode we inspected ended up containing around 5-6 function calls in the common execution path. The equivalent bytecode in Node.js had no call instructions, similar to when all functions are completely inlined. This further enables more aggressive optimization. For example, error handling logic that would be functions in C code can now be grouped together at the end. This leaves the instructions in the non-error paths tightly together and improves cache performance. In addition, developers have a better understanding than the compiler on what the common path is, so that they can manually group the basic blocks that are commonly executed together (and move error handling logic to the end).</p><p>Theoretically GCC could also perform the same level of inlining, and developers can manually use goto statements to move all error handling logic to after the common-path logic. However, performing aggressive inlining unselectively could hurt performance (increased function size, register pressures, etc.), and excessive use of goto could hurt the readability, reliability, and maintainability of the software.</p><p>Performance Sensitivity on Interpreters. We observe that an interpreter may amplify the performance overhead caused by small code changes that would only incur negligible overhead in compiled execution. Under CPython, the iterative version of graph coloring ran 1.66x slower than the recursive version, in contrast to the other interpreters. The function performing the iterative algorithm contained 1.54x more bytecodes than the recursive function, resulting in 22% more instructions recorded by perf. In contrast, for GCC, switching from recursive to iterative adds only 4% more instructions.</p><p>Iterative versions of recursive functions are commonly necessary to avoid stack overflows. Instead of a recursive function call, the iterative function appends the call arguments to an array, and later pops the arguments off the array to perform another iteration of the algorithm. In addition, the iterative algorithm must check if the array is empty at each iteration of the loop. These seemingly simple operations significantly increase the bytecode count and execution time. JIT compilers mitigate these extra operations through optimizations such as reducing the number of redundant checks.</p><p>Startup Overhead for OpenJDK and V8. OpenJDK and V8 spend 843ms and 788ms, respectively, in startup in the Sudoku benchmark. Startup is the primary reason for Open-JDK being slower than GCC when running Sudoku, as it is the shortest benchmark. Specifically, 708ms is spent loading the JVM's large binary from disk, while the rest (135ms) is spent in classloading and interpreter execution. In comparison, OpenJDK's warm execution time is only 868ms (when we run the Sudoku benchmark in a JVM that has already been warmed up by running the same benchmark multiple times), whereas GCC's warm execution time is 611ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">GC Write Barriers</head><p>We were surprised to see that under OpenJDK's default GC setting, it was 10.03x slower than GCC for the Sort benchmark. Sort is also the benchmark where Go performs the worst relative to GCC: 2.14x slower. The source of the slowdown for both OpenJDK and Go is the cost of GC write barriers. This cost occurs despite GC hardly ever running in Sort, as write barriers are necessary to maintain data structures needed to perform GC. Interestingly, for OpenJDK, using the nondefault Parallel GC algorithm drops the slowdown to only 2.07x (shown in Figure <ref type="figure">2</ref>), as it contains fewer instructions. Go's write barrier contains even fewer instructions, and is slightly faster than OpenJDK with Parallel GC.</p><p>Write barriers bring a constant cost to pointer writes regardless of how often GC is actually performed. For our in-place merge sort, swapping two elements is the primary source of write barriers. This requires two write barriers, one for each element being written. OpenJDK's default GC algorithm, G1 <ref type="bibr" target="#b10">[12]</ref>, adds 44 instructions for these write barriers, completely dwarfing the 6 instructions required to swap the elements and 5 for bounds checking. On the other hand, Parallel GC's write barriers only use 5 instructions, 8.8x fewer than G1.</p><p>Both Go and G1 require a write barrier to ensure every live object is captured when they perform marking concurrently with application threads. Furthermore, both G1 and Parallel GC in OpenJDK divide the heap into regions and move live objects across regions to compact the heap. For both, write barriers are used to maintain remembered sets, which are used to find and update pointers to moved objects. However, G1 performs more checks during its write barrier to avoid unnecessary updates to the remembered sets. This avoids work when using the remembered sets to update pointers, and helps minimize pause time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Scalability Limitations</head><p>We found that CPython and Node.js limit the degree of parallelism achievable. We first briefly introduce the background of the Node.js and CPython concurrency model and then describe our findings. Background. Node.js is event driven; by default, it uses a single Node.js thread to drive an event loop and process all incoming events. If the processing of an event blocks (e.g., on I/O), the underlying kernel thread will block, and Node.js's event loop continues with another kernel thread to process the next event. In other words, multiple threads can be blocked at the same time, but CPU execution is serialized. While Node.js supports running multiple Node.js threads (known as worker threads), each runs its own event loop. Hence worker threads do not share the heap (to avoid data races); data sharing requires message passing with data being serialized.</p><p>In essence, CPython's concurrency model is the same as that of Node.js where multiple kernel threads can block on I/O at the same time, except that it is the programmer's job to create the threads; the threads share the same heap. In CPython's case, CPU computation is serialized by the Global Interpreter Lock (GIL) so that only one thread can use the CPU at a time. CPython also supports multiprocessing, forking different processes to avoid the GIL. However, data sharing and communication requires serialization. Node.js and CPython's scalability on LangBench. We can now explain the scalability patterns of Node.js and CPython. We ran three parallel benchmarks, namely log analysis, keyvalue store, and file server, under different configurations, including different number of threads, as well as parallelizing them with multiple processes in CPython. In log analysis and key-value store, the best performance is achieved using a single CPython or Node.js thread, whereas the other runtimes are able to improve performance by adding more threads.</p><p>These two benchmarks, namely log analysis and key-value store, are bottlenecked by CPU or memory accesses, instead of blocking I/O. Therefore, creating multiple threads offers no advantage in Node.js and CPython as their executions are serialized. In the case of Node.js, performance degrades significantly when creating additional worker threads due to the serialization overhead. On indexed search log analysis, Node.js's performance drops 4.7x when we use more than one worker thread. In this benchmark, multiple workers communicate frequently as they share the same dictionaries. Similarly, serialization overhead slows down CPython when we switch to multiprocess, resulting in a 4.9x slowdown on the same benchmark. While multiple CPython threads share the heap, they still introduce thread management overhead compared to using a single thread.</p><p>Specifically, in key-value store, CPython can only scale to one client thread (adding additional concurrent client threads will worsen the completion time). In comparison, Node.js/V8 scales up to 96 client threads, even though it only uses 1 Node.js event-loop thread at server side. However, its completion time can not keep improving with more client threads, whereas it still can under GCC, Go, and OpenJDK. Note that the improvement plateaus when the client thread count increases to 160. Even though GCC achieved its best completion time on 512 client threads, the improvement over 160 threads is negligible. This is why in Figure <ref type="figure">2</ref>, the difference in best completion times is small between GCC, Go, and Open-JDK, even though they are achieved on 512, 256, and 160 client threads, respectively.</p><p>In comparison, Node.js and CPython scale well on the file server benchmark. This benchmark is I/O parallel: there is little communication between different threads, and they are bottlenecked by disk I/O. Creating multiple threads (or processes in CPython) thus improves the performance (when there are concurrent client connections).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Runtime Advantages</head><p>We found that the high-level abstractions provided by the runtimes can, in some cases, result in better performance and scalability. This is counter-intuitive given the conventional wisdom that abstractions generally come at the expense of performance <ref type="bibr" target="#b67">[74]</ref>. We discuss three findings: (1) object relocations in OpenJDK's moving GC can result in better cache locality; (2) Go's scheduler automatically maps user threads to kernel threads, and hence abstracts away the direct usage of kernel threads, reducing the number of context switches and the number of kernel threads used; (3) abstracting away the low-level I/O operations allows runtimes to use the optimal I/O system call configurations. <ref type="foot" target="#foot_4">6</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">GC Improved Cache Locality</head><p>OpenJDK's moving garbage collector can significantly improve cache locality, resulting in speedups in three benchmarks: single threaded key-value store and both iterative and recursive implementations graph coloring. In particular, Open-JDK was much faster than GCC at the single threaded keyvalue store, with 1.46x speedup. This is the largest speedup any runtime had over GCC.</p><p>Key-value store. We found that the source of cache locality was from iterating over linked lists. Our key-value store implements a hashtable with separate chaining, meaning hash The key-value store before and after a GC pause. White boxes logically represent Java objects, and the shaded boxes represent the objects' location in the JVM heap. A 'B' denotes a bucket mapped to by the hash function, and an 'N' denotes a node in the bucket's linked list. The number of the node represents the order they are inserted into the hashtable. The memory for the nodes of the bucket begins scattered, but after GC relocation is ordered by the traversal of the bucket's linked lists. OpenJDK uses bump pointer allocation. Therefore, nodes in the hashtable are laid out sequentially in memory based on their insertion order. Figure <ref type="figure" target="#fig_4">5</ref> (a) shows how the nodes of a bucket would be initially laid out in memory. There is little locality, as adjacent nodes of the same linked list are scattered. Therefore, whenever there is a lookup, insertion, or a deletion of a key in the linked list, the traversal of the linked list is expensive due to poor locality.</p><p>However, OpenJDK's moving GC reorders the objects in memory. It scans for all live objects that are reachable from the GC roots (e.g., objects on the stack) by following the pointers, copying them to a different memory region, before freeing the old region. For the linked list, this means that the objects will be allocated adjacently, in the same order as in the linked list, as shown in Figure <ref type="figure" target="#fig_4">5 (b)</ref>.</p><p>In comparison, GCC uses a size segregated allocator (malloc). Since nodes have the same size, they will be placed in the same region, resulting in a similar pattern as with bump pointer allocation, with nodes laid out in insertion order. When profiling the iteration, we found that GCC actually executed fewer instructions than OpenJDK, but was still slower. In the tight loop iteration, the bucket GCC took only 5 assembly instructions compared to OpenJDK's 11.</p><p>This behavior presents the unintuitive case where the more frequently GC is performed, the better the performance. Figure <ref type="figure" target="#fig_5">6</ref> shows that with more frequent GC cycles, objects are re-ordered in memory more often, leading to improved performance. We control the frequency of GC by using different heap sizes. The larger the heap, the fewer GC cycles. When it is 128 GB, performance is the worst because GC is never triggered; objects are never moved, so there is no locality.</p><p>To verify that cache locality was the source of the performance gap, we modified OpenJDK to expose a method to print the virtual address a reference points to. We do this as GC obfuscates perf cache hit rates, making them impractical to compare. In one run where no GC was performed, over 99% of the distances between nodes of the linked lists were different, with a median distance of 724 KB. A run with GC was 1.86x faster; 57% of nodes were 88 bytes apart, and 41% were 192 bytes apart. Although the size of a cache line on our processors is 64 bytes, so two nodes would not be in the same cache line, it is likely that they are in adjacent cache lines, opening the opportunity for prefetching.</p><p>Graph coloring. We found OpenJDK outperformed GCC (by 1.37x) on graph coloring, when the C++ program uses the standard library. Our investigation showed that GC had a similar effect as for the key-value benchmark given that graph coloring also uses a hash table. Both hash table implementations on OpenJDK (HashMap and HashSet) and C++'s standard libraries (std::unordered_set and std::unordered_map) use an open hashing design; i.e., it uses separate chaining to connect the elements in a linked list upon collision. As a result, both GCC and OpenJDK suffer from poor locality initially. However, OpenJDK quickly gains locality through GC, as with the key-value store benchmark. <ref type="foot" target="#foot_5">7</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Scalability in Go</head><p>In the multithreaded key-value store implementation, Go has a 1.02x speedup compared to GCC, despite being 1.16x slower than GCC in the single threaded version. Go outperforms GCC by avoiding 2.2 million context switches through the use of asynchronous networking I/O and significantly fewer kernel threads. With GCC, network I/O is performed using synchronous system calls, blocking the kernel thread, resulting in a context switch. When goroutines perform I/O, the work is offloaded to an internal goroutine which uses asynchronous system calls. A goroutine performing I/O is blocked by Go's scheduler, but the underlying kernel thread is not blocked; instead, Go schedules another goroutine on the same kernel thread. As a result, Go only uses at most 42 kernel threads, regardless of the number of concurrent client threads. (The number of kernel threads is automatically chosen by the Go runtime depending on the workload's characteristic.)</p><p>We verified that context switching causes the majority of the 600 ms gap between the fastest multithreaded Go and GCC execution times. Using LEBench <ref type="bibr" target="#b65">[72]</ref>, we measured the average cost of a context switch on our machine to be 5.84 ?s. With 32 cores, perf reports approximately 70K context switches per core, which adds up to 409ms of overhead, making up the majority of the 600ms performance gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">I/O System Calls in the File Server</head><p>To read a file in the file server benchmark in C++, we initially used the more general, idiomatic approach which uses iterators. This results in repeated fixed size read system calls. Unlike C++, all the managed runtimes abstract away the lowlevel system call interfaces when performing I/O, so that they can transparently issue system calls in an optimal way, by first calling fstat to get the file size, followed by a single read for its entire contents. All runtimes use this approach when reading a file. So any developer using the runtimes will benefit from the optimizations without any burden of knowledge. In comparison, we have to manually optimize our C++ implementation to switch to fstat and read, leading to a 2x speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Related Work</head><p>Ours is the first performance study to analyze and compare the implementations of multiple widely used runtimes, and provide the necessary instrumentations to do so. There are existing benchmarks to evaluate software performance, but they focus on novel benchmark methodologies. Marr et al. designed a benchmark suite with the goal of having a methodology for evenly comparing a common subset of language abstractions <ref type="bibr" target="#b57">[64]</ref>. They limit their applications to a minimal set of primitive operations and exclude built-in data structures such as hashtables to ensure that no language has an advantage. Rather than strictly stressing the compiler on specific primitive operations, we evaluated all aspects of a runtime on how they affect performance under different scenarios using idiomatic code. Both DaCapo <ref type="bibr" target="#b43">[50]</ref> and Renaissance [70] created benchmark suites consisting only of Java applications for various workloads. TailBench created a statistically sound methodology for measuring latency-critical applications in C++ and Java <ref type="bibr" target="#b51">[58]</ref>. SPEC [31] and the Computer Language Benchmarks Game <ref type="bibr" target="#b2">[3]</ref> provide a variety of benchmarks, covering many languages, but present no analysis. In contrast, our work focuses on understanding and providing an explanation for the technical details of language implementations that cause performance differences.</p><p>Other studies of languages have had different scopes, focusing tightly on a specific language or aspect. By utilizing the Rosetta Code <ref type="bibr" target="#b25">[29]</ref> repository, Nanz et al. present statistical findings, such as the fact that scripting languages are more concise than procedural languages <ref type="bibr" target="#b60">[67]</ref>. Nanz et al. further studied the usability and performance of Chapel, Cilk, and Go in multicore workloads <ref type="bibr" target="#b59">[66]</ref>. Prokopski et al. study interpreter code-copying optimizations in the SableVM, OCaml, and Yarv interpreters <ref type="bibr" target="#b64">[71]</ref>. Wade et al. quantify the impact of profile data on JIT compiled code quality in the HotSpot VM.</p><p>Lion et al. instrumented the JVM to measure startup times (i.e., the total time spent in class loading and interpreter) <ref type="bibr" target="#b55">[62]</ref>. However, they did not provide fine-grained instrumentation to profile the execution of each bytecode instruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Concluding Remarks</head><p>We presented an in-depth performance analysis of runtimes under a variety of scenarios. We implemented LangBench, a benchmark suite that enables an objective comparison of language implementation performance. Our runtime instrumentations facilitate understanding why a runtime performs well or poorly. We demonstrated that our instrumentations provide valuable profiling information that enables optimizations. We have open-sourced our instrumentations and LangBench so that practitioners can use and enhance them to analyze and optimize their applications. benchmarks, taking 0.90x and 0.93x less time for the single and multithreaded versions, respectively. Further, V8 has the same performance as GCC on the single-threaded regular expression based log analysis. In both cases, the good performance comes from the library implementation of pointer copying (in the case of Go on indexed log analysis) and the regular expression engine (in the case of V8 on regular expression based log search).</p><p>Surprisingly, whereas Go spends a total of 0.04 seconds in a critical section in indexed log search, GCC takes 2.49 seconds. In the log analysis benchmarks, each worker thread returns a list of matched log messages to the main thread by appending a thread local list of results to the main thread's global list, while holding a lock. Inside this critical section, for the append operation, Go copies more pointers per loop iteration than GCC.</p><p>In Go, appending to a list (referred to as a slice) is done efficiently because slices are an intrinsic type and appending is performed by a builtin function, which uses hand written assembly. Both GCC and Go use 128-bit wide XMM registers <ref type="bibr" target="#b8">[9]</ref> to move two 64-bit pointers at a time with a single mov instruction. The assembly in Go unrolls the loop as much as possible, using all 16 XMM registers to move 32 64-bit pointers per iteration. Furthermore, rather than checking if a write barrier is required for each pointer, Go checks once if write barriers are required for all pointers, as they do not need to be performed when concurrent marking is not active <ref type="foot" target="#foot_6">8</ref> .</p><p>For the same operation in C++ with std::vector, GCC only moves 2 64-bit pointers in a single XMM register per iteration. Therefore, for every 2 pointers (or single XMM register move) GCC must also execute a compare and jump instruction to iterate the loop. On the other hand, Go will only execute these two loop iteration instructions every 32 pointers (16 XMM register moves). Furthermore, because we use std::unique_ptr, GCC must set the pointers in the thread local vector to NULL, as ownership has been transferred to the global vector. GCC stores NULL to 2 pointers each iteration of the loop using another XMM register.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The sequence of assembly instructions inlined into the processing of each bytecode instruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Checks required to access board[x][y] in V8/Node.js.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>/Figure 4 :</head><label>4</label><figDesc>Figure 4: Code showing where Go and OpenJDK perform array bounds checking when accessing board[x][i] in a loop.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5:The key-value store before and after a GC pause. White boxes logically represent Java objects, and the shaded boxes represent the objects' location in the JVM heap. A 'B' denotes a bucket mapped to by the hash function, and an 'N' denotes a node in the bucket's linked list. The number of the node represents the order they are inserted into the hashtable. The memory for the nodes of the bucket begins scattered, but after GC relocation is ordered by the traversal of the bucket's linked lists.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The OpenJDK single threaded key-value store benchmark run with increasing heap sizes, corresponding to fewer GC cycles.collisions are added to a bucket by appending the key-value (KV) pair to a list. This is shown as the white boxes in Figure5. For example, N2, N3, and N6 are different KV pairs hashed to the same bucket B1.OpenJDK uses bump pointer allocation. Therefore, nodes in the hashtable are laid out sequentially in memory based on their insertion order. Figure5(a) shows how the nodes of a bucket would be initially laid out in memory. There is little locality, as adjacent nodes of the same linked list are scattered. Therefore, whenever there is a lookup, insertion, or a deletion of a key in the linked list, the traversal of the linked list is expensive due to poor locality.However, OpenJDK's moving GC reorders the objects in memory. It scans for all live objects that are reachable from the GC roots (e.g., objects on the stack) by following the pointers, copying them to a different memory region, before freeing the old region. For the linked list, this means that the objects will be allocated adjacently, in the same order as in the linked list, as shown in Figure5 (b).In comparison, GCC uses a size segregated allocator (malloc). Since nodes have the same size, they will be placed in the same region, resulting in a similar pattern as with bump pointer allocation, with nodes laid out in insertion order. When profiling the iteration, we found that GCC actually executed fewer instructions than OpenJDK, but was still slower. In the tight loop iteration, the bucket GCC took only 5 assembly instructions compared to OpenJDK's 11.This behavior presents the unintuitive case where the more</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,-9.00,-10.01,630.00,255.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="1,-9.00,543.00,630.00,259.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>into EDX and EAX registers 5 shlq rdx,32 ; shift tsc's higher 32 bits up in rdx 6 orq rax,rdx ; or onto rax 7 movq dst ,rax ; output to a scratch register dst</figDesc><table><row><cell>1 push rax</cell></row><row><cell>2 push rcx</cell></row><row><cell>3 push rdx</cell></row><row><cell>4 rdtscp ; saves tsc 8 pop rdx</cell></row><row><cell>9 pop rcx</cell></row><row><cell>10 pop rax</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The applications and the component(s) they stress.</figDesc><table><row><cell>Application</cell><cell>CPU Memory I/O Parallel</cell></row><row><cell>Sudoku solver</cell><cell></cell></row><row><cell>String sorting</cell><cell></cell></row><row><cell>Graph coloring</cell><cell></cell></row><row><cell>Key-Value store</cell><cell></cell></row><row><cell>Log analysis</cell><cell></cell></row><row><cell>File server</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>We modified V8's JIT compiler and removed each of the checks performed for a 2D array access to board[x][y] shown in Figure</figDesc><table><row><cell>Code Version</cell><cell>Time (s)</cell><cell>Overhead of Checks (%)</cell></row><row><cell>Default</cell><cell>2.369</cell><cell>-</cell></row><row><cell>1-2 Remove Obj./Int Checks</cell><cell>2.177</cell><cell>8.105</cell></row><row><cell>3 Remove Shape Check</cell><cell>2.219</cell><cell>6.332</cell></row><row><cell>4 Remove Bounds Check</cell><cell>2.154</cell><cell>9.076</cell></row><row><cell>5 Remove Hole Check</cell><cell>2.051</cell><cell>13.423</cell></row><row><cell>1-5 Remove All Checks</cell><cell>1.378</cell><cell>41.832</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Statistics for array access bytecodes (BC) performed by various interpreters for the sudoku benchmark.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>An analysis of resource usage is left to the Appendix, because the findings have largely already been established by prior studies<ref type="bibr" target="#b56">[63]</ref>, and we did not need to implement any additional profiling mechanisms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>We have to manually save the registers because we directly inject code into the assembly code, in contrast to injecting assembly code into C where the __asm__ block saves the registers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Intel allows tsc to be synchronized across multicore<ref type="bibr" target="#b41">[48]</ref>, and Linux enables this synchronization<ref type="bibr" target="#b11">[13]</ref>. This ensures a meaningful counter value even if the interpreter thread is migrated to another core during the processing of a bytecode instruction. In reality, however, migration is rare given the processing of bytecode instruction typically only takes tens of cycles.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>There is no overhead for removing the type and bounds checking in V8 as the compiler only removes instructions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>There are a few more cases where runtimes demonstrated better performance than GCC; they are related to the implementation of libraries. We discuss them in detail in the Appendix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>We optimized our C++ benchmark by switching to hashtable implementations from Google's Abseil library<ref type="bibr" target="#b1">[2]</ref>, which uses a closed hashing implementation that achieves better locality.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>Write barriers are explained in more detail in Section 6.3.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the anonymous reviewers and the shepherd for their insightful comments. This research was supported by the <rs type="funder">Canada Research Chair fund</rs>, an <rs type="funder">NSERC</rs> <rs type="grantName">Discovery grant</rs>, and a VMware gift.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gWrTuVS">
					<orgName type="grant-name">Discovery grant</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We discuss two additional results in the Appendix: (1) memory usage analysis of different runtimes on LangBench, and (2) other speedups from the runtimes over GCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resource Usage: Memory</head><p>Figure <ref type="figure">7</ref> shows the peak memory usage of the different runtimes. Compared to Figure <ref type="figure">2</ref>, it also shows the completion time under the minimum memory usage configuration (e.g., the heap size setting in OpenJDK) of each benchmark. Recall that, for OpenJDK and V8, the minimum amount of memory was set by determining the first heap configuration that did not cause a crash; for Go, GOGC was set to 5%. We then </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Peak Memory Usage</head><p>Figure <ref type="figure">7</ref>: Relative completion time and peak memory usage for various language implementations as a multiplicative factor compared to optimized code under GCC. Each benchmark uses the most optimized version for that language implementation. continuously increased the heap settings until performance no longer improved. Peak memory was measured using the reported maximum RSS from getrusage.</p><p>The figure shows that all language implementations use at least 2x more memory than GCC. The more complex runtimes are the worst offenders with V8/Node.js and OpenJDK using 3.70x and 3.38x more memory than GCC on average. Go and CPython use less memory, but still use 2.12x and 2.08x more memory than GCC on average. (As an exception, Go surprisingly manages to use 0.59x less memory than our idiomatic C++ version of the sudoku benchmark.) It is crucial for these runtimes to trade off increased memory usage and performance. Optimal performance can require increased memory usage, which prevents jobs from being scheduled when datacenters allocate resources to fit peak usage. This additional memory is also rarely returned to the OS causing reduced memory utilization.</p><p>For both OpenJDK and V8/Node.js, the two runtimes that require the most memory to achieve optimal performance, their worst case was the sudoku benchmark with OpenJDK using 10.94x more memory than GCC. The sort benchmark has the lowest memory usage with GCC only requiring 3.42MB. However, OpenJDK and V8/Node.js also had benchmarks that did not have any memory overhead when compared to GCC. Both runtimes used the same amount of memory as GCC for the key-value store benchmark, despite it being the next smallest benchmark with GCC requiring only 33.43MB. CPython's peak memory usage was the closest to that of GCC, but requiring 2.12x more memory on average. It also had the lowest worse case, requiring 4.06x more than GCC for the sort benchmark. Despite being more memory efficient than the other runtimes in most of the benchmarks, CPython still used more memory than any other runtimes in the sort benchmark. Go was also able to use less memory than CPython for the sudoku and graph colouring benchmarks.</p><p>In fact, Go was the only language implementation able to use noticeably less memory than GCC, using 0.59x less in the sudoku benchmark. Upon inspection this stemmed from our version of the benchmark using the C++ standard library. The complete C++ version peaks at 3.46MB of RSS, but almost all of this memory is allocated immediately upon running the program. We found that using a C++ implementation that did not use iostreams but instead used open and read reduced the memory usage to 2.72MB. However, the largest improvement was from not linking the C++ standard library by removing the -lstdc++ flag when compiling. This dropped the usage to 1.33MB and well under Go's 2.05MB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runtime Speedup from Library Implementations</head><p>In addition to the cases discussed in ?8, there are other cases where managed runtimes performed better than GCC. Go performed better than GCC on the indexed search log analysis</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://medium.com/paypal-engineering/10-myths-of-enterprise-python-8302" />
		<title level="m">10 Myths of Enterprise Python</title>
		<imprint>
			<date>8f21f82</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Abseil</surname></persName>
		</author>
		<ptr target="https://abseil.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="https://benchmarksgame-team.pages.debian.net/benchmarksgame/" />
		<title level="m">The Computer Language Benchmarks Game</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A distributed, reliable key-value store for the most critical data of a distributed system</title>
		<ptr target="https://etcd.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="https://blog.twitch.tv/en/2019/04/10/go-memory-ballast-how-i-learnt-to-stop-worrying-and-love-the" />
		<title level="m">Go memory ballast: How I learnt to stop worrying and love the heap</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<ptr target="https://golang.org/" />
	</analytic>
	<monogr>
		<title level="j">The Go Programming Language</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<ptr target="https://go.dev/doc/" />
		<title level="m">Go Programming Language Documentation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<ptr target="https://golang.org/ref/spec" />
	</analytic>
	<monogr>
		<title level="j">Go Programming Language Specification</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="https://software.intel.com/content/www/us/en/develop/articles/introduction-to-intel-advanced-vector-extensions.html" />
		<title level="m">Introduction to Intel Advanced Vector Extensions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Java</surname></persName>
		</author>
		<ptr target="https://github.com/OpenLiberty/sample.daytrader8" />
		<title level="m">DayTrader Benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Make G1 the Default Garbage Collector</title>
		<ptr target="http://openjdk.java.net/jeps/248" />
	</analytic>
	<monogr>
		<title level="j">JEP</title>
		<imprint>
			<biblScope unit="volume">248</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<ptr target="https://github.com/torvalds/linux/blob/df" />
		<title level="m">Check tsc synchronization</title>
		<imprint>
			<biblScope unit="volume">53202</biblScope>
		</imprint>
	</monogr>
	<note>Linux source tsc_sync. cc57e057f18e44dac8e6c18aba47ab</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<ptr target="https://eng.uber.com/m3/" />
		<title level="m">M3: Uber&apos;s Open Source, Large-scale Metrics Platform for Prometheus</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="https://github.com/oprogramador/github-languages" />
		<title level="m">Most popular languages on GitHub</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<ptr target="https://openjdk.java.net/projects/jdk/13/" />
		<title level="m">OpenJDK 13</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Openstack</forename><surname>Overview</surname></persName>
		</author>
		<ptr target="https://www.openstack.org/software/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<ptr target="https://coralogix.com/log-analytics-blog/optimizing-a-golang-service-to-reduce-over-40-cpu/" />
		<title level="m">Optimizing a Golang service to reduce over 40% CPU</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Our journey to type checking 4 million lines of Python</title>
		<ptr target="https://blogs.dropbox.com/tech/2019/09/our-journey-to-type-checking-4-million-lines-of-python/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Production-Grade Container Orchestration -Kubernetes</title>
		<ptr target="https://kubernetes.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<ptr target="https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html" />
		<title level="m">Project Tungsten: Bringing Apache Spark Closer to Bare Metal</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<ptr target="http://pypl.github.io/PYPL.html" />
		<title level="m">PYPL PopularitY of Programming Language</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="https://wiki.python.org/moin/PythonImplementations" />
		<title level="m">Python Implementations -Python Wiki</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><surname>Quora</surname></persName>
		</author>
		<ptr target="https://www.quora.com/In-what-cases-is-Java-faster-if-at-all-than-C" />
		<title level="m">what cases is Java faster than C</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><surname>Quora</surname></persName>
		</author>
		<ptr target="https://www.quora.com/In-what-cases-is-Java-slower-than-C-by-a-big-margin" />
		<title level="m">what cases is Java slower than C by a big margin</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><surname>Redis</surname></persName>
		</author>
		<ptr target="https://redis.io" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Rosetta</forename><surname>Code</surname></persName>
		</author>
		<ptr target="https://rosettacode.org/wiki/Rosetta_Code" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><surname>Spec Cpu</surname></persName>
		</author>
		<ptr target="https://www.spec.org/cpu2017/Docs/#benchmarks" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<ptr target="https://www.spec.org/jbb" />
		<title level="m">SPECjbb 2015 Benchmark</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<ptr target="https://stackoverflow.com/questions/14205096/c11-regex-slower-than-python" />
		<title level="m">Stack Overflow: C++11 regex slower than python</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Why do std::string operations perform poorly?</title>
		<author>
			<persName><forename type="first">Stack</forename><surname>Overflow</surname></persName>
		</author>
		<ptr target="https://stackoverflow.com/questions/8310039/why-do-stdstring-operations-perform-poorly" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Why is python faster than c++ in this case?</title>
		<author>
			<persName><forename type="first">Stack</forename><surname>Overflow</surname></persName>
		</author>
		<ptr target="https://stackoverflow.com/questions/24895881/why-is-python-faster-than-c-in-this-case" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The State of Developer Ecosystem</title>
		<ptr target="https://www.jetbrains.com/lp/devecosystem-2019/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<ptr target="https://www.datadoghq.com/state-of-serverless/" />
		<title level="m">The State of Serverless</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<ptr target="https://octoverse.github.com" />
		<title level="m">The State of the Octoverse</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<ptr target="https://itnext.io/using-transducers-to-speed-up-javascript-arrays-92677d000096" />
		<title level="m">Transducers Speed Up JavaScript Arrays</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<ptr target="https://www.infoq.com/articles/twitter-java-use/" />
		<title level="m">Twitter Shifting More Code to JVM, Citing Performance and Encapsulation As Primary Drivers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<ptr target="https://blog.discord.com/why-discord-is-switching-from-go-to-rust" />
		<title level="m">Why Discord is switching from Go to Rust</title>
		<imprint>
			<date>190bbca2b1f</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Why is Dynamic Type Checking Expensive?</title>
		<ptr target="https://stackoverflow.com/questions/41622341/why-is-type-checking-expensive" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Why the Hell Would You Use Node</title>
		<ptr target="https://medium.com/the-node-js-collection/why-the-hell-would-you-use-node-js-4" />
		<imprint/>
	</monogr>
	<note>b053b94ab8e</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<ptr target="https://getstream.io/blog/switched-python-go/#reason-performance" />
		<title level="m">Why we switched from Python to Go</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Python is Slow, and I Don&apos;t Care</title>
		<author>
			<persName><surname>Yes</surname></persName>
		</author>
		<ptr target="https://medium.com/pyslackers/yes-python-is-slow-and-i-dont-care" />
		<imprint>
			<date>13763980b5a1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Intel? 64 and IA-32 architectures software developer&apos;s manual</title>
		<ptr target="https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-vol-3b-part-2-manual.pdf" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
	<note>: System programming guide</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Bladerunner: Stream processing at scale for a live view of backend data mutations at the edge</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laney</forename><surname>Kuenzel Zamore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahid</forename><surname>Jazayeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Erlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Savor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stumm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th ACM Symp. on Operating Principles (SOSP&apos;21)</title>
		<meeting>28th ACM Symp. on Operating Principles (SOSP&apos;21)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021-10">October 2021</date>
			<biblScope unit="page" from="708" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The dacapo benchmarks: Java benchmarking development and analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asjad</forename><forename type="middle">M</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Khang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rotem</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amer</forename><surname>Bentzur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Diwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">Z</forename><surname>Frampton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Guyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antony</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Hosking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jump</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">B</forename><surname>Eliot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aashish</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darko</forename><surname>Phansalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Stefanovi?</surname></persName>
		</author>
		<author>
			<persName><surname>Vandrunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Daniel Von Dincklage</surname></persName>
		</author>
		<author>
			<persName><surname>Wiedermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Conf. on Object-oriented Programming Systems, Languages, and Applications (OOP-SLA&apos;06)</title>
		<meeting>21st Conf. on Object-oriented Programming Systems, Languages, and Applications (OOP-SLA&apos;06)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="169" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic vertical memory scalability for OpenJDK cloud applications</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulo</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Synytsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetiana</forename><surname>Fydorenchyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Symp. on Memory Management (ISMM&apos;18)</title>
		<meeting>Intl. Symp. on Memory Management (ISMM&apos;18)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The benefits and costs of writing a POSIX kernel in a high-level language</title>
		<author>
			<persName><forename type="first">Cody</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Frans</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Symp. on Operating Systems Design and Implementation (OSDI&apos;18)</title>
		<meeting>13th Symp. on Operating Systems Design and Implementation (OSDI&apos;18)</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-10">October 2018</date>
			<biblScope unit="page" from="89" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Statistically rigorous Java performance evaluation</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Georges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dries</forename><surname>Buytaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications (OOPSLA&apos;07)</title>
		<meeting>22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications (OOPSLA&apos;07)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="57" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hadoop</surname></persName>
		</author>
		<ptr target="https://hadoop.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<ptr target="https://www.linkedin.com/pulse/comparing-hotspot-openj9-handra-/" />
	</analytic>
	<monogr>
		<title level="j">Handra. Comparing Hotspot and OpenJ</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The HiBench benchmark suite: Characterization of the MapReduce-based data analysis</title>
		<author>
			<persName><forename type="first">Shengsheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinquan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Frontiers in Information and Software as Services</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="209" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">HiBench: A representative and comprehensive Hadoop benchmark suite</title>
		<author>
			<persName><forename type="first">Shengsheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinquan</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDE Workshops, ICDEW &apos;16</title>
		<meeting>ICDE Workshops, ICDEW &apos;16</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Tailbench: a benchmark suite and evaluation methodology for latency-critical applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl. Symp. on Workload Characterization (IISWC&apos;16)</title>
		<meeting>IEEE Intl. Symp. on Workload Characterization (IISWC&apos;16)</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016-09">Sep. 2016</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Practical in-place mergesort</title>
		<author>
			<persName><forename type="first">Jyrki</forename><surname>Katajainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomi</forename><surname>Pasanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jukka</forename><surname>Teuhola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nordic J. of Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="40" />
			<date type="published" when="1996-03">March 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Krevl</surname></persName>
		</author>
		<ptr target="http://snap.stanford.edu/data" />
		<title level="m">SNAP Datasets: Stanford Large Network Dataset Collection</title>
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Lindholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Yellin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Bracha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://docs.oracle.com/javase/specs/jvms/se13/html/" />
		<title level="m">The Java?Virtual Machine Specification -Java SE 13 Edition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Don&apos;t get caught in the cold, warm-up your JVM: Understand and eliminate JVM warm-up overhead in data-parallel systems</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Grcevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Symp. on Operating Systems Design and Implementation (OSDI&apos;16)</title>
		<meeting>12th Symp. on Operating Systems Design and Implementation (OSDI&apos;16)</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016-11">November 2016</date>
			<biblScope unit="page" from="383" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">M3: Endto-end memory management in elastic system software stacks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th European Conf. on Computer Systems (EUROSYS&apos;21)</title>
		<meeting>16th European Conf. on Computer Systems (EUROSYS&apos;21)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="507" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Cross-language compiler benchmarking: Are we fast yet?</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Daloze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanspeter</forename><surname>M?ssenb?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Symp. on Dynamic Languages (DLS&apos;16)</title>
		<meeting>12th Symp. on Dynamic Languages (DLS&apos;16)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="120" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Improving cloud function cold ctart time</title>
		<author>
			<persName><forename type="first">Colt</forename><surname>Mcanlis</surname></persName>
		</author>
		<ptr target="https://medium.com/@duhroach/improving-cloud-function-cold-start" />
	</analytic>
	<monogr>
		<title level="m">Google Cloud Performance Atlas</title>
		<imprint>
			<biblScope unit="page" from="5700" to="5706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Benchmarking usability and performance of multicore languages</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S D</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Symp. on Empirical Software Engineering and Measurement</title>
		<meeting>Intl. Symp. on Empirical Software Engineering and Measurement</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A comparative study of programming languages in Rosetta code</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><forename type="middle">A</forename><surname>Furia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 37th Intl. Conf. on Software Engineering (ICSE&apos;15)</title>
		<meeting>37th Intl. Conf. on Software Engineering (ICSE&apos;15)</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="778" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">How to benchmark code execution times on Intel IA-32 and IA-64 instruction set architectures</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Paoloni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Intel Coporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Marius</forename><surname>Pirvu</surname></persName>
		</author>
		<ptr target="https://developer.ibm.com/articles/optimize-jvm-startup-with-eclipse-openjj9/" />
		<title level="m">Optimize JVM start-up with Eclipse OpenJ9</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Renaissance: Benchmarking suite for parallel applications on the JVM</title>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Leopoldseder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Duboscq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>T?ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubom?r</forename><surname>Studener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yudi</forename><surname>Bulej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Villaz?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>W?rthinger</surname></persName>
		</author>
		<author>
			<persName><surname>Binder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 40th Conf. on Programming Language Design and Implementation (PLDI&apos;19)</title>
		<meeting>40th Conf. on Programming Language Design and Implementation (PLDI&apos;19)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="31" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Analyzing the performance of code-copying virtual machines</title>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">B</forename><surname>Prokopski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Verbrugge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Conf. on Object-Oriented Programming Systems Languages and Applications (OOPSLA&apos;08)</title>
		<meeting>23rd Conf. on Object-Oriented Programming Systems Languages and Applications (OOPSLA&apos;08)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="403" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">An analysis of performance evolution of Linux&apos;s core operations</title>
		<author>
			<persName><forename type="first">(</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirk</forename><surname>Jenny) Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luyuan</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camilo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Stumm</surname></persName>
		</author>
		<author>
			<persName><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th ACM Symp. on Operating Systems Principles (SOSP&apos;19)</title>
		<meeting>27th ACM Symp. on Operating Systems Principles (SOSP&apos;19)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="554" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">CLP: Efficient and scalable search on compressed text logs</title>
		<author>
			<persName><forename type="first">Kirk</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Symp. on Operating Systems Design and Implementation (OSDI&apos;21)</title>
		<meeting>15th Symp. on Operating Systems Design and Implementation (OSDI&apos;21)</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2021-07">July 2021</date>
			<biblScope unit="page" from="183" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">End-to-end Arguments in System Design</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Saltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="277" to="288" />
			<date type="published" when="1984-11">November 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Hang</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Pirvu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobi</forename><surname>Ajila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Sundaresan</surname></persName>
		</author>
		<ptr target="https://blog.openj9.org/2021/06/15/innovations-for-java-running-in-containers/" />
		<title level="m">Innovations for Java running in containers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<ptr target="http://spark.apache.org" />
		<title level="m">Spark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Replayable execution optimized for page sharing for a managed runtime environment</title>
		<author>
			<persName><forename type="first">Kai-Ting Amy</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rayson</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th European Conf. on Computer Systems (EUROSYS&apos;19)</title>
		<meeting>14th European Conf. on Computer Systems (EUROSYS&apos;19)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Improving the performance guarantee for approximate graph coloring</title>
		<author>
			<persName><forename type="first">Avi</forename><surname>Wigderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="729" to="735" />
			<date type="published" when="1983-10">October 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">CRAMM: Virtual memory support for garbage-collected applications</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emery</forename><forename type="middle">D</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">F</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Eliot</surname></persName>
		</author>
		<author>
			<persName><surname>Moss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Symp. on Operating Systems Design and Implementation (OSDI&apos;06)</title>
		<meeting>7th Symp. on Operating Systems Design and Implementation (OSDI&apos;06)</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="103" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Characterizing serverless platforms with Serverlessbench</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyu</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenggang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th ACM Symp. on Cloud Computing (SOCC&apos;20)</title>
		<meeting>11th ACM Symp. on Cloud Computing (SOCC&apos;20)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="30" to="44" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
