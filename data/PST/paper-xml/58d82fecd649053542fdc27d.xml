<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Domain Adaptation with Label and Structural Consistency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cheng-An</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yao-Hung</forename><forename type="middle">Hubert</forename><surname>Tsai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi-Ren</forename><surname>Yeh</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yu-Chiang</forename><forename type="middle">Frank</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Domain Adaptation with Label and Structural Consistency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8A2C24066F392FEDCAC0422AE2783DB6</idno>
					<idno type="DOI">10.1109/TIP.2016.2609820</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2609820, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Domain adaptation</term>
					<term>unsupervised learning</term>
					<term>structure discovery</term>
					<term>label propagation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation deals with scenarios in which labeled data are available in the source domain, but only unlabeled data can be observed in the target domain. Since the classifiers trained by source-domain data would not be expected to generalize well in the target domain, how to transfer the label information from source to target-domain data is a challenging task. A common technique for unsupervised domain adaptation is to match cross-domain data distributions, so that the domain and distribution differences can be suppressed. In this paper, we propose to utilize the label information inferred from the source domain, while the structural information of the unlabeled target-domain data will be jointly exploited for adaptation purposes. Our proposed model not only reduces the distribution mismatch between domains, improved recognition of target-domain data can be achieved simultaneously. In the experiments, we will show that our approach performs favorably against state-of-the-art unsupervised domain adaptation methods on benchmark datasets. We will also provide convergence, sensitivity, and robustness analysis, which support the use of our model for cross-domain classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>I N many real-world classification tasks, one cannot expect that the test data would always exhibit the same or similar distributions as the training data do. The distribution mismatch between training and test data typically comes from the fact that such data are collected from different domains (e.g., videos captured by cameras at different views, images taken by cameras with different resolutions, etc.) <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b26">[27]</ref>. As a result, classifiers learned from training data cannot be expected to properly recognize test data.</p><p>To address the aforementioned problems, researchers advance the idea of domain adaptation and aim at associating cross-domain data for recognition purposes. If the difference between source and target domains can be eliminated, test data observed in the target domain can be recognized by sourcedomain training data accordingly. Thus, domain adaptation and its applications have been widely exploited in computer vision <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b11">[12]</ref> and machine learning <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b23">[24]</ref> communities.</p><p>C.-A. Hou is with the Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213 (e-mail: c.a.andyhou@gmail.com).</p><p>Y.-H. H. Tsai is with the Research Center for Information Technology Innovation, Academia Sinica, Taipei 11529, Taiwan (e-mail: y.h.huberttsai@gmail.com).</p><p>Y.-R. Yeh is with the Department of Mathematics, National Kaohsiung Normal University, Kaohsiung 824, Taiwan (e-mail: yryeh@nknu.edu.tw).</p><p>Y.-C. F. Wang is with the Research Center for Information Technology Innovation, Academia Sinica, Taipei 11529, Taiwan (e-mail: ycwang@ citi.sinica.edu.tw).</p><p>Depending on the availability of labeled data in the target domain, domain adaptation approaches can be generally divided into two different categories. For semi-supervised domain adaptation <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b4">[5]</ref>, one can collect source-domain labeled data in advance, but only a small amount of labeled data can be observed in the target domain. Given such crossdomain data and label information, the task is to recognize the remaining target-domain data. On the other hand, unsupervised domain adaptation <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[22]</ref> (UDA) deals with totally unlabeled target-domain data, with only labeled data available in the source domain. In this paper, we focus on unsupervised domain adaptation.</p><p>Among existing domain adaptation methods, the most common strategy is to derive feature representations for reducing the domain differences <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b11">[12]</ref>, so that recognition can be performed in the resulting feature spaces. While some advocated the adaptation of marginal distributions across data domains <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b32">[33]</ref>, several works have been proposed to further adapt both marginal and conditional distributions for improved performance <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b21">[22]</ref>. It is worth noting that, however, adaptation of conditional distributions is not trivial for unsupervised domain adaptation problems. This is because that, only unlabeled data can be observed in the target domain. Therefore, how to properly transfer the source-domain label information to the target domain for associating cross-domain data becomes a challenging task.</p><p>As noted above, we particularly address the unsupervised domain adaptation problem in this paper. As illustrated in Figure <ref type="figure" target="#fig_2">1</ref>, we propose to exploit the structural information of target-domain data, together with the label information transferred from the source domain for performing domain adaptation. While we share similar goals with <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b2">[3]</ref> to adapt classification models from source to target domains, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b2">[3]</ref> utilize only information observed from the source do-main to learn their classifiers, followed by the selection of crossdomain domain data for performing adaption. Our proposed method aims at identifying representative target-domain data, and use the associated pseudo labels for performing joint clas-sification and adaptation. Based on Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b13">[14]</ref>, we utilize the above information and approach domain adaptation by solving a label-propagation based optimization task, aiming at matching cross-domain marginal and conditional feature distributions. As verified in our experiments, the proposed method not only exhibits improved domain adaptation ability, it also outperforms several state-of-the-art unsupervised domain adaptation approaches in terms of cross-domain visual classification performance.</p><p>We now summarize the contributions of this paper:  II. RELATED WORKS Semi-supervised domain adaptation: To associate source and target domain data during the learning process, semisupervised domain adaptation allows the users to collect either a small amount of target-domain labeled data <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b30">[31]</ref> or a number of cross-domain instance pairs <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b16">[17]</ref>. For example, Shekhar et al. <ref type="bibr" target="#b30">[31]</ref> addressed cross-domain image classification problems by constructing a common domainadaptive dictionary through the labeled data in the target domain. By exploiting cross-domain data correspondence information, Huang and Wang <ref type="bibr" target="#b16">[17]</ref> proposed to learn a coupled dictionary for relating source and target domains for classification and synthesis purposes.</p><p>With the use of cross-domain label or correspondence information, adaptation problems with large domain differences (e.g., cross-pose face recognition <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b28">[29]</ref>) or those dealing with distinct features across domains (e.g., image-to-text classification <ref type="bibr" target="#b37">[38]</ref>) can be possibly solved. Nevertheless, for semi-supervised domain adaptation tasks, the performance would be highly dependent upon the amount of label/correspondence information available.</p><p>Unsupervised domain adaptation: Many real-world classification problems deal with training and test data collected from different domains. Since it is often expensive to collect labeled data in the target domain or cross-domain data pairs for training purposes, this results in the challenging task of unsupervised domain adaptation. For unsupervised domain adaptation, the users are able to collect labeled data in the source domain, but only unlabeled test data can be observed in the target domain (and no cross-domain instance pair is available during training either) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>Due to the absence of target-domain label information, an underlying observation for unsupervised domain adaptation is that the source and target domains have different data distributions but still exhibit relatedness <ref type="bibr" target="#b1">[2]</ref>. As a result, with the goal to eliminate the bias across different domains <ref type="bibr" target="#b34">[35]</ref>, recent works advance Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b13">[14]</ref> to match cross-domain data distributions <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Nevertheless, modeling data distributions across domains is not a trivial task. Previously, researchers chose to model and match cross-domain marginal distributions and assume the conditional distributions are the same across domains (i.e., covariate shift <ref type="bibr" target="#b31">[32]</ref>). For example, Huang et al. <ref type="bibr" target="#b17">[18]</ref> proposed kernel mean matching by weighting source-domain data, so that the mean difference between cross-domain data can be minimized. Pan et al. <ref type="bibr" target="#b23">[24]</ref> proposed Transfer Component Analysis (TCA) to determine a low-dimensional embedding for cross-domain data, so that matching of cross-domain data can be performed accordingly. Based on TCA, Long et al. <ref type="bibr" target="#b22">[23]</ref> further presented Transfer Feature Matching (TJM), which combines instance reweighting and distribution adaptation techniques for improved adaptation.</p><p>However, adapting marginal distributions only might not be sufficient to associate cross-domain data for classification purposes. To address this issue, Long et al. <ref type="bibr" target="#b21">[22]</ref> proposed Joint Distribution Adaptation (JDA) to match both marginal and conditional data distributions in the derived common feature space. Since no label information can be observed in the target domain, they applied the prediction outputs of source-domain classifiers as the pseudo labels of target-domain data, while the classifiers were updated during their adaptation process.</p><p>Since the direct use of such pseudo labels might not be preferable due to possible domain mismatch, we not only transfer label information but further exploit target-domain structural information during the adaptation process. Inspired by recent semi-supervised learning techniques presented in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b39">[40]</ref>, we approach the original problem of unsupervised  <ref type="bibr" target="#b35">[36]</ref> and Deep Adaptation Networks (DAN) <ref type="bibr" target="#b20">[21]</ref> were both proposed for solving domain adaptation problems. With the goal of eliminating domain differences, DDC <ref type="bibr" target="#b35">[36]</ref> applies AlexNet models for handling source and target-domain data (one for each domain), while a single layer is introduced for regularizing the similarity between their last fully connected layers. Sharing similar ideas, DAN <ref type="bibr" target="#b20">[21]</ref> chooses to regularize each fully connected layer for matching cross-domain data. Later in our experiments, we will compare our proposed method with these two state-of-the-art deep learning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motivation</head><p>We start from the problem definition, and introduce the notations which will be used in the following of this paper. Let D S = {(x s 1 , y s 1 ), . . . , (x s M , y s M )} = {X S , y S }, where X S ∈ R d×M represents M d-dimensional data in the source domain, and each entry in y S ∈ R M ×1 indicates the corresponding label (from 1 to C). On the other hand, we have N unlabeled instances observed in the target domain (with the same feature dimension), i.e., D T = {x t n } N n=1 = X T ∈ R d×N . Thus, we determine the cross-domain data matrix as X = [X S , X T ] ∈ R d×(M +N ) . By assuming that both source and target domains contain data of the same C classes of interest, the goal of our work is to predict the label vector y T ∈ R N ×1 for classification purposes, while each element in y T is the assigned class label for the corresponding instance in the target domain.</p><p>In this paper, we perform transfer feature learning for unsupervised domain adaptation (i.e., only labeled and unlabeled data are available in source and target domains, respectively). We not only eliminate domain differences for associating cross-domain data, we also need to leverage label information from source to target domains for recognition purposes. To address the above issues, we propose and integrate two components highlighted below, which will be detailed in Sections III-B and III-C, respectively: i) Adaptation of joint feature distributions. Let P S (X S ) and P T (X T ) as marginal distributions of data in source and target domains, respectively, and we have P S (y S |X S ) and P T (y T |X T ) as the corresponding conditional distributions. As noted in <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b21">[22]</ref>, we typically have P S (X S ) = P T (X T ) and P S (y S |X S ) = P T (y T |X T ). Thus, our goal is to match both cross-domain marginal and conditional distributions, so that recognition of target-domain data can be performed accordingly. Minimizing the differences between cross-domain marginal and conditional distributions effectively matches the joint distribution of P (X, y). Following JDA <ref type="bibr" target="#b21">[22]</ref>, we do not approximate and align P (y) due to the assumption of equal prior probabilities for the UDA problem of interest. To be more precise (and as seen in our experiments), we consider the same label numbers across domains, with equal numbers of instances to be observed across different categories.</p><p>ii) Exploitation of cross-domain data with label and structural consistency. We advance the technique of label propagation <ref type="bibr" target="#b39">[40]</ref> for domain adaptation. More specifically, we utilize label information inferred from the source domain and observe the target-domain data structure for performing adaptation. This allows us to tackle the unsupervised domain adaptation problem with improved recognition of targetdomain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Distribution Adaptation</head><p>As highlighted in Section III-A, the primary goal of this work is to match both marginal and conditional feature distributions of cross-domain data, so that data in the target domain can be classified accordingly. However, as noted in <ref type="bibr" target="#b21">[22]</ref>, since the modeling of conditional distributions P (y S |X S ) and P (y T |X T ) is not explicitly applicable, an alternative way is to observe and adapt class-conditional distributions P (X S |y S ) and P (X T |y T ) based on their sufficient statistics.</p><p>In our work, we aim at determining a feature transformation Φ for cross-domain data, so that both P S (Φ(X S )) ≈ P T (Φ(X T )) and P S (Φ(X S )|y S ) ≈ P T (Φ(X T )|y T ) can be satisfied. For simplicity, we apply empirical criteria of MMD <ref type="bibr" target="#b13">[14]</ref> for adapting the above distribution. To be more precise, we need to minimize the difference between feature distributions calculated by the distance between data means in a reproducing kernel Hilbert space (RKHS):</p><formula xml:id="formula_0">Dist(PS(XS), PT (XT )) + Dist(PS(XS|yS), PT (XT |yT )) = 1 M M i=1 φ(x s i ) - 1 N N j=1 φ(x t j ) 2 H + C c=1 1 |D (c) S | x s i ∈D (c) S φ(x s i ) - 1 | D(c) T | x t i ∈ D(c) T φ(x t i ) 2 H ,<label>(1)</label></formula><p>where Dist measures the distance between feature distributions, and φ is the feature transformation induced by universal kernels. In (1), we have D For unsupervised domain adaptation, a major challenge is to transfer the label information from source to target domains when reducing domain biases. Later in Section III-C, we will explain how we determine the label information for targetdomain data, so that the adaptation of the above conditional distributions can be achieved.</p><p>By applying the kernel tricks, we can rewrite (1) as tr(KL)</p><formula xml:id="formula_1">+ C c=1 tr(KL c ), where K ∈ R (M +N )×(M +N )</formula><p>indicates the kernel matrix of data matrix X, and</p><formula xml:id="formula_2">L ij = 1 M 2 , xi, xj∈DS 1 N 2 , xi, xj∈DT -1 M N , otherwise.</formula><p>1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2609820, IEEE Transactions on Image Processing</p><formula xml:id="formula_3">(L c ) ij =                1 |D (c) S | 2 , xi, xj∈D (c) S 1 | D(c) T | 2 , xi, xj∈ D(c) T -1 |D (c) S || D(c) T | , xi∈D (c) S , xj∈ D(c) T xi∈ D(c) T , xj∈D (c) S 0, otherwise.</formula><p>As noted in <ref type="bibr" target="#b23">[24]</ref>, the above optimization problem with respect to K requires high computational costs. In our work, we utilize the empirical kernel map <ref type="bibr" target="#b27">[28]</ref> as suggested in <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b21">[22]</ref> to rewrite the K into (KK -1/2 )(K -1/2 K). Then, we derive a lower-dimensional space in terms of K by determining the projection matrices W and W (both of size (M + N ) × k, and k &lt;&lt; d). Now, the kernel matrix can be written as</p><formula xml:id="formula_4">(KK -1/2 W )(W K -1/2 K) = KWW K, where W = K -1/2 W .</formula><p>As a result, by replacing the original K with KWW K, the original objective function of (1) turns into:</p><formula xml:id="formula_5">min W tr(W KLK W) + C c=1 tr(W KLcK W) + λ W 2 F s.t. W KHK W = I.<label>(2)</label></formula><p>It can be seen that, the first two terms in (2) are associated with the adaptation of marginal and conditional distributions, respectively. The sum of the these two terms corresponds to the MMD distance between the cross-domain data. The third term in (2) regularizes the projection W, weighted by parameter λ. The centering matrix H in the constraint of ( <ref type="formula" target="#formula_5">2</ref>) is defined as H = I -1 M +N 1, in which 1 is the matrix of ones. As noted in <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b21">[22]</ref>, adding this constraint would preserve the data variance after adaptation, which implies and introduces additional data discriminating ability into the learned model W. By applying Lagrange techniques, we can rewrite the objective function of (2) into the following Lagrangian function:</p><formula xml:id="formula_6">L(W, Ψ) ≡ tr(W (KLK + K C c=1 LcK + λI)W) + tr((I -W KHK W)Ψ),<label>(3)</label></formula><p>where Ψ is a diagonal matrix with Lagrange Multipliers (i.e., Ψ = diag(ψ 1 , ..., ψ k )∈R k×k ). By setting the derivative of (3) with respective to W equal to zero, we approach the original optimization problem by solving the following generalized eigen-decomposition problem:</p><formula xml:id="formula_7">(KLK + K C c=1 LcK + λI)W = KHK WΨ.<label>(4)</label></formula><p>Taking the k-smallest eigenvectors from (4) would satisfy <ref type="bibr" target="#b1">(2)</ref>, which determines the optimal solution of W (recall that k &lt;&lt; d). Once W is obtained, we project crossdomain data into the resulting k-dimensional latent space i.e., Z = W K = [Z S , Z T ]∈R k×(M +N ) , where Z S and Z T represent the transformed data projected from source and target domains, respectively. In other words, the data matrix Z can be viewed as adapted cross-domain data with matched marginal and conditional distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Exploiting Label and Structural Consistency for Unsupervised Domain Adaptation</head><p>Due to the lack of label information in the target domain, matching of cross-domain conditional distributions is a challenging task. Without proper prediction of class labels for the target-domain data DT , adapting the conditional distributions for cross-domain data cannot be achieved.</p><p>To solve the above problem, we propose to take the knowledge which is exploited across domains into the adaptation process, with the goal of suppressing domain biases with cross-domain recognition guarantees. To begin with, we apply SVM-based classifiers <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b36">[37]</ref> for estimating the class posterior probability of the transformed target-domain data Z T . For the setting of unsupervised domain adaptation, these SVM classifiers are trained by labeled source-domain data in the transformed feature space (i.e., Z S ). Based on the estimated posterior probabilities, we construct an uncertain label matrix Y∈R N ×C for target-domain instances, in which each entry is defined as:</p><formula xml:id="formula_8">Y ij =      2p(y t i = j|z t i ) -1, p(y t i = j|z t i ) &gt; δ p(y t i = j|z t i ) = max c p(y t i = c|z t i ) -1, otherwise.<label>(5)</label></formula><p>It is worth noting that, the posterior probability p(y t i = j|z t i ) indicates how likely the projected target-domain instance z t i belongs to class j. Obviously, we have -1</p><p>Y ij 1, and a larger Y ij value implies that the instance of interest is of the corresponding class. The parameter δ controls the number of uncertain labels to be transferred from the source domain (i.e., the aggressiveness of label propagation). For simplicity, we set δ equal to the lower quantile (i.e., 25%) of the maximum posterior probabilities observed from each targetdomain instance. In other words, 75% of target-domain data will be assigned the predicted uncertain labels for adaptation purposes. Later in the experiments, we will provide additional remarks on our choice of δ.</p><p>Once the above uncertain label matrix is constructed, we effectively set a semi-supervised setting for the target-domain data. However, unlike standard semi-supervised learning problems in which a portion of the data are given specific class labels, we do not directly take the labels predicted by sourcedomain data due to possible domain mismatch. In other words, we cannot directly apply existing semi-supervised techniques, since they cannot deal with data collected from different domains. This is the reason why the use of our uncertain label matrix together with feature adaptation is preferable, which offers additional robustness in adapting and assigning class labels.</p><p>In addition to the use of uncertain labels predicted from the source domain, we further take the data structure observed in the target domain into our adaptation process. This allows us to better determine the target-domain labels for improved recognition. To observe target-domain structural information, we advance graph-based semi-supervised learning by constructing a k-nearest neighbors (k-NN) graph over targetdomain data <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b6">[7]</ref>. We note that, we choose to construct this k-NN based graph in the transformed space (i.e., Z T ). The use of the transformed space not only allows us to better observe data structural information due to reduced feature dimensions, the learned transformation model W also exhibits capabilities in eliminating biases across source and target domains. This is why improved recognition of targetdomain data can be expected.</p><p>Based on the above observations, we calculate the distance between target-domain data pairs as d(z t i , z t j ) = z t i -z t j , and apply Gaussian kernels for converting such distances into similarity scores: s(z t i , z t j ) = exp(-d(z t i , z t j )/2σ 2 ). With this structural information determined, the k-NN based similarity matrix E∈R N ×N can be formulated, in which each entry is:</p><formula xml:id="formula_9">Eij = s(z t i , z t j ), if z t j is one of k-NN of z t i and i = j 0, otherwise.<label>(6)</label></formula><p>Once we observe the uncertain label matrix Y and the target-domain structural similarity matrix E, we apply the technique of label propagation <ref type="bibr" target="#b39">[40]</ref> to determine the labels for the target-domain data. More specifically, by constructing S = D -1/2 ED -1/2 where D is a diagonal matrix that d ii = j E ij , we update the following equation for propagating the label information in the target domain:</p><formula xml:id="formula_10">Y (t+1) = αSY (t) + (1 -α)Y (0) .</formula><p>At each iteration, each target-domain instance observes the structural information from its neighbors via S, while retaining the original label information (i.e., soft labels Y (0) with regularization parameter α ∈ (0, 1]). We note that, for Y (t) , it can be reformulated as</p><formula xml:id="formula_11">Y (t) = (αS) t Y (0) + (1 -α) t-1 i=0 (αS) i Y (0) .</formula><p>Since 0 &lt; α ≤ 1 and the eigenvalues of S are in [-1, 1] (see detailed derivations in <ref type="bibr" target="#b39">[40]</ref>), we have lim t→∞ (αS) t = 0, and lim</p><formula xml:id="formula_12">t→∞ t-1 i=0 (αS) i = (I -αS) -1 .</formula><p>Therefore, the optimal label matrix Y * can be derived as:</p><formula xml:id="formula_13">Y * = (I -α)(I -αS) -1 Y (0) .<label>(7)</label></formula><p>With Y * obtained, the final label of each target-domain instance is determined by ŷt i = arg max j C Y * ij based on the winner-take-all strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Adaptation via Iterative Optimization</head><p>Finally, we integrate the techniques and learning models presented in Sections III-B and III-C for performing joint unsupervised domain adaptation and cross-domain recognition. The proposed method is summarized in Algorithm 1. It can be seen that, except for the initialization stage which only adapts marginal distributions of cross-domain data, the optimization process take both marginal and conditional distributions into consideration, while the class labels are updated from the previous adaptation iterations. Later in the experiments, we will show that the proposed method converges to the optimal solution in terms of both MMD and accuracy in few iterations, which verify the effectiveness of the proposed model for domain adaptation and recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets and Settings</head><p>MNIST and USPS: We first consider cross-domain digit recognition using MNIST <ref type="bibr" target="#b19">[20]</ref> and USPS <ref type="bibr" target="#b18">[19]</ref> datasets. MNIST contains a training set of 60,000 images and a test set of 10,000 images, while each image is of size 28×28 pixels. On the other hand, each image in USPS is of size 16×16 pixels, and a total of 7291 and 2007 images are available for training and testing, respectively. Figure <ref type="figure" target="#fig_4">2(a)</ref> shows example images of these two datasets.</p><p>Following the setting of <ref type="bibr" target="#b21">[22]</ref>, we randomly sample 2000 and 1800 images from MNIST and USPS (scaled to the same 16×16 pixels), respectively, and take pixel intensities as the features. Two cross-domain pairs are considered: MNIST → USPS and USPS → MNIST. Take MNIST → USPS for example, we have MNIST as the source domain with 2000 labeled training data, and USPS as the target domain with 1800 instances to be recognized. Similar remarks can be applied to USPS → MNIST.</p><p>Caltech-256 and Office: For experiments on cross-domain object recognition, we consider the Caltech-256 <ref type="bibr" target="#b14">[15]</ref> and Office <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b11">[12]</ref> datasets. The former consists of object images of 256 categories (with at least 80 instances per category), while the latter contains 31 objects categories collected from three different sub-datasets: Amazon, DSLR, and webcam. Following the same settings applied in <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b7">[8]</ref>, we select the 10 over-lapping object categories of Caltech-256 and Office for experiments, and produce four different domains of interest: Caltech (C), Amazon (A), DSLR (D), and webcam (W). As a result, a total of 12 different cross-domain pairs will be available (e.g., C → A, C → W, etc.).</p><p>To describe each object image in the Caltech-256 and Office datasets, we apply the DeCAF 6 features <ref type="bibr" target="#b5">[6]</ref>. As shown in <ref type="bibr" target="#b5">[6]</ref>, these features are able to achieve very promising results for image classification. With the use of DeCAF 6 features, each image will be converted into a 4096-dimensional representation for training and testing.</p><p>It is worth noting that, since only unlabeled (test) data are available in the target domain, one cannot apply crossvalidation to select the parameters for the learning models. For fair comparisons, we follow the same parameter settings as <ref type="bibr" target="#b21">[22]</ref> did, and set λ = 0.1 and 1 for digit and object datasets, respectively. When performing data embedding, we choose   k = 100 as the reduced feature dimension. In addition, we follow the recent works of <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref> and apply the linear kernels for constructing the kernel matrix K. For simplicity, we fix the parameter α = 0.5 for label propagation, and set the number of neighbors (for the graph-based similarity matrix E) as 15 for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation</head><p>For cross-domain digit recognition, we consider the approaches of TCA <ref type="bibr" target="#b23">[24]</ref>, JDA <ref type="bibr" target="#b21">[22]</ref> and TJM <ref type="bibr" target="#b22">[23]</ref>. It is worth repeating that, JDA also adapts both marginal and conditional distributions for unsupervised domain adaptation as we do. For baseline approaches, we consider the direct use of SVMs trained by source-domain data in the original feature space (i.e., no domain adaptation). Table I lists the recognition results of cross-domain digit recognition.</p><p>Recall that, when using our proposed method, recognition is achieved when the domain adaptation process is complete (i.e., via label propagation). To show that we can also train the SVM classifiers in the derived transformed feature space using projected labeled source-domain data, and apply such classifiers to recognize the projected target-domain data as other recent methods do (e.g., TCA and JDA), we provide additional results of ours in Table <ref type="table" target="#tab_2">I</ref> (denoted as Ours*). Nevertheless, as shown in this table, our methods clearly outperformed baseline and state-of-the-art methods for the task of cross-domain digit recognition.</p><p>For cross-domain object recognition, we consider another state-of-the-art method of Landmarks (LM) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. LM can only be applied to 9 out of 12 cross-domain pairs. As explained in <ref type="bibr" target="#b10">[11]</ref>, LM requires a sufficient amount of source-domain data for adaptation, and it cannot be applied to the cases when DSLR is applied as the source domain. In addition to TCA <ref type="bibr" target="#b23">[24]</ref>, JDA <ref type="bibr" target="#b21">[22]</ref>, and TJM <ref type="bibr" target="#b22">[23]</ref>, we further consider recent ap-proaches of Landmarks (LM) <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b0">[1]</ref>, Transductive Transfer Machine (TTM) <ref type="bibr" target="#b7">[8]</ref>, DASVM <ref type="bibr" target="#b2">[3]</ref>, Geodesic Flow Kernel <ref type="bibr" target="#b11">[12]</ref>, and Subspace Alignment (SA) <ref type="bibr" target="#b8">[9]</ref>. And, for the purpose of fair comparisons, we following the experiment settings of JDA <ref type="bibr" target="#b21">[22]</ref> and TTM <ref type="bibr" target="#b7">[8]</ref>, and apply Decaf-fc6 as the features for all methods to be evaluated. As noted in <ref type="bibr" target="#b9">[10]</ref>, LM requires a sufficient amount of source-domain data for adaptation, so it can only be applied to 9 out of 12 cross-domain pairs (i.e., it cannot handle the case in which DSLR is taken as the source domain). TTM <ref type="bibr" target="#b7">[8]</ref> is a sample-based trans-formation approach for matching cross-domain data marginal distributions. As for DASVM <ref type="bibr" target="#b2">[3]</ref>, it first derives SVM models using labeled source-domain data, and then such models will be updated by observing the target-domain data with their pseudo labels during adaptation. Note that LP <ref type="bibr" target="#b39">[40]</ref> in Table <ref type="table" target="#tab_4">II</ref> denotes the results using Direct as baseline classifiers followed by label propagation for adaptation.</p><p>We further consider two state-of-the-art deep-learning based approaches for unsupervised domain adaption: Deep Domain Confusion (DDC) <ref type="bibr" target="#b35">[36]</ref> and Deep Adaptation Networks (DAN) <ref type="bibr" target="#b20">[21]</ref>. With the goal of eliminating domain differences, DDC <ref type="bibr" target="#b35">[36]</ref> applies AlexNet models for handling source and targetdomain data (one for each domain), while a single layer is introduced for regularizing the similarity between their last fully connected layers. DAN <ref type="bibr" target="#b20">[21]</ref> further extends the above idea and regularizes each fully connected layer for matching cross-domain data. From Table <ref type="table" target="#tab_4">II</ref>, it can be seen that our method performed favorably against these deep-learning based approaches.</p><p>Comparisons of the recognition results using the aforementioned approaches are shown in Table <ref type="table" target="#tab_4">II</ref>. From this table, we see that our method significantly outperformed all others, including the deep-learning based approaches. It is also worth noting that, the performance of LP was poorer than that of Direct (71.65% vs. 80.36%). This is because that, without aiming at associating cross-domain data, the approach of LP only views label propagation as a post processing stage during adaptation. In other words, the direct use of label propagation without properly adapting cross-domain data distributions would not be preferable for domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Imbalanced Domain Adaptation</head><p>For imbalanced cross-domain data, we may assume either a single domain consisting of multi sub-domains, or imbalanced label numbers across domains.    We first present our experiments of imbalanced class numbers across domains. It is worth noting that, for unsupervised domain adaptation, label information can only be obtained from the data in the source domain. Thus, we do not consider the case of unseen object categories (and data) in the target domain. In other words, we always have the target-domain labels as a subset of those in the source domain.</p><formula xml:id="formula_14">S → T C → A D → A W → A A → C D → C W → C A → D C → D W → D A → W C → W D → W</formula><p>For the dataset of Office+Caltech, we fix the source-domain label number as 10, while that in the target domain ranging from C T = 3 to 10. Table III lists and compares the results of different domain adaptation approaches. For the sake of simplicity, we only present the average results of 12 crossdomain pairs in Table <ref type="table" target="#tab_6">III</ref>. And, since DASVM cannot produce satisfactory results on such imbalanced settings, we do not include their results in this table. Nevertheless, from Table <ref type="table" target="#tab_6">III</ref>, it is clear that our proposed method consistently outperformed others over different C T numbers. Thus, the use of our approach for imbalanced domain adaptation can be verified.</p><p>For unsupervised domain adaptation with multiple subdomains, we consider 10 cross-domain data pairs from the dataset of Office+Caltech (see Table <ref type="table" target="#tab_8">IV</ref>) for evaluation. For each row in Table <ref type="table" target="#tab_8">IV</ref>, either domain consists of multiple subdomain data, and complete results of different approaches are listed. From this table, we see that our approach performed favorably against state-of-the-art unsupervised domain adaptation methods. Therefore, the effectiveness of our approach for mix-domain UDA problems can be successfully verified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Parameter Sensitivity, Convergence and Computation Time</head><p>Recall that, δ in (5) determines the aggressiveness of label propagation by controlling the number of uncertain labels to be transferred from the source domain. In our experiments, we fixed δ as the lower quantile (i.e., 25%) of the maximum posterior probabilities observed from target-domain data, which allows 75% of target-domain data to be assigned uncertain labels during each iteration. Although a larger δ value would imply fewer (and less noisy) target-domain data with uncertain labels for propagation, the adaptation capability would be limited due to less information adapted from the source domain. Thus, the choice of is a trade-off between adaptation and propagation.</p><p>Figures <ref type="figure">3(a</ref>) and (b) compare the recognition performance of two example domain pairs over different δ values. It can be seen that, while extreme δ (i.e., close to 0 or 1) cannot achieve satisfactory performance, our δ choice (based on the above guideline) was able to achieve improved results when comparing to state-of-the-art methods. Intuitively, the choice of δ should be domain dependent. For example, if the mismatch between source and target domains is marginal, one would expect that a small δ would be sufficient for performing adaptation. The study of domain biases and its effect on adaptatoin/propagation aggressiveness would be among our future research directions.</p><p>As noted in Section III-D, we iteratively solve the proposed model for adapting cross-domain data. To assess its convergence property, Figures <ref type="figure">3(c</ref>) and (d) show the recognition accuracy and MMD distance with increasing iteration numbers, respectively. Recall that, the MMD distance is calculated by summing up the first two terms in (2) using target-domain data with ground truth labels. Due to space limit, only selected cross-domain data pairs are presented. From the above figures,   we see that both accuracy and distance converged within 5-10 iterations during optimization.</p><p>In our experiments, we did not fine tune the parameters of "k" and "σ" when constructing the k-NN graph for label propagation. Since there is no labeled data in the target domain, one cannot perform cross-validation to select such parameters. We now provide additional results in Table <ref type="table" target="#tab_9">V</ref>, in which our default parameter choice achieved satisfactory performance. As noted in our paper, we fix k as 15 and σ as 1 2 in all of our experiments.</p><p>Finally, we compare the computation time of different methods on the domain pair of A → W in Table <ref type="table" target="#tab_10">VI</ref>. The runtime estimates were performed on an Intel Core i5 PC with 2.6 GHz CPU and 8G RAM. It can be seen that the computation time of our proposed approach (including iterative optimization and label propagation) was comparable to those of state-of-theart methods, while the recognition performance was greatly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Remarks 1) Adaptation capability:</head><p>As discussed in Section III-C, a major contribution of our work is the ability in exploiting label and structure consistency for unsupervised domain adaptation. This allows us to better match cross-domain conditional distributions for improved classification performance. For verification purposes, we consider an additional JDA-based approach (denoted as GT JDA), which utilizes different amounts of target-domain instances with their ground-truth labels (not pseudo labels) to construct L c in <ref type="bibr" target="#b1">(2)</ref>. With more labeled targetdomain data observed for adaptation, the improvements of cross-domain recognition can be expected. Figure <ref type="figure">4</ref> compares TCA, JDA, our method, and GT JDA with varying amounts of labeled target-domain instances. From this figure, it can be seen that our method was able to achieve comparable results with GT JDA using a large amount of ground truth target-domain labels (e.g., those using 30% of ground truth labeled data in the target domain or more). It is worth repeating that, compared to GT JDA, our method did not consider any labeled data in the target domain during adaptation. Therefore, from the above experiments, the capability of our approach in associating cross-domain data for unsupervised domain adaptation can be successfully verified.</p><p>2) Robustness to initialization error: Similar to JDA, or other MMD-based domain adaptation approaches, we apply an iterative optimization process to associate cross-domain data (as shown in Algorithm 1). It would be a crucial issue if the prediction errors observed during initialization would affect the resulting adaptation and classification performance. To verify that our proposed method exhibits sufficient robustness to such initialization errors, we perform additional experiments with different degrees of perturbed errors when initializing our iteration process. To be more specific, instead of applying the observed uncertain matrix Y, we deliberately and randomly introduce different amounts of prediction errors to the targetdomain data in the first iteration of Algorithm 1.</p><p>Figure <ref type="figure" target="#fig_6">5</ref> presents and compares the results. It can be seen that, the introduced errors did not significantly affect the adaptation (and thus recognition) performance. For some dataset pairs (e.g., D → W), only negligible differences can be observed even if 70% of the target-domain data were intentionally assigned incorrect labels in the first iteration.  Therefore, we not only verify the robustness of our approach to initialization errors. The above experiments also support the exploitation of both label and structure consistency for unsupervised domain adaptation.</p><p>3) Kernel choice: Although only universal kernels like Gaussian RBF are justified in the literature on MMD-based UDA <ref type="bibr" target="#b13">[14]</ref>, we note that our use of linear kernels is more than matching the data means across domains. This is because that, in addition to minimizing the difference between cross-domain data means, our method also matches the class-conditional means across data domains. Moreover, the constraint in (2) needs to be satisfied for preserving the covariance information of cross-domain data. In order words, subtracting the data means from each domain would not result in zero for our MMD calculation. Recent works of TCA <ref type="bibr" target="#b23">[24]</ref>, DIP <ref type="bibr" target="#b0">[1]</ref>, TJM <ref type="bibr" target="#b22">[23]</ref> also confirmed show that the use of linear kernels in a MMD-based formulation would achieve satisfactory performance. And, our experiments also show that our proposed method performed favorably against these recent approaches. Nevertheless, additional experiments using Gaussian RBF kernels are provided in our proposed formulation. Figure <ref type="figure" target="#fig_7">6</ref> compares the performance of the uses of linear v.s. RBF kernels. From this figure, we see that the use of RBF kernels only achieved comparable results as that of linear kernels over different σ 2 rbf choices. Therefore, the use of linear kernels for our method would still be preferable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We proposed an unsupervised domain adaptation based on transfer feature learning. In addition to matching both marginal and conditional distributions of cross-domain data, our proposed model further leverages rich label and structural information across domains. This allows us to achieve improved adaptation and recognition of cross-domain data. Our experiments on cross-domain digit and object recognition confirmed that our proposed model performed favorably against state-of-the-art domain adaptation methods. Future research directions include landmark (i.e., instance) selection for crossdomain data and domain-adaptive label propagation, which could further improve the domain adaptation and recognition performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>We propose a novel unsupervised domain adaptation approach, which exploits the local structural information of the target-domain data, together with the label information inferred from the source domain for matching crossdomain feature distributions. (Section III) • Compared to several state-of-the-art UDA methods, improved performance on several cross-domain classification tasks can be obtained by our method, including the challenging task of imbalanced domain adaptation problems. (Section IV) • Additional experiments on convergence analysis, parameter sensitivity, kernel choice, and robustness to initialization errors are provided. These supporting materials further verify the effectiveness of our proposed model. (Section IV)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>i : y s i = c} indicating source-domain data of class c, and D(c) T = {x t i : ŷt i = c} as those in the target domain with the same predicted label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Our Proposed Model Input: Kernel matrix K of cross-domain data, labels yS of sourcedomain data 1. Initialize D(c) T as ∅ while not converged do 2. W ← Distribution adaptation ( D(c) T , ŷT ) in (4) and let [ZS, ZT ] = W K 3. Assign Y (0) by classifiers trained by ZS and (5) 4. Construct the k-NN graph matrix E and S within target domain ZT 5. ( D(c) T , ŷT ) ← label propagation (Y (0) , S) in (7) end while 6. yT ← ŷT Output: yT as labels of target-domain data IV. EXPERIMENTS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example images of (a) MNIST + USPS datasets and (b) Caltech-256 + Office datasets.</figDesc><graphic coords="6,70.38,82.98,201.74,85.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Parameter sensitivity and convergence analysis. For the former issue, we show the recognition rates over different δ values on (a) A→W and (b) USPS→MNIST. Note that the vertical dotted lines indicate the δ values determined by our approach (see Section III-C). For convergence analysis, we report (c) recognition accuracy and (d) MMD distance versus the number of iterations over several domain pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Robustness analysis. The horizontal axis is the percentage of incorrect labels manually introduced into the first iteration of our adaptation process. The vertical axis denotes the accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Performance on A → W of our method with different kernel choices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2609820, IEEE Transactions on Image Processing</figDesc><table><row><cell>domain adaptation by solving a label-propagation based opti-</cell></row><row><cell>mization task. This allows us to better associate cross-domain</cell></row><row><cell>marginal and conditional feature distributions, while additional</cell></row><row><cell>robustness is introduced for alleviating possible errors due</cell></row><row><cell>to domain mismatch. By jointly solving the adaptation and</cell></row><row><cell>recognition tasks in a unified framework, improved recognition</cell></row><row><cell>performance can be expected in the target domain.</cell></row><row><cell>With the recent advances of deep learning techniques,</cell></row><row><cell>Deep Domain Confusion (DDC)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I COMPARISONS</head><label>I</label><figDesc>OF RECOGNITION RATES (%) FOR CROSS-DOMAIN DIGIT RECOGNITION. NOTE THAT S AND T DENOTE SOURCE AND TARGET DOMAINS, RESPECTIVELY.</figDesc><table><row><cell>S → T</cell><cell>MNIST → USPS</cell><cell>USPS → MNIST</cell></row><row><cell>Direct</cell><cell>50.1</cell><cell>33.2</cell></row><row><cell>TCA [24]</cell><cell>52.7</cell><cell>45.7</cell></row><row><cell>JDA [22]</cell><cell>68.5</cell><cell>56.0</cell></row><row><cell>TJM [23]</cell><cell>63.5</cell><cell>52.7</cell></row><row><cell>Ours*</cell><cell>70.6</cell><cell>62.7</cell></row><row><cell>Ours</cell><cell>72.3</cell><cell>65.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2609820, IEEE Transactions on Image Processing</figDesc><table /><note><p>1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II COMPARISONS</head><label>II</label><figDesc>OF RECOGNITION RATES (%) ON Caltech256 + Office DATASETS.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III CROSS</head><label>III</label><figDesc>-DOMAIN CLASSIFICATION ON OFFICE+CALTECH WITH IMBALANCED CROSS-DOMAIN LABEL NUMBERS. NOTE THAT THE SOURCE-DOMAIN LABEL NUMBER IS FIXED AT 10, WHILE THE TARGET-DOMAIN LABEL NUMBER C T ≤ 10.</figDesc><table><row><cell>C T</cell><cell>Direct</cell><cell>TCA</cell><cell>GFK</cell><cell>SA</cell><cell>JDA</cell><cell>TJM</cell><cell>Ours</cell></row><row><cell>3</cell><cell>77.38</cell><cell>86.71</cell><cell>74.45</cell><cell>63.45</cell><cell>82.29</cell><cell>80.53</cell><cell>93.27</cell></row><row><cell>4</cell><cell>77.34</cell><cell>87.22</cell><cell>76.52</cell><cell>67.09</cell><cell>84.42</cell><cell>82.29</cell><cell>93.94</cell></row><row><cell>5</cell><cell>77.26</cell><cell>87.29</cell><cell>78.34</cell><cell>69.96</cell><cell>86.74</cell><cell>82.73</cell><cell>94.10</cell></row><row><cell>6</cell><cell>75.47</cell><cell>86.32</cell><cell>78.71</cell><cell>71.07</cell><cell>86.00</cell><cell>81.77</cell><cell>92.36</cell></row><row><cell>7</cell><cell>76.10</cell><cell>86.53</cell><cell>80.99</cell><cell>73.31</cell><cell>88.35</cell><cell>83.09</cell><cell>92.52</cell></row><row><cell>8</cell><cell>75.41</cell><cell>86.63</cell><cell>80.55</cell><cell>75.69</cell><cell>89.52</cell><cell>83.56</cell><cell>92.60</cell></row><row><cell>9</cell><cell>75.36</cell><cell>86.18</cell><cell>81.31</cell><cell>78.24</cell><cell>89.94</cell><cell>84.32</cell><cell>92.52</cell></row><row><cell>10</cell><cell>80.36</cell><cell>85.44</cell><cell>81.91</cell><cell>78.63</cell><cell>88.91</cell><cell>86.62</cell><cell>92.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>1057-7149 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2016.2609820, IEEE Transactions on Image Processing</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>OF IMBALANCED UDA ON OFFICE+CALTECH WITH MULTIPLE SUB-DOMAINS.</figDesc><table><row><cell>S → T</cell><cell>Direct</cell><cell>DASVM</cell><cell>TCA</cell><cell>GFK</cell><cell>SA</cell><cell>JDA</cell><cell>LM</cell><cell>TJM</cell><cell>Ours</cell></row><row><cell>A, C, D → W</cell><cell>81.69</cell><cell>81.36</cell><cell>86.10</cell><cell>79.78</cell><cell>78.10</cell><cell>92.88</cell><cell>90.51</cell><cell>92.88</cell><cell>93.22</cell></row><row><cell>A, C, W → D</cell><cell>96.18</cell><cell>94.90</cell><cell>97.45</cell><cell>84.85</cell><cell>89.04</cell><cell>97.45</cell><cell>94.27</cell><cell>98.73</cell><cell>98.73</cell></row><row><cell>C, D, W → A</cell><cell>82.88</cell><cell>85.91</cell><cell>92.28</cell><cell>84.91</cell><cell>88.47</cell><cell>92.69</cell><cell>93.01</cell><cell>91.65</cell><cell>93.95</cell></row><row><cell>A, D, W → C</cell><cell>78.01</cell><cell>78.36</cell><cell>84.42</cell><cell>79.69</cell><cell>81.11</cell><cell>88.25</cell><cell>84.42</cell><cell>83.53</cell><cell>88.78</cell></row><row><cell>D, W → A, C</cell><cell>70.49</cell><cell>65.83</cell><cell>81.69</cell><cell>77.06</cell><cell>73.80</cell><cell>87.84</cell><cell>81.55</cell><cell>74.10</cell><cell>89.57</cell></row><row><cell>C, W → A, D</cell><cell>86.19</cell><cell>84.93</cell><cell>93.36</cell><cell>84.82</cell><cell>88.74</cell><cell>93.18</cell><cell>91.84</cell><cell>92.83</cell><cell>95.07</cell></row><row><cell>C, D → A, W</cell><cell>83.64</cell><cell>86.75</cell><cell>92.58</cell><cell>83.59</cell><cell>87.80</cell><cell>92.34</cell><cell>90.66</cell><cell>91.70</cell><cell>94.01</cell></row><row><cell>A, W → C, D</cell><cell>81.02</cell><cell>78.67</cell><cell>86.25</cell><cell>78.48</cell><cell>82.73</cell><cell>88.98</cell><cell>85.31</cell><cell>84.69</cell><cell>90.23</cell></row><row><cell>A, D → C, W</cell><cell>80.89</cell><cell>82.51</cell><cell>85.40</cell><cell>78.40</cell><cell>83.07</cell><cell>89.00</cell><cell>85.33</cell><cell>84.91</cell><cell>89.99</cell></row><row><cell>A, C → D, W</cell><cell>64.82</cell><cell>66.15</cell><cell>83.63</cell><cell>77.73</cell><cell>78.16</cell><cell>86.73</cell><cell>87.39</cell><cell>86.73</cell><cell>88.50</cell></row><row><cell>average</cell><cell>80.58</cell><cell>80.54</cell><cell>88.32</cell><cell>80.93</cell><cell>83.10</cell><cell>90.93</cell><cell>88.43</cell><cell>88.17</cell><cell>92.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V RECOGNITION</head><label>V</label><figDesc>PERFORMANCE ON A → W WHEN CHOOSING DIFFERENT VALUES OF NEIGHBORS (K) AND σ FOR CONSTRUCTING OUR K-NN GRAPH. NOTE THAT THE RESULT WITH OUR DEFAULT PARAMETER CHOICE</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">SHOWN IN BOLDFACE.</cell><cell></cell><cell></cell></row><row><cell>k</cell><cell>-3</cell><cell>-2</cell><cell>-1</cell><cell>log 2 (σ) 0</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>5</cell><cell>85.08</cell><cell>85.42</cell><cell>84.75</cell><cell>84.75</cell><cell>84.75</cell><cell>84.75</cell><cell>84.41</cell></row><row><cell>10</cell><cell>86.78</cell><cell>91.19</cell><cell>90.85</cell><cell>89.49</cell><cell>89.49</cell><cell>89.49</cell><cell>89.49</cell></row><row><cell>15</cell><cell>90.51</cell><cell>90.85</cell><cell>88.81</cell><cell>87.80</cell><cell>87.46</cell><cell>87.80</cell><cell>87.80</cell></row><row><cell>20</cell><cell>90.51</cell><cell>90.85</cell><cell>87.46</cell><cell>86.10</cell><cell>86.10</cell><cell>86.10</cell><cell>86.10</cell></row><row><cell>25</cell><cell>90.51</cell><cell>90.85</cell><cell>86.44</cell><cell>85.42</cell><cell>85.42</cell><cell>85.42</cell><cell>85.42</cell></row><row><cell>30</cell><cell>90.51</cell><cell>89.15</cell><cell>85.76</cell><cell>84.41</cell><cell>84.41</cell><cell>84.41</cell><cell>84.41</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VI COMPARISONS</head><label>VI</label><figDesc>OF RUNTIME ESTIMATES (IN SECONDS) OF DIFFERENT METHODS FOR A → W. NOTE THE METHODS OF JDA, TJM, AND OURS ALL CONVERGED WITHIN 10 ITERATIONS.</figDesc><table><row><cell>TCA [24]</cell><cell>JDA [22]</cell><cell>LM [11]</cell><cell>TJM [23]</cell><cell>Ours</cell></row><row><cell>4.15 (s)</cell><cell>34.12 (s)</cell><cell>1204 (s)</cell><cell>36.17 (s)</cell><cell>45.41 (s)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was supported in part by the Ministry of Science and Technology under Grant MOST103-2221-E-001-021-MY2, MOST105-2221-E-001-028-MY2, and MOST 105-2221-E-017-009-MY3.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>, IEEE Transactions on Image Processing Cheng-An Hou received the B.S. degree from the Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan, in 2013. From 2013 to 2015, he was a Research Assistant with the Research Center for Information Technology Innovation (CITI), Academia Sinica, Taiwan. He is currently a Master Student with Robotics Institute, Carnegie Mellon University,</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Impossibility theorems for domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pál</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain adaptation problems: A DASVM classification technique and a circular validation strategy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marconcini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2007">2010. 1, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/∼cjlin/libsvm.4" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Co-regularization based semisupervised domain adaptation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname><genName>III</genName></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting structures in image collections for object recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transductive transfer machine</title>
		<author>
			<persName><forename type="first">N</forename><surname>Farajidavar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ACCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006">2013. 1, 2, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning kernels for unsupervised domain adaptation with applications to visual object recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<date type="published" when="2008">2014. 2, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2007">2012. 1, 2, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A kernel method for the two sample problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007. 1, 2, 3, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient learning of domain-invariant image representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coupled dictionary and feature space learning with applications to cross-domain image synthesis and recognition</title>
		<author>
			<persName><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2013</title>
		<imprint>
			<date type="published" when="2008">1, 2, 3, 4, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009">2014. 2, 6, 7, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Networks</title>
		<imprint>
			<date type="published" when="2009">2011. 1, 2, 3, 4, 6, 7, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain adaptive dictionary learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transfer learning in a transductive setting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bypassing synthesis: Pls for face recognition with pose, low-resolution and sketch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generalized multiview analysis: A discriminative latent space</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generalized domain-adaptive dictionaries</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bregman divergence-based regularization for transfer subspace learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Shifting weights: Adapting object detectors from image to video</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="638" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Probability estimates for multiclass classification by pairwise coupling</title>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="975" to="1005" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Heterogeneous domain adaptation and classification by exploiting the correlation subspace</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Verscheure. Cross domain distribution adaptation via kernel mapping</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003">2003. 2, 3, 4, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
