<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pedestrian Detection with a Large-Field-Of-View Deep Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
							<email>anelia@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Anelia Angelova is with Google Research</orgName>
								<address>
									<addrLine>Mountain view</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
							<email>akrizhevsky@google.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Alex Krizhevsky is with Google</orgName>
								<address>
									<addrLine>Mountain view</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
							<email>vanhoucke@google.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Vincent Vanhoucke is with Google Research</orgName>
								<address>
									<addrLine>Mountain view</addrLine>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pedestrian Detection with a Large-Field-Of-View Deep Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C21D4E1779A519C9636E912785A13AF9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pedestrian detection is of crucial importance to autonomous driving applications. Methods based on deep learning have shown significant improvements in accuracy, which makes them particularly suitable for applications, such as pedestrian detection, where reducing the miss rate is very important. Although they are accurate, their runtime has been at best in seconds per image, which makes them not practical for onboard applications. We present a Large-Field-Of-View (LFOV) deep network for pedestrian detection, that can achieve high accuracy and is designed to make deep networks work faster for detection problems.</p><p>The idea of the proposed Large-Field-of-View deep network is to learn to make classification decisions simultaneously and accurately at multiple locations. The LFOV network processes larger image areas at much faster speeds than typical deep networks have been able to, and can intrinsically reuse computations. Our pedestrian detection solution, which is a combination of a LFOV network and a standard deep network, works at 280 ms per image on GPU and achieves 35.85 average miss rate on the Caltech Pedestrian Detection Benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Pedestrian detection is naturally very important in applications such as driving assistance or autonomous driving. These applications are becoming more and more mainstream with several auto makers already offering pedestrian detection warning systems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, and others planning to have fully integrated such systems within a couple of years <ref type="bibr" target="#b3">[4]</ref>.</p><p>Pedestrian detection has also seen intense development in computer vision <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, incorporating various classification algorithms to recognize pedestrians: Adaboost, SVM, Decision Forests <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Recently, deep learning methods have become the top performing approaches for pedestrian detection <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, and have also shown overwhelmingly good results in other computer vision applications <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, and in other domains <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Deep networks are also versatile, as they do not need task-specific or hand-crafted features and can perform equally well on both rigid (e.g. traffic signs, cars) and deformable object categories without having to explicitly model parts and their relationships. Furthermore, deep networks readily transfer knowledge between domains by training on one domain and improving results by finetuning in another <ref type="bibr" target="#b14">[15]</ref>. However, their main drawback has been the speed of classification.</p><p>The task of detection is more complex than classification alone, as it is required to also localize where the object is, including its extents and scale. One of the most common approaches for detection has been the sliding window approach <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b4">[5]</ref>, in which a classifier is applied to a window of pre-specified size, then the same task is repeated for the neighbouring window and so on. A cascadeof-classifiers <ref type="bibr" target="#b19">[20]</ref> is typically employed to save time, by applying simpler models first and complex models on only the hardest patches. Irrespective of the of use of a cascade, a sliding-window style detection requires thousands of classifications per image, which increases the computational time of the individual classifier by the number of locations tested.</p><p>We propose an alternative way to perform detection that is based on deep networks. Instead of exhaustively sliding a classifier, we deploy a Large-Field-Of-View deep neural network that understands larger areas of the image, and thus examines fewer positions in the image. The LFOV deep network is trained so that it can do multiple detections simultaneously at multiple locations, Figure <ref type="figure">1</ref>. As a result, much fewer of those classifiers can be distributed in the image to fully cover it for detection. Secondly, because of its deep architecture, the LFOV network can reuse computations, that is, the computations for detecting pedestrians at multiple locations are shared within the network. It also can take advantage of context, due to the large field of view of operation. Thirdly, the LFOV deep network is designed so that it is faster than if we would have deployed separate classifiers with the same input dimensions and same capacity (Section III-E). In combination with a standard deep network, the LFOV classifier is much faster than any prior deep learning techniques and achieves competitive performance.</p><p>The contributions of this work are as follows: The main objective of the proposed LFOV deep network is to provide an algorithmic speedup for the very powerful but otherwise extremely slow deep networks. Our end-to-end pedestrian detection solution, which is entirely based on deep learning, provides more than 60x speedup over the single deep network approach, for both CPU and GPU implementations. This is done with a very small loss in accuracy compared to a standard deep learning method. Secondly, we show that a deep network can be successful at simultaneously making decisions at multiple locations. Lastly, we demonstrate that the LFOV deep network can be made faster than individual smaller networks. This realization is very important to be able to achieve a speed advantage. We also show that our solution is very competitive with the state-of-the-art in both accuracy and computational time, by running the whole detection at 280 ms per image on an NVIDIA Tesla K20 GPU, whereas prior such methods are on the order of seconds <ref type="bibr" target="#b11">[12]</ref>. Fig. <ref type="figure">1</ref>. Large-field-of-view deep classifier is a deep network that is trained to detect simultaneously multiple objects on a regular grid. At a single scale, a larger portion of the image is taken as input. The output label is here 16 dimensional, where each position encodes whether there is a pedestrian in the corresponding 4x4 grid cell of the input image. For example, here two pedestrians can be discovered by the application of a single LFOV classifier, while all possible 16 locations are tested simultaneously. The LFOV is run at multiple scales, similar to other detectors.</p><p>The LFOV deep network alone can also be viewed as a new way to generate proposals for a detection task that requires precise localization, such as pedestrian or traffic sign detection. The LFOV network, when used as a mechanism for generating proposal boxes, takes about 100ms per image on GPU, and also has very high recall for pedestrian detection (Section IV-B), which is hard to achieve through other standard bounding-box proposal methods.</p><p>We implemented our LFOV classifier using the open source system <ref type="bibr" target="#b12">[13]</ref>, and provided all details to the best of our knowledge (Section III-A), so that they can be easily reproduced. We submitted our results to the open pedestrian detection benchmark <ref type="bibr" target="#b20">[21]</ref>, so they can be directly comparable with others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PREVIOUS WORK</head><p>Pedestrian detection has been a topic of many recent works, many of which have had significant impact <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b8">[9]</ref>. The algorithm of Viola and Jones for face detection, which proposed fast features and cascade of Adaboost classifiers <ref type="bibr" target="#b19">[20]</ref>, has been applied to pedestrian detection <ref type="bibr" target="#b25">[26]</ref>, generating interest in this domain too. Dalal and Triggs <ref type="bibr" target="#b4">[5]</ref> developed the HOG features, again for the purposes of pedestrian detection. This work has been influential to many other computer vision problems.</p><p>An important recent work has been of Dollar et al <ref type="bibr" target="#b23">[24]</ref>, who developed a publicly available toolbox and a benchmarking dataset <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b26">[27]</ref>. As a result, many present and future methods can be evaluated in the same setting. Another important work has been on increasing the speed of pedestrian detection with proposed methods reaching speeds of 100 to 135 fps <ref type="bibr" target="#b6">[7]</ref>. These methods are the fastest reported for pedestrian detection. A lot of methods have introduced or experimented with a large variety of features, thus pushing the progress of pedestrian detection and steadily improving the state-of-the-art <ref type="bibr" target="#b8">[9]</ref>. Some methods, based on the Deformable Parts Models, have also been successfully applied in the context of pedestrian detection <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Other methods have explored multi-scale solutions, or employed context or motion features <ref type="bibr" target="#b27">[28]</ref>.</p><p>Deep learning methods have been applied to pedestrian detection, improving on the detection accuracy over many prior methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. However, their run-time has been somewhat slow, i.e. about 1-1.5 seconds per image <ref type="bibr" target="#b11">[12]</ref>, or even reaching runtimes in the order of minutes <ref type="bibr" target="#b29">[30]</ref>. Our method is also based on deep networks, and because of that is very accurate, and at the same time is several times faster than other deep network methods. Prior deep networks have used edge filters to initialize weights rather than learning from raw pixels values, or have used deep nets in combination in a deformable-parts model style to model parts <ref type="bibr" target="#b9">[10]</ref>. We train our deep network LFOV classifier from raw pixel values and without hand-made or any special features, and we show that it can achieve competitive results. In addition, our framework allows for simultaneous processing at multiple locations, and thus reuses computation and takes advantage of context, whereas prior methods take advantage of context by running separate classifiers, e.g. for cars <ref type="bibr" target="#b28">[29]</ref>, or other pedestrians <ref type="bibr" target="#b30">[31]</ref>.</p><p>In the domain of object detection, the sliding window detection has been the most common. It essentially samples the image densely and evaluates all patches and is often applied in a cascade-style, i.e. progressively employes harder classifiers. Our approach is closer to the traditional sliding window cascades as it will de facto examine all locations. It differs by offering a larger field of view to make its decision, and thus speed up the detection.</p><p>An alternative to a cascade approach is to first identify proposal boxes <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Previous methods that apply proposal windows are much slower (in tens of seconds) and have not been applied to applications that demand accurate localization. For example, in the most recent work of Girshick et al <ref type="bibr" target="#b32">[33]</ref>, detection with proposal boxes takes 53 seconds per frame for a CPU implementation and 13 seconds per frame on GPU. When using the LFOV classifier as a mechanism for generating proposal boxes, it runs about 100 ms per image, and it can recall a large portions of pedestrians, which typically take very small areas of the image.</p><p>Recent work has focused on reducing the computational time needed for detection of deep nets <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. These methods optimize detection by reusing computation or make computations more efficient. These approaches are still relatively slow, more than one second per image, and not suitable for time-sensitive problems. Instead of speeding up the computations after the models are learned, our LFOV classifier is designed to reuse computations at the time of training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. LARGE-FIELD-OF-VIEW DEEP NETWORK</head><p>The Large-Field-of-View (LFOV) classifier is a deep neural network which works on larger areas of the image as input. For the input it takes, e.g. the bottom left image of Figure <ref type="figure">1</ref>, it is trained to output whether there is a pedestrian at multiple locations. To be specific, we subdivide the input image into a 4x4 grid and for each grid cell, the network learns whether there is a pedestrian that is centered within this cell or not (Figure <ref type="figure">1</ref>). The LFOV deep neural network is applied to the input (details provided in Section III-A) and the output that the network is going to learn is a 16 dimensional output, in which each output dimension corresponds to one cell in the grid, and its interpretation is whether this cell has a pedestrian or not. In fact, for convenience we used a 17-dimensional output, details are below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LFOV architecture</head><p>The Large-Field-of-View (LFOV) classifier is going to be used as a first stage in generating proposal boxes for pedestrian detection. We implemented it here as a very simple convolutional network. We have also designed it so that its computational time is very fast. The architecture is inspired by the original work of Krizhevsky et al <ref type="bibr" target="#b12">[13]</ref> but is much simpler.</p><p>More specifically the LFOV classifier works on 64x64 RGB image input and has the following layers:</p><p>-A 5x5 convolutional layer with a filter depth of 32, with a stride of 2. It is followed by a normalization layer, as defined by <ref type="bibr" target="#b12">[13]</ref> and a pooling layer (3x3, stride of 2).</p><p>-A 1x1 convolutional layer of depth 32. This type of layer was first introduced here <ref type="bibr" target="#b35">[36]</ref> and is added as it essentially adds depth to the full network at very little computational cost.</p><p>-A fully connected layer of depth 512. It is followed by a standard dropout layer <ref type="bibr" target="#b36">[37]</ref> of 0.5 dropout rate.</p><p>-Finally, a fully connected layer of 17 outputs is attached at the end, to which a cross-entropy cost is applied. Each output is responsible for one of the cells in the 4x4 grid, plus an additional output for detecting a pedestrian that spans the full image. Although we did not have to add the pedestrian that takes the full image, we found it useful to seamlessly detect pedestrians at this higher resolution as well.</p><p>All layers, except the 1x1 convolutional one, are followed by a rectified linear unit. Figure <ref type="figure" target="#fig_2">3</ref> visualizes the architecture.</p><p>The main idea of LFOV classifier is that the deep network classifier, through several layers of inference, is capable of learning about the presence of pedestrians at different smaller scales. Indeed we observed that it can learn very successfully the presence of pedestrians at these lower scales, but at the same time the pedestrian at 1x1 grid is learned with higher success rate than the ones at the 4x4 grid. Furthermore, in our implementation, to save time, we use an input image size of 64x64, so we are effectively detecting pedestrians of very low resolution, 16x16, in each grid cell. We note here that larger and more complex deep network can also be directly trained over the 4x4 grid inputs. We did not pursue that approach, because applying a LFOV classifier at full resolution would be too slow for applications such as pedestrian detection, which strive for at least several frames per second in processing time.</p><p>Note that, as our model uses fully-connected layers in the design, the decision about a pedestrian in a cell will be dependent on the information that is available in the neighbouring cells and the whole grid, which will provide context. That said, the design of our classifier is block-wise, so we will not always have full context available e.g. the most top-right pedestrian may not have full context. We could avoid that by testing overlapping decisions, but we chose not to, in the interest of speed.  in a cell are not considered as positive examples. Figure <ref type="figure" target="#fig_0">2</ref> shows examples that are used as inputs to our network. Note that although the pedestrians may look quite hard to spot in those images at a first glance, our LFOV network can successfully learn to detect them (even when rescaled so that each pedestrian is approximately 16x16 pixels in size). Section IV-B has details about the successful recall of the trained LFOV classifier, i.e. what percentage of the available pedestrians in all images have been found by LFOV classifier on the test data. As the pedestrians came at different sizes, we generate data at different scales, which is later rescaled to the same size network. Similarly, at detection time, the network is applied at multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training the LFOV network</head><p>To generate negative examples, we sampled randomly across the image and included examples that do not have pedestrians in any of the boxes. Pedestrians that are inbetween grid cells are not included as either positive or negative. Because of this, for the last stage of the classifier, we harvested hard negative examples, by running the trained classifier and including additional negative examples that are not classified correctly. This is necessary because the random sampling of negative examples will generate too many and 'too easy' examples, which do not benefit the classifier.</p><p>Pre-training. As is standard with deep neural networks, we used pre-training from an equivalent network that was trained on Imagenet dataset <ref type="bibr" target="#b12">[13]</ref>. Pre-training was applied only to our baseline (last-stage) classifier. While this is not a strict requirement, because of the richness of the features obtained from a more diverse dataset, pre-training can be helpful for tasks for which training data may not be fully sufficient. We observed a very small improvement with pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Detection with LFOV</head><p>When detecting with a regular sliding window classifier the step size at which each classifier is tested is very important. For too large step sizes, one can miss important detections, conversely, small sizes, e.g. 1 or 2 pixels will make the processing extremely slow. We choose a step size of 4 pixels for our baseline algorithm, as a good tradeoff for all our evaluations. It is likely that better results are achieved with more dense sampling, both in terms of final accuracy and in terms of increase in pedestrian recall.</p><p>The LFOV classifier was implemented to use the same step size as dense sampling would. Let us assume that the LFOV network is implemented with a 4x4 grid and is considering an input of size 64x64. This means that within each grid cell we are considering detecting a pedestrian of size 16x16. When applying the LFOV classifier, for this example, we can see that at positions 0, 16, 32, and 48 in both horizontal and vertical directions, we would detect simultaneously a pedestrian if there is one. However, for the desired step size of 4 pixels, in this example, we would not very successfully detect a pedestrian, where its top left corner starts at positions 4, 8, 12, 20, etc. To do that we actually apply a local sliding window to cover all displacements, so as to test all locations at the desired step size. Note however, because of the LFOV trick, we only need to test these displacements to cover the top left grid cell and the LFOV classifier will automatically be testing all step sizes for all the grid cells necessary. After doing the local dense sampling of steps 0, 4, 8, 12, the next step size we need to test with the LFOV classifier will be directly 64 rather than 16. <ref type="foot" target="#foot_0">1</ref> In effect, for this example, the step sizes the LFOV classifier needs to take are as follows: 0, 4, 8, 12, 64, 68, 72, 76, 140, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. LFOV Cascade</head><p>Since we need the final pedestrian detector to work fast, we are going to use a cascade of deep networks, where the LFOV classifier is the first stage.</p><p>Our algorithm consists essentially of 3 stages: 1) A LFOV classifier, Figure <ref type="figure" target="#fig_2">3</ref>, which works at larger blocks of the image and is specifically designed to be fast (as described in Section III-A). Its purpose is to generate proposal boxes where pedestrians can be detected. 2) A small deep network that uses the same deep network model architecture as the LFOV (as shown in Figure <ref type="figure" target="#fig_2">3</ref>), but works on 16x16 image inputs instead of the large-field 64x64 inputs. This classifier is applied to all candidate boxes that the LFOV produces as positive classifications. It also has a single 1-dimensional output, which determines whether the input image contains a pedestrian or not. 3) The last stage is a standard deep network as in Krizhevsky et al <ref type="bibr" target="#b12">[13]</ref>. Naturally it is applied to all candidates that the previous stages have generated. The middle classifier is only needed for speed, and can lose a bit of precision (as shown later in our experiments). The architecture of the last stage is described in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>The first two stages are trained to have high recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Design of the Large-Field-of-View Network</head><p>The most important component of the LFOV deep network is that it is several times faster than the cumulative work of standard deep networks of the same capacity. This means that the LFOV network is beneficial in terms of speed compared to other deep network models. Here are the details.</p><p>Among the models at the first stage, out fastest model for 16x16 input size (of 1 convolutional and 1 fully connected layer) which is trained to have sufficiently high recall, works at 0.5 milliseconds per 128 patches on GPU (the model we actually used had an additional 1x1 convolution and is slower, taking 0.67 milliseconds). An image of this size typically has about 80K candidates. This makes the processing of this image at least 80000*0.0005=40 seconds. When we divide by 128, we get 40/128 = 0.3125 seconds, which means it needs at least 300 ms to apply the fastest possible individual network (it will take more than 400 ms for the model with 1x1 convolutions). The LFOV classifier that can simultaneously localize 16 pedestrians and has 4x4 larger field of view, with the architecture as in Figure <ref type="figure" target="#fig_2">3</ref>, works at 3 msec per 128 input images. Per image we evaluate 4500 large-field-of-view patches, so 4500/128*3=105 msec. Compare this to the standard classifier, we can see that we have 3x speedup compared the best timing we can achieve with the same architecture, but with a standard deep network classifier. Thus the LFOV classifier has the advantage of working on the same input (with the same capacity) but doing that several times faster. This is a key observation in the design of the LFOV classifier. We note that the quality of detections for LFOV does not diminish, it achieves its speedup through reuse of computations, rather than trading off quality as prior fast detectors <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Caltech pedestrian detection dataset</head><p>We evaluated our results on the Caltech Pedestrian detection dataset <ref type="bibr" target="#b5">[6]</ref>, which has been the main benchmark for pedestrian detection and a large number of methods have been evaluated on it <ref type="bibr" target="#b20">[21]</ref>.</p><p>We use the standard training and test protocols established in the Caltech benchmark and report the results by measuring the average miss rate, as had prior methods. We use the code provided in the toolbox of this benchmark <ref type="bibr" target="#b5">[6]</ref> to do the evaluation. The results below are obtained when training with the standard Caltech-Training dataset for training. Other works have included additional Inria dataset, but we chose not to because the Inria dataset is less relevant to pedestrian detection for autonomous driving. Our baseline classifier has started training from a model pretrained on Imagenet, as in <ref type="bibr" target="#b12">[13]</ref>.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows the performance of our method in comparison to the state-of-the-art methods, using Piotr Dollar's evaluation toolbox and results provided therein <ref type="bibr" target="#b5">[6]</ref>. We tested on the Caltech Test set in the 'reasonable' setting, as is standard for all methods. Figure <ref type="figure" target="#fig_4">5</ref> shows the average miss rate and the false positive rate per image vs. the miss rate, which follows the standard evaluation protocol. In the interest of showing less cluttered plot, we visualize only the best methods and also a number of common methods, as is suggested by <ref type="bibr" target="#b26">[27]</ref>. Table <ref type="table" target="#tab_0">I</ref> shows additional details, including the most recent successful approaches.</p><p>As seen in Figure <ref type="figure" target="#fig_4">5</ref>, the LFOV pedestrian detector achieves average miss rate of 35.85%, and is among the best performing results. When the middle stage of the cascade is ignored (LFOV-2St), our method performs at 35.31%. The InformedHaar <ref type="bibr" target="#b37">[38]</ref> classifier is the best so far with average miss rate at 34.6%. We also note that further improvements have been reported after the preparation of the manuscript, reaching 29% when training on Caltech and Inria datasets, and 22% when using additional motion features <ref type="bibr" target="#b38">[39]</ref>.</p><p>Table <ref type="table" target="#tab_0">I</ref> provides additional information regarding runtime and training data and additional approaches. There we focus on comparable methods that have used primarily Caltech data as their main training source. The runtime of our method is 280 ms on GPU, which very good, especially considering that deep network methods have been notoriously slow to run either on CPU or GPU, especially when applied to detection. We can notice that most methods have not considered runtime as very important and did not report it. Prior deep-learning based methods work at more than one second per frame. Fast methods, such as WordChannels <ref type="bibr" target="#b39">[40]</ref> or VeryFast <ref type="bibr" target="#b6">[7]</ref>, which run at more than 16 frames per second of GPU or CPU, are at the same time not among the best performing. We also note that prior methods that have used deep learning techniques <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, use other fast features such as HOG <ref type="bibr" target="#b4">[5]</ref>, for early elimination of most patches. Despite prior deep network-based methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref> taking more sophisticated deep learning approaches and applying much faster features for early elimination, we can see that our LFOV classifier is both better in accuracy and also 3-5 times faster, compared to <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>Although we do not use explicitly context to detect pedestrians it is implicit in our model since the large network can in principle observe larger areas of the image (and make use of context). We can see that our method outperforms others that have used context. For example, Ouyang and Wang <ref type="bibr" target="#b30">[31]</ref> use two-pedestrian detection to improve on single-pedestrian detection, Ouyang et al <ref type="bibr" target="#b40">[41]</ref> jointly detect two neighbouring pedestrians. Yan et al <ref type="bibr" target="#b28">[29]</ref> use a vehicle detector to help remove false alarms and improve on the pedestrian detection. Considering that adding context in prior methods is likely to increase computational time and slow down the detection, the  LFOV classifier has an advantage, as the context is built-in in the large field of view and no extra computation is needed. We believe, however, that some additional sources of context, such as the presence of a vehicle as in Yan et al <ref type="bibr" target="#b28">[29]</ref> can further improve our proposed method. Additionally, motion information can be quite helpful, e.g. Park et al <ref type="bibr" target="#b42">[43]</ref> utilize motion stabilization to improve the baseline classifier. Since some pedestrians are extremely hard to identify as individual examples, without any motion data, we believe that it can greatly improve our classifier too. Figure <ref type="figure">6</ref> shows some example detections. Here we can see both successful detections, but also some missed and false alarms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. LFOV network recall</head><p>We here compute the average recall each of the stages our classifier can provide. The first stage (LFOV) can be viewed as a mechanism for generating proposal boxes, and we believe it can be suitable for detecting other objects, e.g. traffic signs. Table <ref type="table" target="#tab_1">II</ref> shows the recall of each network (prior to NMS). The recall measures what percentage of the available pedestrians in all images have been found by the classifier on the test data.</p><p>As seen, the recall of LFOV deep network is reduced by adding more stages of the cascade, which is as expected. At the same time, the cascade is needed for obtaining speedup. We can also observe that the NMS algorithm is also contributing to a considerable loss in recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Timing</head><p>We measured the timing of each of the stages on the GPU and estimated the final runtime<ref type="foot" target="#foot_1">2</ref> . The LFOV classifier which is applied at the first stage takes about 105 ms, The second stage takes about 20-25 milliseconds, depending how many boxes are sampled. The last stage takes about 150ms, for a baseline model that runs at about 50 milliseconds per 128 patches. Overall the full LFOV-based detection algorithm works at 280 ms per frame on the GPU. If, on the other hand, we apply the baseline model to all input candidate windows, it will need 30.47 seconds, which is a 108.8 speedup of the LFOV classifier. For a faster baseline algorithm, e.g. 30ms, we will have 18.2 seconds for the baseline and 220 milliseconds for LFOV, which is 83x speedup. We also measured the corresponding CPU runtime in both cases. Table <ref type="table" target="#tab_1">III</ref> shows the speedups we can achieve for our classifier when running on both CPU and GPU (where our CPU implementation is not optimized and somewhat slow). As seen the speedups for both are larger than 60x, which is a very good algorithmic speedup of the LFOV classifier.</p><p>We note here that the final runtime is not yet real-time, and a 2-3 times speedup is desirable. For clarity, we kept our algorithm and implementation simple. Easy speed ups are possible in the implementation itself, for example, reusing computations while doing the sliding of the LFOV classifier, can probably give large improvements in speed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>This paper proposes a Large-Field-of-View classifier, which is a deep network which processes larger areas of the image and simultaneously makes decisions of the presence of pedestrians at multiple locations. The LFOV network is designed in such a way that it is faster than the cumulative time of standard deep networks. As a result, it can process the full image much faster. Our method can be applied at 3.6 fps on GPU for online pedestrian detection, and can also be valuable as and offline speedup for detection algorithms: it is a way to reliably detect small objects (such as pedestrians) without losing much recall and keeping original precision, and at the same time increasing the speed by more than 60 times.</p><p>Deep networks have been shown to be "large capacity" classifiers, so future improvements in deeper and more complex networks are bound to yield even more accuracy gains. So we believe the proposed combination of employing deep networks for both improved accuracy and for doing more work, e.g. as in this case for detecting multiple pedestrians simultaneously, is a novel direction which can allow for new practical applications.</p><p>We demonstrated end-to-end detection solution that is based entirely on deep neural networks, but the proposed solution can become even more interesting when the capacity of the neural networks becomes bigger, i.e. when they can handle larger inputs. For example, right now we can consider a 4x larger field of view, and thus getting the neural network do 16x detections simultaneously. But for networks with even larger fields of view, one can obtain even more speedups.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Procedure for generating examples for training for the LFOV classifier: each of these examples is generated by positioning a pedestrian in the center of each cell of a 4x4 grid, in as many cell positions as possible.</figDesc><graphic coords="3,89.09,50.08,433.83,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure2visualizes how we generated examples for training the LFOV network. More specifically, for each positive example, we generate all possible square boxes around the pedestrian, so that the person falls into all of the cells of the 4x4 grid, where possible. Pedestrians that are not centered</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Architecture of the Large-field-of-view (LFOV) deep network which runs as a first stage of our pedestrian detection. We use 's' to denote the step size, and 'd' to denote the filter depth per layer.</figDesc><graphic coords="4,114.74,50.09,123.32,113.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Architecture of the deep network classifier which runs as a third and final stage of our detection algorithm.</figDesc><graphic coords="5,57.78,50.09,237.25,181.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results on Caltech test data compared to state-of-the-art results in pedestrian detection. Plots are ordered by their average miss rate.</figDesc><graphic coords="6,16.61,251.77,342.25,226.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="7,79.23,349.74,453.54,340.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF OUR METHOD TO STATE-OF-THE-ART RESULTS FOR PEDESTRIAN DETECTION, INCLUDING TIME FOR TESTING. WE FOCUS ON RECENT METHODS THAT USED CALTECH TRAINING DATA AS THEIR MAIN DATA SOURCE.</figDesc><table><row><cell>Method</cell><cell cols="2">Average miss rate (%) Timing (seconds per image)</cell><cell>Training dataset</cell></row><row><cell>MultiResC [28] (multires)</cell><cell>48.5</cell><cell></cell><cell>Caltech</cell></row><row><cell>DBN-Mut [41] (deep)</cell><cell>48.2</cell><cell></cell><cell>Caltech, Inria</cell></row><row><cell>Roerei [9]</cell><cell>46.13</cell><cell>1</cell><cell>Inria</cell></row><row><cell>MOCO[42]</cell><cell>45.5</cell><cell></cell><cell>Caltech</cell></row><row><cell>MultiSDP [11] (deep, w. context)</cell><cell>45.4</cell><cell></cell><cell>Caltech, Inria + Context</cell></row><row><cell>WordChannels [40] (multires)</cell><cell>42.3</cell><cell>0.06 (GPU)</cell><cell>Caltech, Inria</cell></row><row><cell>MT-DPM [29]</cell><cell>40.5</cell><cell>1</cell><cell>Caltech</cell></row><row><cell>JointDeep[10] (deep)</cell><cell>39.3</cell><cell></cell><cell>Caltech, Inria</cell></row><row><cell>SDN[12] (deep)</cell><cell>37.9</cell><cell>1-1.5 (GPU)</cell><cell>Caltech, Inria</cell></row><row><cell>MT-DPM+Context [29] (w. context)</cell><cell>37.64</cell><cell></cell><cell>Caltech + Context</cell></row><row><cell>ACF+SDt [43] (w. motion)</cell><cell>37.3</cell><cell></cell><cell>Caltech + Motion</cell></row><row><cell>InformedHaar [38]</cell><cell>34.6</cell><cell>1.6</cell><cell>Caltech, Inria</cell></row><row><cell>Ours (LFOV, deep)</cell><cell>35.85</cell><cell>0.28 (GPU)</cell><cell>Caltech, Pretr.</cell></row><row><cell>Ours (LFOV-2St, deep)</cell><cell>35.31</cell><cell>0.55 (GPU)</cell><cell>Caltech, Pretr.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II RECALL</head><label>II</label><figDesc>OF EACH STAGE OF OUR CLASSIFIER MEASURED ON THE CALTECH TEST SET. RESULTS AFTER NON-MAXIMUM SUPPRESSION (NMS) ARE SHOWN AT THE BOTTOM.</figDesc><table><row><cell>Classifier</cell><cell>Pedestrian Recall</cell></row><row><cell>Baseline (Stage 3)</cell><cell>93.49</cell></row><row><cell>LFOV (Stages 1,3)</cell><cell>89.55</cell></row><row><cell>LFOV (Stages 1,2,3)</cell><cell>87.08</cell></row><row><cell>LFOV w. NMS (Stages 1,3)</cell><cell>82.29</cell></row><row><cell>LFOV w. NMS (Stages 1,2,3)</cell><cell>78.01</cell></row><row><cell>TABLE III</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>SPEEDUP</head><label></label><figDesc>OF OUR LFOV DEEP NEURAL NETWORK DETECTOR COMPARED TO THE BASELINE DEEP NEURAL NETWORK.</figDesc><table><row><cell>Classifier</cell><cell cols="2">Speed CPU (seconds) Speed GPU (seconds)</cell></row><row><cell>LFOV (Stages 1,2,3)</cell><cell>122</cell><cell>0.22-0.28</cell></row><row><cell>Baseline</cell><cell>7738</cell><cell>18.2-30</cell></row><row><cell>Speedup</cell><cell>63</cell><cell>83-108</cell></row></table><note><p>Fig. 6. Example detections.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In theory the next step can be at 64+12=76 since the last grid cell would have covered these additional step sizes, but our implementation is more straightforward and did not take advantage of that possible speedup.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Timings are approximate, since only individual stages' performance is measured on the GPU.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Abhijit Ogale for providing the CPU implementation. We also thank Dave Ferguson and Abhijit Ogale for their comments and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mobileye Pedestrian Collision Warning System www</title>
		<ptr target="mobileye.com/technology/applications/pedestrian-detection/pedestrian-collision-warning/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collision warning with full auto brake and pedestrian detection -a practical example of automatic emergency braking</title>
		<author>
			<persName><forename type="first">E</forename><surname>Coelingh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eidehall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bengtsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th International IEEE Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="http://www.bmw.com/" />
		<title level="m">BMV Driving Assistance Package</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="http://safecarnews.com/next-gen-vw-passat-to-feature-emergency-assistance-systems/" />
		<title level="m">VW Emergency Assistance System</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pedestrian detection: A benchmark</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pedestrian detection at 100 frames per second</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matthias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crosstalk cascades for frame-rate pedestrian detector</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Seeking the strongest rigid detector</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matthias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytrlaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-stage contextual deep learning for pedestrian detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Switchable deep network for pedestrian detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Deep Learning in Computer Vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A committee of neural networks for traffic sign classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCNN</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Caltech Pedestrians Benchmark www</title>
		<author>
			<persName><forename type="first">"</forename><forename type="middle">P</forename><surname>Dollar</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>vision.caltech.edu/image datasets/caltechpedestrians</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A trainable pedestrian detection system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intelligent Vehicles</title>
		<meeting>Intelligent Vehicles<address><addrLine>Stuttgart</addrLine></address></meeting>
		<imprint>
			<publisher>Germany</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-cue onboard pedestrian detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The fastest pedestrian detector in the west</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contextual boost for pedestrian detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detecting pedestrians using patterns of motion and appearance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiresolution models for object detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Folwkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust multi-resolution pedestrian detection in trafc scenes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single pedestrian detection aided by multipedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<ptr target="http://arxiv.org/pdf/1311" />
		<imprint>
			<date type="published" when="2013">2524v4.pdf, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Densenet: Implementing efficient convnet descriptor pyramids</title>
		<author>
			<persName><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv technical report</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast image scanning with deep max-pooling convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1312.4400" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Informed haar-like features improve pedestrian detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Strengthening the effectiveness of pedestrian detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Word channel based multiscale pedestrian detection without image resizing and using only one classifier</title>
		<author>
			<persName><forename type="first">A</forename><surname>Costea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nedevschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling mutual visibility relationship in pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Detection evolution with multi-order contextual co-occurrence</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring weak stabilization for motion feature extraction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
