<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEQUENCE-BASED MULTI-LINGUAL LOW RESOURCE SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siddharth</forename><surname>Dalmia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ramon</forename><surname>Sanabria</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SEQUENCE-BASED MULTI-LINGUAL LOW RESOURCE SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multi-lingual speech recognition</term>
					<term>cross-lingual adaptation</term>
					<term>connectionist temporal classification</term>
					<term>feature representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Techniques for multi-lingual and cross-lingual speech recognition can help in low resource scenarios, to bootstrap systems and enable analysis of new languages and domains. End-to-end approaches, in particular sequence-based techniques, are attractive because of their simplicity and elegance. While it is possible to integrate traditional multi-lingual bottleneck feature extractors as front-ends, we show that end-to-end multi-lingual training of sequence models is effective on context independent models trained using Connectionist Temporal Classification (CTC) loss. We show that our model improves performance on Babel languages by over 6% absolute in terms of word/phoneme error rate when compared to mono-lingual systems built in the same setting for these languages. We also show that the trained model can be adapted cross-lingually to an unseen language using just 25% of the target data. We show that training on multiple languages is important for very low resource cross-lingual target scenarios, but not for multi-lingual testing scenarios. Here, it appears beneficial to include large well prepared datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>State-of-the-art speech recognition systems with human-like performance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> are trained on hundreds of hours of well-annotated speech. Since annotation is an expensive and time-consuming task, similar performance is typically unattainable on low resource languages. Multi-lingual or cross-lingual techniques allow transfer of models or features from well-trained scenarios to those where large amounts of training data may not be available, cannot be transcribed, or are otherwise hard to come by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>The standard approach is to train a context dependent Hidden Markov Model based Deep Neural Network acoustic model with a "bottleneck" layer using a frame based criterion on a large multilingual corpus <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. The network up to the bottleneck layer can be used as a language-independent feature extractor while adapting to a new language. Generating such a model requires the preparation of frame level segmentation in each language, which is usually achieved by training separate mono-lingual systems first. This is a cumbersome multi-step process. Moreover, if the speaking style, acoustic quality, or linguistic properties of the recordings are very different across a set of languages, the segmentations may be inconsistent across languages and thus sub-optimal for generating features in a new language.</p><p>On the other hand, end-to-end training approaches which directly model context independent phones are elegant, and greatly facilitate speech recognition training. Most do not require an explicit alignment of transcriptions with the training data, and there are typically fewer hyper-parameters to tune. We show that sequence training in multi-lingual settings can create feature extractors, which can directly be ported to new languages using a linear transformation (on very limited data), or re-trained on more data, opening a door to end-to-end language universal speech recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK AND BABEL DATASET</head><p>Some of the early works in multi-lingual and cross-lingual speech recognition involved the use of language independent features like articulatory features <ref type="bibr" target="#b7">[8]</ref> to train HMM based systems. Authors in <ref type="bibr" target="#b8">[9]</ref> used subspace Gaussian mixture model to map phonemes of different languages together. Authors in <ref type="bibr" target="#b9">[10]</ref> introduce the use of a shared phone set to build HMM based language independent acoustic models and show the adaptation of pre-existing models towards a new language. With the on-set of deep learning the focus of the models shifted to learning features across languages which can be mapped to the same space <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>. Authors in <ref type="bibr" target="#b11">[12]</ref> looked at unsupervised pretraining on different languages for a cross lingual recognition. The dominant architecture for multi-lingual or cross-lingual speech recognition has been the so-called "shared hidden layer" model, in which data is passed through a series of shared feed-forward layers, before being separated into multiple language-specific softmax layers, which are trained using cross-entropy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref>. This architecture can also be used as a "bottleneck" feature extractor, from which "language independent" features are extracted, on top of which a target-language acoustic model can be built. Authors in <ref type="bibr" target="#b14">[15]</ref> showed that these multilingual models can be adapted to the specific language to improve performance further. The work by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16]</ref> presented bottleneck features for multi-lingual systems where they showed feature porting is possible and gave competitive results when compared to systems with mono-lingual features. Other approaches <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref> constructed a shared language independent phone set, which could then also be adapted to the target language. Our proposed model is inspired by the former approach which tries to learn latent features by sharing hidden layers across languages.</p><p>Connectionist Temporal Classification (CTC, <ref type="bibr" target="#b19">[19]</ref>) lends itself to low-resource multi-lingual experiments, because systems built on CTC tend to be significantly easier to train than those that have been trained using hidden Markov models <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>. <ref type="bibr" target="#b22">[22]</ref> shows that multi-lingual CTC systems with shared phones can improve performance in a limited data setting. As per our knowledge there has not been any prior work that have looked into learning "bottleneck" like features for a CTC based model and seen how it performs multilingually and cross-lingually with adaptation.</p><p>For this paper we use several languages from IARPA's Babel<ref type="foot" target="#foot_0">1</ref> project to test our model. These are mostly telephony (8kHz) conversational speech data in a low resource language. These were accompanied by a lexicon and dictionary in Extended Speech Assessment Methods Phonetic Alphabet (X-SAMPA) format. Table <ref type="table" target="#tab_0">1</ref> summarizes the amount of training data in hours along with the number of phonemes (including the CTC blank symbol) present for the languages we used in our experiments on the "Full Language Pack" (FLP) condition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MULTI-LINGUAL CTC MODEL</head><p>A model trained with CTC loss is a sequence based model which automatically learns alignment between input and output by introducing an additional label called the blank symbol (∅), which corresponds to 'no output' prediction. Given a sequence of acoustic fea-tures X = (x1, . . . , xn) with the label sequence z = (z1, . . . , zu), the model tries to maximize the likelihood of all possible CTC paths p = (p1, . . . , pn) which lead to the correct label sequence z after reduction. A reduced CTC path is obtained by grouping the duplicates and removing the ∅ (e.g. B(AA∅AABBC) = AABC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P (z|X) =</head><p>p∈CTC Path(z)</p><formula xml:id="formula_0">P (p|X)</formula><p>Like in <ref type="bibr" target="#b20">[20]</ref> we use this loss along with stacked Bidirectional LSTM layers to encode the acoustic information and make frame-wise predictions.</p><p>In our CTC multi-lingual model, we share the bidirectional LSTM encoding layer till the final layer and project the learned embedding layer to the phones of the respective target languages. The intuition behind this model is that training on more than one language will help in better regularization of weights and learning a better representation of features, as it will be trained on more data. We hypothesize that the final phoneme discrimination can be learned in a linear projection of the last layer. Figure <ref type="figure" target="#fig_0">1</ref> shows the schematic diagram of our multi-lingual model. Mathematically this can be written as,</p><formula xml:id="formula_1">X = {XL1 ∪ XL2 ∪ XL3 . . . XLn} XLi = (x 1 Li , . . . , x n Li ) e = EncoderBiLST M (X) e ∈ R n×2 * h dim P (p|X) =          softmax(WL1e + bL1) if X ∈ XL1 softmax(WL2e + bL2) if X ∈ XL2 . . . softmax(WLne + bLn) if X ∈ XLn</formula><p>Unlike <ref type="bibr" target="#b4">[5]</ref>, we do not have any bottleneck layer, and the whole model is sequence trained based on CTC loss. Note that here we recognize a sequence of phonemes which is a much harder problem. Traditional HMM/DNN systems perform frame-wise recognition of individual phonemes, usually relying on alignments that have been generated by mono-lingual models. This can be considered a much simpler task than the recognition of a phone sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND OBSERVATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-lingual CTC model</head><p>To align with project goals, we chose to perform experiments on a set of four languages which are the closest/ have maximum phone overlap with Kurmanji -Kazakh, Turkish, Mongolian and Haitian. We used a 6-layer bidirectional LSTM network with 360 cells in each direction, which performed best on average across the majority of Babel languages in a systematic search experiment. Table <ref type="table" target="#tab_1">2</ref> shows the results. For consistency, we used absolutely identical settings across all languages, and did not perform any language-specific tuning, other than choosing the lowest perplexity language model between 3-gram and 4-gram models for WFST-based decoding. Techniques such as blank scaling and applying a softmax temperature can often improve results significantly, but we did not apply any of them here for consistency.</p><p>In our multi-lingual experiments, we use the same 6-layer Bi-LSTM network with 360 cells (per direction) in each layer as our shared encoded representation<ref type="foot" target="#foot_1">2</ref> . Again, this setup performed best on average on a larger set of languages. Multi-lingual training on As expected, reductions in the error rates tend to be higher for the lower resource languages, like Kazakh and Mongolian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Data Selection</head><p>Given that adding a seemingly unrelated, but high resource language improved the performance of the model on four low resource languages, we further studied the impact of varying the source(s) of the extra data. Specifically, we replaced the 300 h Switchboard corpus with four more unrelated Babel languages, "BAB300" composed of Tamil, Amharic, Pashto, and Tagalog. The results on the test data are summarized in Table <ref type="table" target="#tab_2">3</ref>. We can see that adding Switchboard data outperforms adding more unrelated Babel languages. While our main goal here has been the creation of a multi-lingual recognizer, we verified that models that have been trained on a single Babel language plus 300 h of Switchboard do not outperform the fine-tuned MLing+SWBD system, while there is no clear pattern on other languages. This indicates that it is generally beneficial to train (sequence-based) multi-lingual systems on closely related languages, and/or on large amounts of well-prepared but unrelated mono-lingual data, but that adding a large number of languages may in fact prevent the model from training well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Representation Learning</head><p>In order to study to what extent the CTC sequence models have learned useful bottleneck like discriminatory audio features that are independent of the input language, we attempt to port a model to an unseen language. We aim to use the trained model as a languageindependent feature extractor that can linearly separate any language into a phoneme sequence. To do this, we replace the softmax layer (or "layers" in the multi-lingual case) of a "donor" CTC model with a single softmax, which we then train with varying amounts of data from the target language, Kurmanji in our case. Figure <ref type="figure" target="#fig_1">2</ref> shows how different "donor" models behave in this situation. In the crosslingual case, it becomes beneficial to train the LSTM layers with as many different languages as possible ("MLing+BAB300" outperforms "MLing+SWBD" and "MLing"), while a single related language (Turkish) outperforms adaptation on a larger amount of data from an unrelated language (SWBD). There is a large gap between mono-lingual systems and multi-lingual systems. Improvements become smaller once training is performed on 4 h (10%) of data or more, but even then the re-estimation of the softmax layer (with ca. 32k parameters) benefits from more data. It thus seems that multi-lingual systems do indeed learn a portable, language independent representation, which is useful when porting to a new language, while the sheer amount of data is less beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Cross-lingual Explorations</head><p>Figure <ref type="figure" target="#fig_4">4</ref> shows that for both related and unrelated languages, a multilingual system surpasses the mono-lingual baseline once about 25% of the original data has been seen. The behavior of retraining ("full network adaptation") seems independent of the original trained languages.</p><p>To further investigate how multi-lingual models can be used in cross-lingual settings, and with varying amounts of training data, we  compare "softmax" adaptation and full network adaptation (retraining) on Kurmanji and Swahili, two languages which we did not see in training. We use the (MLing + SWBD) and (MLing + BAB300) "donor" models. Figure <ref type="figure" target="#fig_3">3</ref> shows that for small amounts of adaptation data, and a target language that is related to the pre-trained languages (Kurmanji), "softmax adaptation" is competitive, and an initialization with many languages is beneficial. When the entire network can be retrained ("full network adaptation", shown on the right side of Figure <ref type="figure" target="#fig_3">3</ref>), there is very little difference between the "donor" systems' performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we demonstrate that it is possible to train multi-lingual and cross-lingual acoustic models directly on phone sequences, rather than frame-level state labels. Unlike multi-lingual bottleneck features, these CTC models do not require the generation of state alignments, which facilitates their use.</p><p>In multi-lingual settings, it seems beneficial to train on related languages only, or on large amounts of clean data; there is no benefit simply from training on many languages. It is thus possible to combine e.g. Switchboard and Babel data.</p><p>In very low resource cross-lingual scenarios, it is possible to adapt a model to a previously unseen language by re-training the softmax layer only. CTC models can learn a language independent representation at the input to the softmax layer. We find that training the models trained on related languages help, as does training on many languages, rather than large amounts of data. As more and more data is available, and the whole network can be retrained, and the effect of the choice of language for the multi-lingual training disappears.</p><p>As future work, we are investigating on decoding the CTC output using a phoneme based neural language models trained on nonparallel text, thereby facilitating us to do zero-resource speech recognition. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number OCI-1053575. Specifically, it used the Bridges system, which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head><p>We are grateful to Anant Subramanian and Soumya Wadhwa for their feedback on the presentation of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Multi-lingual CTC model following the "shared hidden layer" approach for LSTM layers.</figDesc><graphic url="image-1.png" coords="1,315.21,444.64,243.77,243.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Cross-lingual training of CTC softmax layer only on top of different "donor" models.</figDesc><graphic url="image-2.png" coords="3,315.21,345.37,243.78,182.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Adaptation of softmax layer only for Kurmanji and Swahili targets. Kurmanji performs well, because the language is similar to some training languages. (b) Adaptation of entire network (re-training) to target languages. This outperforms softmax adaptation (on the left) as soon as 2-4 h of data become available.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Cross-lingual training of Kurmanji and Swahili systems.</figDesc><graphic url="image-3.png" coords="4,72.09,81.96,234.61,175.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. PER on different amounts of cross-lingual data using a full network end-to-end adaptation (retraining).</figDesc><graphic url="image-5.png" coords="4,54.43,412.33,243.78,182.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>This project was sponsored by the Defense Advanced Research Projects Agency (DARPA) Information Innovation Office (I2O), program: Low Resource Languages for Emergent Incidents (LORELEI), issued by DARPA/I2O under Contract No. HR0011-15-C-0114.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Overview of the FLP Babel Corpora used in this work.</figDesc><table><row><cell>Subset</cell><cell>Language</cell><cell cols="2"># Phones + ∅ Training Data</cell></row><row><cell></cell><cell>Turkish</cell><cell>50</cell><cell>79 hrs</cell></row><row><cell>MLing</cell><cell>Haitian</cell><cell>40</cell><cell>67 hrs</cell></row><row><cell></cell><cell>Kazakh</cell><cell>70</cell><cell>39 hrs</cell></row><row><cell></cell><cell cols="2">Mongolian 61</cell><cell>46 hrs</cell></row><row><cell></cell><cell>Amharic</cell><cell>67</cell><cell>43 hrs</cell></row><row><cell cols="2">Bab300 Tamil</cell><cell>41</cell><cell>69 hrs</cell></row><row><cell></cell><cell>Tagalog</cell><cell>48</cell><cell>85 hrs</cell></row><row><cell></cell><cell>Pashto</cell><cell>54</cell><cell>78 hrs</cell></row><row><cell>For</cell><cell>Kurmanji</cell><cell>45</cell><cell>42 hrs</cell></row><row><cell>testing</cell><cell>Swahili</cell><cell>40</cell><cell>44 hrs</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Word (% WER) and phoneme error rate (% PER) for each of the test languages, on the Babel conversational development test sets. the "MLing" set (the four languages shown in Table2) improves WER by 1.7% (absolute) on average, while keeping the LSTM layers shared across all languages. If we fine-tune the entire model towards each language specifically, performance improves further, by 4.4% on average over the baseline. If we roughly double the amount of training data by adding the Switchboard 300h training set to the</figDesc><table><row><cell>Model</cell><cell cols="2">Kazakh</cell><cell cols="2">Turkish</cell><cell cols="2">Haitian</cell><cell cols="2">Mongolian</cell></row><row><cell></cell><cell cols="8">WER PER WER PER WER PER WER PER</cell></row><row><cell>Mono-lingual</cell><cell>55.9</cell><cell>40.9</cell><cell>53.1</cell><cell>36.2</cell><cell>49.0</cell><cell>36.9</cell><cell>58.2</cell><cell>45.2</cell></row><row><cell>Multi-lingual (MLing)</cell><cell>53.2</cell><cell>36.5</cell><cell>52.8</cell><cell>34.4</cell><cell>47.8</cell><cell>34.9</cell><cell>55.9</cell><cell>41.1</cell></row><row><cell>MLing &amp; FineTuning (FT)</cell><cell>50.6</cell><cell>35.1</cell><cell>49.0</cell><cell>32.2</cell><cell>46.6</cell><cell>33.2</cell><cell>53.4</cell><cell>39.6</cell></row><row><cell>MLing + SWBD</cell><cell>52.3</cell><cell>36.6</cell><cell>51.3</cell><cell>33.0</cell><cell>45.8</cell><cell>33.9</cell><cell>54.5</cell><cell>40.2</cell></row><row><cell>MLing + SWBD &amp; FT</cell><cell>48.2</cell><cell>33.5</cell><cell>48.7</cell><cell>31.9</cell><cell>44.3</cell><cell>31.9</cell><cell>51.5</cell><cell>37.8</cell></row></table><note>"MLing" training data, performance improves yet again, for both the universal (MLing+SBWD) and language-specific (MLing+SWBD &amp; FT) case. Overall, WER and PER improve by about 6% absolute (&gt;10% relative), which is in line with other results reported on comparable tasks discussed in section 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Word error rate (% WER) on the test languages when switching the SWBD data with 300 hrs equivalent of Babel.</figDesc><table><row><cell>Model</cell><cell cols="4">Kazakh Turkish Haitian Mongolian</cell></row><row><cell>MLing + BAB300</cell><cell>57.5</cell><cell>52.0</cell><cell>47.8</cell><cell>56.7</cell></row><row><cell>MLing + SWBD</cell><cell>52.3</cell><cell>51.3</cell><cell>45.8</cell><cell>54.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">This work used releases IARPA-babel105b-v0.4, IARPA-babel201b-v0.2b, IARPA-babel401b-v2.0b, IARPA-babel302b-v1.0a (these 4 languages will be called the "MLing" set), and IARPA-babel106b-v0.2g, IARPA-babel307b-v1.0b, IARPA-babel204b-v1.1b, IARPA-babel104b-v0.4bY (these 4 languages will be called the "BAB300" set), and IARPA-babel202b-v1.0d and IARPA-babel205b-v1.0 for testing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The code to train the multi-lingual model will be released as part of EESEN<ref type="bibr" target="#b20">[20]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">English conversational telephone speech recognition by humans and machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kurata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-L</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02136</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Achieving human parity in conversational speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05256</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-domain and cross-language portability of acoustic features estimated by multilayer perceptrons</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grezl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vergyri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2006">2006. 2006. 2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="I" to="I" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Study of large data resources for multilingual training and system porting</title>
		<author>
			<persName><forename type="first">F</forename><surname>Grézl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Egorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The language-independent bottleneck features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Veselỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grézl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Janda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Egorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="336" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Investigation of multilingual deep neural networks for spoken term detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Gales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="138" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilingual bottle-neck features and its application for under-resourced languages</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Workshop on Spoken Language Technologies for Under-resourced Languages, Cape Town; S. Africa</title>
				<meeting>3rd Workshop on Spoken Language Technologies for Under-resourced Languages, Cape Town; S. Africa<address><addrLine>MICA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Integrating multilingual articulatory features into speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stuker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth European Conference on Speech Communication and Technology</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multilingual acoustic modeling for speech recognition based on subspace Gaussian mixture models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Akyazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4334" to="4337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language-independent and language-adaptive acoustic modeling for speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="51" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multilingual training of deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="7319" to="7323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual knowledge transfer in dnn-based lvcsr</title>
		<author>
			<persName><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="246" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the use of a multilingual neural network front-end</title>
		<author>
			<persName><forename type="first">S</forename><surname>Scanzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laface</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fissore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gemello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilingual acoustic models using distributed deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="8619" to="8623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptation of multilingual stacked bottle-neck neural network structure for new language</title>
		<author>
			<persName><forename type="first">F</forename><surname>Grézl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="7654" to="7658" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Study of probabilistic and bottle-neck features in multilingual environment</title>
		<author>
			<persName><forename type="first">F</forename><surname>Grézl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Janda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on</title>
		<imprint>
			<biblScope unit="page" from="359" to="364" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Investigation of Deep Neural Networks for Multilingual Speech Recognition Training and Adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<idno>num- ber EPFL-CONF-229214</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
				<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilingual deep neural network based acoustic modeling for rapid language adaptation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Imseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="7639" to="7643" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
				<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An empirical exploration of CTC acoustic models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2623" to="2627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Language Adaptive Multilingual CTC Speech Recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPECOM</title>
				<meeting>SPECOM<address><addrLine>Hatfield, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
