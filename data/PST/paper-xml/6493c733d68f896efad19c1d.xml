<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Weihao</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhuo</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Niu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fuju</forename><surname>Rong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chucheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenze</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daimin</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenjie</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaoyi</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">National Health Commission Capacity Building and Continuing Education Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenbin</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Beijing Tongren Eye Center</orgName>
								<address>
									<settlement>Beijing Tongren Hospital</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lan</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large multimodal language models (LMMs) have achieved significant success in general domains. However, due to the significant differences between medical images and text and general web content, the performance of LMMs in medical scenarios is limited. In ophthalmology, clinical diagnosis relies on multiple modalities of medical images, but unfortunately, multimodal ophthalmic large language models have not been explored to date. In this paper, we study and construct an ophthalmic large multimodal model. Firstly, we use fundus images as an entry point to build a disease assessment and diagnosis pipeline to achieve common ophthalmic disease diagnosis and lesion segmentation. Then, we establish a new ophthalmic multimodal instruction-following and dialogue fine-tuning dataset based on disease-related knowledge data and publicly available real-world medical dialogue. We introduce visual ability into the large language model to complete the ophthalmic large language and vision assistant (OphGLM). Our experimental results demonstrate that the OphGLM model performs exceptionally well, and it has the potential to revolutionize clinical applications in ophthalmology. The dataset, code, and models will be made publicly available at https://github.com/ML-AILab/OphGLM .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The web sources data like web images and their associated captions are abundantly available in recent years. This kind of parallel image-text data is of great potential to be applied in self-supervised vision-language modeling. As demonstrated by multimodal GPT-4 <ref type="bibr" target="#b14">[15]</ref> and open-sourced efforts such as LLaVA <ref type="bibr" target="#b10">[11]</ref>, generative pretraining has effectively leveraged their performance. In attempting to improve the model's knowledge of general facts based on * * = equal contribution, ? = corresponding author multimodal input, several large-scale multimodal models (LMMs) successfully show strong zero-shot task completion performance. The application of LLMs in various useroriented visual-language tasks (such as image understanding and reasoning) has shown their great potential of developing universal multimodal conversation assistants.</p><p>Although LLMs have succeeded in the general domain, their performance is limited in biomedical scenarios. Since biomedical image-text pairs are quite different from general web content, the LLMs trained with web source data may have problems dealing with professional conversations. Therefore, a visual assistant in the general domain may behave like a non-specialist and be able to answer biomedical questions precisely or produce totally incorrect answers or false facts. Though much progress has been made in biomedical visual question answering (VQA), previous methods often formalize questions into classification (e.g., their answers are limited to ones observed in the training set) and are not suitable for open-ended guidance following. Meanwhile, current research is often based on single-modal text data rather than multimodal image-text data. Therefore, although conversational AI has demonstrated great potential in biomedical applications <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b13">14]</ref>, more effort is required in this field.</p><p>Ophthalmology is a discipline that relies on multimodal diagnosis with various ophthalmology images, including fundus images, OCT, ultra-widefield fundus images, and fundus fluorescence angiography (FFA) images. However, currently, it lacks an available multimodal medical dialogue system constructed based on large-scale language models to assist the diagnosis progress. In this study, we attempt to construct a ophthalmology large language-and-vision assistant (OphGLM) with fundus images as the entry point. With this LLM, we can further explore the possibility of developing universal multimodal conversation assistants of good performance for biomedical and other scenarios.</p><p>Our contributions can be summarized as follows: arXiv:2306.12174v2 [cs.CV] 22 Jun 2023</p><p>? We use knowledge graphs and real-world medical dialogues to build an instructions and dialogue fine-tuning dataset for ophthalmic diseases using ChatGPT. This improves the authenticity and usability of LLM in medical question-answering in the healthcare field.</p><p>? Based on fundus images from public datasets, we construct a computer vision model for common disease diagnosis, which classifies diabetic retinopathy, agerelated macular degeneration, pathological myopia, and glaucoma. In addition, we also develop a diabetic retinopathy grading model, a diabetic retinopathy lesion segmentation model, and an ophthalmic rare disease fundus diagnosis model, which serve as disease diagnosis models.</p><p>? We propose a novel ophthalmology large languageand-vision assistant (OphGLM). To the best of our knowledge, it is the first attempt to combine visual models with large language models in ophthamology.</p><p>The experimental results demonstrate our OphGLM has the great potential in ophthalmology clinical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Biomedical Chatbots</head><p>Large Language Models (LLMs) have achieved immense success in the general domain, with ChatGPT <ref type="bibr" target="#b14">[15]</ref> being one of the most representative instances. Moreover, certain advancements have been made in research concerning the open-source fine-tuning of these large language models. Inspired by these successful cases in LLMs, a series of biomedical large language models have been successively introduced, including ChatDoctor <ref type="bibr" target="#b23">[24]</ref>, Med-Alpaca <ref type="bibr" target="#b3">[4]</ref>, PMC-LLaMA <ref type="bibr" target="#b21">[22]</ref>, DoctorGLM <ref type="bibr" target="#b22">[23]</ref>, and Huatuo <ref type="bibr" target="#b20">[21]</ref>.Typically, these models start with pre-trained opensource LLMs and are then fine-tuned on domain-specific biomedical instruction-following datasets. The fine-tuned LLMs exhibit considerable application potential within specific biomedical fields, as they are able to perform a variety of tasks including patient dialogue, provision of diagnostic and treatment recommendations, and explanation of medical knowledge.</p><p>HuatuoGPT is a biomedical chatbot, incorporating distilled data generated by ChatGPT along with real-world physician response data. Its consistency and reliability have been validated both in automatic and human evaluations. However, HuatuoGPT only accepts text inputs, significantly limiting its application scenarios. Currently, multimodal biomedical chatbots capable of accepting image inputs are scarce, with Visual Med-Alpaca <ref type="bibr" target="#b16">[17]</ref> and LLaVA-Med <ref type="bibr" target="#b7">[8]</ref> being the only two known. Visual Med-Alpaca can accept image inputs and generate text dialogue out-puts. Specifically, Visual Med-Alpaca processes input images through various image recognition models, combining the obtained visual information with textual prompts, enabling the model to generate appropriate responses. However, Visual Med-Alpaca's training on a limited biomedical dataset presents certain restrictions. Recently, Microsoft introduced LLaVA-Med, an end-to-end neural network model that processes images directly without the need for separate medical visual models. Furthermore, LLaVA-Med conducts experiments on a more diverse and larger dataset, demonstrating superior generalization and effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Biomedical Visual Question Answering</head><p>The development of large models capable of autonomously generating answers to questions based on biomedical image inputs would greatly benefit physicians and patients alike, bearing significant implications for the enhancement of clinical diagnostic and treatment efficiency. Existing approaches to biomedical visual question answering principally fall into two categories: discriminative and generative methods.For discriminative methods, Visual Question Answering (VQA) is treated as a classification problem where the model seeks an answer from a predefined answer set <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref>. Although this method can yield satisfactory performance, it operates within the constraints of a finite predefined answer dataset, thus requires adjustments or retraining when presented with a new or custom set of answers. Consequently, discriminative methods are not suited to the development of a biomedical chatbot capable of addressing open-ended queries. To counter this limitation, researchers have pioneered generative methods that predict answers in the form of unrestricted text sequences <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>. Generative methods not only accommodate custom answer sets, but also offer increased flexibility and yield more accurate responses. A novel data generation method was proposed in <ref type="bibr" target="#b7">[8]</ref>, where GPT-4 was employed to generate multimodal instruction-following data pairs using medical image-text pairs from the publicly available dataset in PubMed Central <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model Architecture</head><p>There are mainly two methods for fine-tuning Large Language Models (LLMs) through biomedical visual questionanswering dialogues. One approach involves individually fine-tuning the LLM on a medical dialogue dataset. The other method is the prefix tuning of language models (LMs), where a new trainable module connects the frozen image encoder and the causal LM, allowing for standard supervised fine-tuning of the entire model <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>. In <ref type="bibr" target="#b18">[19]</ref>, a threelayer MLP network is employed to map visual features to the visual prefix. Furthermore, <ref type="bibr" target="#b18">[19]</ref> incorporates a variety of pretrained LMs including GPT2-XL <ref type="bibr" target="#b15">[16]</ref>, BioMedLM <ref type="bibr" target="#b19">[20]</ref>, and BioGPT <ref type="bibr" target="#b12">[13]</ref>, with sizes ranging from 1.5B to 2.7B. Conversely, <ref type="bibr" target="#b7">[8]</ref> employs linear projection alongside a 7B LM <ref type="bibr" target="#b17">[18]</ref>. In our model, ChatGLM <ref type="bibr" target="#b24">[25]</ref> was used as the LLM and was fine-tuned on an ophthalmology dialogue dataset. ChatGLM is an open-source dialogue language model that supports both English and Chinese conversations, boasting 6.2 billion parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Ophthalmology Dataset</head><p>Based on fundus medical imaging, the diagnosis of diseases can be obtained. In this study, we are currently able to provide diagnostic results for diabetic retinopathy, agerelated macular degeneration, pathological myopia, glaucoma, and intraocular tumors, as well as lesion segmentation and staging for diabetic retinopathy. In order to maximize LLM's question-answering performance in specific diseases, we have devised two strategies to improve its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Instruction Data Construction</head><p>Based on the open database crawler technology, we have constructed a large-scale medical knowledge graph with more than 100,000 nodes from Wikipedia, Dingxiangyuan and other sources. At the downstream of the Knowledge graph, we build the api parameters based on medical scenarios. Specifically, we construct instruction fine-tuning datasets based on five different scenarios, as is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. The knowledge based prompt example to create instructions is shown in appendix Fig. <ref type="figure" target="#fig_6">4</ref>.</p><p>Medical Imaging Description: About the basic description of disease classification, grading, and lesion based on medical imaging.</p><p>Causes and symptoms: Information about the symptoms of a disease.</p><p>Diagnosis and examination: How to diagnose and examine a particular disease, including common examination and testing methods.</p><p>Treatment and prevention: How to treat and prevent a particular disease, including medication, surgical treatment, rehabilitation, and other aspects of treatment.</p><p>Prognosis and lifestyle: The prognosis of a disease and how to alleviate symptoms or prevent a disease by changing lifestyle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Medical Conversation Construction</head><p>Instructions based on disease diagnosis and knowledge graph construction can be built around various scenarios of specific diseases using ChatGPT, leveraging additional knowledge to enable smaller LLMs to provide more realistic and robust medical Q&amp;A. However, the actual situation may be more complex.</p><p>The construction of knowledge instructions based on knowledge graphs cannot reflect the true concerns of patients. For example, patients generally have concerns about the heritability of the disease, medical costs, and efficacy. Instruction based fine-tuning can improve the authenticity of specific problems, but it cannot provide a better interactive experience.</p><p>In order to provide a dataset that is closer to real Q&amp;A scenarios and to create a better interactive experience, we proposed a dialogue prompt strategy based on real doctorpatient conversations. First, we extracted real conversations about ophthalmic diseases from the MedDialog dataset <ref type="bibr" target="#b1">[2]</ref>, which contains genuine doctor-patient dialogues, as material. Then, we designed a set of prompts to enable Chat-GPT to play the role of a medical expert, extracting patients' intentions from publicly available real doctor-patient dialogues and providing as professional and detailed medical explanations as possible. This approach not only enables the extraction of patients' intentions from real conversations, but also creates a more suitable dialogue dataset for patients.</p><p>Our process for building the fine-tuned fundus dialog dataset is illustrated in Fig. <ref type="figure" target="#fig_3">2</ref>. In step 1, prompts are created using both real-world medical-patient conversations and knowledge graphs. In step 2, medical knowledge based instructions and conversation are created using the Chat-GPT interface. In step 3, data cleaning is performed to create instances. In step 4, duplicate data is removed by validating against existing datasets. In step 5, instance quality is evaluated using both manual review and GPT4. Finally, new instruction and conversation are added to fundus dialog pool.The construction of such a fine-tuned dataset not only enhances the authenticity and accuracy of LLM in medical knowledge but also creates a more friendly doctor-patient dialogue mechanism. The conversation based prompt example to create patient friendly conversation is shown in appendix Fig. <ref type="figure" target="#fig_7">5</ref>.</p><p>Using these two strategies, we have constructed a Fundus Dialog Dataset consisting of over 20k instances related to ophthalmic diseases. Our experiments have shown that fine-tuning this dataset with ChatGLM can effectively improve the authenticity of disease-related Q&amp;A downstream from fundus medical imaging diagnosis. The finetuned OphGLM model possesses medical knowledge richness close to that of ChatGPT and can provide a more patient-friendly experience. Although the relevant data volume is not large, significant improvement has been achieved in LLM's factual answering ability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different Scenarios</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causes and symptoms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagnosis and examination</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Treatment and prevention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prognosis and lifestyle</head><p>Medical Imaging description: About the basic description of disease classification, grading, and lesion based on medical imaging.</p><p>Causes and symptoms: Information about the symptoms of a disease.</p><p>Diagnosis and examination: How to diagnose and examine a particular disease, including common examination and testing methods.</p><p>Treatment and prevention: How to treat and prevent a particular disease, including medication, surgical treatment, rehabilitation, and other aspects of treatment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prognosis and lifestyle:</head><p>The prognosis of a disease and how to alleviate symptoms or prevent a disease by changing lifestyle. method is illustrated in Fig. <ref type="figure" target="#fig_5">3</ref>. Within the fundus diagnosis pipeline, there are two primary components: the fundus image classification segment and the fundus image segmentation segment. These segments work together to extract information from the input fundus images. The extracted visual information is integrated into structured text templates, forming diagnostic reports based on the input fundus images. As for the OphGLM pipeline, it merges the fundus image diagnostic report with the fundus dialogue through a process of text concatenation. This forms a prompt that is then input into the OphGLM, ultimately generating a highquality response.  they are integrated into our predetermined text template, thus forming the diagnostic report for the fundus images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medical imaging description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fundus Diagnosis Pipeline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">OphGLM Pineline</head><p>During the training process of OphGLM, we fine-tuned the base model, ChatGLM <ref type="bibr" target="#b24">[25]</ref>, using a dataset consisting of over 20k fundus dialogues related to retinal diseases. As a result, we obtained OphGLM, which demonstrated a stronger capability in both disease diagnosis and questionanswering. Within the OphGLM pipeline, the fundus image diagnostic report and the user's input of fundus dialogue undergo text concatenation. By employing our strategically designed method, a dialogue prompt is generated and fed into OphGLM, which ultimately yields a high-quality response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implement Details</head><p>We provide a detailed overview of the training details for the fundus diagnosis pipeline. Our fundus diagnosis pipeline includes diabetic retinopathy classification, glaucoma classification, pathological myopia classification, age-related macular degeneration classification, fundus tumor detection, and diabetic retinopathy lesion segmentation. The segmentation model for diabetic retinopathy lesions enables the segmentation of four types of lesions: hard exudates, soft exudates, microaneurysms, and hemorrhages. The detailed information of the datasets is provided in Table 1. In this experiment, the evaluation metric for classification tasks is accuracy(ACC), and for segmentation tasks, it is dice coefficient(DSC).The equations of ACC and DSC are as follows:</p><formula xml:id="formula_0">ACC = T P + T N T P + T N + F P + F N (1) DSC = 2 ? |X ? Y | |X| + |Y |<label>(2)</label></formula><p>For ACC equation, TP(True Positive) is the number of positive samples correctly classified as positive, TN(True Negative) is the number of negative samples correctly classified as negative, FP(False Positive) is the number of nega-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AMD class results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Glaucoma class results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Palm class results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tumor class results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EX HE MA SE segmentation results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diagnostic report</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Peformance</head><p>The performance of each model in our fundus diagnosis pipeline is shown in Table <ref type="table" target="#tab_2">2</ref>. The visual input example of a DR fundus image is shown in appendix Fig. <ref type="figure" target="#fig_8">6</ref> and some medical knowledge based Q&amp;A examples are shown in appendix Table <ref type="table" target="#tab_3">3</ref> and<ref type="table" target="#tab_4">Table 4.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have developed OphGLM, a large-scale language and vision assistant for ophthalmology based on instructions and dialogue. Our model was fine-tuned on a dataset Despite significant progress in ophthalmic visual dialogue, there is still room for improvement in our model. We will continue to conduct research and generate more high-quality image-language data to enhance the overall capabilities of the model. In addition, we plan to enrich and strengthen the upstream diagnostic models, providing multi-label disease classification and lesion annotation capabilities. Finally, we plan to incorporate other medical imaging modalities in ophthalmology, such as OCT, into the model to create a comprehensive medical-assistant system.    Additionally, glaucoma medication is just one method of controlling intraocular pressure and patients need to take other measures, such as avoiding eye strain, undergoing regular vision checks, and following their doctor's recommendations.</p><p>Pathological myopia, also known as high myopia or degenerative myopia, is a common type of myopia characterized by an excessively elongated eyeball that causes light to not focus fully on the retina, leading to retinal damage. Here are some common retinal lesions that can be seen in pathological myopia:</p><p>1. Retinal detachment: In pathological myopia, the retina may become detached due to excessive stretching and pressure, which can lead to vision loss.</p><p>2. Neovascularization: Due to the elongated eyeball in pathological myopia, new blood vessels may grow in the retina, causing malnutrition and bleeding.</p><p>3. Fissures: The elongated eyeball in pathological myopia can cause fissures in the retina, which can lead to retinal rupture and vision loss.</p><p>4. Choroidal neovascularization: Excessive stretching and pressure on the retina in pathological myopia can lead to choroidal neovascularization, a common retinal disease that can result in vision loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Leopard-spot-like vision:</head><p>Due to retinal damage caused by excessive stretching and pressure in pathological myopia, leopard-spot-like vision patterns may appear in the retina.</p><p>These are common retinal lesions associated with pathological myopia, and if you have any questions, it is recommended to consult a doctor for more detailed advice and diagnosis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>4. 1 .</head><label>1</label><figDesc>OverviewOur model, OphGLM, is a multimodal biomedical chatbot capable of accepting fundus images as input. OphGLM consists of two modules: the fundus diagnosis pipeline and the OphGLM pipeline. The overall pipeline of our proposed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Instruction fine-tuning datasets based on five different scenarios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>In the fundus diagnosis pipeline of our model, the input fundus images are processed via a disease diagnosis module and a lesion segmentation module. Within the fundus image disease diagnosis module, we employ five different disease classification models, including a grading model for diabetic retinopathy (DR), an age-related macular degeneration (AMD) classification model, a glaucoma classification model, a pathological myopia classification model, and a fundus tumor classification model. The specific functionalities of each disease classification model are as follows: The DR grading model can classify fundus images into normal, mild, non-proliferative, moderate non-proliferative, severe non-proliferative, and proliferative DR. The AMD classification model can classify fundus images into normal and AMD categories. The glaucoma classification model can classify fundus images into normal and glaucoma categories. The pathological myopia classification model can classify fundus images into normal and pathological myopia categories. The fundus tumor classification model can classify fundus images into normal and fundus tumor categories. In the fundus image lesion segmentation module, we implemented segmentation of common fundus lesions, including hard exudates (EX), soft exudates (SE), microaneurysms (MA), and hemorrhages (HE). Our visual models, based on Convolutional Neural Networks (CNN) and transformer architectures, simultaneously boast high accuracy and inference speeds. Once we have obtained the structured classification and segmentation results from these models,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The process of building the fine-tuned Fundus dialog dataset.</figDesc><graphic url="image-1.png" coords="5,50.11,72.00,495.00,257.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>During the model training process, we utilized both publicly available datasets and private datasets to train and finetune the fundus diagnosis models. Specifically, the training dataset for the diabetic retinopathy lesion classification model is sourced from the Diabetic Retinopathy Detection dataset and DDR dataset [10]. The training dataset for glaucoma classification model is obtained from REFUGE and ORIGA650. The training dataset for pathological myopia classification model is from BAIDU PALM. The training dataset for age-related macular degeneration classification model is IChallenge AMD400. The training dataset for fundus tumor classification is from a private dataset of Beijing Tongren Hospital. The diabetic retinopathy lesion segmentation dataset is sourced from DDR, IDRID, and ADAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Overall architecture of the proposed OphGLM. (a) Fundus diagnosis pipeline. (b) OphGLM fine-tuning pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Knowledge Based Prompt.</figDesc><graphic url="image-5.png" coords="9,50.11,90.72,495.01,267.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Conversation Based Prompt.</figDesc><graphic url="image-6.png" coords="9,50.11,424.88,495.01,246.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Visual Input Example.</figDesc><graphic url="image-7.png" coords="10,50.12,90.36,495.00,581.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The datasets, number of images, and evaluation metrics used for the tasks in fundus image diagnosis model</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell cols="2">Number of images Evaluation Index</cell></row><row><cell>DR classification</cell><cell>DRD(Kaggle) DDR</cell><cell>88702 13673</cell><cell>Acc Acc</cell></row><row><cell>glaucoma classification</cell><cell>REFUGE ORIGA650</cell><cell>1200 650</cell><cell>Acc Acc</cell></row><row><cell>PALM classification</cell><cell>BAIDU PALM</cell><cell>800</cell><cell>Acc</cell></row><row><cell>AMD classification</cell><cell>IChallenge AMD400</cell><cell>400</cell><cell>Acc</cell></row><row><cell>tumor classification</cell><cell>Private Dataset</cell><cell>-</cell><cell>Acc</cell></row><row><cell></cell><cell>DDR</cell><cell>757</cell><cell>Dice</cell></row><row><cell>lesion segmentation</cell><cell>IDRID</cell><cell>81</cell><cell>Dice</cell></row><row><cell></cell><cell>ADAM</cell><cell>400</cell><cell>Dice</cell></row></table><note><p>Class Fundus</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Dialog OphGLM Seg Input Image disease diagnosis models lesion segmentation models DR AMD Glaucoma Palm Tumor Text Template DR class results</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The performance of models in fundus diagnosis pipeline.</figDesc><table><row><cell>Algorithm Type</cell><cell>Algorithm</cell><cell>Acc</cell><cell>Dice</cell></row><row><cell></cell><cell>DR class model</cell><cell>0.970</cell><cell>-</cell></row><row><cell></cell><cell cols="2">Glaucoma class model 0.940</cell><cell>-</cell></row><row><cell>classification</cell><cell>PALM calss model</cell><cell>0.998</cell><cell>-</cell></row><row><cell></cell><cell>AMD class model</cell><cell>0.984</cell><cell>-</cell></row><row><cell></cell><cell>Tumor class model</cell><cell>0.999</cell><cell>-</cell></row><row><cell></cell><cell>EX seg model</cell><cell>-</cell><cell>0.854</cell></row><row><cell>segmentation</cell><cell>HE seg model MA seg model</cell><cell>--</cell><cell>0.805 0.699</cell></row><row><cell></cell><cell>SE seg model</cell><cell>-</cell><cell>0.812</cell></row><row><cell cols="4">of over 20k instructional fine-tuning and dialogue dataset,</cell></row><row><cell cols="4">carefully crafted based on real-world conversations and</cell></row><row><cell cols="4">medical knowledge. This process provided OphGLM with</cell></row><row><cell cols="4">powerful question answering abilities tailored specifically</cell></row><row><cell>for ophthalmology.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Improving the ability to answer medical basic questions</figDesc><table><row><cell></cell><cell>example 1</cell><cell>example 2</cell><cell>example 3</cell></row><row><cell>Ralated Disease</cell><cell>AMD</cell><cell>AMD</cell><cell>DR</cell></row><row><cell></cell><cell>What kind of disease is AMD?</cell><cell>Is age-related macular degenera-</cell><cell cols="2">What are the lesions in the fun-</cell></row><row><cell>Question</cell><cell></cell><cell>the incidence rate among people tion a common disease? What is</cell><cell cols="2">dus of diabetic retinopathy?</cell></row><row><cell></cell><cell></cell><cell>over 50 years old?</cell><cell></cell></row><row><cell></cell><cell>AMD is a variant of AMD, also</cell><cell>The prevalence of age-related</cell><cell cols="2">The following retinal lesions</cell></row><row><cell></cell><cell>known as Houssay-Holopigian</cell><cell>macular degeneration in people</cell><cell cols="2">may appear in the fundus images</cell></row><row><cell></cell><cell>syndrome.</cell><cell>over the age of 50 is approx-</cell><cell cols="2">of diabetic retinopathy: reti-</cell></row><row><cell>ChatGLM</cell><cell></cell><cell>imately 10% according to data</cell><cell cols="2">nal neovascularization, retinal</cell></row><row><cell></cell><cell></cell><cell>from the American Heart Asso-</cell><cell cols="2">detachment, retinal nerve fiber</cell></row><row><cell></cell><cell></cell><cell>ciation.</cell><cell cols="2">degeneration, and retinal nerve</cell></row><row><cell></cell><cell></cell><cell></cell><cell>fiber inflammation.</cell></row><row><cell></cell><cell>AMD(Age-related Macular De-</cell><cell>Age-related Macular Degenera-</cell><cell cols="2">Diabetic retinopathy is a disease</cell></row><row><cell></cell><cell>generation) is a type of retinal</cell><cell>tion (AMD) is a common mac-</cell><cell cols="2">caused by damage to the reti-</cell></row><row><cell></cell><cell>disease which mainly manifests</cell><cell>ular disease that typically affects</cell><cell cols="2">nal blood vessels due to diabetes,</cell></row><row><cell></cell><cell>as lesions in the macular region</cell><cell>people over the age of 50. Ac-</cell><cell cols="2">and is common in people with</cell></row><row><cell></cell><cell>of the retina. It is a chronic and</cell><cell>cording to the World Health Or-</cell><cell cols="2">diabetes. Diabetic retinopathy</cell></row><row><cell></cell><cell>progressive disease that usually</cell><cell>ganization, the global prevalence</cell><cell cols="2">mainly includes the following le-</cell></row><row><cell></cell><cell>occurs with increasing age, and</cell><cell>of AMD significantly increases</cell><cell>sions:</cell></row><row><cell></cell><cell>is more common in people over 60 years old.</cell><cell>with age, particularly in Asia and South America. According to the International Age-Related Macular Degeneration Founda-tion's data in 2019, the global</cell><cell cols="2">1. Microvascular abnormal-ities, including microa-neurysms, narrowing, and abnormal tortuosity.</cell></row><row><cell></cell><cell></cell><cell>prevalence of AMD is around</cell><cell cols="2">2. Hard exudates: due to</cell></row><row><cell></cell><cell></cell><cell>20.6%, with a rate of about 30%</cell><cell cols="2">damage to the retinal cap-</cell></row><row><cell></cell><cell></cell><cell>for those aged over 50. However,</cell><cell cols="2">illaries and neurons, fluid,</cell></row><row><cell></cell><cell></cell><cell>the prevalence may vary in dif-</cell><cell cols="2">protein, and other sub-</cell></row><row><cell></cell><cell></cell><cell>ferent countries and regions de-</cell><cell cols="2">stances in the blood will</cell></row><row><cell></cell><cell></cell><cell>pending on factors such as popu-</cell><cell cols="2">penetrate into the retinal</cell></row><row><cell></cell><cell></cell><cell>lation structure, lifestyle habits,</cell><cell cols="2">area, forming hard exu-</cell></row><row><cell>OphGLM</cell><cell></cell><cell>and medical conditions. AMD</cell><cell>dates.</cell></row><row><cell></cell><cell></cell><cell>is a common macular disease</cell><cell cols="2">3. Non-proliferative diabetic</cell></row><row><cell></cell><cell></cell><cell>that can lead to vision loss and</cell><cell>retinopathy:</cell><cell>including</cell></row><row><cell></cell><cell></cell><cell>visual impairment. To prevent</cell><cell cols="2">microvascular embolism,</cell></row><row><cell></cell><cell></cell><cell>AMD, people over the age of</cell><cell>punctate</cell><cell>hemorrhage,</cell></row><row><cell></cell><cell></cell><cell>50 are advised to take active</cell><cell cols="2">exudation and crystalline</cell></row><row><cell></cell><cell></cell><cell>measures such as regular physi-</cell><cell>exudation.</cell></row><row><cell></cell><cell></cell><cell>cal examinations, maintaining a healthy lifestyle, and protecting their eyes.</cell><cell cols="2">4. Proliferative retinopathy: neovascularization diabetic including and</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">scar tissue, these lesions</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">may extend into the vitre-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ous cavity and may cause</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">retinal detachment.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Diabetic retinopathy has vari-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ous types of lesions and degrees</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">of severity.If you have diabetes,</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">please pay close attention to eye</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">health, undergo regular eye ex-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ams, and follow your doctor's</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">advice for treatment and preven-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">tive measures, such as control-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">ling blood sugar, blood pressure,</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">and lipids. Early detection and</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">treatment of lesions can prevent</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">or slow down their progression</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">and reduce the risk of eye com-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>plications.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Improving the ability to answer medical basic questions</figDesc><table><row><cell></cell><cell>example 4</cell><cell>example 5</cell><cell>example 6</cell></row><row><cell>Ralated Disease</cell><cell>DR</cell><cell>Glaucoma</cell><cell>PM</cell></row><row><cell></cell><cell>How can I prevent further pro-</cell><cell>What medications can be used to</cell><cell>What are the lesions of patho-</cell></row><row><cell>Question</cell><cell>gression of diabetic retinopathy</cell><cell>treat or slow the progression of</cell><cell>logic myopia in fundus images?</cell></row><row><cell></cell><cell>if I already have diabetes?</cell><cell>glaucoma?</cell><cell></cell></row><row><cell></cell><cell>To prevent diabetic retinopathy,</cell><cell>Medications used to treat glau-</cell><cell>The fundus images of pathologi-</cell></row><row><cell></cell><cell>it is necessary to control diet and</cell><cell>coma include Kaiyipin and other</cell><cell>cal myopia may show retinal de-</cell></row><row><cell></cell><cell>exercise, and regularly monitor</cell><cell>drugs, which can reduce the ac-</cell><cell>tachment, choroidal detachment,</cell></row><row><cell>ChatGLM</cell><cell>blood sugar levels.</cell><cell>and lower the risk of glaucoma cumulation of aqueous humor</cell><cell>and retinal neovascularization.</cell></row><row><cell></cell><cell></cell><cell>by lowering intraocular pressure</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(IOP) and promoting the outflow</cell><cell></cell></row><row><cell></cell><cell></cell><cell>of aqueous humor.</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vision-language model for visual question answering in medical imagery</title>
		<author>
			<persName><forename type="first">Yakoub</forename><surname>Bazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamad</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Rahhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laila</forename><surname>Bashmal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mansour</forename><surname>Zuair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioengineering</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">380</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Shu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqian</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongchao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruisi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penghui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03329</idno>
		<title level="m">Meddialog: a large-scale medical dialogue dataset</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pubmedclip: How much does clip benefit visual question answering in the medical domain?</title>
		<author>
			<persName><forename type="first">Sedigheh</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melo</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EACL 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1151" to="1163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">C</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens-Michalis</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Oberhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>L?ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Truhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keno</forename><forename type="middle">K</forename><surname>Bressem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.08247</idno>
		<title level="m">Medalpaca-an open-source collection of medical conversational ai models and training data</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pathvqa: 30000+ questions for medical visual question answering</title>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luntian</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10286</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Petro</surname></persName>
		</author>
		<idno type="PMID">36988602</idno>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">388</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1233" to="1239" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The ai revolution in medicine: Gpt-4 and beyond</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carey</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Kohane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>The name of the publisher</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.00890</idno>
		<title level="m">Llava-med: Training a large languageand-vision assistant for biomedicine in one day</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Self-supervised vision-language pretraining for medical visual question answering</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinying</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenjun</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.13594</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diagnostic assessment of deep learning algorithms for diabetic retinopathy screening</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanruo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">501</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="511" to="522" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Visual instruction tuning</title>
		<author>
			<persName><forename type="first">Haotian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Q2atransformer: Improving medical vqa via an answer querying decoder</title>
		<author>
			<persName><forename type="first">Yunyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luping</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="445" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Biogpt: generative pretrained transformer for biomedical text generation and mining</title>
		<author>
			<persName><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno>2022. 2</idno>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Capabilities of gpt-4 on medical challenge problems</title>
		<author>
			<persName><forename type="first">Harsha</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">Mayer</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Carignan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">OpenAI. Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023. 1, 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Visual med-alpaca: A parameter-efficient biomedical llm with visual capabilities</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Shareghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Openended medical visual question answering through prefix tuning of language models</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Tom Van Sonsbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivona</forename><surname>Mahdi Derakhshani</surname></persName>
		</author>
		<author>
			<persName><surname>Najdenkoska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><surname>Worring</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.05977</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Biomedlm: a domainspecific large language model for biomedical text</title>
		<author>
			<persName><surname>Venigalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><surname>Carbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">Dec, 23:3, 2022. 2</date>
		</imprint>
	</monogr>
	<note type="report_type">Mo-saicML</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Haochun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuwa</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Huatuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.06975</idno>
		<title level="m">Tuning llama model with chinese medical knowledge</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pmc-llama: Further finetuning llama on medical papers</title>
		<author>
			<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.14454</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Doctorglm: Finetuning your chinese doctor is not a herculean task</title>
		<author>
			<persName><forename type="first">Honglin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinggang</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.01097</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yunxiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zihan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Ruilong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>You</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.14070</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02414</idno>
		<title level="m">Glm-130b: An open bilingual pre-trained model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaspreet</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Preston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<pubPlace>Naveen</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Large-scale domain-specific pretraining for biomedical vision-language processing</title>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Valluri</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.00915</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><surname>Appendix</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
