<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Shadi</forename><surname>Albarqouni</surname></persName>
							<email>shadi.albarqouni@tum.de</email>
						</author>
						<author>
							<persName><forename type="first">)</forename><forename type="middle">S</forename><surname>Albarqouni</surname></persName>
						</author>
						<author>
							<persName><forename type="first">C</forename><surname>Baur</surname></persName>
						</author>
						<author>
							<persName><forename type="first">F</forename><surname>Achilles</surname></persName>
						</author>
						<author>
							<persName><forename type="first">S</forename><surname>Demirci</surname></persName>
						</author>
						<author>
							<persName><forename type="first">N</forename><surname>Navab</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Chair for Computer Aided Medical Procedure (CAMP)</orgName>
								<orgName type="institution">Technische Univer-sität München (TUM)</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Deutsches Zentrum für Neurodegenerative Erkrankungen (DZNE)</orgName>
								<address>
									<settlement>Bonn</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">CAMP</orgName>
								<orgName type="institution">Technische Universität München</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<settlement>Oxford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Johns Hopkins University (JHU)</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E0C0981099F88DB2CAAD7D0F9C03473C</idno>
					<idno type="DOI">10.1109/TMI.2016.2528120</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMI.2016.2528120, IEEE Transactions on Medical Imaging</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Crowdsourcing</term>
					<term>Gamification</term>
					<term>Deep Learning</term>
					<term>Online Learning</term>
					<term>Aggregation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The lack of publicly available ground-truth data has been identified as the major challenge for transferring recent developments in deep learning to the biomedical imaging domain. Though crowdsourcing has enabled annotation of large scale databases for real world images, its application for biomedical purposes requires a deeper understanding and hence, more precise definition of the actual annotation task. The fact that expert tasks are being outsourced to non-expert users may lead to noisy annotations introducing disagreement between users. Despite being a valuable resource for learning annotation models from crowdsourcing, conventional machine-learning methods may have difficulties dealing with noisy annotations during training. In this manuscript, we present a new concept for learning from crowds that handle data aggregation directly as part of the learning process of the convolutional neural network (CNN) via additional crowdsourcing layer (AggNet). Besides, we present an experimental study on learning from crowds designed to answer the following questions: (i) Can deep CNN be trained with data collected from crowdsourcing?, (ii) How to adapt the CNN to train on multiple types of annotation datasets (ground truth and crowd-based)?, (iii) How does the choice of annotation and aggregation affect the accuracy? Our experimental setup involved Annot8, a self-implemented web-platform based on Crowdflower API realizing image annotation tasks for a publicly available biomedical image database. Our results give valuable insights into the functionality of deep CNN learning from crowd annotations and prove the necessity of data aggregation integration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Being initially considered as market research strategy <ref type="bibr" target="#b2">[3]</ref>, it is nowadays widely seen as an economical way to recruit crowds for tedious and time-consuming tasks such as annotations for character recognition <ref type="bibr" target="#b3">[4]</ref>, image classification <ref type="bibr" target="#b4">[5]</ref>, and natural language processing <ref type="bibr" target="#b5">[6]</ref>. As a result of this trend, many crowdsourcing plattforms such as Amazon Mechanical Turk (AMT) <ref type="foot" target="#foot_0">1</ref> , Games with a Purpose<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b6">[7]</ref>, Crowdflower <ref type="foot" target="#foot_2">3</ref> , and LabelMe <ref type="foot" target="#foot_3">4</ref> have emerged within the past decade. Here, users are not only confronted with simple, every-day tasks, but are also engaged in highly complex processes involving innovation creation.</p><p>A good example for this, is the medical domain where, very recently, crowdsourcing has been presented as a solution to the immense lack in publicly available ground-truth data. Various applications such as medical pictogram <ref type="bibr" target="#b7">[8]</ref>, correspondence finding for stereo endoscopic imaging <ref type="bibr" target="#b8">[9]</ref>, device detection in angiographic sequences <ref type="bibr" target="#b9">[10]</ref>, telepathology <ref type="bibr" target="#b10">[11]</ref>, and medical image segmentation <ref type="bibr" target="#b11">[12]</ref> and classification <ref type="bibr" target="#b12">[13]</ref> have already shown that crowdsourcing can provide efficient and inexpensive data annotation. With the very recent launch of the CrowdTruth framework <ref type="foot" target="#foot_4">5</ref> , IBM, Google and Amsterdam University have paved the way towards machinehuman computing for collecting ground-truth annotation data on text, images and videos in the medical domain. Similarly, Celi et al. <ref type="bibr" target="#b13">[14]</ref> organised several events and data marathons, where engineers, data scientists, and clinicians were invited to address specific challenges during the clinical routines and procedures. As a result, many innovative ideas and prototypes have been developed, clinicians as well as medical students become part of a data-driven learning system. The most astonishing fact about crowdsourcing studies in the medical domain, however, is the conclusion that a crowd of nonprofessional, inexperienced users do not underperform medical experts <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>Improving the crowd's quality is very essential for being able to generate a reliable ground-truth and creating an interest within the research community. Redundancy and Aggregation (R&amp;A) (i.e. majority voting) is the baseline approach that has been proposed in this context <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. However, there is no control on the sensitivity and specificity of single participants. All aforementioned crowdsourcing platforms integrate qualification tests in order to restrict "noisy" annotations.</p><p>This information can then be incorporated into the groundtruth generation process via aggregation. Recently, Raykar et al. <ref type="bibr" target="#b17">[18]</ref> have proposed a probabilistic model for supervised learning to evaluate different users and estimate the groundtruth labels. Having such ground-truth is very important for both training many machine learning algorithms as well as for evaluation.</p><p>Indeed, deep learning has advanced the field of computer vision the last few years <ref type="bibr" target="#b18">[19]</ref> leading to powerful methods for various applications such as object classification <ref type="bibr" target="#b4">[5]</ref>, detection <ref type="bibr" target="#b19">[20]</ref>, segmentation <ref type="bibr" target="#b20">[21]</ref>, robust regression <ref type="bibr" target="#b21">[22]</ref> and depth prediction <ref type="bibr" target="#b22">[23]</ref>. The most established realization of deep learning are Convolutional Neural Networks (ConvNets or CNN) that have also been successfully applied for biomedical imaging purposes <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b26">[27]</ref>. The bottleneck, however, for deep CNN to yield decent accuracy is the availability of a large number of annotated training samples. In particular in the biomedical domain, sufficient ressources are not available.</p><p>We believe that crowdsourcing platforms will engage various crowds to collaborate with clinicians and frontline healthcare workers in translating questions into methodologies and innovative solutions of which ground truth data is an essential part. However, it is not clear how state-of-the-art machine learning methods behave when fed with training data consisting of reliable (expert) and unreliable (crowd) annotations <ref type="bibr" target="#b14">[15]</ref>. As suggested by Aroyo et al. <ref type="bibr" target="#b14">[15]</ref>, it is our goal to evaluate the trustworthiness of participants and integrate this knowledge into the analysis and further processing of annotations.</p><p>In this manuscript, we present a first attempt to apply the concept of learning from crowds within a biomedical environment. Being inspired by prominent previous work in this field <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>, we define the specific contribution of our own work as: i) Learning of a multi-scale CNN model for mitosis detection, ii) Incorporation of aggregation schemes into CNN layers, and iii) Augmentation and retraining of the CNN model with crowd's annotation labels.</p><p>In our analysis comparing performance of the CNN model when incorporating different types of aggregations schemes, we aim at answering the following questions: i) Can deep CNN be trained with data collected from crowdsourcing and is it robust against "noisy" labels?, ii) How to adapt the CNN when we have both ground-truth label and multiple annotations that could be "noisy"?, and iii) How is the accuracy compared to that obtained by ground-truth or majority voting?</p><p>In this manuscript, after recapitulating previous work in this field, we introduce AggNet, a novel aggregation layer that is integrated into our multi-scale CNN. We further present an analysis of the behavior of CNN with and without aggregation on a publicly available large-scale pathological dataset (including ground truth annotations). However, to the best of our knowledge, there has not yet been any effort to incorporate this information into machine learning algorithms analyzing the quality of models learned from non-expert annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHODOLOGY</head><p>In this section, we introduce the proposed CNN for aggregating annotations from crowds in conjunction with learning a model for a challenging classification task. Unlike typical supervised methods, which learn a model from ground truth labeled data, learning from crowd annotations is different in the sense that there may be (possibly noisy) multiple labels for the same sample. Our idea is to learn multiple CNN models with the same basic architecture on different image scales (c.f. step 1 in Fig. <ref type="figure" target="#fig_0">1</ref>), perform mitosis detection using these models (c.f. step 2 in Fig. <ref type="figure" target="#fig_0">1</ref>) and provide the crowds with detected mitosis candidates for annotation (c.f. step 3 in Fig. <ref type="figure" target="#fig_0">1</ref>). The collected annotations are then passed to the existing CNN (c.f. step 5 in Fig. <ref type="figure" target="#fig_0">1</ref>) with our aggregation layer attached in order to refine the models and simultaneously generate a ground-truth. This multi-scale approach ensures that we have redundant responses of the same data instances at different scales, with the goal to increase robustness of both aggregation and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notation</head><p>The input to our network is an observation set D = {x i , y j i ; i = 1, ..., N, j = 1, ..., P } containing N instances of x i ∈ R d (RGB image as d-dimensional vector) with corresponding labels y i ∈ C (i.e. C := {0, 1} for binary classification) annotated by P independent participants. The goal is to learn a robust CNN model, represented by f : X → Y, from aggregated labels which generalizes well on unseen data:</p><formula xml:id="formula_0">p = f (x, y; θ),<label>(1)</label></formula><p>where p is the predicted label for an unseen image x, and θ is the learned model parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-scale CNN Model</head><p>Our network architecture consists of three convolutional blocks followed by two fully connected (FC) layers as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Each convolutional block consists of a convolutional layer followed by a rectified linear unit (ReLU) <ref type="bibr" target="#b27">[28]</ref> and max-pooling layer. The output of the softmax layer is the probabilistic score of the mitotic figures.</p><p>In our proposed multi-scale CNN model the input image is first down-sampled to different scales (i.e. 0.33, 0.66 and 1). Then, 33 × 33 patches are collected and passed to the model (scale-wise). On new unlabelled data, we apply the learned model to mirrored and rotated versions (0, 90, 180, and 270 deg) of each image and compute a final detection map (FDM) as the mean of all those detection results.</p><p>FDM of different scales are then geometrically averaged to filter out weak responses. By doing this, we aim at obtaining more accurate detections.</p><p>During learning from crowd annotations phase, we augment the CNN architecture with our novel aggregation layer (AG) (Sec. II-C) in order to i) aggregate the ground-truth from crowdvotes matrix, ii) compute the sensitivity and specificity of each annotator, and iii) jointly learn the classifier by back propagating the derivative of the loss function. We refer to this augmented architecture as AggNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Aggregation Layer (AG):</head><p>The straightforward method to aggregate labels annotated by users, is to employ majority voting (MV) <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_1">µ = 1 ȳ ≥ 0.5 0 ȳ &lt; 0.5<label>(2)</label></formula><p>where ȳ = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|P |</head><p>P j=1 y j is the average label of P users. However, this strategy assumes all users to be on an equal level of trustworthiness.</p><p>In our framework, we integrate the method initially proposed by Raykar et al. <ref type="bibr" target="#b17">[18]</ref>, showing a good performance in many applications <ref type="bibr" target="#b16">[17]</ref>. On top of our CNN architecture, we aggregate labels µ, estimate the sensitivity α j and the specificity β j for each annotator j ∈ P , and jointly learn the classifier. The method is solved using the well-known expectation-maximization (EM) algorithm and adapted to learn the softmax classifier as follows:</p><p>• Initialization: Using the crowdvotes matrix Y, the aggregated labels µ i initialized with majority voting, α j and β j are initially computed from µ i . • E-Step: Given the observation set D and a current estimate of parameters ψ := {α, β, µ}, the conditional expectation is computed as</p><formula xml:id="formula_2">E{ln P r[D, g|ψ]} = N i=1 µ i ln p i a i +(1-µ i ) ln(1-p i )b i ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">a i = P j=1 [α j ] y j i [1 -α j ] 1-y j i , b i = P j=1 [β j ] 1-y j i [1 -β j ] y j i , p i = σ(z i ) = e z ic</formula><p>C c=1 e z ic , the output of softmax layer, z i = w T x i , the output of FC layer, g is the hidden variable (ground-truth), and the expectation is with respect to P r[g|D, ψ]. Using Bayes' theorem, the aggregated labels µ i can be computed as follows:</p><formula xml:id="formula_4">µ i = a i p i a i p i + b i (1 -p i ) .<label>(4)</label></formula><p>The loss function in our aggregation layer (AG) is defined as the Negative log-likelihood:</p><formula xml:id="formula_5">£(µ i , p i ) = -E{ln P r[D, g|ψ]},<label>(5)</label></formula><p>• M-Step: Based on the observation set D and the current estimate µ i , the model parameters ψ can be computed by taking the derivative of £ with respect to each parameter and equate it to zero. The updates for α j and β j can be obtained as follows:</p><formula xml:id="formula_6">α j = N i=1 µ i y j i N i=1 µ i , β j = N i=1 (1 -µ i )(1 -y j i ) N i=1 (1 -µ i ) ,<label>(6)</label></formula><p>The softmax function is non-linear and the gradient with respect to parameter w should be back propagated to the CNN layers <ref type="bibr" target="#b29">[30]</ref>. For this purpose, we can employ the chain rule:</p><formula xml:id="formula_7">∂£ ∂w = ∂£ ∂p i ∂p i ∂z i ∂z i ∂w ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_8">∂£ ∂pi = µi-pi pi(1-pi) , the output of AG Layer, ∂pi ∂zi = p i (δ ij -p j )</formula><p>, the output of softmax layer<ref type="foot" target="#foot_6">6</ref> , ∂zi ∂w = x i , the output of FC layer. Then, weights are updated using Stochastic Gradient Descent (SGD) <ref type="bibr" target="#b30">[31]</ref>. It is notable that E-Step and M-Step are computed in forward and backward propagation respectively, which means one EM iteration per epoch. The refining process is stopped when the loss function output barely changes to avoid overfitting. To this end, the aggregation method takes into account the sensitivity and specificity of each annotator to aggregate the labels. Furthermore, the algorithm is adapted to handle:</p><p>• Trustworthiness: Some crowdsourcing platforms provide the customer with a single accuracy score γ that each user achieved on a qualitative test for a specific task. It has been suggested by Raykar et al. <ref type="bibr" target="#b17">[18]</ref> to model a prior distribution on sensitivity and specificity to trust some participants more than others. With only a single accuracy score as provided by our scenario, this is however not possible. • Missing Labels: A common failure in crowdsourcing is that some users annotate a few samples only. If all these samples happen to fall within one single class, sensitivity or specificity remains unknown. Furthermore, the user who annotates few samples only might be equally or even more trusted than a user who annotates more samples. Therefore, we reformulate α j and β j , without loss of generality, in such a way to augment the number of True Positives (TP) and True Negatives (TN) when the user has high confidence (i.e. accuracy score) as follows:</p><formula xml:id="formula_9">α j = τ γ j |Np|+ Np i=1 µiy j i τ γ j |Np|+ N i=1 µi = (1+τ γ j )T P (1+τ γ j )T P +F N , β j = τ γ j |Nn|+ Nn i=1 (1-µi)(1-y j i ) τ γ j |Nn|+ N i=1 (1-µi) = (1+τ γ j )T N (1+τ γ j )T N +F P ,<label>(8)</label></formula><p>where γ j is the accuracy score for a particular user and τ is the hyper-parameter that leverage the user's confidence. To avoid numerical issues, we set the sensitivity and specificity to 0.5 for unknown cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS AND RESULTS</head><p>We have designed our experimental setup such that first, the proposed multi-scale CNN architecture is validated before evaluating the aggregated labels from the crowdvotes and validating the proposed augmented CNN (AggNet).</p><p>Dataset. We have validated our proposed network on the publicly available MICCAI-AMIDA13 challenge dataset 7 . It contains annotated histology images of a total of 23 patients, who underwent invasive breast biopsy. During this medical examination, sections of suspicious breast tissue are collected AMIDA13: http://amida13.isi.uu.nl/ and stained using hematoxylin and eosin (H&amp;E). A histology RGB image of 2k × 2k is then acquired with the Aperio ScanScope XT scanner at 40X magnification and with a spatial resolution of 0.25µm/pixel. Then, a region of interest is identified and digitized to several high power field (HPF) images. The standard procedure in pathology is to count the mitotic figures in this area for the purpose of cancer grading (mitotic count score criteria). The annotation in the AMIDA13 dataset was done by two expert pathologists. Concordant annotations of both experts were taken as ground truth objects directly, whereas discordant cases were presented to two additional observers, such that the ground truth have been agreed upon by at least two experts. The reader is referred to <ref type="bibr" target="#b31">[32]</ref> for more information about the dataset and its clinical/pathological background. In our experiment, we learn the proposed initial multi-scale model from 12 patients (311 HPF images), validate on 20% of the training set (60 HPF images) and test it on the whole testing data of AMIDA Challenge, including 11 patients (295 HPF images).</p><p>Implementation details. Each input RGB image is first pre-processed by staining appearance normalization <ref type="bibr" target="#b32">[33]</ref>. Then, small patches of 33 × 33 are collected. Furthermore, to handle highly imbalanced data, patches showing positive classes are augmented with rotation and mirroring in such a way to leverages the ratio of positive to negative classes about</p><p>The muti-scale CNN is implemented using MATLAB and MatConvNet <ref type="bibr" target="#b33">[34]</ref> and conducted on an Intel i7 machine with a GeForce GT 750M graphics card. Concerning the network parameters, the learning rate is set to 1 × 10 -3 , momentum to 0.9, weight decay to 5 × 10 -4 , and the batch size fixed to 200 samples. Note that some of these parameters are changed in the refining process, i.e. learning rate is set to 5 × 10 -5 and the batch size is set to the whole crowdsourcing set. For the sake of reusability and to overcome the limitations of the crowdsourcing platform Crowdflower, we have designed and implemented Annot8 8 , a Ruby-on-Rails based webplatform, allowing registered users to create datasets, upload images and labels, and categorize the labels with the help of a powerful tagging system. Collections of existing labels can be sent to and crowdsourced labels can be imported from Crowdflower easily. Our web-platform also offers an online image processing frontend for on-demand patch extraction and computation of biomedical image filtering. On the participant side, each user was introduced briefly about the disease and the instructions of the actual task showing some good and bad examples as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. Then, participants had to conduct a few test questions for quality control purposes. Without being made aware of the quiz mode, each annotator was presented with patches with known labels. Only then, he/she started to annotate five patches presented along with the filtered images. In order to ensure continuous quality control, a few randomly seeded test patches were still shown during the actual annotation job.</p><p>Evaluation metrics. We calculate different validation measures for comparison purpose, such as Recall </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof-of-Concept Evaluation</head><p>The objective of this experimental setup is to analyze the functionality of the entire AggNet framework (c.f. Fig. <ref type="figure" target="#fig_0">1</ref>), specifically i) the accuracy of multi-scale CNN, ii) the performance of our novel aggregation layer when fed with noisy In this experiment, we have crowdsourced around 550 patches, where each patch was annotated by 10 participants at least, resulting in more than 5500 labels stored in the crowdvotes matrix Y. We have then evaluated results according to the different aspects related to the objectives defined for this specific experimental setup:</p><p>1) Multi-scale CNN: In order to measure the performance of the multi-scale CNN, we train the network on three different scales (0.33, 0.66 and 1) individually. For inference, we geometricaly average the resulting FDMs from all scales to obtain the final positive responses. As alternative method to extract positive responses, we also threshold the FDM for each scale. For each remaining response, we check whether it is a TP, FP or FN detection. A positive response is considered a TP if its Euclidean distance to a ground-truth mitotic figure is less or equal than 30px. Multiple responses within the same radius around a mitotic figure are counted as a single TP. If there is no response for a mitotic figure within this radius, we count a FN detection. Any responses that are not inside any 30px region around a mitotic figure are counted as FP. It should be noted that a 30px radius (7.5µm) is used on the original scale, however, this is adjusted for different scales. Using these numbers, we calculate the Precision, Recall, and F 1 -score over all HPF slides at once and also per patient. TABLE <ref type="table" target="#tab_0">II</ref> shows the F 1 -score of 22 HPF images, the corresponding testing dataset, while the bar plot in Fig. <ref type="figure" target="#fig_4">5</ref> displays the other metrics. It is obvious that the multi-scale CNN approach pushed the overall F 1 -score about 22.5%±6.8, which validates our initial hypothesis of the proposed multi-scale CNN approach yielding a more robust classification due to detection consensus at various scales.</p><p>2) Aggregated Labels (AL): To investigate the aggregated labels of the crowdvotes matrix Y, we first run majority voting (MV) <ref type="bibr" target="#b28">[29]</ref> and GLAD <ref type="bibr" target="#b34">[35]</ref> methods on Y without any quality control, referred to as MV-NoQ and GLAD-NoQ respectively.  Then, we use the existing 0.33-scale model from the previous multi-scale CNN experiment and augment it with our AG layer. Subsequently, we retrain it further for 100 epochs. We refer to this architecture as AggNet, and the aggregation results as AG-NoQ. Further, to test the quality control, we filtered first the crowdvotes matrix Y in order to keep only the annotations from the users who achieved more than 70% accuracy score in their qualitative test (in a quiz stage, each user had to annotate a few samples extracted from training data with known ground-truth). Then, we run the same aggregation methods, however, MV is replaced with Honeypot (HP) <ref type="bibr" target="#b35">[36]</ref>. We refer to the aggregated labels using 70% quality control as HP-70Q, GLAD-70Q, and AG-70Q.</p><p>In addition, to figure out how the accuracy scores γ of the participant can influence the proposed aggregation method, we validate the hyper-parameter τ = [0.1, 0.2, ..., 1] (ref.</p><p>Sec. II-C) and run similar experiments, referred to as AG-NoQ-τ and AG-70Q-τ respectively.</p><p>To evaluate the aforementioned aggregated labels of crowdsourced patches (c.f. TABLE <ref type="table" target="#tab_1">III</ref>), we compute the F 1 -score between the ground-truth and the aggregated labels of different methods as well as the proposed method (c.f. Fig. <ref type="figure">6a</ref>). Further, to give the reader more insight on the probabilistic scores of different aggregation methods, we plot the ROC curves of the significant methods as shown in Fig. <ref type="figure">7a</ref>.</p><p>Unlike the weak agreement of the aggregated labels of both MV-NoQ and GLAD-NoQ with the ground-truth, both AG-NoQ and AG-NoQ-τ (-70Q as well) achieve an outperforming agreement at the first few epochs before getting decayed and saturated at still good agreement compared with the other aggregation methods as shown in Fig. <ref type="figure">6a</ref>. MV-NoQ and GLAD-NoQ coincide in this setup due to the choice of our label threshold = 0.5. The ROC curve of the respective methods shows that GLAD is slightly superior to MV.</p><p>Interestingly, the aggregation methods without any quality control perform better than the ones with 70% quality control, which shows that the quality control strategy should be revised for such challenging data. Notably, AG-NoQ-τ as well as AG-70Q-τ , which incorporate the gamer accuracies as a prior, underperform the other AG-models. Once again, this shows that the quality control strategy and thus the accuracy is a potential bottleneck.</p><p>3) Augmented Models (AM): We further investigate how aggregated labels obtained from the previous experiment, influence the detection quality of our CNN compared to the ground-truth model. For this purpose, we compute several augmented models based on 0.33-scale of the initially trained ground truth model (GT). Besides AggNet, which we obtain by attaching our AG layer to GT, we compute three additional distinct models by retraining with aggregated labels from MV and GLAD as well as the real ground truth labels. We refer to the augmented models as AM-MV, AM-GLAD and AM-GT respectively. In fact, we retrain for 100 epochs, but pick the best performing model for each. Note that AM-GT is the 0.33-scale CNN model, however, its operating point is set to the same threshold that was used to select positive candidates for crowdsourcing. Once retrained, we utilize the models to perform mitosis detection on the corresponding testing set. Fig. <ref type="figure">7b</ref> and<ref type="figure">TABLE</ref> IV nicely outline how the proposed Ag-gNet model almost performs as good as the augmented ground truth model AM-GT and easily outperforms both AM-MV and AM-GLAD (around 7.6% of AUC). Furthermore, some HPF  images are visualized in TABLE V to show how different augmented models perform on their optimal operating points.</p><p>It is worth noticing also here how the proposed AggNet hits only the ground-truth in the perfect scenario outperforming even the AM-GT, or miss some ground-truth while having very few false positive in off scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Use Case Evaluation</head><p>This experiment aims at proving the overall impact of AggNet on a large standardized dataset (entire AMIDA13 Challenge training and testing datasets).</p><p>To first evaluate the performance of our proposed multi-CNN, we have participated in the AMIDA13 Challenge with our novel approach achieving 0.433 as overall F 1 -score. As reported by the challenge organizers, our method yields rank three of 15 participating methodologies.</p><p>Response maps resulting from the AMIDA13 Challenge testing dataset on 0.33-Scale have then been thresholded at an operating point of 0.99 (calculation based on the dataset) and repatched samples have been forwarded to CrowdFlower. Similarly to the previous experimental setup, crowdvotes Y and confidences have been fed back to AggNet in order  to augment the previously trained model. For performance evaluation, different augmented models have also participated in the AMIDA13 Challenge. TABLE VII shows the evaluation metrics of different models augmented with MV, GLAD and our robust aggregation layer (AggNet). The overall F 1 -score of AggNet easily outperforms the other augmented models of MV and GLAD, however, it falls slightly behind the previously trained model (0.33-Scale).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION</head><p>Our results confirm that aggregation and deep learning from crowd annotations using the proposed AggNet is robust to "noisy" labels and positively influences the performance of our CNN in the refining phase.</p><p>Proper selection of operating point for crowdsourcing is quite challenging, for instance, it can be chosen based on the validation set, however, it might not be optimal for crowdsourcing where you need more positives. Therefore, it is recommended to plot the AUC, which provides the clinicians with more flexibility to run the model at the optimal operating point from their perspective. The augmented model AggNet has a gain of 7.6% in AUC at lower than the optimal operating point, however, this can not be computed for the use case experiment due to the limited number of submissions to the AMIDA Challenge.</p><p>However, the aggregation results from quality-filtered crowd annotations shed the light on the applied quality control. Surprisingly, they turned out to be worse than the aggregation from the complete, "noisy" set of crowd votes. This is clearly related to the high ambiguity and the high level of difficulty in detecting mitotic figures in general. Indeed, such quality tests need to be carefully planned and well designed in order to make sure they do not carry more "noise" than the actual crowdsourcing task, which we believe, the latter was the case in our experiments. However, this problem can also be related to the community of the crowd itself <ref type="bibr" target="#b36">[37]</ref>.</p><p>The low agreement among the crowd together with the small number of patches might be the reasons of the noticed decay in the aggregation F 1 -score curve, which leads to an overfitting after only few epochs (see Fig. <ref type="figure">6</ref>). Nevertheless, it is obvious that our robust aggregation layer can detect the novices (spammers) and weight their votes less. Therefore, our AggNet outperforms easily the other augmented models,   where the spammers hurt the aggregations (c.f. TABLE <ref type="table" target="#tab_5">VII</ref>). Fig. <ref type="figure" target="#fig_7">8</ref> shows the accuracy scores γ (based on the qualitative test) and the spammer scores S p (based on the participant's sensitivity α and specificity β, where S p = (α + β -1) 2 <ref type="bibr" target="#b37">[38]</ref>) of 100 participants in the crowdsourcing task. During our research, the very natural question arose whether it may be possible to learn a model from crowdsourcing labels alone. For this purpose, we ran two additional crowdsourcing experiments. First, we utilized the crowdsourcing set (i.e. 5500 patches) and the binary classification crowdvotes to learn a model from scratch. Very soon, we realized that this model quickly overfited due to the small number of training instances. Second, instead of publishing patches only, we asked the crowd to label full HPF images. In this case, however, due to insufficient settings within CrowdFlower platform, each participant labeled only few potential mitotic figures per image and left most of the challenging cases besides. This led to a large number of missing annotations and poor overall agreement among participants, which renders aggregation and training impossible. Still, these initial experiments gave us evidence that it is very difficult and maybe even impossible to entirely outsource the task of labeling mitotic figures in histology images to crowds. Instead, we decided to rather augment a small and narrow model learned from expert labels with wide but noisy crowd annotations to enhance variations encoded within the model.</p><p>Validating our methodology based on the smallest scale of our initial model is theoretically feasible. However, in future work, we want to conduct even more involved experiments including also the models of the other scales. This includes independent crowd sourcing rounds for each scale separately since retraining all the scales of the multi-scale model from the same crowd-sourced and aggregated labels hurts the concept of "redundancy &amp; aggregation". Additionally, we want to consider multi-class classification which is, due to the binary nature of sensitivity and specificity, not directly possible, but can be performed in an one-vs-all fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have introduced a novel concept for learning from crowds. Our new multi-scale CNN AggNet is designed to handle data aggregation directly as part of the learning process via an additional crowdsourcing layer. In our experimental study, we have further presented valuable insights into the functionality of deep CNN learning from crowd annotations and proven the impact of our novel aggregation scheme. To the best of knowledge, this is the first time that deep learning has been applied to generate a ground-truth labeling from non-expert crowd annotation in a biomedical context.</p><p>Although data aggregation is certainly necessary to learn from crowds, computational aggregation models have a limited impact, in particular if noisy crowd annotations are not significant, i.e. do not arise from ambiguous contexts. Besides clear guidelines, non-expert users need to be motivated to perform the task until the very end. Gamification is the ultimate solution here and we will focus future work on novel solutions on how to transform complex expert tasks in the biomedical domain into a game for non-expert users.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: AggNet Framework: (1) The multi-scale CNN model is trained from gold-standard annotations. (2) Then for any incoming unlabelled image, (3) the AggNet will produce a response map which is thresholded at selected optimal operating point. (4) These few resulting positive candidates are outsourced to crowds. (5) AggNet collects back the crowd votes and jointly aggregates the ground truth and refine the CNN model.</figDesc><graphic coords="3,51.11,56.72,246.76,394.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: AggNet architecture: The same CNN architecture is used for different scales, where p i , µ i , y j i represents the classifier output, the aggregated label, and the crowdvotes respectively.</figDesc><graphic coords="4,51.11,56.72,246.74,95.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>TABLEFig. 3 :</head><label>3</label><figDesc>Fig. 3: Instructions and guidelines</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: First row shows results of one single image using multi-scale CNN, Green: the true positives, Orange: the false positives. Second row shows the corresponding final detection map (FDM) before thresholding. Best viewed in color.</figDesc><graphic coords="6,52.45,355.37,117.18,59.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Evaluation Metrics: Precision, Recall, and F 1 -score of Patients 9, 11 and 12.</figDesc><graphic coords="6,179.32,430.01,117.18,59.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) F 1 -score (b) Loss function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>Fig. 6: (a) The aggregated labels of the crowdsourcing set are evaluated using the F 1 -score metric. (b) The loss function barely changes at 3-8 epochs before starting to overfit and the gap between the validation and training curves becomes significant. The shaded area depicts the change of τ .</figDesc><graphic coords="7,52.06,299.77,117.69,103.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Fig.8: Participants Analysis: accuracy and spammer scores of 100 participants. Arrows in green show some participants achieve high accuracy scores in the qualitative test, however, they are spammer. Arrows in red show very few participants who have good accuracy score as well as spammer score. Note that spammer score "0" means the participant is spammer.</figDesc><graphic coords="8,48.96,387.89,257.02,126.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,48.96,168.90,257.04,371.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II :</head><label>II</label><figDesc>F 1 -SCORES</figDesc><table><row><cell></cell><cell>Patient 9</cell><cell>Patient 11</cell><cell>Patient 12</cell><cell>Overall</cell></row><row><cell>0.33-Scale</cell><cell>0.8000</cell><cell>0.5833</cell><cell>0.7778</cell><cell>0.6479</cell></row><row><cell>0.66-Scale</cell><cell>0.5000</cell><cell>0.5556</cell><cell>0.6957</cell><cell>0.5882</cell></row><row><cell>Orig.-Scale</cell><cell>0.5000</cell><cell>0.5490</cell><cell>0.6957</cell><cell>0.5854</cell></row><row><cell>Multi-Scale</cell><cell>0.8000</cell><cell>0.7368</cell><cell>0.7368</cell><cell>0.7419</cell></row><row><cell>Improvement</cell><cell>40%±36</cell><cell>39%±11</cell><cell>2.2%±6.4</cell><cell>22.5%±6.8</cell></row><row><cell cols="5">annotations, and iii) the influence of the augmented model</cell></row><row><cell cols="5">on the detection quality of the multi-scale CNN. In order to</cell></row><row><cell cols="5">perform quantitative analysis with respect to real ground-truth</cell></row><row><cell cols="5">data, we decided to employ the AMIDA13 Challenge training</cell></row><row><cell cols="5">dataset only as it comes with ground truth annotations (c.f.</cell></row><row><cell cols="5">TABLE I). The setup is designed according to our overall</cell></row><row><cell cols="5">framework pipeline depicted in Fig. 1. First, a model is learned</cell></row><row><cell cols="5">on 8 random patients of the AMIDA13 Challenge training</cell></row><row><cell cols="5">dataset and tested on different 3 patients. Training and testing</cell></row><row><cell cols="5">is performed employing our novel multi-scale CNN AggNet.</cell></row><row><cell cols="5">Then, response maps of AggNet are thresholded at a lower</cell></row><row><cell cols="5">operating point ensuring a large number of positive candidates,</cell></row><row><cell cols="5">repatched and sent to CrowdFlower using our Annot8 web-</cell></row><row><cell>platform.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III :</head><label>III</label><figDesc>AGGREGATED LABELS</figDesc><table><row><cell></cell><cell>Aggregation Method</cell><cell>Quality Control</cell><cell>Prior</cell></row><row><cell>MV-NoQ</cell><cell>MV</cell><cell>No</cell><cell>-</cell></row><row><cell>GLAD-NoQ</cell><cell>GLAD</cell><cell>No</cell><cell>-</cell></row><row><cell>AG-NoQ</cell><cell>Proposed</cell><cell>No</cell><cell>-</cell></row><row><cell>AG-NoQ-τ</cell><cell>Proposed</cell><cell>No</cell><cell>τ</cell></row><row><cell>HP-70Q</cell><cell>HP</cell><cell>70%</cell><cell>-</cell></row><row><cell>GLAD-70Q</cell><cell>GLAD</cell><cell>70%</cell><cell>-</cell></row><row><cell>AG-70Q</cell><cell>Proposed</cell><cell>70%</cell><cell>-</cell></row><row><cell>AG-70Q-τ</cell><cell>Proposed</cell><cell>70%</cell><cell>τ</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV :</head><label>IV</label><figDesc>AUGMENTED MODELS</figDesc><table><row><cell></cell><cell>AM-GT</cell><cell>AM-MV</cell><cell>AM-GLAD</cell><cell>AggNet</cell></row><row><cell>F 1 -score</cell><cell>0.6250</cell><cell>0.6097</cell><cell>0.6097</cell><cell>0.6133</cell></row><row><cell>AUC</cell><cell>0.8433</cell><cell>0.8082</cell><cell>0.8082</cell><cell>0.8695</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE VI :</head><label>VI</label><figDesc>USE CASE RESULTS</figDesc><table><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>Overall F 1 -score</cell></row><row><cell>0.33-Scale</cell><cell>0.211</cell><cell>0.538</cell><cell>0.303</cell></row><row><cell>0.66-Scale</cell><cell>0.296</cell><cell>0.583</cell><cell>0.393</cell></row><row><cell>Orig.-Scale</cell><cell>0.172</cell><cell>0.400</cell><cell>0.241</cell></row><row><cell>Multi-Scale</cell><cell>0.441</cell><cell>0.424</cell><cell>0.433</cell></row><row><cell>Improvement</cell><cell>105%±54</cell><cell>-14%±18</cell><cell>44%±35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V :</head><label>V</label><figDesc>AGGREGATION AND DETECTION RESULTS</figDesc><table><row><cell>Augmented Models</cell><cell>AM-GT</cell><cell>AM-MV</cell><cell>AM-GLAD</cell><cell>The proposed AggNet</cell></row><row><cell>Perfect</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Off</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII :</head><label>VII</label><figDesc>USE CASE AUGMENTED MODELS</figDesc><table><row><cell></cell><cell>0.33-Scale</cell><cell>AM-MV</cell><cell>AM-GLAD</cell><cell>AggNet</cell></row><row><cell>Precision</cell><cell>0.211</cell><cell>0.006</cell><cell>0.006</cell><cell>0.374</cell></row><row><cell>Recall</cell><cell>0.538</cell><cell>0.004</cell><cell>0.004</cell><cell>0.208</cell></row><row><cell>F 1 -score</cell><cell>0.303</cell><cell>0.004</cell><cell>0.005</cell><cell>0.267</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Amazon Mechanical Turk, https://www.mturk.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Game with a Purpose, https://www.gwap.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>CrowdFlower, http://www.crowdflower.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>LabelMe, http://labelme.csail.mit.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>CrowdTruth framework -http://crowdtruth.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, MONTH 20XX</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>The Kronecker delta,δ ij = 1 i = j 0 i = j</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank the reviewers for their constructive feedback and all the users who participated in this study. We are also very grateful to Dr. Mitko Veta for giving us the permission to use the AMIDA13 dataset in our research and supporting us during validation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards an integrated crowdsourcing definition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Estellés-Arolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>González-Ladrón-De-Guevara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Inf. Science</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="200" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The rise of crowdsourcing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Howe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wired Magazine</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Un(der)paid innovators: The commercial utiliza-tion of consumer work through crowdsourcing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kleemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Voß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">STI Studies</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">recaptcha: Human-based character recognition via web security measures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcmillen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="issue">5895</biblScope>
			<biblScope unit="page" from="1465" to="1468" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inf. Proc. Sys. (NIPS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Crowdtruth: Machinehuman computation framework for harnessing disagreement in gathering annotated data</title>
		<author>
			<persName><forename type="first">O</forename><surname>Inel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Khamkham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dumitrache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rutjes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Der Ploeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romaszko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Inter. Semantic Web Conf., Part II</title>
		<meeting>Inter. Semantic Web Conf., Part II</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="486" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Games with a purpose</title>
		<author>
			<persName><forename type="first">L</forename><surname>Von Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="92" to="94" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Crowdsourcing participatory evaluation of medical pictograms using amazon mechanical turk</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Willis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Internet Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">e108</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Crowdsourcing for reference correspondence generation in endoscopic images</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mersmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Kenngott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Preukschas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Wekerle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Helfert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<imprint>
			<biblScope unit="page" from="349" to="356" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online tracking of interventional devices for endovascular aortic repair</title>
		<author>
			<persName><forename type="first">D</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Sarhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghotbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Demirci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comp. Assist. Radiol. Surg</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="773" to="781" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Biogames: a platform for crowd-sourced biomedical image analysis and telediagnosis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mavandadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Games Health J</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="373" to="376" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How to collect segmentations for biomedical images? a benchmark evaluating the performance of experts, crowdsourced non-experts, and algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gurari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Theriault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sameki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Purwada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Solski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Winter Conf. Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conf. Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ground truth generation in medical imaging: a crowdsourcing-based iterative approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Foncubierta Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multim. Work. Crowdsourcing for Multimedia</title>
		<meeting>ACM Multim. Work. Crowdsourcing for Multimedia</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Crowdsourcing knowledge discovery and innovations in medicine</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Internet Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Truth is a lie: Crowd truth and the seven myths of human annotation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Aroyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Welty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Get another label? improving data quality and data mining using multiple, noisy labelers</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</title>
		<meeting>ACM SIGKDD Int. Conf. Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An evaluation of aggregation techniques in crowdsourcing</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Quoc</forename><surname>Viet Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aberer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Information Systems Engineering -WISE 2013</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust optimization for deep regression</title>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inf. Proc. Sys. (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond classification: Structured regression for robust cell detection using convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<imprint>
			<biblScope unit="page" from="358" to="365" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A novel cell detection method using deep convolutional neural network and maximum-weight independent set</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<imprint>
			<biblScope unit="page" from="349" to="357" />
			<date type="published" when="2015">2015, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mitosis detection in breast cancer histology images with deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention-MICCAI</title>
		<imprint>
			<biblScope unit="page" from="411" to="418" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The segmentation of the left ventricle of the heart from ultrasound data using deep learning architectures and derivative-based search methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="968" to="982" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning (ICML)</title>
		<meeting>Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Limits on the majority vote accuracy in classifier fusion</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Kuncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Shipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Analysis &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient backprop</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="9" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computational Statistics</title>
		<meeting>Computational Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Assessment of algorithms for mitosis detection in breast cancer histopathology images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Veta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Van Diest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cruz-Roa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vestergaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="237" to="248" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A method for normalizing histology slides for quantitative analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Macenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Woosley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISBI</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1107" to="1110" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Matconvnet-convolutional neural networks for matlab</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Multimedia</title>
		<meeting>ACM Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Whose vote should count more: Optimal integration of labels from labelers of unknown expertise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>-F. Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Ruvolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2035" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The social honeypot project: protecting online communities from spammers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. World Wide Web</title>
		<meeting>Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1139" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Community-based bayesian aggregation models for crowdsourcing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Venanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guiver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shokouhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. World Wide Web</title>
		<meeting>Int. Conf. World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ranking annotators for crowdsourced labeling tasks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. Neural Inf. Proc. Sys. (NIPS)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1809" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
