<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Confounding-Robust Policy Improvement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nathan</forename><surname>Kallus</surname></persName>
							<email>kallus@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Cornell Tech</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Angela</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Confounding-Robust Policy Improvement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2868C88C6D7D8FDE2162D0B1FAB55DCD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of learning personalized decision policies from observational data while accounting for possible unobserved confounding in the data-generating process. Unlike previous approaches that assume unconfoundedness, i.e., no unobserved confounders affected both treatment assignment and outcomes, we calibrate policy learning for realistic violations of this unverifiable assumption with uncertainty sets motivated by sensitivity analysis in causal inference. Our framework for confounding-robust policy improvement optimizes the minimax regret of a candidate policy against a baseline or reference "status quo" policy, over an uncertainty set around nominal propensity weights. We prove that if the uncertainty set is well-specified, robust policy learning can do no worse than the baseline, and only improve if the data supports it. We characterize the adversarial subproblem and use efficient algorithmic solutions to optimize over parametrized spaces of decision policies such as logistic treatment assignment. We assess our methods on synthetic data and a large clinical trial, demonstrating that confounded selection can hinder policy learning and lead to unwarranted harm, while our robust approach guarantees safety and focuses on well-evidenced improvement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of learning personalized decision policies to study "what works and for whom" in areas such as medicine and e-commerce often endeavors to draw insights from observational data, since data from randomized experiments may be scarce and costly or unethical to acquire <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13]</ref>. These and other approaches for drawing conclusions from observational data in the Neyman-Rubin potential outcomes framework generally appeal to methodologies such as inverse-propensity weighting, matching, and balancing, which compare outcomes across groups constructed such that assignment is almost as if at random <ref type="bibr" target="#b22">[23]</ref>. These methods rely on the controversial assumption of unconfoundedness, which requires that the data are sufficiently informative of treatment assignment such that no unobserved confounders jointly affect treatment assignment and individual response <ref type="bibr" target="#b23">[24]</ref>. This key assumption may be made to hold ex ante by directly controlling the treatment assignment policy as sometimes done in online advertising <ref type="bibr" target="#b3">[4]</ref>, but in other domains of key interest such as personalized medicine where electronic medical records (EMRs) are increasingly being analyzed ex post, unconfoundedness may never truly hold in fact. Assuming unconfoundedness, also called ignorability, conditional exogeneity, or selection on observables, is controversial because it is fundamentally unverifiable since the counterfactual distribution is not identified from the data, thus rendering any insights from observational studies vulnerable to this fundamental critique <ref type="bibr" target="#b10">[11]</ref>. If the data is truly unconfounded, it would be known by construction because it would come from an RCT or logged bandit; any data whose unconfoundedness is uncertain must be confounded to some extent. The growing availability of richer observational data such as found in EMRs renders unconfoundedness more plausible, yet it still may never be fully satisfied in practice. Because unconfoundedness may fail to hold, existing policy learning methods that assume it can lead to personalized decision policies that seek to exploit individual-level effects that are not really there, may intervene where not necessary, and may in fact lead to net harm rather than net good. Such dangers constitute obvious impediments to the use of policy learning to enhance decision making in such sensitive applications as medicine, public policy, and civics.</p><p>To address this deficiency, in this paper we develop a framework for robust policy learning and improvement that can ensure that a personalized decision policy derived from observational data, which inevitably may have some unobserved confounding, does no worse than a current policy such as the standard of care and in fact does better if the data can indeed support it. We do so by recognizing and accounting for the potential confounding in the data and require that the learned policy improve upon a baseline no matter the direction of confounding. Thus, we calibrate personalized decision policies to address sensitivity to realistic violations of the unconfoundedness assumption. For the purposes of informing reliable and personalized decision-making that leverages modern machine learning, point identification of individual-level causal effects, which previous approaches rely on, may not be at all necessary for success, but accounting for the lack of identification is.</p><p>Functionally, our approach is to optimize a policy to achieve the best worst-case improvement relative to a baseline treatment assignment policy such as treat all or treat none, where the improvement is measured using a weighted average of outcomes and weights take values in an uncertainty set around the nominal inverse propensity weights (IPW). This generalizes the popular class of IPWbased approaches to policy learning, which optimize an unbiased estimator for policy value under unconfoundedness <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b26">27]</ref>. Unlike standard approaches, in our approach the choice of baseline is material and changes the resulting policy chosen by our method. This framing supports reliable decision-making in practice, as often a practitioner is seeking evidence of substantial improvement upon the standard of care or a default option, and/or the intervention under consideration introduces risk of toxicity or adverse effects and should not be applied without strong evidence.</p><p>Our contributions are as follows: we provide a framework for performing policy improvement which is robust in the face of unobserved confounding. Our framework allows for the specification of data-driven uncertainty sets, based on the sensitivity parameter describing a pointwise multiplicative bound, as well as allowing for a global uncertainty budget which restricts the total deviation proportionally to the maximal `1 discrepancy between the true propensities and nominal propensities. Leveraging the optimization structure of the robust subproblem, we provide algorithms for performing policy optimization. We assess performance on a synthetic example as well as a large clinical trial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Statement and Preliminaries</head><p>We assume the observational data consists of tuples of random variables {(X i , T i , Y i ) : i = 1, . . . , n}, comprising of covariates X i 2 X , assigned treatment T i 2 { 1, 1}, and real-valued outcomes Y i 2 R. Using the Neyman-Rubin potential outcomes framework, we let Y i ( 1) and Y i (1) denote the potential outcomes of applying treatment 1 and 1, respectively. We assume that the observed outcome is potential outcome for the observed treatment, Y i = Y i (T i ), encapsulating non-interference and consistency, also known as SUTVA <ref type="bibr" target="#b24">[25]</ref>. We also use the convention that the outcomes Y i corresponds to losses so that lower outcomes are better.</p><p>We consider evaluating and learning a (randomized) treatment assignment policy mapping covariates to the probability of assigining treatment, ⇡ : X ! [0, 1]. We focus on a policy class ⇡ 2 F of restricted complexity. Examples include linear policies</p><formula xml:id="formula_0">⇡ (X) = I[ | x], logistic policies ⇡ (X) = ( |</formula><p>x) where (z) = 1/(1 + e z ), or decision trees of a bounded certain depth. We allow the candidate policy ⇡ to be either deterministic or stochastic, and denote the random variable indicating the realization of treatment assignment for some X i to be a Bernoulli random variable</p><formula xml:id="formula_1">Z ⇡ i such that ⇡(X i ) = Pr[Z ⇡ i = 1 | X i ].</formula><p>The goal of policy evaluation is to assess the policy value,</p><formula xml:id="formula_2">V (⇡) = E[Y (Z ⇡ )] = E[⇡(X i )Y (1) + (1 ⇡(X i ))Y ( 1)],</formula><p>the population average outcome induced by the policy ⇡. The problem of policy optimization seeks to find the best such policy over the parametrized function class F. Both of these tasks are hindered by residual confounding since then V (⇡) cannot actually be identified from the data.</p><p>Motivated by the sensitivity model in <ref type="bibr" target="#b21">[22]</ref> and without loss of generality, we assume that there is an additional but unobserved covariate U i such that unconfoundedness would hold if we were to control for both</p><formula xml:id="formula_3">X i and U i , that is, such that E[Y i (t) | X i , U i , T i ] = E[Y i (t) | X i , U i ] for t 2 { 1, 1}.</formula><p>Equivalently, we can treat the data as collected under an unknown logging policy that based its assignment on both X i and U i and that assigned T i = 1 with probability e(X i , U</p><formula xml:id="formula_4">i ) = Pr[T = 1 | X i , U i ].</formula><p>Here, e(X i , U i ) is precisely the true propensity score of unit i. Since we do not have access to U i in our data, we instead presume that we have access only to nominal propensities ê(X i ) = Pr[T = 1 | X i ], which do not account for the potential unobserved confounding. These are either part of the data or can be estimated directly from the data using a probabilistic classification model such as logistic regression. For compactness, we denote êTi (</p><formula xml:id="formula_5">X i ) = 1 2 (1 + T i )ê(X i ) + 1 2 (1 T i )(1 ê(X i )) and e Ti (X i , U i ) = 1 2 (1 + T i )e(X i , U i ) + 1 2 (1 T i )(1 e(X i , U i )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Work</head><p>Our work builds upon the literatures on policy learning from observational data and on sensitivity analysis in causal inference.</p><p>Sensitivity analysis. Sensitivity analysis in causal inference tests the robustness of qualitative conclusions made from observational data to model specification or assumptions such as unconfoundedness. In this work, we focus on structural assumptions bounding how unobserved confounding affects selection, without restriction on how unobserved confounding affects outcomes. In particular, we focus on the implications of confounding on personalized treatment decisions.</p><p>Rosenbaum's model for sensitivity analysis assesses the robustness of matched-pairs randomization inference to the presence of unobserved confounding by considering a uniform bound on the impact of confounding on the odds ratio of treatment assignment <ref type="bibr" target="#b21">[22]</ref>. Motivated by a logistic specification, in this model, the odds-ratio for two units with the same covariates X i = X j , which differs due to the units' different values U i , U j for the unobserved confounder, is e log( )(Ui Uj ) , and U i , U j 2 [0, 1] may be arbitrary. We consider a variant, also called the "marginal sensitivity model" in <ref type="bibr" target="#b33">[34]</ref>, which instead bounds the log-odds ratio between e(X i ), e(X i , U i ).</p><p>In the sampling literature, the weight-normalized estimator for population mean is known as the Hajek estimator, and Aronow and Lee <ref type="bibr" target="#b0">[1]</ref> derive sharp bounds on the estimator arising from a uniform bound on the sampling weights, showing a closed-form solution for the solution to the fractional linear program for a uniform bound on the sampling probabilities. <ref type="bibr" target="#b33">[34]</ref> considers bounds on the Hajek estimator, but imposes a parametric model on the treatment assignment probability.</p><p>Sensitivity analysis is also related to the literature on partial identification of treatment effects <ref type="bibr" target="#b16">[17]</ref>. Similar bounds studied in <ref type="bibr" target="#b32">[33]</ref> in the transfer learning setting rely on no knowledge but the law of total probability. Our approach instead uses sensitivity analysis based on the estimated propensities as a starting point and leverages additional information about how far it is from true propensities to achieve tighter bounds that interpolate between the fully-unconfounded and arbitrarilyconfounded regimes. <ref type="bibr" target="#b18">[19]</ref> considers tightening the bounds from the Hajek estimator by adding shape constraints, such as log-concavity, on the cumulative distribution of outcomes Y . <ref type="bibr" target="#b17">[18]</ref> considers sharp partially identified bounds under the assumption of an uniform bound on nominal propensities,</p><formula xml:id="formula_6">sup U | Pr[T = 1 | X] Pr[T = 1 | X, U ]|  c.</formula><p>We focus on the implications of sensitivity analysis for policy-learning based approaches for learning optimal treatment policies from observational data.</p><p>Policy learning from observational data under unconfoundedness. A variety of approaches for learning personalized intervention policies that maximize causal effect have been proposed, but all under the assumption of unconfoundedness. These fall under regression-based strategies <ref type="bibr" target="#b20">[21]</ref> or reweighting-based strategies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28]</ref>, or doubly robust combinations thereof <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref>. Regression-based strategies estimate the conditional average treatment effect (CATE),</p><formula xml:id="formula_7">E[Y (1) Y ( 1) | X],</formula><p>either directly or by differencing two regressions, and use it to score the policy. Without unconfoundedness, however, CATE is not identifiable from the data and these methods have no guarantees.</p><p>Reweighting-based strategies use inverse-probability weighting (IPW) to change measure from the outcome distribution induced by a logging policy to that induced by the policy ⇡. Specifically, these methods use the fact that, under unconfoundedness, V IPW (⇡) is unbiased for V (⇡) <ref type="bibr" target="#b14">[15]</ref>, where</p><formula xml:id="formula_8">V IPW (⇡) = 1 n P n i=1 (1+Ti(2⇡(Xi) 1))Yi 2ê T i (Xi)<label>(1)</label></formula><p>Optimizing V IPW (⇡) can be phrased as a weighted classification problem <ref type="bibr" target="#b2">[3]</ref>. Since dividing by propensities can lead to extreme weights and high variance estimates, additional strategies such as clipping the probabilities away from 0 and normalizing by the sum of weights as a control variate are typically necessary for good performance <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32]</ref>. With or without these fixes, if there are unobserved confounders, none of these are consistent for V (⇡) and learned policies may introduce more harm than good.</p><p>A separate literature in reinforcement learning considers the idea of safe policy improvement by minimizing the regret against a baseline policy, forming an uncertainty set around the presumed unknown transition probabilities between states as in <ref type="bibr" target="#b28">[29]</ref>, or forming a trust region for safe policy exploration via concentration inequalities on the importance-reweighted estimates of policy risk <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Robust policy evaluation and improvement</head><p>Our framework for confounding-robust policy improvement minimizes a bound on policy regret against a specified baseline policy ⇡ 0 , R ⇡0 (⇡) = V (⇡) V (⇡ 0 ). Our bound is achieved by maximizing a reweighting-based regret estimate over an uncertainty set around the nominal propensities. This ensures that we cannot do any worse than ⇡ 0 and may do better, even if the data is confounded.</p><p>The baseline policy ⇡ 0 can be any fixed policy that we want to make sure not to do worse than, or deviate from unnecessarily. This is usually the current standard of care, established from prior evidence, and can be a policy that actually depends on x. Generally, we think of this as the policy that always assigns control. Alternatively, if a reliable estimate of the average treatment effect,</p><formula xml:id="formula_9">E[Y (1) Y ( 1)], is available then ⇡ 0 can be the constant ⇡ 0 (x) = I[E[Y (1) Y ( 1)] &lt; 0].</formula><p>In an agnostic extreme, ⇡ 0 can be the complete randomization policy ⇡ 0 (x) = 1/2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Confounding-robust policy learning by optimizing minimax regret</head><p>If we had oracle access to the true inverse propensities W ⇤ i = 1/e Ti (X i , U i ) we could form the correct IPW estimate by replacing nominal with true propensities in eq. ( <ref type="formula" target="#formula_8">1</ref>). We may go a step further and, recognizing that E[1/e Ti (X i , U i )] = 2, use the empirical sum of true propensities as a control variate by normalizing our IPW estimate by them. This gives rise to the following Hajek estimators of V (⇡) and correspondingly R ⇡0 (⇡)</p><formula xml:id="formula_10">V ⇤ (⇡) = P n i=1 W ⇤ i (1+Ti(2⇡(Xi) 1))Yi P n i=1 W ⇤ i , R⇤ ⇡0 (⇡) = V ⇤ (⇡) V ⇤ (⇡ 0 ) = 2 P n i=1 W ⇤ i (⇡(Xi) ⇡0(Xi))TiYi P n i=1 W ⇤ i</formula><p>It follows by Slutsky's theorem that these estimates remain consistent (if we know W ⇤ i ). Note that had we known W ⇤ i , both the normalization and choice of ⇡ 0 would have amounted to constant shifts and scales to R⇤ ⇡0 (⇡) that would not have changed the choice of ⇡ to minimize the regret estimate. This will not be true of our bound, where both the normalization and the choice of ⇡ 0 will be material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Since the oracle weights W ⇤</head><p>i are unknown, we instead minimize the worst-case possible value of our regret estimate, by ranging over the space of possible values for e Ti (X i , U i ) that are consistent with the observed data and our assumptions about the confounded data-generating process. Specifically, our model restricts the extent to which unobserved confounding may affect assignment probabilities. We first consider an uncertainty set motivated by the odds-ratio characterization in <ref type="bibr" target="#b21">[22]</ref>, which restricts how far the weights can vary pointwise from the nominal propensities. Given a bound &gt; 1, the odds-ratio restriction on e(x, u) is that it satisfy the following inequalities</p><formula xml:id="formula_11">1  (1 ê(x))e(x,u) ê(x)(1 e(x,u))  . (<label>2</label></formula><formula xml:id="formula_12">)</formula><p>This restriction is motivated by (but more general than) considering a logistic model where e(x, u) = (g(x) + u), g is any function, u 2 [0, 1] is bounded without loss of generality, and | |  log( ). Such a model would necessarily give rise to eq. ( <ref type="formula" target="#formula_11">2</ref>). This restriction also immediately leads to an uncertainty set for the true inverse propensities of observed treatments of each unit, 1/e(X i , U i ), which we denote as follows</p><formula xml:id="formula_13">U n = W 2 R n + : a i  W i  b i 8i = 1, . . . , n ,</formula><p>where</p><formula xml:id="formula_14">a i = 1 êTi (X i ) + êTi (X i ) êTi (X i ) , b i = (1 êTi (X i )) + êTi (X i ) êTi (X i )</formula><p>The corresponding bound on empirical regret is R ⇡0 (⇡; U n ), where for any U ⇢ R n + we define</p><formula xml:id="formula_15">R ⇡0 (⇡; U ) = sup W 2U 2 P n i=1 Wi(⇡(Xi) ⇡0(Xi))TiYi P n i=1 Wi</formula><p>We then choose the policy ⇡ in our class that minimizes this regret bound, i.e., ⇡(F, U n , ⇡ 0 ), where</p><formula xml:id="formula_16">⇡(F, U , ⇡ 0 ) 2 argmin ⇡2F R ⇡0 (⇡; U )<label>(3)</label></formula><p>In particular, for our estimate R ⇡0 (⇡; U n ), weight normalization is crucial for only enforcing robustness against consequential realizations of confounding which affect the relative weighting of patient outcomes; otherwise robustness against confounding would simply assign weights to their highest possible bounds for positive Y i T i . If the baseline policy is in the policy class F, it already achieves 0 regret; thus, minimizing regret necessitates learning regions of policy treatment assignment where evidence from observed outcomes suggests benefits in terms of decreased loss. Different baseline policies ⇡ 0 = 0, 1 structurally change the solution to the adversarial subproblem by shifting the contribution of the loss term Y i T i (⇡(X i ) ⇡ 0 ) to emphasize improvement upon the baseline.</p><p>Budgeted uncertainty sets to address "local" confounding. Our approach can be pessimistic in ensuring robustness against worst-case realizations of unobserved confounding "globally" for each unit, whereas concerns about unobserved confounding may be restricted to a subset of the population, due to subgroup risk factors or outliers. For the Rosenbaum model in hypothesis testing, this has been recognized by <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> who address it by limiting the average of the unobserved propensities by an additional sensitivity parameter. Motivated by this, we next consider an alternative uncertainty set, where we fix a budget ⇤ for how much the weights can diverge from the nominal inverse propensity weights in total. Specifically, letting Ŵi = 1 /ê T i (Xi), we construct the uncertainty set</p><formula xml:id="formula_17">U ,⇤ n = n W 2 R n + : P n i=1 |W i Ŵi |  ⇤, a i  W i  b i 8i = 1, . . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>, n o</head><p>When plugged into eq. ( <ref type="formula" target="#formula_16">3</ref>), this provides an alternative policy choice criterion that is less conservative. We suggest to calibrate ⇤ as a fraction ⇢ &lt; 1 of the total deviation allowed by U n . Specifically, ⇤ = ⇢ P n i=1 max( Ŵi a i , b i Ŵi ). This is the approach we take in our empirical investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Improvement Guarantee</head><p>We next prove that if we appropriately bounded the potential hidden confounding then our worst-case empirical regret objective is asymptotically an upper bound on the true population regret. On the one hand, since our objective is necessarily non-positive if ⇡ 0 2 F, this says we do no worse. On the other hand, if our objective is negative, which we can check by just evaluating it, then we are assured some strict improvement. Our result is generic for both U n and U ,⇤ n . Our upper bound depends on the complexity of our policy class. Define its Rademacher complexity:</p><formula xml:id="formula_18">R n (F) = 1 2 n P ✏2{ 1,+1} n sup ⇡2F 1 n P n i=1 ✏ i ⇡(X i )</formula><p>All the policy classes we consider have p n-vanishing complexities, i.e., R n (F) = O(n 1/2 ).</p><p>Theorem 1. Suppose that (1/e(X 1 , U 1 ), . . . , 1/e(X n , U n )) 2 U and that ⌫  e(x, u)  1 ⌫ for some ⌫ &gt; 0 and |Y |  C for some C 1. Then for any &gt; 0 such that n ⌫ 2 log(5/ )/2, we have that with probability at least 1 ,</p><formula xml:id="formula_19">R ⇡0 (⇡) = V (⇡) V (⇡ 0 )  R ⇡0 (⇡; U ) + 2R n (F) + C ⌫ q 8 log(5/ ) n 8⇡ 2 F<label>(4)</label></formula><p>In particular, if we let ⇡ = ⇡(F, U , ⇡ 0 ) be as in eq. (3) then eq. ( <ref type="formula" target="#formula_19">4</ref>) holds for ⇡, which minimizes the right hand side. So, if the objective R ⇡0 (⇡; U ) is negative, we are (almost) assured of getting some improvement on ⇡ 0 . At the same time, so long as ⇡ 0 2 ⇧, the objective is necessarily non-positive, so we are also (almost) assured of doing no worse than ⇡ 0 . Our guarantee of improvement holds, under well-specification, without requiring effect identification due to hidden confounding. Thus, Theorem 1 exactly captures the appeal of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Calibration of the uncertainty parameter</head><p>In our framework, appropriate choice of is both important for ensuring that we avoid harm and will be context-dependent. The assumption that there exists a finite &lt; 1 that satisfy eq. ( <ref type="formula" target="#formula_11">2</ref>) is itself untestable, just like unconfoundedness (which corresponds to = 1). Since we focus on enabling safe policy learning in domains where one errs toward safety in case of ignorance, if absolutely nothing is known then = 1 is the right choice and there is no hope for strictly safe improvement. However, practitioners generally have domain-level knowledge on the missing variables that may impact selection. This can guide the choice of &lt; 1, which our method leverages to offer some improvement while ensuring safety. In particular, one way that the value of can be calibrated is by judging its value against the discrepancies in estimated propensities that are induced by omitting observed variables <ref type="bibr" target="#b9">[10]</ref>. Then, determining a reasonable upper bound for can be phrased in terms of whether one thinks one has omitted a variable that could have increased or decreased the probability of treatment by as much as a particular observed variable. For example, a bound for can be implied by claiming one has not omitted a variable with as much impact on treatment as does, say, age, if age were observed. Additionally, when alternative outcome data is available, other approaches such as negative controls can be used to provide a lower bound for <ref type="bibr" target="#b15">[16]</ref>. If one knows that the treatment does not have an effect on a particular outcome but one is observed in the data, then must be sufficiently large to invalidate that observed effect. These tools can be combined to derive a reasonable range for in practice. Since our focus is on safety, we suggest to err toward larger .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Optimizing Robust Policies</head><p>We next discuss how to optimize the policy optimization problem in eq. ( <ref type="formula" target="#formula_16">3</ref>). We focus on differentiable parametric policies, F = {⇡( • ; ✓) : ✓ 2 ⇥}, such as logistic policies. We first discuss how to solve the worst-case regret subproblem for a fixed policy, which we will then use to develop our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dual Formulation of Worst-Case Regret</head><p>The minimization in eq. ( <ref type="formula" target="#formula_16">3</ref>) for U = U n involves an inner supremum, namely R ⇡0 (⇡; U n ). Moreover, this supremum over weights W does not on the face of it appear to be convex. We next proceed to characterize this supremum, formulate it as a linear program, and, by dualizing it, provide an efficient procedure for finding the pessimal weights.</p><p>For compactness and generality, we address the optimization problem Q(r; U n ) parameterized by an arbitrary reward vector r 2 R n , where</p><formula xml:id="formula_20">Q(r; U ) = max W 2U P n i=1 riWi / P n i=1 Wi.<label>(5)</label></formula><p>To recover R ⇡0 (⇡; U ), we would simply set r i = 2(⇡(X i ) ⇡ 0 (X i ))T i Y i . Since U n involves only linear constraints on W , eq. ( <ref type="formula" target="#formula_20">5</ref>) for U = U n is a linear fractional program. We can reformulate it as a linear program by applying the Charnes-Cooper transformation <ref type="bibr" target="#b4">[5]</ref>, requiring weights to sum to 1, and rescaling the pointwise bounds by a nonnegative scale factor t. We obtain the following equivalent linear program, where we let w 2 R n + denote the normalized weights:</p><formula xml:id="formula_21">Q(r; U n ) = max t,w 0 P n i=1 r i w i : P n i=1 w i = 1; ta i  w i  tb i , 8 i = 1, . . . , n<label>(6)</label></formula><p>The dual problem to eq. ( <ref type="formula" target="#formula_21">6</ref>) has dual variables 2 R for the weight normalization constraint and u, v 2 R n + for the lower bound and upper bound constraints on weights, respectively, and is given by</p><formula xml:id="formula_22">min u,v 0, 2R { : b | v + a | u 0, v i u i + r i 8 i = 1...n}<label>(7)</label></formula><p>We use this to show that solving the adversarial subproblem requires only sorting the data and ternary search to optimize a unimodal function, generalizing the result of Aronow and Lee <ref type="bibr" target="#b0">[1]</ref> for arbitrary pointwise bounds on the weights. Crucially, the algorithmically efficient solution will allow for faster subproblem solutions when optimizing our regret bound over policies in a given policy classes.</p><p>Theorem 2 (Normalized optimization solution). Let (i) denote the ordering such that r</p><formula xml:id="formula_23">(1)  r (2)  • • •  r (n) . Then, Q(r; U n ) = (k ⇤ ),</formula><p>where k ⇤ = inf{k = 1, . . . , n + 1 : (k) &lt; (k 1)} and</p><formula xml:id="formula_24">(k) = P i&lt;k a (i) r (i) + P i k b (i) r (i) P i&lt;k a (i) + P i k b (i)<label>(8)</label></formula><p>Moreover, (k) is a discrete concave unimodal function.</p><p>Next we consider Q(r; U ,⇤ n ). Write an extended formulation for U ,⇤ n using only linear constraints:</p><formula xml:id="formula_25">U ,⇤ n = n W 2 R n + : 9d s.t. P n i=1 d i  ⇤, d i W i Ŵi , d i Ŵi W i , a i  W i  b i 8i o</formula><p>This immediately shows that Q(r; U ,⇤ n ) remains a fractional linear program. Indeed, letting, w 0 = P n i=1 Ŵi a similar Charnes-Cooper transformation as used above with the additional normalization d 0 i = d i t yields a non-fractional linear programming formulation:</p><formula xml:id="formula_26">Q(r; U ,⇤ n ) = max d,w,t 0 ⇢ P n i=1 w i r i : P i d 0 i ⇤t  0, P i w i = 1, a i t  w i  b i t 8i d 0 i  w i + w 0 i t, d 0 i  w i w 0 i t, 8i</formula><p>The corresponding dual problem is:</p><formula xml:id="formula_27">min g,h,u,v,⌫ 0, 2R ⇢ : v u + g h + r, v g + h b | v + a | u ⇤⌫ + g | w 0 + h | w 0 = 0 As Q(r; U ,⇤ n</formula><p>) remains a linear program, we can easily solve it using off-the-shelf solvers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Optimizing Parametric Policies</head><p>We next consider the case where F = {⇡(•; ✓) : ✓ 2 ⇥}, ⇥ is convex (usually ⇥ = R m ), and ⇡(x; ✓) is differentiable with respect to ✓. We suppose that r ✓ ⇡(x; ✓) is given as an evaluation oracle. An example is logistic policies where ⇡(X; ↵, ) = (↵ + | X) and ⇥ = R d+1 . Since 0 (z) = (z)(1 (z)), evaluating derivatives is simple.</p><p>Our method follows a parametric optimization approach <ref type="bibr" target="#b25">[26]</ref>. Note that Q(r; U ) is convex in r since it is a maximum over linear functions in r. Correspondingly, its subdifferential at r is given by the argmax set:</p><formula xml:id="formula_28">@ r Q(r; U ) = n W P n i=1 Wi : W 2 U, P n i=1 Wiri P n i=1 Wi Q(r; U ). o If we set r i (✓) = 2(⇡(X i ; ✓) ⇡ 0 (X i ))T i Y i , so that Q(r; U ) = R ⇡0 (⇡(•; ✓); U ), then @ri(✓) @✓j = 2T i Y i @⇡(Xi;✓) @✓j</formula><p>. Although F (✓) := R ⇡0 (⇡(•; ✓); U ) may not be convex in ✓, this suggests a subgradient descent approach. Let</p><formula xml:id="formula_29">g(✓; W ) = r ✓ 2 P n i=1 Wi(⇡(Xi;✓) ⇡0(Xi))TiYi P n i=1 Wi = 2 P n i=1 WiTiYir ✓ ⇡(Xi;✓) P n i=1 Wi</formula><p>.</p><p>Note that whenever @ r Q(r(✓); U ) = {W/ P n i=1 W i } is a singleton then g(✓; W ) is in fact a gradient of F (✓).</p><p>At each step, our algorithm starts with a current value of ✓, then proceeds by finding the weights W that realize R ⇡0 (⇡(• ; ✓) by using an efficient method as in the previous section, and then takes a step in the direction of g(✓; W ). Using this method, we can optimize policies over both the unbudgeted uncertainty set U n and the budgeted uncertainty set U ,⇤ n . Because descent is not always guaranteed at each step, at the end, we return the value of ✓ that corresponds to the best objective value seen so far. Our method is summarized in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Simulated data. We first consider a simple linear model specification demonstrating the possible effects of significant confounding on inverse-propensity weighted estimators.</p><formula xml:id="formula_30">⇠ ⇠ Bern( 1 /2), X ⇠ N (µ x , I 5 ), U = I[Y i (1) &lt; Y i ( 1)] Y (t) = | 0 x + 1 /2(t + 1) | treat x + ↵ 1 /2(t + 1) + ⌘⇠t + !⇠ + ✏</formula><p>The constant treatment effect is 2.5 with the linear interaction treat = [ 1.5, 1, 1.5, 1, 0.5]. The covariate mean is µ x = [ 1, .5, 1, 0, 1]. The noise term ⇠ affects outcomes with coefficients ⌘ = 2, ! = 1, in addition to a uniform noise term ✏ ⇠ N (0, 1). We let the nominal propensities be logistic in X, ê(X i ) = ( | x) with = [0, .75, .5, 0, 1, 0], and we generate T i according to the true propensities, which we set to e(X i , U i ) = 4+5U +ê(Xi) (2 5U )   6ê <ref type="bibr">(Xi)</ref> .</p><p>We compare the policies learned by a variety of methods. We consider two commonplace standard methods that assume unconfoundedness: the logistic policy minimizing the IPW estimate with Algorithm 1: Parametric Subgradient Method 1: Input: step size ⌘ 0 , step-schedule exponent  2 (0, 1], initial iterate ✓ 0 , number of iterations N 2: for t = 0, . . . , N 1 do: ✓ t+1 Projection ⇥ (✓ t ⌘ t g(✓ t ; W )) return ✓ arg mint lt nominal propensities <ref type="foot" target="#foot_1">1</ref> and the direct comparison policy gotten by estimating CATE using causal forests and comparing it to zero [CF <ref type="bibr">; 31]</ref>. We compare these to our methods with a never-treat baseline policy ⇡ 0 (x) = 0: our robust logistic policy using the unbudgeted uncertainty set, our robust logistic policy using the budgeted uncertainty set and multipliers ⇢ = 0.5, 0.3, 0.2. For each of these we vary the parameter in {0.3, 0.4, . . . , 1.6, 1.7, 2, 3, 4, 5}. The causal forest policy achieves slightly better regret than the IPW policy, but remains confounded. By construction, for log( ) very small (left end of plot), the confounding-robust approach tracks IPW with the nominal propensities and incurs some regret relative to control. When we add robustness, our policies achieve substantial improvements. As log( ) increases, the learned robust logistic policies are able to achieve negative regret, meaning we improve upon ⇡ 0 . As log( ) grows very large (right end of plot), we are very robust to any size of confounding and almost always default to ⇡ 0 as a policy that ensures never doing worse and our true regret converges to 0. Even in this extreme example of confounding where the true propensities achieve the odds-ratio bounds, the budgeted version is able to attain similar improvements to the unbudgeted version for ⇢ = 0.3, 0.2, and uniformly better improvements for ⇢ = 0.5. These improvements are relatively insensitive to the exact value of ⇢ and the budgeted version is able to achieve improvement even when the budgeted uncertainty set is misspecified. The best improvements for the parametric policies are achieved at log( ) = 1.5, consistent with the model specification.</p><p>Assessment with Clinical Data: International Stroke Trial. We build an evaluation framework for our methods from real-world data, where the counterfactuals are not known, by simulating confounded selection into a training dataset, and estimating out-of-sample policy regret on a held-out "test set" from the completely randomized controlled trial. We study the International Stroke Trial (IST), restricting attention to two treatment arms from the original factorial design: the treatment arm of both aspirin and heparin (high dose) (T = 1) vs. only aspirin (T = 1) treatment arms, numbering 7233 cases with Pr[T = 1] = 1 /3 <ref type="bibr" target="#b7">[8]</ref>. We defer some details about the dataset to Appendix C. Findings from the study suggest clear reduction in adverse events (recurrent stroke or death) from aspirin, whereas heparin efficacy is inconclusive since small (nonsignificant) benefit on rates of death at 6 months was offset by greater incidence of other adverse events such as hemorrhage or cranial bleeding. We construct an evaluation framework from the dataset by first sampling a split into a training set S train and a held-out test set S test , and subsampling a final set of initial patients, whose data is then used to train treatment assignment policies. We generate nominal selection probabilities into the final training set, letting Z = 1 denote inclusion, as Pr[Z = 1 | X age ] = 0.6 + 0.2X age , where X age 2 [0, 1] is rescaled. Then the nominal propensities of treatment assignment in the final training set are Pr[Z = 1, T = 1 | X] = 0.2 + 0.1X age . We introduce confounding by censoring the treated patients with the worst 10% of outcomes, and the 10% best patients in the control group.</p><p>The original trial measured a set of clinical outcomes including death, stroke recurrence, adverse side effect, and full recovery at six months: we scalarize these outcomes as a composite loss function. A a. Out-of-sample policy regret b. % of patients with ⇡(X) &gt; 0.4 c. Avg death prognosis in treated Figure <ref type="figure">2</ref>: Comparison of policy performance on clinical trial (IST) data as increases difference-in-means estimate of the ATE for the composite score in full data is significant at 0.13, suggesting that heparin is overall harmful. Without access to the true counterfactual outcomes for patients, our oracle estimates are IPW-based estimates from the held-out RCT data with probabilities of treatment assignment as p 1 = 2 3 and p 1 = 1 3 . We use an out-of-sample Horvitz-Thompson estimate of policy regret relative to ⇡ 0 (x) = 0 based on the held-out dataset S test , R test ⇡0 (⇡) =</p><formula xml:id="formula_31">1 |Stest| P i2Stest Y i T i ⇡(X i ) 1 p T i</formula><p>. In Fig. <ref type="figure">2a</ref>, we evaluate on 10 draws from the dataset, comparing our policies against the vanilla IPW estimator</p><formula xml:id="formula_32">P i Yi Pr[⇡i=Ti]</formula><p>Pr[T =Ti] with a probabilistic policy, and assigning based on the sign of the CATE prediction from causal forests <ref type="bibr" target="#b30">[31]</ref>. The selected datasets average a size of n train = 2430. We evaluate logistic parametric policies (CRLogit) and budgeted (CRLogit.L1) with ⇢ = 0.5. For the parametric policies, we optimize with the same parameters as earlier. We evaluate log( ) = 0.1, 0.2, every 0.025 between 0.25 and 0.45, every 0.2 between log( ) = 0.5, 1.5 and = 2. For small values of log( ), our methods perform similarly as IPW. As log( ) increases, our methods achieve policy improvement, though the L1-budgeted method (CRLogit.L1) achieves worse performance. For log( ) &gt; 0.9, the robust policy essentially learns the all-control policy; our finite-sample regret estimator simply indicates good regret for a neglible number of patients <ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref>.</p><p>In Figs. <ref type="figure">2b-2c</ref>, we study the behavior of the robust policies. The IST trial recorded a prognosis score of probability of death at 6 months for patients, using an externally validated model, which we do not include in the training data, but use to assess the validity of our robust policy. In Fig. <ref type="figure">2c</ref>, we consider the average prognosis score of death for among patients treated with ⇡(X) &gt; 0.4. In Fig. <ref type="figure">2b</ref>, for log( ) 2 [0.3, 0.5], the policy considers treating 1 20% of patients and the subsequent average prognosis score of the population under consideration increases, indicating that the policy is learning and treating on appropriate indicators of severity from the available covariates. For log( ) &gt; 0.9, the noise in the prognosis score is due to the small treated subgroups (while the unbudgeted policy does not learn a policy that improves upon control, so we default to control and truncate the plot).</p><p>Our learned policies suggest that improvements from heparin may be seen in the highest-risk patients, consistent with the findings of <ref type="bibr" target="#b1">[2]</ref>, a systematic review comparing anticoagulants such as heparin against aspirin. They conclude from a study of a number of trials, including IST, that heparin provides little therapeutic benefit, with the caveat that the trial evidence base is lacking for the highest-risk patients where heparin may be of benefit. Thus, our robust method appropriately treats those, and only those, who stand to benefit from the more aggressive treatment regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We developed a framework for estimating and optimizing for robust policy improvement, which optimizes the minimax regret of a candidate personalized decision policy against a baseline policy. We optimize over uncertainty sets centered at the nominal propensities, and leverage the optimization structure of normalized estimators to perform policy optimization efficiently by subgradient descent on the robust risk. Assessments on synthetic and clinical data demonstrate the benefits of robust policy improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Out of sample policy performance on synthetic data, where the true generating log( ) ⇤ = 1.5.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>We also tried the self-normalized variant of Swaminathan and Joachims<ref type="bibr" target="#b26">[27]</ref> and report the results in Sec. B in the appendix.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This material is based upon work supported by the National Science Foundation under Grant No. 1656996. Angela Zhou is supported through the National Defense Science &amp; Engineering Graduate Fellowship Program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interval estimation of population means under unknown but bounded probabilities of sample selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Aronow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Anticoagulants versus antiplatelet agents for acute ischaemic stroke</title>
		<author>
			<persName><forename type="first">E</forename><surname>Berge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Sandercock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>The Cochrane Library of Systematic Reviews</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The offset tree for learning with partial labels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Counterfactual reasoning and learning systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quinonero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Portugaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Snelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Programming with linear fractional functionals</title>
		<author>
			<persName><forename type="first">A</forename><surname>Charnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Doubly robust policy evaluation and optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An extended sensitivity analysis for heterogeneous unmeasured confounding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hasegawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The international stroke trial (ist): a randomised trial of aspirin, subcutaneous heparin, both, or neither among 19435 patients with acute ischaemic stroke. international stroke trial collaborative group</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S T C</forename><surname>Group</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sensitivity analysis for matched pair analysis of binary data: From worst case to average case analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Calibrating sensitivity analyses to observed covariates in observational studies</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Small</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="803" to="811" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Causal Inference for Statistics, Social, and Biomedical Sciences</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recursive partitioning for personalization using observation data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-fourth International Conference on Machine Learning</title>
		<meeting>the Thirty-fourth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Kitagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tetenov</surname></persName>
		</author>
		<title level="m">Empirical welfare maximization</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Probability in Banach Spaces: isoperimetry and processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ledoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Talagrand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unbiased offline evaluation of contextual-banditbased news article recommendation algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM international conference on web search and data mining</title>
		<meeting>the fourth ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Negative controls: A tool for detecting confounding and bias in observational studies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lipsitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Tchetgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Social Choice with Partial Knoweldge of Treatment Response</title>
		<author>
			<persName><forename type="first">C</forename><surname>Manski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>The Econometric Institute Lectures</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identification of treatment effects under conditional partial independence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Masten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shape-constrained partial identification of a population mean under unknown probabilities of sample selection</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">W</forename><surname>Miratrix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Zubizarreta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Petrik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chow</surname></persName>
		</author>
		<title level="m">Safe policy improvement by minimizing robust baseline regret. 29th Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance guarantees for individualized treatment rules</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1180</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Observational Studies</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rosenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer Series in Statistics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating causal effect of treatments in randomized and nonrandomized studies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Comments on &quot;randomization analysis of experimental data: The fisher randomization test comment</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">371</biblScope>
			<biblScope unit="page" from="591" to="593" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lectures on parametric optimization: An introduction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Still</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization Online</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The self-normalized estimator for counterfactual learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Counterfactual risk minimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High confidence policy improvement</title>
		<author>
			<persName><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Theocharous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<title level="m">Efficient policy learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Estimation and inference of heterogeneous treatment effects using random forests</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>just-accepted</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimal and adaptive off-policy evaluation in contextual bandits</title>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dudik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Neural Information Processing Systems</title>
		<meeting>Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transfer learning in multi-armed bandits: A causal approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/186</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2017/186" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1340" to="1346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sensitivity analysis for inverse probability weighting estimators via the percentile bootstrap</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
