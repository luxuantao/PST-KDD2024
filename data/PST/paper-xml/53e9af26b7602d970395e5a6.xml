<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Max-Cover in Map-Reduce</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Flavio</forename><surname>Chierichetti</surname></persName>
							<email>chierichetti@di.uniroma1.it</email>
						</author>
						<author>
							<persName><forename type="first">Ravi</forename><surname>Kumar</surname></persName>
							<email>ravikumar@yahoo-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Tomkins</surname></persName>
							<email>atomkins@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica La Sapienza Univ. of Rome</orgName>
								<address>
									<postCode>00198</postCode>
									<settlement>Roma</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Yahoo! Research</orgName>
								<address>
									<addrLine>701 First Avenue Sunnyvale</addrLine>
									<postCode>94089</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Google, Inc</orgName>
								<address>
									<addrLine>1600 Amphitheater Parkway Mountain View</addrLine>
									<postCode>94043</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<postCode>2010</postCode>
									<settlement>Raleigh</settlement>
									<region>North Carolina</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Max-Cover in Map-Reduce</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1C43F46B507EA19AC4BCE5DDCF38D628</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>. F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems Algorithms</term>
					<term>Experimentation</term>
					<term>Theory Maximum cover</term>
					<term>Greedy algorithm</term>
					<term>Map-reduce</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The NP-hard Max--Cover problem requires selecting sets from a collection so as to maximize the size of the union. This classic problem occurs commonly in many settings in web search and advertising. For moderately-sized instances, a greedy algorithm gives an approximation of (1 -1/ ). However, the greedy algorithm requires updating scores of arbitrary elements after each step, and hence becomes intractable for large datasets.</p><p>We give the first max cover algorithm designed for today's large-scale commodity clusters. Our algorithm has provably almost the same approximation as greedy, but runs much faster. Furthermore, it can be easily expressed in the Map-Reduce programming paradigm, and requires only polylogarithmically many passes over the data. Our experiments on five large problem instances show that our algorithm is practical and can achieve good speedups compared to the sequential greedy algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Say a search engine wishes to focus its attention on one thousand queries such that as many users as possible will see one of them. Selecting these queries is a maximum coverage (or max cover ) problem. These problems arise whenever we seek the "best" collection of items, but the items may partially overlap in the value they provide, and should not be double-counted. All of the following are instances of max cover problems: how should one select 100 movie stars that reach as many people on the planet as possible? If a web search engine has resources to change the appearance of results from 500 websites, how should the websites be chosen to touch as many queries as possible? What collection of five vitamins wards off the most ailments? Where should a set of charity dropboxes be placed to be available to as many people as possible?</p><p>In fact, all these examples are instances of the canonical problem of this type, Max--Cover, which asks to select sets from a family of subsets of a universe so that their union is as large as possible. The problem is NP-hard, but can be approximated by a simple greedy algorithm (called Greedy) to within a factor of (1 -1/ ) of the best possible. However, after selecting the best remaining set, Greedy must remove all elements of that set from other sets. The number of sets that must be touched to perform this update could be enormous, and the operation must be repeated for each set to be output. For disk-resident datasets, Greedy is not a scalable approach.</p><p>Nonetheless, max cover problems arise frequently at large scale. Dasgupta et al. <ref type="bibr" target="#b8">[8]</ref>, for instance, employ this formulation in pursuit of discovering fresh content for web crawler scheduling policies: finding as much new content as possible by crawling at most webpages. Saha and Getoor <ref type="bibr" target="#b34">[34]</ref> used the max cover formulation for a multi-topic blog-watch application: finding at most blogs to read interesting articles on a list of topics.</p><p>Our motivation for studying max cover problems arose in one of the examples described above: we wished to find a large but bounded number of web hosts that appeared within the top-three results for as many web queries as possible. As the analysis was to take place over weeks or months of data, the scale of the problem quickly reached tens of billions of queries, making the performance of Greedy unacceptable. Thus, we began to pursue a version of this classical algorithm that could be implemented on the massively parallel large-scale data processing clusters based on the Map-Reduce paradigm <ref type="bibr" target="#b9">[9]</ref> that are increasingly available today. These clusters are characterized by a focus on sequential data processing, another challenge in the context of the existing algorithm.</p><p>Max cover problems arise frequently in the context of web search and advertising, and when they do, they are often at daunting data scales. In addition to the host analysis problem described above, consider an advertiser interested in placing banner ads at various points of the Yahoo! network. The advertiser pays a fixed amount for each impression, independent of the location, and wishes to reach as many users as possible. Or consider an experiment in which we wish to deploy a manually-generated result in response to a small set of queries that will reach users from as many US zip codes as possible. Algorithmic tools for these natural questions are not generally available using the programming paradigms at our disposal.</p><p>Our contributions. In this paper we develop a Map-Reduce-based algorithm for Max--Cover. Specifically, our algorithm obtains a (1 -1/ -)-approximation and can be implemented to run in (poly( ) log 3 (</p><p>)) Map-Reduce steps over the input instance, where is the number of elements and is the number of sets. Thus, we end up in the best of two worlds: nearly matching the performance of Greedy, while obtaining an algorithm that can be implemented in the scalable Map-Reduce framework. This is one of the few instances of a non-trivial Map-Reduce realization of a classical algorithm with provable guarantees of performance (more on this point in Section 3).</p><p>The main idea behind our algorithm, which we call Mr-Greedy, is to simultaneously choose many sets to include in the solution. This objective cannot be met at every stepidentifying when to perform the parallel choice is the crux of our algorithm. In this regard, we derive inspiration from the parallel algorithm of Berger, Rompel, and Shor <ref type="bibr" target="#b2">[2]</ref> for Set-Cover. Our problem, however, requires new ideas to carry out the analysis; see Section 4. To this end, we consider a weaker sequential version of Greedy, called MrGreedy, and show that its approximation ratio is essentially equal to that of Greedy. Then, we show that MrGreedy (whose parallel running time we can bound to be polylogarithmic) has an approximation ratio at least as good as the weaker sequential algorithm. In addition, MrGreedy has a strong output guarantee that we call prefix-optimality: for each , the prefix of length in the ordering output by MrGreedy is a (1 -1/ -)-approximation to the corresponding Max--Cover.</p><p>We then show how to implement our algorithm in the Map-Reduce setting, where the parallel running time translates to the number of Map-Reduce steps over the input. A crucial feature of MrGreedy is that it does not rely on main memory to store the elements of a set or the sets to which an element belongs. (Making such assumptions can be unrealistic -a common website may appear in the top results for tens of millions of queries.) We conduct experiments on five large real-world instances. Our experiments show that it is feasible to implement MrGreedy in practice, and obtain reasonable speedup over Greedy. Moreover, the approximation performance of MrGreedy is virtually indistinguishable from that of Greedy.</p><p>Since the set system can be alternatively viewed as a bipartite graph (elements and sets on each side and elementset membership determines the edges), the number of elements coverable with sets can be viewed as a purely structural property of bipartite graphs. In this aspect, our work paves the way for the development of better understanding of bipartite graphs such as how much duplication exists in the covering of elements by sets; this is along the lines of <ref type="bibr" target="#b23">[23]</ref>.</p><p>Organization of the paper. The rest of the paper is organized as follows. Section 2 contains the necessary background material on Max--Cover, Greedy, and Map-Reduce. Section 3 discusses related work on both Max--Cover and Map-Reduce. Section 4 presents our main result: Mr-Greedy, a parallel algorithm for Max--Cover, and its Map-Reduce realization. Section 5 discusses the preliminary experimental results on the implementation of this algorithm. Section 6 contains the concluding thoughts and directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head><p>In this section we set up the basic notation and provide the necessary background for the problem and the computational model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Max--Cover problem</head><p>Let = {1, . . . , } be a universe of elements. Let , | | = be a family of non-empty subsets of .</p><formula xml:id="formula_0">Given ′ ⊆ , the coverage cov( ′ ) of ′ is simply cov( ′ ) = ∪ ∈ ′ .</formula><p>Without loss of generality, we can assume that cov( ) = .</p><p>Definition 1 (Max -cover). Given an integer &gt; 0, * ⊆ is a max -cover if | * | = and the coverage of * is maximized over all subsets of of size .</p><p>Since the Max--Cover problem is known to be NP-hard, we will focus on provably good approximation algorithms.</p><p>Definition 2 ( -approximate -cover). For &gt; 0, a set ′ ⊆ , | ′ | ≤ , is an -approximate max -cover if for any max -cover * , cov( ′ ) ≥ ⋅ cov( * ).</p><p>A polynomial-time algorithm producing an -approximate max -cover for every instance of Max--Cover is said to be an -approximation algorithm for the problem.</p><p>We define the degree deg( ) of an element ∈ to be the number of sets containing , i.e., deg( ) = |{ ∈ | ∋ }|. We define the maximum degree Δ of an instance as Δ = max ∈ deg( ). We also define the degree of an element ∈ with respect to a subfamily ′ ⊆ as deg</p><formula xml:id="formula_1">′ ( ) = |{ ∈ ′ | ∋ }|.</formula><p>Weighted, budgeted versions. In the weighted version of the problem, the universe is equipped with a weight function :</p><formula xml:id="formula_2">→ R + . For ′ ⊆ , let ( ′ ) = ∑ ∈ ′ ( ). For ′ ⊆ , let ( ′ ) = (∪ ∈ ′ ) = ∑ ∈ ∪ ∈ ′ ( ).</formula><p>The goal is find * ⊆ such that | * | = and has maximum weight ( * ).</p><p>In the budgeted version, each set ∈ is equipped with a cost : → R + . The cost of ′ ⊆ is given by ( ′ ) = ∑ ∈ ′ ( ). Given a budget , the goal now is to output * ⊆ whose cost is at most and the weight of the elements it covers is maximized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Map-Reduce model</head><p>We focus on the Map-Reduce model of computation <ref type="bibr" target="#b9">[9]</ref>. In the Map-Reduce model, computations are distributed across several processors, split as a sequence of map and reduce steps. The map step consumes a stream of key-value tuples and outputs a set of (possibly different or amended) key-value tuples. In the reduce step, all tuples with same key are brought and processed together. For example, transposing an adjacency list -a crucial component in our algorithm -can be done effortlessly in Map-Reduce: given tuples of the form ⟨ ; 1, . . . , ⟩, meaning that the element (key) is present in sets 1, . . . , , the transpose operation will produce an output of the form ⟨ ; 1, . . . , ℓ ⟩, where 1, . . . , ℓ are the elements of the set (key). Realizing this in Map-Reduce is easy: the map step outputs tuples of the form ⟨ ; ⟩ while the reduce step groups together all the elements that belong to the set , the key.</p><p>Map-Reduce is a powerful computational model that has proved successful in enabling large-scale web data mining. For example, many matrix-based algorithms, such as PageRank computation <ref type="bibr" target="#b31">[31]</ref>, have been implemented successfully in the Map-Reduce model and used to process gargantuan snapshots of the web graph. While the general Map-Reduce model allows for greater flexibility, to make our algorithms practical and truly scalable, we list three requirements on their efficiency.</p><p>• Many Map-Reduce algorithms can be iterative; PageRank for instance is such an algorithm. In most scenarios, the number of iterations can be a pre-determined constant, albeit large. In our case, we require that the number of iterations is at most polylogarithmic in the input size.</p><p>• While in principle the output of the map or reduce step can be much bigger than the input, in our case, we require that it still remain linear in the input size. Also, we require the map and reduce steps to run in time linear in their input sizes.</p><p>• We require that the map/reduce steps use constant or logarithmic amount of memory. In particular, we do not assume that it is possible to store the elements of a set or all the sets to which an element belongs, in memory. In other words, we look for an algorithm that works in a truly streaming fashion, without relying on the main memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The classical Greedy algorithm</head><p>A classical greedy algorithm achieves a constant-factor approximation to Max--Cover. The factor of approximation is 1 -1/ ≈ .63. We first recall this algorithm.</p><p>Algorithm 1 The Greedy algorithm. Require: 1, . . . ,</p><p>, and an integer 1: while &gt; 0 do 2:</p><p>Let be a set of maximum cardinality 3: Output 4:</p><p>Remove and all elements of from other remaining sets 5:</p><p>= -1</p><p>Note that this algorithm is blatantly sequential: after a set is picked to be included in the solution, bookkeeping is necessary to keep the remaining sets and elements up-todate. This may not be expensive if the data structures to maintain the element-set memberships can be held in memory and if passes over the data is acceptable; however, neither of these is feasible with massive data. As we will see, MrGreedy also has an overall greedy and an iterative flavor, but if an opportunity permits, it will try to pick many sets in parallel to be included in the solution. Doing this, while simultaneously preserving the approximation guarantee is the balance our algorithm will strive to achieve. Since our algorithm is iterative, it does not need to keep the data structures in main memory. We rely on the Map-Reduce framework and the transpose operation in order to keep the element-set memberships up-to-date.</p><p>Note also that the Greedy algorithm can be easily extended to output a total ordering of the input sets 1, . . . , , with the guarantee that the prefix of length , for each , of this ordering will be a (1 -1/ )-approximation to the corresponding Max--Cover; we call this the prefix-optimality property. In fact, our algorithm will also enjoy this prefixoptimality property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RELATED WORK</head><p>There are two principal lines of work that we need to address. The first is the extensive literature on the Max--Cover and related combinatorial optimization problems in sequential and parallel settings. The second is the burgeoning body of work on Map-Reduce-based algorithms for large web mining problems.</p><p>The complexity of Max--Cover. The study of Max--Cover and the related Set-Cover problem is classical. As we mentioned earlier, Max--Cover is NP-hard <ref type="bibr" target="#b15">[15]</ref>. Hochbaum and Pathria <ref type="bibr" target="#b17">[17]</ref> (see also <ref type="bibr" target="#b16">[16]</ref>) present an elegant analysis of the Greedy algorithm for Max--Cover, proving its (1 -1/ )-approximation guarantee. Khuller, Moss, and Naor <ref type="bibr" target="#b21">[21]</ref> extended the Greedy algorithm to the budgeted case, maintaining the same approximation guarantee. Gandhi, Khuller, and Srinivasan <ref type="bibr" target="#b14">[14]</ref> consider the related problem of choosing the minimum number of sets to cover at least elements, which is harder to approximate than Set-Cover and therefore Max--Cover. It is also known that Max--Cover cannot be approximated to 1 -1/ -, unless NP ⊆ DTIME( (log log ) ) <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b21">21]</ref>. Thus the sequential complexity of Max--Cover is understood well. To the best of our knowledge, the parallel or distributed complexity of Max--Cover has not been examined yet.</p><p>The related Set-Cover problem has been studied in a parallel setting. Berger, Rompel, and Shor <ref type="bibr" target="#b2">[2]</ref> give an NC algorithm that approximates Set-Cover to (log ), which is optimal <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b12">12]</ref>. The algorithm of Berger et al. uses a number of processors linear in the number of sets, and runs in (polylog(</p><p>)) rounds. Even though our Max--Cover algorithm is inspired by theirs, our analysis needs new ideas since our setting is different from theirs.</p><p>Max--Cover-based framework has been used a lot in many data mining applications; earliest ones include identifying the most influential users in social networks, under a model of influence propagation <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b5">5]</ref>.</p><p>Algorithms in the Map-Reduce model. With an increasing demand to mine large amounts of web-originated data, the Map-Reduce paradigm of computation <ref type="bibr" target="#b9">[9]</ref> and Map-Reduce-based algorithms have come to the rescue. For example, PageRank, and in fact most matrix-vector based iterative algorithms, have efficient Map-Reduce implementations as long as the matrix is reasonably sparse. Map-Reduce-based realizations of existing algorithms have been extensively developed in machine learning, including algorithms for regression, naive Bayes, -means clustering, principal and independent component analysis, EM, and SVM <ref type="bibr" target="#b6">[6]</ref>; see also <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b10">10]</ref>. Likewise, in text and natural language processing, Map-Reduce-based algorithms are constantly being developed. For example, algorithms have been developed for pair-wise document similarity <ref type="bibr" target="#b11">[11]</ref>, word cooccurrences <ref type="bibr" target="#b27">[27]</ref>, language modeling <ref type="bibr" target="#b3">[3]</ref>, and indexing <ref type="bibr" target="#b28">[28]</ref>; for more details, see the essay <ref type="bibr" target="#b25">[25]</ref>, the recent tutorial <ref type="bibr" target="#b26">[26]</ref>, and the website http://cluster-fork.info. There have been some attempts to abstractly model Map-Reduce computations; see <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b19">19]</ref>.</p><p>There has been some work on developed Map-Reducebased algorithms and heuristics for large scale graph mining problems. Tsourakakis et al. <ref type="bibr" target="#b18">[18]</ref> propose heuristics for diameter estimation of large graphs. The problem of triangle-counting was considered by Tsourakakis <ref type="bibr" target="#b35">[35]</ref> and Tsourakakis et al. <ref type="bibr" target="#b36">[36]</ref>. Papadimitriou and Sun <ref type="bibr" target="#b32">[32]</ref> develop algorithms for co-clustering. Rao and Yarowsky <ref type="bibr" target="#b33">[33]</ref> obtain simple algorithms for ranking and semi-supervised classification on graphs. Very recently, Karloff, Suri, and Vassilvitskii <ref type="bibr" target="#b19">[19]</ref> obtain an algorithm for finding the minimum spanning tree. The application of Map-Reduce model to graph algorithms has been somewhat limited (and to a certain extent, disappointing) so far. See the wishful thinking article by Cohen <ref type="bibr">[7]</ref> and the inspiration and frustration expressed by Muthukrishnan <ref type="bibr" target="#b30">[30]</ref>.</p><p>There is a large and growing body of work on obtaining graph algorithms in the streaming and semi-streaming models; we refer to <ref type="bibr" target="#b29">[29]</ref> for an overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">THE MrGreedy ALGORITHM</head><p>In this section we develop the MrGreedy algorithm that tries to emulate the sequential Greedy algorithm for Max--Cover, without making sequential choices. As mentioned earlier, the main idea is to add multiple sets to the solution in parallel, as long as it is appropriate. We will prove that MrGreedy in fact outputs a total ordering of the sets with the prefix-optimality property, i.e., for any ′ ≤ , the first ′ sets are an approximate solution to the Max-′ -Cover instance. For simplicity, we focus on the unweighted, unbudgeted version of the problem. The MrGreedy algorithm is inspired by the Set-Cover algorithm of Berger, Rompel, and Shor <ref type="bibr" target="#b2">[2]</ref>. Unfortunately, their algorithm and analysis cannot be used "as is" for two main reasons. First, they are tailored for Set-Cover and not Max--Cover. Second, their algorithm does not return a total ordering of the sets and hence does not enjoy the prefix-optimality property. Therefore, we need to modify their algorithm to work for Max--Cover, return an ordering on the sets, and require the ordering to be prefixoptimal.</p><p>To do these, we incorporate two main ideas. First, we show that it suffices to work with a version of Greedy that guarantees something weaker than the usual Greedy. Next, we show that a modification of the algorithm of <ref type="bibr" target="#b2">[2]</ref> can mimic the weaker Greedy. We also incorporate the prefixoptimality requirement in this modification by imposing an ordering of the sets. At the end, we will prove that the chosen ordering is good enough for the algorithm to return the stated approximation for each .</p><p>Suppose we fix a prefix of the first -1 sets chosen by some approximate greedy algorithm and Greedy. Now, let be the ratio of the new elements covered by to those covered by Greedy with their respective choice of the th set. Let be the average of 1, . . . , : = (1/ ) ∑ =1 . We show a simple fact about the 's.</p><p>Algorithm 2 The MrGreedy algorithm.</p><p>Require: A ground set , a set system ⊆ 2 .</p><p>1: Let be an empty list 2:</p><formula xml:id="formula_3">for = ⌈log 1+ 2 | |⌉ downto 1 do 3: Let = { | ∈ ∧ | | ≥ (1 + 2 ) -1 } 4: for = ⌈log 1+ 2 Δ⌉ downto 1 do 5: Let ′ = { | ∈ ∧ deg ( ) ≥ (1 + 2 ) -1 } 6: while ′ ∕ = ∅ do 7: if there exists ∈ such that | ∩ ′ | ≥ 6 1+ 2 ⋅ | ′ | then 8:</formula><p>Append to the end of 9: else 10:</p><p>Let be a random subset of chosen by including each set in independently with probability = (1+ 2 )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>if</p><formula xml:id="formula_4">∪ ∈ ≥ | | ⋅ (1 + 2 ) ⋅ (1 -8 2 ) then 12:</formula><p>We say that an element is bad if it is contained in more than one set of 13:</p><p>A set ∈ is bad if it contains bad elements of total weight more than 4 ⋅ (1 + 2 ) 14:</p><p>Append all the sets of that are not bad to the end of in any order 15:</p><p>Append the bad sets of to the end of in any order 16:</p><p>Remove all the sets in from 17:</p><p>Remove all the elements in ∪ ∈ from and from the sets in 18:</p><formula xml:id="formula_5">Let = { | ∈ ∧ | | ≥ (1 + 2 ) -1 } 19: Let ′ = { | ∈ ∧ deg ( ) ≥ (1 + 2 ) -1 } 20:</formula><p>Return the list Lemma 3. For each = 1, . . . , -1, we have</p><formula xml:id="formula_6">( 1 - ) ⋅ ( 1 - +1 ) ≤ ( 1 - +1 ) +1 .</formula><p>Proof. By the AM-GM inequality we have</p><formula xml:id="formula_7">( 1 - ) ⋅ ( 1 - +1 ) ≤ ( ⋅ ( 1 - ) + ( 1 -+1 ) + 1 ) +1 = ( 1 - ⋅ + +1 ( + 1) ⋅ ) +1 = ( 1 - +1 ) +1 .</formula><p>The following lemma lower bounds the approximation ratio of in terms of its 's.</p><p>Lemma 4. For each ≥ 1 it holds that the first sets in the list produced by is a 1 -exp(-) approximate solution of the Max--Cover problem.</p><p>Proof. Fix some ≥ 1 as the length of the prefix. We will mimic the well-known proof for sequential max cover, changing the inductive step proof, since our algorithm has a weaker guarantee than the usual sequential greedy for its selection step. Let . We want to lower bound cov( ′ )/cov( * ). Fix ≥ 1, and consider the first -1 sets chosen by the algorithm; the total number of elements covered by * , that are not covered by the sets ′</p><p>(1) , . . . , ′ ( -1) , is at least cov( * ) -∑ -1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=1</head><p>. This implies that there must exist some set * in the optimal solution, different from ′</p><p>(1) , . . . , ′ ( -1) , that covers a subset of those elements having cardinality at least (1/ )(cov( * )-∑ -1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=1</head><p>). Suppose that the algorithm will choose as the th set an approximation of the "best" set, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>≥</head><p>⋅ max</p><formula xml:id="formula_8">∈ ( ∖ -1 ∪ =1 ( ) ) .<label>(1)</label></formula><p>Lower bounding the max in <ref type="bibr" target="#b1">(1)</ref> with its value at * , we obtain</p><formula xml:id="formula_9">≥ ⋅ cov( * ) - ∑ -1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=1</head><p>.</p><p>We show by induction that</p><formula xml:id="formula_10">∑ =1 ≥ ( 1 -<label>( 1 -</label></formula><p>) )</p><p>⋅ cov( * ), for each = 1, . . . , . This directly implies the main statement since (1 -/ ) ≤ exp(-), for any ≥ 1, ≥ 0. The base case is trivial, since</p><formula xml:id="formula_11">1 ≥ 1 ⋅ cov( * ) = 1 ⋅ cov( * ) .</formula><p>Now, suppose the property holds for ≥ 1, then</p><formula xml:id="formula_12">+1 ∑ =1 = ∑ =1 + +1 ≥ ∑ =1 + +1 ⋅ cov( * ) - ∑ =1 = ( 1 - +1 ) ∑ =1 + +1 ⋅ cov( * ) ≥ ( 1 - +1 ) ⋅ ( 1 - ( 1<label>-</label></formula><formula xml:id="formula_13">) ) ⋅ cov( * ) + +1 ⋅ cov( * ) ≥ ( 1 - +1 ) ⋅ cov( * ) - ( 1 - +1 ) +1 ⋅ cov( * ) + +1 ⋅ cov( * ) = ( 1 - ( 1 - +1 ) +1 ) ⋅ cov( * ),</formula><p>where the first inequality follows from (1), the second inequality follows from the induction hypothesis, and the third inequality follows from Lemma 3. The proof is complete.</p><p>Now that we have determined the approximation ratio of in terms of its 's, we turn to analyze MrGreedy. We start by showing that the number of bad sets in line 13 is small -this property will be crucial in our main approximation theorem.</p><p>Lemma 5. At line 13, the number of bad sets is at most 4 ⋅ | |.</p><p>Proof. Observe that, at any execution of line 12, each set in has weight upper bounded by (1 + 2 ) . Indeed, this is trivially true when = ⌈log 1+ 2 | |⌉, since in this case the upper bound is greater than the size of the ground set, (1 + 2 ) ≥ | |. Now, observe that for to be decreased, the loop on must be exhausted. Further, when = 1, the only way of exiting the loop at line 6, i.e., the only way of letting ′ = ∅ is to remove all sets of weight at least (1 + 2 ) -1 (lines <ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19)</ref>. Thus, whenever we execute line 3 and line 12, no set of weight more than (1 + 2 ) exists. Now, at any execution of line 11, we have ⊆ , and thus ∀ ∈ we have (1</p><formula xml:id="formula_14">+ 2 ) -1 ≤ | | &lt; (1 + 2 ) . Then, ∑ ∈ | | ≤ | | ⋅ (1 + 2 ) .</formula><p>If the test at line 11 is negative, we just cycle all the way through line 10 without changing the state of the algorithm. When we do get to line 12, we have</p><formula xml:id="formula_15">∪ ∈ ≥ | | ⋅ (1 + 2 ) ⋅ (1 -8 2 ).</formula><p>For each element that occurs in more than one set in , mark all the occurrences of that element. Observe that the bad elements of line 12 are exactly those whose occurrences are marked. The total number of the marked occurrences is</p><formula xml:id="formula_16">≤ 2 ⋅ ⎛ ⎝ ∑ ∈ | | - ∪ ∈ ⎞ ⎠ ≤ | | ⋅ (1 + 2 ) ⋅ 16 2 .</formula><p>Observe that no more than 4 ⋅ | | sets in can contain more than 4 ⋅ (1 + ) marked elements, since otherwise we would have a contradiction with the upper bound on . That is, no more than 4 ⋅ | | sets can be bad.</p><p>To fill the final missing step, we now prove a lower bound on 's of MrGreedy. Lemma 6. For each ≥ 1, it holds that ≥ 1 -( ).</p><p>Proof. Fix an arbitrary 1 ≤ ≤ . If we add the th set to via line 8, we will have ≥ 1 1+ 2 ≥ 1 -( ). Indeed, at the beginning of the loop at line 6, the largest uncovered weight of the remaining sets will be ≤ (1 + 2 ) , and each set in , thus in particular the one being added to via line 8, has uncovered weight at least (1+ 2 ) -1 ; thus ≥ 1-( ). On the other hand, whenever we add two batches of sets via lines 14, 15, the following will happen. The first batch (line 14) will be composed of sets each containing a number of unique elements at least (1</p><formula xml:id="formula_17">+ 2 ) -1 -4 ⋅ (1 + 2 ) = (1-( ))⋅(1+ 2 ) -1 .</formula><p>Since the largest number of uncovered elements of a yet-to-be-taken set is at most (1 + 2 ) , no matter how we sort the sets in the first batch, each of them will have an</p><formula xml:id="formula_18">≥ 1-( ) 1+ 2 = 1 -( ).</formula><p>As for the second batch, we lower bound the 's of its sets with 0.</p><p>Applying </p><formula xml:id="formula_19">, . . . , 0 +| |-1 is (1 -( )) ⋅ | | ⋅ (1 -( )) | | = 1 -( ).</formula><p>Since (i) we can cut into the sequences added by line 8, or by lines 14-15, and (ii) each sequence contains sets whose average 's are at least 1 -( ), and (iii) the lower bound on the average of a part holds even if we cut that part to any of its prefix, the lemma is proved: for each , ≥ 1 -( ).</p><p>Combining Lemmas 4 and 6, with set to MrGreedy, gives us the main result. (In the algorithm of <ref type="bibr" target="#b2">[2]</ref>, there are two independent parameters and . For simplicity, we set = since the approximation guarantee will be maximized when they are equal. Further, we set our to be the square root of the = in <ref type="bibr" target="#b2">[2]</ref>.)</p><p>From Lemma 9, independent of the result of the test at line 7, with probability Ω(1), an Ω( <ref type="formula">6</ref>) fraction of ′ will be removed from consideration. Since at the beginning</p><formula xml:id="formula_20">| ′ | ≤</formula><p>, by a Chernoff bound, with high probability the loop at line 6 will end after ( -6 log ) iterations. Observing that the loop at line 4 iterates for ( -2 log Δ) times, and that the loop at line 2 iterates for ( -2 log ) times, gives the stated upper bound on the number of executions of lines 7-15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Realization in Map-Reduce</head><p>In this section we comment on the implementation of Mr-Greedy in Map-Reduce. Recall the transpose operation we discussed earlier, which lets us switch between elementset and set-element representations using Map-Reduce.</p><p>The steps involving the selection of and ′ in the algorithm (lines 3 and 5) can be realized by a combination of map, transpose, and a reduce operation. Line 7 can be realized by computing the size of an intersection, which can be done with a map and a reduce. The random selection in line 10 is a simple map operation and the test in line 11 is equivalent to computing the size of a union, which is once again a transpose, map, followed by a transpose. Lines 13-15 that determine the goodness of sets and elements can be determined using a transpose and a map operation. Finally, the updates in lines 16-17 are once again easy in Map-Reduce using a transpose, map, and reduce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Weighted, budgeted versions</head><p>We observe how the weighted case can be easily reduced to the unweighted one. For instance, assuming integer weights, we could, for each element , replace (in all the sets that contain it) with ( ) unweighted copies of . Then, for each class ′ ⊆ of sets, the weight of ′ in the original instance will equal the total coverage of ′ in its unweighted counterpart. This reduction has two downsides: it is not strongly polynomial and it requires each element weight to be integral.</p><p>It is possible to overcome these hindrances using another reduction. First of all, observe that multiplying all the weights by the same positive number only scales the value of each solution by that same number. Further, if = max ∈ ( ), then one can easily check that removing all elements of weight less than ( ⋅ )/ from and from the sets that contain them in changes the value of each solution of value with a constant factor of the optimum by a 1 ± ( ) factor. Finally, rounding each weight ( ) to ⌈ ( ) ⌉ changes the value of each solution by a (1 ± ( )) factor. So, suppose we rescale all weights so that the maximum weight becomes -2 ⋅ . Then, we delete all elements having new weight less than -1 , and we round each remaining weight to ⌈ ⌉, losing only a (1 ± ( )) approximation factor. Observe that the new weights are all integers. Substituting an element of integral weight ∈ N with new elements each of weight 1 and each contained in all the sets that contained , does not change the value of any solution. The reduction is complete. The downside of this reduction is that the number of elements gets squared, which may not be desirable in practice. Now we comment on the budgeted version of Max--Cover. Khuller et al. <ref type="bibr" target="#b21">[21]</ref> show how a budgeted version of Greedy, along with a simple external check, gives a (1 -1/ √ )-approximation to the budgeted, weighted, Max--Cover problem (in fact, this approximation is obtained if one returns the best of the solution produced by the algorithm, and the set of maximum weight among those having cost less than or equal to the budget). Using an argument similar to the one we gave for the unbudgeted case, we can show that the budgeted greedy algorithm of Khuller et al. can be parallelized like the unbudgeted one. The approximation guarantee is (1 -1/ √ -( )) and the parallel running time remains polylogarithmic. We omit the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussions</head><p>In this section we comment on other possible approaches for obtaining a parallel approximation algorithm for Max--Cover. Suppose one could strengthen our weak version of greedy algorithm to return a set covering a number of new elements within a factor of 1 -of the maximum. Then, a result of Hochbaum and Patria [16, Chapter 3, Theorem 3.9] could directly be applied to obtain a 1 --1+ = 1 --1 -( ) approximation to the Max--Cover problem. It is in fact unclear if such an algorithm can exist in the parallel setting. For instance, the algorithm of <ref type="bibr" target="#b2">[2]</ref> can choose some bad sets (i.e., sets having large intersection with the other chosen sets). Our contribution is to show that (i) such sets are few and (ii) can be positioned in the total ordering in such a way that the approximation guarantee is changed by just a 1 -( ) factor.</p><p>We observe how a partially different approach could have been taken if we did not insist on prefix-optimality. For in-stance, we could have randomly permuted each equivalence class of sets returned by the algorithm of <ref type="bibr" target="#b2">[2]</ref>; this would have ensured that with some Ω(1) probability few bad sets would have ended in the -prefix of the ordering -we could have then ignored those few bad sets and considered only the others. This approach would have still required to prove that the number of bad sets is small, and that, being few, they could not have caused havoc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section we detail an experimental study of our new algorithm. For purposes of the experiments, we use five large, real-world instances of set systems. For each of these instances, we ran the standard Greedy algorithm and our MrGreedy algorithm. The goal of the experiments is to demonstrate three things. First, it is feasible to implement MrGreedy in practice. Second, the performance of Mr-Greedy is almost indistinguishable from Greedy, both for various values of and for instances with various characteristics. Third, our algorithm exploits and achieves parallelism in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data description</head><p>We use the following five data sets in our experiments.</p><p>(1) User-hosts. This instance consists of users (elements) and hosts (sets). Here, the set for a given host consists of all the users who visited some web page on the host during browsing. This data is derived from the Yahoo! toolbar logs; the users are anonymized and the hosts hashed. The instance is a subset of the toolbar data during the week of <ref type="bibr">July 18-25, 2009</ref>. The coverage problem is to determine the hosts that are visited by as many users as possible.</p><p>(2) Query-hosts. Our second instance consists of queries (elements) and hosts (sets). The set for a given host contains all queries for which the host appeared in the top ten search results. These queries were sub-sampled from Yahoo! query logs on July 1, 2009. We may study both weighted and unweighted versions by introducing a weight to each query corresponding to the number of times it occurs. We work here with the unweighted instance, so we seek the hosts that together cover as many unique queries as possible.</p><p>(3) Photos-tags. Our third graph is derived from the Flickr data set. Here, the elements are photographs uploaded to the Flickr web service, and the sets are tags. The set for a given tag contains the photos to which that tag has been applied. The coverage problem is to find those tags that together cover as many photos as possible.</p><p>(4) Page-ads. Our fourth instance is derived from Yahoo!'s content match advertising system, which allows arbitrary internet content publishers to place textual ads on their pages. The elements are web pages, and the sets are the ads shown on those pages. The coverage problem is to determine a collection of ads that covers as many pages as possible.</p><p>(5) User-queries. Our fifth graph is derived once again from the web search query logs in Yahoo! We consider query logs over a 75-day period in which each query is annotated with an anonymized userid. The elements are the anonymized ids, and the sets are queries. The set for a particular query contains all the anonymized userids that issued the query in our sample. The coverage problem is to determine the queries that cover as many users as possible.</p><p>Table <ref type="table" target="#tab_1">1</ref> shows the overall statistics of the data including the number of sets ( ), number of elements ( ), the number of element-set memberships ( ), the maximum degree of an element (Δ), and the maximum cardinality of a set ( * ( )). From the table it is clear that the five instances are widely varying in terms of their characteristics. Unless otherwise specified, we use the large User-hosts instance to illustrate our experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Approximation performance</head><p>First, we wish to show that the approximation guarantee of MrGreedy is almost on par with Greedy. For the purposes of this experiment, we choose = .75. (In theory, this is a non-sensical choice since has to be at most 1 -1/ ; but as we will see, this choice does not impact the algorithm.) We also illustrate the performance of the naive algorithm (Naive) that sorts the sets by sizes and takes the prefix as a solution.</p><p>We plot the value of the solution returned by the various algorithms on the instances. We choose varying values of for each instance. In Figure <ref type="figure" target="#fig_3">1</ref>, we show the approximations on the User-hosts instance. The -axis specifies and the -axis gives the fraction of elements in the universe that are covered by a prefix of length in the solution. It is clear that MrGreedy is almost indistinguishable from Greedy for all values of .  The reader might wonder why some of the plots in Figure <ref type="figure" target="#fig_4">2</ref> and Figure <ref type="figure" target="#fig_7">3</ref> have a "saw tooth" pattern. This is caused due to the parallel nature of MrGreedy and each "spike" corresponds to the beginning of a book-keeping phases. Mr-Greedy arbitrarily sorts the sets output during a bookkeeping phase to increase parallelism at the cost of precision, i.e., if a prefix that cuts a phase in two parts is chosen, the algorithm incurs a small loss in the quality of the cover returned. Both Greedy and Naive return the sets in a total order instead, i.e., each prefix is correctly sorted. Thus, the relative performances of MrGreedy, compared to both Greedy and Naive, decreases between book-keeping phases. The performance of the Naive algorithm varies widely. In some instances, such as the Query-hosts dataset, the performance of Naive is within a few percent of the other algorithms, which may be acceptable for some settings. In other settings, such as the Photos-tags dataset, the approximation performance of Naive is quite poor and worsens as the result size increases. Naive in general will perform poorly when there are many high-coverage sets with significant overlap, which is a common occurrence in max cover problems; hence, algorithms that take overlaps into account are usually necessary.</p><p>Comparing the performance of MrGreedy with Greedy, we see that MrGreedy performs comparably over all instances and all ranges of . So, in terms of approximation guarantees, we conclude that MrGreedy performs on par with Greedy, despite our setting of = 0.75. Even if choosing such a large does not give us, theoretically, any approximation guarantee (see Theorem 7 and Lemma 5), it produces very good results in practice. In the next section, we further study the role of in the experimental evaluation of MrGreedy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of</head><p>In this section we study the effect of choosing the value of . We examine two parameters: the coverage obtained and the running time. Figure <ref type="figure" target="#fig_6">4</ref> shows the relative coverage (base is when = 0.75) for two smaller values of , namely, 0.1 and 0.01, on the User-hosts instance. From the curves, we see how a smaller value of achieves only a 1-2% improvement, even for moderate values of . On the other hand, a smaller value of means a higher running time. Clearly, the benefit of using a lower value of is unnoticeable in practice. In other words, even though the performance of our algorithm is 1 -1/ -( ), the effect of a large value of seems negligible in practice.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Exploiting the parallelism</head><p>In this section we study the running time of MrGreedy compared to Greedy. Specifically, we study two aspects: the wall-clock time and the number of parallel executions made possible by our algorithm (as a function of ). Figure <ref type="figure" target="#fig_0">5</ref> shows the running time of MrGreedy vs Greedy on the User-hosts graph. It is clear that MrGreedy vastly outperforms Greedy in terms of the running time. Observe the "horizontal steps" in the running of MrGreedy. These are precisely the points where a "batch" addition to the current solution is performed: lines 14-15 of MrGreedy.</p><p>To study this further, we count the number of parallel steps enabled by MrGreedy; in particular, we study the number of times lines 14-15 were invoked for the User-hosts instance. In all, these were invoked about 90 times and more than 2,400 sets were added in that step. Note that each of these 90 additions is a "batch" update, i.e., the element-set memberships have to be updated only 90 times (as opposed to each time in Greedy). Figure <ref type="figure" target="#fig_11">6</ref> shows these updates over time. It is clear that as the algorithm progresses, on average, it is able to add more and more sets to the solution in a single batch.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>We developed a Map-Reduce-based algorithm for Max--Cover. Our algorithm obtains a nearly 0.63-approximation and can be implemented with polylogarithmic many Map-Reduce steps over the input instance. Thus, we match the performance of Greedy, while obtaining an algorithm that can be implemented in the scalable and widely-used Map-Reduce framework. This is one of the few instances of a non-trivial Map-Reduce realization of a classical algorithm with provable guarantees of performance. Our experiments on five large-scale real-world instances show that it is feasible to implement MrGreedy in practice and obtain reasonable speedup over Greedy. Moreover, the approximation performance of MrGreedy is virtually indistinguishable from that of Greedy.  As we stated in Section 3, there has been very little work on Map-Reduce-versions of classical graph algorithms. With the growth of graph mining in web contexts, it becomes inevitable to focus on developing Map-Reduce-friendly versions of those graph algorithms that have an inherently sequential flavor, with provable performance and running time guarantees. A viable and promising candidate in this aspect is the sequential greedy algorithm to find the densest subgraph <ref type="bibr" target="#b4">[4]</ref>: a Map-Reduce version of this algorithm will be of immense interest to practitioners who are interested in finding dense communities in massive graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Lemma 5 ,</head><label>5</label><figDesc>we can upper bound the number of sets in the second batch by at most 4 ⋅ | | so that there are at least | | ⋅ (1 -4 ) = (1 -( )) ⋅ | | sets in the first batch. Thus, if lines 14 and 15 add sets starting from position 0 of , we have 0 , . . . , 0 +(1-( ))⋅| |-1 ≥ 1 -( ), and 0 +(1-( ))⋅| | , . . . , 0 +| |-1 ≥ 0. It is easy to see that a lower bound on the average of the 's in any prefix of 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 7 .</head><label>7</label><figDesc>The approximation guarantee of MrGreedy is 1 -1/ -( ).Let us now comment on the number of Map-Reduce passes of MrGreedy. Lemma 8. The number of times lines 7-15 is executed is ( -10 ⋅ log 2 ⋅ log Δ), with high probability.Proof. To prove this, we first need a fact about the sampling step of MrGreedy. Lemma 9 ([2]). With probability at least 1/8 (over the random sampling), (i) the test at line 11 succeeds and (ii) the elements covered by ∪ ∈ ≥ | | contain at least a 2 /2 fraction of ′ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance of MrGreedy, Greedy, and Naive on User-hosts. To visualize the plot better, in Figure 2, we show the relative performances, where we compare the approximation of Greedy and Naive against MrGreedy. The relative performances on the other three instances are shown in Figure 3. The top line shows the performance of Naive against MrGreedy whereas the bottom line shows the performance of Greedy against MrGreedy. As we see, the bottom line is nearly horizontal and very close to = 1, indicating that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Relative performance on User-hosts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of : relative deviation when using = 0.75 and running times for different on Queryhosts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Relative performance on other four data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Running times for MrGreedy vs Greedy on User-hosts, on a semi-log scale (time vs. log-).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Number of "batch" additions over time for MrGreedy on User-hosts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Full Paper April 26-30 • Raleigh • NC • USA</head><label></label><figDesc>′ = { ′ (1) , . . . , ′ ( ) }, and let * 1 , . . . , * be the sets in the optimal solution (sorted arbitrarily), * = { * 1 , . . . , * }. Let be the number of elements that (i) are covered by set ′ ( ) and (ii) are not covered by sets ′ (1) , . . . , ′ ( -1) . Then the coverage of the algorithm solution ′(1) , . . . , ′ ( ) is cov( ′ ) = ∑</figDesc><table><row><cell>=1</cell></row></table><note><p>′ (1) , . . . , ′ ( ) be the first sets picked by WWW 2010 • ,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Details of the data sets.</figDesc><table><row><cell>set</cell><cell></cell><cell></cell><cell></cell><cell>Δ</cell><cell>*  ( )</cell></row><row><cell>User-hosts</cell><cell cols="5">5.64M 2.96M 72.8M 2,115 1.19M</cell></row><row><cell cols="2">Query-hosts 625K</cell><cell>239K</cell><cell>2.8M</cell><cell>10</cell><cell>164K</cell></row><row><cell>Photo-tags</cell><cell>89K</cell><cell>704K</cell><cell>2.7M</cell><cell>145</cell><cell>54.3K</cell></row><row><cell>Page-ads</cell><cell>321K</cell><cell>357K</cell><cell cols="3">9.1M 24,825 164K</cell></row><row><cell cols="3">User-queries 14.2M 100K</cell><cell>72M</cell><cell>5,369</cell><cell>21.4K</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>April 26-30 • Raleigh • NC • USA</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithmic construction of sets for -restrictions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moshkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Safra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Algorithms</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="153" to="177" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient NC algorithms for set cover with applications to learning and geometry</title>
		<author>
			<persName><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rompel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Shor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="454" to="477" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large language models in machine translation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="858" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Greedy approximations for finding dense components in a graph</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd APPROX</title>
		<meeting>3rd APPROX</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient influence maximization in social networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th KDD</title>
		<meeting>15th KDD</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Map-reduce for machine learning on multicore</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph twiddling in a MapReduce world</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The discoverability of the web</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th WWW</title>
		<meeting>16th WWW</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mapreduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">C. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MapReduce for data intensive scientific analyses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ekanayake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pallickara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Fourth International Conference on eScience</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="277" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pairwise document similarity in large collections with MapReduce</title>
		<author>
			<persName><forename type="first">T</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 46th ACL/HLT</title>
		<meeting>46th ACL/HLT</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="265" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A threshold of ln for approximationg set cover</title>
		<author>
			<persName><forename type="first">U</forename><surname>Feige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="634" to="652" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On distributing symmetric streaming computations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Svitkina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th SODA</title>
		<meeting>19th SODA</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="710" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Approximation algorithms for partial covering problems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="84" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Computers and Intractability: A Guide to the Theory of NP-Completeness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>W. H. Freeman and Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Approximation Algorithms for NP-Hard Problems. Course Technology</title>
		<editor>D. Hochbaum</editor>
		<imprint>
			<date type="published" when="1996-07">July 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Analysis of the greedy approach in covering problems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hochbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pathria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>Unpublished manuscript</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">HADI: Fast diameter estimation and mining in massive graphs with Hadoop</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Tsourakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>CMU-ML-08-117</idno>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>CMU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A model of computation for MapReduce</title>
		<author>
			<persName><forename type="first">H</forename><surname>Karloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th SODA</title>
		<meeting>20th SODA</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maximizing the spread of influence through a social network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th KDD</title>
		<meeting>9th KDD</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The budgeted maximum coverage problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="45" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Parallel graph algorithms with MapReduce</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kimball</surname></persName>
		</author>
		<ptr target="http://youtube.com/watch?v=BT-piFBP4fE" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Connectivity structure of bipartite graphs via the KNC-plot</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st WSDM</title>
		<meeting>1st WSDM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cost-effective outbreak detection in networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanbriesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Glance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th KDD</title>
		<meeting>13th KDD</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring large-data issues in the curriculum: A case study with MapReduce</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Workshop on Issues in Teaching Computational Linguistics</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="54" to="61" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text processing with MapReduce</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<ptr target="http://www.umiacs.umd.edu/˜jimmylin/cloud-computing/NAACL-HLT-2009/index.html" />
	</analytic>
	<monogr>
		<title level="m">NAACL/HLT Tutorial</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scalable language processing algorithms for the masses: A case study in computing word co-occurrence matrices with MapReduce</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="419" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On single-pass indexing with MapReduce</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M C</forename><surname>Mccreadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 32nd SIGIR</title>
		<meeting>32nd SIGIR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="742" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Data streams: Algorithms and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">MapReduce again</title>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<ptr target="http://mysliceofpizza.blogspot.com/2008/01/mapreduce-again.html" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Disco: Distributed co-clustering with Map-Reduce: A case study towards petabyte-scale end-to-end mining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 8th ICDM</title>
		<meeting>of 8th ICDM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ranking and semi-supervised classification on large scale graphs using Map-Reduce</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th TextGraphs at ACL/IJCNLP</title>
		<meeting>4th TextGraphs at ACL/IJCNLP</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On maximum coverage in the streaming model &amp; application to multi-topic blog-watch</title>
		<author>
			<persName><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th SDM</title>
		<meeting>9th SDM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="697" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast counting of triangles in large real networks without counting: Algorithms and laws</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Tsourakakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th ICDM</title>
		<meeting>8th ICDM</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="608" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DOULION: Counting triangles in massive graphs with a coin</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Tsourakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th KDD</title>
		<meeting>15th KDD</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="837" to="846" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
