<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identity-Aware Convolutional Neural Network for Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zibo</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">South Carolina University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sony Electronics</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">South Carolina University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shizhong</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">South Carolina University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Tong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">South Carolina University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Identity-Aware Convolutional Neural Network for Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/FG.2017.140</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial expression recognition suffers under realworld conditions, especially on unseen subjects due to high inter-subject variations. To alleviate variations introduced by personal attributes and achieve better facial expression recognition performance, a novel identity-aware convolutional neural network (IACNN) is proposed. In particular, a CNN with a new architecture is employed as individual streams of a bi-stream identity-aware network. An expression-sensitive contrastive loss is developed to measure the expression similarity to ensure the features learned by the network are invariant to expression variations. More importantly, an identity-sensitive contrastive loss is proposed to learn identity-related information from identity labels to achieve identity-invariant expression recognition. Extensive experiments on three public databases including a spontaneous facial expression database have shown that the proposed IACNN achieves promising results in real world.</p><p>* indicates equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Facial activity is the most powerful and natural means for understanding emotional expression for humans. Extensive efforts have been devoted to facial expression recognition in the past decades <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b35">[36]</ref>. An automatic facial expression recognition system is desired in emerging applications in human-computer interaction (HCI), such as online/remote education, interactive games, and intelligent transportation.</p><p>Although great progress has been made from posed or deliberate facial displays, facial expression recognition in real-world suffers from various factors including unconstrained face pose, illumination change, and high intersubject variations. Moreover, recognition performance usually degrades on unseen subjects, primarily due to high intersubject variations introduced by age, gender, and especially person-specific characteristics associated with identity as discussed in <ref type="bibr" target="#b42">[43]</ref>. Since these factors are nonlinearly coupled with facial expressions in a multiplicative way, features extracted through existing methods are not purely related to expressions. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, I 1 and I 2 are the same subject displaying different expressions, whereas I 1 and I 3 are different subjects displaying the same expression. For facial expression recognition, it is desired to have the distance D 1 between images I 1 and I 2 larger than D 2 between I 1 and I 3 in the feature space, as in Fig. <ref type="figure" target="#fig_0">1b</ref>. However, due to high inter-subject variations, D 1 is usually smaller than D 2 in the feature spaces of existing approaches, as in Fig. <ref type="figure" target="#fig_0">1a</ref>. This motivates us to attack the challenging problem of learning and extracting personal-independent and expression-discriminative features. To solve this problem, we develop an identity-aware convolutional neural network (IACNN) to learn expression-related representations and, at the same time, to learn identity-related features to facilitate identity-invariant facial expression recognition.</p><p>In addition to minimizing the classification errors, a similarity metric for expression is developed and employed in the IACNN to pull the samples with the same expression together while pushing those with different expressions apart in the feature space. As a result, the intra-expression variations are reduced, while the inter-expression differences are increased. However, the learned expression representations may contain irrelevant identity information such that the performance of facial expression recognition is affected by high intersubject variations as illustrated in Fig. <ref type="figure" target="#fig_0">1a</ref>. To alleviate the effect of the inter-subject variations, a similarity metric for identity is proposed in the IACNN to learn identity-related features, which will be combined with the expression-related representations for facial expression recognition.</p><p>The architecture of the proposed IACNN is illustrated in Fig 2 . During the training process, expression and identity related features are jointly estimated through a deep CNN framework, which is composed of two identical CNN streams and trained by simultaneously minimizing the classification errors while maximizing the expression and identity similarities. Specifically, given a pair of images, each of which is fed into one CNN stream, the similarity losses are computed using the expression and identity related features, respectively. In addition, the classification errors in terms of expression recognition are also calculated for both images and used to fine-tune the model parameters to ensure the learned features are meaningful for expression recognition. During testing, an input image is fed into one CNN stream, and predictions are generated based on both the expression- related and the identity-related features. In summary, our main contributions in this paper are:</p><p>1) Developing an IACNN, which is capable of utilizing both expression-related and identity-related information for facial expression recognition; 2) Introducing a new auxiliary layer with an identitysensitive contrastive loss to learn identity-related representations to alleviate high inter-subject variations; 3) Proposing a joint loss function, which considers classification errors of expression recognition as well as expression and identity similarities, to fine tune the expression-related and identity-related features simultaneously. Extensive experiments on two well-known posed facial expression databases, i.e. Extended Cohn-Kanade database (CK+) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b24">[25]</ref> and MMI database <ref type="bibr" target="#b31">[32]</ref>, have demonstrated the effectiveness of the proposed method for facial expression recognition. Furthermore, the proposed method was evaluated on a spontaneous facial expression dataset, i.e. Static Facial Expressions in the Wild (SFEW) <ref type="bibr" target="#b5">[6]</ref>, which contains face images with large head pose variations and different illuminations and has been widely used for benchmarking facial expression recognition. Experimental results on the SFEW dataset have shown that the proposed approach achieves promising results in real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>As elaborated in the surveys <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b35">[36]</ref>, facial expression recognition has been extensively studied over the past decades. Both 2D and 3D features have been extracted from static images or image sequences to capture the appearance and geometry facial changes caused by target expressions. The features employed can be human-designed including Histograms of Oriented Gradients (HOG) <ref type="bibr" target="#b1">[2]</ref>, Scale Invariant Feature Transform (SIFT) features <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b4">[5]</ref>, histograms of Local Binary Patterns (LBP) <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b2">[3]</ref>, histograms of Local Phase Quantization (LPQ) <ref type="bibr" target="#b11">[12]</ref>, and their spatiotemporal extensions <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b46">[47]</ref>. Other spatiotemporal approaches, such as temporal modeling of shapes (TMS) <ref type="bibr" target="#b9">[10]</ref>, interval temporal Bayesian network (ITBN) <ref type="bibr" target="#b45">[46]</ref>, expressionlets on spatiotemporal manifold (STM-ExpLet) <ref type="bibr" target="#b21">[22]</ref>, and spatiotemporal covariance descriptors (Cov3D) <ref type="bibr" target="#b34">[35]</ref>, have been developed to utilize both spatial and temporal information in an image sequence.</p><p>Human-crafted features can achieve high performance on posed expression data. However, performance degenerates on spontaneous data with large variations in head pose and illumination. In contrast, features can be learned in a datadriven manner by sparse coding <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b27">[28]</ref> or deep learning <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b52">[53]</ref>. Most recently, CNN-based deep learning approaches <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b29">[30]</ref> have been demonstrated to be more robust to real world conditions in EmotiW2015 <ref type="bibr" target="#b5">[6]</ref>.</p><p>Most of the aforementioned approaches focus on improving person-independent recognition, while a few of them learn models from person-specific data. For example, Chen et al. <ref type="bibr" target="#b2">[3]</ref> learn a person-specific model from a few personspecific samples based on transfer learning.</p><p>Existing approaches learn a similarity metric using either human-designed features <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b36">[37]</ref> or deep metric learn- As depicted in the dashed rectangle, the component CNN includes three convolutional layers, each of which is followed by a PReLU layer and a BN layer. A max pooling layer is employed after each of the first two BN layers. Following the third convolutional layer, two FC layers are used to generate the representation for each input sample. Finally, a softmax loss layer (L Exp Sof tmax ) is employed to produce the distribution over the target expressions and to calculate the classification errors for fine-tuning the parameters. A contrastive loss, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L Exp</head><p>Contrastive , is employed to reduce the intra-class variations, while to decrease the inter-class similarity. Loss layers are highlighted by gray blocks. Best viewed in color.</p><p>ing such as CNNs with pairwise-constraints <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b52">[53]</ref> or triplet-constraints <ref type="bibr" target="#b44">[45]</ref>. Compared with those based on hand-crafted features, deep metric learning achieves more powerful representations with low intra-class yet high interclass distances in a data driven manner and thus, has shown promising results in many applications. For example, Zhao et al. <ref type="bibr" target="#b52">[53]</ref> considered similarity between peak and nonpeak frames from the same subject to achieve invariance to expression intensity. In this work, we explicitly learn the expression-related and identity-related features to achieve identity-invariant expression recognition, where pairwiseconstraints based contrastive loss is chosen as the loss function for learning the similarity metric for either expression or identity. Unlike previous approaches, we utilize both contrastive loss and softmax loss to learn both features jointly. More importantly, a new auxiliary layer plus a contrastive loss is employed, which is responsible for learning identityrelated representations to alleviate the inter-subject variations introduced by personal attributes. The proposed framework is an end-to-end system, which can be optimized via standard stochastic gradient descent (SGD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Component CNN</head><p>Instead of utilizing an off-the-shelf model, e.g. AlexNet <ref type="bibr" target="#b18">[19]</ref>, a new CNN architecture is designed for the component network of IACNN, due to limited expressionlabeled data. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, the CNN enclosed in the dashed rectangle includes three convolutional layers, each of which is followed by a parametric rectified unit (PReLU) layer <ref type="bibr" target="#b7">[8]</ref> as the activation function. As an extension of a rectified linear unit (ReLU) activation function, PReLU has better fitting capability than the sigmoid function or hyperbolic tangent function <ref type="bibr" target="#b18">[19]</ref> and further boosts the classification performance as compared to the traditional ReLU. After each PReLU layer, a batch normalization (BN) layer <ref type="bibr" target="#b8">[9]</ref> is utilized to normalize each scalar feature to zero mean and unit variance. The BN layer has been shown to improve classification performance and accelerate the training process <ref type="bibr" target="#b8">[9]</ref>. After each of the first two BN layers, there is a max pooling layer. Following the third PReLU layer, two fully connected (FC) layers consisting of 1,024 neurons are employed. Finally, a softmax loss layer is used to generate the probabilistic distribution over the K target expressions and to calculate the classification loss given the expression labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. An Exp-Net for Facial Expression Recognition</head><p>In real-world scenarios, facial expression recognition suffers from intra-expression variations. As a result, CNNs can generate quite different representations for image samples containing the same expression. To cope with the problem, an Exp-Net composed of two identical component CNNs is constructed, as illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. Two softmax losses and one contrastive loss are employed to learn representations that are meaningful for facial expression recognition. Given the expression labels, a softmax loss is employed on top of each component network to calculate the classification errors and is used to fine tune the parameters in the lower layers. In addition, a contrastive loss is utilized to learn a similarity metric for image pairs to make sure that the samples with the same expression have similar representations, and at the same time, those with different expressions are far away in the feature space.</p><p>1) Expression-Sensitive Contrastive Loss: As illustrated in Fig. <ref type="figure" target="#fig_4">4</ref>, an expression-sensitive contrastive loss is designed to pull the samples with the same expression towards each other, and to push the samples with different expressions away from each other at the same time. Specifically, the similarity between two images I 1 and I 2 is defined as the squared Euclidean distance in the feature space:  where f E (?) is the expression-related image representation, i.e., the output of the F C exp layer in Fig. <ref type="figure" target="#fig_2">3</ref>. A smaller distance D f (I 1 ), f (I 2 ) indicates that the two images are more similar in the feature space.</p><formula xml:id="formula_0">D f E (I 1 ), f E (I 2 ) = f E (I 1 ) -f E (I 2 ) 2</formula><p>Then, the expression-sensitive contrastive loss is defined as follows:</p><formula xml:id="formula_1">L exp Contrastive = M L E z E i , f E (I i,1 ), f E (I i,2 )<label>(2)</label></formula><formula xml:id="formula_2">L E z E i , f E (I i,1 ), f E (I i,2 ) = z E i 2 * D f E (I i,1 ), f E (I i,2 ) + 1 -z E i 2 * max 0, ? E -D f E (I i,1 ), f E (I i,2 )<label>(3)</label></formula><p>where M is the number of image pairs; and</p><formula xml:id="formula_3">z E i , f E (I i,1 ), f E (I i,2</formula><p>) represents the label and the expression-related image features for the i th pair of training samples, respectively. When the pair of images has the same expression label, z E i = 1 and the loss is</p><formula xml:id="formula_4">L E 1, f E (I i,1 ), f E (I i,2 ) = 1 2 D f E (I i,1 ), f E (I i,2 ) .<label>(4)</label></formula><p>Otherwise, z E i = 0 and the loss becomes</p><formula xml:id="formula_5">L E 0, f E (I i,1 ), f E (I i,2 ) = 1 2 max 0, ? E -D f E (I i,1 ), f E (I i,2 )<label>(5)</label></formula><p>where ? E &gt; 0 is a parameter to determine how much dissimilar pairs contribute to the loss function. When the distance between two dissimilar samples in a pair is less than ? E , then the loss will be calculated. In our experiment, ? E is set to 10 for the expression-sensitive contrastive loss, empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The IACNN for Facial Expression Recognition</head><p>The performance of facial expression recognition often drops significantly for unseen subjects, mainly due to high inter-subject variations introduced by age, gender, and especially person-specific characteristics associated with identity. To deal with this problem, an IACNN is proposed by introducing an auxiliary FC layer into the Exp-Net, which takes the input from the lower layers of the Exp-Net, but has its own set of learnable weights.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the F C ID layer is on top of the first FC layer of the Exp-Net after the convolutional layers. Given the identity labels, the F C ID is responsible for learning identity-related features using an identity-sensitive contrastive loss L ID Contrastive , defined as follows</p><formula xml:id="formula_6">L ID Contrastive = M i=1 L ID z ID i , f ID (I i,1 ), f ID (I i,2 )<label>(6)</label></formula><p>where f ID (?) is the identity-related feature, i.e., the output of the F C ID layer; and</p><formula xml:id="formula_7">L ID z ID i , f ID (I i,1 ), f ID (I i,2</formula><p>) is defined similar to the expression-sensitive contrastive loss as below:</p><formula xml:id="formula_8">L ID z ID i ,f ID (I i,1 ),f ID (I i,2 ) = z ID i 2 * D f ID (I i,1 ), f ID (I i,2 ) + 1 -z ID i 2 * max 0, ? ID -D f ID (I i,1 ), f ID (I i,2 )<label>(7)</label></formula><p>where z ID i = 1, if the pair of images comes from the same subject; otherwise, z ID i = 0. ? ID is set to 10 for the identitysensitive contrastive loss, empirically.</p><p>The identity-related features are then concatenated with the expression-related features encoded by the F C exp layer to form the final feature vector F C f eat for facial expression recognition. Therefore, the overall loss function of the proposed IACNN is defined as</p><formula xml:id="formula_9">L = ? 1 L Exp Contrastive + ? 2 L ID Contrastive + ? 3 L 1 Sof tmax + ? 4 L 2 Sof tmax + ? 5 L Exp1 Sof tmax + ? 6 L Exp2 Sof tmax<label>(8)</label></formula><p>where ? 1 -? 6 are the weights of each loss, respectively. L Exp Contrastive and L ID Contrastive are the expression-sensitive contrastive loss and the identity-sensitive contrastive loss, as defined in Eq. 2 and Eq. 6, respectively. L Exp1 Sof tmax and L Exp2</p><p>Sof tmax represent the classification errors using only the expression-related features from the component CNN; while L 1 Sof tmax and L 2 Sof tmax represent the classification errors using the concatenated feature vector F C f eat .</p><p>The overall loss is back-propagated to both F C exp and F C ID . As a result, the expression-related and identityrelated features are fine-tuned jointly in the IACNN.</p><p>During the testing, only one stream, i.e., the component CNN, is employed for making the decision. Given a testing sample I i , the expression and identity related representations, i.e. f E (I i ) and f ID (I i ) are calculated and concatenated to construct the final feature vector, i.e. F C f eat , for facial expression recognition. In this work, classification is performed using the softmax classifier of the CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To demonstrate the effectiveness of the proposed method in terms of facial expression recognition, extensive experiments have been conducted on three public databases including two posed facial expression databases, i.e. the CK+ database <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b24">[25]</ref> and the MMI database <ref type="bibr" target="#b31">[32]</ref>, and more importantly, a spontaneous facial expression database, i.e., the SFEW datasets <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preprocessing</head><p>Face alignment is conducted to reduce variation in face scale and in-plane rotation across different facial images. Specifically, 66 landmarks are detected using a state-ofthe-art face alignment method, i.e., Discriminative Response Map Fitting (DRMF) <ref type="bibr" target="#b0">[1]</ref>. The face regions are aligned based on three fiducial points: the centers of the eyes and the mouth, and then are cropped and scaled to a size of 60 ? 60.</p><p>It may be not sufficient to learn a deep model with the limited number of sequences or images in the facial expression databases. To alleviate the chance of over-fitting, an augmentation procedure is employed to train the CNN models, where a 48 ? 48 patch is randomly cropped from an image and randomly flipped horizontally as the input of the CNN, resulting 288 times larger than the original training data. During testing, only the 48 ? 48 patch centered at the face image is used as the input to one stream of the IACNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>The proposed component CNN is fine-tuned from a CNN model pretrained on the Facial Expression Recognition (FER-2013) dataset <ref type="bibr" target="#b6">[7]</ref> using stochastic gradient decent with a batch size of 128, momentum of 0.9, and a weight decay parameter of 0.005. Dropout is applied to each FC layer with a probability of 0.6, i.e. zeroing out the output of a neuron with probability of 0.6. In Eq. 8, ? 2 is set to 5 for CK+/SFEW and 2 for MMI, while other parameters are set to 1 for all datasets empirically. The CNNs are implemented using the Caffe library <ref type="bibr" target="#b10">[11]</ref>.</p><p>Identity information is required to train the IACNN model. Labeled subject IDs are provided in the CK+ and MMI databases and we manually labeled subject IDs for the SFEW database. In practice, the proposed system only requires weakly supervised identity information, i.e., whether the two images are from the same subject, which can be automatically obtained by an off-the-shelf face verification method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results</head><p>To better demonstrate the effectiveness of the proposed model, two baseline methods are employed, i.e. the onestream component CNN described in Section III-A, denoted by CN N , and the Exp-Net introduced in Section III-B, denoted by Exp -N et.</p><p>1) Results on CK+ dataset: CK+ database <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b24">[25]</ref> is widely used for evaluating facial expression recognition system. It contains 327 image sequences collected from 118 subjects, each of which is labeled as one of 7 expressions, i.e. anger, contempt, disgust, fear, happiness, sadness, and surprise. For each sequence, the label is only provided for TABLE I: Confusion matrix of the proposed IACNN method evaluated on the CK+ database <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The ground truth and the predicted labels are given by the first column and the first row, respectively.   <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b24">[25]</ref> in terms of the average accuracy of 7 expressions.</p><p>Method Accuracy 3DCNN <ref type="bibr" target="#b20">[21]</ref> 85.9 MSR <ref type="bibr" target="#b32">[33]</ref> 91.4 HOG 3D <ref type="bibr" target="#b17">[18]</ref> 91.44 TMS <ref type="bibr" target="#b9">[10]</ref> 91.89 Cov3D <ref type="bibr" target="#b34">[35]</ref> 92.3 3DCNN-DAP <ref type="bibr" target="#b20">[21]</ref> 92.4 STM-ExpLet <ref type="bibr" target="#b21">[22]</ref> 94.19 DTAGN <ref type="bibr" target="#b12">[13]</ref> 97. the last frame (the peak frame). To collect more data, the last three frames of each sequence are selected as peak frames associated with the provided expression label. Thus, an experimental database consisting of 981 images is built. The database is further divided into 8 subsets, where the subjects in any two subsets are mutually exclusive. Then an 8-fold cross-validation strategy is employed, where, for each run, data from 6 subsets are used for training and that from the remaining two subsets for validation and testing, respectively.</p><p>The proposed IACNN and the two baseline methods are trained and tested on static images. The final sequencelevel predictions are obtained by choosing the class with the highest average score of the three images. The results are reported as the average of the 8 runs. The confusion matrix of the proposed IACNN model is reported in Table <ref type="table">I</ref>, where diagonal entries represent the recognition accuracy for each expression. As shown in Table <ref type="table" target="#tab_1">II</ref>, the performance of the proposed IACNN outperforms the two baseline methods, especially the one-stream component CNN, in terms of the average accuracy of the 7 expressions. The IACNN model is also compared with the state-of-the-art methods evaluated on the CK+ database including methods using human crafted features (HOG 3D <ref type="bibr" target="#b17">[18]</ref>, TMS <ref type="bibr" target="#b9">[10]</ref>, Cov3D <ref type="bibr" target="#b34">[35]</ref>, and STM-ExpLet <ref type="bibr" target="#b21">[22]</ref>), methods using sparse coding (MSR <ref type="bibr" target="#b32">[33]</ref>), and CNN-based methods (3DCNN and 3DCNN-DAP <ref type="bibr" target="#b20">[21]</ref> and DTAGN <ref type="bibr" target="#b12">[13]</ref>). As shown in Table <ref type="table" target="#tab_1">II</ref>, the IACNN outperforms the methods based on human crafted features or sparse coding and also performs better or is at least comparable to the CNN-based methods. Note that all these methods except the MSR employed temporal information extracted from image sequences. In contrast, the proposed IACNN learns and extracts features from static images, which is more  suitable for applications, where videos or image sequences are not available.</p><p>Visualization Study: To further demonstrate the effectiveness in terms of learning good representations for expression recognition, we visualize the features learned by the component CNN, the Exp-Net, and the IACNN, respectively, using t-SNE <ref type="bibr" target="#b43">[44]</ref>, which is widely employed to visualize high dimensional data. As shown in Fig. <ref type="figure" target="#fig_5">5a</ref>, the training samples are denoted by dots, validation samples denoted by stars, and testing samples denoted by diamonds. As illustrated in Fig. <ref type="figure" target="#fig_5">5a</ref>, the original raw images are randomly distributed. In contrast, the learned features (Fig. <ref type="figure" target="#fig_5">5b, c,</ref> and<ref type="figure">d</ref> 2) Results on MMI dataset: The MMI dataset <ref type="bibr" target="#b31">[32]</ref> consists of 213 image sequences, among which 208 sequences containing frontal-view faces of 31 subjects will be used in our experiment. Each sequence is labeled as one of six basic expressions, i.e. anger, disgust, fear, happiness, sadness, and surprise. Starting with a neutral expression, each sequence develops the facial expression as time goes on, reaches peak near the middle of the sequence, and ends with a neutral expression. Since the actual location of the peak frame is not provided, three frames in the middle of each image sequence are collected as peak frames and associated with the provided TABLE IV: Performance comparison on the MMI database <ref type="bibr" target="#b31">[32]</ref> in terms of the average accuracy of 6 expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy 3DCNN <ref type="bibr" target="#b20">[21]</ref> 53.2 ITBN <ref type="bibr" target="#b45">[46]</ref> 59.7 HOG 3D <ref type="bibr" target="#b17">[18]</ref> 60.89 3DCNN-DAP <ref type="bibr" target="#b20">[21]</ref> 63.4 3D SIFT <ref type="bibr" target="#b37">[38]</ref> 64.39 DTAGN <ref type="bibr" target="#b12">[13]</ref> 70.24 STM-ExpLet <ref type="bibr" target="#b21">[22]</ref> 75. expression labels. Hence, there are a total of 208 ? 3 images used in our experiments. Similar to that on the CK+ database, the proposed IACNN and the two baseline methods are trained and tested on static images and the final sequence-level predictions are made by selecting the class with the highest average score of the three images. The dataset is divided into 10 subsets for personindependent 10-fold cross validation, where, for each run, data from 8 subsets are used for training and those from the remaining 2 subsets are used for validation and testing, respectively. The results are reported as the average of 10 runs. The confusion matrix of the proposed IACNN model evaluated on the MMI dataset is reported in Table <ref type="table" target="#tab_3">III</ref>.</p><p>As shown in Table <ref type="table" target="#tab_5">IV</ref>, the proposed IACNN outperforms the two baseline methods significantly. Furthermore, the IACNN also outperforms most of the state-of-the-art methods. Note that the image sequences in the MMI database contain a full temporal pattern of expressions, i.e., from neutral to apex, and then released, and are especially favored by these methods exploiting temporal information, e.g., all the state-of-art methods in comparison.</p><p>Since the MMI dataset contains a small number of samples, i.e. only 624 face images, it is not large enough to train a deep model. To demonstrate that the IACNN can achieve better performance given more training data, we employed additional data of the six basic expressions from the CK+ dataset. Specifically, for each run, the 8 subsets of the MMI dataset plus the data from the CK+ dataset will be used as  3) Results on SFEW dataset: The SFEW <ref type="bibr" target="#b5">[6]</ref> is the most widely used benchmark for facial expression recognition, which "targets the efforts required towards affect analysis in the wild" <ref type="bibr" target="#b5">[6]</ref>. The SFEW database is composed of 1,766 images, i.e. 958 for training, 436 for validation, and 372 for testing. Each of the images has been assigned to one of seven expression categories, i.e., anger, disgust, fear, neutral, happy, sad, and surprise. The expression labels of the training and validation sets are provided, while those of the testing set is held back by the challenge organizer.</p><p>As illustrated in  <ref type="bibr" target="#b29">[30]</ref>, and Yao et al. <ref type="bibr" target="#b47">[48]</ref> are ranked at the 1 st to 4 th among the 18 teams in the EmotiW2015 challenge, respectively. Note that the first two methods, i.e. Kim et al. <ref type="bibr" target="#b15">[16]</ref> and Yu et al. <ref type="bibr" target="#b48">[49]</ref>, used an ensemble of CNNs. For example, Kim et al. <ref type="bibr" target="#b15">[16]</ref> employed an ensemble of CNNs with different architectures, which can boost the final performance. The proposed IACNN model outperforms the baseline of SFEW (35.93 on the validation set and 39.13 on the testing set) by a large margin. The proposed IACNN is ranked at the 5 th place on both the validation set and the testing set among all the methods compared and achieves comparable performance with Ng et al. <ref type="bibr" target="#b29">[30]</ref> and Yao et al. <ref type="bibr" target="#b47">[48]</ref>, which demonstrates the effectiveness of the IACNN model in the real world.</p><p>4) Cross-database validation: To further demonstrate that the proposed IACNN is less affected by identity, a cross-  In this work, we proposed a novel identity-aware CNN to capture both expression-related and identity-related information to alleviate the effect of personal attributes on facial expression recognition. Specifically, a softmax loss combined with an expression-sensitive contrastive loss is utilized to learn the expression-related representations. To alleviate the variations introduced by different identities, a new auxiliary layer with an identity-sensitive contrastive loss is employed to learn identity-related representations. Both the expressionrelated and identity-related features are concatenated and employed to achieve an identity-invariant facial expression recognition.</p><p>Experimental results on two posed facial expression datasets have demonstrated that the proposed IACNN model outperformed the baseline CNN methods as well as most of the state-of-the-art methods that exploit dynamic information extracted from image sequences. More importantly, IACNN has shown promise on a spontaneous facial expression dataset, which demonstrates its effectiveness in real world. In the future, we plan to recognize face expressions from videos by incorporating temporal information in the proposed IACNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGEMENT</head><p>This work is supported by National Science Foundation under CAREER Award IIS-1149787.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Illustration for features learned by (a) existing methods, and (b) the proposed IACNN. Best viewed in color.</figDesc><graphic url="image-1.png" coords="1,324.22,181.47,92.02,92.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The architecture of the IACNN used for training, where a pair of images are input into two identical CNNs with sharing weights, respectively. In each CNN, there are two FC layers, i.e. F Cexp, and F CID, on top of the first FC layer for learning the expression-related and identity-related features, respectively. L Exp Contrasitve / L ID Contrasitve is a contrastive loss used to minimize the differences between the samples with the same expression/identity. With explicit expression labels, L Exp Sof tmax and L Sof tmax are employed to ensure the learned features are meaningful for expression recognition. Loss layers are highlighted by gray blocks. Best viewed in color.</figDesc><graphic url="image-38.png" coords="2,114.22,71.28,383.05,240.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The proposed Exp-Net for facial expression recognition includes a pair of identical component CNNs, whose weights are shared.</figDesc><graphic url="image-44.png" coords="3,114.48,71.28,383.05,150.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: During the training process, the contrastive loss (a) pulls the samples with the same expression towards each other, and (b) pushes the samples with different expressions apart. Lp and Ln represent the contrastive losses for the image pairs with the same expression and different expressions, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: A visualization study of (a) the original raw images and the features learned by (b) the component CNN, (c) the Exp-Net, and (d) the IACNN model on the CK+ database. The number of samples is 981 including 247 ? 3 training data from 6 subsets, 38 ? 3 validation data, and 42 ? 3 testing data. The dots, stars, and diamonds represent training, validation, and testing data, respectively. The features learned by the IACNN are better separated according to expressions for both the validation and testing data. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>) are clustered based on their expression labels. A close-up enclosed by the rectangle is given for Fig.5b, c, and d. Comparing Fig. 5b (the component CNN) with Fig. 5c (Exp-Net) and d (IACNN), the samples of the same subject with different expressions are closer to each other rather than to their corresponding cluster centers marked by magenta circles. As compared to the features learned by the component CNN and the Exp-Net, the proposed IACNN model yields a better separation of the features, as depicted in Fig. 5d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Performance comparison on the CK+ database</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Confusion matrix of the proposed IACNN method evaluated on the MMI database<ref type="bibr" target="#b31">[32]</ref>. The ground truth and the predicted labels are given by the first column and the first row, respectively.</figDesc><table><row><cell></cell><cell>An</cell><cell>Di</cell><cell>Fe</cell><cell>Ha</cell><cell>Sa</cell><cell>Su</cell></row><row><cell>An</cell><cell cols="2">81.8% 3%</cell><cell cols="4">3% 1.5% 10.6% 0%</cell></row><row><cell>Di</cell><cell cols="6">10.9% 71.9% 3.1% 4.7% 9.4% 6%</cell></row><row><cell>Fe</cell><cell cols="6">5.4% 8.9% 41.1% 7.1% 7.1% 30.4%</cell></row><row><cell>Ha</cell><cell cols="6">1.1% 3.6% 0% 92.9% 2.4% 0%</cell></row><row><cell>Sa</cell><cell cols="6">17.2% 7.8% 0% 1.6% 73.4% 0%</cell></row><row><cell>Su</cell><cell cols="6">7.3% 0% 14.6% 1.2% 0% 76.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Confusion matrix of the proposed IACNN method evaluated on the SFEW<ref type="bibr" target="#b5">[6]</ref> validation set. The ground truth and the predicted labels are given by the first column and the first row, respectively.</figDesc><table><row><cell></cell><cell>An Di Fe</cell><cell>Ha</cell><cell>Ne</cell><cell>Sa</cell><cell>Su</cell></row><row><cell>An</cell><cell cols="5">70.7% 0% 2.7% 5.2% 6.7% 8% 6.7%</cell></row><row><cell>Di</cell><cell cols="5">19.1% 0% 0% 4.8% 33.3% 38% 4.8%</cell></row><row><cell>Fe</cell><cell cols="5">37.8% 0% 8.9% 11.1% 15.6% 13.3% 13.3%</cell></row><row><cell>Ha</cell><cell cols="5">9.9% 0% 0% 70.4% 9.9% 8.4% 1.4%</cell></row><row><cell>Ne</cell><cell cols="5">5.1% 0% 2.6% 1.3% 60.3% 24.3% 6.4%</cell></row><row><cell>Sa</cell><cell cols="5">7.4% 0% 2.9% 5.9% 20.6% 58.8% 4.4%</cell></row><row><cell>Su</cell><cell cols="5">23.1% 0% 3.8% 3.8% 28.9% 11.5% 28.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>Confusion matrix of the proposed IACNN method evaluated on the SFEW<ref type="bibr" target="#b5">[6]</ref> testing set. The ground truth and the predicted labels are given by the first column and the first row, respectively.</figDesc><table><row><cell></cell><cell>An Di Fe</cell><cell>Ha</cell><cell>Ne</cell><cell>Sa</cell><cell>Su</cell></row><row><cell>An</cell><cell cols="5">79.8% 0% 1.4% 0% 8.7% 1.4% 8.7%</cell></row><row><cell>Di</cell><cell cols="5">35.3% 0% 0% 29.4% 11.8% 23.5% 0%</cell></row><row><cell>Fe</cell><cell cols="5">34.1% 0% 9.8% 12.2% 9.8% 7.3% 26.8%</cell></row><row><cell>Ha</cell><cell cols="5">8.4% 0% 0% 75.8% 4.2% 7.4% 4.2%</cell></row><row><cell>Ne</cell><cell cols="5">17.2% 0% 1.7% 3.5% 60.4% 10.3% 6.9%</cell></row><row><cell>Sa</cell><cell cols="5">23.6% 0% 7.3% 20% 5.5% 34.6% 9%</cell></row><row><cell>Su</cell><cell cols="5">24.3% 0% 2.7% 8.1% 10.8% 8.1% 46%</cell></row></table><note><p>training set and the remaining two subsets of the MMI dataset are used as the validation and testing sets, respectively. The results are reported at the bottom of Table IV denoted as CNN CK , CNN CK , and IACNN CK for the one-stream CNN, the Exp-Net, and the proposed IACNN, respectively. With additional training data, the recognition accuracy of the IACNN further improves from 69.48 to 71.55.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table VII ,</head><label>VII</label><figDesc>Kim et al. [16], Yu et al. [49], Ng et al.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII :</head><label>VII</label><figDesc>Performance comparison on the SFEW database<ref type="bibr" target="#b5">[6]</ref> in terms of the average accuracy of 7 expressions.</figDesc><table><row><cell>Method</cell><cell>Validation Set</cell><cell>Test Set</cell></row><row><cell>Kim et al. [16]</cell><cell>53.9</cell><cell>61.6</cell></row><row><cell>Yu et al. [49]</cell><cell>55.96</cell><cell>61.29</cell></row><row><cell>Ng et al. [30]</cell><cell>48.5</cell><cell>55.6</cell></row><row><cell>Yao et al. [48]</cell><cell>43.58</cell><cell>55.38</cell></row><row><cell>Sun et al. [40]</cell><cell>51.02</cell><cell>51.08</cell></row><row><cell>Zong et al. [55]</cell><cell>N/A</cell><cell>50</cell></row><row><cell>Kaya et al. [15]</cell><cell>53.06</cell><cell>49.46</cell></row><row><cell>Mollahosseini et al. [29]</cell><cell>47.7</cell><cell>N/A</cell></row><row><cell>Dhall et al. [6] (baseline of SFEW)</cell><cell>35.93</cell><cell>39.13</cell></row><row><cell>CNN (baseline)</cell><cell>47.80</cell><cell>50.54</cell></row><row><cell>Exp-Net</cell><cell>49.51</cell><cell>52.96</cell></row><row><cell>IACNN</cell><cell>50.98</cell><cell>54.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII :</head><label>VIII</label><figDesc>Cross-database facial expression recognition performance in terms of average accuracy. experiment was conducted. The classifiers trained on CK+ and MMI in the previous experiments are directly applied to MMI and CK+, respectively. The results are reported as the average of 10 runs for CK+ and 8 runs for MMI. As shown in TableVIII, the proposed IACNN outperforms all the state-of-the-art methods as well as the baseline CNN method.V. CONCLUSION</figDesc><table><row><cell>Test Set</cell><cell>[26]</cell><cell>[39]</cell><cell>[27]</cell><cell>CNN</cell><cell>IACNN</cell></row><row><cell>CK+</cell><cell>47.1</cell><cell>-</cell><cell>56</cell><cell>69.26</cell><cell>71.29</cell></row><row><cell>MMI</cell><cell>51.4</cell><cell>50.8</cell><cell>36.8</cell><cell>54.87</cell><cell>55.41</cell></row></table><note><p>database</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust discriminative response map fitting with constrained local models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3444" to="3451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-dataset learning and person-specific normalisation for automatic action unit detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning person-specific models for facial expression and action unit recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aragones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1964" to="1970" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Selective transfer machine for personalized facial expression analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video and image based emotion recognition challenges in the wild: Emotiw 2015</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ramana Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="423" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Challenges in representation learning: A report on three machine learning contests</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facial expression recognition with temporal modeling of shapes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1642" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Action unit detection using sparse appearance descriptors in space-time video volumes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FG</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comprehensive database for facial expression analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="46" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contrasting and combining least squares based learners for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>G?rpinar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Salah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="459" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical committee of deep cnns with exponentially-weighted decision fusion for static facial expression recognition</title>
		<author>
			<persName><forename type="first">B.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transformation equivariant boltzmann machines</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IACNN</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marsza?ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="275" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AU-aware deep networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeply learning deformable facial action parts model for dynamic expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1749" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Facial expression recognition via a boosted deep belief network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1805" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving facial expression analysis using histograms of log-transformed nonnegative sparse representation with a spatial pyramid structure</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In FG</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete expression dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-database evaluation for facial expression recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Radig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition and image analysis</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="124" to="132" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-domain facial expression recognition using supervised kernel mean matching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMLA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="326" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Simultaneous recognition of facial expression and identity via sparse representation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1066" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper in facial expression recognition using deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In WACV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning for emotion recognition on small datasets using transfer learning</title>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vonikakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="443" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human computing and machine understanding of human behavior: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nijholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence for Human Computing</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nijholt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Web-based database for facial expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Manifold based sparse representation for robust expression recognition without neutral subtraction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ptucha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsagkatakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2136" to="2143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disentangling factors of variation for facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="808" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatiotemporal covariance descriptors for action and gesture recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sanin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial affect: A survey of registration, representation and recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sariyanidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1113" to="1133" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning rotation-aware features: From invariant priors to equivariant descriptors</title>
		<author>
			<persName><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2050" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Facial expression recognition based on Local Binary Patterns: A comprehensive study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. IVC</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="803" to="816" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combining multimodal features within a fusion network for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICMI</title>
		<imprint>
			<biblScope unit="page" from="497" to="502" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep learning using linear support vector machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Metaanalysis of the first facial expression recognition challenge</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-SMC-B</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="966" to="979" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Capturing complex spatio-temporal relations among facial muscles for facial expression recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3422" to="3429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Boosting encoded dynamic features for facial expression recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132" to="139" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Capturing AU-aware facial features and their latent relations for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="451" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image based static facial expression recognition with multiple deep network learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="435" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Discriminant multi-label manifold embedding for facial action unit detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yuce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FG</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A survey of affect recognition methods: Audio, visual, and spontaneous expressions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pieti?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007-06">June 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Peak-piloted deep network for facial expression recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="425" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning multiscale active facial patches for expression analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1499" to="1510" />
			<date type="published" when="2015-08">August 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Transductive transfer lda with riesz-based volume lbp for emotion recognition in the wild</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="491" to="496" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
