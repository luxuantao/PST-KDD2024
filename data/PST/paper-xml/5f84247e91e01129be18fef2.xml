<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Survey of Knowledge-Enhanced Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-22">22 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
							<email>chezhu@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
							<email>zhitinghu@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
							<email>qingyun4@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Heng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
							<email>mjiang2@nd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zaitang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
							<email>hengji@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">ZAITANG LI</orgName>
								<orgName type="institution" key="instit2">The Chinese University of Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of California at San Diego</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">University of Notre Dame</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<addrLine>Chenguang Zhu</addrLine>
									<postCode>46556</postCode>
									<region>Indiana</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>Washington</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">Zaitang Li</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<addrLine>Zhiting Hu</addrLine>
									<postCode>999077</postCode>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution">University of California at San Diego</orgName>
								<address>
									<postCode>92092</postCode>
									<settlement>San Diego</settlement>
									<region>California, Qingyun Wang</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<region>Illinois</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana, Meng Jiang</settlement>
									<region>Illinois</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff14">
								<orgName type="institution" key="instit1">University of Notre Dame</orgName>
								<orgName type="institution" key="instit2">Notre Dame</orgName>
								<address>
									<postCode>46556</postCode>
									<region>Indiana</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff15">
								<orgName type="institution">ConceptNet Input</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Survey of Knowledge-Enhanced Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-22">22 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3512467</idno>
					<idno type="arXiv">arXiv:2010.04389v4[cs.CL]</idno>
					<note type="submission">Accepted to ACM Computing Survey (CUSR)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Natural language generation, Knowledge-enhanced Methods</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of text-to-text generation is to make machines express like a human in many applications such as conversation, summarization, and translation. It is one of the most important yet challenging tasks in natural language processing (NLP). Various neural encoder-decoder models have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating (i) internal knowledge embedded in the input text and (ii) external knowledge from outside sources such as knowledge base and knowledge graph into the text generation system. This research topic is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on this topic over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.</p><p>CCS Concepts: • General and reference → Surveys and overviews; • Computing methodologies → Natural language processing; Neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Text generation, which is often formally referred as natural language generation (NLG), is one of the most important yet challenging tasks in natural language processing (NLP) <ref type="bibr" target="#b37">[37]</ref>. NLG aims at producing understandable text in human language from linguistic or non-linguistic data in a variety of forms such as textual data, numerical data, image data, structured knowledge bases, and knowledge graphs. Among these, text-to-text generation is one of the most important applications and thus often shortly referred as "text generation". Researchers have developed numerous technologies for this task in a wide range of applications <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b125">125]</ref>. Text generation takes text (e.g., a sequence, keywords) as input, processes the input text into semantic representations, and generates desired output text. For example, machine translation generates text in a different language based on the source text; summarization generates an abridged version of the source text to include salient information; question answering (QA) generates textual answers to given questions; dialogue system supports chatbots to communicate with humans with generated responses.</p><p>With the recent resurgence of deep learning technologies <ref type="bibr" target="#b66">[66]</ref>, deep neural NLG models have achieved remarkable performance in enabling machines to understand and generate natural language. A basic definition of the text generation task is to generate an expected output sequence from a given input sequence, called sequence-to-sequence (Seq2Seq). The Seq2Seq task and model were first introduced in 2014 <ref type="bibr" target="#b117">[117]</ref>. It maps an input text to an output text under encoder-decoder schemes. The encoder maps the input sequence to a fixed-sized vector, and the decoder maps the vector to the target sequence. Since then, developing NLG systems has rapidly become a hot topic. Various text generation models have been proposed under deep neural encoder-decoder architectures. Popular architectures include recurrent neural network (RNN) encoder-decoder <ref type="bibr" target="#b117">[117]</ref>, convolutional neural network (CNN) encoder-decoder <ref type="bibr" target="#b39">[39]</ref>, and Transformer encoder-decoder <ref type="bibr" target="#b122">[122]</ref>.</p><p>Nevertheless, the input text alone contains limited knowledge to support neural generation models to produce the desired output. Meanwhile, the aforementioned methods generally suffer from an inability to well comprehend language, employ memory to retain and recall knowledge, and reason over complex concepts and relational paths; as indicated by their name, they involve encoding an input sequence, providing limited reasoning by transforming their hidden state given the input, and then decoding to an output. Therefore, the performance of generation is still far from satisfaction in many real-world scenarios. For example, in dialogue systems, conditioning on only the input text, a text generation system often produces trivial or non-committal responses of frequent words or phrases in the corpus <ref type="bibr" target="#b139">[139,</ref><ref type="bibr" target="#b160">159]</ref>, such as "Me too. " or "Oh my god!" given the input text "My skin is so dry. " These mundane responses lack meaningful content, in contrast to human responses rich in knowledge. In comparison, humans are constantly acquiring, understanding, and storing knowledge from broader sources so that they can be employed to understand the current situation in communicating, reading, and writing. For example, in conversations, people often first select concepts from related topics (e.g., sports, food), then organize those topics into understandable content to respond; for summarization, people tend to write summaries containing keywords used in the input document and perform necessary modifications to ensure grammatical correctness and fluency; in question answering (QA), people use commonsense or professional knowledge pertained to the question to infer the answer. Therefore, it is often the case that knowledge beyond the input sequence is required to produce informative output text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">What is Knowledge-enhanced Text Generation?</head><p>In general, knowledge is the familiarity, awareness, or understanding that coalesces around a particular subject. In NLG systems, knowledge is an awareness and understanding of the input text and its surrounding context. These knowledge sources can be categorized into internal knowledge and external knowledge (see Figure <ref type="figure" target="#fig_7">1</ref>). Internal knowledge creation takes place within the input</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>External knowledge</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge acquisition</head><p>Fig. <ref type="figure" target="#fig_7">1</ref>. We divide different knowledge sources into internal knowledge and external knowledge. Internal knowledge creation takes place within the input text(s), while external knowledge acquisition occurs when knowledge is provided from outside sources (e.g., Wikipedia, ConceptNet <ref type="bibr" target="#b115">[115]</ref>).</p><p>text(s), including but not limited to keyword, topic, linguistic features, and internal graph structure. External knowledge acquisition occurs when knowledge is provided from outside sources, including but not limited to knowledge base, external knowledge graph, and grounded text. These sources provide information (e.g., commonsense triples, topic words, reviews, background documents) that can be used as knowledge through various neural representation learning methods, and then applied to enhance the process of text generation. In addition, knowledge introduces interpretability for models with explicit semantics. This research direction of incorporating knowledge into text generation is named as knowledge-enhanced text generation.</p><p>Problem 1 (Knowledge-enhanced Text Generation). Given a text generation problem where the system is given an input sequence 𝑋 , and aims to generate an output sequence 𝑌 . Assume we also have access to additional knowledge denoted as 𝐾. Knowledge-enhanced text generation aims to incorporate the knowledge 𝐾 to enhance the generation of 𝑌 given 𝑋 , through leveraging the dependencies among the input text, knowledge, and output text.</p><p>Many existing knowledge-enhanced text generation systems have demonstrated promising performance on generating informative, logical, and coherent texts. In dialogue systems, a topicaware Seq2Seq model helped understand the semantic meaning of an input sequence and generate a more informative response such as "Then hydrate and moisturize your skin. " to the aforementioned example input "My skin is so dry." In summarization, knowledge graph produced a structured summary and highlight the proximity of relevant concepts, when complex events related with the same entity may span multiple sentences. A knowledge graph enhanced Seq2Seq model generated summaries that were able to correctly answer 10% more topically related questions <ref type="bibr" target="#b54">[54]</ref>. In question answering (QA) systems, facts stored in knowledge bases completed missing information in the question and elaborate details to facilitate answer generation <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b48">48]</ref>. In story generation, using commonsense knowledge acquired from knowledge graph facilitated understanding of the storyline and better narrate following plots step by step, so each step could be reflected as a link on the knowledge graph and the whole story would be a path <ref type="bibr" target="#b46">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Why a Survey of Knowledge-enhanced Text Generation?</head><p>Recent years have witnessed a surge of interests in developing methods for incorporating knowledge in NLG beyond input text. However, there is a lack of comprehensive survey of this research topic. Related surveys have laid the foundation of discussing this topic. For example, Garbacea et al. <ref type="bibr" target="#b37">[37]</ref> and Gatt et al. <ref type="bibr" target="#b38">[38]</ref> reviewed model architectures for core NLG tasks but did not discuss knowledgeenhanced methods. Ji et al. <ref type="bibr" target="#b58">[58]</ref> presented a review on knowledge graph techniques which could be used for enhancing NLG. Wang et al. <ref type="bibr" target="#b125">[125]</ref> summarized how to represent structural knowledge such as knowledge base and knowledge graph for reading comprehension and retrieval. Fig. <ref type="figure">2</ref>. Categorization of information sources and methods for knowledge-enhanced text generation. Knowledge can be learnt from various information sources, and then integrated into the generation process.</p><p>To the best of our knowledge, this is the first survey that presents a comprehensive review of knowledge-enhanced text generation. It aims to provide NLG researchers a synthesis and pointer to related research. Our survey includes a detailed discussion about how NLG can benefit from recent progress in deep learning and artificial intelligence, including technologies such as graph neural network, reinforcement learning, and neural topic modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">What are the Challenges in Knowledge-enhanced Text Generation?</head><p>To start with, we note that the first challenge in knowledge-enhanced NLG is to obtain useful related knowledge from diverse sources. There has been a rising line of work that discovers knowledge from topic, keyword, knowledge base, knowledge graph and knowledge grounded text. The second challenge is how to effectively understand and leverage the acquired knowledge to facilitate text generation. Multiple methods have been explored to improve the encoder-decoder architecture (e.g., attention mechanism, copy and pointing mechanism).</p><p>Based on the first challenge, the main content of our survey is divided into two parts: (1) general methods of integrating knowledge into text generation (Section 2); (2) specific methods and applications according to different sources of knowledge enhancement (Sections 3-4). More concretely, since knowledge can be obtained from different sources, we first divide existing knowledge enhanced text generation work into two categories: internal knowledge enhanced and external knowledge enhanced text generation. The division of internal and external knowledge is widely adopted by management science <ref type="bibr" target="#b88">[88]</ref>, which can be analogous with knowledge enhanced text generation. Based on the second challenge, we categorize recent knowledge-enhanced text generation methods evolved from how knowledge is extracted and incorporated into the process of text generation in each section (named as M1, M2, and etc). Furthermore, we review methods for a variety of natural language generation applications in each section to help practitioners choose, learn, and use the methods. In total, we discuss seven mainstream applications presented in more than 80 papers that were published or released in or after the year of 2016.</p><p>As shown in Figure <ref type="figure">2</ref>, the remainder of this survey is organized as follows. Section 2 presents basic NLG models and general methods of integrating knowledge into text generation. Sections 3 reviews internal knowledge-enhanced NLG methods and applications. The internal knowledge is obtained from topic, keyword, linguistic features and internal graph structures. Sections 4 reviews external knowledge-enhanced NLG methods and applications. The external knowledge sources include knowledge bases, knowledge graphs, and grounded text. Section 5 presents knowledge-enhanced NLG benchmarks. Section 6 discusses future work and concludes the survey. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GENERAL METHODS OF INTEGRATING KNOWLEDGE INTO NLG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Basic Text Generation Models</head><p>Early encoder-decoder frameworks are often based on recurrent neural network (RNN) such as RNN-Seq2Seq <ref type="bibr" target="#b117">[117]</ref>. Convolutional neural network (CNN) based encoder-decoder <ref type="bibr" target="#b39">[39]</ref> and Transformer encoder-decoder <ref type="bibr" target="#b122">[122]</ref> have been increasingly widely used. From a probabilistic perspective, the encoder-decoder frameworks learn the conditional distribution over a variable length sequence conditioned on yet another variable length sequence:</p><formula xml:id="formula_0">𝑃 (𝑌 |𝑋 ) = 𝑃 (𝑦 1 , • • • , 𝑦 𝑚 |𝑥 1 , • • • , 𝑥 𝑛 ) = 𝑚 𝑡 =1 𝑝 (𝑦 𝑡 |𝑋, 𝑦 1 , • • • , 𝑦 𝑡 −1 ).<label>(1)</label></formula><p>Encoder. The encoder learns to encode a variable length sequence into a fixed length vector representation. RNN encoder reads the input sentence 𝑋 sequentially. CNN encoder performs convolutional operations on a word and its surrounding word(s) in a sequential window. Transformer encoder eschews recurrence and instead relying entirely on the self-attention mechanism to draw global dependencies between different tokens in the input 𝑋 . We denote them uniformly as:</p><formula xml:id="formula_1">(h 1 , h 2 , • • • , h 𝑛 ) = Encoder(e(𝑥 1 ), e(𝑥 2 ), • • • , e(𝑥 𝑛 )),<label>(2)</label></formula><p>where e(𝑥 𝑖 ) is the word embedding of word 𝑥 𝑖 , h 𝑖 is the contextualized hidden representation of 𝑥 𝑖 .</p><p>Decoder. The decoder is to decode a given fixed length vector representation into a variable length sequence <ref type="bibr" target="#b117">[117]</ref>. Specially, the decoder generates an output sequence one token at each time step. At each step the model is auto-regressive, consuming the previously generated tokens as additional input when generating the next token. Formally, the decoding function is represented as:</p><formula xml:id="formula_2">s 𝑡 = Decoder(s 𝑡 −1 , e(𝑦 𝑡 −1 )),<label>(3)</label></formula><formula xml:id="formula_3">𝑝 (𝑦 𝑡 |𝑦 𝑡 −1 , 𝑦 𝑡 −2 , • • • , 𝑦 1 ) = Readout(s 𝑡 ),<label>(4)</label></formula><p>where Readout(•) is a nonlinear multi-layered function that outputs the probability of 𝑦 𝑡 .</p><p>Optimization. A generation process is regarded as a sequential multi-label classification problem. It can be directly optimized by the negative log likelihood (NLL) loss. Therefore, the objective of a text generation model via maximum likelihood estimation (MLE) is formulated as:</p><formula xml:id="formula_4">L 𝑁 𝐿𝐿 (𝜃 ) = − log 𝑝 𝜃 (𝑌 |𝑋 ) = − 𝑚 ∑︁ 𝑡 =1 log (𝑝 𝜃 (𝑦 𝑡 |𝑦 &lt;𝑡 , 𝑋 )) .</formula><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge-enhanced Model Architectures</head><p>The most popular idea of incorporating knowledge is designing specialized architectures of text generation models that can reflect the particular type of knowledge. In the context of neural networks, several general neural architectures are widely used and customized to bake the knowledge about the problems being tackled into the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Attention Mechanism.</head><p>It is useful to capture the weight of each time step in both encoder and decoder <ref type="bibr" target="#b2">[3]</ref>. During the decoding phase, the context vector c 𝑡 is added, so the hidden state s 𝑡 is:</p><formula xml:id="formula_5">s 𝑡 = Decoder(s 𝑡 −1 , e(𝑦 𝑡 −1 ), c 𝑡 ).<label>(6</label></formula><p>) Unlike Eq.( <ref type="formula" target="#formula_2">3</ref>), here the probability is conditioned on the distinct context vector c 𝑡 for target word 𝑦 𝑡 , and c 𝑡 depends on a sequence of hidden states H = {h 𝑖 } 𝑛 𝑖=1 that were mapped from input sequence. In RNN-Seq2Seq decoder, the c 𝑡 is computed as a weighted sum of {h 𝑖 } 𝑛 𝑖=1 : where 𝜂 (•) is parametrized as a multi-layer perception to compute a soft alignment. 𝜂 (•) enables the gradient of loss function to be backpropagated. There are six alternatives for the 𝜂 (•) function (see Table <ref type="table">2</ref> in <ref type="bibr" target="#b37">[37]</ref>). The probability 𝛼 𝑡𝑖 reflects the importance of the hidden state of input sequence in presence of the previous hidden state s 𝑡 −1 for deciding the next hidden state.</p><formula xml:id="formula_6">c 𝑡 = 𝑛 ∑︁ 𝑖=1 𝛼 𝑡𝑖 h 𝑖 , where 𝛼 𝑡𝑖 = exp(𝜂 (s 𝑡 −1 , h 𝑖 )) 𝑛 𝑘=1 exp(𝜂 (s 𝑡 −1 , h 𝑘 )) ,<label>(7)</label></formula><p>In Transformer decoder, on top of the two sub-layers in the encoder, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack H. Efficient implementations of the transformer use the cached history matrix S 𝑡 to generate next token. To compare with RNN-Seq2Seq, we summarize the Transformer decoder using recurrent notation:</p><formula xml:id="formula_7">S 𝑡 = Transformer-Decoder(S 𝑡 −1 , e(𝑦 𝑡 −1 ), H),<label>(8)</label></formula><p>where</p><formula xml:id="formula_8">S 𝑡 = [(K (1) 𝑡 , V (1) 𝑡 ), • • • , (K (𝑙) 𝑡 , V (𝑙) 𝑡 )],</formula><p>where (K (𝑖) 𝑡 , V (𝑖) 𝑡 ) corresponds to the key-value pairs from the 𝑖-th layer generated at all time-steps from 0 to 𝑡. Instead of noting a specific name, we will use Encoder(•) and Decoder(•) to represent encoder and decoder in the following sections.</p><p>Knowledge-related attention. Attention mechanism has been widely used to incorporate knowledge representation in recent knowledge-enhanced NLG work. The general idea is to learn a knowledge-aware context vector (denoted as c 𝑡 ) by combining both hidden context vector (c 𝑡 ) and knowledge context vector (denoted as c 𝐾 𝑡 ) into decoder update, such as c 𝑡 = 𝑓 𝑚𝑙𝑝 (c 𝑡 ⊕ c 𝐾 𝑡 ). The knowledge context vector (c 𝐾 𝑡 ) calculates attentions over knowledge representations (e.g., topic vectors, node vectors in knowledge graph). Table <ref type="table" target="#tab_0">1</ref> summarizes a variety of knowledge attentions, including keyword attention <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b70">70,</ref><ref type="bibr" target="#b73">73]</ref>, topic attention <ref type="bibr" target="#b79">[79,</ref><ref type="bibr" target="#b134">134,</ref><ref type="bibr" target="#b139">139,</ref><ref type="bibr" target="#b152">152]</ref>, knowledge base attention <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b48">48]</ref>, knowledge graph attention <ref type="bibr" target="#b54">[54,</ref><ref type="bibr" target="#b63">63,</ref><ref type="bibr" target="#b151">151]</ref>, and grounded text attention <ref type="bibr" target="#b9">[9,</ref><ref type="bibr">87]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.2</head><p>Copy and Pointing Mechanisms. CopyNet and Pointer-generator (PG) are used to choose subsequences in the input sequence and put them at proper places in the output sequence.</p><p>CopyNet and PG have a differentiable network architecture <ref type="bibr" target="#b43">[43]</ref>. They can be easily trained in an end-to-end manner. In CopyNet and PG, the probability of generating a target token is a combination of the probabilities of two modes, generate-mode and copy-mode. First, they represent unique tokens in the global vocabulary V and the vocabulary of source sequence V 𝑋 . They build an extended vocabulary V ext = V ∪ V 𝑋 ∪ {unk}. The difference between CopyNet and PG is the way to calculate distribution over the extended vocabulary. CopyNet calculates the distribution by</p><formula xml:id="formula_9">𝑝 (𝑦 𝑡 ) = 𝑝 𝑔 (𝑦 𝑡 ) + 𝑝 𝑐 (𝑦 𝑡 ),<label>(9)</label></formula><p>where 𝑝 𝑔 (•|•) and 𝑝 𝑐 (•|•) stand for the probability of generate-mode and copy-mode. Differently, PG explicitly calculates a switch probability 𝑝 𝑚 between generate-mode and copy-mode. It recycles the attention distribution to serve as the copy distribution. The distribution over V ext is calculated by</p><formula xml:id="formula_10">𝑝 (𝑦 𝑡 ) = 𝑝 𝑚 (g) • 𝑝 𝑔 (𝑦 𝑡 ) + (1 − 𝑝 𝑚 (g)) • 𝑝 𝑐 (𝑦 𝑡 ),<label>(10)</label></formula><p>where 𝑝 𝑚 (g) indicates the probability of choosing generate-mode, which is obtained by a nonlinear multi-layered (MLP) function. Importantly, CopyNet and pointer-generator network have been used as the base module for a lot of knowledge-enhanced NLG work.</p><p>Knowledge-related mode. A knowledge-related mode chooses subsequences in the obtained knowledge and puts them at proper places in the output sequence. It helps NLG models to generate words that are not included in the global vocabulary (V) and input sequence (V 𝑋 ). For example, by adding the model of knowledge base, the extended vocabulary (V 𝑒𝑥𝑡 ) adds entities and relations from the knowledge base, i.e., V 𝑒𝑥𝑡 = V + V 𝑋 + V 𝐾𝐵 . The probability of generating a target token is a combination of the probabilities of three modes: generate-mode, copy-mode and knowledge base-mode. Therefore, knowledge-related mode is not only capable of regular generation of words but also operation of producing appropriate subsequences in knowledge sources. Table <ref type="table" target="#tab_0">1</ref> summarizes different kinds of knowledge-related modes such as topic mode <ref type="bibr" target="#b139">[139]</ref>, keyword mode <ref type="bibr" target="#b70">[70]</ref>, knowledge base mode <ref type="bibr" target="#b48">[48]</ref>, knowledge graph mode <ref type="bibr" target="#b151">[151,</ref><ref type="bibr" target="#b160">159]</ref>, and background mode <ref type="bibr">[87,</ref><ref type="bibr" target="#b107">107]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.3</head><p>Memory Network. Memory networks (MemNNs) are recurrent attention models over a possibly large external memory <ref type="bibr" target="#b116">[116]</ref>. They write external memories into several embedding matrices, and use query (generally speaking, the input sequence 𝑋 ) vectors to read memories repeatedly. This approach encodes long dialog history and memorize external information.</p><p>Given an input set {𝑚 1 , • • • , 𝑚 𝑖 } to be stored in memory. The memories of MemNN are represented by a set of trainable embedding matrices C = {C 1 , • • • , C 𝐾+1 }, where each C 𝑘 maps tokens to vectors, and a query (i.e., input sequence) vector h 𝑘 𝑋 is used as a reading head. The model loops over 𝐾 hops and it computes the attention weights at hop 𝑘 for each memory 𝑚 𝑖 using:</p><formula xml:id="formula_11">p 𝑘 𝑖 = softmax((h 𝑘 𝑋 ) ⊤ C 𝑘 𝑖 ),<label>(11)</label></formula><p>where C 𝑘 𝑖 = C 𝑘 (𝑚 𝑖 ) is the memory content in 𝑖-th position, i.e., mapping 𝑚 𝑖 into a memory vector. Here, p 𝑘 is a soft memory selector that decides the memory relevance with respect to the query vector h 𝑘 𝑋 . Then, the model reads out the memory o 𝑘 by the weighted sum over</p><formula xml:id="formula_12">C 𝑘+1 , o 𝑘 = ∑︁ 𝑖 p 𝑘 𝑖 C 𝑘+1 𝑖 .<label>(12)</label></formula><p>Then, the query vector is updated for the next hop by using h 𝑘+1 𝑋 = h 𝑘 𝑋 + o 𝑘 . The result from the encoding step is the memory vector o 𝐾 and becomes the input for the decoding step.</p><p>Knowledge-related memory. Memory augmented encoder-decoder framework has achieved promising progress for many NLG tasks. For example, MemNNs are widely used for encoding dialogue history in task-oriented dialogue systems <ref type="bibr" target="#b106">[106,</ref><ref type="bibr" target="#b135">135]</ref>. Such frameworks enable a decoder to retrieve information from a memory during generation. Recent work explored to model external knowledge with memory network such as knowledge base <ref type="bibr" target="#b82">[82,</ref><ref type="bibr" target="#b144">144]</ref> and topic <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b159">158]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Graph Network.</head><p>Graph network captures the dependence of graphs via message passing between the nodes of graphs. Graph neural networks (GNNs) <ref type="bibr" target="#b138">[138]</ref> and graph-to-sequence (Graph2Seq) <ref type="bibr" target="#b6">[6]</ref> potentiate to bridge up the gap between graph representation learning and text generation. Knowledge graph, dependency graph, and other graph structures can be integrated into text generation through various GNN algorithms. Here we denote a graph as G = (U, E), where U is the set of entity nodes and E is the set of (typed) edges. Modern GNNs typically follow a neighborhood aggregation approach, which iteratively updates the representation of a node by aggregating information from its neighboring nodes and edges. After 𝑘 iterations of aggregation, a node representation captures the structural information within its 𝑘-hop neighborhood. Formally, the 𝑘-th layer of a node 𝑢 ∈ U is:</p><formula xml:id="formula_13">u (𝑘) = Combine 𝑘 (u (𝑘−1) , Aggregate 𝑘 ( (u (𝑘−1) 𝑖 , e (𝑘−1) 𝑖 𝑗 , u (𝑘−1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑗</head><p>) : ∀(𝑢 𝑖 , 𝑒 𝑖 𝑗 , 𝑢 𝑗 ) ∈ N (𝑢) )), <ref type="bibr" target="#b13">(13)</ref> where N (𝑢) = {(𝑢 𝑖 , 𝑒 𝑖 𝑗 , 𝑢 𝑗 ) ∈ E |𝑢 𝑖 = 𝑢 or 𝑢 𝑗 = 𝑢} denotes the set of edges containing node 𝑢, u (𝑘)  and e (𝑘)  𝑖 𝑗 are feature vectors of a node 𝑢 and the edge between 𝑢 𝑖 and 𝑢 𝑗 at the 𝑘-th iteration/layer. The choice of Aggregate(•) and Combine(•) in GNNs is crucial. A number of architectures for Aggregate(•) have been proposed in different GNN works such as GAT <ref type="bibr" target="#b123">[123]</ref>. Meanwhile, the Aggregate(•) function used in labeled graphs (e.g., a knowledge graph) is often taken as those GNNs for modeling relational graphs <ref type="bibr" target="#b108">[108]</ref>. To obtain the representation of graph G (denoted as h 𝐺 ), the Readout(•) function (either a simple permutation invariant function or sophisticated graph-level pooling function) pools node features from the final iteration 𝐾,</p><formula xml:id="formula_14">h 𝐺 = Readout( u (𝐾) : 𝑢 ∈ U ).<label>(14)</label></formula><p>Applications. Graph network has been commonly used in integrating knowledge in graph structure such as knowledge graph and dependency graph. Graph attention network <ref type="bibr" target="#b123">[123]</ref> can be combined with sequence attention and jointly optimized <ref type="bibr" target="#b151">[151,</ref><ref type="bibr" target="#b160">159]</ref>. We will introduce different graph structure knowledge in subsequent sections such as knowledge graph (Section 4.2), dependency graph (Section 3.3.2-3.3.3), and open knowledge graph (OpenKG) (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.5</head><p>Pre-trained Language Models. Pre-trained language models (PLMs) aims to learn universal language representation by conducting self-supervised training on large-scale unlabeled corpora. Recently, substantial PLMs such as BERT <ref type="bibr" target="#b25">[25]</ref> and T5 <ref type="bibr" target="#b104">[104]</ref> have achieved remarkable performance in various NLP downstream tasks. However, these PLMs suffer from two issues when performing on knowledge-intensive tasks. First, these models struggle to grasp structured world knowledge, such as concepts and relations, which are very important in language understanding. For example, BERT cannot deliver great performance on many commonsense reasoning and QA tasks, in which many of the concepts are directly linked on commonsense knowledge graphs <ref type="bibr" target="#b146">[146]</ref>. Second, due to the domain discrepancy between pre-training and fine-tuning, these models do not perform well on domain-specific tasks. For example, BERT can not give full play to its value when dealing with electronic medical record analysis task in the medical field <ref type="bibr" target="#b78">[78]</ref>.</p><p>Recently, a lot of efforts have been made on investigating how to integrate knowledge into PLMs <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b78">78,</ref><ref type="bibr">80,</ref><ref type="bibr" target="#b140">140,</ref><ref type="bibr" target="#b146">146,</ref><ref type="bibr" target="#b162">161]</ref>. Specifically, we will introduce some PLMs designed for NLG tasks. Overall, these approaches can be grouped into two categories: The first one is to explicitly inject entity representation into PLMs, where the representations is pre-computed from external sources <ref type="bibr">[80,</ref><ref type="bibr" target="#b155">155]</ref>. For example, KG-BART encoded the graph structure of KGs with knowledge embedding algorithms like TransE <ref type="bibr" target="#b11">[11]</ref>, and then took the informative entity embeddings as auxiliary input <ref type="bibr">[80]</ref>. However, the method of explicitly injecting entity representation into PLMs has been argued that the embedding vectors of words in text and entities in KG are obtained in separate ways, making their vector-space inconsistent <ref type="bibr" target="#b78">[78]</ref>. The second one is to implicitly modeling knowledge information into PLMs by performing knowledge-related tasks, such as concept order recovering <ref type="bibr" target="#b162">[161]</ref>, entity category prediction <ref type="bibr" target="#b146">[146]</ref>. For example, CALM proposed a novel contrastive objective for packing more commonsense knowledge into the parameters, and jointly pre-trained both generative and contrastive objectives for enhancing commonsense NLG tasks <ref type="bibr" target="#b162">[161]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Knowledge-enhanced Learning and Inference</head><p>Besides specialized model architectures, one common way of injecting knowledge to generation models is through the supervised knowledge learning. For example, one can encode knowledge into the objective function that guides the model training to acquire desired model behaviors <ref type="bibr" target="#b27">[27,</ref><ref type="bibr">61]</ref>. Such approaches enjoy the flexibility of integrating diverse types of knowledge by expressing them as certain forms of objectives. In general, knowledge-enhanced learning is agnostic to the model architecture, and can be combined with the aforementioned architectures. 2.3.1 Learning with knowledge-related tasks. One could devise learning tasks informed by the knowledge so that the model is trained to acquire the knowledge information.</p><p>Knowledge as target. The methods can be mainly divided into two categories as shown in Figure <ref type="figure" target="#fig_1">3</ref>. The first category of knowledge-related tasks creates learning targets based on the knowledge, and the model is trained to recover the targets. These tasks can be combined as auxiliary tasks with the text generation task, resulting in a multi-task learning setting. For example, knowledge loss is defined as the cross entropy between the predicted and true knowledge sentences, and it is combined with the standard conversation generation loss to enhance grounded conversation <ref type="bibr" target="#b27">[27,</ref><ref type="bibr">61]</ref>. Similar tasks include keyword extraction loss <ref type="bibr" target="#b70">[70]</ref>, template re-ranking loss <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b129">129]</ref>, link prediction loss on knowledge graph <ref type="bibr" target="#b57">[57]</ref>, path reasoning loss <ref type="bibr" target="#b81">[81]</ref>, mode loss <ref type="bibr" target="#b137">[137,</ref><ref type="bibr" target="#b160">159]</ref>, bag-of-word (BOW) loss <ref type="bibr" target="#b74">[74,</ref><ref type="bibr" target="#b143">143]</ref>, etc. The second category of methods directly derive the text generation targets from the knowledge, and use those (typically noisy) targets as supervisions in the standard text generation task. The approach is called weakly-supervised learning. Weakly-supervised learning enforces the relevancy between the knowledge and the target sequence. For example, in the problem of aspect based summarization, the work <ref type="bibr" target="#b118">[118]</ref> automatically creates target summaries based on external knowledge bases, which are used to train the summarization model in a supervised manner.</p><p>Knowledge as condition. The second way of devising knowledge-related tasks is to augment the text generation task by conditioning the generation on the knowledge. That is, the goal is to learn a function 𝑝 𝜃 (𝑌 |𝑋, 𝐾), where 𝑋 is the input sequence, 𝑌 is the target text and 𝐾 is the knowledge. Generally, the knowledge 𝐾 is first given externally (e.g., style, emotion) or retrieved from external resources (e.g., facts from knowledge base, a document from Wikipedia) or extracted from the given input text (e.g., keywords, topic words). Second, a conditional text generation model is used to incorporate knowledge and generate target output sequence. In practice, knowledge is often remedied by soft enforcing algorithms such as attention mechanism <ref type="bibr" target="#b2">[3]</ref> and copy/pointing mechanism <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b109">109]</ref>. Regarding knowledge as condition is widely used in knowledge-enhanced text generation. For examples, work has been done in making personalized dialogue response by taking account of persona <ref type="bibr" target="#b154">[154]</ref> and emotion <ref type="bibr" target="#b159">[158]</ref>, controlling various aspects of the response such as politeness <ref type="bibr" target="#b96">[96]</ref>, grounding the responses in external source of knowledge <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b160">159]</ref> and generating topic-coherent sequence <ref type="bibr" target="#b119">[119,</ref><ref type="bibr" target="#b143">143]</ref>. Besides, using variational autoencoder (VAE) to enforce the generation process conditioned on knowledge is one popular approach to unsupervised NLG. By manipulating latent space for certain attributes, such as topic <ref type="bibr" target="#b132">[132]</ref> and style <ref type="bibr" target="#b50">[50]</ref>, the output sequence can be generated with desired attributes without supervising with parallel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3.2</head><p>Learning with knowledge constraints. Instead of creating training objectives in standalone tasks that encapsulate knowledge, another paradigm of knowledge-enhanced learning is to treat the knowledge as the constraints to regularize the text generation training objective.</p><p>The posterior regularization (PR) framework was proposed to restrict the space of the model posterior on unlabeled data as a way to guide the model towards desired behavior <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b165">164]</ref>. PR has been used as a principled framework to impose knowledge constraints on probabilistic models (including deep networks) in general <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b153">153]</ref>. PR augments any regular training objective L (𝜃 ) (e.g., negative log-likelihood, as in Eq.( <ref type="formula">5</ref>)) with a constraint term to encode relevant knowledge. Formally, denote the constraint function as 𝑓 (𝑋, 𝑌 ) ∈ R such that a higher 𝑓 (𝑋, 𝑌 ) value indicates a better generated sequence 𝑌 that incorporates the knowledge. PR introduces an auxiliary distribution 𝑞(𝑌 |𝑋 ), and imposes the constraint on 𝑞 by encouraging a large expected 𝑓 (𝑋, 𝑌 ) value: E𝑞 [𝑓 (𝑋, 𝑌 )]. Meanwhile, the model 𝑝 𝜃 is encouraged to stay close to 𝑞 through a KL divergence term. The learning problem is thus a constrained optimization:</p><formula xml:id="formula_15">max 𝜃,𝑞 L (𝜃 ) − KL(𝑞(𝑌 |𝑋 )||𝑝 𝜃 (𝑌 |𝑋 )) + 𝜉 (15) 𝑠.𝑡 . E𝑞 [𝑓 (𝑋, 𝑌 )] &gt; 𝜉, (<label>16</label></formula><formula xml:id="formula_16">)</formula><p>where 𝜉 is the slack variable. The PR framework is also related to other constraint-driven learning methods <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b83">83]</ref>. We refer readers to <ref type="bibr" target="#b35">[35]</ref> for more discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.3.3</head><p>Inference with knowledge constraints. Pre-trained language models leverage large amounts of unannotated data with a simple log-likelihood training objective. Controlling language generation by particular knowledge in a pre-trained model is difficult if we do not modify the model architecture to allow for external input knowledge or fine-tuning with specific data <ref type="bibr" target="#b24">[24]</ref>.</p><p>Plug and play language model (PPLM) opened up a new way to control language generation with particular knowledge during inference. At every generation step during inference, the PPLM shifts the history matrix in the direction of the sum of two gradients: one toward higher log-likelihood of the attribute 𝑎 under the conditional attribute model 𝑝 (𝑎|𝑌 ) and the other toward higher loglikelihood of the unmodified pre-trained generation model 𝑝 (𝑌 |𝑋 ) (e.g., GPT). Specifically, the attribute model 𝑝 (𝑎|𝑌 ) makes gradient based updates to ΔS 𝑡 as follows:</p><formula xml:id="formula_17">ΔS 𝑡 ← ΔS 𝑡 + ∇ ΔS 𝑡 log 𝑝 (𝑎|S 𝑡 + ΔS 𝑡 ) ||∇ ΔS 𝑡 log 𝑝 (𝑎|S 𝑡 + ΔS 𝑡 )|| 𝛾 , (<label>17</label></formula><formula xml:id="formula_18">)</formula><p>where 𝛾 is the scaling coefficient for the normalization term; ΔS 𝑡 is update of history matrix S 𝑡 (see Eq.( <ref type="formula" target="#formula_7">8</ref>)) and initialized as zero. The update step is repeated multiple times. Subsequently, a forward pass through the generation model is performed to obtain the updated S 𝑡 +1 as S 𝑡 +1 = Decoder((S 𝑡 +ΔS 𝑡 ), e(𝑦 𝑡 ), H). The perturbed S 𝑡 +1 is then used to generate a new logit vector. PPLMs is efficient and flexible to combine differentiable attribute models to steer text generation <ref type="bibr" target="#b102">[102]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NLG ENHANCED BY INTERNAL KNOWLEDGE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">NLG Enhanced by Topic</head><p>Topic, which can be considered as a representative or compressed form of text, has been often used to maintain the semantic coherence and guide the NLG process. Topic modeling is a powerful tool for finding the high-level content of a document collection in the form of latent topics <ref type="bibr" target="#b10">[10]</ref>. A classical topic model, Latent Dirichlet allocation (LDA), has been widely used for inferring a low dimensional representation that captures latent semantics of words and documents <ref type="bibr" target="#b10">[10]</ref>. In LDA, each topic is defined as a distribution over words and each document as a mixture distribution over topics. LDA generates words in the documents from topic distribution of document and word distribution of topic. Recent advances of neural techniques open a new way of learning low dimensional representations of words from the tasks of word prediction and context prediction, making neural topic models become a popular choice of finding latent topics from text <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b47">47]</ref>. Next, we introduce popular NLG applications enhanced by topics:</p><p>• Dialogue system. A vanilla Seq2Seq often generates trivial or non-committal sentences of frequent words or phrases in the corpus <ref type="bibr" target="#b139">[139]</ref>. For example, a chatbot may say "I do not know", "I see" too often. Though these off-topic responses are safe to reply to many queries, they are boring with very little information. Such responses may quickly lead the conversation to an end, severely hurting user experience. Thus, on-topic response generation is highly needed. • Machine translation. Though the input and output languages are different (e.g., translating English to Chinese), the contents are the same, and globally, under the same topic. Therefore, topic can serve as an auxiliary guidance to preserve the semantics information of input text in one language into the output text in the other language. • Paraphrase. Topic information helps understand the potential meaning and determine the semantic range to a certain extent. Naturally, paraphrases concern the same topic, which can serve as an auxiliary guidance to promote the preservation of source semantic. As shown in Figure <ref type="figure" target="#fig_2">4</ref>, we summarize topic-enhanced NLG methods into three methodologies: (M1) leverage topic words from generative topic models; (M2) jointly optimize generation model and CNN topic model; (M3) enhance NLG by neural topic models with variational inference. 3.1.1 M1: Leverage Topic Words from Generative Topic Models. Topics help understand the semantic meaning of sentences and determine the semantic spectrum to a certain extent. To enhanced text generation, an effective solution is to first discover topics using generative topic models (e.g., LDA), and then incorporate the topics representations into neural generation models, as illustrated in Figure <ref type="figure" target="#fig_2">4</ref>(a). In existing work, there are two mainstream methods to represent topics obtained from generative topic models. The first way is to use the generated topic distributions for each word (i.e., word distributions over topics) in the input sequence <ref type="bibr" target="#b94">[94,</ref><ref type="bibr" target="#b152">152]</ref>. The second way is to assign a specific topic to the input sequence, then picks the top-𝑘 words with the highest probabilities under the topic, and use word embeddings (e.g., GloVe) to represent topic words <ref type="bibr" target="#b79">[79,</ref><ref type="bibr" target="#b139">139]</ref>. Explicitly making use of topic words can bring stronger guidance than topic distributions, but the guidance may deviate from the target output sequence when some generated topic words are irrelevant. Zhang et al. proposed the first work of using a topic-informed Seq2Seq model by concatenating the topic distributions with encoder and decoder hidden states <ref type="bibr" target="#b152">[152]</ref>. Xing et al. designed a topic-aware Seq2Seq model in order to use topic words as prior knowledge to help dialogue generation <ref type="bibr" target="#b139">[139]</ref>. 3.1.2 M2: Jointly Optimize Generation Model and CNN Topic Model. The LDA models were separated from the training process of neural generation model and were not able to adapt to the diversity of dependencies between input and output sequences. Therefore, the idea of addressing this issue is to use neural topic models. Convolutional neural networks (CNN) were used to learn latent topic representations through iterative convolution and pooling operations. There are growing interests of using the CNNs to map latent topics implicitly into topic vectors that can be Table <ref type="table">2</ref>. Natural language generation methods that incorporate topic knowledge in text generation. Since most of the methods are tested on different tasks and datasets, we only compare the performance between "w/o topic" setting and "with topic" setting. For evaluation metrics, PPL is short for perplexity (lower is better); B-4 is short for BLEU-4 (higher is better); R-L is short for ROUGE-L (higher is better). used to enhance text generation tasks <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b134">134]</ref>. Empirical analyses showed that convolution-based topic extractors could outperform LDA-based topic models for multiple applications (e.g., dialogue system, text summarization, machine translation). However, theoretical analysis was missing to ensure the quality of the topics captured by the convolutions. And their interpretability is not as satisfactory as the LDA-based topic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence decoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output text</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixture distribution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LDA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input text</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">M3:</head><p>Enhance NLG by Neural Topic Models with Variational Inference. Neural topic models can be trained efficiently by backpropagation <ref type="bibr" target="#b12">[12]</ref>. In neural topic models, Dirichlet distributions can be employed as the prior to generate the parameters of the multinomial distribution 𝜃 𝑑 for each document <ref type="bibr" target="#b89">[89]</ref>. The generative process of LDA is represented as: (1) 𝜃 𝑑 ∼ Dirichlet(𝛼);</p><p>(2) 𝑡 𝑖 ∼ Multinomial(𝜃 𝑑 ); (3) 𝑤 𝑖 ∼ Multinomial(𝛽 𝑡 𝑖 ), where 𝑑 denotes the bag-of-words representation of a document, 𝑡 𝑖 represents the topic assignment for word 𝑤 𝑖 , and 𝛽 𝑡 𝑖 represents the topic distribution over words given topic assignment 𝑡 𝑖 . However, a directed generative model comes up against the problem of establishing low variance gradient estimators. Miao et al. parameterized the multinomial distributions with neural networks and jointly learned the model parameters via variational inference <ref type="bibr" target="#b89">[89]</ref>. They created neural structures for constructing topic distributions conditioned on a draw from a multivariate Gaussian distribution, represented as 𝜃 𝑑 ∼ G(𝜇 0 , 𝜎 2 0 ), where G(𝜇 0 , 𝜎 2 0 ) is composed of a neural network conditioned on an isotropic Gaussian N(𝜇 0 , 𝜎 2 0 ). Taking a Gaussian prior distribution makes re-parameterization feasible to build an unbiased and low-variance gradient estimator for the variational distribution <ref type="bibr" target="#b26">[26]</ref>. Without conjugacy prior, the updates of the parameters are derived directly and easily from the variational lower bound. Formally, a variational lower bound for the document log-likelihood is:</p><formula xml:id="formula_19">J 𝑡𝑜𝑝𝑖𝑐 = E 𝑞 (𝜃 |𝑑) [log 𝑝 (𝑑 |𝛽, 𝜃 )] − KL(𝑞(𝜃 |𝑑)||𝑝 (𝜃 |𝜇 0 , 𝜎 2 0 )),<label>(18)</label></formula><p>where 𝑞(𝜃 |𝑑) is the variational distribution approximating the true posterior 𝑝 (𝜃 |𝑑). Its lower bound is estimate by sampling 𝜃 from 𝑞(𝜃 |𝑑) = G(𝜃 |𝜇 (𝑑), 𝜎 2 (𝑑)).</p><p>In order to combine neural topic model and neural generation model, the idea is to use the Variational Auto-Encoder (VAE) <ref type="bibr" target="#b26">[26]</ref>. It adopts autoregressive networks (e.g., LSTM) both as the encoder and decoder. VAE can learn latent codes 𝑧 of texts by reconstructing texts with its decoder. It assumes that the generation process is controlled by codes in a continuous latent space. This kind of VAE implementation considers sequential information of texts that can model the linguistic structure of texts. Wang et al. proposed topic guided variational autoencoder (TGVAE), to draw latent code 𝑧 from a topic-dependent Gaussian Mixture Prior in order to incorporate the topical knowledge into latent variables <ref type="bibr" target="#b132">[132]</ref>. The topic-dependent Gaussian Mixture Model (GMM) is defined as: 𝑝 (𝑧|𝛽, 𝑡) = 𝑇 𝑖=1 𝑡 𝑖 N(𝜇 (𝛽 𝑖 ), 𝜎 2 (𝛽 𝑖 )), where 𝑇 is the number of topics, 𝜇 (𝑑) and 𝜎 2 (𝑑) are functions implemented by MLP. TGVAE uses bag-of-words as input and embeds an input document into a topic vector. The topic vector is then used to reconstruct the bag-of-words input, and the learned topic distribution over words is used to model a topic-dependent prior to generate an output sequence 𝑌 from conditioned on an input sequence 𝑋 . Therefore, to maximize the log-likelihood log 𝑝 (𝑌 , 𝑑 |𝑋 ), a variational objective function is constructed as:</p><formula xml:id="formula_20">J 𝑠𝑒𝑞2𝑠𝑒𝑞 = E 𝑞 (𝑧 |𝑋 ) [log 𝑝 (𝑌 |𝑋, 𝑧)] − E 𝑞 (𝜃 |𝑑) [KL(𝑞(𝑧|𝑋 )||𝑝 (𝑧|𝛽, 𝜃 ))],<label>(19)</label></formula><p>where 𝑞(𝑧|𝑋 ) is variational distributions for 𝑧. The combined object function is given by:</p><formula xml:id="formula_21">J = J 𝑡𝑜𝑝𝑖𝑐 + J 𝑠𝑒𝑞2𝑠𝑒𝑞 .<label>(20)</label></formula><p>3.1.4 Discussion and Analysis of Different Methods. For M1, topic models (e.g., LDA) has a strict probabilistic explanation since the semantic representations of both words and documents are combined into a unified framework. Besides, topic models can be easily used and integrated into generation frameworks. For example, topic words can be represented as word embeddings; topic embeddings can be integrated into the decoding phase through topic attention. However, LDA models are separated from the training process of generation, so they cannot adapt to the diversity of dependencies between input and output sequences. For M2, it is an end-to-end neural framework that simultaneously learns latent topic representations and generates output sequences. Convolutional neural networks (CNN) are often used to generate the latent topics through iterative convolution and pooling operations. However, theoretical analysis is missing to ensure the quality of the topics captured by the convolutions. And their interpretability is not as good as the LDA-based topic models.</p><p>For M3, neural topic models combine the advantages of neural networks and probabilistic topic models. They enable back propagation for joint optimization, contributing to more coherent topics, and can be scaled to large data sets. Generally, neural topic models can provide better topic coherence than LDAs <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b132">132,</ref><ref type="bibr" target="#b143">143]</ref>. However, neural variational approaches share a same drawback that topic distribution is assumed to be an isotropic Gaussian, which makes them incapable of modeling topic correlations. Existing neural topic models assume that the documents should be i.i.d. to adopt VAE, while they are commonly correlated. The correlations are critical for topic modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NLG Enhanced by Keywords</head><p>Keyword (aka., key phrase, key term) is often referred as a sequence of one or more words, providing a compact representation of the content of a document. The mainstream methods of keyword acquisition for documents can be divided into two categories <ref type="bibr" target="#b112">[112]</ref>: keyword assignment and keyword extraction. Keyword assignment means that keywords are chosen from a controlled vocabulary of terms or predefined taxonomy. Keyword extraction selects the most representative words explicitly presented in the document, which is independent from any vocabulary. Keyword extraction techniques (e.g., TF-IDF, TextRank, PMI) have been widely used over decades. Many NLG tasks can benefit from incorporating such a condensed form of essential content in a document to maintain the semantic coherence and guide the generation process.</p><p>Next, we introduce popular NLG applications enhanced by keywords:</p><p>• Dialogue system. Keywords help enlighten and drive the generated responses to be informative and avoid generating universally relevant replies which carry little semantics. Besides, recent work introduced personalized information into the generation of dialogue to help deliver better dialogue response such as emotion <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b114">114,</ref><ref type="bibr" target="#b159">158]</ref>, and persona <ref type="bibr" target="#b154">[154,</ref><ref type="bibr" target="#b158">157]</ref>. • Summarization. Vanilla Seq2Seq models often suffer when the generation process is hard to control and often misses salient information <ref type="bibr" target="#b69">[69]</ref>. Making use of keywords as explicit guidance can provide significant clues of the main points about the document <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b70">70]</ref>. It is closer to the way that humans write summaries: make sentences to contain the keywords, and then perform necessary modifications to ensure the fluency and grammatically correctness. • Question generation. It aims to generate questions from a given answer and its relevant context. Given an answer and its associated context, it is possible to raise multiple questions with different focuses on the context and various means of expression. Researchers have developed a great line of keyword-enhanced NLG methods. These methods can be categorized into two methodologies: (M1) Incorporate keyword assignment into text generation; (M2) Incorporate keyword extraction into text generation. 3.2.1 M1: Incorporate Keyword Assignment into Text Generation. When assigning a keyword to an input document, the set of possible keywords is bounded by a pre-defined vocabulary <ref type="bibr" target="#b112">[112]</ref>. The keyword assignment is typically implemented by a classifier that maps the input document to a word in the pre-defined vocabulary <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b71">71,</ref><ref type="bibr" target="#b114">114,</ref><ref type="bibr" target="#b159">158]</ref>. Unfortunately, some NLG scenarios do not hold an appropriate pre-defined vocabulary, so keyword assignment cannot be widely used to enhance NLG tasks. One applicable scenario is to use a pre-determined domain specific vocabulary to maintain relevance between the input and the output sequence <ref type="bibr" target="#b23">[23]</ref>. Another scenario is to generate dialogue with specific attributes such as persona <ref type="bibr" target="#b113">[113,</ref><ref type="bibr" target="#b143">143]</ref>, emotion <ref type="bibr" target="#b71">[71,</ref><ref type="bibr" target="#b114">114,</ref><ref type="bibr" target="#b159">158]</ref>.</p><p>M1.1: Adding assigned keyword into the decoder. A straightforward method of keyword assignment is to assign the words from pre-defined vocabulary and use them as the keywords <ref type="bibr" target="#b113">[113,</ref><ref type="bibr" target="#b143">143]</ref>. Sometimes, the input sequence does not have an explicit keyword, but we can find one from the pre-defined vocabulary. For example, a dialogue utterance "If you had stopped him that day, things would have been different. " expresses sadness but it does not have the word "sad. " To address this issue, Li et al. propose a method to predict an emotion category by fitting the sum of hidden states from encoder into a classifier <ref type="bibr" target="#b71">[71]</ref>. Then, the response will be generated with the guidance of the emotion category. In order to dynamically track how much the emotion is expressed in the generated sequence, Zhou et al. propose a memory module to capture the emotion dynamics during decoding <ref type="bibr" target="#b159">[158]</ref>. Each category is initialized with an emotion state vector before the decoding phase starts. At each step, the emotion state decays by a certain amount. Once the decoding process is completed, the emotion state decays to zero, indicating that the emotion is completely expressed. M1.2: Assigning keyword for generated sequence. As mentioned in <ref type="bibr" target="#b114">[114]</ref>, explicitly incorporating emotional keywords suffers from expressing a certain emotion overwhelmingly. Instead, Song et al. propose to increase the intensity of the emotional experiences not by using emotional words explicitly, but by implicitly combining neutral words in distinct ways on emotion <ref type="bibr" target="#b114">[114]</ref>. Specifically, they use an emotion classifier to build a sentence-level emotion discriminator, which helps to recognize the responses that express a certain emotion but not explicitly contain too many literal emotional words. The discriminator is connected to the end of the decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">M2:</head><p>Incorporate Keyword Extraction into Text Generation. Keyword extraction selects salient words from input documents <ref type="bibr" target="#b112">[112]</ref>. Recent work has used statistical keyword extraction techniques (e.g., PMI <ref type="bibr" target="#b73">[73]</ref>, TextRank <ref type="bibr" target="#b69">[69]</ref>), and neural-based keyword extraction techniques (e.g., BiLSTM <ref type="bibr" target="#b70">[70]</ref>). The process of incorporating extracted keywords into generation is much like the process discussed in Section 3.2.1. It takes keywords as an additional input into decoder. Recent work improves encoding phase by adding another sequence encoder to represent keywords <ref type="bibr" target="#b69">[69,</ref><ref type="bibr" target="#b70">70]</ref>. Then, the contextualized keywords representation is fed into the decoder together with input sequence representation. To advance the keyword extraction, Li et al. propose to use multi-task learning for training a keyword extractor network and generating summaries <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b70">70]</ref>. Because both summarization and keyword extraction aim to select important information from input document, these two tasks can benefit from sharing parameters to improve the capacity of capturing the gist of the input text. In practice, they take overlapping words between the input document and the ground-truth summary as keywords, and adopt a BiLSTM-Softmax as keyword extractor. Similar idea has also been used in question generation tasks <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b133">133]</ref>. They use overlapping words between the input answer context and the ground-truth question as keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Discussion and Analysis of Different Methods.</head><p>Pros and cons. For M1, the primary advantage of keyword assignment is that the quality of keywords is guaranteed, because irrelevant keywords are not included in the pre-defined vocabulary. Another advantage is that even if two semantically similar documents do not have common words, they can still be assigned with the same keyword. However, there are mainly two drawbacks. On one hand, it is expensive to create and maintain dictionaries in new domains. So, the dictionaries might not be available. On the other hand, potential keywords occurring in the document would be unfortunately ignored if they were not in the vocabulary. Therefore, keyword assignment is suitable for the task that requires specific categories of keywords to guide the generated sentences with these key information. For example, dialogue systems generate responses with specific attitudes.</p><p>For M2, keyword extraction selects the most representative words explicitly presented in the document, which is independent from any vocabulary. It is easy to use but has two drawbacks. First, it cannot guarantee consistency because similar documents may still be represented by different keywords if they do not share the same set of words. Second, when an input document does not have a proper representative word, and unfortunately, the keyword extractor selects an irrelevant word from the document as a keyword, this wrong guidance will mislead the generation. Therefore, keyword extraction is suitable for the task that the output sequence needs to keep important information in the input sequence such as document summarization and paraphrase.</p><p>Quantitative analysis. Table <ref type="table" target="#tab_2">3</ref> summarizes tasks and datasets used in keyword-enhanced NLG work. Comparing with keyword-enhanced methods (E-SCBA <ref type="bibr" target="#b71">[71]</ref>) and the basic Seq2Seq attention model, keyword-enhanced methods can greatly improve both generation quality (evaluated by BLEU) and emotional expression (evaluated by emotion-w and emotion-s) on the NLPCC dataset. Besides, as shown in Table <ref type="table" target="#tab_2">3</ref>(a), EmoDS <ref type="bibr" target="#b114">[114]</ref> achieved the best performance among three M1 methods, which indicates taking keyword assignment as a discriminant task can make better improvement than assigning keyword before the sentence decoding. For M2 methods, since most methods were evaluated on different tasks, we can only compare the performance between "without using keyword" and "using keyword". As shown in Table <ref type="table" target="#tab_2">3</ref>(b), leveraging extracted keywords from input sequence into Seq2Seq model can improve the generation quality on summarization and question generation tasks. Comparing with KGAS <ref type="bibr" target="#b70">[70]</ref> and KIGN <ref type="bibr" target="#b69">[69]</ref>, we can observe using BiLSTM-Softmax to extract keyword (a supervised manner by using overlapping words between 𝑋 and 𝑌 as labels) can make better performance than using TextRank (an unsupervised manner).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">NLG Enhanced by Linguistic Features</head><p>Feature enriched encoder means that the encoder not only reads the input sequence, but also incorporates auxiliary hand-crafted features <ref type="bibr" target="#b110">[110,</ref><ref type="bibr" target="#b149">149,</ref><ref type="bibr" target="#b161">160]</ref>. Linguistic features are the most common hand-crafted features, such as part-of-speech (POS) tags, dependency parsing, and semantic parsing. 3.3.1 POS tags and NER tags. Part-of-speech tagging (POS) assigns token tags to indicate the token's grammatical categories and part of speech such as noun (N), verb (V), adjective (A). Namedentity recognition (NER) classifies named entities mentioned in unstructured text into pre-defined categories such as person (P), location (L), organization (O). CoreNLP is the most common used tool <ref type="bibr" target="#b84">[84]</ref>. In spite of homonymy and word formation processes, the same surface word form may be shared between several word types. Incorporating NER tags and POS tags can detect named entities and understand input sequence better, hence, further improve NLG <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b93">93,</ref><ref type="bibr" target="#b161">160]</ref>. 3.3.2 Syntactic dependency graph. Syntactic dependency graph is a directed acyclic graph representing syntactic relations between words <ref type="bibr" target="#b3">[4]</ref>. For example, in the sentence "The monkey eats a banana", "monkey" is the subject of the predicate "eats", and "banana" is the object. Enhancing sequence representations by utilizing dependency information captures source long-distance dependency constraints and parent-child relation for different words <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">15]</ref>. In NLG tasks, dependency information is often modeled in three different ways as follows: (i) linearized representation: linearize dependency graph and then use sequence model to obtain syntax-aware representation <ref type="bibr" target="#b0">[1]</ref>; (ii) path-based representation: calculate attention weights based on the linear distance between a word and the aligned center position, i.e., the greater distance a word to the center position on the dependency graph is, the smaller contribution of the word to the context vector is <ref type="bibr" target="#b15">[15]</ref>; and (iii) graph-based representation: use GNNs to aggregate information from dependency relations <ref type="bibr" target="#b3">[4]</ref>. 3.3.3 Semantic dependency graph. Semantic dependency graph represents predicate-argument relations between content words in a sentence and have various semantic representation schemes (e.g., DM) based on different annotation systems. Nodes in a semantic dependency graph are extracted by semantic role labeling (SRL) or dependency parsing, and connected by different intra-semantic and inter-semantic relations <ref type="bibr" target="#b98">[98]</ref>. Since semantic dependency graph introduces a higher level of information abstraction that captures commonalities between different realizations of the same underlying predicate-argument structures, it has been widely used to improve text generation <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b75">75,</ref><ref type="bibr" target="#b98">98]</ref>. Jin et al. propose a semantic dependency guided summarization model <ref type="bibr" target="#b59">[59]</ref>. They incorporate the semantic dependency graph and the input text by stacking encoders to guide summary generation process. The stacked encoders consist of a sequence encoder and a graph encoder, in which the sentence encoder first reads the input text through stacked multi-head self-attention, and then the graph encoder captures semantic relationships and incorporates the semantic graph structure into the contextual-level representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">NLG Enhanced by Open Knowledge Graphs</head><p>For those KGs (e.g., ConceptNet) constructed based on data beyond the input text, we refer them as external KGs. On the contrary, an internal KG is defined as a KG constructed solely based on the input text. In this section, we will discuss incorporating internal KG to help NLG <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b54">54]</ref>.</p><p>Internal KG plays an important role in understanding the input sequence especially when it is of great length. By constructing an internal KG intermediary, redundant information can be merged or discarded, producing a substantially compressed form to represent the input document <ref type="bibr" target="#b30">[30]</ref>. Besides, representations on KGs can produce a structured summary and highlight the proximity of relevant concepts, when complex events related with the same entity may span multiple sentences <ref type="bibr" target="#b54">[54]</ref>. One of the mainstream methods of constructing an internal KG is using open information extraction (OpenIE). Unlike traditional information extraction (IE) methods, OpenIE is not limited to a small set of target entities and relations known in advance, but rather extracts all types of entities and relations found in input text <ref type="bibr" target="#b95">[95]</ref>. In this way, OpenIE facilitates the domain independent discovery of relations extracted from text and scales to large heterogeneous corpora.</p><p>After obtaining an internal KG, the next step is to learn the representation of the internal KG and integrate it into the generation model. For example, Zhu et al. use a graph attention network (GAT) to obtain the representation of each node, and fuse that into a transformer-based encoder-decoder architecture via attention <ref type="bibr" target="#b164">[163]</ref>. Their method generates abstractive summaries with higher factual correctness. Huang et al. extend by first encoding each paragraph as a sub-KG using GAT, and then connecting all sub-KGs with a Bi-LSTM <ref type="bibr" target="#b54">[54]</ref>. This process models topic transitions and recurrences, which enables the identification of notable content, thus benefiting summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NLG ENHANCED BY EXTERNAL KNOWLEDGE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NLG Enhanced by Knowledge Base</head><p>One of the biggest challenges in NLG is to discover the dependencies of elements within a sequence and/or across input and output sequences. The dependencies are actually various types of knowledge such as commonsense, factual events, and semantic relationship. Knowledge base (KB) is a popular technology that collects, stores, and manages large-scale information for knowledge-based systems like search engines. It has a great number of triples composed of subjects, predicates, and objects. People also call them "facts" or "factual triplets". Recently, researchers have been designing methods to use KB as external knowledge for learning the dependencies easier, faster, and better.</p><p>Next, we introduce popular NLG applications enhanced by knowledge base:</p><p>• Question answering. It is often difficult to generate proper answers only based on a given question. This is because, depending on what the question is looking for, a good answer may have different forms. It may completes the question precisely with the missing information.</p><p>It may elaborate details of some part of the question. It may need reasoning and inference based on some facts and/or commonsense. So, only incorporating input question into neural generation models often fails the task due to the lack of commonsense/factual knowledge <ref type="bibr" target="#b8">[8]</ref>. Related structured information of commonsense and facts can be retrieved from KBs. • Dialogue system. The needs of KB in generating conversations or dialogues are relevant with QA but differ from two aspects. First, a conversation or dialogue could be open discussions when started by an open topic like "Do you have any recommendations?" Second, responding an utterance in a certain step needs to recall previous contexts to determine involved entities. KB will play an important role to recognize dependencies in the long-range contexts.</p><p>To handle different kinds of relationships between KB and input/output sequences, these methods can be categorized into two methodologies which is shown in Figure <ref type="figure" target="#fig_3">5</ref>: (M1) design supervised tasks around KB for joint optimization; (M2) enhance incorporation by selecting KB or facts.</p><p>4.1.1 M1: Design Supervised Tasks around KB for Joint Optimization. Knowledge bases (KBs) that acquire, store, and represent factual knowledge can be used to enhance text generation. However, designing effective incorporation to achieve a desired enhancement is challenging because a vanilla Seq2Seq often fails to represent discrete isolated concepts though they perform well to Jet Li was born in Singapore. He is now a Singaporean citizen. learn smooth shared patterns (e.g., language diversity). To fully utilize the knowledge bases, the idea is to jointly train neural models on multiple tasks. For example, the target task is answer sequence generation, and additional tasks include question understanding and fact retrieval in the KB. Knowledge can be shared across a unified encoder-decoder framework design. Typically, question understanding and fact retrieval are relevant and useful tasks, because a question could be parsed to match (e.g., string matching, entity linking, named entity recognition) its subject and predicate with the components of a fact triple in KB, and the answer is the object of the triple. KBCopy was the first work to generate responses using factual knowledge bases <ref type="bibr" target="#b29">[29]</ref>. During the generation, KBCopy is able to copy words from the KBs. However, the directly copying relevant words from KBs is extremely challenging. CoreQA used both copying and retrieving mechanisms to generate answer sequences with an end-to-end fashion <ref type="bibr" target="#b48">[48]</ref>. Specifically, it had a retrieval module to understand the question and find related facts from the KB. Then, the question and all retrieved facts are transformed into latent representations by two separate encoders. During the decoding phase, the integrated representations are fed into the decoder by performing a joint attention on both input sequence and retrieved facts. Figure <ref type="figure" target="#fig_3">5</ref>(a) demonstrates a general pipeline that first retrieves relevant triples from KBs, then leverages the top-ranked triples into the generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-3 triples</head><p>4.1.2 M2: Enhance Incorporation by Selecting KB or Facts in KB. Ideally, the relevance of the facts is satisfactory with the input and output sequence dependencies, however, it is not always true in real cases. Lian et al. addressed the issue of selecting relevant facts from KBs based on retrieval models (e.g. semantic similarity) might not effectively achieve appropriate knowledge selection <ref type="bibr" target="#b74">[74]</ref>. The reason is that different kinds of selected knowledge facts can be used to generate diverse responses for the same input utterance. Given a specific utterance and response pair, the posterior distribution over knowledge base from both the utterance and the response may provide extra guidance on knowledge selection. The challenge lies in the discrepancy between the prior and posterior distributions. Specifically, the model learns to select effective knowledge only based on the prior distribution, so it is hard to obtain the correct posterior distribution during inference.</p><p>To tackle this issue, the work of Lian et al. <ref type="bibr" target="#b74">[74]</ref> and Wu et al. <ref type="bibr" target="#b137">[137]</ref> (shown in Figure <ref type="figure" target="#fig_3">5</ref>(b)) approximated the posterior distribution using the prior distribution in order to select appropriate knowledge even without posterior information. They introduced an auxiliary loss, called Kullback-Leibler divergence loss (KLDivLoss), to measure the proximity between the prior distribution and the posterior distribution. The KLDivLoss is defined as follows:  existing KG-enhanced methods still suffer from effectively selecting precise triples. Methods of M2 improve the selection of facts, in which the ground-truth responses used as the posterior context knowledge to supervise the training of the prior fact probability distribution. Wu et al. used exact match and recall to measure whether the retrieved triples is used to generate the target outputs <ref type="bibr" target="#b136">[136]</ref>. Table <ref type="table" target="#tab_4">4</ref> shows the entity recall scores of M1-based methods and M2-based methods reported in <ref type="bibr" target="#b136">[136,</ref><ref type="bibr" target="#b137">137]</ref>. We observe that compared to M1-based methods, M2-based methods can greatly improve the accuracy of triple retrieval, as well as the generation quality.</p><formula xml:id="formula_22">L KLDiv (𝜃 ) = 𝑁 ∑︁ 𝑖=1 𝑝 (𝑘 = 𝑘 𝑖 |𝑋, 𝑌 ) log 𝑝 (𝑘 = 𝑘 𝑖 |𝑋, 𝑌 ) 𝑝 (𝑘 = 𝑘 𝑖 |𝑋 ) ,<label>(21)</label></formula><p>There are still remaining challenges in KB-enhanced methods. One is that retrieved facts may contain noisy information, making the generation unstable <ref type="bibr">[61]</ref>. This problem is extremely harmful in NLG tasks, e.g., KB-based question answering and task-oriented dialogue system, since the information in KB is usually the expected entities in the response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">NLG Enhanced by Knowledge Graph</head><p>Knowledge graph (KG), as a type of structured human knowledge, has attracted great attention from both academia and industry. A KG is a structured representation of facts (a.k.a. knowledge triplets) consisting of entities * , relations, and semantic descriptions <ref type="bibr" target="#b58">[58]</ref>. The terms of "knowledge base" and "knowledge graph" can be interchangeably used, but they do not have to be synonymous. The knowledge graph is organized as a graph, so the connections between entities are first-class citizens in it. In the KG, people can easily traverse links to discover how entities are interconnected to express certain knowledge. Recent advances in artificial intelligence research have demonstrated the effectiveness of using KGs in various applications like recommendation systems <ref type="bibr" target="#b127">[127]</ref>.</p><p>Next, we introduce popular NLG applications that have been enhanced by knowledge graph:</p><p>• Commonsense reasoning. It aims to empower machines to capture the human commonsense from KG during generation. The methods exploit both structural and semantic information of the commonsense KG and perform reasoning over multi-hop relational paths, in order to augment the limited information with chains of evidence for commonsense reasoning. Popular tasks in commonsense reasoning generation include abductive reasoning (e.g., the 𝛼NLG task) <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b57">57]</ref>, counterfactual reasoning <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b57">57]</ref>, and entity description generation <ref type="bibr">[21]</ref>. • Dialogue system. It frequently makes use of KG for the semantics in linked entities and relations <ref type="bibr" target="#b97">[97,</ref><ref type="bibr" target="#b121">121,</ref><ref type="bibr" target="#b151">151,</ref><ref type="bibr" target="#b160">159]</ref>. A dialogue may shift focus from one entity to another, breaking one discourse into several segments, which can be represented as a linked path connecting the entities and their relations. • Creative writing. This task can be found in both scientific and story-telling domains. Scientific writing aims to explain natural processes and phenomena step by step, so each step can be reflected as a link on KG and the whole explanation is a path <ref type="bibr" target="#b63">[63,</ref><ref type="bibr" target="#b130">130]</ref>. In story generation, the implicit knowledge in KG can facilitate the understanding of storyline and better predict what will happen in the next plot <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr">80]</ref>. Compared with separate, independent knowledge triplets, knowledge graph provides comprehensive and rich entity features and relations for models to overcome the influence of the data distribution and enhance its robustness. Therefore, node embedding and relational path have played important roles in various text generation tasks. The corresponding techniques are knowledge graph embedding (KGE) <ref type="bibr" target="#b131">[131]</ref> and path-based knowledge graph reasoning <ref type="bibr" target="#b17">[17]</ref>. Furthermore, it has been possible to encode multi-hop and high-order relations in KGs using the emerging graph neural network (GNN) <ref type="bibr" target="#b138">[138]</ref> and graph-to-sequence (Graph2Seq) frameworks <ref type="bibr" target="#b6">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4.1 (Knowledge graph (KG))</head><p>. A knowledge graph (KG) is a directed and multi-relational graph composed of entities and relations which are regarded as nodes and different types of edges. Formally, a KG is defined as G = (U, E, R), where U is the set of entity nodes and E ⊆ U × R × U is the set of typed edges between nodes in U with a certain relation in the relation schema R.</p><p>Then given the input/output sequences in the text generation task, a subgraph of the KG which is associated with the sequences can be defined as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4.2 (Sequence-associated K-hop subgraph).</head><p>A sequence-associated K-hop subgraph is defined as G 𝑠𝑢𝑏 = (U 𝑠𝑢𝑏 , E 𝑠𝑢𝑏 , R), where U 𝑠𝑢𝑏 is the union of the set of entity nodes mapped through an entity linking function 𝜓 : U × X → U 𝑠𝑢𝑏 and their neighbors within K-hops. Similarly, E 𝑠𝑢𝑏 ⊆ U 𝑠𝑢𝑏 × R × U 𝑠𝑢𝑏 is the set of typed edges between nodes in U 𝑠𝑢𝑏 .</p><p>Sequence-associated subgraph provides a graphical form of the task data (i.e., sequences) and thus enables the integration of KGs and the sequences into graph algorithms.</p><p>Many methods have been proposed to learn the relationship between KG semantics and input/output sequences. They can be categorized into four methodologies as shown in Figure <ref type="figure">6</ref>: (M1) incorporate knowledge graph embeddings into language generation; (M2) transfer knowledge into language model with triplet information; (M3) perform reasoning over knowledge graph via path finding strategies; and (M4) improve the graph embeddings with graph neural networks. <ref type="bibr" target="#b131">[131]</ref>. KGE aims to capture the semantic relatedness between entity nodes from their connectivity information (i.e., different types of relations) in the KG. The primary idea is to represent entities and relations in a low-dimensional vector space R 𝑑 , where 𝑑 ≪ |U ∪ R|, to reduce data dimensionality while preserving the inherent structure of the KG. TransE <ref type="bibr" target="#b11">[11]</ref> is the most widely used KGE technique. In TransE, given a KG edge (𝑢 𝑖 , 𝑟, 𝑢 𝑗 ), the relation is seen as a translation vector r so that the embedded entities u 𝑖 and u 𝑗 can be connected with low translation error, namely u 𝑖 + r ≈ u 𝑗 . For example, we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">M1: Incorporate Knowledge Graph Embeddings into Language Generation. Knowledge graph embedding (KGE) techniques learn node embedding from a KG</head><formula xml:id="formula_23">− −−−− → 𝑇𝑜𝑘𝑦𝑜 + − −−−−−−−−−−− → 𝐼𝑠𝐶𝑎𝑝𝑡𝑖𝑐𝑎𝑙𝑂 𝑓 ≈ − −−− →</formula><p>𝐽𝑎𝑝𝑎𝑛 for the knowledge edge (Tokyo, IsCapticalOf, Japan). As shown in Figure <ref type="figure">6</ref>(a), a common strategy of incorporating KGE into NLG is to concatenate the original word representations (x) with the corresponding entity representations (u) from KGE <ref type="bibr" target="#b151">[151,</ref><ref type="bibr" target="#b160">159]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">M2: Transfer Knowledge into Language Model with Knowledge</head><p>Triplet Information. The vector spaces of entity embeddings (from KGE) and word embeddings (from pre-trained language models) are usually inconsistent <ref type="bibr">[80]</ref> have explored to fine-tune the language models directly on knowledge graph triplets. Guan et al. transformed the commonsense triplets (in ConceptNet and ATOMIC) into readable sentences using templates, as illustrated in Figure <ref type="figure">6(b)</ref>. And then the language model (e.g., GPT-2) is fine-tuned on the transformed sentences to learn the commonsense knowledge to improve text generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">M3: Perform</head><p>Reasoning over Knowledge Graph via Path Finding Strategies. KGE learns node representations from one-hop relations through a certain semantic relatedness (e.g. TransE). However, Xiong et al. argued that an intelligent machine is supposed to be able to conduct explicit reasoning over relational paths to make multiple inter-related decisions rather than merely embedding entities in the KGs <ref type="bibr" target="#b141">[141]</ref>. Take the QA task an example. The machine performs reasoning over KGs to handle complex queries that do not have an obvious answer, infer potential answerrelated entities, and generate the corresponding answer. So, the challenge lies in identifying a subset of desired entities and mentioning them properly in a response <ref type="bibr" target="#b91">[91]</ref>. Because the connected entities usually follow natural conceptual threads, they help generate reasonable and logical answers to keep conversations engaging and meaningful. As shown in Figure <ref type="figure">6</ref>(c), path-based methods explore various patterns of connections among entity nodes such as meta-paths and meta-graphs. Then they learn from walkable paths on KGs to provide auxiliary guidance for the generation process.</p><p>The path finding based methods can be mainly divided into two categories: (1) path ranking based methods and (2) reinforcement learning (RL) based path finding methods.</p><p>M3.1: Path routing and ranking. Path ranking algorithm (PRA) emerges as a promising method for learning and inferring paths on large KGs <ref type="bibr" target="#b65">[65]</ref>. PRA uses random walks to perform multiple bounded depth-first search processes to find relational paths. Coupled with elastic-net based learning <ref type="bibr" target="#b167">[166]</ref>, PRA picks plausible paths and prunes non-ideal, albeit factually correct KG paths. For example, Tuan et al. proposed a neural conversation model with PRA on dynamic knowledge graphs <ref type="bibr" target="#b121">[121]</ref>. In the decoding phase, it selected an output from two networks, a general GRU decoder network and a PRA based multi-hop reasoning network, at each time step. Bauer et al. ranked and filtered paths to ensure both the information quality and variety via a 3-step scoring strategy: initial node scoring, cumulative node scoring, and path selection <ref type="bibr" target="#b4">[5]</ref>. Ji et al. heuristically pruned the noisy edges between entity nodes and proposed a path routing algorithm to propagate the edge probability along multi-hop paths to the entity nodes <ref type="bibr" target="#b56">[56]</ref>.</p><p>M3.2: Reinforcement learning based path finding. Reinforcement learning (RL) based methods make an agent to perform reasoning to find a path in a continuous space. These methods incorporate various criteria in their reward functions of path finding, making the path finding process flexible. Xiong et al. proposed DeepPath, the first work that employed Markov decision process (MDP) and used RL based approaches to find paths in KGs <ref type="bibr" target="#b141">[141]</ref>. Leveraging RL based path finding for NLG tasks typically consists of two stages <ref type="bibr" target="#b81">[81,</ref><ref type="bibr" target="#b97">97]</ref>. First, they take a sequence as input, retrieve a starting node 𝑢 0 on G, then perform multi-hop graph reasoning, and finally arrive at a target node 𝑢 𝑘 that incorporates the knowledge for output sequence generation. Second, they represent the sequence 𝑋 and selected path Φ 𝑘 (𝑢 0 , 𝑢 𝑘 ) through two separate encoders. They decode a sequence with multi-source attentions on the input sequence and selected path. Path-based knowledge graph reasoning converts the graph structure of a KG into a linear path structure that can be easily represented by sequence encoders (e.g, RNN) <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b97">97,</ref><ref type="bibr" target="#b121">121]</ref>. For example, Niu et al. encoded selected path and input sequence with two separate RNNs and generated sequence with a general attention-based RNN decoder <ref type="bibr" target="#b97">[97]</ref>. To enhance the RL process, Xu et al. proposed six reward functions for training an agent in the reinforcement learning process. For example, the functions looked for accurate arrival at the target node as well as the shortest path between the start and target node, i.e., minimize the length of the selected path Φ 𝑘 (𝑢 0 , 𝑢 𝑘 ) <ref type="bibr" target="#b142">[142]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">M4: Improve the Graph Embeddings with Graph Neural Networks.</head><p>The contexts surrounding relevant entities on KGs play an important role in understanding the entities and generating proper text about their interactions <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b63">63]</ref>. For example, in scientific writing, it is important to consider the neighboring nodes of relevant concepts on a taxonomy and/or the global context of a scientific knowledge graph <ref type="bibr" target="#b63">[63]</ref>. However, neither KGE nor relational path could fully represent such information. Graph-based representations aim at aggregating the context/neighboring information on graph data; and recent advances of GNN models demonstrate a promising advancement in graph-based representation learning <ref type="bibr" target="#b138">[138]</ref>. In order to improve text generation, graph-to-sequence (Graph2Seq) models encode the structural information of the KG in a neural encoder-decoder architecture <ref type="bibr" target="#b6">[6]</ref>. Since then, GNNs have been playing an important role in improving the NLG models. They have been applied to both encoding and decoding phases.</p><p>Learning KG-aware input text representation with GNNs (Encoding). For encoding phase, a general process of leveraging GNNs for incorporating KG is to augment semantics of a word in the input text by combining with the vector of the corresponding entity node vector to the word on the KG <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b150">150,</ref><ref type="bibr" target="#b151">151,</ref><ref type="bibr" target="#b160">159]</ref>. A pre-defined entity linking function 𝜓 : U × X → U 𝑠𝑢𝑏 maps words in the input sequence to entity nodes on the KG. Given an input sequence, all the linked entities and their neighbors within 𝐾-hops compose a sequence-associated K-hop subgraph G 𝑠𝑢𝑏 (formally defined in Definition 4.2). For each entity node in G 𝑠𝑢𝑏 , it uses the KG structure as well as entity and edge features (e.g., semantic description if available) to learn a representation vector u. Specifically, a GNN model follows a neighborhood aggregation approach that iteratively updates the representation of a node by aggregating information from its neighboring nodes and edges. After 𝑘 iterations of aggregation, the node representation captures the structural information within its 𝑘-hop neighborhood. Formally, the 𝑘-th layer of a node 𝑢 ∈ U 𝑠𝑢𝑏 is: Table <ref type="table">5</ref>. Tasks, datasets and KG sources used in different KG-enhanced papers. We also compared the performance of different models before and after incorporating KG into the generation process, in which "w/o KG" performance comes from the best baseline method; "with KG" comes from the KG-enhanced method. </p><formula xml:id="formula_24">u (𝑘) = Combine 𝑘 (u (𝑘−1) , Aggregate</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks</head><formula xml:id="formula_25">✓ × × × CCM [159] ✓ ×, one-hop × × KEPM [45] ✓ × × × AKGCM [81] ✓ × ✓, Markov decision ✓, Path selection IE+MSA [46] ✓ ✓, by GNN × × ConceptFlow [151] ✓ ✓, by GNN × × CE-PR [56] ✓ × ✓, Path routing ✓, Concept selection GRF [57] ✓ ✓, by GNN ✓, Path scoring ✓, Link prediction</formula><p>The sub-graph representation h 𝑠𝑢𝑏𝐺 is learned thorough a Readout(•) function from all entity node representations (i.e., h 𝑠𝑢𝑏𝐺 = Readout( u (𝑘) , 𝑢 ∈ U 𝑠𝑢𝑏 ). Zhou et al. was the first to design such a knowledge graph interpreter to enrich the context representations with neighbouring concepts on ConceptNet using graph attention network (GAT) <ref type="bibr" target="#b160">[159]</ref>.</p><p>Dynamically attending KG representation (Decoding). The sequence decoder uses attention mechanism to find useful semantics from the representation of KG as well as the hidden state of the input text, where the KG's representation is usually generated by GNNs. Specially, the hidden state is augmented by subgraph representation h 𝑠𝑢𝑏𝐺 , i.e., s 0 = h 𝑛 ⊕ h 𝑠𝑢𝑏𝐺 <ref type="bibr" target="#b6">[6]</ref>. Then, the decoder attentively reads the retrieved subgraph to obtain a graph-aware context vector. Then it uses the vector to update the decoding state <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr">80,</ref><ref type="bibr" target="#b151">151,</ref><ref type="bibr" target="#b160">159]</ref>. It adaptively chooses a generic word or an entity from the retrieved subgraph to generate output words. Because graph-level attention alone might overlook fine-grained knowledge edge information, some recent methods adopted the hierarchical graph attention mechanism <ref type="bibr" target="#b46">[46,</ref><ref type="bibr">80,</ref><ref type="bibr" target="#b160">159]</ref>. It attentively read the retrieved subgraph G 𝑠𝑢𝑏 and then attentively read all knowledge edges E 𝑠𝑢𝑏 involved in G 𝑠𝑢𝑏 . Ji et al. added a relevance score that reflected the relevancy of the knowledge edge according to the decoding state <ref type="bibr" target="#b57">[57]</ref>  Pros and cons. Knowledge graph embedding (M1) was the earliest attempt to embed components of a KG including entities and relations into continuous vector spaces and use them to improve text generation. Those entity and relation embeddings can simply be used to enrich input text representations (e.g., concatenating embeddings), bridging connections between entity words linked from input text in latent space. Because the graph projection and text generation are performed as two separate steps, the embedding vectors from knowledge graph and the hidden states from input text were in two different vector spaces. The model would have to learn to bridge the gap, which might make a negative impact on the performance of text generation.</p><p>Fine tuning pre-trained language models on the KG triplets (M2) can eliminate the gap between the two vector spaces. Nevertheless, M1 and M2 share two drawbacks. First, they only preserve information of direct (one-hop) relations in a KG, such as pair-wise proximity in M1 and KG triplet in M2, but ignore the indirect (multi-hop) relations of concepts. The indirect relations may provide plausible evidence of complex reasoning for some text generation tasks. Second, from the time KGs were encoded in M1 or M2 methods, the generation models would no longer be able to access the KGs but their continuous representations. Then the models could not support reasoning like commonsense KG reasoning for downstream tasks. Due to these two reasons, M1 and M2 were often used to create basic KG representations upon which the KG path reasoning (M3) and GNNs (M4) could further enrich the hidden states <ref type="bibr" target="#b151">[151,</ref><ref type="bibr" target="#b160">159]</ref>.</p><p>The path finding methods of KG reasoning (M3) perform multi-hop walks on the KGs beyond one-hop relations. It enables reasoning that is needed in many text generation scenarios such as commonsense reasoning and conversational question answering. At the same time, it provides better interpretability for the entire generation process, because the path selected by the KG reasoning algorithm will be explicitly used for generation. However, the selected paths might not be able to capture the full contexts of the reasoning process due to the limit of number. Besides, reinforcement-learning based path finding uses heuristic rewards to drive the policy search, making the model sensitive to noises and adversarial examples.</p><p>The algorithms of GNN and Graph2Seq (M4) can effectively aggregate semantic and structural information from multi-hop neighborhoods on KGs, compared to M3 that considers multi-hop paths. Therefore, the wide range of relevant information can be directly embedded into the encoder/decoder hidden states. Meanwhile, M4 enables back propagation for jointly optimizing text encoder and graph encoder. Furthermore, the attention mechanism that has been applied in GNN and Graph2Seq (e.g., graph attention) can explain the model's output at some extent, though the multi-hop paths from M3 has better interpretability.</p><p>M3 and M4 are able to use multi-hop relational information, compared to M1 and M2. However, they have two weak points. First, they have higher complexity than M1 and M2. In M3, the action space of path finding algorithms can be very large due to the large size and sparsity of the knowledge graph. In M4, the decoder has to attentively read both input sequence and knowledge graph. Second, the subgraphs retrieved by M3 and M4 might provide low coverage of useful concepts for generating the output. For example, people use ConceptNet, a widely used commonsense KG, to retrieve the subgraph on three generative commonsense reasoning tasks. The task datasets are ComVE <ref type="bibr" target="#b57">[57]</ref>, 𝛼-NLG <ref type="bibr" target="#b7">[7]</ref>, and ROCSories <ref type="bibr" target="#b46">[46]</ref>. We found 25.1% / 24.2% / 21.1% of concepts in the output could be found on ConceptNet, but only 11.4% / 8.1% / 5.7% of concepts in the output can be found on the retrieved 2-hop sequence-associated subgraph, respectively. It means that a large portion of relevant concepts on the KG are not utilized in the generation process.</p><p>Quantitative analysis. Table <ref type="table">5</ref> summarizes tasks, datasets, and KG sources used in existing KG-enhanced works. Three important things should be mentioned. First, all the datasets in the table are public, and we include their links in Table <ref type="table" target="#tab_13">11</ref>. CommonGen <ref type="bibr" target="#b76">[76]</ref>, ComVE <ref type="bibr" target="#b124">[124]</ref> and 𝛼-NLG <ref type="bibr" target="#b7">[7]</ref> 1:25 B: I like country music. It is the most listened to rush hour radio genre. (2) Country is a musical genre that originated in the southern US in the early 1920s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">relevant docs</head><p>(3) George Glenn Jones was an American musician, singer and songwriter.</p><p>A dialogue between A and B A: Do you know George Glenn Jones? B: Yes, he was a famous American singer and songwriter. A: Cool! You sure know some stuff about country music!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieve from Wikipedia</head><p>Background: ... but if you like ben stiller, go see "meet the fockers". Dustin's antics will favorite character was jack (the older one), because he was so serious but always plotting and putting up a front. I think it was $279,167,575 awards ASCAP film and television music awards 2005 top box office films MTV… (~250 words)</p><p>A dialogue between A and B A: That name is so ridiculous but funny. B: First off, the writers did not miss a single opportunity to play off of the name "focker". A: Yeah, I heard it was a pretty successful movie overall.</p><p>A background-based conversion (BBC) have a public leaderboard for competition. Second, for KG sources, we observe that eight (57.1%) papers use ConceptNet as external resource, while six (42.9%) papers constructed their own KGs from domain-specific corpus. For example, Koncel et al. created a scientific knowledge graph by applying the SciIE tool (science domain information extraction) <ref type="bibr" target="#b63">[63]</ref>. Besides, Zhao et al. compared the performance of models between using ConceptNet and using a self-built KG, and found the model with self-built KG could work better on story generation and review generation tasks <ref type="bibr" target="#b157">[156]</ref>. Third, we observed that KG-enhanced NLG methods made the largest improvement on generative commonsense reasoning tasks, in which the average improvement is +2.55% in terms of ΔBLEU, while the average improvement on all different tasks is +1.32%.</p><p>Qualitative analysis. Table <ref type="table" target="#tab_6">6</ref> compares different KG-enhanced methods from three dimensions: multi-hop information aggregation, multi-hop path reasoning, and auxiliary knowledge graph related tasks. M3 is commonly used for multi-hop path reasoning and M4 is used for multi-hop information aggregation, except that CCM <ref type="bibr" target="#b160">[159]</ref> only aggregates one-hop neighbors. Besides, the auxiliary KG-related tasks are often used to further help the model learn knowledge from the KG. For example, ablation studies in <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b81">81]</ref> show that the tasks of path selection, concept selection and link prediction can further boost the generation performance. GRF <ref type="bibr" target="#b57">[57]</ref> learns these three abilities at the same time. It achieves the state-of-art performance on three generation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NLG enhanced by Grounded Text</head><p>Knowledge grounded text refers to textual information that can provide additional knowledge relevant to the input sequence. The textual information may not be found in training corpora or structured databases, but can be obtained from massive textual data from online resources. These online resources include encyclopedia (e.g., Wikipedia), social media (e.g., Twitter), shopping websites (e.g., Amazon reviews). Knowledge grounded text plays an important role in understanding the input sequence and its surrounding contexts. For example, Wikipedia articles may offer textual explanations or background information for the input text. Amazon reviews may contain necessary descriptions and reviews needed to answer a product-related question. Tweets may contain people's comments and summaries towards an event. Therefore, knowledge grounded text is often taken as an important external knowledge source to help with a variety of NLG applications.</p><p>Next, we introduce popular NLG applications enhanced by knowledge grounded text:</p><p>• Dialogue system. Building a fully data-driven dialogue system is difficult since most of the universal knowledge is not presented in the training corpora <ref type="bibr" target="#b42">[42]</ref>. The lack of universal knowledge considerably limits the appeal of fully data-driven generation methods, as they are bounded to respond evasively or defectively and seldom include meaningfully factual contents. To infuse the response with factual information, an intelligent machine is expected to obtain necessary background information to produce appropriate response. • Summarization. Seq2Seq models that purely depend on the input text tend to "lose control" sometimes. For example, 3% of summaries contain less than three words, and 4% of summaries repeat a word for more than 99 times as mentioned in <ref type="bibr" target="#b13">[13]</ref>. Furthermore, Seq2Seq models usually focus on copying source words in their exact order, which is often sub-optimal in abstractive summarization. Therefore, leveraging summaries of documents similar as the input document as templates can provide reference for the summarization process <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b129">129]</ref>. • Question answering (QA). It is often difficult to generate proper answers only based on the given question. For example, without knowing any information of an Amazon product, it is hard to deliver satisfactory answer to the user questions such as "Does the laptop have a long battery life?" or "Is this refrigerator frost-free?" So, the product description and customer reviews can be used as a reference for answering product-related questions <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b16">16]</ref>.</p><p>To handle different kinds of relationships between grounded text and input/output sequences, these methods can be categorized into two methodologies as shown in Figure <ref type="figure" target="#fig_8">7</ref>: (M1) guiding generation with retrieved information; (M2) modeling background knowledge into response generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">M1:</head><p>Guiding Generation with Retrieved Information. Because knowledge grounded text is not presented in the training corpora, an idea is to retrieve relevant textual information (e.g., a review, a relevant document, a summary template) from external sources based on the input text and to incorporate the retrieved grounded text into the generation process. This process is similar to designing knowledge acquisition and incorporation of KBs and KGs in text generation tasks. The difference is that ground text is unstructured and noisy. So, researchers design knowledge selection and incorporation methods to address the challenges. Based on the number of stages, we further divide related methods into two categories: retrieve-then-generate (also known as retrieval-augmented generation, short as RAG, in many existing papers <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b68">68,</ref><ref type="bibr">99]</ref>) methods (2-stage methods) and retrieve, rerank and rewrite methods (3-stage methods).</p><p>M1.1: Retrieval-augmented generation (RAG). RAG follows a two-stage process: retrieval and generation. Specially, as shown in Figure <ref type="figure" target="#fig_8">7</ref>(a), a retriever 𝑝 (𝑍 |𝑋 ) first returns (usually top-K truncated) distributions over text passages given a query 𝑋 , and then a generator 𝑝 (𝑦 𝑖 |𝑋, 𝑍, 𝑦 1:𝑖−1 ) generates a current token based on a context of the previous tokens 𝑦 1:𝑖−1 , the original input 𝑋 and a retrieved passage 𝑍 . Methods for retrieving fact or review snippets are various, including matching from a collection of raw text entries indexed by named entities <ref type="bibr" target="#b42">[42]</ref>; scoring relevant documents within a large collection by statistical approaches such as BM25 <ref type="bibr" target="#b27">[27]</ref>, or neural-based retrieval approaches such as dense paragraph retrieval (DPR) <ref type="bibr" target="#b68">[68]</ref>. For training the retriever and generator, most of existing work has jointly optimized these two components, without any direct supervision on what document should be retrieve <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b68">68]</ref>. However, by asking human experts to label what document should be retrieved and adding the retrieval loss (resulting in a multi-task learning setting), the generation performance can be greatly improved <ref type="bibr" target="#b27">[27,</ref><ref type="bibr">61]</ref>, though the labelling process is an extremely time-consuming and labor-intensive task.</p><p>Ghazvininejad et al. proposed a knowledge grounded neural conversation model (KGNCM), which is the first work to retrieve review snippets from Foursquare and Twitter. Then it incorporates the snippets into dialogue response generation <ref type="bibr" target="#b42">[42]</ref>. It uses an end-to-end memory network <ref type="bibr" target="#b116">[116]</ref> to generate responses based on the selected review snippets. Lewis et al. introduced a general retrieval-augmented generation (RAG) framework by leveraging a pre-trained neural retriever and generator. It can be easily fine-tuned on downstream tasks, and it has demonstrated state-of-theart performance on various knowledge intensive NLG tasks <ref type="bibr" target="#b68">[68]</ref>. Recently, the fusion-in-decoder Table <ref type="table">7</ref>. Tasks, datasets and evidence sources used in retrieve-then-generate (M1) papers. We also include their document(d)/sentence(s) retrieval space and the number of retrieved document(d)/sentence(s). methods (i.e., the decoder performs attention over the concatenation of the resulting representations of all retrieved passages <ref type="bibr" target="#b72">[72,</ref><ref type="bibr" target="#b145">145]</ref>) could even outperform RAG as reported in KILT benchmark <ref type="bibr">[99]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evidence</head><p>M1.2: Retrieve, rerank and rewrite (𝑅 3 ). Different from RAG, a 𝑅 3 -based method is expected to retrieve a most precise reference document that can be directly used for rewriting/editing. 𝑅 3based method has proved successful in a number of NLG tasks such as machine translation <ref type="bibr" target="#b44">[44]</ref>, and summarization <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b129">129]</ref>. In summarization, Seq2Seq models that purely depend on the input document to generate summaries tend to deteriorate with the accumulation of word generation, e.g., they generate irrelevant and repeated words frequently <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b129">129]</ref>. Template-based summarization assume the golden summaries of the similar sentences (i.e., templates) can provide a reference point to guide the input sentence summarization process <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b129">129]</ref>. These templates are often called soft templates in order to distinguish from the traditional rule-based templates. Soft template-based summarization typically follows a three-step design: retrieve, rerank, and rewrite. The step of retrieval aims to return a few candidate templates from a summary collection. The reranking identifies the best template from the retrieved candidates. And the rewriting leverages both the source document and template to generate more faithful and informative summaries.</p><p>Difference between RAG and 𝑅 3 . Compared with 𝑅 3 -based methods, RAG-based have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">M2: Modeling Background Knowledge into Response Generation</head><p>Background document, with more global and comprehensive knowledge, has been often used for generating informative responses and ensuring a conversation to not deviate from its topic. Keeping a conversation grounded on a background document is referred as background based conversation (BBC) <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b90">90]</ref>. Background knowledge plays an important role in human-human conversations. For example, when talking about a movie, people often recall important points (e.g., a scene or review about the movie) and appropriately mention them in the conversation context. Therefore, an intelligent NLG model is expected to find an appropriate background snippet and generate response based on the snippet. As shown in Figure <ref type="figure" target="#fig_8">7</ref>(b), the task of BBC is often compared with machine reading comprehension (MRC), in which a span is extracted from the background document as a response to a question <ref type="bibr" target="#b105">[105]</ref>. However, since BBC needs to generate natural and fluent responses, the challenge lies in not only locating the right semantic units in the background, but also referring to the right background information at the right time in the right place during the decoding phase.</p><p>As MRC models tie together multiple text segments to provide a unified and factual answer, many BBC models use the same idea to connect different pieces of information and find the appropriate background knowledge based on which the next response is to be generated <ref type="bibr">[87,</ref><ref type="bibr">101]</ref>. For instance, Qin et al. proposed an end-to-end conversation model that jointly learned response generation together with on-demand machine reading <ref type="bibr">[101]</ref>. The MRC models can effectively encode the input utterance by treating it as a question in a typical QA task (e.g., SQuAD <ref type="bibr" target="#b105">[105]</ref>) and encode the background document as the context. Then, they took the utterance-aware background representation as input into decoding phase. Pros and cons. For M1, guiding generation with retrieved information explicitly exposes the role of world knowledge by asking the model to decide what knowledge to retrieve and use during language generation. Since retrieval-augmented generation (RAG) captures knowledge in a interpretable and modular way, it is often used for knowledge-intensive tasks such as long-form QA and argument generation. However, a knowledge retriever is expected to retrieve documents from a large-scale corpus, e.g., the entire Wikipedia, which causes significant computational challenge. Besides, one input often requires retrieved text whose amount is much larger than the input itself (as indicated in Table <ref type="table">7</ref>), leading to serious information overwhelming for the generation model.</p><p>For M2, background based conversations (BBCs) avoid generating generic responses in a dialogue system and are able to generate more informative responses by exploring related background information. However, existing methods still cannot solve inherent problems effectively, such as tending to break a complete semantic unit and generate shorter responses <ref type="bibr">[87]</ref>.</p><p>Qualitative analysis. Table <ref type="table">7</ref> summarizes tasks, datasets and evidence sources used in existing grounded text enhanced work. Three important things should be mentioned. First, all the datasets in the table are public, and we include their links in Table <ref type="table" target="#tab_13">11</ref>. Second, Wikipedia is the most commonly used evidence source since it is the largest free online encyclopedia. Besides, some online platforms contain plenty of product-related textural information, e.g., product reviews on Amazon, which are often used to build up task/goal oriented dialogue systems for business purpose. Third, the retrieval space of candidate documents are usually larger than 1 million and only 7-10 documents are selected. So, the process of retrieving relevant documents is challenging.</p><p>Table <ref type="table" target="#tab_7">8</ref> compares different grounded text enhanced methods from three dimensions: retrieval supervision, pre-training of the retriever, and number of stages. First, as mentioned above, retrieving relevant documents from a large candidate set is a challenging task. To improve the retrieval accuracy, four (57.1%) papers added the retrieval supervision either by human annotated labels or pseudo labels, resulting in a multi-task learning setting. Besides, three (42.9%) papers used pretrained language models to produce document representation for better retrieval. Though existing work has greatly improved the retrieval accuracy, the performance is still far from satisfactory in many text generation tasks <ref type="bibr" target="#b64">[64,</ref><ref type="bibr" target="#b68">68]</ref>. How to learn mutually enhancement between retrieval and generation is still a promising direction in the grounded text enhanced text generation systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">BENCHMARK, TOOLKIT AND LEADERBOARD PERFORMANCE</head><p>The development of general evaluation benchmarks for text generation helps to promote the development of research in related fields. Existing text generation benchmarks did not specially focus on choosing the tasks and datasets that have been widely used for knowledge-enhanced text generation. Therefore, we re-screened from the existing four text generation benchmarks, i.e., GLGE <ref type="bibr" target="#b77">[77]</ref>, GEM <ref type="bibr" target="#b40">[40]</ref>, KilT <ref type="bibr">[99]</ref>, GENIE <ref type="bibr" target="#b60">[60]</ref>, and determined 9 benchmark datasets for evaluating knowledge-enhanced NLG methods. Here is our criteria for selection:</p><p>• We only consider benchmark datasets that have open-access downloading link.</p><p>• We focus on diverse text generation tasks, involving various applications.</p><p>• We select at most three benchmark datasets for each text generation task.</p><p>• We include a mix of internal and external knowledge focused datasets.</p><p>• We prefer multi-reference datasets for robust automatic evaluation.</p><p>Based on the benchmark selection criteria, we finalize 9 knowledge-centric tasks that covers various NLG tasks, including commonsense reasoning, text summarization, question generation, generative question answering, and dialogue. The data statistics is shown in Table <ref type="table" target="#tab_9">9</ref>. Descriptions and dataset links are listed as follows:  . We choose 9 knowledge-enhanced NLG benchmark datasets. These datasets have been included in four existing general NLG benchmarks (i.e., GLGE <ref type="bibr" target="#b77">[77]</ref>, GEM <ref type="bibr" target="#b40">[40]</ref>, Kilt <ref type="bibr">[99]</ref>, GENIE <ref type="bibr" target="#b60">[60]</ref>) or in SemEval tasks. Designing more effective ways to represent knowledge and integrate them into the generation process is still the most important trend in knowledge-enhanced NLG systems. From a broader perspective, we provide three directions that make focusing such efforts worthwhile now: (i) incorporating knowledge into visual-language generation tasks, (ii) learning knowledge from broader sources, especially pre-trained language models, (iii) learning knowledge from limited resources, (iv) learning knowledge in a continuous way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Incorporate Knowledge into Visual-Language Generation Tasks</head><p>Beyond text-to-text generation tasks, recent years have witnessed a growing interest in visuallanguage (VL) generation tasks, such as describing visual scenes <ref type="bibr" target="#b49">[49]</ref>, and answering visual-related questions <ref type="bibr" target="#b85">[85]</ref>. Although success has been achieved in recent years on VL generation tasks, there is still room for improvement due to the fact that image-based factual descriptions are often not enough to generate high-quality captions or answers <ref type="bibr" target="#b163">[162]</ref>. External knowledge can be added in order to generate attractive image/video captions. We observed some pioneer work has attempted to utilize external knowledge to enhance the image/video captioning tasks. For example, Tran et al. proposed to detect a diverse set of visual concepts and generate captions by using an external knowledge base (i.e., Freebase), in recognizing a broad range of entities such as celebrities and landmarks <ref type="bibr" target="#b120">[120]</ref>. Zhou et al. used a commonsense knowledge graph (i.e., ConceptNet), to infer a set of terms directly or indirectly related to the words that describe the objects found in the scene by the object recognition module <ref type="bibr" target="#b163">[162]</ref>. In addition, Mao et al. <ref type="bibr" target="#b85">[85]</ref> proposed a neuro-symbolic learner for improving visual-language generation tasks (e.g., visual question answering). However, existing approaches for knowledge-enhanced visual-language generation tasks still have a lot of space for exploration. Some promising directions for future work include using other knowledge sources, such as retrieving image/text to help solve open-domain visual question answering and image/video captioning tasks; bringing structured knowledge for providing justifications for the captions that they produce, tailoring captions to different audiences and contexts, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Learning Knowledge from Broader Sources</head><p>More research efforts should be spent on learning to discover knowledge more broadly and combine multiple forms of knowledge from different sources to improve the generation process. More knowledge sources can be but not limited to network structure, dictionary and table. For examples, Yu et al. <ref type="bibr" target="#b147">[147]</ref> and An et al. <ref type="bibr">[2]</ref> augmented the task of scientific papers intention detection and summarization by introducing the citation graph; Yu et al. augmented the rare word representations by retrieving their descriptions from Wiktionary and feed them as additional input to a pretrained language model <ref type="bibr" target="#b148">[148]</ref>. Besides, structured knowledge and unstructured knowledge can play a complementary role in enhancing text generation. To improve knowledge richness, Fu et al. combined both structured (knowledge base) and unstructured knowledge (grounded text) <ref type="bibr" target="#b34">[34]</ref>.</p><p>Leveraging Knowledge from Pre-trained Language Models. Pre-trained language models can learn a substantial amount of in-depth knowledge from data without any access to an external memory, as a parameterized implicit knowledge base <ref type="bibr" target="#b68">[68,</ref><ref type="bibr" target="#b104">104]</ref>. However, as mentioned in <ref type="bibr" target="#b45">[45]</ref>, directly fine-tuning pre-trained language generation models on the story generation task still suffers from insufficient knowledge by representing the input text thorough a pre-trained encoder, leading to repetition, logic conflicts, and lack of long-range coherence in the generated output sequence. Therefore, discovering knowledge from pre-trained language models can be more flexible, such as knowledge distillation, data augmentation, and using pre-trained models as external knowledge <ref type="bibr" target="#b100">[100]</ref>. More efficient methods of obtaining knowledge from pre-trained language models are expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Learning Knowledge from Limited Resources</head><p>Most of current NLG research conduct on extensively labelled data to favor model training. However, this is in contrast to many real-world application scenarios, where only a few shots of examples are available for new domains. Limited data resources lead to limited knowledge that can be learnt in new domains. For examples, learning topical information of a dialogue occurring under a new domain is difficult since the topic may be rarely discussed before; constructing a syntactic dependency graph of a sequence in a low-resource language is hard since many linguistic features are of great uniqueness. Besides, external knowledge bases are often incomplete and insufficient to cover full entities and relationships due to the human costs of collecting domain-specific knowledge triples. Therefore, quick domain adaptation is an essential task in text generation tasks. One potential route towards addressing these issues is meta-learning, which in the context of NLG means a generation model develops a broad set of skills and pattern recognition abilities at training time, and quickly adapt to a new task given very few examples without retraining the model from scratch. Recently, there has been raising interests in both academia and industry to investigate meta-learning in different NLG tasks. Thus, it is a promising research direction to build efficient meta-learning algorithms that only need a few task-specific fine-tuning to learn the new task quickly. And for knowledge-enhanced text generation, it is of crucial importance to adapt the model quickly on new domains with limited new knowledge (e.g., only a few knowledge triples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Learning Knowledge in a Continuous Way</head><p>A machine learning is expected to learn continuously, accumulate the knowledge learned in previous tasks, and use it to assist future learning. This research direction is referred as lifelong learning <ref type="bibr" target="#b20">[20]</ref>. In the process, the intelligent machine becomes more and more knowledgeable and effective at learning new knowledge. To make an analogy, humans continuously acquire new knowledge and constantly update the knowledge system in the brain. However, existing knowledge-enhanced text generation systems usually do not keep updating knowledge in real time (e.g., knowledge graph expansion). A meaningful exploration of was discussed in <ref type="bibr" target="#b86">[86]</ref>. They built a general knowledge learning engine for chatbots to enable them to continuously and interactively learn new knowledge during conversations. Therefore, it is a promising research direction to continuously update knowledge obtained from various information sources, empowering intelligent machines with incoming knowledge and improving the performance on new text generation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this survey, we present a comprehensive review of current representative research efforts and trends on knowledge-enhanced text generation, and expect it can facilitate future research. To summarize, this survey aims to answer two questions that commonly appears in knowledgeenhanced text generation: how to acquire knowledge and how to incorporate knowledge to facilitate text generation. Base on knowledge acquisition, the main content of our survey is divided into three sections according to different sources of knowledge enhancement. Based on knowledge incorporation, we first present general methods of incorporating knowledge into text generation and further discuss a number of specific ideas and technical solutions that incorporate the knowledge to enhance the text generation systems in each section. Besides, we review a variety of text generation applications in each section to help practitioners learn to choose and employ the methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>Figure <ref type="figure" target="#fig_11">8</ref> demonstrates the statistics of selected publications in this survey. The left figure shows the paper publishing venues. Most papers were published in top machine learning, artificial intelligence, and natural language processing conferences, such as ACL, EMNLP, AAAI, ICLR, NeurIPS. Besides, many selected papers were published in high-impact journals, such as TNNLS, JMLR, TACL. The right figure shows the paper categories. Among 160 selected papers, 87 papers ("general methods (General)", "topic", "keyword", "knowledge base (KG)", "knowledge graph (KG)", "grounded text (Text)") are directly relevant to the different kinds of knowledge-enhanced text generation methods; 10 papers are relevant to benchmark datasets; 10 papers are related survey papers. Besides, other 43 papers are about basic (pre-trained) generation methods (e.g., Seq2Seq, CopyNet, BART, T5), or necessary background (e.g., TransE, OpenIE, GNN, LDA), or future direction.     ROUGE-𝑚 (short as R-𝑚): ROUGE measures the overlap of n-grams between the reference and hypothesis; ROUGE-L measures the longest matched words using longest common sub-sequence.</p><p>Distinct-𝑘 (short as D-k): Distinct measures the total number of unique 𝑘-grams normalized by the total number of generated 𝑘-gram tokens to avoid favoring long sentences.</p><p>(f) Leaderboard performance on Wizard of Wikipedia with seen (S) and unseen (UnS) test set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022. A Survey of Knowledge-Enhanced Text Generation 1:5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Incorporating knowledge into text generation by treating knowledge as the target. The first category of methods (left) combine knowledge-related tasks as auxiliary into the text generation task, resulting in a multi-task learning setting. The second category of methods (right) create weakly-supervised labels from knowledge, enforcing the relevancy between the knowledge and the target sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Three typical methodologies for incorporating topics into NLG. Detailed designs are not included.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The left figure demonstrates retrieving relevant triples, then using them for generation; the right figure demonstrate using KL to measure the proximity between prior and posterior distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>𝑘 ( (u (𝑘−1) 𝑖 , e (𝑘−1) 𝑖 𝑗 , u (𝑘−1) 𝑗 ) : ∀(𝑢 𝑖 , 𝑒 𝑖 𝑗 , 𝑢 𝑗 ) ∈ N (𝑢) )). (22) ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4. 2 . 5</head><label>25</label><figDesc>Discussion and Analysis of the Methodologies and Methods. ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( 1 )</head><label>1</label><figDesc>M1: Retrieve relevant documents, use them for generation Input text B: It made $279,167,575 at the box office. M2: Read background document and generate output Input text In 2009, country music was the most listened to rush hour radio genre in the US.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The left figure demonstrates retrieving relevant documents, then using them for generation; the right figure demonstrate reading background document to conduct conversions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>4. 4 . 1</head><label>41</label><figDesc>Discussion and Analysis of Different Methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9</head><label>9</label><figDesc>Figure 9 summarized different papers according to years, knowledge sources, and methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Paper statistics of selected publications in this survey.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Knowledge-enhanced text generation has been gaining emerging interests in the recent five years.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>NLG methods that incorporates knowledge attention ( §2.2.1) and knowledge mode ( §2.2.2).</figDesc><table><row><cell></cell><cell>Topic</cell><cell cols="4">Keyword Knowledge base Knowledge graph Grounded text</cell></row><row><cell cols="3">Knowledge-related attention [134, 139, 152] [69, 70, 73]</cell><cell>[34, 48]</cell><cell>[46, 54, 151, 159]</cell><cell>[9, 87]</cell></row><row><cell>Knowledge-related mode</cell><cell>[139]</cell><cell>[70]</cell><cell>[48]</cell><cell>[57, 151, 159]</cell><cell>[87, 107]</cell></row><row><cell>Knowledge-related memory</cell><cell>[34, 158]</cell><cell>-</cell><cell>[82, 135]</cell><cell>[144]</cell><cell>[61]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Natural language generation methods that incorporate keyword in text generation.(a) (M1) Descriptions and quantitative comparisons between three methods for emotional dialogue systems. As most methods are tested on different tasks and datasets, we only compare the performance between "w/o keyword" setting and "with keyword" setting. Besides, HM is short for human evaluation.</figDesc><table><row><cell>Task</cell><cell>Method</cell><cell cols="2">Ref. Assignment method</cell><cell></cell><cell></cell><cell cols="2">Experiments on NLPCC dataset BLEU D-1/D-2 Emotion w/s</cell></row><row><cell></cell><cell>Seq2Seq</cell><cell>[3]</cell><cell cols="3">Seq2Seq attention without using keywords</cell><cell>1.50 0.38/1.20</cell><cell>33.5/37.1</cell></row><row><cell>Dialogue</cell><cell>E-SCBA</cell><cell cols="4">[71] MLP classifier to 7 emotions (categories)</cell><cell>1.69 0.54/4.84</cell><cell>72.0/51.2</cell></row><row><cell>system</cell><cell cols="6">EmoChat [158] E-SCBA + two memory modules for decoding 1.68 0.90/7.35</cell><cell>76.5/58.0</cell></row><row><cell></cell><cell>EmoDS</cell><cell cols="4">[114] MLP classifier after decoding (discriminator)</cell><cell>1.73 1.13/8.67</cell><cell>81.0/68.7</cell></row><row><cell>(b) (M2) Task</cell><cell>Method</cell><cell>Ref.</cell><cell>Extraction method</cell><cell>Keyword labels</cell><cell>Dataset</cell><cell cols="2">Effect of keyword w/o keyword with keyword</cell></row><row><cell>Summari-</cell><cell>KIGN</cell><cell cols="2">[69] TextRank</cell><cell>Unsupervised</cell><cell cols="2">CNN/DM (R-2) 15.66 Gigaword (R-2) 23.61</cell><cell>(R-2) 17.12 (R-2) 23.93</cell></row><row><cell>zation</cell><cell cols="3">ComGen [73] PMI and TFIDF</cell><cell cols="2">Unsupervised Tencent</cell><cell>(HM) 5.77</cell><cell>(HM) 7.19</cell></row><row><cell></cell><cell>KGAS</cell><cell cols="5">[70] BiLSTM-Softmax w(𝑋 ) ∩ w(𝑌 ) Gigaword (R-2) 23.61</cell><cell>(R-2) 25.06</cell></row><row><cell>Question</cell><cell>Selector</cell><cell cols="4">[22] BiLSTM-Softmax w(𝑋 ) ∩ w(𝑌 ) SQuAD</cell><cell>(B-4) 14.72</cell><cell>(B-4) 15.87</cell></row><row><cell>generation</cell><cell>Prior</cell><cell cols="4">[133] BiLSTM-Softmax w(𝑋 ) ∩ w(𝑌 ) SQuAD</cell><cell>(B-4) 14.72</cell><cell>(B-4) 15.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>M2-based methods can retrieve more precise triples, and further improve the generation performance. 𝑁 is the number of retrieved facts. When minimizing KLDivLoss, the posterior distribution 𝑝 (𝑘 |𝑋, 𝑌 ) can be regarded as labels to apply the prior distribution 𝑝 (𝑘 |𝑋 ) for approximating 𝑝 (𝑘 |𝑋, 𝑌 ). Finally, the total loss is written as the sum of the KLDivLoss and NLL (generation) loss. 4.1.3 Discussion and Analysis of Different Methods. The relevance between triples in KBs and input sequences plays a central role in discovering knowledge for sequence generation. Methods in M1 typically follows the process that parses input sequence, retrieves relevant facts, and subsequently, a knowledge-aware output can be generated based on the input sequence and previously retrieved facts. Even though the improvement by modeling KB with memory network<ref type="bibr" target="#b82">[82]</ref>,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Chinese Weibo (large) [137]</cell><cell cols="4">Chinese Weibo (small) [136]</cell></row><row><cell>Method</cell><cell>Cat.</cell><cell>Ref.</cell><cell cols="2">Entity score</cell><cell cols="2">Generation score</cell><cell cols="2">Entity score</cell><cell cols="2">Generation score</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Match Recall BLEU-2 Dist-2</cell><cell cols="4">Match Recall BLEU-2 Dist-2</cell></row><row><cell>GenDS</cell><cell>M1</cell><cell>[165]</cell><cell>0.97</cell><cell>0.37</cell><cell>3.42</cell><cell>4.27</cell><cell>0.75</cell><cell>0.26</cell><cell>2.09</cell><cell>1.66</cell></row><row><cell>CCM</cell><cell>M1</cell><cell>[159]</cell><cell>1.09</cell><cell>0.37</cell><cell>4.75</cell><cell>4.87</cell><cell>0.99</cell><cell>0.28</cell><cell>3.26</cell><cell>2.59</cell></row><row><cell cols="2">ConKADI M2</cell><cell>[136]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.48</cell><cell>0.38</cell><cell>5.06</cell><cell>23.93</cell></row><row><cell>TaFact</cell><cell>M2</cell><cell>[137]</cell><cell>1.81</cell><cell>0.47</cell><cell>5.07</cell><cell>23.56</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>where</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>. Beyond a simple concatenation, recent methods</figDesc><table><row><cell cols="6">A Survey of Knowledge-Enhanced Text Generation</cell><cell></cell><cell></cell><cell></cell><cell>1:21</cell></row><row><cell cols="4">Pretrained through TransE</cell><cell cols="2">Language generation model</cell><cell></cell><cell></cell><cell cols="2">KG-enhanced language model</cell></row><row><cell>id</cell><cell>entity</cell><cell cols="2">vector</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">training with all KG triples</cell></row><row><cell>3 1 2</cell><cell>future chat talk</cell><cell cols="2">[0.6, 0.1, 0.1] [-0.1, 0.2, 0.5] [0.3, 0.4, -0.2]</cell><cell cols="2">+ + + + + + +</cell><cell>word entity emb.</cell><cell></cell><cell>Eiffel tower</cell><cell>Eiffel tower is</cell><cell>at Paris &lt;/s&gt;</cell></row><row><cell>4</cell><cell>based</cell><cell cols="2">[-0.4, 0.4, 0.2]</cell><cell cols="2">Chat based on knowledge is the future</cell><cell>emb.</cell><cell>Paris</cell><cell>AtLocation</cell><cell>Pretrained language model</cell></row><row><cell cols="4">KG embedding</cell><cell cols="2">Input text</cell><cell></cell><cell></cell><cell></cell><cell>&lt;s&gt; Eiffel tower is</cell><cell>at Paris</cell></row><row><cell cols="6">(M1) Incorporate KGE into language generation</cell><cell></cell><cell cols="3">(M2) Transfer knowledge into pretrained LM</cell></row><row><cell cols="2">Output text</cell><cell></cell><cell cols="3">Yeah, it 's not a dream to have a talk with robot!</cell><cell></cell><cell cols="2">Output text</cell><cell>Yeah, it 's not a dream to have a talk with robot!</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Sequence Decoder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sequence Decoder</cell></row><row><cell cols="4">Sequence encoder</cell><cell cols="2">Path encoder</cell><cell></cell><cell cols="3">Sequence encoder</cell><cell>GNN encoder</cell></row><row><cell></cell><cell cols="2">Input text</cell><cell></cell><cell cols="2">Input paths</cell><cell></cell><cell></cell><cell cols="2">Input text</cell><cell>Input subgraph</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dream</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Path 1</cell><cell>Path 2</cell><cell></cell><cell></cell><cell></cell><cell>future</cell></row><row><cell cols="4">Chat based on knowledge is the future</cell><cell>future</cell><cell></cell><cell></cell><cell cols="3">Chat based on knowledge is the future</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dream</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>locate</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>locate</cell></row><row><cell>concepts source</cell><cell cols="2">KG</cell><cell>subgraph</cell><cell>chat</cell><cell>talk</cell><cell></cell><cell>source concepts</cell><cell>KG</cell><cell>subgraph</cell><cell>chat : GNN aggregation talk</cell></row><row><cell></cell><cell cols="5">(M3) Performing path reasoning on KG</cell><cell></cell><cell></cell><cell cols="2">(M4) Aggregating sub-KG via GNN</cell></row></table><note>ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.Fig. 6. Four typical methodologies for incorporating KG semantics into text generation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Qualitative comparison between different KG-enhanced methods.</figDesc><table><row><cell></cell><cell>Methods</cell><cell cols="2">Ref. Cat.</cell><cell cols="4">Dataset Information Name #Instance w/o KG with KG ΔBLEU Effect of KG</cell><cell>KG source</cell></row><row><cell>Common-sense reasoning</cell><cell>KG-BART CE-PR GRF MGCN</cell><cell cols="3">[80] M4 CommonGen [56] M3 ComVE [57] M4 𝛼NLG-ART [21] M3 EntDesc</cell><cell>77,449 30,000 60,709 110,814</cell><cell>28.60 15.70 9.62 24.90</cell><cell>30.90 +2.30 17.10 +1.60 11.62 +2.00 30.00 +4.30 Self-built KG ConceptNet ConceptNet ConceptNet</cell></row><row><cell>Story generation</cell><cell>IE+MSA GRF KEPM</cell><cell cols="3">[46] M4 ROCStories [57] M4 (split-1) [45] M2 ROCStories (split-2)</cell><cell>98,162 98,162</cell><cell>8.25 10.40 14.10</cell><cell>9.36 +1.11 11.00 +0.60 14.30 +0.20</cell><cell>ConceptNet ConceptNet ConceptNet &amp; ATOMIC</cell></row><row><cell></cell><cell>MRG</cell><cell cols="3">[156] M3 VisualStory</cell><cell>50,000</cell><cell>3.18</cell><cell>3.23 +0.05</cell><cell>ConceptNet</cell></row><row><cell>Scientific</cell><cell cols="4">GraphWriter [63] M4 AGENDA</cell><cell>40,000</cell><cell>12.20</cell><cell>14.30 +1.90 Self-built KG</cell></row><row><cell>writing</cell><cell>PaperRobot</cell><cell cols="3">[130] M4 PaperWriting</cell><cell>27,001</cell><cell>9.20</cell><cell>13.00 +3.80 Self-built KG</cell></row><row><cell>Dialogue system</cell><cell cols="4">ConceptFlow [151] M4 Reddit-10M AKGCM [81] M3 EMNLP dialog AKGCM [81] M3 ICLR dialog</cell><cell>3,384K 43,192 21,569</cell><cell>1.62 32.45 6.74</cell><cell>2.46 +0.84 30.84 -1.61 6.94 +0.20 Self-built KG ConceptNet Self-built KG</cell></row><row><cell>Question answering</cell><cell>MHPGM</cell><cell>[5]</cell><cell cols="2">M3 NarrativeQA</cell><cell>46,765</cell><cell>19.79</cell><cell>21.07 +1.28 Self-built KG</cell></row><row><cell>Methods</cell><cell>Ref.</cell><cell cols="5">Method category M1 M2 M3 M4 aggregation Multi-hop info. Multi-hop path reasoning</cell><cell>Auxiliary (knowledge related) task(s)</cell></row><row><cell>THOTH</cell><cell>[92]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Qualitative comparison between different grounded text enhanced methods.</figDesc><table><row><cell>sources</cell><cell>Tasks</cell><cell></cell><cell>Methods</cell><cell>Ref.</cell><cell cols="3">Dataset Information Name #Instance space (d/s) eved d/s Retrieval # Retri-</cell></row><row><cell></cell><cell cols="2">Dialogue system</cell><cell>MemNet SKT</cell><cell cols="2">[27] Wizard of Wikipedia (WoW) [61]</cell><cell cols="2">22,311 5.4M/93M</cell><cell>7 7</cell></row><row><cell>Wikipedia</cell><cell cols="2">Question answering</cell><cell cols="3">RAG BART+DPR RT+C-REALM [64] [68] MS-MARCO [99] ELI5</cell><cell>267,287 274,741</cell><cell>21M/-3.2M/-3.2M/-</cell><cell>10 -7</cell></row><row><cell></cell><cell cols="2">Argument generation</cell><cell>H&amp;W CANDELA</cell><cell cols="2">[53] ChangeMyView [52]</cell><cell>287,152</cell><cell>5M/-5M/-</cell><cell>10 10</cell></row><row><cell>Online platform</cell><cell cols="2">Dialogue (for</cell><cell>AT2T</cell><cell cols="2">[62] Amazon books</cell><cell>937,032</cell><cell>-/131K</cell><cell>10</cell></row><row><cell>(e.g., Amazon)</cell><cell cols="2">business)</cell><cell>KGNCM</cell><cell cols="2">[42] Foursquare</cell><cell>1M</cell><cell>-/1.1M</cell><cell>10</cell></row><row><cell>Gigawords</cell><cell cols="2">Summari-zation</cell><cell>R 3 Sum BiSET</cell><cell cols="2">[13] Gigawords [129]</cell><cell>3.8M</cell><cell>-/3.8M -/3.8M</cell><cell>30 30</cell></row><row><cell>Methods</cell><cell>Ref.</cell><cell cols="4">Method category Retrieval supervision M1.1 M1.2 M2</cell><cell cols="2">Retriever pre-training of stages Number</cell></row><row><cell>MemNet</cell><cell>[27]</cell><cell>✓</cell><cell></cell><cell cols="3">✓, Human annotated labels ×</cell><cell>2</cell></row><row><cell>SKT</cell><cell>[61]</cell><cell>✓</cell><cell></cell><cell cols="3">✓, Human annotated labels ×</cell><cell>2</cell></row><row><cell>R 3 Sum</cell><cell>[13]</cell><cell></cell><cell>✓</cell><cell cols="2">✓, Pseudo labels</cell><cell>×</cell><cell>3, with rerank</cell></row><row><cell>BiSET</cell><cell>[129]</cell><cell></cell><cell>✓</cell><cell cols="2">✓, Pseudo labels</cell><cell>×</cell><cell>3, with rerank</cell></row><row><cell>RefNet</cell><cell>[87]</cell><cell></cell><cell cols="2">✓ ×</cell><cell></cell><cell>×</cell><cell>1, no retrieval</cell></row><row><cell>GLKS</cell><cell>[107]</cell><cell></cell><cell cols="2">✓ ×</cell><cell></cell><cell>×</cell><cell>1, no retrieval</cell></row><row><cell>RAG</cell><cell>[68]</cell><cell>✓</cell><cell></cell><cell>×</cell><cell></cell><cell>✓, DPR</cell><cell>2</cell></row><row><cell>Kilt</cell><cell>[99]</cell><cell>✓</cell><cell></cell><cell>×</cell><cell></cell><cell>✓, DPR</cell><cell>2</cell></row><row><cell cols="2">RT+C-REALM [64]</cell><cell>✓</cell><cell></cell><cell>×</cell><cell></cell><cell cols="2">✓, REALM 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>•</head><label></label><figDesc>Wizard of Wikipedia (WOW): It is an open-domain dialogue dataset, where two speakers conduct an open-ended conversion that is directly grounded with knowledge retrieved from Wikipedia. (Data link: https://parl.ai/projects/wizard_of_wikipedia/) • CommonGen: It is a generative commonsense reasoning dataset. Given a set of common concepts, the task is to generate a coherent sentence describing an everyday scenario using these concepts. (Data link: https://inklab.usc.edu/CommonGen/) • 𝛼NLG-ART: It is a generative commonsense reasoning dataset. Given the incomplete observations about the world, the task it to generate a valid hypothesis about the likely explanations to partially observable past and future. (Data link: http://abductivecommonsense.xyz/) • ComVE: It is a generative commonsense reasoning dataset. The task is to generate an explanation given a counterfactual statement for sense-making. (Data link: https://github. com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation • ELI5: It is a dataset for long-form question answering. The task is to produce explanatory multi-sentence answers for diverse questions. Web search results are used as evidence documents to answer questions. (Data link: https://facebookresearch.github.io/ELI5/) • SQuAD: It is a dataset for answer-aware question generation. The task is to generate a question asks towards the given answer span based on a given text passage or document. (Data link: https://github.com/magic282/NQG) • CNN/DailyMail (CNN/DM): It is a dataset for summarization. Given a news aticles, the goal is to produce a summary that represents the most important or relevant information within the original content.</figDesc><table /><note>(Data link: https://www.tensorflow.org/datasets/catalog/cnn_dailymail) • Gigaword: It is a dataset for summarization. Similar with CNN/DM, the goal is to generate a headline for a news article. (Data link: https://www.tensorflow.org/datasets/catalog/gigaword)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10</head><label>10</label><figDesc>lists the leaderboard performance on ten knowledge-enhanced generation benchmarks. Table 11 lists code links and programming language of representative open-source knowledgeenhanced text generation systems that have been introduced in this survey.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>(g) State-of-the-art performance on SQuAD. Leaderboard performance on ELI5 dataset. The Kilt R-L (KRL) is the primary evaluation metric. Some state-of-the-art performance on Per-sonaChat dataset (no leaderboard on this dataset).</figDesc><table><row><cell cols="4">A Survey of Knowledge-Enhanced Text Generation</cell><cell></cell><cell></cell><cell></cell><cell>1:43</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell cols="2">Ref. Source</cell><cell>B-4</cell></row><row><cell></cell><cell cols="3">Ref. R-1/R-2 (S) R-1/R-2 (UnS)</cell><cell></cell><cell cols="2">Baseline methods</cell></row><row><cell></cell><cell cols="2">Baseline methods</cell><cell></cell><cell>Seq2Seq</cell><cell>[117] -</cell><cell></cell><cell>3.01</cell></row><row><cell>Transformer</cell><cell>[122]</cell><cell>17.8/ -</cell><cell>14.0/ -</cell><cell>Transformer</cell><cell>[122] -</cell><cell></cell><cell>3.09</cell></row><row><cell cols="4">Knowledge-enhanced methods</cell><cell cols="3">Knowledge-enhanced methods</cell></row><row><cell>MemNet</cell><cell>[27]</cell><cell>16.9/ -</cell><cell>14.4/ -</cell><cell>NQG++</cell><cell cols="2">[160] LF</cell><cell>13.27</cell></row><row><cell>PostKS</cell><cell>[74]</cell><cell>18.1/5.3</cell><cell>13.5/2.0</cell><cell>SELECTOR</cell><cell cols="2">[22] LF+Keyword</cell><cell>15.87</cell></row><row><cell>SKT</cell><cell>[61]</cell><cell>19.3/6.8</cell><cell>16.1/4.2</cell><cell>G2S+BERT</cell><cell cols="3">[19] LF+DepGraph 17.49</cell></row><row><cell cols="2">PIPM+KDBTS [18]</cell><cell>19.9/7.3</cell><cell>17.6/5.4</cell><cell cols="4">G2S+BERT+RL [19] LF+DepGraph 18.30</cell></row><row><cell>(h) Method</cell><cell cols="3">Ref. Source KRL R-L</cell><cell>(i) Method</cell><cell>Ref.</cell><cell cols="2">B-1/B-2 D-1/D-2</cell></row><row><cell></cell><cell cols="2">Baseline methods</cell><cell></cell><cell></cell><cell cols="2">Baseline methods</cell></row><row><cell>T5</cell><cell cols="2">[104] -</cell><cell>0.0 19.1</cell><cell>Seq2Seq</cell><cell cols="2">[117] 18.2/9.3</cell><cell>2.6/7.4</cell></row><row><cell>BART</cell><cell cols="2">[67] -</cell><cell>0.0 20.1</cell><cell cols="4">Knowledge-enhanced methods</cell></row><row><cell cols="4">Knowledge-enhanced methods</cell><cell>MemNet(soft)</cell><cell>[27]</cell><cell>17.7/9.1</cell><cell>3.5/9.6</cell></row><row><cell>RAG</cell><cell cols="2">[68] Text</cell><cell>1.7 17.4</cell><cell cols="2">MemNet(hard) [27]</cell><cell>18.6/9.7</cell><cell>3.7/9.9</cell></row><row><cell>BART+DPR</cell><cell cols="2">[99] Text</cell><cell>1.9 17.4</cell><cell>PostKS</cell><cell>[74]</cell><cell cols="2">19.0/9.8 4.6/13.4</cell></row><row><cell cols="3">RT+c-REALM [61] Text</cell><cell>2.4 23.2</cell><cell>PEE</cell><cell cols="2">[143] 23.2/11.5</cell><cell>-/ -</cell></row></table><note>ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 .</head><label>11</label><figDesc>A list of representative open-source knowledge-enhanced text generation systems.</figDesc><table><row><cell>Task</cell><cell cols="2">Ref. Paper title and open source code/toolkit</cell><cell cols="2">Programming Venue language &amp; Year</cell></row><row><cell></cell><cell></cell><cell>Topic-enhanced methods</cell><cell></cell><cell></cell></row><row><cell>Summarization</cell><cell>[94] [135]</cell><cell>Topic-Aware Convolutional Neural Networks for Extreme Summarization --Code: https://github.com/EdinburghNLP/XSum Friendly Topic Assistant for Transformer Based Abstractive Summarization --Code: https://github.com/BoChenGroup/TA</cell><cell>PyTorch -</cell><cell>EMNLP 2018 EMNLP 2020</cell></row><row><cell></cell><cell></cell><cell>Keyword-enhanced methods</cell><cell></cell><cell></cell></row><row><cell>Dialogue</cell><cell>[94]</cell><cell>A Content-Introducing Approach to Generative Short-Text Conversation --Code: https://github.com/MaZhiyuanBUAA/Seq2BFforDialogueGeneration</cell><cell>Tensorflow</cell><cell>COLING 2016</cell></row><row><cell>system</cell><cell>[135]</cell><cell>Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory --Code: https://github.com/loadder/ECM-tf</cell><cell>PyTorch</cell><cell>AAAI 2018</cell></row><row><cell cols="2">Summarization [94]</cell><cell>Coherent Comment Generation with a Graph-to-Sequence Model --Code: https://github.com/lancopku/Graph-to-seq-comment-generation</cell><cell>PyTorch</cell><cell>ACL 2018</cell></row><row><cell></cell><cell></cell><cell>KB-enhanced methods</cell><cell></cell><cell></cell></row><row><cell></cell><cell>[82]</cell><cell>Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Dialog Systems --Code: https://github.com/HLTCHKUST/Mem2Seq</cell><cell>PyTorch</cell><cell>ACL 2019</cell></row><row><cell></cell><cell>[135]</cell><cell>Global-to-local Memory Pointer Networks for Task-Oriented Dialogue --Code: https://github.com/jasonwu0731/GLMP</cell><cell>PyTorch</cell><cell>ICLR 2019</cell></row><row><cell>Dialogue system</cell><cell>[128]</cell><cell>Improving Knowledge-aware Dialogue Generation via Knowledge Base Question Answering --Code: https://github.com/siat-nlp/TransDG</cell><cell>PyTorch</cell><cell>AAAI 2020</cell></row><row><cell></cell><cell>[136]</cell><cell>Diverse and Informative Dialogue Generation with Context-Specific Knowledge Awareness --Code: https://github.com/pku-sixing/ACL2020-ConKADI</cell><cell>Tensorflow</cell><cell>ACL 2020</cell></row><row><cell></cell><cell>[137]</cell><cell>TopicKA: Generating Commonsense Knowledge-Aware Dialogue Responses --Code: https://github.com/pku-sixing/IJCAI2020-TopicKA</cell><cell>Tensorflow</cell><cell>IJCAI 2020</cell></row><row><cell></cell><cell></cell><cell>KG-enhanced methods</cell><cell></cell><cell></cell></row><row><cell></cell><cell>[159]</cell><cell>Commonsense Knowledge Aware Conversation Generation with Graph Attention --Code: https://github.com/thu-coai/ccm</cell><cell>Tensorflow</cell><cell>IJCAI 2018</cell></row><row><cell>Dialogue system</cell><cell>[121]</cell><cell>DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs --Code: https://github.com/Pascalson/DyKGChat</cell><cell>Tensorflow</cell><cell>EMNLP 2019</cell></row><row><cell></cell><cell>[151]</cell><cell>Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs --Code: https://github.com/thunlp/ConceptFlow</cell><cell>PyTorch</cell><cell>ACL 2020</cell></row><row><cell>Scientific</cell><cell>[63]</cell><cell>Text Generation from Knowledge Graphs with Graph Transformers --Code: https://github.com/rikdz/GraphWriter</cell><cell>PyTorch</cell><cell>NAACL 2019</cell></row><row><cell>writing</cell><cell>[130]</cell><cell>PaperRobot: Incremental Draft Generation of Scientific Ideas --Code: https://github.com/EagleW/PaperRobot</cell><cell>PyTorch</cell><cell>ACL 2019</cell></row><row><cell>Commonsense</cell><cell>[46]</cell><cell>Story Ending Generation with Incremental Encoding and Commonsense Knowledge --Code: https://github.com/JianGuanTHU/StoryEndGen</cell><cell>Tensorflow</cell><cell>AAAI 2019</cell></row><row><cell>reasoning</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&amp;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Story</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>generation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank all anonymous reviewers for valuable comments. We also appreciate the suggestions from readers of the pre-print version. We thank Dr. Michael Zeng (Microsoft) and Dr. Nazneen Rajani (Saleforce) for their constructive comments and suggestions. Wenhao Yu and Dr. Meng Jiang's research is supported by National Science Foundation grants IIS-1849816, CCF-1901059, and IIS-2119531. Qingyun Wang and Dr. Heng Ji's research is based upon work supported by Agriculture and Food Research Initiative (AFRI) grant no. 2020-67021-32799/project accession no.1024178 from the USDA National Institute of Food and Agriculture, U.S. DARPA SemaFor Program No. HR001120C0123, DARPA AIDA Program No. FA8750-18-2-0014, and DARPA KAIROS Program No. FA8750-19-2-1004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copy-right annotation therein.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards String-To-Tree Neural Machine Translation</title>
		<author>
			<persName><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enhancing scientific papers summarization with citation graph</title>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
				<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph Convolutional Encoders for Syntax-aware Neural Machine Translation</title>
		<author>
			<persName><forename type="first">Joost</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Marcheggiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalil</forename><surname>Sima'an</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Commonsense for Generative Multi-Hop Question Answering Tasks</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2022-01">January 2022</date>
		</imprint>
	</monogr>
	<note>Publication date</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph-to-Sequence Learning using Gated Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Abductive commonsense reasoning</title>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incorporating External Knowledge into Machine Reading for Generative Question Answering</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangnan</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing</title>
				<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating Well-Formed Answers by Machine Reading with Stochastic Selector Networks</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Jiangnan Xia, and Chenliang</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research (JMLR)</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel neural topic model and its supervised extension</title>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Retrieve, rerank and rewrite: Soft template based neural summarization</title>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Guiding semi-supervision with constraint-driven learning</title>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">annual meeting of the association of computational linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Syntax-directed attention for neural machine translation</title>
		<author>
			<persName><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Driven answer generation for product-related questions in e-commerce</title>
		<author>
			<persName><forename type="first">Shiqian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Web Search and Data Mining (WSDM)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A review: Knowledge reasoning over knowledge graph</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengbin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Expert Systems with Applications</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation</title>
		<author>
			<persName><forename type="first">Xiuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feilong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reinforcement learning based graph-to-sequence model for natural question generation</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lifelong machine learning</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
				<imprint>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Entdesc: Entity description generation by exploringknowledge graph</title>
		<author>
			<persName><forename type="first">Liying</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dekun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanming</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mixture Content Selection for Diverse Sequence Generation</title>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Domain aware neural dialog system</title>
		<author>
			<persName><forename type="first">Sajal</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prerna</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Sedoc</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1708.00897</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Plug and play language models: a simple approach to controlled text generation</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma Diederik</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
				<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Wizard of wikipedia: Knowledge-powered conversational agents</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Injecting Entity Types into Entity-Guided Text Generation</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue</title>
		<author>
			<persName><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the European Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang and International Joint Conference on Natural Language Processing</title>
				<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019-01">2019. January 2022</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Conference on Empirical Methods in Natural Language Processing ACM Comput</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ELI5: Long Form Question Answering</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An Enhanced Knowledge Injection Model for Commonsense Generation</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yameng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Document Summarization with VHTM: Variational Hierarchical Topic-Aware Mechanism</title>
		<author>
			<persName><forename type="first">Xiyan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinmao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenglu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Natural answer generation with heterogeneous memory</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Posterior regularization for structured latent variable models</title>
		<author>
			<persName><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research (JMLR)</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A topic-driven model for learning to generate diverse sentences</title>
		<author>
			<persName><forename type="first">Ce</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurocomputing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Garbacea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15780</idno>
		<title level="m">Neural Language Generation: Formulation, Methods, and Evaluation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Survey of the state of the art in natural language generation: Core tasks, applications and evaluation</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>JAIR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The gem benchmark: Natural language generation, its evaluation and metrics</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tosin</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karmanya</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Sasanka Ammanamanchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aremu</forename><surname>Anuoluwapo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavi</forename><surname>Khyathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miruna</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Clinciu</surname></persName>
		</author>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
		<author>
			<persName><surname>Kaustubh D Dhole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bottom-Up Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A knowledge-grounded neural conversation model</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Search engine guided neural machine translation</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
				<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A knowledge-enhanced pretraining model for commonsense story generation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Story ending generation with incremental encoding and commonsense knowledge</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Recurrent Hierarchical Topic-Guided RNN for Language Generation</title>
		<author>
			<persName><forename type="first">Dandan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiying</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning (ICML)</title>
				<meeting>the 37th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generating natural answers by incorporating copying and retrieving mechanisms in sequence-to-sequence learning</title>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2017">Jun Zhao. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A comprehensive survey of deep learning for image captioning</title>
		<author>
			<persName><forename type="first">Ferdous</forename><surname>Md Zakir Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohd</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Fairuz Shiratuddin</surname></persName>
		</author>
		<author>
			<persName><surname>Laga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Computing Surveys (CSUR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep Generative Models with Learnable Knowledge Constraints</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoye</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Argument Generation with Retrieval, Planning, and Realization</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Neural Argument Generation Augmented with Externally Retrieved Evidence</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward</title>
		<author>
			<persName><forename type="first">Luyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The Survey: Text Generation Models in Deep Learning</title>
		<author>
			<persName><forename type="first">Touseef</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaima</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of King Saud University-Computer and Information Sciences</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Generating Commonsense Explanation by Extracting Bridge Concepts from Reasoning Paths</title>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and International Joint Conference on Natural Language</title>
				<imprint>
			<publisher>AACL-IJCNLP</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph</title>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A survey on knowledge graphs: Representation, acquisition and applications</title>
		<author>
			<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:2002.00388</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">SemSUM: Semantic Dependency Guided Neural Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Hanqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Genie: A leaderboard for human-in-the-loop evaluation of text generation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:2101.06561</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue</title>
		<author>
			<persName><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Retrieval-Augmented Controllable Review Generation</title>
		<author>
			<persName><forename type="first">Jihyeok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungtaek</forename><surname>Reinald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Amplayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung-Won</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Text Generation from Knowledge Graphs with Graph Transformers</title>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhanush</forename><surname>Bekal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hurdles to Progress in Long-form Question Answering</title>
		<author>
			<persName><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Deep learning. In Nature</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Nature Publishing Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandara</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Guiding generation for abstractive text summarization based on key information guide network</title>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Keywords-guided abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Chengqing Zong, and Xiaodong He</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A Syntactically Constrained Bidirectional-Asynchronous Approach for Emotional Conversation Generation</title>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Leveraging Graph to Improve Abstractive Multi-Document Summarization</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junping</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Coherent Comments Generation for Chinese articles with a Graph-to-Sequence Model</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yancheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengli</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of Association Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning to select knowledge for response generation in dialog systems</title>
		<author>
			<persName><forename type="first">Rongzhong</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for Multi-Document Summarization</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Lebanoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning</title>
		<author>
			<persName><forename type="first">Wangchunshu</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<publisher>EMNLP-Findings</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Glge: A new general language generation evaluation benchmark</title>
		<author>
			<persName><forename type="first">Dayiheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">K-BERT: Enabling Language Representation with Knowledge Graph</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Generating Paraphrase with Topic as Prior Knowledge</title>
		<author>
			<persName><forename type="first">Yuanxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fenglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information and Knowledge Management (CIKM)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Knowledge aware conversation generation with reasoning on augmented graph</title>
		<author>
			<persName><forename type="first">Zhibin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing and International 1:36 Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang Joint Conference on Natural Language Processing</title>
				<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Simple, robust, scalable semi-supervised learning via expectation regularization</title>
		<author>
			<persName><forename type="first">Gideon</forename><forename type="middle">S</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics: System Demonstration (ACL)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision</title>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Towards a continuous knowledge learning engine for chatbots</title>
		<author>
			<persName><forename type="first">Sahisnu</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianzu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1802.06024</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">RefNet: A referenceaware network for background based conversation</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Valuing internal vs. external knowledge: Explaining the preference for outsiders</title>
		<author>
			<persName><forename type="first">Tanya</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pfeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Management science</title>
				<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Discovering discrete latent topics with neural variational inference</title>
		<author>
			<persName><forename type="first">Yishu</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Towards Exploiting Background Knowledge for Building Conversation Systems</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Moghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suman</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Opendialkg: Explainable conversational reasoning with attention-based walks over knowledge graphs</title>
		<author>
			<persName><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pararth</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Subba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">THOTH: Neural Translation and Enrichment of Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Moussallem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Soru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference (ISWC)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Natural Language Learning (SIGNLL)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Don&apos;t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">A Survey on Open Information Extraction</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Cetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siegfried</forename><surname>Handschuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics (COLING)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Polite dialogue generation without parallel data</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics (TACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Knowledge Aware Conversation Generation with Explainable Reasoning over Augmented Graphs</title>
		<author>
			<persName><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing</title>
				<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Semantic Graphs for Generating Deep Questions</title>
		<author>
			<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">KILT: a benchmark for knowledge intensive language tasks</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Language Models as Knowledge Bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing</title>
				<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Conversing by Reading: Contentful Neural Conversation with On-demand Machine Reading</title>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Backpropagation-based Decoding for Unsupervised Counterfactual and Abductive Reasoning</title>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OpenAI Blog</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research (JMLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Multi-Level Memory for Task Oriented Dialogs</title>
		<author>
			<persName><forename type="first">Revanth</forename><surname>Gangi Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danish</forename><surname>Contractor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachindra</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">North American Chapter</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Thinking Globally, Acting Locally: Distantly Supervised Global-to-Local Knowledge Selection for Background Based Conversation</title>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference (ESWC)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Get To The Point: Summarization with Pointer-Generator Networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Linguistic Input Features Improve Neural Machine Translation</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Machine Translation (WMT)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Keyword and keyphrase extraction techniques: a literature review</title>
		<author>
			<persName><forename type="first">Sifatullah</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Sharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal of Computer Applications (IJCA). Foundation of Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Exploiting persona information for diverse generation of conversational responses</title>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Nan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Generating responses with a specific emotion in dialog</title>
		<author>
			<persName><forename type="first">Zhenqiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Summarizing Text on Any Aspects: A Knowledge-Informed Weakly-Supervised Approach</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Target-Guided Open-Domain Conversation</title>
		<author>
			<persName><forename type="first">Jianheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Rich image captioning in the wild</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cornelia</forename><surname>Carapcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Thrasher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Yi-Lin</forename><surname>Tuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing</title>
				<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">SemEval-2020 Task 4: Commonsense Validation and Explanation</title>
		<author>
			<persName><forename type="first">Cunxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuailong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yili</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Semantic Evaluation</title>
				<meeting>the Fourteenth Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Towards information-rich, logical text generation with knowledgeenhanced neural models</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwen</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00814</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Retrieval Enhanced Model for Commonsense Generation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><forename type="middle">Gong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Knowledge-aware graph neural networks with label smoothness regularization for recommender systems</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Improving Knowledge-aware Dialogue Generation via Knowledge Base Question Answering</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Kejing He, Ruifeng Xu, and Min Yang</note>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">PaperRobot: Incremental Draft Generation of Scientific Ideas</title>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of Association Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Topic-Guided Variational Auto-Encoder for Text Generation</title>
		<author>
			<persName><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Diversify Question Generation with Continuous Content Selectors and Question Type Modeling</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangjian</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<publisher>EMNLP-Findings</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Translating with Bilingual Topic Knowledge for Neural Machine Translation</title>
		<author>
			<persName><forename type="first">Xiangpeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxi</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Global-to-local memory pointer networks for taskoriented dialogue</title>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Diverse and Informative Dialogue Generation with Context-Specific Commonsense Knowledge Awareness</title>
		<author>
			<persName><forename type="first">Sixing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhonghai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">TopicKA: Generating Commonsense Knowledge-Aware Dialogue Responses Towards the Recommended Topic Fact</title>
		<author>
			<persName><forename type="first">Sixing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhonghai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Conversational graph grounded policy learning for open-domain conversation generation</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">A Neural Topical Expansion Framework for Unstructured Persona-oriented Dialogue Generation</title>
		<author>
			<persName><forename type="first">Minghong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Artificial Intelligence (ECAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Enhancing Topic-to-Essay Generation with External Commonsense Knowledge</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question answering</title>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Jaket: Joint pre-training of knowledge graph and language understanding</title>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Identifying referential intention with heterogeneous contexts</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxia</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference (WebConf)</title>
				<meeting>The Web Conference (WebConf)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<monogr>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06490</idno>
		<title level="m">Dict-BERT: Enhancing Language Model Pre-training with Dictionary</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Sentence-Permuted Paragraph Generation</title>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Enhancing Taxonomy Completion with Concept Generation via Fusing Relational Representations</title>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Cleland-Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Grounded conversation generation as guided traverses in commonsense knowledge graphs</title>
		<author>
			<persName><forename type="first">Houyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Topic-Informed Neural Machine Translation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics: Technical Papers (COLING)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Prior Knowledge Integration for Neural Machine Translation using Posterior Regularization</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Personalizing Dialogue Agents: I have a dog, do you have pets?</title>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of Association Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced Language Representation with Informative Entities</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">A Survey of Knowledge-Enhanced Text Generation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2022-01">January 2022</date>
		</imprint>
	</monogr>
	<note>ACM Comput. Surv.</note>
</biblStruct>

<biblStruct xml:id="b157">
	<monogr>
		<title level="m" type="main">Graph-based multi-hop reasoning for long text generation</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:2009.13282</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">A Pre-Training Based Personalized Dialogue Generation Model with Persona-Sparse Data</title>
		<author>
			<persName><forename type="first">Yinhe</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongsheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxi</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Emotional chatting machine: Emotional conversation generation with internal and external memory</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Commonsense knowledge aware conversation generation with graph attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Neural question generation from text: A preliminary study</title>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Natural Language Processing and Chinese Computing (NLPCC)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<monogr>
		<title level="m" type="main">Pre-training text-to-text transformers for concept-centric common sense</title>
		<author>
			<persName><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Ho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><forename type="middle">Kiran</forename><surname>Selvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>International Conference for Learning Representation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Improving image captioning by leveraging knowledge graphs</title>
		<author>
			<persName><forename type="first">Yimin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasant</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Boosting Factual Correctness of Abstractive Summarization with Knowledge Graph</title>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hinthorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Bayesian inference with posterior regularization and applications to infinite latent SVMs</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Journal of Machine Learning Research (JMLR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<monogr>
		<title level="m" type="main">Flexible end-to-end dialogue system for knowledge grounded conversation</title>
		<author>
			<persName><forename type="first">Wenya</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiang</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangbin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezheng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno>CoRR, abs/1709.04264</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of the royal statistical society</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Method Ref. Source B-4</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R-L</forename><surname>Baseline Methods Seq2seq</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">117</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">(c) Leaderboard performance on ComVE dataset. Both B-4 and R-L are commonly used. Method Ref</title>
		<idno>GRF [80] KG 11.62 34.62</idno>
	</analytic>
	<monogr>
		<title level="j">Source B-4</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R-L</forename><surname>Baseline Methods Seq2seq</surname></persName>
		</author>
		<idno>117] - 6.10 25.80</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<monogr>
		<title level="m" type="main">Leaderboard performance on CommonGen dataset. SPICE is the primary evaluation metric. Method Ref. Source B-4 SPICE Baseline methods</title>
		<idno>31.83 27.99</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<monogr>
		<title level="m" type="main">(e) Leaderboard performance on Holl-E (mix-short setting) dataset. R-L are the primary metric. Method Ref. Source R-L B-4 Baseline methods Seq2Seq</title>
		<idno>117] - 21.48 5.26</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<monogr>
		<author>
			<persName><surname>Kg-Bart</surname></persName>
		</author>
		<ptr target="https://github.com/yeliu918/KG-BART2021" />
		<title level="m">Knowledge Graph-Augmented BART for Generative PyTorch AAAI Commonsense Reasoning --Code</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<monogr>
		<ptr target="https://github.com/facebookresearch/ParlAI2019" />
		<title level="m">Ground text-enhanced methods Dialogue system [27] Wizard of Wikipedia: Knowledge-Powered Conversational agents PyTorch ICLR --Code</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>ENT-DESC: Entity Description Generation by Exploring Knowledge Graph MXNet EMNLP --Code</note>
</biblStruct>

<biblStruct xml:id="b176">
	<monogr>
		<ptr target="https://github.com/qkaren/converse_reading_cmr2019" />
		<title level="m">Contentful Neural Conversation with On-demand PyTorch ACL Machine Reading --Code</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<monogr>
		<title level="m" type="main">Sequential Latent Knowledge Selection for Knowledge-Grounded Dialogue Tensorflow ICLR</title>
		<ptr target="https://github.com/bckim92/sequential-knowledge-transformer2020" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<monogr>
		<ptr target="https://github.com/ChuanMeng/RefNet2020" />
		<title level="m">Reference-aware Network for Background Based Tensorflow AAAI Conversation --Code</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Question answering [68] Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks PyTorch Neurips</title>
		<ptr target="https://github.com/huggingface/transformers2020" />
	</analytic>
	<monogr>
		<title level="m">BiSET: Bi-directional Selective Encoding with Template for PyTorch ACL Abstractive Summarization --Code</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Summarization [129</note>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<ptr target="https://github.com/facebookresearch/KILT2021" />
		<title level="m">Benchmark for Knowledge Intensive Language Tasks PyTorch NAACL --Code</title>
				<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
