<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Novelty Detection via Contrastive Learning with Negative Data Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chengwei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
							<email>yxie@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaohui</forename><surname>Lin</surname></persName>
							<email>shlin@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruizhi</forename><surname>Qiao</surname></persName>
							<email>ruizhiqiao@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Youtu Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Tan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<email>zhangyi620@zhejianglab.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">Zhejiang Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lizhuang</forename><surname>Ma</surname></persName>
							<email>lzma@cs.ecnu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">East China Normal University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Novelty Detection via Contrastive Learning with Negative Data Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Novelty detection is the process of determining whether a query example differs from the learned training distribution. Previous generative adversarial networks based methods and self-supervised approaches suffer from instability training, mode dropping, and low discriminative ability. We overcome such problems by introducing a novel decoder-encoder framework. Firstly, a generative network (a.k.a. decoder) learns the representation by mapping the initialized latent vector to an image. In particular, this vector is initialized by considering the entire distribution of training data to avoid the problem of mode-dropping. Secondly, a contrastive network (a.k.a. encoder) aims to "learn to compare" through mutual information estimation, which directly helps the generative network to obtain a more discriminative representation by using a negative data augmentation strategy. Extensive experiments show that our model has significant superiority over cutting-edge novelty detectors and achieves new state-of-the-art results on various novelty detection benchmarks, e.g. CIFAR10 and DCASE. Moreover, our model is more stable for training in a non-adversarial manner, compared to other adversarial based novelty detection methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Novelty detection can be described as a one-class classification, which aims to detect the samples whether drawing far away from the learned distribution of training samples from the target class. Generative adversarial networks (GANs) <ref type="bibr">[Sabokrou et al., 2018;</ref><ref type="bibr">Perera et al., 2019;</ref><ref type="bibr" target="#b1">Chen et al., 2020a]</ref> have been a common choice for novelty detection. Generator and discriminator compete mutually while collaborating to learn a representative latent space for the target class. In this latent space, the reconstructed features of novelty samples (outliers) have higher reconstruction errors than normal without the negative data augmentation strategy, the learned distribution (blue oval) of target samples may include the outliers (yellow dots). Bottom: The novelty-like samples (red dots) are generated from target samples by negative data augmentation where we employ multiple transformations combination. Compared with outliers, the distribution of novelty-like samples (red oval) is more close to that of the target samples. Through mutual information estimation, the learned distribution of target class can be pulled to close to the true target distribution (gray oval), and pushed to far away from the distribution of hard novelty-like samples. samples (inliers), which can be used for distinguishing between normal and novelty classes. Recently, self-supervised learning <ref type="bibr">[Komodakis and Gidaris, 2018;</ref><ref type="bibr" target="#b5">Ji et al., 2019]</ref> holds great for improving representations when labeled data are scarce. In the training process, the network learns useful feature representation by solving some specialized pretext tasks, such as geometric transformations prediction <ref type="bibr" target="#b3">[Hendrycks et al., 2019]</ref>. During inference, the model is transferred to the downstream task, like novelty detection.</p><p>However, some weaknesses are still existing in the previous work. For the GAN based methods, it suffers from three critical problems: mode-dropping, instable training, and low discriminative ability. First, various GAN based methods (e.g. <ref type="bibr">ALOCC [Sabokrou et al., 2018]</ref>, <ref type="bibr">OCGAN [Perera et al., 2019]</ref> and <ref type="bibr">DualGAN [Chen et al., 2020a]</ref>), only learn the partial modes of target distributions, which causes the problem of mode dropping <ref type="bibr" target="#b0">[Arora et al., 2018]</ref>. Second, the imbalance capacity of generator and discriminator causes the training of model unstable <ref type="bibr">[Zhao et al., 2017]</ref>, which affects the learning of latent representation of normal samples. Third, the latent features with low discriminative ability <ref type="bibr" target="#b6">[Liu et al., 2020]</ref> are generated in self-representation, since the decoder of GANs tends to learn more structive representation than discriminative characteristics.</p><p>The existing self-supervised learning based novelty detection requires specialized implementation for the pretext tasks, such as design supervised labels, loss functions, and network architectures. Rotation prediction <ref type="bibr">[Komodakis and Gidaris, 2018]</ref> and transformations prediction <ref type="bibr" target="#b3">[Hendrycks et al., 2019]</ref> could capture the semantic information of object shapes that are useful for target tasks. However, it lacks other semantic information such as object textures and colors, which leads to the low discriminative ability of features that fails to effectively detect novelty samples. Besides, some <ref type="bibr" target="#b5">[Lim et al., 2018;</ref><ref type="bibr" target="#b9">Sinha et al., 2021]</ref> use data augmentation as an additional source of data in the GAN. Inspired by these works, contrasting shifted instance (CSI) <ref type="bibr">[Tack et al., 2020]</ref> contrasts distributionally-shifted augmentations with an auxiliary softmax classifier to learn the feature representation of encoder based on SimCLR <ref type="bibr" target="#b2">[Chen et al., 2020b]</ref>. However, the augmented images as negative samples only use one random augmentation, which cannot generate effective negative samples. Beside, the detection score function of CSI is complex with high computation and memory cost.</p><p>To address these issues, we propose a novel decoderencoder framework for one-class novelty detection to learn more discriminative latent representation by contrastive learning. Our framework consists of three parts: a generative network, a contrastive network, and a mutual information estimator. First, the generative network (decoder) aims to learn the representation of target class by mapping each initialized latent vector to each target image; The initialization of the latent vectors is obtained by encoding the entire distribution of training data to alleviate the problem of modedropping. Then, we employ the contrastive network (encoder) to extract both local feature maps from positive and negative samples. It also captures the global latent features from positive samples. In particular, we select the normal training data as positive samples and use negative data augmentation strategy to generate hard negative samples (a.k.a. novelty-like samples) from their corresponding positive samples by using multiple random transformations. Different from outliers, these novelty-like samples are more close to the normal ones. Therefore, it can better help to separate the distribution of normal and novelty samples by learning the disjointness between positive and novelty-like samples, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Finally, mutual information estimator is adopted to generate discriminative latent features through contrastive learning on the features of input pairs by maximizing the local, global, and prior mutual information. Our decoder-encoder framework is presented in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>We summarize our main contributions of this paper:</p><p>• We propose a novel effective decoder-encoder framework for the novelty detection task. Contrastive learning is introduced to learn more discriminative latent features for distinguishing between positive and negative augmentation samples.</p><p>• The mutual information estimator is trained in a nonadversarial way by distinguishing between features of local parts and global context constituted by only positive and novelty-like samples, which helps the model to train more stably with faster convergence than GANbased methods.</p><p>• Extensive experiments demonstrate the superior performance of our approach for novelty detection in several challenging datasets. For instance, our method achieves the highest mean AUC of 0.843 and 0.899 on CIFAR-10 and DCASE, compared to state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>2.1 Our Decoder-encoder Framework</p><p>Our framework consists of three components: a generative network, a contrastive network and a mutual information estimator, as it shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The generative network learns a mapping from latent space to high-dimensional image space, while the contrastive network maps a positive/negative image to local feature maps and a global latent feature. The mutual information estimator is used for distinguishing between the features from the target samples and their corresponding hard negative samples to effectively learn more discriminative latent feature presentation.  <ref type="bibr">, 2006]</ref> as the reconstruction loss; MSE is easy to yield the blurry image, while the Laplacian pyramid loss can generate better reconstructed images by Laplacian pyramid representation. Therefore, the reconstruction loss between the reconstructed output ψ θg (z i ) and the input image x i is formulated as:</p><formula xml:id="formula_0">Notations. Let X = {x 1 , • • • , x N } denote the origi- nal input images of target class with N samples, Z = {z 1 , • • • , z N }</formula><formula xml:id="formula_1">L lap = j 2 2j Lap j (ψ θg (z i )) − Lap j (xi) 1 ,<label>(1)</label></formula><p>where Lap j (•) is the j-th level of Laplacian pyramid representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent representation loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image reconstruction loss</head><p>Learnable latent space</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative network</head><p>Contrastive network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual information estimator</head><p>Feature map</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ Latent vector</head><p>Negative samples (Augmentations )</p><p>Positive samples (Input images)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual information loss</head><p>Local / Global / Prior discriminatior Contrastive network. To improve the discriminative ability of the latent features of the target class, the contrastive network uses a pair of positive and negative samples as an input to extract both local feature maps (A p i , A n i ) and a global latent vector z i of the positive sample, where the samples from different classes compete each other through mutual information estimator. As shown in Fig. <ref type="figure" target="#fig_1">2</ref> (blue box), a positive samples x i is an original image and its negative sample is generated by the negative data augmentation M as M (x i ) (called noveltylike samples). Our transformations of augmentation include random resized crop, random color jitter, random grayscale, and random horizontal flip. We employ the combination of multiple random transformations to capture more rich semantic information, including object shape, textures, and colors. In addition, this augmentation can better help to separate the distribution of normal and novelty samples by learning the disjointness between positive and novelty-like samples, as the distribution of novelty-like samples is more close to positive ones than outliers. In our framework, we employ cooperative learning between the generative network and contrastive network to generate better reconstructed images and provide more discriminative global latent vectors. It also motivates us to use the decoder-encoder framework instead of the conventional encoder-decoder frameworks with unilateral learning <ref type="bibr">[Vincent et al., 2010;</ref><ref type="bibr" target="#b7">Marchi et al., 2017]</ref>. Therefore, we need to minimize the distance between the initialized latent vector z i and the global latent vector φ θc (x i ) as:</p><formula xml:id="formula_2">L lat = z i − φ θc (xi) 2 2 .</formula><p>(2)</p><p>Inspired by <ref type="bibr">[Sabokrou et al., 2018]</ref>, the reconstruction space is more effective for distinguishing the positive and novelty samples compared to the latent space. Therefore, a test example x first go through contrastive network and then generative network in a encoder-decoder manner during testing. The constraint by Eq. ( <ref type="formula">2</ref>) makes the testing feasible. Without this constraint, the reconstructed images from positive samples will have significantly large reconstructed error even with the constraint of Eq. (1). Mutual information estimator. Mutual information measures the essential relevance of two instances. The larger mutual information is, the more similar two variables have. Given two random variables x and y, mutual information <ref type="bibr" target="#b0">[Belghazi et al., 2018]</ref> can be estimated by the JS divergence between the joint p(y | x)p(x) and the product of the margins p(x)p(y). According to the definition of the variational estimation of JS divergence <ref type="bibr" target="#b8">[Nowozin et al., 2016]</ref>, the maximization of the mutual information between variables X and Y can be formulated as:</p><formula xml:id="formula_3">min θe −I(X, Y ) = min θe − E (x,y)∼p(y|x)p(x) [log σ(T (x, y))] +E (x,y)∼p(y)p(x) [log(1 − σ(T (x, y)))] },<label>(3)</label></formula><p>where σ denotes the sigmoid function and T (x) = log 2p(x)  p(x)+q(x) . Here p(z|x)p(x) and p(z)p(x) are utilized to replace p(x) and q(x).</p><p>In our paper, we divide mutual information estimator into three parts: global estimator, local estimator and prior estimator. The goal of the mutual information estimator is to generate discriminative global latent features by contrastive learning between positive samples and negative samples. We first consider the global mutual information (see Fig. <ref type="figure" target="#fig_3">3(a)</ref>). Based on Eq. ( <ref type="formula" target="#formula_3">3</ref>), the maximization of global mutual information is also equivalent to minimize Eq. ( <ref type="formula" target="#formula_4">4</ref>) by introducing  a global estimator τ θge ([A z , z]), which can be formulated as:</p><formula xml:id="formula_4">L global = −β E (A p ,z)∼p(z|A p )p(A p ) [log σ(τ θge ([A p z , z]))] +E (A n ,z)∼p(z)p(A n ) [log(1 − σ(τ θge ([A n z , z])))] ,<label>(4)</label></formula><p>where A p z and A n z are the vectors downscaled from the feature maps A p and A n from the positive sample and the negative sample, respectively. They all have the same dimension to z.</p><p>[A z , z] is the concatenation between the downscaled feature A z and z as an input pair. β is a hyperparameter. Similar to <ref type="bibr" target="#b4">[Hjelm et al., 2019]</ref>, the optimization of Eq. ( <ref type="formula" target="#formula_4">4</ref>) is to estimate the global latent feature distribution from positive samples by distinguishing the input from positive samples or negative samples.</p><p>Second, we consider local mutual information (see Fig. <ref type="figure" target="#fig_3">3(b</ref>)), and also construct the relationship between the local feature map and the global latent feature. The process of estimation is the same as global mutual information. Thus, the objective function of local mutual information loss can be formulated as:</p><formula xml:id="formula_5">L local = − β HW Σi,j E (A p ,z)∼p(z|A p )p(A p ) log σ τ θ le ([A p ij , z A ]) +E (A n ,z)∼p(z)p(A n ) log 1 − σ τ θ le ([A n ij , z A ]) ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">τ θ le ([A ij , z A ]</formula><p>) is a local estimator to output the reverent probability between the local feature map A and latent representation z A , which consists of a wide range of replicated feature vectors from z with the same dimension to A.</p><p>An input pair is either from a positive sample</p><formula xml:id="formula_7">[A p ij , z A ] or a negative sample [A n ij , z A ] at coordinates (i, j</formula><p>). H and W represent the height and width of the feature map.</p><p>Third, we employ the KL-divergence between the global latent feature and the prior distribution (e.g., normal distribution) to encourage the latent feature to be more regular. Thus we can construct the following objective function:</p><formula xml:id="formula_8">Lprior = γE A∼p(A) [KL(p(z | A) q(z))],<label>(6)</label></formula><p>where q(z) is a prior distribution (e.g., normal distribution) and p(z|A) is the output of prior estimator τ θpe . By combining the aforementioned three loss functions (i.e., Eqs (3), ( <ref type="formula" target="#formula_4">4</ref>) and ( <ref type="formula" target="#formula_5">5</ref>)), we obtain the final mutual information estimator loss function:</p><formula xml:id="formula_9">L mie = L global + L local + L prior .</formula><p>(7) By minimizing the above function, we generate discriminative latent features z that helps to make the normal and novelty samples separable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Overall Loss Function</head><p>According to the above discussion, we can construct the overall loss function for our decoder-encoder framework as:</p><formula xml:id="formula_10">L all = λ 1 L lap + λ 2 L lat + λ 3 L mie<label>(8)</label></formula><p>where λ 1 , λ 2 and λ 3 are the hyperparameters for balancing these three different terms. For solver, SGD optimizer can be directly used to minimize Eq. ( <ref type="formula" target="#formula_10">8</ref>) in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementation Details</head><p>We adopt the structure of <ref type="bibr">DCGAN [Radford et al., 2016]</ref> as the structure of the generative network and contrastive network. The global mutual information estimator is a fullyconnected network with two 512-unit hidden layers. A 1 x 1 convnet with two 512-unit hidden layers is regarded as the local mutual information estimator. The prior mutual information estimator is a fully-connected network with two hidden layers of 1000 and 200 units.</p><p>Our image augmentation M contains cropping, horizontal flip, color jitter, rotation, and grayscale for random augmentations. During inference, the test sample x first goes through the contrastive network to be encoded into a latent vector. The generative network then upscales the latent vector to reconstruct the image from the learned discriminative latent feature space. Finally, the abnormal score is calculated by image reconstruction error between test image sample x and the corresponding generated image x . The test sample x is regarded as a novelty instance if the image reconstruction error is larger than a predefined threshold T , and a normal instance otherwise. We use PyTorch <ref type="bibr" target="#b8">[Paszke et al., 2019]</ref> to implement our method. For training parameters, the learning rate and the number of total epochs are set to 0.002 and 100, respectively. SGD optimizer with momentum is adopted to  optimize the parameters of our framework. Batch size, momentum and weight decay are set to 128, 0.9 and 0.005, respectively. For hyperparameters, β and γ are set to 0.5 and 0.1, respectively. λ 1 , λ 2 and λ 3 are all set to 1.</p><p>3 Experiments   <ref type="bibr">, 2004]</ref> for spoofing face detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Ablation Study</head><p>In this part, we evaluate the effect of each mutual information loss in Eq. ( <ref type="formula">7</ref>), the effect of information fusion between feature maps and latent feature, the effect of Laplacian pyramid loss and the effect with/without PCA initialization. We conduct the experiments on CIFAR-10 for ablation study.</p><p>The effect of mutual information estimation. We first evaluate the effect of each component in mutual information estimation. The results are summarized in Tab. 1. Obviously, our model achieves a significantly higher AUC score by 0.843 with all mutual information estimation compared to that without any mutual information estimation (i.e. 0.75 AUC). This is due to the improvement of discriminative ability for the latent feature learned by the mutual information estimators. We also observe that global mutual information estimation (i.e. global loss Eq. ( <ref type="formula" target="#formula_4">4</ref>)) achieves the best performance when using only one mutual information estimation loss. Note that training our model with two mutual information estimation losses achieves comparable results to the full three losses (see the 6th, 7th and 8th column in Tab. 1). Overall, three mutual information estimation losses indeed help to improve the performance of our model.</p><p>We further evaluate the mutual information estimation can help the model to obtain more discriminative feature representation by visualization. To this end, we randomly select 500 in-class samples (frog class) and 500 out-of-class samples from the dataset for testing<ref type="foot" target="#foot_1">1</ref> . As illustrated in Fig. <ref type="figure" target="#fig_4">4</ref>, two global latent features are generated by our framework with/without mutual information estimation on CIFAR-10 by using t-SNE [van der <ref type="bibr">Maaten and Hinton, 2008]</ref>. The normal samples (indicated by blue dots in Fig. <ref type="figure" target="#fig_4">4</ref> left) are significantly separated from novelty samples (indicated by yellow dots in Fig. <ref type="figure" target="#fig_4">4</ref> right) by using mutual information estimation, compared to that without this estimator.  The effect to remove z A or z. In local and global mutual information estimation, the concatenation of feature maps and latent vector are used as the inputs of their estimators during training. The discriminative feature representation is learned by distinguishing the concatenated features all from positive samples and part from negative samples. To evaluate the effectiveness of the concatenated features, we remove the information of latent matrix z A in the local estimator or latent vector z in the global estimator. In the first part of Tab. 2, the (first 3 rows) combination without any removal in our estimators achieves the highest AUC, compared to the removals of z A or z.</p><p>The effect of Laplacian pyramid loss. As shown in the second part of Tab. 2, the framework without any reconstruction loss results in the worst average AUC score of 0.53. We further compare the Laplacian pyramid loss with the MSE loss. As presented in the second part of Tab. 2, the performance of Laplacian pyramid loss significantly outperforms the MSE loss (i.e. 0.843 AUC vs. 0.810 AUC). To explain, Laplacian pyramid loss is a perception-level error, which is more effective for novelty detection than MSE loss.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparison with State-of-the-art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Novelty Detection</head><p>We first evaluate the effectiveness of our method on CIFAR-10. As shown in Tab. 3, our method achieves the highest mean AUC of 0.843, compared to other SOTA methods. We also found that the self-supervised learning methods based on the pretext tasks (e.g., <ref type="bibr">RotNet [Komodakis and Gidaris, 2018]</ref>, Geometric <ref type="bibr" target="#b3">[Hendrycks et al., 2019]</ref>) achieves higher performance, compared with GAN-based methods (e.g., <ref type="bibr">OCGAN [Perera et al., 2019]</ref> and <ref type="bibr">DualGAN [Chen et al., 2020a]</ref>). For the individual class, our method also shows the best results, except the class car, dog, horse and truck. The semantic information of these classes has high related to the object shape in CIFAR-10. Since the RotNet focuses on the semantic information of object shape, it achieves high performance in these classes. However, it lack capturing other semantic feature(e.g. object color and object texture).</p><p>Following <ref type="bibr">[Perera et al., 2019]</ref>, we also evaluate the performance of novelty detection methods on COIL100, MNIST, fMNIST by using the following evaluation setting: The 80% of in-class samples are regarded as a normal class for training, while the rest of 20% of in-class samples is adopt for testing. Out-of-class test samples have the same number of in-class test samples, which are randomly selected from the test set. As shown in Tab. 4, our method achieves the best performance across different datasets, compared to state-of-theart methods. For example, in MNIST dataset and fMNIST dataset, we achieve the improvement of average AUC score 0.1% and 0.4% respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acoustic Anomaly Detection</head><p>The original <ref type="bibr">WaveNet [Oord et al., 2016]</ref> has been successfully applied into raw audio generation and music synthesis, which benefits from its powerful convolutional autoregressive architecture. Recently, [Rushe and Mac Namee, 2019] has extended its structure for anomaly detection in raw audio. Thus, we also make a comparison with it on the DCASE dataset, which is denoted by WaveNet for convenience. As shown in Tab. 5, we present the results of different methods across 15 classes/scenarios. Obviously, our method achieves the best performance in most of the scenarios, except the restaurant and home scenarios, compared to CAE, WaveNet, and DualGAN. For example, our method outperforms the best DualGAN by 8% AUC on library scenario. Note that Dual-GAN achieves amazing performance with 0.9 AUC. We conjecture this is due to the randomness of the DualGAN method in the home background. Interestingly, self-supervised learning based methods (e.g. <ref type="bibr">RotNet [Komodakis and Gidaris, 2018]</ref> and Geometric <ref type="bibr" target="#b3">[Hendrycks et al., 2019]</ref>) shows worse performance on DCASE. This is due to the failure to presentation of object colors and object textures. Actually, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>, the main discriminative features between normal and novelty audio in spectrogram are from object colors and textures. Rich semantic information of object shapes obtained by these methods cannot separate the normal samples from novelty samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spoofing Face Detection</head><p>Our method also works for spoofing face detection. Actually, we formulate face anti-spoofing detection as a novelty detection task by only using the normal (live faces) samples for training. We intuitively take the optical flow obtained from consecutive video frames as input, since the data source of spoof face detection is from the video. To this end, we first extract the optical flow using FlowNet2.0 <ref type="bibr" target="#b4">[Ilg et al., 2017]</ref> from each video with 30 fps.</p><p>Following <ref type="bibr" target="#b10">[Tu et al., 2020]</ref>, the detection models select the training set from one of the training set in CASIA-MFSD and Replay-Attack dataset, and the test set from the other test dataset. The results are summarized in Tab. 6. The proposed method achieves the best performance (HTER = 0.175) on the Replay-Attack test set of which includes different types of spoofing attacks. On the other dataset setting, our method achieves a competitive performance (HTER = 0.308) on the testing set of the CASIA-MFSD dataset. Actually, the HTER achieved by our unsupervised method significantly lower than supervision methods except auxiliary method <ref type="bibr" target="#b5">[Liu et al., 2018]</ref>. This is probably due to the help of the additional depth information and rPPG signal. Nevertheless, our method achieves better performance by simultaneously evaluating these two tasks on the average HTER. We also found that the generalization of these models trained on  CASIA-MFSD dataset is better than the models trained on the Replay-Attack dataset. We speculate that the detection scenario on the CASIA-MFSD dataset is more complex than the Replay-Attack dataset, which leads to the easy learning of more knowledge during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Stability and Convergence</head><p>We further evaluate the effectiveness of our method during training. Our decoder-encoder framework achieves more stable training and fast convergence speed in CIFAR-10 dataset, compared to other encoder-decoder frameworks, e.g. OC-GAN, ALOCC and DualGAN. As shown in Fig. <ref type="figure" target="#fig_6">6</ref>, the proposed decoder-encoder framework tends to be convergent after the 25-th epoch, while the convergent value of other methods is significantly larger (e.g. about 40 epochs in OCGAN). In addition, the image reconstruction error is reduced steadily using the decoder-encoder framework, while the significant large fluctuations occur in other methods. This is due to the usage of the mutual information estimator that adversarial optimization is removed in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel decoder-encoder framework for novelty detection. To alleviate mode dropping of GANs, each latent initialized vector is mapped to an image by the PCA initialization in the generative network. To learn a more discriminative latent feature representation, we introduce a contrastive network to learn to compare the different input pairs through mutual information estimation. Specifically, our framework is trained without adversarial optimization, which benefits fast convergence and stable training of our model. We have comprehensively evaluated the performance of our method on a variety of novelty detection tasks over different datasets, which demonstrates the superior performance gains over the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of negative data augmentation strategy. Top: without the negative data augmentation strategy, the learned distribution (blue oval) of target samples may include the outliers (yellow dots). Bottom: The novelty-like samples (red dots) are generated from target samples by negative data augmentation where we employ multiple transformations combination. Compared with outliers, the distribution of novelty-like samples (red oval) is more close to that of the target samples. Through mutual information estimation, the learned distribution of target class can be pulled to close to the true target distribution (gray oval), and pushed to far away from the distribution of hard novelty-like samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of our decoder-encoder framework for novelty detection. It consists of three components: a generative network, a contrastive network, and a mutual information estimator. The generative network (decoder) learns a reconstructed image representation by mapping each initialized latent vector to each target image. The contrastive network (encoder) encodes a pair of target and novelty-like samples from data augmentation to extract their latent global features and local feature maps. The mutual information estimator is adopted to generate discriminative latent features through contrastive learning on the features of input pairs by maximizing the local, global and prior mutual information.</figDesc><graphic url="image-10.png" coords="3,-46.95,151.66,170.38,173.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Global mutual information (b) Local mutual information (c) Prior mutual information</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Local, global and prior mutual information estimation.</figDesc><graphic url="image-79.png" coords="4,54.76,65.00,179.61,129.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The visualization of latent space learned from target class (Frog class) by using proposed method with/without mutual information estimation.</figDesc><graphic url="image-80.png" coords="5,56.78,117.87,115.15,109.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The visualization of test sample: Metro background audio mixed with baby crying.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The visualization of training loss comparison between previous methods and proposed method on CIFAR-10.</figDesc><graphic url="image-92.png" coords="7,326.48,256.70,237.97,98.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>denote their corresponding global latent features where z i = φ θc (x i ) ∈ R d is learned by the contrastive network C with parameters θ c . d is the dimension of latent features. A i = φ θ f p (x i ) ∈ R H×W ×S is the local feature maps extracted from contrastive network C, where θ f p ⊂ θ c and H × W × S is the dimension of feature maps.x i = ψ θg (z i ) presents the reconstructed image by generative network G with parameters θ g , where z i is the initialized latent vector. In mutual information estimator, we denote the global estimator, local estimator and prior estimator as GE, LE and PE with parameters θ ge , θ le and θ pe , respectively.</figDesc><table><row><cell>Generative network. The input latent vectors z 1 , , • • • , z N</cell></row><row><cell>of generative network are first initialized by the PCA projec-</cell></row><row><cell>tion of all input images, which helps to alleviate the problem</cell></row><row><cell>of mode dropping. Like a decoder, the generative network</cell></row><row><cell>outputs ψ θg (z i ) should be regained close to the input image</cell></row><row><cell>x i . Instead of MSE loss, we employ the Laplacian pyramid</cell></row><row><cell>loss [Ling and Okada</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The effect of different mutual information estimation.</figDesc><table><row><cell>Prior MI</cell><cell></cell><cell></cell><cell></cell><cell>√</cell><cell>√</cell><cell></cell><cell>√</cell><cell>√</cell></row><row><cell>CIFAR-10</cell><cell>0.750</cell><cell>0.832</cell><cell>0.838</cell><cell>0.823</cell><cell>0.841</cell><cell>0.842</cell><cell>0.842</cell><cell>0.843</cell></row><row><cell cols="4">With mutual information estimation</cell><cell cols="5">Without mutual information estimation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study for proposed method. Part1:Removal of information inputs for the estimators. Part2:Comparsion of different reconstruction losses in generative network. Part3:The effect of different priors in generator inputs. N=Normal distribution, C=Contrastive prior, P=PCA prior. Evaluation methodology. The protocol in the literature is proposed for one-class novelty detection [Perera et al., 2019]. All of in-class training samples from only one class are used for training, and all samples in test set are used for testing. We use Area Under Curve (AUC) to evaluate the performance in novelty detection and acoustic anomaly detection. We also use Half Total Error Rate (HTER) [Bengio and Mariéthoz</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>AUC of different novelty detection methods on CIFAR-10. Plane and car denote Airplane and Automobile in CIFAR-10, respectively.</figDesc><table><row><cell></cell><cell>MNIST</cell><cell>COIL</cell><cell>fMNIST</cell></row><row><cell>ALOCC DR ('18)</cell><cell>0.88</cell><cell>0.809</cell><cell>0.753</cell></row><row><cell>ALOCC D ('18)</cell><cell>0.82</cell><cell>0.686</cell><cell>0.601</cell></row><row><cell>DCAE ('14)</cell><cell>0.899</cell><cell>0.949</cell><cell>0.908</cell></row><row><cell>GPND ('18)</cell><cell>0.932</cell><cell>0.968</cell><cell>0.901</cell></row><row><cell>Rot ('18)</cell><cell>0.933</cell><cell>0.970</cell><cell>0.935</cell></row><row><cell>OCGAN ('19)</cell><cell>0.977</cell><cell>0.995</cell><cell>0.924</cell></row><row><cell>DualGAN ('20)</cell><cell>0.985</cell><cell>1.0</cell><cell>0.995</cell></row><row><cell>Ours</cell><cell>0.986</cell><cell>1.0</cell><cell>0.999</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Mean AUC results of the different methods on MNIST, COIL and fMNIST.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>AUC scores for all methods on DCASE dataset with 15 scenarios.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Classification performance of the proposed approach in terms of HTER.</figDesc><table><row><cell>Methods</cell><cell>Train on CASIA-MFSD &amp;Test on Replay-Attack</cell><cell>Train on Replay-Attack &amp;Test on CASIA-MFSD</cell></row><row><cell>LBP ('13)</cell><cell>0.470</cell><cell>0.396</cell></row><row><cell>LBP-TOP ('13)</cell><cell>0.497</cell><cell>0.606</cell></row><row><cell>Motion ('13)</cell><cell>0.502</cell><cell>0.479</cell></row><row><cell>CNN ('14)</cell><cell>0.485</cell><cell>0.455</cell></row><row><cell>Color LBP ('15)</cell><cell>0.379</cell><cell>0.354</cell></row><row><cell>Color Tex ('16)</cell><cell>0.303</cell><cell>0.377</cell></row><row><cell>Auxiliary ('18)</cell><cell>0.276</cell><cell>0.284</cell></row><row><cell>De-Spoof ('18)</cell><cell>0.285</cell><cell>0.411</cell></row><row><cell>DA ('18)</cell><cell>0.274</cell><cell>0.360</cell></row><row><cell>D-texture ('18)</cell><cell>0.222</cell><cell>0.350</cell></row><row><cell>OF Domain ('18)</cell><cell>0.301</cell><cell>0.368</cell></row><row><cell>ADA ('19)</cell><cell>0.175</cell><cell>0.416</cell></row><row><cell>GFA-CNN ('20)</cell><cell>0.214</cell><cell>0.343</cell></row><row><cell>DualGAN ('20)</cell><cell>0.223</cell><cell>0.246</cell></row><row><cell>Ours</cell><cell>0.175</cell><cell>0.308</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">√ √ √ 0.828 √ √ √ 0.821 √ √ √ 0.843 √ √ 0.530 √ √ √ 0.810 √ √ √ 0.745 √ √ √ 0.795</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">We run data selection 5 times and obtain a similar visualization result. For simplicity, we select one of them for visualization.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank for the support from National Natural Science Foundation of China (61772524, 61902129, 61972157,  61876161, 61701235, 61373077), Shanghai Pujiang Talent Program (19PJ1403100), the Science and Technology Commission of Pudong (NO. PKJ2018-Y46) and Shanghai Jiaotong University Translational Medicine Cross Foundation (ZH2018ZDA25), Shanghai Sailing Program (21YF1411200), Natural Science Foundation of Shanghai (20ZR1417700), the Fundamental Research Funds for the Central Universities, CAAI-Huawei MindSpore Open Fund.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A statistical significance test for person authentication</title>
		<author>
			<persName><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey</title>
				<editor>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2004">2018. 2018. 2018. 2018. 2004. 2004</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Latent regularized generative dual adversarial network for abnormal detection</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2020">2020a. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020b. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName><surname>Chingovska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mantas Mazeika, Saurav Kadavath, and Dawn Song</title>
				<imprint>
			<publisher>Dan Hendrycks</publisher>
			<date type="published" when="2012">2012. 2012. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<editor>
			<persName><forename type="first">Nikolaus</forename><surname>Ilg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tonmoy</forename><surname>Mayer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Margret</forename><surname>Saikia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexey</forename><surname>Keuper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Dosovitskiy</surname></persName>
		</editor>
		<editor>
			<persName><surname>Brox</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2019. 2019. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lecun and Bottou, 1998] Yann Lecun and Leon Bottou. Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<editor>
			<persName><forename type="first">Kiat</forename><surname>Swee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yi</forename><surname>Lim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ngoc-Trung</forename><surname>Loo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ngai-Man</forename><surname>Tran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gemma</forename><surname>Cheung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuval</forename><surname>Roig</surname></persName>
		</editor>
		<editor>
			<persName><surname>Elovici</surname></persName>
		</editor>
		<imprint>
			<publisher>Amin Jourabloo, and Xiaoming Liu</publisher>
			<date type="published" when="1998">2019. 2019. 2018. 2018. 2009. 2009. 1998. 2018. 2018. 2006. 2006. 2018. 2018</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Selfsupervised learning: Generative or contrastive</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08218</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep recurrent neural network-based autoencoders for acoustic novelty detection. Computational intelligence and neuroscience</title>
		<author>
			<persName><surname>Marchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DCASE 2017-Workshop on Detection and Classification of Acoustic Scenes and Events</title>
				<editor>
			<persName><forename type="first">Annamaria</forename><surname>Mesaros</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Toni</forename><surname>Heittola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aleksandr</forename><surname>Diment</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Benjamin</forename><surname>Elizalde</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ankit</forename><surname>Shah</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2017. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>Bhiksha Raj, and Tuomas Virtanen</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><surname>Nene</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
	</analytic>
	<monogr>
		<title level="m">Pramuditha Perera, Ramesh Nallapati, and Bing Xiang</title>
				<imprint>
			<publisher>Ellen Rushe and Brian Mac Namee</publisher>
			<date type="published" when="1996-02">1996. February 1996. 2016. 2016. 2016. 2016. 2019. 2019. 2019. 2016. 2019. 2019. 2018</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<author>
			<persName><surname>Sinha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05113</idno>
	</analytic>
	<monogr>
		<title level="m">Burak Uzkent, Hongxia Jin, and Stefano Ermon. Negative data augmentation</title>
				<imprint>
			<date type="published" when="2020">2021. 2021. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research</title>
				<editor>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008-12">2020. 2020. 2008. 2008. Dec. 2010</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
		</imprint>
	</monogr>
	<note>Journal of machine learning research</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A face antispoofing database with diverse attacks</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<editor>
			<persName><forename type="first">Jake</forename><surname>Zhao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2012">2012. 2012. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>ICB</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
