<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analysis and Optimization of the Memory Hierarchy for Graph Processing Workloads</title>
				<funder>
					<orgName type="full">CRISP</orgName>
				</funder>
				<funder>
					<orgName type="full">DARPA</orgName>
				</funder>
				<funder ref="#_rWz6Zug #_9Hu2cW7 #_2MpdnnF">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Abanti</forename><surname>Basak</surname></persName>
							<email>abasak@ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara* Alibaba, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuangchen</forename><surname>Li</surname></persName>
							<email>shuangchenli@ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara* Alibaba, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xing</forename><surname>Hu</surname></persName>
							<email>xinghu@ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara* Alibaba, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sang</forename><forename type="middle">Min</forename><surname>Oh</surname></persName>
							<email>sangminoh@ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara* Alibaba, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinfeng</forename><surname>Xie</surname></persName>
							<email>xinfeng@ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara* Alibaba, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara* Alibaba, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaowei</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara* Alibaba, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
							<email>yuanxie@ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara* Alibaba, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Analysis and Optimization of the Memory Hierarchy for Graph Processing Workloads</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/HPCA.2019.00051</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph processing is an important analysis technique for a wide range of big data applications. The ability to explicitly represent relationships between entities gives graph analytics a significant performance advantage over traditional relational databases. However, at the microarchitecture level, performance is bounded by the inefficiencies in the memory subsystem for single-machine in-memory graph analytics. This paper consists of two contributions in which we analyze and optimize the memory hierarchy for graph processing workloads.</p><p>First, we perform an in-depth data-type-aware characterization of graph processing workloads on a simulated multi-core architecture. We analyze 1) the memory-level parallelism in an out-of-order core and 2) the request reuse distance in the cache hierarchy. We find that the load-load dependency chains involving different application data types form the primary bottleneck in achieving a high memory-level parallelism. We also observe that different graph data types exhibit heterogeneous reuse distances. As a result, the private L2 cache has negligible contribution to performance, whereas the shared L3 cache shows higher performance sensitivity.</p><p>Second, based on our profiling observations, we propose DROPLET, a !!ata-awa!!e decQuPLed pr~feIcher for graph applications. DROPLET prefetches different graph data types differently according to their inherent reuse distances. In addition, DROPLET is physically decoupled to overcome the serialization due to the dependency chains between different data types. DROPLET achieves 19%-102% performance improvement over a no-prefetch baseline, 9%-74% performance improvement over a conventional stream prefetcher, 14%-74% performance improvement over a Variable Length Delta Prefetcher, and 19%-115% performance improvement over a delta correlation prefetcher implemented as a global history buffer. DROPLET performs 4%-12.5% better than a monolithic Ll prefetcher similar to the state-of-the-art prefetcher for graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Graph processing is widely used to solve big data problems in multiple domains such as social networks, web searches, recommender systems, fraud detection, financial money flows, and transportation. The high potential of graph processing is due to its rich, expressive, and widely applicable data representation consisting of a set of entities (vertices) connected to each other by relational links (edges). Numerous vendors such as Oracle <ref type="bibr" target="#b0">[1]</ref>, Amazon (AWS) <ref type="bibr" target="#b1">[2]</ref>, and Microsoft <ref type="bibr" target="#b2">[3]</ref> provide graph processing engines for enterprises. Moreover, companies such as Google <ref type="bibr" target="#b3">[4]</ref>, Facebook <ref type="bibr" target="#b4">[5]</ref>, and Twitter <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> have built customized graph processing frameworks to drive their products. Graph technology is also predicted to be the driver for many emerging data-driven markets, such as an expected $7 trillion worth market of self-driving cars by 2050 <ref type="bibr" target="#b7">[8]</ref>.</p><p>Prior work in graph analytics primarily spans hardware platforms such as distributed systems <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, out-of-core systems <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b15">[16]</ref>, and customized accelerators <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b20">[21]</ref>. However, many common case industry and academic graphs have recently been reported to fit comfortably in the RAM of a single high-end server <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>, given the availability and affordability of high memory density (for example, a quad-socket Intel Xeon machine with 1.5TB RAM costs about $35K as of 2017 <ref type="bibr" target="#b24">[25]</ref>). Consequently, single-machine in-memory CPU platform has become an attractive choice for many common-case graph analytics scenarios. As opposed to distributed systems, a scale-up bigmemory system does not need to consider the challenging task of graph partitioning among nodes and avoids network communication overhead. Programming frameworks developed for this kind of platform in both academia <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> and industry <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b27">[28]</ref> have shown excellent performance while significantly reducing the programming efforts compared to distributed systems. Both out-of-core systems and accelerators require expensive pre-processing to prepare partitioned data structures to improve locality. Single-machine in-memory graph analytics can avoid costly pre-processing which has been shown to often consume more time than the algorithm execution itself <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. 0.0 -'--------L_ _---=-=-..:.::....:..::.::....:==-=-_ _----l_ Figure <ref type="figure">1</ref>: Cycle stack of PageRank on orkut dataset However, the performance of single-machine in-memory graph analytics is bounded by the inefficiencies in the memory subsystem, making the cores stall as they wait for data to be fetched from the DRAM. As shown in the cycle stack <ref type="bibr" target="#b30">[31]</ref> in Fig. <ref type="figure">1</ref> for one of the benchmarks used in our evaluation, 45% of the cycles are DRAM-bound stall cycles, IEEE ~computer society whereas the core is fully utilized without stalling in only 15% of the cycles.</p><p>The goal of this paper is to solve the memory inefficiency issue for single-machine in-memory graph analytics. We adopt a two-phase approach to achieve our goal. In the first phase, we develop an in-depth understanding of the memory-bound behavior observed in Fig. <ref type="figure">1</ref> by characterizing two features on a simulated multi-core architecture: (1) the memory-level parallelism (MLP) <ref type="bibr" target="#b31">[32]</ref> in an out-of-order (000) core and (2) the request reuse distance in cache hierarchy. In the second phase, we use the findings from our characterization to propose an architecture design for an application-specific prefetcher.</p><p>In the first phase, we extend or fill the gaps in prior characterization work <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b36">[37]</ref> in two aspects. First, we perform a data-aware profiling which provides clearer guidelines on the management of specific data types for performance optimization. Second, with the flexibility of a simulated platform, we vary the instruction window and cache configuration design parameters to explicitly explore their performance sensitivity. Beyond prior profiling work, our key findings include:</p><p>? Load-load dependency chains that involve specific application data types, rather than the instruction window size limitation, make up the key bottleneck in achieving a high MLP.</p><p>? Different graph data types exhibit heterogeneous reuse distances. The architectural consequences are (1) the private L2 cache shows negligible impact on improving system performance, (2) the shared L3 cache shows higher performance sensitivity, and (3) the graph property data type benefits the most from a larger shared L3 cache.</p><p>In the second phase, we use the guidelines from our characterization to design DROPLET, a Data-awaB:e decQuPLed pr ?feIcher for graphs. DROPLET is a physically decoupled but functionally cooperative prefetcher co-located at the L2 cache and at the memory controller (MC). We adopt a decoupled design to overcome the serialization due to the dependency between different graph data types. Moreover, DROPLET is data-aware because it prefetches different graph data types differently according to their intrinsic reuse distances. DROPLET achieves 19%-102% performance improvement over a no-prefetch baseline, 9%-74% performance improvement over a conventional stream prefetcher, 14%-74% performance improvement over a Variable Length Delta Prefetcher (VLDP) <ref type="bibr" target="#b37">[38]</ref>, and 19%-115% performance improvement over a delta correlation prefetcher implemented as a global history buffer (GHB) <ref type="bibr" target="#b38">[39]</ref>. DROP-LET performs 4%-12.5% better than a monolithic Ll prefetcher similar to the state-of-the-art prefetcher for graphs <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>In this section, we first discuss the graph data layout and the data type terminology used in the rest of the paper. Next, we briefly review the concepts of MLP, caches, and prefetchers, which are the three latency tolerance techniques addressed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph Data Types and Data Layout</head><p>One of the most widely used graph data layouts is the Compressed Sparse Row (CSR) representation because of its efficient memory space usage. As shown in Fig. <ref type="figure" target="#fig_2">2</ref>, the CSR format consists of three main components: the offset pointers, the neighbor IDs, and the vertex data. Each entry in the offset pointer array belongs to a unique vertex V and points to the start of the list of V's neighbors in the neighbor ID array. In the case of weighted graphs, each entry in the neighbor ID array also includes the weight of the corresponding edge. The vertex data array stores the property of each vertex and is indexed by the vertex ID. In the rest of the paper, we use the following terminology:</p><p>? Structure data: the neighbor ID array.</p><p>? Property data: the vertex data array.</p><p>? Intermediate data: any other data. Property data is indirectly indexed using information from the structure data. Using the example in Fig. <ref type="figure" target="#fig_2">2</ref>, to find the property of the neighbors of vertex 6, the structure data is first accessed to obtain the neighbor IDs (59 and 78), which in turn are used to index the property data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Latency Tolerance in Modem CPUs</head><p>Three key latency tolerance techniques used in modern CPUs are MLP arising from 000 execution, on-chip caches, and prefetching. First, 000 execution relies on the reorder buffer (ROB) to look ahead down the instruction stream and it can support multiple in-flight memory requests with the help of load/store queues, non-blocking caches, and parallelism in the DRAM. For memory-intensive applications, a larger instruction window exposes more MLP and improves performance by overlapping the long latencies of multiple DRAM requests <ref type="bibr" target="#b40">[41]</ref>. Second, caches reduce offchip access latencies by locating data closer to the core to leverage spatial and temporal locality. A typical modern cache hierarchy consists of multiple levels of both private and shared caches, which makes up about 50% of the total chip area <ref type="bibr" target="#b41">[42]</ref>. Third, hardware prefetchers located in the cache hierarchy monitor simple access patterns, such as streams or strides with low overhead, and fetch data into the caches ahead of the demand access <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENT SETUP</head><p>In this section, we present the experimental platform and the benchmark used for our characterization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Profiling Platform</head><p>Profiling experiments have been done using SNIPER simulator, an x86 simulator based on the interval simulation model <ref type="bibr" target="#b43">[44]</ref>. We selected SNIPER over other simulators because it has been validated against Intel Xeon X7460 Dunnington <ref type="bibr" target="#b43">[44]</ref> and, with an enhanced core model, against Intel Xeon X5550 Nehalem <ref type="bibr" target="#b44">[45]</ref>. Moreover, a recent study of x86 simulators simulating the Haswell microarchitecture has shown that SNIPER has the least error when validated against a real machine <ref type="bibr" target="#b45">[46]</ref>. Cache access timings for different cache capacities were extracted using CACTI <ref type="bibr" target="#b46">[47]</ref>. The baseline architecture is described in Table <ref type="table" target="#tab_0">I</ref>. We used fewer cores than typically present in a server node because previous profiling work has shown that resource utilization for parallel and single-core executions are similar <ref type="bibr" target="#b32">[33]</ref>. Hence, we do not expect the number of cores to change our observations. We marked the region of interest (ROI) in the application code. We ran the graph reading portion in cache warm-up mode and, upon entering the ROI, collected statistics for 600 million instructions across all the cores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmark</head><p>We use the GAP benchmark <ref type="bibr" target="#b47">[48]</ref> which consists of optimized multi-threaded C++ implementations of some of the most representative algorithms in graph analytics. For our profiling, we select GAP over a software framework to rule out any framework-related performance overheads and extract the true hardware bottlenecks l . We use five algorithms from GAP, which are summarized in Table <ref type="table">n</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A summary of the datasets is shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CHARACTERIZATION</head><p>To understand the memory-bound behavior in Fig. <ref type="figure">1</ref>, we analyze both the core and the cache hierarchy <ref type="bibr" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Analysis of the Core and the MLP</head><p>Observation#l: Instruction window size is not the factor impeding MLP. In general, a larger instruction window improves the hardware capability of utilizing more MLP for a memory-intensive application <ref type="bibr" target="#b40">[41]</ref>. In addition, previous profiling work on a real machine concludes that the ROB size is the bottleneck in achieving a high MLP for graph analytics workloads <ref type="bibr" target="#b32">[33]</ref>. However, by changing the design parameters in our simulator-based profiling, we observe that even a 4X larger instruction window fails to expose more MLP. As shown in Fig. <ref type="figure" target="#fig_3">3a</ref>, for a 4X instruction window, the average increase in memory bandwidth utilization is only 2.7%. Fig. <ref type="figure" target="#fig_3">3b</ref> shows the corresponding speedup. The average speedup is only 1.44%, which is very small for the large amount of allotted instruction window resources. <ref type="bibr" target="#b29">30</ref> " "" 1.2 "</p><p>BC BFS' PR :SSSP: CC</p><p>(5</p><formula xml:id="formula_0">BC BFS PR 'SSSP' CC - ~1.1 Q) 20 Q) ;:g' E'</formula><p>llJ ~,!l!1.0 ~~10 oS 0:: .S!</p><p>?~o.9</p><p>Om 0.</p><p>~:::l , f " ' l 1l l l i ~f ' ! f r ' l $ r ! ! j l ~! j l l ! ! ; : r l l l ~i ' l j l 1 L~==~=====,:!o===+-&lt;= 'ffi '5 '0 '0 &lt;= 'ffi '5 '0 '0 &lt;= 'ffi '5 '0 '0 , &lt;= 'ffi '5 '0 '0 &lt;= 'ffi '5 '0 '0 &lt;= , e ? -1" '" &lt;= e ? -1" '" &lt;= e ? -1" ' " &lt;= , ] ? -1" '" &lt;= e ? -1" '" &lt;= '" shows that, on average, graph structure data is mostly a producer (41.4%) rather than a consumer (6%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis of the Cache Hierarchy</head><p>Observationtt4: The private L2 cache shows negligible performance sensitivity, whereas the shared ILC shows higher performance sensitivity. As shown in Fig. <ref type="figure">4a</ref>, we vary the LLC size from 8MB to 64MB and find the optimal point of 17.4% (max 3.25X) performance improvement for a 4X increase in the LLC capacity. The mean LLC MPKI (misses per kilo instructions) is reduced from 20 in the baseline to 16 (16MB) to 12 (32MB) to 10 (64MB). The corresponding speedups are 7%, 17.4%, and 7.6%. The optimal point is a balance between a reduced miss rate and a larger LLC access latency. by an inherent application-level dependency characteristic. For every load, we track its dependency backward in the ROB until we reach an older load instruction. We call the older load a producer load and the younger load a consumer load. We find that short prOducer-consumer load dependency chains are inherent in graph processing and can be a serious bottleneck in achieving a high MLP even for a larger ROB. The two loads cannot be parallelized as they are constrained by true data dependencies and have to be executed in program order. Fig. <ref type="figure">5</ref> shows that, on average, 43.2% of the loads are part of a dependency chain with an average chain length of only 2.5, where we define chain length as the number of instructions in the dependency chain. Figure <ref type="figure">5</ref>: load-load dependency in ROB Observatio1l#3: Graph property data is the consumer in a dependency chain. To identify the position of each application data type in the observed load-load dependency chains, we show the breakdown of producer and consumer loads by data type in Fig. <ref type="figure" target="#fig_6">6</ref>. On average, we find that the graph property data is mostly a consumer (53.6%) rather than a producer (5.9%). Issuing graph property data loads is delayed and cannot be parallelized because it has to depend on a producer load for its address calculation.   evicted from both the L2 and L3 caches. The fact that the reuse distance is beyond the servicing capability of the LLC explains why a larger LLC fails to significantly reduce the proportion of off-chip structure accesses in Fig. <ref type="figure">4c</ref>. On the other hand, most of the property data loads missed in the Ll cache cannot be serviced by the L2 cache but can be serviced by the LLC and the DRAM. Overall, the LLC is more useful in servicing property accesses rather than structure accesses. Thus, the property cacheline has a comparatively smaller reuse distance that is still larger than that captured by the L2 cache. Finally, Fig. <ref type="figure">7</ref>  </p><formula xml:id="formula_1">60 BC PR SSSP: CC 4 ' E, &lt;= _50 3.l1 i ~40 . -g .230 ? 2 "5 ! ~20 1 [ ~20. 10 "C ' 0?' " _ 0 _ chain</formula><formula xml:id="formula_2">'1J QI 20 E 0 -'Y--Y-'T'--'T''-Y''-r'-'1r'-Y-Y-'T'i,-Y-'r'-'r'-?-Y''1r'-Y-Y-'..-'-'-i'i'T'--Y-'r'-Y'-Y-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IDDRAM_L3C!L2DL11</head><p>?ery low at 10.6% In the baselIne) Increases to only 15.3% after a 2X increase in the capacity while a 4X increase in set associativity has no impact (hit rate rises to only 10.9%). Fig. <ref type="figure">4b</ref>(ii) shows that the system performance exhibits little sensitivity to different L2 cache configurations (in both capacity and set associativity). The leftmost bar represents an architecture with no private L2 caches and no slowdown compared to a 256KB cache. Therefore, an architecture without private L2 caches is just as fine for graph processing.</p><p>Observationl/5: Property data is the primary beneficiary of LLC capacity. To understand which data type benefits from a larger LLC, Fig. <ref type="figure">4c</ref> shows, for each data type, the percentage of memory references that ends up getting data from the DRAM. We observe that the most reduction in the number of off-chip accesses comes from the property data. Structure and intermediate data benefit negligibly from a higher L3 capacity. Intermediate data is already accessed mostly in on-chip caches since only 1.9% of the accesses to this data type is DRAM-bound in the baseline. On the other hand, structure data has a higher percentage of offchip accesses (7.5%), which remains mostly irresponsive to a larger LLC capacity.</p><p>Observationll6: Graph structure cacheline has the largest reuse distance among all the data types. Graph property cacheline has a larger reuse distance than that serviced by the ?2 cache. To further understand the different performance sensitivities of the L2 and L3 caches, we break down the memory hierarchy usage by application data type as shown in Fig. <ref type="figure">7</ref>. In most benchmarks, accesses to the structure data are serviced by the Ll cache and the DRAM, which indicates that a cacheline missed in Ll is one that was referenced in the distant past such that it has been memory requests and shows negligible benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Summary and Opportunities</head><p>The memory-bound stalling behavior in graph analytics arises due to two issues: ? Heterogeneous reuse distances of different data types lead to intensive DRAM accesses to retrieve structure and property data. ? Low MLP due to load-load dependency chains, limiting the possibility of overlapping DRAM accesses. Our profiling motivates the adoption of prefetching as a latency tolerance technique for graph analytics. A good prefetcher can fix the first issue by locating data in on-chip caches ahead of the demand accesses. Prefetching can also bring an additional benefit by mitigating the effect of low MLP. A dependency chain with a consumer property data means serialization in the address calculation and a delay in issuing the property data load. However, once issued, prefetching ensures that the property data hits on-chip, mitigating low-MLP induced serially exposed latency. However, due to heterogeneous reuse distances of the different graph data types, it is challenging to employ simple hardware prefetchers that can efficiently prefetch all data types. To address this challenge, we leverage our observations from the profiling to make prefetch decisions for DROPLET architecture, as summarized in Table <ref type="table" target="#tab_2">IV</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DROPLET ARCHITECTURE</head><p>In this section, we first provide an overview of our proposed prefetcher. Next, we discuss the detailed architecture design of the components of DROPLET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview and Design Choice</head><p>We propose a physically decoupled but functionally cooperative prefetcher co-located at the L2 cache level and at the Me. Specifically, DROPLET consists of two dataaware components: the L2 streamer for prefetching structure data (Fig. <ref type="figure" target="#fig_8">8 0</ref>) and the memory controller based property prefetcher (MPP) (Fig. <ref type="figure" target="#fig_8">8 8</ref>) which is guided by the structure stream requests from the L2 streamer. Fig. <ref type="figure" target="#fig_8">8</ref> shows the entire prefetching flow. The blue path shows the flow of prefetching the structure data using the data-aware L2 streamer. The stream prefetcher is triggered only by structure data and the structure prefetch request is almost always guaranteed to go to the DRAM as observed in our profiling. On the refill path from the DRAM, the prefetched structure cacheline is transferred all the way to L2. Also, a copy of the structure data is forwarded to the MPP, which triggers the property data prefetching. The green path shows the prefetching flow of the property data. The MPP reacts to the prefetched structure cacheline by scanning it for the neighbor node IDs, computing the virtual addresses of the target property prefetches using those neighbor IDs, and performing virtualphysical address translation. To avoid unnecessary DRAM 378 accesses for property data that may already be on-chip, the physical prefetch address is used to check the coherence engine. If not on-chip, the property prefetch address is queued in the MC. Upon being serviced by the DRAM, the cacheline is sent to the LLC and the private L2 cache. Instead, if the to-be-prefetched property cacheline is detected to be on-chip, it is further copied from the inclusive LLC into the private L2 cache.  <ref type="table" target="#tab_2">IV</ref>. The L2 cache is the location in which the L2 streamer brings in structure prefetches and the MPP sends property prefetches. Despite this fact, we design a decoupled property prefetcher in the MC in order to break the serialization arising from consumer property data in a dependency chain. A single prefetcher at the L2 cache would have to wait until the structure prefetch returns to the L2 cache before the property address can be calculated and issued for prefetching. By decoupling the prefetcher, the property address can be calculated as soon as the producer structure prefetch arrives on the chip at the Me. This overlaps the return of the structure prefetch on the refill path through the caches and the issuing of the subsequent property prefetch, breaking the serialization. A previous work, which dynamically ojJloads and accelerates dependent load chains in the MC, shows a 20% lower latency when a dependent load is issued from the MC rather than from the core side <ref type="bibr" target="#b51">[52]</ref>. We use this insight in our prefetching scheme to achieve better timeliness by aggressively issuing property prefetch requests directly from the MC.</p><formula xml:id="formula_3">struct req! - struct_dat prop_dat</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. L2 Streamer for Structure Data</head><p>First, we discuss the difficulties posed by a conventional streamer for graph workloads. Next, to address these challenges, we propose the design of a data-aware L2 streamer.</p><p>1) Shortcomings of a Conventional Streamer: A conventional L2 streamer, as shown in Fig. <ref type="figure" target="#fig_10">9</ref>(a), can snoop all cacheblock addresses missed in the Ll cache and can track multiple streams, each with its own tracker (tracking address) <ref type="bibr" target="#b52">[53]</ref>. Once a tracker has been allocated, the stream where base is the base virtual address of the property array and the granularity of the property data is 4B. The prefetched data in the L2 cache.</p><p>In contrast to a conventional streamer, as shown in Fig. <ref type="figure" target="#fig_10">9</ref>(a), our design contains two fundamental modifications guided by our observations in Section IV. First, our dataaware L2 streamer is triggered only by structure data to overcome the shortcomings of a conventional streamer mentioned in Section V-BI. Second, as opposed to a conventional L2 streamer that buffers the target prefetch addresses in the L2 request queue, we buffer them in the L3 request queue. This design choice is guided by our observation that new structure cachelines are serviced by a lower level in the memory hierarchy. Later in Section VII, we quantitatively analyze the superiority of our data-aware structure streamer over the conventional design in terms of both performance and bandwidth savings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Memory Controller (MC)</head><p>First, the memory request buffer (MRB) is used to give the MC the knowledge of (1) whether a cacheline coming from the DRAM corresponds to a structure prefetch and (2) which core sent the structure prefetch request. Second, we incorporate the MPP unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Memory Request Buffer (MRB):</head><p>When cachelines are on their refill path from the DRAM, the MC needs to filter the ones corresponding to structure prefetches to transfer their copies to the MPP for property prefetching. To enable this, we reinterpret the C-bit (criticality bit) in the baseline MRB, as shown in Fig. <ref type="figure" target="#fig_11">10</ref> (in the lower left comer), which differentiates a prefetch request from a demand request for priority-based scheduling purposes in modem MCs <ref type="bibr" target="#b53">[54]</ref>. If the C-bit is set, in addition to indicating a prefetch request, it specifically indicates a structure prefetch since it comes from the L2 streamer, which only sends structure prefetch requests. We also add a core ID field to give the MPP the knowledge of which core sent the structure prefetch request so that it can later send the property prefetches into that core's private L2 cache.</p><p>2) MC based Property Pre/etcher (MPP): As shown in Fig. <ref type="figure" target="#fig_11">10</ref>, the MPP consists of a property address generator (PAG), a FIFO virtual address buffer (VAB), a near-memory TLB (MTLB), and a FIFO physical address buffer (PAB). If a cacheline from the DRAM is detected to be a structure prefetch (see Section V-CI), its copy is transferred to the MPP.</p><p>The prefetched structure cacheline is propagated to the PAG which is shown in detail in Fig. <ref type="figure" target="#fig_11">10</ref>. The PAG scans the cacheline in parallel for neighbor IDs that are indices to the property data array. To calculate the target virtual address of the property prefetch from a scanned neighbor ID, we use the following equation: property address = base + 4 x neighbor ID <ref type="bibr" target="#b0">(1)</ref> requires two additional miss addresses to confirm a stream before starting to prefetch <ref type="bibr" target="#b52">[53]</ref>. In graph processing, such a streamer is detrimental to both structure and property data in terms of prefetch accuracy and coverage. First, due to the streamer's ability to snoop all LI miss addresses, many trackers in the streamer get wastefully assigned to track pages corresponding to the property and the intermediate data types. These trackers are wasteful due to one of the two reasons: (1) often, a stream is not successfully detected due to large reuse distances, which results in small property and intermediate prefetch volumes and low coverage; (2) due to random accesses, these trackers get the wrong signals, i.e., random streams are detected, leading to mostly useless prefetches (low prefetch accuracy). The direct consequence of property and intermediate data trackers is the reduction in the effective number of trackers available for structure data (the data type which actually shows stream behavior) at any given time. This reduces the volume of structure prefetches and the coverage for structure data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Data-aware Streamer in DROPLET:</head><p>To address the problems of a conventional streamer, our proposed streamer is data-aware, i.e., it operates only on structure data to bring in a high volume of accurate structure prefetches. Fig. <ref type="figure" target="#fig_10">9(b)</ref> shows how this is achieved. We use a variant of malloc to allocate graph structure data (see Section VI), which updates page table entries with an extra bit ("I"=structure data). Upon address translation at the LID cache, this extra bit in the TLB entry (Fig. <ref type="figure" target="#fig_10">9(b) 0</ref>) is copied to the LID cache controller. When an access misses in the LID cache, the LID controller buffers this extra bit, together with the miss address, into the L2 request queue. Hence, the L2 request queue is augmented with an additional bit (Fig. <ref type="figure" target="#fig_10">9(b) @</ref>) to identify whether a request address corresponds to the structure data. The L2 cache controller in our design preferentially guides into the streamer only the addresses in the request queue which belong to structure data (as recognized by the extra bit being "I"). Another input to the streamer is a feedback from the L2 cache. When a L2 hit occurs to an address corresponding to structure data, the address is sent to the streamer. Finally, the L2 streamer buffers its target prefetch addresses in the L3 request queue and stores the PAG receives two pieces of information from the software (see Section VI): (1) the base in Equation 1 and ( <ref type="formula">2</ref>) the granularity at which the structure cacheline needs to be scanned (4B for unweighted graphs and 8B for weighted graphs). One structure cacheline can generate 8 and 16 property addresses per cycle for weighted and unweighted graphs, respectively.</p><p>The virtual addresses generated by the PAG, together with the corresponding core ill, are buffered in the VAB for translation. The translation happens through a small MTLB (see Section V-C3), upon which the resulting physical address is buffered in the PAB. The physical address is then used to check the coherence engine and determine the subsequent prefetch path of the property data, as described in Section V-A.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Virtual Address Translation in MTLB:</head><p>The MTLB caches mappings of the property data pages to allow virtualphysical translation of the property prefetch addresses in the MPP. A MTLB miss is handled by page-walking, but a property prefetch address encountering a page fault is simply dropped. Like core-side TLBs, the MTLB should also be involved in the TLB shootdown process so that the MPP does not continue using stale mappings to prefetch wrong data. To optimize the coherency maintenance between the core-side TLBs and the MTLB, and to reduce the number of invalidations to the MTLB, we leverage (1) the extra bit in the TLBs used to differentiate between structure and nonstructure data and (2) the fact that the MTLB caches only property mappings. Due to these two features, during a TLB shootdown, MTLB entries are invalidated using only the core-side TLB invalidations for entries that have an extra bit with the value "0" (indicating non-structure data).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hardware Overhead</head><p>DROPLET incurs negligible area overhead. The dataaware streamer in DROPLET requires storing extra information in the page table and the L2 request queue entries. In the hierarchical paging scheme in x86-64, each paging structure consists of 512 64-bit entries, leading to a 4KB storage <ref type="bibr" target="#b54">[55]</ref>. The addition of an extra bit in each page table entry to identify structure data results in a 64B (1.56%) storage overhead in the paging structure. Assuming a 32entry L2 request queue <ref type="bibr" target="#b55">[56]</ref>, with each entry containing the miss address and the status <ref type="bibr" target="#b56">[57]</ref>, the addition of an extra bit results in a 4B (1.54%) queue storage overhead. In addition, property prefetching in DROPLET introduces the MPP and requires additional storage in the MRB. Using McPAT integrated with SNIPER, we find the area of the baseline chip to be 188 mm 2 in a 45nm technology node. The area of the MPP, with its parameters shown in Table <ref type="table" target="#tab_6">V</ref>, is 0.0654 mm 2 , resulting in a 0.0348% area overhead with respect to the entire chip. With a 7.7KB storage, the VAB, the PAB, and the MTLB comprise 95.5% of the total MPP area. For the MRB, the additional storage required for the core ID field for our quad-core system is only 64B, if assuming a 256-entry MRB. VI. DISCUSSION  System support for address identification. DROPLET is data-aware in two aspects. First, the L2 streamer is only triggered by structure data. Second, the MPP prefetches only property data with the knowledge of the base address of the property array and the scan granularity of the structure cacheline. To obtain this information, DROPLET needs help from the operating system and the graph data allocation layer of graph processing frameworks. Similar to prior work <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, we develop a specialized maUoe function which can label all allocated pages with an extra bit in the page table to help identify structure data during address translation at the LID cache (the extra bit being "I" indicates structure data). The specialized maUoe function can also pass in some information to be written into the hardware registers in the MPP. When allocating the structure data, the maUoe function writes the scan granularity of the structure cacheline in the MPP. The maUoe function for the property data writes the base address of the property array in the MPP. We add a special store instruction to provide an interface for the processor to write these two registers in the MPP 2. User Interface. DROPLET requires the above maUoe modification only at the framework level and is transparent to user source code. This is because, in the system stack for graph processing frameworks, the layer of graph data allocation and management (the layer at which we introduce a specialized maUoe) is separate from user code <ref type="bibr" target="#b58">[59]</ref>. Hence, our modification can be handled "behind the scenes" by the data management layer without modifying the API for users. Applicability to diITerent programming models and data layouts. In this work, we assume a CSR data layout and a programming abstraction focused on the vertex (see Section II-A). DROPLET is easily extensible to other abstractions because a common aspect across most abstractions is that they maintain data in two distinct parts which can be mapped to our data type terminology (structure and property 2The configuration registers of the MPP are part of the context and are backed up and restored when switching between graph analytics processes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. lla and lIb</head><p>show the performance improvement of the six configurations normalized to a no-prefetch baseline. Among all of them, DROPLET provides the best performance for CC (102%), PR (30%), BC (19%), and SSSP (32%). In these four algorithms, the only exception is the road dataset (CC-road, PR-road, and SSSP-road) where streamMPPI is the best performer (see Section VII-Cl). For</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prefetch Experiment Setup</head><p>Beyond Table <ref type="table" target="#tab_0">I</ref>, the features of the prefetchers used for evaluation are shown in Table <ref type="table" target="#tab_6">V</ref>. Evaluation is performed for six prefetcher configurations listed below. GHB <ref type="bibr" target="#b38">[39]</ref>: This is a GIDC (GloballDelta Correlation) L2 prefetcher. VWP <ref type="bibr" target="#b37">[38]</ref>: This is a L2 prefetcher which, unlike GHB, can use longer and variable delta histories to make more accurate prefetch predictions. stream: This is the conventional L2 streamer which can snoop all Ll miss addresses. streamMPPl: This is a conventional L2 streamer together with a MPP. This configuration shows the benefit of letting property prefetching be guided by structure streaming. However, since the L2 streamer is not data-aware, relying only on the C-bit in the MRB to identify structure data does not work. So, we use MPPI which is equivalent to MPP equipped with the ability to recognize structure data. DROPLET: Unlike stream and strearnMPPl, the L2 streamer is structure-only. When compared to streamMPP1, DROPLET shows the additional benefit by restricting the streamer to work on structure data only. monoDROPLETLl: This prefetcher is a data-aware streamer + MPP1 3 implemented monolithically at the Ll cache, an arrangement close to the state-of-the-art graph prefetching proposition of Ainsworth et al. <ref type="bibr" target="#b39">[40]</ref>. Although <ref type="bibr" target="#b39">[40]</ref> uses a worklist-triggered recursive prefetcher instead of a dataaware streamer + MPP1, monoDROPLETLI imitates their design philosophies of 1) a monolithic prefetcher and 2) bringing all prefetches into the Ll cache. Hence, we select this design point to evaluate if DROPLET provides any performance benefit over a <ref type="bibr" target="#b39">[40]</ref>-like monolithic Ll prefetcher. data). For example, in edge-centric graph processing <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b28">[29]</ref>, structure data would be equivalent to a sorted or unsorted edge array which is typically streamed in from slower to faster memory <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b28">[29]</ref>. DROPLET can prefetch these edge streams and use them to trigger a MPP located near the slower memory to prefetch property data. Moreover, DROPLET can easily handle multi-property graphs by using the information from prefetched structure data to index one property array (in case the property data layout is an array of vectors) or multiple property arrays (in the case of separate arrays for each property). Applicability to larger graphs. DROPLET is based on the observations obtained from profiling graph processing workloads on a simulated multi-core architecture. As opposed to a real machine, a simulated platform provides much higher flexibility for data-aware profiling and for studying the explicit performance sensitivities of the instruction window and the cache hierarchy. However, the trade-off is the restriction to somewhat small datasets to achieve manageable simulation times. However, our profiling conclusions (hence the effectiveness of our proposed DROPLET design) still hold for larger graphs because of two aspects. First, we use small datasets that are still larger than that fully captured by the on-chip caches, which stresses the memory subsystem sufficiently. Second, we explain the observed architecture bottlenecks in terms of inherent data type features such as dependency characteristics and reuse distances, which are independent of the data size. Multiple MCs. Our experimental platform consists of a single MC in a quad-core system. However, platforms with more cores may contain multiple MCs and data may be interleaved across DRAM channels connected to different MCs. It is possible that a property prefetch address generated by the MPP of one MC may actually be located in a DRAM channel connected to another MC. In such a case, we adopt a technique similar to prior work <ref type="bibr" target="#b51">[52]</ref>. The property prefetch request, together with the core ill, is directly sent to the MRB of the destination MC for prefetching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DROPLET EVALUATION</head><p>In this section, we discuss the experimental setup and the evaluation results for DROPLET. BFS, DROPLET provides a 26% performance improvement, but the best prefetcher is streamMPPI with an average improvement of 36% (see Section VII-Cl). DROPLET could easily be extended to adaptively turn off the streamer's dataawareness to convert it into the streamMPPI design. In that case, our design would be no worse than streamMPPI for BFS and the road dataset.</p><p>Compared to a conventional stream prefetcher, DROP-LET provides performance improvements of 74% for CC, 9% for PR, 19% for BC, and 16.8% for SSSP. These improvements come from both the MPP and the dataaware streamer, as shown by the progressive speedup from stream to DROPLET. For BFS, streamMPPI outperforms the conventional streamer by 5.4% but DROPLET slightly degrades performance by 2.3%.</p><p>Compared to monoDROPLETLI (close to <ref type="bibr" target="#b39">[40]</ref>), DROP-LET provides performance improvements of 11% for CC, 4% for PR, 6% for BC, 12.5% for BFS, and 4% for SSSP. DROPLET differs from monoDROPLETLl by 1) adopting a physically decoupled design and 2) prefetching into the L2 instead of the L1. DROPLET performs better because it achieves better prefetch timeliness by decoupling the structure and property prefetehing (Section V-A). The benefit of decoupling is more so because computation per memory access is very low in graph processing. In addition, DROP-LET does not pollute the more useful Ll cache. Instead, it prefetches into the L2 cache which is poorly utilized in graph processing (Section IV-B). In addition to the performance benefit, DROPLET offers two critical advantages over <ref type="bibr" target="#b39">[40]</ref> in terms of practicality and simplicity. First, <ref type="bibr" target="#b39">[40]</ref> triggers its prefeteher with an implementation specific data structure called the worklist. However, many all-active algorithms are the simplest to implement without having to maintain a worklist <ref type="bibr" target="#b47">[48]</ref>. For <ref type="bibr" target="#b39">[40]</ref>, these algorithms have to re-written to integrate a worklist data structure. In contrast, DROPLET does not depend on any worklist, leverages the intrinsic reuse distance feature of graph structure data to trigger the prefetcher, and introduces a special maUoc which is transparent to user code (Section VI). Second, unlike <ref type="bibr" target="#b39">[40]</ref>, we do not require additional hardware to ensure prefetch timeliness.</p><p>We achieve timeliness by using a decoupled design to break the serialization between structure and property data.</p><p>DROPLET outperforms GIDC GHB by 115% for CC, 35% for PR, 23% for BC, 19% for BFS, and 31% for SSSP. GHB is overall the least performing prefetcher in our evaluation. This is because it is hard to identify consistent correlation patterns in graph processing due to the heterogeneous reuse distances of different data types.</p><p>Compared to VLDP, DROPLET provides performance improvements of74% for CC, 23% for PR, 14% for BC, and 20% for SSSP. For BFS, streamMPPI outperforms VLDP by 6% but DROPLET slightly degrades performance by 1.6%. On average, the performance improvement of DROPLET compared to VLDP is similar to its improvement compared to the conventional stream prefeteher. Due to complex reuse distances of different data types, delta histories may not always make effective prefetch predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Explaining DROPLET's Performance</head><p>In this section, we zoom in on DROPLET to demystify its performance benefits. Since DROPLET is an enhancement upon the L2 streamer to include a data-aware streamer and a MPP, we compare to stream and streamMPPI configurations to show the additional effect of each component. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>~20</head><p>Figure <ref type="figure" target="#fig_2">12</ref>: L2 cache performance. The set of numbers shows the average for the three prefetch configurations across the five datasets.</p><p>1) Benefit to Demand Accesses: L2 cache performance: Fig. <ref type="figure" target="#fig_2">12</ref> shows that DROPLET converts the seriously underutilized L2 cache (Fig. <ref type="figure">4b</ref>) into a higher performance resource by increasing the L2 hit rate to 62%, 76%, 14%,38%, and 50% for CC, PR, BC 4 , BFS, and SSSP, respectively. The L2 hit rate for DROPLET is the highest on average for CC, PR, BC, and SSSP. Fig. <ref type="figure" target="#fig_2">12</ref> also explains why streamMPPI is the ideal prefetcher for the road dataset and most BFS benchmarks. For these benchmarks, the conventional streamer provides a comparatively higher cache performance, indicating that the streamer may also be effective at capturing some property prefetches. Hence, by making the streamer structure-only in DROPLET, we lose the streamer-induced VIII. RELATED WORK Graph characterization. Prior characterization on real hardware <ref type="bibr" target="#b32">[33]</ref>- <ref type="bibr" target="#b36">[37]</ref> provides insights such as high cache miss rates <ref type="bibr" target="#b34">[35]</ref>, under-utilization of memory bandwidth <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, and limited instruction window size <ref type="bibr" target="#b32">[33]</ref>. Our profiling fills in the gaps in previous work through its data-aware feature and an explicit performance sensitivity analysis of the instruction window and the caches.  For structure data, DROPLET has the highest prefetch accuracy for all the algorithms. The accuracies are 100% for CC, 95% for PR, 53% for BC, 66% for BFS, and 64% for SSSP. For property data, DROPLET's prefetch accuracy is the highest for CC (94%), PR (95%), BC (46%), and SSSP (70%). For BFS, the property prefetch accuracy for DROPLET (47%) is lower than that of stream (70%). Overall, CC and PR have a higher prefetch accuracy for structure and property data than BC, BFS, and SSSP. This is because the former two algorithms process vertices in a very sequential order. On the other hand, the BC, BFS, and SSSP algorithms depend on intermediate data structures, such as bins or worklists, when processing vertices. Consequently, the access pattern for structure data in BC, BFS, and SSSP consists of random starting points from which data can start being streamed.</p><p>2) Overheads of Prefetching: One of the side effects of prefetching is additional bandwidth consumption due to inaccurate prefetches, which may offset performance gains. DROPLET incurs low extra bandwidth consumption. Fig. <ref type="figure">15</ref> shows the extra bandwidth consumption for the three configurations measured in BPKI (bus accesses per kilo instructions). Compared to a no-prefetch baseline, DROPLET requires 6.5%, 7%, 11.3%, 19.9%, and 15.1% additional bandwidth for CC, PR, BC, BFS, and SSSP, respectively. The bandwidth requirement for CC and PR is smaller due to the comparatively higher structure and property prefetch accuracies.</p><p>property prefetches, leading to a lower L2 cache hit rate when using DROPLET over streamMPP1.</p><p>1_ baseline _ stream _ slreamMPP1 0 DROPLETI ~:</p><p>:</p><p>Figure <ref type="figure" target="#fig_3">13</ref>: Off-chip demand accesses Off-chip demand accesses: To understand how the two components of DROPLET (MPP and data-aware streamer) affect the off-chip demand accesses to structure and property data, Fig. <ref type="figure" target="#fig_3">13</ref> breaks down the demand MPKI from the LLC by data type. Below, we summarize our observations on the additive benefit of each prefetch configuration:</p><p>(1) Compared to the no-prefetch baseline, stream reduces structure demand MPKI in all cases (71.3%, 58.9%, 21.5%, 39.7%, and 44.4% for CC, PR, BC, BFS, and SSSP, respectively), but it mostly fails in significantly reducing the property demand MPKI because of its inability to capture such difficult-to-predict large reuse distances. The only exception is BFS, in which the property MPKI is reduced by 23%, corroborating the earlier observation that stream is comparatively better at capturing property prefetches for most BFS benchmarks.</p><p>(2) Compared to stream, streamMPPI significantly reduces the property MPKI for all algorithms (39.5%, 48.7%, 24.8%, 92.8%, and 41.4% for CC, PR, BC, BFS, and SSSP, respectively), but it does not significantly reduce the structure MPKI. The property MPKI reduction comes from the MPP, which follows structure prefetches to bring in a high volume of useful property prefetches. Structure MPKI does not benefit because the streamers in stream and streamMPPI are the same.</p><p>(3) Compared to streamMPPl, DROPLET further reduces structure demand MPKI (76.6%, 62.4%, 45.7%, 5.73%, and 41% for CC, PR, BC, BFS, and SSSP, respectively) because a structure-only streamer leads to a larger volume of structure prefetches by dedicating all the trackers to structure memory regions. Correspondingly, the property MPKI is also reduced since property prefetches accompany the structure prefetches (47.5%, 48.5%, 44.6%, 3.6%, and 39% for CC, PR, BC, BFS, and SSSP, respectively). However, we observe that the benefit for BFS is small compared to other algorithms. Hence, the small reduction in the LLC MPKI, together with a lower L2 cache performance, further explains why streamMPPI rather than DROPLET is the ideal prefetcher for the BFS algorithm. Prefetch accuracy: Fig. <ref type="figure" target="#fig_14">14</ref> shows the prefetch accuracies and to achieve aggressive and timely property prejetching (Section V-A). Note that acceleration and prefetching are two orthogonal techniques, and can be combined to gain benefit from both.</p><p>IX. CONCLUSION This paper analyzes and optimizes the memory hierarchy for single-machine in-memory graph analytics. Based on the workload characterization, we propose a novel architecture design for an application-specific prefetcher called DROP-LET, which is data-aware and prefetches graph structure and property data in a physically decoupled but functionally cooperative manner. Experimental results show that DROPLET can achieve high performance improvement over various previous work. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: CSR data layout for graphs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Increase in DRAM bandwidth utilization and (b) overall speedup from a 4X larger ROB Observation#2: Load-load dependency chains prevent achieving high MLP. To understand why a larger ROB does not improve MLP, we track the dependencies of the load instructions in the ROB and find that the MLP is bounded</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 6 :</head><label>46</label><figDesc>Figure4: (a) Sensitivity of i) L3 MPKI and ii) system performance to shared L3 cache size ?XIY)=access times for (tags/data) in cycles); (b) Sensitivity of (i) L2 cache hit rate and (ii) system performance to private L2 cache configurations (average across all benchmarks); (c) Effect of larger L3 cache on off-chip accesses of different data types (average across all benchmarks).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>.... -o-o1c-..... -o-oc-..... -o-oc-..... -o-oc-.... -ouc "'C ~~~e ~: . ? ~~e ~: E ~~e ~: ~~~e ~: E ~~e :E: : l o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6</head><label>6</label><figDesc>Figure5: load-load dependency in ROB Observatio1l#3: Graph property data is the consumer in a dependency chain. To identify the position of each application data type in the observed load-load dependency chains, we show the breakdown of producer and consumer loads by data type in Fig.6. On average, we find that the graph property data is mostly a consumer (53.6%) rather than a producer (5.9%). Issuing graph property data loads is delayed and cannot be parallelized because it has to depend on a producer load for its address calculation. Fig.6 also</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>'S "C "Cll: Cii '5 "C "CI l: iV 'S "C "C 1 c: Cii 'S "C "Cl c: Ri '5 "C 1:' c: ~~~e ~l~~~e ~:~~~e ~I~~~e ~:~~-g e '~: ::II.~:::I,.~:::II.~:::I,.~:::l .~~.~.~.F igure 7: Breakdown of memory hierarchy usageby application data type ~100 e.... 80 ~60 QI 40 go 20 a.0-' Y--Y-' T' --' r' -' r' -!y.&lt;,r' -Y-Y-' -r' i' T' -' ,-' --Y-' -r' -9~Y-' -r.1..Y-W,.u,.u.,.' -Y-Y-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Overview of DROPLET Our design choices are based on our profiling observations as summarized in TableIV. The L2 cache is the location in which the L2 streamer brings in structure prefetches and the MPP sends property prefetches. Despite this fact, we design a decoupled property prefetcher in the MC in order to break the serialization arising from consumer property data in a dependency chain. A single prefetcher at the L2 cache would have to wait until the structure prefetch returns to the L2 cache before the property address can be calculated and issued for prefetching. By decoupling the prefetcher, the property address can be calculated as soon as the producer structure prefetch arrives on the chip at the Me. This overlaps the return of the structure prefetch on the refill path through the caches and the issuing of the subsequent property prefetch, breaking the serialization. A previous work, which dynamically ojJloads and accelerates dependent load chains in the MC, shows a 20% lower latency when a dependent load is issued from the MC rather than from the core side<ref type="bibr" target="#b51">[52]</ref>. We use this insight in our prefetching scheme to achieve better timeliness by aggressively issuing property prefetch requests directly from the MC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>pre etc a resses ~o L~reg gu~e) (a) conventional L2 streamer (b) propose daa-aware streamer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: L2 streamer for prefetching structure data. Extra bits in purple and newly added cache controller decisionmaking in blue.2) Data-aware Streamer in DROPLET: To address the problems of a conventional streamer, our proposed streamer is data-aware, i.e., it operates only on structure data to bring in a high volume of accurate structure prefetches. Fig.9(b)shows how this is achieved. We use a variant of malloc to allocate graph structure data (see Section VI), which updates page table entries with an extra bit ("I"=structure data). Upon address translation at the LID cache, this extra bit</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: MC-based Property Prefetcher (MPP)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>DFigure 11 :</head><label>11</label><figDesc>Figure 11: (a) Performance improve~ent of the six configurations; (b) Performance summary for Fig. lla (each entry is the geomean of the speedups across all the five datasets).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Prefetch accuracy for the three prefetch configurations and the two data types.For structure data, DROPLET has the highest prefetch accuracy for all the algorithms. The accuracies are 100% for CC, 95% for PR, 53% for BC, 66% for BFS, and 64% for SSSP. For property data, DROPLET's prefetch accuracy is the highest for CC (94%), PR (95%), BC (46%), and SSSP (70%). For BFS, the property prefetch accuracy for DROPLET (47%) is lower than that of stream (70%). Overall, CC and PR have a higher prefetch accuracy for structure and property data than BC, BFS, and SSSP. This is because the former two algorithms process vertices in a very sequential order. On the other hand, the BC, BFS, and SSSP algorithms depend on intermediate data structures, such as bins or worklists, when processing vertices. Consequently, the access pattern for structure data in BC, BFS, and SSSP consists of random starting points from which data can start being streamed.2) Overheads of Prefetching: One of the side effects of prefetching is additional bandwidth consumption due to inaccurate prefetches, which may offset performance gains. DROPLET incurs low extra bandwidth consumption. Fig.15shows the extra bandwidth consumption for the three configurations measured in BPKI (bus accesses per kilo instructions). Compared to a no-prefetch baseline, DROPLET requires 6.5%, 7%, 11.3%, 19.9%, and 15.1% additional bandwidth for CC, PR, BC, BFS, and SSSP, respectively. The bandwidth requirement for CC and PR is smaller due to the comparatively higher structure and property prefetch accuracies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>Baseline architecture</figDesc><table><row><cell>core</cell><cell>4 cores, ROB = 128-entry, load queue = 48-entry, store queue =32-entry, reservation station entries = 36, dispatch width =issue width =commit width = 4, frequency =2.66GHz</cell></row><row><cell></cell><cell>3-level hierarchy, inclusive at all levels, writeback,</cell></row><row><cell>caches</cell><cell>least recently used (LRU) replacement policy, data and tags parallel access, 64B cacheline, separate LI</cell></row><row><cell></cell><cell>data and instruction caches</cell></row><row><cell>LlDlI cache</cell><cell>private, 32KB, 8-way set-associative, data access time =4 cycles, tag access time = I cycle</cell></row><row><cell>L2 cache</cell><cell>private, 256KB, 8-way set-associative, data access time =8 cycles, tag access time =3 cycles</cell></row><row><cell cols="2">L3 (LLC) cache shared, 8MB, 16-way set-associative, data access time =30 cycles, tag access time = 10 cycles</cell></row><row><cell>DRAM</cell><cell>DDR3, device access latency -45ns, queue delay modeled</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table ill</head><label>ill</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Table II: Algorithms</cell><cell></cell></row><row><cell>Algorithm</cell><cell></cell><cell>Description</cell><cell></cell><cell></cell></row><row><cell cols="2">Betweenness Centrality (BC)</cell><cell cols="3">Measure the centrality of a vertex, Le., the number of shortest paths between any two other nodes passing through it</cell></row><row><cell cols="4">Breadth First Search Traverse a graph level by level (BFS)</cell><cell></cell></row><row><cell>PageRank (PR)</cell><cell></cell><cell cols="3">Rank each vertex on the basis of the ranks of its neighbors</cell></row><row><cell cols="5">Single Source Shortest Find the minimum cost path from a source</cell></row><row><cell>Path (SSSP)</cell><cell></cell><cell cols="2">vertex to all other vertices</cell><cell></cell></row><row><cell>Connected</cell><cell></cell><cell cols="3">Decompose the graph into a set of connected</cell></row><row><cell>Components (CC)</cell><cell></cell><cell>subgraphs</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Table ill: Datasets</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">vertices edges</cell><cell>Size</cell><cell>Description</cell></row><row><cell>kron [48]</cell><cell>16.8M</cell><cell cols="2">260M 2. 1GB/2GB'</cell><cell>synthetic</cell></row><row><cell>urand [481</cell><cell>8.4M</cell><cell>134M</cell><cell>1.1GBI2.1GB</cell><cell>synthetic</cell></row><row><cell>orkut [50]</cell><cell>3M</cell><cell cols="3">117M 941MB/1.8GB social network</cell></row><row><cell cols="2">livejournal [501 4.8M</cell><cell cols="3">68.5M 597MB/1.1GB social network</cell></row><row><cell>road [48]</cell><cell>23.9M</cell><cell cols="3">57.7M 806MB/1.3GB mesh network</cell></row><row><cell cols="5">? Weighted graph is smaller due to generation from a smaller degree</cell></row><row><cell cols="3">for a manageable simulation time.</cell><cell></cell><cell></cell></row><row><cell>(size</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>unweighted/weighted).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table IV :</head><label>IV</label><figDesc>Prefeteh decisions based on profiling observations</figDesc><table><row><cell>BC</cell><cell>BFS</cell><cell>PR</cell><cell>SSSP</cell><cell>CC</cell></row><row><cell>~100</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-.! 60 80 .!!! 40</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>provides evidence that the accesses to intermediate data are mostly on-chip cache hits in the Ll cache and the LLC. The reuse distances of the three data types explain why the private L2 cache fails to service</figDesc><table><row><cell>Profiling Observation</cell><cell cols="2">Prefetch Design Answer Question</cell></row><row><cell>Negligible impact of L2 (Observationlt4).</cell><cell cols="2">In which cache L2 cache, because (1) no risk of cache pollution and (2) level to put prefetched data? technique to make the under-utilized L2 cache useful.</cell></row><row><cell cols="2">Intermediate data are mostly cached, struc-What to prefetch? ture/property data are not (Observationll6).</cell><cell>Structure and property data.</cell></row><row><cell>? Structure data exhibits a large reuse distance</cell><cell></cell><cell></cell></row><row><cell cols="2">with a pattern: a cacheline missed in the Ll cache is almost always serviced by the DRAM (Observation#6). ? Property data exhibits a randomly large reuse distance which is larger than the L2 stack depth, leading to heavy LLC and How to prefetch? DRAM accesses (Observation#6). ? In load-load dependency chains, property data is mostly the consumer, whereas struc-ture data is mostly the producer (Observa-</cell><cell>? Prefetch structure data from the DRAM in a streaming fashion. ? Prefetch property data using explicit address calculation due to difficult-to-predict large reuse distance. ? Let the calculation of target property prefetch addresses be guided by structure data. ? Address calculation of target property prefetches is a serial-ized process. Decoupling the prefetcher will help break the serialization.</cell></row><row><cell>tion#3).</cell><cell></cell><cell></cell></row><row><cell>Load-load dependency chains are short (Obser-vation#2).</cell><cell>When to prefetch?</cell><cell>If property prefetches are guided by structure demand data, they could be late since dependency chains are short. Hence, property prefetches should be guided by structure prefetches.</cell></row></table><note><p><p>Fig. 4b(i)  </p>shows that the L2 hit rate (which is already</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table V :</head><label>V</label><figDesc>Prefetehers for evaluation</figDesc><table><row><cell>L2 GHB</cell><cell>index table size = 512, buffer size = 512</cell></row><row><cell>L2 VLDP</cell><cell>implemented using code from [56], last 64 pages tracked by DRB, 64-entry OPT, 3 cascaded 64-entry DPTs</cell></row><row><cell>L2</cell><cell>implemented as described in section 2.1 of [53],</cell></row><row><cell>strea-</cell><cell>prefetch distance=16, number of streams=64, stops</cell></row><row><cell>mer</cell><cell>at page boundary</cell></row><row><cell></cell><cell>address generation latency in PAG = 2 cycles, 512-</cell></row><row><cell>MPP</cell><cell>entry VAB and PAB, 128-entry MTLB, 2 64-bit registers, coherence engine checking overhead=lO</cell></row><row><cell></cell><cell>cycles</cell></row><row><cell>MPPI</cell><cell>MPP augmented with the ability to identify a prefetched structure cacheline</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>::!: 8 4.95,1.42,1.37,0.32: 8.88,3.65,3.48,1.31 : 2.47,1.94,1.97,1.07 5.22,3.15,3.14,2.96 5.72,3.18,3.17,1.87</figDesc><table><row><cell>&lt;ll ;;:: 12 :; 0. 10</cell><cell>CC</cell><cell>: : .</cell><cell>F!IR</cell><cell>: I :</cell><cell>BC</cell><cell>BFS</cell><cell>I</cell><cell>SSSP</cell></row><row><cell>20 6 US j 4</cell><cell></cell><cell>I :</cell><cell></cell><cell>I :</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>o</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>1Previous study shows a 2-30X slowdown of software frameworks compared to hand-optimized implementations<ref type="bibr" target="#b48">[49]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>3The MPPI in monoDROPLETI..l can reuse the core-side TI..B and does not require aMTI..B.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>4The value is relatively lower due to BC-road and BC-urand exceptions which do not benefit highly from any prefetcher configuration. However, DROPLET causes no slowdown for them (Fig.11 a).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>X. ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Onur Mutlu</rs>, <rs type="person">Bowen Huang</rs>, <rs type="person">Yungang Bao</rs>, and the anonymous reviewers for their valuable feedback. This work was supported in part by <rs type="funder">NSF</rs> <rs type="grantNumber">1730309</rs>, <rs type="grantNumber">1719160</rs>, <rs type="grantNumber">1500848</rs> and by <rs type="funder">CRISP</rs>, one of six centers in JUMP, a SRC program sponsored by <rs type="funder">DARPA</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rWz6Zug">
					<idno type="grant-number">1730309</idno>
				</org>
				<org type="funding" xml:id="_9Hu2cW7">
					<idno type="grant-number">1719160</idno>
				</org>
				<org type="funding" xml:id="_2MpdnnF">
					<idno type="grant-number">1500848</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prefetching. Although prefetching is a well-studied topic <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b59">[60]</ref>- <ref type="bibr" target="#b67">[69]</ref>, our work shows that many state-ofthe-art prefetchers are not adequate for graph processing (Section VII-B). Our contributions lie in the in-depth analysis of the data type behavior in graph analytics and in the observation-oriented prefetching design decisions (dataaware and decoupled prefetcher).</p><p>IMP <ref type="bibr" target="#b68">[70]</ref> is a hardware-only Ll prefetcher for indirect memory references. Unlike IMP, we use the system support for data awareness to avoid long and complex prefetcher training. A data-aware prefetcher removes the need for IMP's strict and not-widely-applicable assumption of very long streams to be eventually able to train the prefetcher. In addition, we show that, for graph processing, a decoupled architecture provides performance benefits over a monolithic prefetcher (the design adopted by IMP) (Section VIl-B).</p><p>Many prefetchers have been proposed for linked-list type data structures <ref type="bibr" target="#b59">[60]</ref>- <ref type="bibr" target="#b61">[63]</ref>. However, they prefetch at multiple levels of recursion (over-prefetch), whereas the structure-toproperty indirect array access in graph analytics represents only one level of dependency. Runahead execution <ref type="bibr" target="#b64">[66]</ref> uses a stalled ROB to execute ahead in the program path but can only prefetch independent cache misses. Dependence graph precomputation <ref type="bibr" target="#b65">[67]</ref> is not data-aware and it requires large hardware resources to dynamically identify dependent load chains for precomputation. Near-memory prefetchers and accelerators. There exist near-memory prefetchers that perform next-line prefetching from the DRAM row buffer <ref type="bibr" target="#b69">[71]</ref> or prefetch linked-list data structures [61], <ref type="bibr" target="#b70">[72]</ref>. To the best of our knowledge, our work is the first to propose a data-aware prefetcher in the MC for indirect array accesses in graph analytics. In addition, our near-memory MPP is significantly different from previous approaches [61], <ref type="bibr" target="#b69">[71]</ref>, <ref type="bibr" target="#b70">[72]</ref> in how it is guided by a dataaware L2 streamer.</p><p>Hashemi et al. <ref type="bibr" target="#b51">[52]</ref> propose, for SPEC CPU2006 benchmarks, dynamically offloading dependent chains predicted to be cache misses to the MC for acceleration. However, in graph analytics, such a scheme could lead to high overheads from very frequent offloading since cache miss rates are much higher. Instead, we use the concept of issuing requests directly from the MC to decouple our prefetcher 0 i ; "C -g: 0 1 , -g -g '0 1</p><p>; -g -g '0 1</p><p>; -g -g '0 1</p><p>; -g -g ~e ~,0 :~::</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatial and graph anaytics with oracle database 18c</title>
	</analytic>
	<monogr>
		<title level="j">tech. rep</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>Oracle</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Amazon introduces an aws graph database service called amazon neptune</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dillet</surname></persName>
		</author>
		<ptr target="https://techcrunch.com/2017/11/29/amazon-introduces-an-aws-graph-database-service-called-arnazon-neptune/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Trinity: A distributed graph engine on a memory cloud</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Management of Data</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="505" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pregel: a system for largescale graph processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Austem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Czajkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One trillion edges: Graph processing at facebook-scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kabiljo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VWB Endowment</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1804" to="1815" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wtf: The who to follow service at twitter</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on World Wide Web</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="505" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graphjet: real-time content recommendations at twitter</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bommannavar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VWB Endowment</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1281" to="1292" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Graph databases lie at the heart of $7tn self-driving car opportunity</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ismail</surname></persName>
		</author>
		<ptr target="http://www.information-age.com/graph-databases-heart-self-driving-car-opportunity-123468309/" />
		<imprint>
			<date type="published" when="2017-11">Nov 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Powergraph: Distributed graph-parallel computation on natural graphs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OSDI</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012">2012</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed graphlab: a framework for machine learning and data mining in the cloud</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VWB Endowment</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="716" to="727" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High-level programming abstractions for distributed graph processing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kalavri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vlassov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haridi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="305" to="324" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graphchi: Largescale graph computation on just a pc</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USENIX</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Turbograph: a fast parallel graph engine handling billion-scale graphs in a single pc</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Knowledge discovery and data mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gridgraph: Large-scale graph processing on a single machine using 2-level hierarchical partitioning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="375" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Wonderland: A novel abstraction-based out-of-core graph processing system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="608" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mosaic: Processing a trillion-edge graph on a single machine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Systems</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="527" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A scalable processing-in-memory accelerator for parallel graph processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Abn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="105" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Graphicionado: A high-performance and energy-efficient accelerator for graph analytics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graphr: Accelerating graph processing using reram</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="531" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tunao: A high-performance and energy-efficient reconfigurable accelerator for graph processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Cluster, Cloud and Grid Computing</title>
		<imprint>
			<date type="published" when="2017-05">May 2017</date>
			<biblScope unit="page" from="731" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Energy efficient architecture for graph analytics accelerators</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ozdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yesil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ayupov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Greth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bums</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ozturk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="166" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ringo: Interactive graph analytics on big-memory machines</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sosic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Puttagunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Management of Data</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1105" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph analytics through fine-grained parallelism</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Management of Data</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="463" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scale up or scale out for graph processing?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="72" to="78" />
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">System memory at a fraction of the dram cost</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Intel Corporation</publisher>
		</imprint>
	</monogr>
	<note>tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Snap: A general-purpose network analysis and graph-mining library</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sosic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2016-07">July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ligra: a lightweight graph 385 processing framework for shared memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Sigplan Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pixie: A system for recommending 3+ billion items to 200+ million users in realtime</title>
		<author>
			<persName><forename type="first">C</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sugnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>abs/171 1.07601</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">X-stream: Edgecentric graph processing using streaming partitions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mihailovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Operating Systems Principles</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Everything you always wanted to know about multicore graph processing but were afraid to ask</title>
		<author>
			<persName><forename type="first">J</forename><surname>Malicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="631" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using cycle stacks to understand scaling bottlenecks in multi-threaded workloads</title>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Workload Characterization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="38" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Microarchitecture optimizations for exploiting memory-level parallelism</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fahs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="76" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Locality exists in graph processing: Workload characterization on an ivy bridge server</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IISWC</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="56" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parallel graph processing on modem multi-core servers: New findings and remaining challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eisenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Magalhaes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MAS-COTS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graphbig: understanding graph computing in the context of industrial solutions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Tanase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC-International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Parallel graph processing: Prejudice and state of the art</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eisenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Magalhaes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Faraboschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Performance Engineering</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="85" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Performance characterization of multi-threaded graph processing applications on manyintegrated-core architecture</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software</title>
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficiently prefetching complex address patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shevgoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koladiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="141" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Data cache prefetching using a global history buffer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software, IEEE Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="96" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph prefetching using data structure knowledge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Supercomputing</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">39</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mlp-aware dynamic instruction window resizing for adaptively exploiting both ilp and mlp</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Compute caches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jeloka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Subramaniyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blaauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Intel? 64 and IA-32 Architectures Optimization Reference Manual</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sniper: Scalable and accurate parallel multi-core simulation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACACES</title>
		<imprint>
			<biblScope unit="page" from="91" to="94" />
			<date type="published" when="2012">2012</date>
			<publisher>HiPEAC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An evaluation of high-level mechanistic core models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACO</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">x 86 computer architecture simulators: A comparative study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sawalha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Design</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="638" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cacti 6.0: A tool to model large caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HP laboratories</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The gap benchmark suite</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<idno>CoRR abs/1508.03619</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Navigating the maze of graph analytics frameworks using massive graph datasets</title>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M A</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hassaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Management of data</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="979" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">SNAP Datasets: Stanford large network dataset collection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krevl</surname></persName>
		</author>
		<ptr target="http://snap.stanford.eduldata" />
		<imprint>
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploring core and cache hierarchy bottlenecks in graph processing workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="197" to="200" />
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Accelerating dependent cache misses with an enhanced memory controller</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khubaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="444" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Feedback directed prefetching: Improving the performance and bandwidth-efficiency of hardware prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Prefetchaware dram controllers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Narasiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Symposium on Microarchitecture</title>
		<meeting>International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Intel? 64 and IA-32 Architectures Software Developer&apos;s Manual</title>
		<imprint>
			<biblScope unit="page" from="3A" to="92016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">2nd Data Prefetching Championship (DPC2)</title>
		<ptr target="http://comparch-conf.gatech.eduldpc2/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Random fill cache architecture</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2014-12">Dec 2014</date>
			<biblScope unit="page" from="203" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gather-scatter dram: in-dram address translation to improve the spatial locality of non-unit strided accesses</title>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mullins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boroumand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings International Symposium on Microarchitecture</title>
		<meeting>International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="267" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graphpim: Enabling instruction-level pim offloading in graph computing frameworks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A stateless, content-directed data prefetching mechanism;&apos; in International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cooksey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Lebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Supercomputing</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2002. 2000</date>
			<biblScope unit="page" from="176" to="186" />
		</imprint>
	</monogr>
	<note>Push vs. pull: Data movement for linked data structures</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dependence based prefetching for linked data structures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="115" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Techniques for bandwidth-efficient prefetching of linked data structures in hybrid prefetching systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="7" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Prefetching using markov predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on computers</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="133" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Path confidence based lookahead prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Runahead execution: An alternative to very large instruction windows for out-of-order processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High-Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Data prefetching by dependence graph precomputation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Best-offset hardware prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Access map pattern matching for high performance data cache prefetch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Imp: Indirect memory prefetcher</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="178" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Meeting midway: Improving cmp performance with memory-side prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yedlapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kultursay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sivasubramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Parallel architectures and compilation techniques</title>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="289" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Memory-side prefetching for linked data structures for processor-in-memory systems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Adve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal ofParallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="448" to="463" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
