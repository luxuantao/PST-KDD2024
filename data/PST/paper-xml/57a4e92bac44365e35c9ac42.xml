<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supervised Evaluation of Image Segmentation and Object Proposal Techniques</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
							<email>jordi.pont@upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Politècnica de Catalunya</orgName>
								<address>
									<region>BarcelonaTech</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Ferran</forename><surname>Marques</surname></persName>
							<email>ferran.marques@upc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Politècnica de Catalunya</orgName>
								<address>
									<region>BarcelonaTech</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Supervised Evaluation of Image Segmentation and Object Proposal Techniques</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3A5A8A714982CDDF23BFA4FDA67494C4</idno>
					<idno type="DOI">10.1109/TPAMI.2015.2481406</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2015.2481406, IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image segmentation</term>
					<term>object proposals</term>
					<term>supervised evaluation</term>
					<term>meta-measures</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper tackles the supervised evaluation of image segmentation and object proposal algorithms. It surveys, structures, and deduplicates the measures used to compare both segmentation results and object proposals with a ground truth database; and proposes a new measure: the precision-recall for objects and parts. To compare the quality of these measures, eight state-of-the-art object proposal techniques are analyzed and two quantitative meta-measures involving nine state of the art segmentation methods are presented. The meta-measures consist in assuming some plausible hypotheses about the results and assessing how well each measure reflects these hypotheses. As a conclusion of the performed experiments, this paper proposes the tandem of precision-recall curves for boundaries and for objects-and-parts as the tool of choice for the supervised evaluation of image segmentation. We make the datasets and code of all the measures publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>S INCE the advent of sliding window object detec- tors <ref type="bibr" target="#b0">[1]</ref>, much effort has been put into providing better spatial delineation beyond sliding windows <ref type="bibr" target="#b1">[2]</ref>, as a preprocessing step of many state-of-the-art algorithms <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Bottom-up segmentation methods often play an important role in the proposed algorithms <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and thus improving segmentation techniques would entail improvements towards better computer vision applications.</p><p>In such a challenge, providing benchmarks that help researchers understand the weak and strong points of their segmentation and object proposal algorithms is of paramount importance. Among these, the supervised evaluation, i.e., comparing the results with an annotated database called ground truth, is the most common approach; and the measures we use to grade the partitions are the cornerstone of the evaluation.</p><p>The first contribution of this paper is to survey and structure a large set of evaluation measures available in the literature. We first focus on the measures that assume a foreground-background ground truth (Section 2), which we refer to as object-based measures. Given their current relevance, we describe how to extend these measures to evaluate object proposal techniques, i.e., algorithms that propose a reduced set of locations and shapes among which it is probable to find the objects in the image (e.g. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>).</p><p>To evaluate the generic image segmentation measures, which we refer to as partition-based (Section 3), we show that they can be classified depending on the interpretation of image partition they are based on.</p><p>The most obvious one (region-based interpretation) is to interpret an image partition as a clustering of the set of pixels into regions, so any generic measure to evaluate clustering algorithms can be applied in this context. We can also cast the problem to a two-class clustering of the set of all pairs of pixels: those pairs belonging to the same region, and those coming from different regions (pairs-of-pixels interpretation). Finally, we can also interpret segmentation as a detection problem, aiming at telling apart the pixel contours that are true boundaries from those that are not (boundary-based interpretation).</p><p>Many of the most used evaluation measures, however, are limited to provide a single number, that is, given a pair of partitions (machine-generated and ground truth) they give us a single value that somehow reflects the degree of agreement between both. In the field of object detection assessment, Hoiem et al. <ref type="bibr" target="#b11">[12]</ref> refer to these measures as performance summary measures and they stress that results should be evaluated beyond this type of measures in order to "help understand how one method could be improved." In other words, researchers need better feedback from the evaluation than a single number.</p><p>Back to segmentation assessment, the precisionrecall curves for boundaries <ref type="bibr" target="#b12">[13]</ref> are good examples of tools that provide richer feedback than the F measure used as summary. Moreover, as pointed out by <ref type="bibr" target="#b13">[14]</ref>, in addition to measures based on the boundary-based interpretation of a partition, region-based measures should be considered when assessing segmentations. However, the current region-based measures are limited to summary ones (e.g. <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>).</p><p>The second contribution of this work (Section 4) is a precision-recall environment for the assessment of image segmentation that relies on the region-based interpretation of an image partition. Inspired by <ref type="bibr" target="#b17">[18]</ref>, Fig. <ref type="figure">1</ref>. Quantitative meta-measure principles: How good are the evaluation measures at ranking the secondrow partitions better than the third-row ones? <ref type="bibr" target="#b11">[12]</ref> and by the fact that parts of objects are important clues for object detection <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, we present the precision-recall for objects and parts, which is based on classifying the regions into object and part candidates.</p><p>Summary measures also play a role in performance comparison and researchers have a large list to choose from, thus the question that now arises is how to compare the goodness of an evaluation measure. In other words, we should define a meta-measure to compare the evaluation measures. The principle of a meta-measure is to assume a plausible hypothesis about the segmentation evaluation and assess how well measures match this hypothesis.</p><p>Some previous works based their claims on qualitative meta-measures, that is, showing the behavior of the measures on a few particular qualitative examples <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The first approach to an extensive quantitative meta-measure was proposed in <ref type="bibr" target="#b12">[13]</ref>. The hypothesis in this work was that measures should be able to discriminate between two pairs of humanmarked partitions coming from different images (for instance, the two partitions in Figure <ref type="figure">1</ref>.a). In an annotated database with multiple partitions per image, the quantitative meta-measure was defined as the number of same-image partition pairs that the measure judges as less similar than other pairs of partitions coming from different images. <ref type="bibr" target="#b21">[22]</ref> presented a comparison of some measures in terms of this meta-measure.</p><p>The third contribution of this work (Section 5) is to present two new quantitative meta-measures. Moreover, instead of basing our hypotheses only on human-made partitions, we extend the analysis to partitions from nine State-of-the-Art (SoA) segmentation techniques.</p><p>The first hypothesis is that measures should rank higher SoA partitions than those obtained by means of two baseline techniques. The meta-measure is then defined as the number of results from SoA algorithms that are judged better than the baselines. As an example, we assess whether the measures score higher SoA partitions like those in the top row of Figure <ref type="figure">1</ref>.b than the baseline ones in the lower row.</p><p>As a second meta-measure, we assume that any measure should rank higher a partition obtained by a SoA method on a given image than a partition obtained by the same method but on a different image, as the two pairs of partitions shown in Figure <ref type="figure">1</ref>.c. The meta-measure in this case is defined as the number of cases in which the measure correctly judges the sameimage partition as better.</p><p>Finally, Section 6 presents the experimental validation of this paper. For the foreground-background case (object-based), we analyze the boundary-and pixel-based measures, as well as three different generalization strategies to object proposals. We show qualitative results and the quantitative comparison of eight SoA object proposal techniques that show the complementarity of the proposed measures. For the partition-based case, we first compare all surveyed evaluation measures using the three quantitative meta-measures. We show that the two precisionrecall measures (boundary-and objects-and-partsbased) have outstanding results as summary measures with respect to the rest of measures. We further analyze these two precision-recall frameworks by comparing nine SoA segmentation algorithms and show qualitative results illustrating the complementarity between the two frameworks.</p><p>Overall, the experiments show that the tandem of boundary-and region-based measures should be the choice for the supervised evaluation of both image segmentation and object proposals techniques. This work is an extended version of <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OBJECT-BASED MEASURES: REVIEW AND IMPROVEMENTS</head><p>This section focuses on the specific case of image segmentation where both the segmentation and the ground-truth are foreground-background partitions. Measures can focus either on evaluating how well the pixels of the ground truth are detected, or on how accurate the boundaries are represented. Sections 2.1 and 2.2 review and deduplicate the measures found in the literature under both interpretations. Then, Section 2.3 extends these measures to evaluate object proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pixel-based object measures review</head><p>Given an object detection method m, its resulting single-object detection can be written, from a pixel perspective as a division of the image pixel set I into two disjoint classes I = P m ∪ N m , where P m and N m refer to positive and negative pixels, respectively, and the subscript stands for the method used. Equivalently for the ground-truth</p><formula xml:id="formula_0">I = P gt ∪ N gt .</formula><p>The goal of any automatic algorithm is to achieve a perfect detection, i.e., P m = P gt , but if this is not the case, we define the following sets:</p><p>• True positives: Pixels that are detected as object and they are labeled as so in the ground truth: TP = P m ∩ P gt . • False positives: Pixels that are detected as object but they are not labeled as so in the ground truth: FP = P m ∩ N gt . • False negatives: Pixels that are classified as nonobject but they are labeled as object in the ground truth: FN = N m ∩ P gt , also known as misses. The objective is, therefore, to maximize the true positives while minimizing both the false positives and the false negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Precision, Recall, and F Measure</head><p>A widely used and accepted pair of measures to assess a detection algorithm is the following:</p><p>• Precision: Measures the percentage of detected pixels that are actually true:</p><formula xml:id="formula_1">Precision = |TP | |P m | = |P m ∩ P gt | |P m | ≤ 1</formula><p>• Recall: Measures the percentage of ground-truth positives that are actually detected:</p><formula xml:id="formula_2">Recall = |TP | |P gt | = |P m ∩ P gt | |P gt | ≤ 1</formula><p>Our objective is to maximize both measures, but in general there is a trade-off between them, which we can measure using the F measure, that is, the harmonic mean between precision and recall:</p><formula xml:id="formula_3">F = 2 Prec • Rec Prec + Rec = 2 |T P | 2 |T P | + |F N | + |F P | (1)</formula><p>To the knowledge of the authors, this coefficient was first reported by Czekanowski in 1913 <ref type="bibr" target="#b23">[24]</ref>, in the context of anthropology. Later, Dice used it in 1945 <ref type="bibr" target="#b24">[25]</ref> to compare the number of species in two samples, with respect to the shared species in both. He coined it as coincidence index. It was also used in the context of plant sociology by Sørensen in 1948 <ref type="bibr" target="#b25">[26]</ref>. Named after them, the coefficient is also known as Czekanowski, Dice's, or Sørensen's coefficient. More recently, the F measure is used as the evaluation metric in the Weizmann segmentation database <ref type="bibr" target="#b26">[27]</ref>, in the context of multi-object tracking <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, or in the medical imaging context <ref type="bibr" target="#b29">[30]</ref>, where it is also referred to as Spatial Overlap Index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Jaccard Similarity Coefficient</head><p>The Jaccard index was introduced in the context of plant sociology by Jaccard in 1901 <ref type="bibr" target="#b30">[31]</ref>, and in the context of object segmentation it is often referred to as Intersection over Union (IoU) between the machine and the ground-truth results:</p><formula xml:id="formula_4">J(P m , P gt ) = |P m ∩ P gt | |P m ∪ P gt | = |T P | |T P | + |F N | + |F P |<label>(2)</label></formula><p>In the PASCAL Visual Object Classes Challenge 2010 <ref type="bibr" target="#b31">[32]</ref> the Jaccard coefficient (called area of overlap a 0 ) is used to assess whether a particular object has been detected (a 0 ≥ 0.5) or not (a 0 &lt; 0.5). In the context of object detection in <ref type="bibr" target="#b32">[33]</ref>, object accuracy is measured by means of the same value, denoted as A 0 . The performance measure used in the salient object extraction evaluation in <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> is also J, although denoted as P . The work in <ref type="bibr" target="#b35">[36]</ref> uses also this measure but it is denoted as Overlap Score (OS), or spatial support score. In <ref type="bibr" target="#b13">[14]</ref> the Jaccard index is referred to as overlap and in <ref type="bibr" target="#b36">[37]</ref>, as ratio of intersection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">The Jaccard and F measures are equivalent</head><p>Comparing the expression of the F (Eq. 1) and J (Eq. 2), we can deduce the following equality:</p><formula xml:id="formula_5">F 2 -F = 2 |TP| 2|TP|+|FN |+|FP| 2 - 2 |TP| 2|TP|+|FN |+|FP| = 2 |TP | 4|TP | + 2|FN | + 2|FP | -2|TP | = J</formula><p>That is, both measures are functionally related. Figure <ref type="figure" target="#fig_1">2</ref> plots the value of J as a function of F , in the range of interest [0, 1]. Given that their relationship is a monotonically increasing function, any ranking between algorithms using any of the two functions would be the same. In other words, for the purpose of segmentation algorithm comparison, both measures are equivalent. Despite this simple equivalence, there exist works in the literature <ref type="bibr" target="#b37">[38]</ref> that report results using both measures in parallel.</p><p>In this work we will mainly use the Jaccard coefficient, since it is more used in the literature, and therefore, comparing results will be easier. We believe, however, that the main reason why this measure was selected against the F measure is aesthetic: the expression in terms of P m and P gt is more compact; although from a detection point of view, the F measure is theoretically more justified in our opinion.</p><p>We refer the reader to <ref type="bibr" target="#b38">[39]</ref> for a comparison of specific measures to evaluate foreground maps, which although very similar to object segmentations, it is out of the scope of this paper. In it, the authors compare their measures using the meta-measures presented in this work (and previously in <ref type="bibr" target="#b22">[23]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Boundary-based object measures review</head><p>Differently to the pixel-based approach, a single-object detection result can be represented, from a boundary perspective, by the boundary between the foreground and the background pixels. Comparing the boundaries from the ground truth and the result we could therefore assess the quality of the detected objects. Section 3.3 presents a review of boundary-based measures for full partitions, and highlights the wellknown precision-recall for boundaries <ref type="bibr" target="#b12">[13]</ref> as the measure of choice. The main idea behind this measure is to perform a Bipartite Graph Matching (BGM) between the pieces of boundary and then compute the precision, recall, and F measure (F b ). Given that we are evaluating simpler foreground-background masks, therefore, we could even compute more informative measures that specifically evaluate, for instance, how similar the represented shapes are <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, perception-inspired losses <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b32">[33]</ref>, etc.</p><p>When evaluating object proposals, however, we need to compare the ground truth with a large set of potential proposals (in the order of thousands, usually). The measure must be, therefore, very efficient, leaving out the majority of approaches introduced previously, which usually involve a costly BGM. To overcome this issue, we propose to do a simple morphological approximation of the precision-recall for boundaries <ref type="bibr" target="#b12">[13]</ref> (F b ) that avoids the BGM: to compute precision we dilate the boundary pixels of the groundtruth shape and count the object boundary pixels that intersect the resulting mask (recall is computed the other way around). We then compute the morphological boundary F measure ( Fb ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluating object proposals techniques</head><p>A current trend in image and object segmentation is generating object proposals <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b5">[6]</ref>, which aims at generating a pool of region proposals (or candidates) with the objective of being as accurate as possible, while minimizing the size of the pool. From the point of view of object detection, they can be seen as a reduced set of potential locations and shapes where to look for objects, thus we would like the pool of candidates to be as small as possible (for our algorithm to be fast), while not losing set quality due to not considering all the set of possible locations and shapes.</p><p>To evaluate object proposals, therefore, we should account for two counterbalancing aspects: number of proposals versus the maximum achievable quality within the candidates in the pool. When training an algorithm to find its optimal parameterization we could perform optimization in the Pareto front of this two-dimensional space, as in <ref type="bibr" target="#b35">[36]</ref> for generic image segmentation. In this work we focus on the evaluation at testing time, where the parameters are fixed.</p><p>For a given image in the database, we will, therefore, scan all proposals, compute an object-based metric M with respect to the ground truth, and get the maximum value. To compute the overall performance metric, we explore three different strategies. First, we could simply average the maximum measure value for all the annotated objects. Second, we could compute the median instead, to try to be more robust to outliers (e.g. missed objects with M close to 0).</p><p>In both cases, we are summarizing a large set of results into a single number so we are missing the distribution of the results. For instance, we would not distinguish a method whose proposals on half the objects are perfect (M = 1) and half missed (M = 0) from a result whose proposals are always at M = 0.5. We might, however, prefer one strategy against the other depending on the application.</p><p>A histogram reflects well the distribution of M values for a given number of proposals, but then we would end up having a 3-dimensional evaluation measure (number of proposals, binned M , bin counts), which is always tricky to plot. An in-between solution is to plot the percentiles of the histogram with respect to the number of proposals, that is, the percentage of objects on which the achievable M is higher than a threshold. In detection terms, these percentiles are the recall rates for different M thresholds.</p><p>We will discuss and analyze the results obtained using these three measures on eight state-of-the-art object proposal algorithms in the experiments section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PARTITION-BASED MEASURES: REVIEW AND STRUCTURE</head><p>The state-of-the-art supervised evaluation measures can be classified depending on the image partition interpretation on which they are based. The most common interpretation is as a clustering of the pixel set into a number of subsets or regions, which we will refer to as region-based interpretation. A partition can also be interpreted as a two-class clustering of the set of pairs of pixels, with some pairs linking pixels from the same region and others linking pixels from different regions, which we will call pairs-ofpixels interpretation. Finally, a partition can be interpreted as a detection result, aimed at selecting the true boundaries on the image, which we will refer to as boundary-based interpretation. Figure <ref type="figure" target="#fig_2">3</ref> illustrates these three different partition interpretations. The contributions of Sections 3.1 to 3.3 are to review, de-duplicate, and discuss about the main measures found under each of these interpretations, keeping the notation from the original papers where possible. In <ref type="bibr" target="#b49">[50]</ref>, the reader can find an interpretation of most of these measures in terms of simple measures such as the F measure, the Jaccard index, or precision-recall. Finally, Table <ref type="table">1</ref> shows an overview of the measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Region-Based Measures</head><p>The directional Hamming distance from one partition S to another S <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b45">[46]</ref> is defined as:</p><formula xml:id="formula_6">D H (S ⇒ S ) = n - R ∈S max R∈S |R ∩ R|<label>(3)</label></formula><p>where R and R are regions in S and S , respectively, and n is the number of pixels in the image. In <ref type="bibr" target="#b20">[21]</ref> this same measure was coined as asymmetric partition distance. Moreover, it is equivalent to the achievable segmentation accuracy <ref type="bibr" target="#b51">[52]</ref> used in superpixel assessment.</p><p>As shown in <ref type="bibr" target="#b49">[50]</ref>, this measure is a generalization of the local measure precision between R and R :</p><formula xml:id="formula_7">1- 1 n D H (S ⇒ S ) = 1 n R ∈S |R | • max R∈S |R ∩ R| |R |</formula><p>The segmentation covering of a partition S by a partition S was defined in <ref type="bibr" target="#b13">[14]</ref>, and can be interpreted as the generalization of the local measure Jaccard index between R and R :</p><formula xml:id="formula_8">C (S → S) = 1 n R∈S |R| • max R ∈S |R ∩ R | |R ∪ R |<label>(4)</label></formula><p>A symmetric version of D H was presented in <ref type="bibr" target="#b46">[47]</ref> as the van Dongen distance:</p><formula xml:id="formula_9">d vD (S, S ) = D H (S ⇒ S) + D H (S ⇒ S )<label>(5)</label></formula><p>The intuitive step further is to measure the maximum overlap when performing a bijective matching between the regions of the two partitions, instead of the local matchings done in the measures above. This idea was presented in <ref type="bibr" target="#b20">[21]</ref> as symmetric partition distance, in <ref type="bibr" target="#b21">[22]</ref> as bipartite-graph-matching (BGM) distance, and in the context of clustering comparison, in <ref type="bibr" target="#b15">[16]</ref> as classification error distance. It is shown in <ref type="bibr" target="#b20">[21]</ref> that it is equivalent to the minimum number of pixels that must not be taken into account for the two partitions to be identical.</p><p>In <ref type="bibr" target="#b12">[13]</ref>, the consistency of the BSDS300 human partitions is analyzed by means of the bidirectional </p><formula xml:id="formula_10">BCE (S,S ) = 1- 1 n R∈S R ∈S |R∩R | min |R ∩ R | |R| , |R ∩ R | |R | (6)</formula><p>The work in <ref type="bibr" target="#b15">[16]</ref> introduced a new point of view to the measures of clustering assessment based on information-theoretic results. The author defines a discrete random variable taking N values that consists in randomly picking any pixel in the partition S = {R 1 , . . ., R N } and observing the region it belongs to. Assuming all the pixels equally probable to pick, the entropy H(S) associated with a partition is defined as the entropy of such random variable. The mutual information I(S,S ) between two partitions is defined equivalently. The measure variation of information is then:</p><formula xml:id="formula_11">VoI (S, S ) = H(S) + H(S ) -2I(S, S )<label>(7)</label></formula><p>If divided by log N , its maximum possible value, we get the normalized variation of information (nVoI ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pairs-of-Pixels Measures</head><p>An image partition can be viewed as a classification of all the pairs of pixels into two classes: pairs of pixels belonging to the same region, and pairs of pixels from different regions. Formally, let I = {p 1 , . . . , p n } be the set of pixels of the image and consider the set of all pairs of pixels P = { (p i , p j ) ∈ I × I| i &lt; j}. Given two partitions S and S , we divide P into four different sets, depending on where a pair (p i , p j ) of pixels fall <ref type="bibr" target="#b15">[16]</ref>: P 11 : in the same region both in S and S , P 10 : in the same region in S but different in S , P 01 : in the same region in S but different in S, P 00 : in different regions both in S and S . The Rand index, originally defined in <ref type="bibr" target="#b47">[48]</ref> as a clustering evaluation measure, arises naturally in this context:</p><formula xml:id="formula_12">RI (S,S ) = |P 00 | + |P 11 | |P|</formula><p>It counts the pairs of pixels that have coherent labels for the two partitions being compared, with respect to the number of possible pairs of pixels.</p><p>In the context of image segmentation and having a set {G i } of ground-truth partitions of the same image, the Probabilistic Rand Index <ref type="bibr" target="#b14">[15]</ref> is computed as:</p><formula xml:id="formula_13">PRI (S, {G i }) = i RI (S,G i )<label>(8)</label></formula><p>In this same context, the precision-recall for regions <ref type="bibr" target="#b12">[13]</ref> is defined as:</p><formula xml:id="formula_14">P r = |P 11 | |P 11 | + |P 10 | R r = |P 11 | |P 11 | + |P 01 |<label>(9)</label></formula><p>As a summary measure, the F measure F r is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Boundary-Based Measures</head><p>All measures above could be applied to any clustering algorithm, no matter the nature of the elements being classified. In fact, the majority of the indices presented come from the application of general-clustering assessment measures to image segmentation. Image pixels, however, are spatially distributed in the image plane, and so the concept of neighborhood arises naturally. Therefore, an image partition with connected components can be unambiguously defined by their boundaries, i.e., a bijection could be made between all possible image partitions and all possible closed boundaries maps.</p><p>Recalling the definition of P as the set of pairs of pixels in the image, let us define the set of pairs of neighboring pixels as N ⊂ P. One can define a bijection between the set of boundary segments B and N linking each segment to the pair of pixels at each of its sides. Using this notation, boundary detection can be understood as a two-class clustering of B, dividing the segments into those being boundaries and those not. This way, comparing two partitions can be translated into comparing two clustering of B.</p><p>To be robust to unnoticeable shifts of boundary localization, <ref type="bibr" target="#b48">[49]</ref> proposes to compute the optimal matching between the segments of boundaries of the two partitions as a maximum-weight bipartite-graph matching. The algorithm is improved in <ref type="bibr" target="#b12">[13]</ref> leading to the well-known precision-recall for boundaries (P b , R b , and F b ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NEW MEASURE: F MEASURE FOR OB-JECTS AND PARTS</head><p>In the context of image segmentation evaluation, precision-recall curves for boundaries <ref type="bibr" target="#b12">[13]</ref> are a boon for researchers. They statistically reflect, for instance, that an algorithm is providing too coarse segmentations (low recall, high precision) or instead its results are too fragmented (low precision, high recall).</p><p>As we will show in the experiments, and as pointed out by <ref type="bibr" target="#b13">[14]</ref>, however, region benchmarks are also needed apart from the boundary benchmarks when assessing image segmentation. Region benchmarks, however, are currently limited to summary measures as the ones reviewed in Section 3.1. (Note that in the vocabulary used in this paper, region-based measures are the ones based on the interpretation of a partition as a clustering of the set of pixels.)</p><p>This section presents a new region benchmark that goes beyond the summary measures: the precisionrecall curves for objects and parts. Motivated by the fact that image segmentation is increasingly being used as a preliminary step for object detection <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, we propose to assess segmentation under this perspective, that is, we interpret regions in a partition as potential object candidates, and classify them as correct or not depending on their overlap with the ground-truth regions.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows a toy example (left: ground truth, and right: partition) to illustrate the proposed classification. First, we classify those regions from the partition that overlap significantly with a groundtruth region as object candidates (rectangle on the left and background). We then take oversegmentation into account, and define part candidates as those regions that can be used as a part to form a ground-truth region (triangle on the right). Undersegmentation fragmentation candidates are defined equivalently, as those regions that have incorrectly been merged together in the partition (circle and star).</p><p>Precision and recall are then the weighted fraction of candidates with respect to the total number of regions, that is, part candidates are only partially counted. Formally, let S = {R 1 , . . . , R N } be an image partition and {G k } a set of ground-truth partitions of the same image. We consider the set G = {R 1 , . . . , R M } of all the regions in {G k }. For each pair of regions R i ∈ S, R j ∈ G we compute the relative overlaps as:</p><formula xml:id="formula_15">O ij S = |R i ∩ R j | |R i | O ij G = |R i ∩ R j | |R j |</formula><p>We define an object threshold γ o and a part threshold γ p &lt; γ o and classify the regions in both partitions as described in Algorithm 1, where "←" means that a region is classified only if it previously did not have a more favorable classification.</p><p>Algorithm 1 Region candidates classification</p><formula xml:id="formula_16">1: for all R i ∈ S, R j ∈ G do 2: if O ij S &gt; γ o and O ij G &gt; γ o then 3:</formula><p>R i , R j ← Object candidates end if 13: end for Let oc and oc be the number of object candidates in S and G, respectively (note that they can differ, given that G can be formed by more than one partition and thus a region in S can be matched as object with more than one region in G), and pc and pc the number of part candidates. Regarding the fragmentation candidates, we compute the percentage of the object that could be formed from the matched parts. Formally, we define the amount of fragmentation fr (R i ) of a region R i ∈ S as the addition of the relative overlaps of the part candidates matched to R i :</p><formula xml:id="formula_17">fr (R i ) = j O ij G s.t. O ij S &gt; γ o<label>(10)</label></formula><p>fr (R j ) is defined equivalently for G. The global fragmentations fr and fr is computed adding the amount of fragmentation among all fragmentation candidates of S and G, respectively. We then define the precision-recall for objects and parts as follows:</p><formula xml:id="formula_18">P op = oc + fr + β pc |S| R op = oc + fr + β pc |G|<label>(11)</label></formula><p>Intuitively, in a completely oversegmented result, the recall would be high but the precision very low. Conversely, a completely undersegmented result (one single region) would entail a high precision but very low recall. As a summary measure, we propose to use the F measure (F op ) between P op and R op .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">QUANTITATIVE META-MEASURES</head><p>A meta-measure analysis must rely on accepted hypotheses about the segmentation results and assess how coherent the measures are with such hypotheses As an example, an accepted hypothesis can be the human judgment of quality of some particular examples. The meta-measure is then defined as a quantization of how coherent the evaluation measures are with this judgment, as done in works such as <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b20">[21]</ref>.</p><p>To provide statistically significant results, however, one must go beyond a handful of examples and provide a quantitative analysis on an annotated database. The remainder of this section explains one meta-measure already published in the literature (Sec. 5.1) and presents two new meta-measures (Sec. 5.2 and 5.3).</p><p>The two new meta-measures differ significantly from the already-existing one in the sense that, instead of being based only on human-made partitions, we base our analysis on a large set of partitions made by state-of-the-art segmentation techniques. In turn, these meta-measures can be easily updated as new state-of-the-art segmentation techniques are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Swapped-Image Human Discrimination</head><p>Given an image, there is no unique valid segmentation, since it depends on the perception of the scene, the level of details, etc. In order to cope with this variability, the Berkeley Segmentation Dataset (BSDS300 <ref type="bibr" target="#b52">[53]</ref> and BSDS500 <ref type="bibr" target="#b13">[14]</ref>) consists of a set of images each of them manually segmented by more than one individual.</p><p>The hypothesis behind the first meta-measure is that an evaluation metric should be able to tell apart the ground-truth partitions coming from two different images. In other words, given a pair of ground-truth partitions from BSDS500, a measure should be able to tell whether they come from the same image (thus differences are an acceptable refinement) or different images (unacceptable discrepancies).</p><p>As first proposed by <ref type="bibr" target="#b12">[13]</ref> to evaluate the coherence of BSDS300, given an evaluation measure m, we compute the Probability Density Function (PDF) of the values of m for all the pairs of partitions in BSDS500, grouped in two classes: those coming from different images and those from the same one. Figure <ref type="figure" target="#fig_5">5</ref> shows the PDFs for these two types of pairs of partitions using the F b measure.</p><p>A simple classifier was then defined setting a threshold on the measure to discriminate the two types of pairs. The Swapped-Image Human Discrimination (SIHD) meta-measure is defined as the percentage of correct classifications of that classifier, that is, the sum of the area under the curve above and below the threshold for the same-image and different-  image pairs, respectively. (In the original work <ref type="bibr" target="#b12">[13]</ref>, the authors reported the Bayes Risk.)</p><p>As qualitative examples, Figure <ref type="figure" target="#fig_5">5</ref> depicts four pairs of partitions as representatives of the type of mistakes and correct classifications using F b .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SoA-Baseline Discrimination</head><p>One of the reasons why SIHD can be criticized is the fact that it is based only on human-made partitions, that is, it does not show how measures handle the realworld discrepancies found between SoA segmentation methods. This section and the following are devoted to present two meta-mesures based on SoA segmentation results.</p><p>The hypothesis on which we base the meta-measure presented in this section is that evaluation measures should, for a given image, rank higher partitions obtained by any SoA segmentation method than partitions obtained by baseline methods. In particular, in this work we will use nine SoA techniques and two baseline methods.</p><p>As the first baseline technique we consider a quadtree (as in <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b13">[14]</ref>), which consists in hierarchical partitions starting from the whole image support and iteratively dividing the regions into four equal rectangles, regardless of the content of the image.  As a second baseline, we use a random hierarchy, that is, we compute the SLIC <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> superpixels of the image and then iteratively merge random pairs of neighboring regions. Figure <ref type="figure">1</ref>.b(right) shows an example of partition obtained by a SoA method and by a random hierarchy.</p><p>As the partitions given by the baselines can be considered as obtained by chance, the SoA partition should be judged better than the baseline, regardless of the application we are focused on. For each of the techniques considered as SoA segmentation methods, therefore, we compute the number of images in the dataset in which an evaluation measure correctly judges that the baseline result is worse than the SoA generated partition. We refer to the resulting metameasure as SoA-Baseline Discrimination (SABD), and it is defined as the global percentage of correct judgments for a given measure. Figure <ref type="figure" target="#fig_7">6</ref> shows an example of a correct and an incorrect judgment by a segmentation evaluation measure of the quality of a baseline result with respect to a SoA result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Swapped-Image SoA Discrimination</head><p>Segmentation measures are often used to adjust the parameters of a segmentation technique. They are therefore used to compare different partitions created by the same algorithm with slightly different parameterizations and we want the evaluation measures to differentiate between good and better results in order to learn the best parameters. A necessary condition, therefore, it is that a measure should be able to tell apart an acceptable result from a wrong result. Given an image, we consider a SoA partition as acceptable result and a partition done by the same technique and parameters but on a different image as a wrong one.</p><p>In other words, we compare the ground-truth partitions of a certain image with two results obtained using the same algorithm and parameterization: one segmentation of that same image and one of a different image. The hypothesis in this case is that the evaluation measures should judge that the sameimage result is better than the different-image one. In the examples of Figure <ref type="figure">1</ref>.c, the measure should judge that the first-row partitions are better than the secondrow ones, when compared both with the ground-truth of the images of the first row. In this meta-measure, evaluation measures have to tackle the potential bias of the SoA methods towards their specific type of results.</p><p>For each SoA segmentation technique, we compute the number of images in the dataset in which an evaluation measure correctly judges that the sameimage SoA result is better than the different-image one. We define the meta-measure Swapped-Image SoA Discrimination (SISD) as the percentage of results in the database, for all the SoA methods, that the measures correctly discriminate. Figure <ref type="figure" target="#fig_8">7</ref> shows an example of a correct and an incorrect judgment by a segmentation evaluation measure of the quality of a SoA result judged on the same versus a different image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTAL RESULTS</head><p>This section presents the experimental validation of the measures and meta-measures proposed in this paper. We will use the images from BSDS500 <ref type="bibr" target="#b13">[14]</ref>, with the object ground truth from <ref type="bibr" target="#b10">[11]</ref> and partition ground truth from <ref type="bibr" target="#b13">[14]</ref>. Section 6.1 presents a qualitative comparison of the object-based studied measures and Section 6.2 describes the experiments on object proposals, focusing on the behavior and complementarity of the proposed measures. Section 6.3 shows the comparison of all partition-based evaluation measures in terms of the proposed quantitative meta-measures. As a result of the analysis, we propose the two best performing measures F b and F op to be used in tandem. Section 6.4 analyzes the state-of-the-art segmentation techniques in terms of the precision-recall curves of these two measures, illustrating the usefulness of these two frameworks in tandem and the richness gained by going beyond summary measures. We also present some experiments to further analyze their differences and show their complementarity, reinforcing the choice of using them in tandem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Object-based Measures</head><p>This section shows some qualitative results to highlight the differences between the pixel-and boundarybased measures from an object perspective.  </p><p>.75</p><p>.8</p><p>.85</p><p>Pixel-based mean (J)</p><formula xml:id="formula_20">10 2 10 3<label>10 4 .45 .5 .55 .6 .65 .7</label></formula><p>.75</p><p>.8</p><p>.85</p><p>Pixel-based median (J)</p><p>MCG <ref type="bibr" target="#b4">[5]</ref> SCG <ref type="bibr" target="#b4">[5]</ref> CI <ref type="bibr" target="#b10">[11]</ref> GOP <ref type="bibr" target="#b56">[57]</ref> CPMC <ref type="bibr" target="#b5">[6]</ref> GLS <ref type="bibr" target="#b9">[10]</ref> RIGOR <ref type="bibr" target="#b42">[43]</ref> SeSe <ref type="bibr" target="#b43">[44]</ref> 10 10 3</p><p>.9 </p><p>.95 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of proposals</head><p>Boundary-based mean ( Fb)</p><formula xml:id="formula_23">10 2 10 3<label>10 4 .5 .55 .6 .65 .7 .75 .8 .85 .9</label></formula><p>.95 1 Number of proposals Boundary-based median ( Fb) MCG <ref type="bibr" target="#b4">[5]</ref> SCG <ref type="bibr" target="#b4">[5]</ref> CI <ref type="bibr" target="#b10">[11]</ref> GOP <ref type="bibr" target="#b56">[57]</ref> CPMC <ref type="bibr" target="#b5">[6]</ref> GLS <ref type="bibr" target="#b9">[10]</ref> RIGOR <ref type="bibr" target="#b42">[43]</ref> SeSe [44]</p><formula xml:id="formula_24">10 2 10 3<label>10 4 0 .1 .2 .3 .4 .5 .6 .7 .8</label></formula><p>.9</p><p>Fb=0.5 depicts a ground-truth annotated object and four different cases, each of which evaluated with the pixel-based measure Jaccard (J) and the boundarybased Fb . We also report the original F b to intuitively check whether the morphological approximation Fb is acceptable and to adjust its parameters.</p><p>Figure <ref type="figure" target="#fig_9">8</ref>(a) shows a human-made partition to represent the quality upper-bound. The three measures report very high values, although not a perfect 1. Figure <ref type="figure" target="#fig_9">8</ref>(b) shows a degenerate case where the pixelbased measure does not penalize the result, because the number of wrong pixels is very small. In contrast, Fb correctly penalizes it. Figure <ref type="figure" target="#fig_9">8(c)</ref> shows the dual result, in which the object is completely missed in terms of pixels but the boundary-based measure does not penalize it properly because there is a significant boundary overlap. Figure <ref type="figure" target="#fig_9">8(d)</ref> shows a result with altered boundaries, which is considered worse than (c) in terms of Fb , although pixel-wise is a good result.</p><p>All examples show that Fb is a good approximation of the original F b . To achieve so, we adapted the boundary tolerance (8% of the image diagonal) in Fb to be robust to degenerate cases such as (d).</p><p>Overall, we observe a dual behavior between the boundary-based and pixel-based measures, so our proposal is to use both measures in tandem for the object-based evaluation of segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Object Proposals</head><p>The state of the art in object proposals is represented in this work by the following eight methods: GOP <ref type="bibr" target="#b56">[57]</ref>, MCG <ref type="bibr" target="#b4">[5]</ref>, SCG <ref type="bibr" target="#b4">[5]</ref>, CI <ref type="bibr" target="#b10">[11]</ref>, CPMC <ref type="bibr" target="#b5">[6]</ref>, GLS <ref type="bibr" target="#b9">[10]</ref>, RIGOR <ref type="bibr" target="#b42">[43]</ref>, and SeSe <ref type="bibr" target="#b43">[44]</ref>. Figure <ref type="figure">9</ref> shows the pixel-and boundary-based evaluation results for these methods using the three proposed generalization measures.</p><p>As expected, the general trend is that the more proposals, the better the achievable quality. The mean and median measures show similar overall behavior, with MCG being the best performing and slight differences such as the comparison between CPMC [ ] and GOP [ ], which have inverted rankings with the two measures. Being the median consistently better than the mean suggests that there are some outliers on the lower part of the distribution, i.e., some missed objects where the achievable quality is close to zero.</p><p>Focusing on the pixel-based measure (top row), these differences are better reflected in the recall plots, on the right-most part of the plot. We depict the plots for J = 0.5 (very imprecise result), J = 0.7 (approximate result), and J = 0.85 (precise result). Again, it is interesting to compare the behavior between CPMC [ ] and GOP [ ]. GOP has outstanding results for J = 0.5 but the ranking is exchanged for J = 0.7 and 0.85, which reflects that the majority of results by 0162-8828 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. GOP has an imprecise representation but they are not missed. In contrast, CPMC provides results that are much more precise, but also many more misses.</p><p>Focusing on the boundary-based measure (bottom row), we observe that MCG and SCG are even better than the rest of SoA. This suggest that these techniques have very accurate boundaries but they might miss some parts of the objects, which is further penalized in the pixel-based measure. The saturation of the median Fb to 1 (middle plot) tells us that more than 50% of the results have perfectly accurate boundaries (within the matching tolerance) but the ones that do not are almost missed, because the mean is considerably lower than the median.</p><p>Overall, we propose the two measures and three generalization strategies together as a good representative of the quality and behavior of the object proposal methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Quantitative Meta-Measures</head><p>The state of the art of segmentation to compute the meta-measures is represented in this paper by MCG-UCM <ref type="bibr" target="#b4">[5]</ref>, gPb-UCM <ref type="bibr" target="#b13">[14]</ref>, ISCRA <ref type="bibr" target="#b57">[58]</ref>, EGB <ref type="bibr" target="#b58">[59]</ref>, Normalized Cuts <ref type="bibr" target="#b59">[60]</ref>, Mean Shift <ref type="bibr" target="#b60">[61]</ref>, NWMC <ref type="bibr" target="#b61">[62]</ref>, IID-KL <ref type="bibr" target="#b62">[63]</ref>, and Saliency Maps (on grayscale images) <ref type="bibr" target="#b63">[64]</ref>. As baselines, we use two techniques: a rectangular homogeneous grid (Quadtree) and a random merging of superpixels (Random). All methods (SoA and Baselines) are assessed at the Optimal Dataset Scale (ODS) <ref type="bibr" target="#b13">[14]</ref> with respect to each evaluation measure, that is, using the parameters that entail the best value of the measure in mean on the whole training set of BSDS500. In other words, we run the segmentation techniques sweeping their parameters (from coarse to fine partitions), and then choose the optimal parameter in terms of each evaluation measure, globally in the whole training set. Figure <ref type="figure" target="#fig_13">10</ref> shows an image, the various ground-truth partitions, and the baseline and SoA partitions at their ODS with respect to F b .</p><p>The parameter values of the newly proposed measure are: γ o = 0.95, γ p = 0.25, and β = 0.1. They have been trained on the training set of BSDS500, by optimizing the global meta-measure described below. Note that this optimization would not have been feasible without quantitative meta-measures.</p><p>As an additional property of the measures, we analyze their definition when multiple ground-truth annotations {G k } n 1 are available. The most common approach to evaluate a partition P using measure m is to compute the mean over all annotations 1 n n k m(P, G k ). In contrast, some measures have specific definitions that take further advantage of the multiple annotations. We also tested computing the maximum and median instead of the mean over annotations, with no significant differences in the results, thus we show the ones for the mean only.</p><p>Table <ref type="table" target="#tab_4">2</ref> shows the quantitative meta-measure results for the test set of BSDS500, as well as which measures have a specific definition for multiple ground truths.</p><p>In global terms, F b and F op are the two top-ranked summary measures. On top of that, they both provide much richer information in form of precision-recall curves. Interestingly, the two measures are also the only ones with a specific definition for the multipleground-truth case (See Section 4), which reinforces the intuition that specifying the definition is a good choice. We believe, therefore, that the tandem F b -F op should be the evaluation measures of choice. Section 6.4 reinforces this choice by showing their complementarity in realistic scenarios.</p><p>Regarding the computational cost of the measures, the mean time per image to compute the distances to the multiple-partition ground truth of BSDS500 is    3.79 ± 2.06 s for F b and at least one order of magnitude lower for the rest of measures. In particular, F op takes 0.078 ± 0.020 s. In scenarios where the time constraints are tight, therefore, F op would be the recommended measure (or the morphological approximation Fb ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Precision-Recall Frameworks</head><p>This section tests the proposed tandem of measures to compare a large set of state-of-the-art segmentation techniques, and evaluates the complementary behavior of F b and F op , which supports their use in tandem.</p><p>Figure <ref type="figure" target="#fig_14">11</ref> shows the boundary and objects-andparts precision-recall curves for the nine SoA segmentation methods studied, the two baselines, and the human performance. Prior to the assessment of segmentation techniques, let us focus on the comparison of the two evaluation frameworks.</p><p>Precision-recall value ranges: The theoretical range of F b and F op values is [0,1]. To estimate the maximum expectable range of values of each measure in practice, we take advantage of the fact that BSDS500 contains various annotations per image. To estimate the maximum experimental value, we evaluate the groundtruth partitions against the partitions done by other individuals, in a leave-one-out way. In the other extreme, we estimate the minimum experimental value by evaluating the ground-truth partition of a given image against the ground-truth of a different image. We represent both extremes as red asterisks.</p><p>It is noticeable that the human minimum performance for F b is 0.21, which could be interpreted as F b being too lax. In this same direction, the baseline boundary precision for F b is between 0.2 and 0.3, that is, any result, no matter how wrong it is, is judged as providing at least a 0.2 precision.</p><p>While in the case of F op the human baseline is correctly downgraded to 0.05 (as well as the swappedimage results), then the surprising fact is that human maximum performance is as low as 0.56 (0.81 in F b ), which could entail that F op is too strict.</p><p>To sum up, when judging results using both measures, one should take into account that the experimental range of values of F b is 0.21-0.81 and that of F op is 0.06-0.56, and extract the conclusions about their results with respect to these values. Analysis of the precision-recall curves: Regarding the comparison among segmentation techniques, both frameworks confirm that MCG-UCM outperforms the state of the art at all regimes under both measures. If we were to decide between gPb-UCM and ISCRA for the second place, however, gPb-UCM is consistently better in terms of boundary localization, while ISCRA outperforms gPb-UCM from the point of view of regions and parts.</p><p>The advantages of going beyond the summary measures are also clear on these plots. For instance, the summary F b measure of quadtree (0.41) judges this technique close to NWMC (0.55), but in the precisionrecall curves it is clear that quadtree is much worse. Similarly, judging by F b , NWMC would be discarded with respect to NCuts for instance, but if we are interested in low recall rates it could be of interest.</p><p>As common points between the two measures, NCuts is judged as being much better at high recall rates than at low ones and, conversely, NWMC is much better at high precision rates. The measures are coherent also in the fact that human results have a better precision than recall. As one of the main discrepant points, however, EGB is judged as the fourth best technique by F b while being the worse for F op . This behavior is further analyzed below.</p><p>Qualitative results on complementary cases: Figure <ref type="figure" target="#fig_15">12</ref> shows an image and the associated ground truth. The EGB result (a) consists of thin long regions that surround the object but do not close to create the regions of interest. The assessment value of this result is F b = 0.62 and F op = 0.05. From a regionbased point of view, this type of results is correctly penalized by F op and not by F b , since as a contour detector the result is correct.</p><p>We further compare the measures qualitatively by creating two academic examples (Figure <ref type="figure" target="#fig_15">12</ref> (b) and (c)) that show the complementary behavior, that is, examples where the F op behavior is not intuitive. First, partition (b) is composed of two boxes completely included on the objects of interest. F op interprets them as part candidates, since they are completely included in the objects and cover a significant part of them, so it does not penalize the partition significantly. On the other hand, F b penalizes the result because the contours of the boxes do not overlap with the true boundaries. If we slightly increase the size of the boxes (Figure <ref type="figure" target="#fig_15">12</ref> (c)), however, making the contours overlap but having a small part of the boxes outside of the object, the situation is changed: the boxes are not considered parts anymore (F op = 0.04) and the boundary measure does not judge the results as being very bad (F op = 0.28).</p><p>To sum up, intutively, both measures can be tricked by incorrect results giving good evaluation values, but F op will usually not fail when F b does and viceversa. In other words, F b and F op are very complementary.</p><p>Qualitative results at ODS: Figure <ref type="figure" target="#fig_16">13</ref> shows example partitions from four SoA techniques and a baseline. For each of them we plot the ground-truth partitions (first row), and the partition at the Optimal Dataset Scale (ODS) with respect to F b (second row) and F op (third row).</p><p>We observe a general trend especially in the number of regions in the partitions. In the case of F b , the partitions try to cover all the contour segments, even those marked only by one annotator, which usually leads to a number of small regions and over-fragmented results. On the other hand, the ODS partitions for F op have less regions, increasing the probability of having a single region approximating each object but in exchange, they miss more annotated contours.</p><p>This behavior is also reflected in the baseline partitions done by Quadtree (last column), in which having many small rectangles (ODS F b ) entails a better probability to sweep annotated contours, while having only eight big rectangles (ODS F op ) is the best chance to overlap with an annotated object.</p><p>Experiments reproducibility: We present the package SEISM <ref type="bibr" target="#b64">[65]</ref> (Supervised Evaluation of Image Segmentation Methods), which makes the code to compute all the measures publicly available, as well as all the segmentation results and scripts to make our research reproducible and to make it effortless for researchers to assess their segmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions of the experiments:</head><p>To sum up, both measures are complementary in terms of the properties of the partitions they evaluate, they both provide useful precision-recall curves, they achieve the best meta-measure results as summary measures, they have specific definitions for multiple ground truth, and their code is public to ensure reproducibility; thus we propose them in tandem as the tool of choice for image segmentation evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>This paper reviews and structures an extensive set of segmentation evaluation measures, showing that the 0162-8828 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. F measure versus Jaccard index: J and F are functionally related</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The three interpretations of image partition: Clustering of the pixel set (region-based), two-class clustering of the pairs of pixels (pairs-of-pixels-based), and detection of true pixel contours (boundary-based)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Region classification example: Regions are classified into object, part candidates, fragmentation candidates, and noise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FrequencyF</head><label></label><figDesc>b =0.15 F b =0.75 F b =0.28 F b =0.33</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. SIHD example: Distribution of F b values for the same-image pairs of partitions ( ) and differentimage pairs ( ). In gray rectangles, four representative pairs of partitions: a pair of correctly classified as different image (up-left) and as same image (up-right); and a pair incorrectly classified as different image (down-left) and as same image (down-right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 1.b(left) shows an example of partition obtained by a SoA method and by a quadtree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. SABD example: Correct and judgments by a segmentation evaluation measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. SISD example: Correct and incorrect judgments by a segmentation evaluation measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Object-based J versus F b : Complementary examples where the behavior of the two measures differs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fb=0. 7 Fb=0. 85 Fig. 9 .</head><label>7859</label><figDesc>Fig. 9. Object proposal evaluation: Pixel-and boundary-based measures (J and Fb ): mean, median and recall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. State-of-the-art examples. Top row: An image from BSDS500, the five ground-truth partitions done by different humans, and the partitions obtained by the two baseline techniques. Bottom row: Partitions obtained by the nine representative SoA segmentation techniques, each of them at its ODS with respect to F b</figDesc><graphic coords="11,200.07,150.07,52.67,78.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 11 .</head><label>11</label><figDesc>Fig.11. Precision-Recall curves for boundaries (left) and for objects and parts (right). The solid curves represent the nine SoA segmentation methods and the baselines (see legends). In dashed lines with the same color, the SoA techniques assessed on a swapped image. The marker on each curve is placed on the Optimal Dataset Scale (ODS), F measure in the legend. The isolated red asterisks refer to the human performance, i.e. ground truth partitions, assessed on the same image and on a swapped image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. F b versus F op : Complementary examples where the behavior of one of the measures is not the expected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Qualitative comparison of F b and F op : ODS partitions with respect to both measures</figDesc><graphic coords="14,48.28,203.21,93.67,62.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2</head><label>2</label><figDesc>Measure comparison in terms of quant. meta-meas.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2015.2481406, IEEE Transactions on Pattern Analysis and Machine Intelligence</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was partially supported by ERDF project BIGGRAPH-TEC2013-43935-R and FPU grant AP2008-01164.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond sliding windows: Object localization by efficient subwindow search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CPMC: Automatic object segmentation using constrained parametric min-cuts</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1312" to="1328" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic segmentation using regions and parts</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image segmentation by figure-ground composition into maximal cliques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal contour closure by superpixel grouping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levinshtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating object segmentation proposals using global and local search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rantalankila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Category-independent object proposals with diverse ranking</title>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="234" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An empirical approach to grouping and segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-08">Aug 2003</date>
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward objective evaluation of image segmentation algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Unnikrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="929" to="944" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparing clusterings: an axiomatic view</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meilȃ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Supervised assessment of segmentation hierarchies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An experimental comparison of range image segmentation algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hoover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="673" to="689" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recognition using regions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object recognition as ranking holistic figure-ground hypotheses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010-06">june 2010</date>
			<biblScope unit="page" from="1712" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Toward a generic evaluation of image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Corte-Real</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1773" to="1782" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distance measures for image segmentation evaluation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Irniger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Applied Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Measures and meta-measures for the supervised evaluation of image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zarys metod statystycznych w zastosowaniu do antropologii</title>
		<author>
			<persName><forename type="first">J</forename><surname>Czekanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Prace Towarzystwa Naukowego Warszawskiego</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1913">1913</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sørensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biologiske Skrifter</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image segmentation by probabilistic bottom-up aggregation and cue integration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="315" to="327" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evaluating multi-object tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01942</idno>
		<title level="m">MOT challenge 2015: Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Statistical validation of image segmentation quality based on a spatial overlap index</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bharatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Tempany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Haker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Jolesz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kikinis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academic radiology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="189" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Étude comparative de la distribution florale dans une portion des alpes et des jura</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin de la Société Vaudoise des Sciences Naturelles</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="547" to="579" />
			<date type="published" when="1901">1901</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/chal-lenges/VOC/voc2010/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2010 Results</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A comparative evaluation of interactive segmentation algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="434" to="444" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image-segmentation evaluation from the perspective of salient object extraction</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">New benchmark for image segmentation evaluation</title>
	</analytic>
	<monogr>
		<title level="j">Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="33" to="44" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving spatial support for objects via multiple segmentations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Using multiple segmentations to discover objects and their extent in image collections</title>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Left ventricular segmentation challenge from cardiac MRI: A collation study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Suinesiaputra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Statistical Atlases and Computational Models of the Heart</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How to evaluate foreground maps?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Shape matching: Similarity measures and algorithms</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shape Modeling and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Benchmarking image segmentation algorithms</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">RIGOR: Recycling Inference in Graph Cuts for generating Object Regions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Quantitative methods of evaluating image segmentation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Performance criteria for graph clustering and markov cluster experiments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dongen</surname></persName>
		</author>
		<idno>INS-R0012</idno>
	</analytic>
	<monogr>
		<title level="m">Centrum voor Wiskunde en Informatica (CWI)</title>
		<meeting><address><addrLine>Amsterdam, The Nederlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName><forename type="first">W</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">336</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Assignment problem in edge detection performance evaluation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image segmentation evaluation and its application to object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<ptr target="http://jponttuset.github.io" />
	</analytic>
	<monogr>
		<title level="j">UPC BarcelonaTech</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Universitat Politècnica de Catalunya</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A fast algorithm for MDL-based multi-band image segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kanungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Niblack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Steele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Reasearch Division</title>
		<imprint>
			<biblScope unit="volume">9754</biblScope>
			<biblScope unit="issue">84640</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>RJ. Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On parameter learning in crf-based approaches to object class image segmentation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Evaluating image segmentation algorithms using the pareto front</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="255" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">SLIC superpixels</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Üsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EPFL, Tech. Rep. 149300</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SLIC Superpixels Compared to State-of-the-art Superpixel Methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Üsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Geodesic object proposals</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Image segmentation by cascaded region agglomeration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">2004</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Mean shift: a robust approach toward feature space analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002-05">may 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Binary partition trees for object detection</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vilaplana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Salembier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2201" to="2216" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Region merging techniques using information theory statistical measures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Calderero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1567" to="1586" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Geodesic saliency of watershed contours and hierarchical segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1163" to="1173" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Jordi Pont-Tuset is a post-doctoral researcher at ETHZ, Switzerland, in Prof</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<ptr target="http://goo.gl/OLIXZV" />
	</analytic>
	<monogr>
		<title level="m">He received the degree in Mathematics in 2008, the degree in Electrical Engineering in 2008, the M.Sc. in Research on Information and Communication Technologies in 2010, and the Ph.D in 2014; all from the Universitat Polit ècnica de Catalunya</title>
		<imprint>
			<date type="published" when="2013">2013. 2015. 2014</date>
		</imprint>
	</monogr>
	<note>Luc Van Gool&apos;s computer vision group. UPC). He worked at Disney Research, Z ürich</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
