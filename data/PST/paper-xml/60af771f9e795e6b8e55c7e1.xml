<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Emotion-Infused Models for Explainable Psychological Stress Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Elsbeth</forename><surname>Turcan</surname></persName>
							<email>eturcan@cs.columbia.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Data Science Institute</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Data Science Institute</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Data Science Institute</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Emotion-Infused Models for Explainable Psychological Stress Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of detecting psychological stress in online posts, and more broadly, of detecting people in distress or in need of help, is a sensitive application for which the ability to interpret models is vital. Here, we present work exploring the use of a semantically related task, emotion detection, for equally competent but more explainable and human-like psychological stress detection as compared to a black-box model. In particular, we explore the use of multi-task learning as well as emotionbased language model fine-tuning. With our emotion-infused models, we see comparable results to state-of-the-art BERT. Our analysis of the words used for prediction show that our emotion-infused models mirror psychological components of stress.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As crises have begun to multiply worldwide, including the COVID-19 pandemic and the resulting economic downturn, psychological stress has risen dramatically 1 . The problem of detecting psychological stress, and more broadly, of detecting people in distress and in need of help, is a sensitive application; therefore, the ability to interpret the results, in order to understand why, is vital. The consequences of blindly trusting a black-box model and mislabeling users' stress levels could be serious in a deployed application such as a therapeutic chatbot, where some users may not receive the immediate help they need. Furthermore, models that make decisions based on psychology theory about factors that impact stress will be easier for humans to understand, and their mistakes will be more obvious.</p><p>Researchers have recently begun to study psychological stress, but in this work, we propose a new focus on examining the information our models use to make decisions and finding ways to incorporate psychological factors, like emotion, into them.</p><p>1 https://www.apa.org/news/press/ releases/stress/2020/report-october</p><p>To approach the problem of stress detection, which has much less labeled data than many popular classification tasks, we first note that stress has been shown to interact with emotion <ref type="bibr" target="#b23">(Lazarus, 2006;</ref><ref type="bibr" target="#b36">Thoern et al., 2016;</ref><ref type="bibr" target="#b24">Levenson, 2019)</ref>, a task that has far more publicly available labeled data. For example, individuals who are stressed are likely to express emotions such as fear, sadness, or anger and unlikely to express emotions such as happiness.</p><p>Traditional multi-task learning would normally be helpful in this situation, but there are no currently available datasets labeled with both stress and emotion. Even if there were, it would be beneficial to incorporate external information without re-labeling new datasets for each new combination of useful tasks. Here, we present work exploring how to use semantically related tasks-here, emotion detection-to create emotion-infused models capable of equally competent, but explainable, psychological stress detection as compared to a blackbox model. In particular, we explore the use of multi-task learning as well as emotion-infused language model fine-tuning, two existing frameworks which we examine through the lens of interpetability. Our code for this work is available at github. com/eturcan/emotion-infused.</p><p>Our contributions in this work are as follows: (i) consideration of factors suggested by psychological theory in deep learning methods for predicting stress, with a focus on emotion; (ii) an exploration of three different approaches to emotion-infused models, with experimental results showing comparable results to the state-of-the-art in all cases; and (iii) a framework for interpreting our models to show the impact of emotion and other factors in our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Researchers who use natural language approaches for stress detection often rely on external resources such as diagnostic questionnaires (e.g., <ref type="bibr" target="#b14">Guntuku et al. (2018)</ref>) or techniques like pattern matching (patterns such as "I am stressed", e.g., <ref type="bibr" target="#b38">Winata et al. (2018)</ref>; <ref type="bibr" target="#b25">Lin et al. (2017)</ref>) to assign labels. Much of the work that has been done on psychological stress detection focuses either on establishing baseline models with little advancement in computational modeling, or on using external information about the text (e.g., author, time of posting, number of replies), which is usually, but not always available and may differ in meaning or importance across platforms and domains.</p><p>There has also been a substantial amount of work on detecting related mental health concerns such as anxiety (e.g., Shen and Rudzicz (2017); <ref type="bibr" target="#b13">Gruda and Hasan (2019)</ref>; <ref type="bibr" target="#b19">Jiang et al. (2020)</ref>), but these are distinct from the generalized experience of stress.</p><p>The most similar work to ours is <ref type="bibr" target="#b37">Turcan and McKeown (2019)</ref>, our prior work publishing a dataset of psychological stress collected from the social media website Reddit and labeled by crowd workers, and presenting baselines with several basic non-neural and BERT-based models on this data. We use this dataset in our current work; however, we focus on exploring interpretable frameworks for this sensitive task and connecting the stress detection task concretely with emotion detection.</p><p>The models we propose in this work rely on two types of enhancements to the neural representation learned by models like BERT: multi-task learning and pre-training or fine-tuning. Multi-task learning is an increasingly popular framework in which some parameters in a model are shared between or used to inform multiple different tasks. Hard parameter sharing <ref type="bibr" target="#b2">(Caruana, 1993)</ref>, the variant we employ, uses some set of parameters as a shared base representation and then allows each task to have some private parameters on top and perform their own separate predictions. Multi-task learning has been successfully applied to many domains across NLP <ref type="bibr" target="#b35">(Sun et al., 2019;</ref><ref type="bibr" target="#b21">Kiperwasser and Ballesteros, 2018;</ref><ref type="bibr" target="#b26">Liu et al., 2019)</ref>; we are especially interested in instances where it has improved semantic and emotion-related tasks, such as <ref type="bibr" target="#b40">Xu et al. (2018)</ref>, who perform emotion detection with a suite of secondary semantic tasks including personality classification.</p><p>Pre-training and fine-tuning are another type of transfer learning where multiple tasks are trained in sequence rather than at the same time. Pre-trained language models are perhaps the most widely used example, where a large neural language model can Dataset Size Dreaddit 3,553 GoEmotions A,E,S 58K GoEmotions F SJ 4,136 Vent 1.6M be fine-tuned for many different tasks <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. Additionally, continuing to pre-train the language model itself on language from the target domain has been shown to improve performance <ref type="bibr" target="#b17">(Howard and Ruder, 2018;</ref><ref type="bibr" target="#b3">Chakrabarty et al., 2019;</ref><ref type="bibr" target="#b15">Gururangan et al., 2020)</ref> (also note <ref type="bibr" target="#b5">Chronopoulou et al. (2019)</ref>, who perform this task at the same time as the target task, in a form of multi-task learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., <ref type="bibr" target="#b10">Felbo et al. (2017)</ref>, who first fine-tuned on emoji detection and then fine-tuned on target semantic tasks including emotion and sentiment detection).</p><p>It should be noted that the psychological stress is much better studied in settings where researchers have access to some physiological signals (e.g., <ref type="bibr" target="#b42">Zuo et al. (2012)</ref>; <ref type="bibr" target="#b1">Allen et al. (2014)</ref>; <ref type="bibr" target="#b0">Al-Shargie et al. (2016)</ref>; <ref type="bibr" target="#b22">Kumar et al. (2020)</ref>; <ref type="bibr" target="#b18">Jaiswal et al. (2020)</ref>). This work is not as relevant to our task, since we have only text data available when detecting stress from online posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>A comparison of all the datasets we use in this work can be seen in Table <ref type="table" target="#tab_0">1</ref>. The primary dataset we use for this work is Dreaddit <ref type="bibr" target="#b37">(Turcan and McKeown, 2019)</ref>, a dataset of 3,553 segments of Reddit posts from various support communities where the authors believe posters are likely to express stress. The stress detection problem as expressed in this dataset is a binary classification problem, with crowdsourced annotations aggregated as the majority vote from five annotators for each data point. We note that this paper frames the stress classification problem in terms of the author and the time-i.e., a post is labeled stressful only if the poster themselves is currently expressing stress.</p><p>Because this dataset is small for training a deep learning model, we also experiment with larger datasets to provide auxiliary information. We se-lect the GoEmotions dataset <ref type="bibr" target="#b7">(Demszky et al., 2020)</ref>, which consists of 58,009 Reddit comments labeled by crowd workers with one or more of 27 emotions (or Neutral), for its larger size and genre similarity to Dreaddit. In this paper, we refer to the dataset in this form as GoEmotions all or GoEmotions A . The authors also published two relabelings of this dataset, achieved by agglomerative clustering: one where labels are clustered together into the Ekman 6 basic emotions (anger, disgust, fear, joy, sadness, surprise, neutral) (Ekman, 1992) (GoEmotions Ekman/E ), and one into simple polarity (positive, negative, ambiguous, neutral) (GoEmotions sentiment/S ). We run our experiments with each version of this dataset.</p><p>We also explore the use of another social media website, Vent. Vent is a platform more similar to Twitter or Tumblr than Reddit, where users post vents of any length and tag them as they like, and other users react to them or post comments. The benefit of Vent for this purpose is that posters selfidentify some emotion they are feeling from a large list of pre-made emotions. The data we use is collected by <ref type="bibr" target="#b27">Malko et al. (2021)</ref> <ref type="foot" target="#foot_0">2</ref> . We select Vent data that has been labeled with fear or sadness, which we hypothesize to be related to stress, as well as joy, for a contrast. We note that this dataset is strictly single-class, whereas GoEmotions may have more than one emotion label per data point. In all, there are 1.6M vents in our dataset, much larger than Dreaddit or GoEmotions; we randomly sample this data in a stratified manner to create a training, development, and test set with an 80/10/10 ratio. To examine the effects of domain similarity, we also select a subset of GoEmotions with the corresponding emotion labels -we subsample the existing "all" dataset to select only data points originally labeled with fear, joy, or sadness, for a final set of 4,136 data points (3,342 of which are the train set). We call this subset GoEmotions F SJ , and we compare it against Vent to see whether genre similarity or data size is more important in this multitask setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>We experiment with three types of emotion-infused models; that is, we present three different ways to incorporate emotion information into our stress detection models, divided into multi-task learning and fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Alternating Multi-Task Models</head><p>Our first multi-task models, which we refer to as Multi Alt , are simply two single-task models sharing the same base BERT representation layers. The models are alternating in that we train them with two datasets with two different sets of labels-i.e., we train the stress task with the Dreaddit data and the emotion task with the GoEmotions or Vent data. We refer to the variants with a subscript, i.e., Multi Alt</p><p>GoEmotions A (i.e., GoEmotions with all emotions), Multi Alt GoEmotions E (i.e., the Ekman GoEmotions relabeling), Multi Alt V ent (i.e., the Vent data), etc. The Multi Alt models can be seen in Figure <ref type="figure" target="#fig_1">1a</ref>. One loss step for these models consists of only one dataset and task, so they are trained with the negative log-likelihood (NLL) loss for single-label tasks (Dreaddit, Vent, GoEmotions F SJ ) and the binary cross-entropy (BCE) loss for multi-label tasks (GoEmotions A,E,S ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classical Multi-Task Models</head><p>We also experiment with a multi-task learning setup where we perform the two tasks at the same time on the same input data. We call this architecture Multi. However, because the Dreaddit data is labeled only with stress, we first separately train BERT models on the various versions of GoEmotions and use them to predict emotion labels for Dreaddit. We then take these emotion labels to be "silver data" and train on them alongside stress. The Multi model can be seen in Figure <ref type="figure" target="#fig_1">1b</ref>. Since stress detection is our main task in this work, we focus on this task where we have gold labels for stress, but note that it will be interesting in future work to experiment with other task settings, such as whether stress detection can improve emotion classification. In these models, the losses of the stress task and the emotion task are summed together for each batch using a tunable weight parameter, i.e., L = λL stress + (1 − λ)L emotion .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fine-Tuning Models</head><p>We experiment with models in which we first endow the BERT representation with knowledge of the emotion task by fine-tuning and then apply it to stress detection (as in <ref type="bibr" target="#b32">Phang et al. (2018)</ref>). We perform a sequential version of the Multi Alt models, in which we fine-tune a pre-trained BERT language model on another task, and then extract the language model parameters to initialize a BERT model that we continue to fine-tune  on Dreaddit. We denote these models as, e.g., Fine-Tune GoEmotions A Dreaddit for a model that was first trained on GoEmotions all and then on Dreaddit (for space, we will abbreviate Fine-Tune as FT). These fine-tuning models can be seen in Figure <ref type="figure" target="#fig_1">1c</ref>. These models are trained with the NLL and BCE losses as in the Multi Alt models.</p><p>5 Experimental Setup and Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>We present a re-implementation of the same BERTbased fine-tuning model used in <ref type="bibr" target="#b37">Turcan and McKeown (2019)</ref>, where this model performed best on Dreaddit. We report this as an average of 3 runs with distinct random seeds, and our results are, on average, lower than the single model reported, but with high variance. Because of this, we assume that the previously reported performance is from the high end of this variance and use our average score as our baseline in this work. This model is a pre-trained BERT language model (released as bert-base-uncased by <ref type="bibr" target="#b39">Wolf et al. (2019)</ref>; we use this same pre-trained language model as the basis for all our models) followed by a dropout layer and a dense classification layer. We also report a recurrent neural network (RNN) model, which uses either a long short-term memory network (LSTM) <ref type="bibr" target="#b16">(Hochreiter and Schmidhuber, 1997)</ref> or a gated recurrent unit (GRU) <ref type="bibr" target="#b4">(Cho et al., 2014)</ref> in place of the transformer from BERT and is otherwise the same. These models are trained with the NLL and BCE losses as with the Multi Alt models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training</head><p>We train all of our models with minibatch gradient descent using the Adam optimizer (Kingma and Ba, 2015) with a batch size of 16, given GPU space constraints. We perform gradient clipping to 1.0 to prevent exploding gradients. When training any model, we perform early stopping based on the F1 score on the Dreaddit development set and select the model parameters from the epoch that achieved the best development score for our final evaluated model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Hyperparameter Tuning</head><p>We tune hyperparameters for all our models using Bayesian Optimization from the Python library ax<ref type="foot" target="#foot_1">3</ref> . All models train the initial learning rate of the Adam optimizer and the dropout probability before the final classification layer; the Multi models also tune the loss weight parameter λ, and we also note that the RNN model tunes additional parameters such as the type of RNN, hidden dimension, etc. For all models, we tune parameters based on the F1 score on the Dreaddit development set; we train an ensemble of three models with three different, fixed random seeds and average their performance for a given parameter setting. We report the mean and standard deviation of three models, with three different random seeds, trained with the best hyperparameters. More details about hyperparameter tuning can be found in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>We report the results of our multi-task models in Table <ref type="table" target="#tab_1">2</ref> <ref type="foot" target="#foot_2">4</ref> . In general, our Multi Alt models perform similarly, and outperform the Multi models; we assume this is due to the introduction of noise in labeling the silver emotion data. Of these models, Multi Alt V ent performs best. With regards to GoEmotions, the 28-way classification of GoEmotions A naturally leads to lower numerical performance than the tasks with smaller numbers of classes, and we expect that GoEmotions S may group too many distinctly labeled emotions together under the same emotion labels; it seems GoEmotions E is the happy medium for this model. We also note that the Multi Alt V ent and Multi Alt GoEmotions E models perform equally well, which indicates that the genre mismatch is not an issue for this problem, or that Vent has a similar enough genre to Reddit that it does not affect the results. Somewhat surprisingly, Multi </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi Alt</head><p>V ent ; however, the GoEmotions data is much smaller than Vent, especially when subsampled to select specific emotions.</p><p>We further report the results of our finetuning models in Table <ref type="table" target="#tab_2">3</ref>. Because we expect that genre similarity should play a larger role when the secondary task can offer no direct training signal during the primary task finetuning, we evaluate on GoEmotions here and not Vent. Here, we observe that our best model, Fine-Tune GoEmotions F SJ Dreaddit , scores at least one standard deviation above BERT. We see higher increases in performance for the simpler classification problems in GoEmotions S and GoEmotions F SJ and worsened performance for GoEmotions A , suggesting that in the sequential paradigm, more complex tasks are not able to interact appropriately with the main task and instead interfere.</p><p>We also report the performance of the fine-tuning BERT models we trained on GoEmotions in order to label Dreaddit with emotion in Table <ref type="table" target="#tab_3">4</ref>; these results track well with the fine-tuning results reported by <ref type="bibr" target="#b7">Demszky et al. (2020)</ref>. Because these models are intermediates used for labeling, we report the F1 scores of the single model we actually used for labeling, although we tuned their parameters with an average of 3 different instances as with all other models. Many-way classification problems have much more opportunity for error and noise in an already-noisy process of labeling unlabeled data, so we use only the two best-performing GoEmotions models, which are those trained on the fewest-label datasets, GoEmotions S and GoEmotions F SJ , for our Multi models.</p><p>Overall, the inclusion of emotion information results in modest improvements, even though not statistically significant, as compared to BERT. However, our true goal in this work is to analyze the explainability of all of these models, to which we turn next. </p><formula xml:id="formula_0">GoEmo A GoEmo E GoEmo S GoEmo F</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>We perform three different analyses to probe our trained models and discover what information they learn to use. For our Multi Alt models, we investigate the usefulness of the emotion prediction layers in explaining stress classifications, and for all models, we use Local Interpretable Model-agnostic Explanations (LIME) <ref type="bibr" target="#b33">(Ribeiro et al., 2016)</ref> to show that our emotion-infused models rely on meaningfully different types of words than BERT in order to make their predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Multi-task Knowledge</head><p>We perform an analysis of our Multi Alt models to see what information they learn about emotion<ref type="foot" target="#foot_3">5</ref> . We take the development sets of each of the datasets (Dreaddit and GoEmotions) and predict their labels under the other task (i.e., emotion for Dreaddit and vice-versa). We report the correlation of these predicted labels with the gold labels in Table <ref type="table" target="#tab_4">5</ref> <ref type="foot" target="#foot_4">6</ref> . In this case, the GoEmotions F SJ variant is a singlelabel three-way classification problem, so we report the correlation ratio η <ref type="bibr" target="#b11">(Fisher, 1938)</ref>. The other GoEmotions variants are multi-label, so we report the coefficient of determination R 2 <ref type="bibr" target="#b6">(Cohen et al., 2015)</ref>. We further present breakdowns of the correlations per emotion category for the polarity and FSJ subsets of GoEmotions in Table <ref type="table" target="#tab_5">6</ref> and include the All and Ekman sets as well as the Vent data in the appendix. We observe that our multi-task models generally learn a moderate correlation between the stress labels and the emotion labels; they learn that negative emotions like fear and sadness are linked to stress and neutral or positive emotions are linked to non-stress, which makes intuitive sense. These emotion predictions can help explain the stress classifier's predictions; imagine, for example, showing a patient or clinician that the patient's social media shows a strong pattern of fear and anger as a more detailed explanation for places a stress classifier detects stress. From a machine learning perspective, this correlation also suggests the potential for using emotion data as distantly-labeled stress data to supplement the small extant stress datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">LIWC Analysis</head><p>We also investigate the types of information each model is using to make its decisions. In this section, we use the Linguistic Inquiry and Word Count (LIWC) <ref type="bibr" target="#b31">(Pennebaker et al., 2015)</ref>, a hand-crafted lexicon which collects words belonging to psychologically meaningful categories like positive emotion and cognitive processes, to categorize the information our different models use to predict stress.</p><p>We first analyze the unigrams our various models use to perform stress classification using LIME. LIME accepts an input from our development set, perturbs it in the bag-of-unigrams space, and runs one of our classifiers on each perturbation to calculate the importance of various unigrams; through  this process, we acquire the 10 unigrams with the highest magnitude output by LIME for each development example and consider them "explanations". We thus have 2,760 individual unigram explanations for the entire development set to analyze.</p><p>We then use the word lists from LIWC 2015's 72 psychological categories to see what types of words each classifier tends to use to make decisions of stress vs. non-stress. An abbreviated list of results showing our best models from each category is shown in Table <ref type="table" target="#tab_7">7</ref> 7 . We observe small but consistent effects suggesting that, in comparison to the basic BERT model, our emotion-enhanced models broadly learn to use the following information:</p><p>Affective information. Most emotion-infused models except for Multi learn to use affective information, which includes both positive and negative emotion words, more often. We see the largest increase in anger, one of the emotions we had identified as relevant to stress, for Multi Alt GoEmotions E , which makes intuitive sense because anger is one of the Ekman six basic emotions and thus, is explicitly predicted by this model.</p><p>Cognitive processes. All models show some increase in using words related to cognitive processes as compared to BERT; however, its subcategory Certainty, which includes words about absoluteness such as never, obvious, and clearly, shows larger changes. For example, Multi Dreaddit F SJ uses Certainty twice as often as BERT. These cognitive words seem to target the mental aspects of stress. Rumination and a focus on absoluteness are known signs of anxiety disorders, an extreme form of chronic stress <ref type="bibr" target="#b30">(Nolen-Hoeksema et al., 2008;</ref><ref type="bibr" target="#b28">Miranda and Mennin, 2007)</ref>.</p><p>Additional differences. We observe other, 7 More detail on the full table is available in the appendix.</p><p>smaller patterns among LIWC usage for these models. For example, the Multi Alt models use the most achievement-oriented words (although most models show modest increases), suggesting that this information, which includes words about success and failure, is relevant to emotion and to stress. This makes sense, since failing to achieve (e.g., failing a class) can be a major stressor. We also see larger proportions of biological process words used by all emotion-infused models. We suggest this is because Dreaddit includes posts taken from Reddit communities about anxiety and PTSD, where posters are likely to describe their physical and mental symptoms while seeking help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Salient Words</head><p>We then investigate the data itself for highly significant words using the measure of relative salience proposed by Mohammad (2012),</p><formula xml:id="formula_1">RelativeSalience(w|T 1 , T 2 ) = f 1 N 1 − f 2 N 2 .</formula><p>That is, it measures the importance of a token w in two different corpora T 1 , T 2 by subtracting their two relative frequencies (where f 1 , f 2 are the counts of token w in each corpus and N 1 , N 2 are the total tokens in each corpus). We compute this measure for all words in the Dreaddit training data, taking our two corpora to be the subsets labeled stress and notstress. We take the top 200 unigrams for each label (stress as opposed to non-stress and vice-versa) and provide some examples in Table <ref type="table">8</ref> with the full list of words available in the appendix. We examine the words and divide them into related groups in order to understand what types of information should theoretically be most important to classifying the data. For example, we see that different sets of function words are actually among the most important for both classes, with words like conjunctions typically appearing more indicative of stress</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>Example Words</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stress</head><p>Function Words and, but, how, like, no, not, or, where, why Negative Sentiment awful, bad, cry, fear, hate, stress, stupid Helplessness alone, can't, nothing, nowhere, trying</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non-Stress</head><p>Function Words a, for, if, some, the, was, who, will, would Positive Sentiment amazing, best, good, great, hope, nice Support email, helped, support, thank, together, we Table <ref type="table">8</ref>: Some examples of words identified by relative salience on the Dreaddit training data as indicative of stress or non-stress. We group the words by hand into semantically meaningful categories for ease of understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label BERT Multi</head><p>Alt</p><formula xml:id="formula_2">GE E Multi Alt V ent Multi Dr F SJ FT GE F SJ Dr Stress 33% 36% 32% 32% 33% Non-Stress 15% 15% 19% 18% 17%</formula><p>Table <ref type="table">9</ref>: A comparison of how often several of our models rely on words identified as salient for stress or non-stress to make their decisions, according to LIME. These numbers represent the percentage of available relative salience words each model selected in the top 10 LIME explanations. Dreaddit is Dr, and GoEmotions is GE.</p><p>(which echoes <ref type="bibr" target="#b37">Turcan and McKeown (2019)</ref>'s finding that stressful data is typically longer with more clauses), while non-stress includes words expressing future-thinking like if, will, and would. We also naturally find negative words for stress and positive words for non-stress, as well as a dichotomy of isolation and helplessness for stress vs. support and community for non-stress which is supported by psychological literature <ref type="bibr" target="#b12">(Grant et al., 2009)</ref>.</p><p>We then look at the intersection between relative salience and LIME explanations, counting how many LIME explanations are highly salient words for stress or non-stress; abbreviated results are shown in Table <ref type="table">9</ref> and the full table is available in the appendix. We see that our emotion-infused models learn to rely more often on words identified as indicative of non-stress, the minority class, instead of stress, the majority class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Error Analysis</head><p>We note that the presented models do sometimes make some new errors when incorporating emotional information, and that while these methods successfully incorporate such information with no feature crafting, some further innovation may be needed in order to use this information optimally. For example, we reproduce an example from our development set, with profanity censored:</p><p>And everyone was passive aggressive. The manager tried to peg down my salary multiple times like a f**king haggler at a market. Anyway, I decided to go get some antidepressants and the bottle fell out of my pocket, a coworker noticed and reported it to my boss. Who smiled and asked if there was anything I'd like to tell her. The passive aggressive s**t really got to me, and then I realized that I was being illegally paid.</p><p>The annotators for Dreaddit label this post not stress, presumably because there is not enough context for how the poster feels about this story presently, and the poster conveys more anger than anything else. The LIME explanations for the BERT model, which labels this correctly, include some profanity, but largely focus on function words. However, all four of our Multi Alt GoEmotions models misclassify this example as stressed and rely on words like aggressive (from passive aggressive) and the profanity to do so. Meanwhile, the emotion classifiers of our Multi Alt GoEmotions models are misled by words like smiled and label this example joy or positive. This is a difficult example; without noticing that the event happened in the past, it is easy to assume the poster is presently stressed. We believe examples like this require some groundingfor example, an understanding of what passive aggressive means and some representation of the timeline involved, that language models simply cannot express in the traditional classification setup.</p><p>We also reproduce an anonymized example where our emotion-infused models improve upon BERT:</p><p>She comes crying to me and formulates a plan to break up. She talks to &lt;name&gt; about their issues and her will to leave him wilts. She stays with him. Rinse and repeat, except it gets worse over time. How can I break the cycle, or help her break the cycle?</p><p>BERT misclassifies this example, where the author is stressed about a friend's situation, as nonstressful, relying on words like break and help, while our Multi Alt GoEmotions models successfully use the word crying to predict stress. We notice that crying or worse is the highest-ranked explanation for most of our emotion-infused models. These results are promising for the development of models that focus on information that humans consider intuitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we present a suite of emotionenhanced models that incorporate emotional information in various ways to enhance the task of binary stress prediction. All three types of our models achieve comparable performance to a state-of-theart fine-tuning BERT baseline, and, more importantly, we show that they result in more explainable models. We also introduce a new framework for model interpretation using LIME and show that our emotion-enhanced multi-task models offer a new dimension of interpetability by using the predictions of auxiliary tasks to explain the primary task. In our future work, we hope to expand these analyses to tasks in other domains and devise model architectures that can make more direct use of multitask learning to make and explain their predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Ethical Considerations</head><p>Our intended use of stress detection is to help those in distress. We envision systems such as therapeutic chatbots or assistants that can understand users' emotions and identify those in need so that a person can intervene. We would urge any user of stress detection technology to carefully control who may use the system.</p><p>Currently, the presented models may fail in two ways: they may either misclassify stress, or they may use the wrong information to make their predictions. Obviously, there is some potential harm to a person who is truly in need if a system based on this work fails to detect them, and it is possible that a person who is not truly in need may be irritated or offended if someone reaches out to them because of a mistake. In terms of explanations, we note that previous work has shown that focusing on incorrect rationales can unfairly target some groups of people <ref type="bibr" target="#b41">(Zhong et al., 2019)</ref>, although in this work we see that function words truly differ across the stressed and non-stressed populations and we do not observe any language that we know to be representative of minority groups in our explanations.</p><p>We emphasize our intention that emotional systems such as this be used responsibly, with a human in the loop-for example, a guidance counselor who can look at the predicted labels and offered explanations for their students' stress levels and decide whether or not they seem sensible.</p><p>We note that because most of our data was collected from Reddit, a website with a known overall demographic skew (towards young, white, American men<ref type="foot" target="#foot_5">8</ref> ), our conclusions about what stress looks like and how to detect it cannot necessarily be applied to broader groups of people. We also note that we have no way to determine the demographic information of the specific posters in any of our datasets and whether they differ from the overall Reddit statistics. We hope that we, and other researchers, can find ways to consider the specific ways in which minority groups express stress as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Type Range learning rate continuous [10 −6 , 10  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Reproducibility</head><p>We report the contents of the NAACL 2021 Reprodicibility Checklist that apply to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Training and Tuning</head><p>Our Multi Alt V ent , Multi Dreaddit S , and Multi Dreaddit F SJ models were trained on one Tesla V100 GPU with one CPU. All other models were trained on one Nvidia P100 GPU with one CPU.</p><p>Hyperparameter tuning was done the same way for every model, with Bayesian optimization as implemented by ax, with the F1 score on the Dreaddit development set as the criterion to optimize. Multi Dreaddit S and Multi Dreaddit F SJ were given 35 trials for time constraints; all other models were given 50 trials. All models were trained with a patience of 5 epochs and a tolerance of 0.0001 for dev set improvement, and allowed to run for a maximum of 20 epochs. All models tuned the initial learning rate and the dropout probability, with the Multi models also tuning the lambda weight parameter between their two task losses. Additionally, the RNN model was initialized from scratch and additionally tuned the embedding dimension, hidden dimension, number of layers, and type of RNN. Our parameter ranges are shown in Table <ref type="table" target="#tab_9">10</ref>.</p><p>The selected values of learning rate and dropout for all our models are shown in Table <ref type="table" target="#tab_10">11</ref>, rounded to two decimal places. We also note the remaining hyperparameters here: our RNN used a hidden dimension of 506, an embedding dimension of 137, and a 2-layer GRU. Additionally, Multi Dr F SJ selected λ = 0.90 and Multi Dr F SJ , λ = 0.67.</p><p>All of our models are similar in architecture and therefore take similar runtimes and have a similar number of parameters. Running our entire hyperpa-   <ref type="bibr">Stress. i,</ref><ref type="bibr">my,</ref><ref type="bibr">me,</ref><ref type="bibr">do,</ref><ref type="bibr">and,</ref><ref type="bibr">'m,</ref><ref type="bibr">n't,</ref><ref type="bibr">just,</ref><ref type="bibr">',</ref><ref type="bibr">feel,</ref><ref type="bibr">because,</ref><ref type="bibr">like,</ref><ref type="bibr">am,</ref><ref type="bibr">what,</ref><ref type="bibr">even,</ref><ref type="bibr">?,</ref><ref type="bibr">he,</ref><ref type="bibr">but,</ref><ref type="bibr">anxiety,</ref><ref type="bibr">m,</ref><ref type="bibr">so,</ref><ref type="bibr">myself,</ref><ref type="bibr">this,</ref><ref type="bibr">know,</ref><ref type="bibr">ca,</ref><ref type="bibr">it,</ref><ref type="bibr">now,</ref><ref type="bibr">have,</ref><ref type="bibr">out,</ref><ref type="bibr">get,</ref><ref type="bibr">no,</ref><ref type="bibr">about,</ref><ref type="bibr">t,</ref><ref type="bibr">feeling,</ref><ref type="bibr">up,</ref><ref type="bibr">bad,</ref><ref type="bibr">how,</ref><ref type="bibr">'ve,</ref><ref type="bibr">scared,</ref><ref type="bibr">not,</ref><ref type="bibr">him,</ref><ref type="bibr">over,</ref><ref type="bibr">going,</ref><ref type="bibr">all,</ref><ref type="bibr">tell,</ref><ref type="bibr">right,</ref><ref type="bibr">stop,</ref><ref type="bibr">want,</ref><ref type="bibr">anxious,</ref><ref type="bibr">past,</ref><ref type="bibr">to,</ref><ref type="bibr">fucking,</ref><ref type="bibr">need,</ref><ref type="bibr">hate,</ref><ref type="bibr">s,</ref><ref type="bibr">really,</ref><ref type="bibr">why,</ref><ref type="bibr">panic,</ref><ref type="bibr">where,</ref><ref type="bibr">happened,</ref><ref type="bibr">trying,</ref><ref type="bibr">still,</ref><ref type="bibr">when,</ref><ref type="bibr">days,</ref><ref type="bibr">makes,</ref><ref type="bibr">job,</ref><ref type="bibr">tired,</ref><ref type="bibr">or,</ref><ref type="bibr">shit,</ref><ref type="bibr">hard,</ref><ref type="bibr">getting,</ref><ref type="bibr">day,</ref><ref type="bibr">life,</ref><ref type="bibr">nothing,</ref><ref type="bibr">tl,</ref><ref type="bibr">dr,</ref><ref type="bibr">afraid,</ref><ref type="bibr">has,</ref><ref type="bibr">sorry,</ref><ref type="bibr">boyfriend,</ref><ref type="bibr">felt,</ref><ref type="bibr">crying,</ref><ref type="bibr">school,</ref><ref type="bibr">worse,</ref><ref type="bibr">don,</ref><ref type="bibr">go,</ref><ref type="bibr">attacks,</ref><ref type="bibr">sick,</ref><ref type="bibr">leave,</ref><ref type="bibr">deal,</ref><ref type="bibr">attack,</ref><ref type="bibr">anymore,</ref><ref type="bibr">being,</ref><ref type="bibr">work,</ref><ref type="bibr">im,</ref><ref type="bibr">having,</ref><ref type="bibr">constantly,</ref><ref type="bibr">thinking,</ref><ref type="bibr">almost,</ref><ref type="bibr">feels,</ref><ref type="bibr">been,</ref><ref type="bibr">worried,</ref><ref type="bibr">is,</ref><ref type="bibr">stress,</ref><ref type="bibr">which,</ref><ref type="bibr">family,</ref><ref type="bibr">due,</ref><ref type="bibr">fear,</ref><ref type="bibr">something,</ref><ref type="bibr">keep,</ref><ref type="bibr">everything,</ref><ref type="bibr">enough,</ref><ref type="bibr">every,</ref><ref type="bibr">back,</ref><ref type="bibr">worst,</ref><ref type="bibr">...,</ref><ref type="bibr">point,</ref><ref type="bibr">home,</ref><ref type="bibr">sometimes,</ref><ref type="bibr">car,</ref><ref type="bibr">down,</ref><ref type="bibr">making,</ref><ref type="bibr">angry,</ref><ref type="bibr">literally,</ref><ref type="bibr">feelings,</ref><ref type="bibr">actually,</ref><ref type="bibr">cry,</ref><ref type="bibr">horrible,</ref><ref type="bibr">wo,</ref><ref type="bibr">think,</ref><ref type="bibr">anyone,</ref><ref type="bibr">end,</ref><ref type="bibr">move,</ref><ref type="bibr">..,</ref><ref type="bibr">help,</ref><ref type="bibr">terrified,</ref><ref type="bibr">fuck,</ref><ref type="bibr">head,</ref><ref type="bibr">then,</ref><ref type="bibr">pain,</ref><ref type="bibr">losing,</ref><ref type="bibr">situation,</ref><ref type="bibr">depression,</ref><ref type="bibr">depressed,</ref><ref type="bibr">ve,</ref><ref type="bibr">made,</ref><ref type="bibr">money,</ref><ref type="bibr">coming,</ref><ref type="bibr">mom,</ref><ref type="bibr">safe,</ref><ref type="bibr">else,</ref><ref type="bibr">everyday,</ref><ref type="bibr">gets,</ref><ref type="bibr">honestly,</ref><ref type="bibr">thing,</ref><ref type="bibr">unable,</ref><ref type="bibr">turn,</ref><ref type="bibr">whole,</ref><ref type="bibr">terrible,</ref><ref type="bibr">alone,</ref><ref type="bibr">room,</ref><ref type="bibr">heart,</ref><ref type="bibr">saying,</ref><ref type="bibr">wake,</ref><ref type="bibr">awful,</ref><ref type="bibr">sleep,</ref><ref type="bibr">against,</ref><ref type="bibr">mentally,</ref><ref type="bibr">come,</ref><ref type="bibr">absolutely,</ref><ref type="bibr">nightmares,</ref><ref type="bibr">stupid,</ref><ref type="bibr">remember,</ref><ref type="bibr">lot,</ref><ref type="bibr">without,</ref><ref type="bibr">does,</ref><ref type="bibr">abuse,</ref><ref type="bibr">lose,</ref><ref type="bibr">class,</ref><ref type="bibr">sad,</ref><ref type="bibr">stuck,</ref><ref type="bibr">hell,</ref><ref type="bibr">suffer,</ref><ref type="bibr">cant,</ref><ref type="bibr">severe,</ref><ref type="bibr">emotions,</ref><ref type="bibr">leaving,</ref><ref type="bibr">/,</ref><ref type="bibr">flashbacks,</ref><ref type="bibr">hospital,</ref><ref type="bibr">close,</ref><ref type="bibr">memories,</ref><ref type="bibr">off,</ref><ref type="bibr">night,</ref><ref type="bibr">nowhere,</ref><ref type="bibr">abused,</ref><ref type="bibr">knowing,</ref><ref type="bibr">issues,</ref><ref type="bibr">trigger,</ref><ref type="bibr">the,</ref><ref type="bibr">a,</ref><ref type="bibr">her,</ref><ref type="bibr">she,</ref><ref type="bibr">we,</ref><ref type="bibr">for,</ref><ref type="bibr">.,</ref><ref type="bibr">in,</ref><ref type="bibr">your,</ref><ref type="bibr">would,</ref><ref type="bibr">be,</ref><ref type="bibr">),</ref><ref type="bibr">!,</ref><ref type="bibr">will,</ref><ref type="bibr">(,</ref><ref type="bibr">*,</ref><ref type="bibr">:,</ref><ref type="bibr">&lt;,</ref><ref type="bibr">that,</ref><ref type="bibr">are,</ref><ref type="bibr">who,</ref><ref type="bibr">&gt;,</ref><ref type="bibr">as,</ref><ref type="bibr">was,</ref><ref type="bibr">url,</ref><ref type="bibr">more,</ref><ref type="bibr">if,</ref><ref type="bibr">years,</ref><ref type="bibr">first,</ref><ref type="bibr">were,</ref><ref type="bibr">their,</ref><ref type="bibr">thank,</ref><ref type="bibr">us,</ref><ref type="bibr">met,</ref><ref type="bibr">people,</ref><ref type="bibr">his,</ref><ref type="bibr">them,</ref><ref type="bibr">our,</ref><ref type="bibr">an,</ref><ref type="bibr">they,</ref><ref type="bibr">said,</ref><ref type="bibr">one,</ref><ref type="bibr">together,</ref><ref type="bibr">others,</ref><ref type="bibr">share,</ref><ref type="bibr">let,</ref><ref type="bibr">best,</ref><ref type="bibr">food,</ref><ref type="bibr">other,</ref><ref type="bibr">&amp;,</ref><ref type="bibr">person,</ref><ref type="bibr">interested,</ref><ref type="bibr">please,</ref><ref type="bibr">study,</ref><ref type="bibr">each,</ref><ref type="bibr">here,</ref><ref type="bibr">asked,</ref><ref type="bibr">link,</ref><ref type="bibr">treatment,</ref><ref type="bibr">those,</ref><ref type="bibr">free,</ref><ref type="bibr">could,</ref><ref type="bibr">",</ref><ref type="bibr">take,</ref><ref type="bibr">great,</ref><ref type="bibr">same,</ref><ref type="bibr">support,</ref><ref type="bibr">good,</ref><ref type="bibr">",</ref><ref type="bibr">[,</ref><ref type="bibr">some,</ref><ref type="bibr">make,</ref><ref type="bibr">months,</ref><ref type="bibr">may,</ref><ref type="bibr">older,</ref><ref type="bibr">finally,</ref><ref type="bibr">bit,</ref><ref type="bibr">research,</ref><ref type="bibr">online,</ref><ref type="bibr">experience,</ref><ref type="bibr">little,</ref><ref type="bibr">through,</ref><ref type="bibr">hope,</ref><ref type="bibr">#,</ref><ref type="bibr">$,</ref><ref type="bibr">many,</ref><ref type="bibr">helped,</ref><ref type="bibr">edit,</ref><ref type="bibr">decided,</ref><ref type="bibr">friend,</ref><ref type="bibr">see,</ref><ref type="bibr">took,</ref><ref type="bibr">few,</ref><ref type="bibr">homeless,</ref><ref type="bibr">wanted,</ref><ref type="bibr">nice,</ref><ref type="bibr">information,</ref><ref type="bibr">thanks,</ref><ref type="bibr">around,</ref><ref type="bibr">",</ref><ref type="bibr">questions,</ref><ref type="bibr">any,</ref><ref type="bibr">date,</ref><ref type="bibr">went,</ref><ref type="bibr">later,</ref><ref type="bibr">everyone,</ref><ref type="bibr">looking,</ref><ref type="bibr">guys,</ref><ref type="bibr">ask,</ref><ref type="bibr">than,</ref><ref type="bibr">relationship,</ref><ref type="bibr">ago,</ref><ref type="bibr">'ll,</ref><ref type="bibr">sister,</ref><ref type="bibr">post,</ref><ref type="bibr">complete,</ref><ref type="bibr">'d,</ref><ref type="bibr">dating,</ref><ref type="bibr">year,</ref><ref type="bibr">both,</ref><ref type="bibr">current,</ref><ref type="bibr">mental,</ref><ref type="bibr">'s,</ref><ref type="bibr">send,</ref><ref type="bibr">18,</ref><ref type="bibr">moved,</ref><ref type="bibr">amazing,</ref><ref type="bibr">community,</ref><ref type="bibr">provide,</ref><ref type="bibr">items,</ref><ref type="bibr">read,</ref><ref type="bibr">however,</ref><ref type="bibr">name,</ref><ref type="bibr">x200b (i.e.,</ref><ref type="bibr">world,</ref><ref type="bibr">willing,</ref><ref type="bibr">different,</ref><ref type="bibr">guy,</ref><ref type="bibr">3,</ref><ref type="bibr">turned,</ref><ref type="bibr">area,</ref><ref type="bibr">visit,</ref><ref type="bibr">health,</ref><ref type="bibr">open,</ref><ref type="bibr">well,</ref><ref type="bibr">case,</ref><ref type="bibr">survivors,</ref><ref type="bibr">10,</ref><ref type="bibr">hear,</ref><ref type="bibr">'re,</ref><ref type="bibr">give,</ref><ref type="bibr">university,</ref><ref type="bibr">own,</ref><ref type="bibr">],</ref><ref type="bibr">hi,</ref><ref type="bibr">learn,</ref><ref type="bibr">couple,</ref><ref type="bibr">access,</ref><ref type="bibr">old,</ref><ref type="bibr">long,</ref><ref type="bibr">eventually,</ref><ref type="bibr">choose,</ref><ref type="bibr">agreed,</ref><ref type="bibr">began,</ref><ref type="bibr">love,</ref><ref type="bibr">reading,</ref><ref type="bibr">stories,</ref><ref type="bibr">loving,</ref><ref type="bibr">hey,</ref><ref type="bibr">experiences,</ref><ref type="bibr">include,</ref><ref type="bibr">preferences,</ref><ref type="bibr">forward,</ref><ref type="bibr">;</ref><ref type="bibr">,</ref><ref type="bibr">write,</ref><ref type="bibr">sub,</ref><ref type="bibr">1,</ref><ref type="bibr">posted,</ref><ref type="bibr">also,</ref><ref type="bibr">loved,</ref><ref type="bibr">page,</ref><ref type="bibr">email,</ref><ref type="bibr">start,</ref><ref type="bibr">away,</ref><ref type="bibr">sleeping,</ref><ref type="bibr">note,</ref><ref type="bibr">app,</ref><ref type="bibr">liked,</ref><ref type="bibr">helping,</ref><ref type="bibr">seemed,</ref><ref type="bibr">grateful,</ref><ref type="bibr">background,</ref><ref type="bibr">girl,</ref><ref type="bibr">talked,</ref><ref type="bibr">based,</ref><ref type="bibr">amazon,</ref><ref type="bibr">2</ref> We believe the numbers appearing in the nonstress list are indicative of financial posts where the authors indicate some amount of money has been raised or needs to be raised.</p><p>We include the full table of relative salience/LIME explanation counts in Table <ref type="table" target="#tab_5">16</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) The Multi Alt model. (b) The Multi model. (c) The Fine-Tune model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The emotion-informed architectures we use in our experiments.</figDesc><graphic url="image-2.png" coords="4,209.66,80.43,136.07,212.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The datasets we use in this work and their relative sizes (in terms of total number of data points).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results of our multitask models. The best result under each metric is bolded. GE is GoEmotions. Dr 79.75 ± 0.52 80.61 ± 0.40 FT GE F SJ Dr 80.25 ± 0.24 80.98 ± 0.20</figDesc><table><row><cell>Model</cell><cell>Binary F1</cell><cell>Accuracy</cell></row><row><cell>RNN</cell><cell cols="2">67.58 ± 1.22 68.86 ± 1.10</cell></row><row><cell>BERT</cell><cell cols="2">78.88 ± 1.09 79.11 ± 1.32</cell></row><row><cell>Multi Alt GE A Multi Alt GE E Multi Alt GE S Multi Alt GE F SJ Multi Alt V ent</cell><cell cols="2">79.02 ± 0.35 79.72 ± 0.69 80.24 ± 1.39 81.07 ± 1.13 79.46 ± 1.05 79.86 ± 0.50 79.17 ± 0.61 78.69 ± 1.86 80.34 ± 1.39 79.67 ± 2.03</cell></row><row><cell>Multi Dr S</cell><cell cols="2">78.97 ± 0.24 78.55 ± 0.07</cell></row><row><cell cols="3">Multi Dr F SJ 78.90 ± 0.59 78.55 ± 0.07</cell></row><row><cell>Model</cell><cell>Binary F1</cell><cell>Accuracy</cell></row><row><cell>BERT</cell><cell cols="2">78.88 ± 1.09 79.11 ± 1.32</cell></row><row><cell>FT GE A Dr</cell><cell cols="2">76.40 ± 0.50 76.83 ± 0.40</cell></row><row><cell>FT GE E Dr</cell><cell cols="2">79.44 ± 0.29 79.53 ± 0.46</cell></row><row><cell>FT GE S</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results of our fine-tuning models. The best result under each metric is bolded. GE is GoEmotions, and Dr is Dreaddit.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>AltGoEmotions F SJ does not do as well as Performance of our fine-tuning BERT models on the different GoEmotions labelings and datasets.</figDesc><table><row><cell>Dataset</cell><cell>Macro F1</cell></row><row><cell>GoEmotions A</cell><cell>48.98</cell></row><row><cell>GoEmotions E</cell><cell>62.16</cell></row><row><cell>GoEmotions S</cell><cell>69.65</cell></row><row><cell cols="2">GoEmotions F SJ 91.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Correlations of the gold labels for each dataset with labels predicted by the other task's classifier in a Multi Alt model. GoEmotions F SJ (abbreviated for space as GoEmo F SJ ) is starred because its emotion data is not multi-label and therefore the correlation ratio η is used instead of the coefficient of determination R 2 (which is used for the other, multilabel GoEmotions variants).</figDesc><table><row><cell>SJ *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Per-class scores of emotion and stress for Dreaddit (with gold stress and predicted emotion) and GoEmotions (with gold emotion and predicted stress). For GoEmotions S , these numbers are the Pearson correlation r of each individual emotion label with the stress labels; for GoEmotions F SJ , these are the average stress label assigned to data points in each emotion category, where 0 is non-stress and 1 is stress.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>A comparison of how often several of our models rely on words from several LIWC categories to make their decisions, according to LIME. These numbers represent the percentage of available LIWC words each model selected in the top 10 LIME explanations for the entire development set. Dr is Dreaddit, and GE is GoEmotions.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Hyperparameter ranges for our models. BERT-based models tuned the first two; the Multi models additionally tuned λ; the RNN additionally tuned the remainder.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Our models' selected hyperparameters. So that the table fits in a column, GE is GoEmotions, and Dr is Dreaddit. ± 1.16 82.49 ± 2.01 Multi Dr S 81.40 ± 1.54 82.49 ± 1.20 Multi Dr F SJ 82.58 ± 1.11 83.21 ± 1.46 FT GE A Dr 82.58 ± 1.53 82.13 ± 1.04 FT GE E Dr 82.58 ± 1.53 83.57 ± 1.71 FT GE S Dr 80.87 ± 1.15 82.49 ± 0.68 FT GE F SJ Dr 82.88 ± 0.92 84.54 ± 0.74</figDesc><table><row><cell>Model</cell><cell cols="2">Learning Rate P(dropout)</cell></row><row><cell>RNN</cell><cell>1.40×10 −4</cell><cell>0.86</cell></row><row><cell>BERT</cell><cell>4.27×10 −5</cell><cell>0.13</cell></row><row><cell cols="2">Multi Alt GE A Multi Alt GE E Multi Alt GE S Multi Alt GE F SJ Multi Alt V ent Multi Dr S Multi Dr F SJ FT GE A Dr FT GE E Dr FT GE S Dr FT GE F SJ Dr 5.03×10 −6 8.47×10 −6 1.08×10 −5 1.69×10 −5 8.98×10 −6 4.44×10 −5 1.14×10 −5 1.79×10 −5 7.30×10 −5 1.35×10 −5 1.95×10 −5</cell><cell>0.40 0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.00 0.09 0.03</cell></row><row><cell>Model</cell><cell>Binary F1</cell><cell>Accuracy</cell></row><row><cell>RNN</cell><cell cols="2">72.58 ± 0.50 74.15 ± 1.46</cell></row><row><cell>BERT</cell><cell cols="2">81.79 ± 0.45 82.97 ± 0.30</cell></row><row><cell>Multi Alt GE A Multi Alt GE E Multi Alt GE S Multi Alt GE F SJ Multi Alt V ent</cell><cell cols="2">81.31 ± 0.81 82.97 ± 0.51 80.30 ± 0.85 82.25 ± 0.59 80.79 ± 1.31 82.00 ± 0.74 81.87 ± 2.21 82.61 ± 2.42 82.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Results of all our presented models on the Dreaddit development set. So that the table fits in a column, GE is GoEmotions, and Dr is Dreaddit.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Due to license and ethics policy restrictions, we currently do not make this data publicly available.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://github.com/facebook/Ax</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">We did compute statistical significance by calculating the majority vote of each of the models' 3 runs and using the approximate randomization test, but no model is significantly different from BERT.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">We did perform an equivalent analysis on the Multi models, which shows similar trends, but as Multi Alt shows better performance, we omit it for space.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4">  6  We also note the possibility that different combinations of emotions are relevant to stress; however, not enough of our data is labeled with multiple emotion labels (4% of Dreaddit's silver labels from GoEmotionsS, 9% of GoEmotionsE) to test this hypothesis in this work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5">https://social.techjunkie.com/ demographics-reddit</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank our reviewers, as well as the members of our Natural Language Processing research group at Columbia University, for their insightful and constructive comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>rameter tuning setup described above takes about one day, and training one ensemble of three models takes about 25 minutes. BERT makes up the vast majority of our models' parameters, putting all of them at about 109B parameters (aside from the RNN, which has 7B).</p><p>Our performance for each model on the dev set is shown in Table <ref type="table">12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Evaluation</head><p>We note the standard equations of our reported metrics. For our Dreaddit models, we report binary F1, i.e., the harmonic mean of precision and recall for the positive class (here, stress):</p><p>, where TP represents the number of examples that were correctly classified as stress (true positives), FP those incorrectly classified as stress (false positives), TN those correctly classified as non-stress (true negatives), and FN those incorrectly classified as non-stress (false negatives). We also report classification accuracy, which is the fraction of samples classified correctly, i.e., TP+TN TP+FP+TN+FN .</p><p>We note that Table <ref type="table">4</ref> reports macro-averaged F1 in the multi-label and single-label cases. For both of these, we calculate the macro-average F1, which sums up TP, TN, etc. across all emotion labels and then calculates F1 score. For a multi-label input, we treat each label as a separate classificatione.g., if the model is incorrect with respect to one class but correctly identifies a second, the example counts towards incorrect examples for the first class and then again towards correct examples for the second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Data</head><p>Our data is all English social media data. Dreaddit and GoEmotions are taken from Reddit, and Vent from the social media platform Vent. Dreaddit consists of 3,553 labeled segments of posts taken from 10 subreddits: r/domesticviolence, r/survivorsofabuce, r/anxiety, r/stress, r/almosthomeless, r/assistance, r/food_pantry, r/homeless, r/ptsd, and r/relationships. 52.3% of the data is labeled stress, with the remaining 47.4% labeled nonstress. We use the train-dev-test split of <ref type="bibr" target="#b37">Turcan and McKeown (2019)</ref> into 2,562 train, 276 development, and 715 test examples. GoEmotions consists of 58,009 labeled Reddit comments taken from non-disclosed selection of subreddits <ref type="bibr" target="#b7">Demszky et al. (2020)</ref>. We refer the reader to the original publication's Figure <ref type="figure">1</ref> for details on the label distribution; GoEmotions uses 28 labels with a widely varying amount of data for each. We use the label groupings that the authors provide in order to evaluate on the Ekman labels and sentiment labels; these schemes group several of the 28 original labels together into smaller sets.</p><p>Our Vent data consists of 1.6 million Vents gathered in collaboration with the Vent platform. A much larger amount of data was collected, but we select the data with self-labeled emotion tags related to joy, sadness, and fear. These data were collected from 2013 to 2016; in their current form, we do not retain metadata about the posters. A group with whom we collaborate has collected this data, and due to licensing and ethics requirements, we are not able to release it publicly. We selected sadness, fear, and happiness based on intuition that they should be relevant to stress; we partitioned off a label-stratified 10% of the data for development and test each and the training set is 1.3 million examples. The label distribution of this dataset is 24.0% fear, 36.0% sadness, and 40.0% happiness.</p><p>We do not filter or remove any examples. The only preprocessing we perform is to apply the pretrained bert-base-uncased tokenizer from <ref type="bibr" target="#b39">Wolf et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Extended Analysis</head><p>We include the full tables of per-emotion correlations for Multi Alt GoEmotions A in Table <ref type="table">13</ref> and Multi Alt GoEmotions E in Table <ref type="table">14</ref>. We note that the alternating multi-task models do not predict all of their possible emotions on Dreaddit, although all possible emotions do occur in the GoEmotions development set. We also report the correlation coefficients η and mean stress prediction for the Multi Alt V ent model in Table <ref type="table">15</ref>. The full table of LIWC/LIME explanation counts for every model is too large to fit comfortably on a page, so we make a spreadsheet available at www.cs.columbia.edu/~eturcan/ data/emotion_infused_explanations. csv.</p><p>We include the top 200 relative salience unigrams (in order of relative salience) for stress and non-stress from the Dreaddit train set here. These are tokens as split by the Natural Language Toolkit (NLTK). We reproduce these exactly as they appear, and caution that they may include explicit or   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mental stress assessment using simultaneous measurement of eeg and fnirs</title>
		<author>
			<persName><forename type="first">Fares</forename><surname>Al-Shargie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Kiguchi</surname></persName>
		</author>
		<idno type="DOI">10.1364/BOE.7.003882</idno>
	</analytic>
	<monogr>
		<title level="j">Biomedical Optics Express</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3882" to="3898" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Nasreen Badruddin, Sarat C. Dass, and Ahmad Fadzil Mohammad Hani</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Biological and psychological markers of stress in humans: Focus on the trier social stress test</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">F</forename><surname>Cryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">G</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neubiorev.2013.11.005</idno>
	</analytic>
	<monogr>
		<title level="j">Neuroscience &amp; Biobehavioral Reviews</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="94" to="124" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multitask learning: A knowledge-based source of inductive bias</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
				<meeting>the Tenth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">IMHO fine-tuning improves claim detection</title>
		<author>
			<persName><forename type="first">Tuhin</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hidey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1054</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="558" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach for transfer learning from pretrained language models</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chronopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Baziotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Potamianos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1213</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2089" to="2095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Applied multiple regression/correlation analysis for the behavioral sciences</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">G</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leona</forename><forename type="middle">S</forename><surname>Aiken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Routledge</publisher>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GoEmotions: A dataset of finegrained emotions</title>
		<author>
			<persName><forename type="first">Dorottya</forename><surname>Demszky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeongwoo</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Cowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="4040" to="4054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Are there basic emotions? Psychological Review</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Ekman</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-295X.99.3.550</idno>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="550" to="553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName><forename type="first">Bjarke</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iyad</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sune</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1169</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-09">2017. 2017. September 9-11, 2017</date>
			<biblScope unit="page" from="1615" to="1625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Statistical methods for research workers</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Social Isolation and Stress-related Cardiovascular, Lipid, and Cortisol Responses</title>
		<author>
			<persName><forename type="first">Nina</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Steptoe</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12160-009-9081-z</idno>
	</analytic>
	<monogr>
		<title level="j">Annals of Behavioral Medicine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Feeling anxious? perceiving anxiety in tweets using machine learning</title>
		<author>
			<persName><forename type="first">Dritjon</forename><surname>Gruda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Souleiman</forename><surname>Hasan</surname></persName>
		</author>
		<idno>CoRR, abs/1909.06959</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Understanding and measuring psychological stress using social media</title>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Sharath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anneke</forename><surname>Guntuku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kokil</forename><surname>Buffone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><forename type="middle">C</forename><surname>Jaidka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Eichstaedt</surname></persName>
		</author>
		<author>
			<persName><surname>Ungar</surname></persName>
		</author>
		<idno>CoRR, abs/1811.07430</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MuSE: a multimodal dataset of stressed emotion</title>
		<author>
			<persName><forename type="first">Mimansa</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian-Paul</forename><surname>Bara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Burzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">Mower</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
				<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1499" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detection of mental health from Reddit via deep contextualized representations</title>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Ita Levitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Zomick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis</title>
				<meeting>the 11th International Workshop on Health Text Mining and Information Analysis</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scheduled multi-task learning: From syntax to translation</title>
		<author>
			<persName><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="225" to="240" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Stressnet: Detecting stress in thermal videos</title>
		<author>
			<persName><forename type="first">Satish</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>S M Iftekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Bullock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">H</forename><surname>Maclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Santander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Giesbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">T</forename><surname>Grafton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Stress and emotion: a new synthesis</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Lazarus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stress and illness: A role for specific emotions</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Levenson</surname></persName>
		</author>
		<idno type="DOI">10.1097/PSY.0000000000000736</idno>
	</analytic>
	<monogr>
		<title level="j">Psychosomatic Medicine</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="720" to="730" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detecting stress based on social interactions in social networks</title>
		<author>
			<persName><forename type="first">Huijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2017.2686382</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">09</biblScope>
			<biblScope unit="page" from="1820" to="1833" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1441</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
				<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
	<note>Long Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Expressing and reacting to emotions in a specialised online community</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Malko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cecile</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Duenser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mervi</forename><surname>Kangas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Wan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CSIRO</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Depression, generalized anxiety disorder, and certainty in pessimistic predictions about the future. Cognitive Therapy and Research</title>
		<author>
			<persName><forename type="first">Regina</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">S</forename><surname>Mennin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="71" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From once upon a time to happily ever after: Tracking emotions in mail and books</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saif</surname></persName>
		</author>
		<author>
			<persName><surname>Mohammad</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.dss.2012.05.030</idno>
	</analytic>
	<monogr>
		<title level="j">Decis. Support Syst</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="730" to="741" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking rumination</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Nolen-Hoeksema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blair</forename><forename type="middle">E</forename><surname>Wisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonja</forename><surname>Lyubomirsky</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1745-6924.2008.00088.x</idno>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="400" to="424" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The development and psychometric properties of liwc2015</title>
		<author>
			<persName><forename type="first">James</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">L</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayla</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Blackburn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Févry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>CoRR, abs/1811.01088</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Why should I trust you?&quot;: Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Túlio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939778</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-08-13">2016. August 13-17, 2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting anxiety through Reddit</title>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hanwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Rudzicz</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3107</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Computational Linguistics and Clinical Psychology -From Linguistic Signal to Clinical Reality</title>
				<meeting>the Fourth Workshop on Computational Linguistics and Clinical Psychology -From Linguistic Signal to Clinical Reality<address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">ERNIE 2.0: A continual pre-training framework for language understanding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/1907.12412</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attentional bias towards positive emotion predicts stress resilience</title>
		<author>
			<persName><forename type="first">Hanna</forename><forename type="middle">A</forename><surname>Thoern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Grueschow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Ehlert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">C</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brigit</forename><surname>Kleim</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0148368</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dreaddit: A reddit dataset for stress analysis in social media</title>
		<author>
			<persName><forename type="first">Elsbeth</forename><surname>Turcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-6213</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis LOUHI@EMNLP 2019</title>
				<meeting>the Tenth International Workshop on Health Text Mining and Information Analysis LOUHI@EMNLP 2019<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3, 2019</date>
			<biblScope unit="page" from="97" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Attention-based LSTM for psychological stress detection from spoken language using distant supervision</title>
		<author>
			<persName><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onno</forename><surname>Pepijn Kampman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno>CoRR, abs/1805.12307</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><surname>Gugger</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.03771</idno>
		<editor>Mariama Drame, Quentin Lhoest, and Alexander M. Rush</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Emo2vec: Learning generalized emotion representation by multitask training</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">Ho</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w18-6243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis</title>
				<meeting>the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31">2018. October 31. 2018</date>
			<biblScope unit="page" from="292" to="298" />
		</imprint>
	</monogr>
	<note>WASSA@EMNLP 2018. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fine-grained sentiment analysis with faithful attention</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<idno>CoRR, abs/1908.06870</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A multilingual natural stress emotion database</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012)</title>
				<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC-2012)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1174" to="1178" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
