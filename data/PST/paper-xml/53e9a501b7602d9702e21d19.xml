<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lneuro 1.0: A Piece of Hardware LEG0 for Building Neural Network Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Mauduit</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Duranton</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jean</forename><surname>Gobert</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jacques-Ariel</forename><surname>Sirat</surname></persName>
						</author>
						<title level="a" type="main">Lneuro 1.0: A Piece of Hardware LEG0 for Building Neural Network Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">43FDB52BE1E7F628BE01D7BBE5B0B64E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The status of our experiments on neural networks simulations on a parallel architecture is presented. A digital architecture was selected that is scalable and flexible enough to be useful for simulating various kinds of networks and paradigms. The computing device is based on an existing coarse-grain parallel framework (INMOS Transputers), improved with finer-grain parallel abilities through VLSI chips, which are called Lneuro 1.0 (for LEP neuromimetic circuit). The modular architecture of the circuit makes it possible to build various kinds of boards to match the expected range of applications or to increase the power of the system by adding more hardware. The resulting machine remains reconfigurable to accommodate a specific problem to some extent, both at the system level, through the Transputer framework, and at the circuit level. A small-scale machine has been realized using 16 Lneuros arranged in clusters composed of four circuits and a controller, to experimentally test the behavior of this architecture (the communication, control, primitives required, etc.). Results are presented on an integer version of Kohonen feature maps. The speedup factor increases regularly with the number of clusters involved (to a factor of SO). Some ways to improve this family of neural network simulation machines are also investigated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>UCH is expected from parallel computing: it must M exceed the performances of machines derived from the Von Neumann model and be able to investigate the kinds of problems for which the solution should emerge from a collective process. After half a century of studies involving many fields, neural networks are generating increasing interest since recent advances in learning algorithms [ 11, <ref type="bibr">[2]</ref> have enabled researchers to more efficiently master their behavior. To build a network related to the nature of the problem, a basic neuron is replicated in a variety of structures. In spite of the simplicity of the mathematical model, many current applications (signal processing, image analysis, etc.) use large networks and data bases and need the development of rapid, large-capacity simulation devices.</p><p>Various solutions have been proposed worldwide to the simulation problem, particularly using dedicated architectures <ref type="bibr">[3]</ref>. The VLSI architecture group at the Laboratoire d'Electronique Philips (LEP), Limeil-BrCvannes, France, has been pursuing a CMOS digital hardware approach since 1987. Our aim has <ref type="bibr">Manuscript received July 11, 1991;</ref><ref type="bibr">revised December 20, 1991.</ref>  been to build general-purpose, scalable simulation machines, to acquire a more accurate knowledge of the problems involved with parallelism and neural networks, and to improve dedicated architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">MACHINES FOR SIMULATING NEURAL NETWORKS</head><p>A major point of interest in neural networks is the intrinsic parallelism of the model. This leads to implementations on specific and often analog devices, to make the most of technological peculiarities. Such highly tuned designs attain very high performance, but this comes at the expense of flexibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architectural Requirements</head><p>In order to focus on networks rather than technology, we looked at general-purpose parallel machines of differing granularity. Having mapped the simulated "neurons" on the available processing elements (PE's), the major problem is then to allow efficient communications between them. Two digital machines illustrate the limits we fixed on our prototype:</p><p>The Connection Machine [4], [5], a fine-grain parallel computer, has 1-bit PE's. This seems well adapted to the simplicity of a "neuron." In addition, it was designed around communication paths, and a good efficiency is expected for problems of sufficient size. An important disadvantage is its closed structure, which cannot be tailored to a small application in a cost-effective way. Also, a 1-bit processor is not always well suited for certain neural network algorithms. SuperNode, the result of the European Project ESPRIT P1085, is a large-grain parallel Transputer-based computer. The Transputer is a 32-bit RISC microprocessor endowed with four asynchronous communication links. Messages are software routed if one PE has to communicate with more than four others. Here each PE is more complex, and may contain many "neurons" [6], <ref type="bibr">[7]</ref>. These machines present the important advantage of being scalable, with the possibility of adding Transputers boards when more computing power is needed. An affordable machine should keep the flexibility of SuperNode machines based on a high-level parallel framework, while increasing the computing power of each PE in a superscalar way, making them suited to neural network simulations. An increasing number of digital circuits and machines dedicated to neural networks simulations [8]-[ll] also tend to justify this design choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Computing Primitives</head><p>Most models rely on two vectorial primitives <ref type="bibr">[3]</ref>. If uJ represents the state of neuron j and wij the synaptic weight of the link from neuron i to neuron j , these primitives are as follow:</p><p>The weighted sum for the recall phase, followed by a bias and a scalar activation function, f [l], [3]:</p><formula xml:id="formula_0">N uj = f ( x w i j * u i + B j ) . (<label>1</label></formula><formula xml:id="formula_1">) i = l</formula><p>The bias term Bi can be integrated in the sum as the weight of a neuron clamped at + l . The Hebbian learning rule for training <ref type="bibr">[12]</ref> an extended vectorial sum (77 being a gain parameter):</p><p>A w ~, = 77 * * Sj.</p><p>(2)</p><p>For instance, Sj is replaced by e 3 , the error at node j in the case of the error back-propagation algorithm, or by vj for the Hebb rule. The computation of the error makes use of the first primitive, with a transposition of the synaptic matrix:</p><formula xml:id="formula_2">N e j = f ' ( u j ) * wji * ei.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a=1</head><p>A machine dedicated to neural network simulations might aim for an efficient implementation of those two primitives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">LNEURO 1.0: A BUILDING BLOCK</head><p>A. Choices and Implications 1) Technology and Design: We chose to follow a conservative approach for the design and technology of the first prototype, using 1.6,um CMOS standard cells with library RAM'S and compiled data paths. This helped to minimize the number of mistakes on the first run and facilitated the interfacing of the device with a traditional host computer to handle preprocessing and postprocessing in addition to the system services required by most applications. In addition, the use of an automated design flow (VLSI technology) enabled us to spend more time on the inner architecture of the circuit to develop the control abilities.</p><p>The main drawback of such an approach is a loss of silicon area, reducing the speed (because of interconnection capacitances) and decreasing the number of PE's per chip. A more up-to-date technology, the next-generation 1.0pm CMOS standard cells, should cut the area in half and increase the speed by 40%. In any event, aiming at a very fast circuit would imply making it more autonomous (thresholding function, advanced microcoded control abilities, etc.), and this would probably reduce its generality. In addition, building blocks of small size make it possible to tailor applications more closely, and also to further cluster level parallelism.</p><p>Giving up circuit-level optimization allowed us to quickly start experiments at the system level, where homogeneous behavior is expected from the architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p"</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LNeuro input parallelism</head><p>Systolic oulpul p;mllelism Fig. <ref type="figure">1</ref>. Semiparallelism over several circuits.</p><p>2) Inner Architecture: Lneuro is a semiparallel device: it holds the connections between two layers and computes in a single instruction the next state of one neuron or updates the weights reaching one neuron. Updating the complete set of neurons or weights is time-serialized. Sixteen PE's are working in parallel in this version of the circuit, either in the learning phase or in the propagation phase ( o ( N ) ) .</p><p>Compared with the alternative way of serializing [ l l ] , [13], which is more prone to systolic implementations, this one allows us to minimize the response time when the state of one neuron determines the next step of the algorithm (treelike architectures, constructive algorithms <ref type="bibr">[ 141-[ 161, etc.)</ref>. It also avoids useless computations in the case of partially used circuits and offers more generality by putting out of the circuit the threshold function, accessed once per cycle. The reconfiguration or the extension of a systolic machine requires modifying the computing loop covered by the data or opening it to the outside of the circuit. In an architecture with input parallelism, the reconfiguration of a layer relies only on the control (address of the current neuron), and layer extension is achieved by gathering out of the circuit aggregated partial computation results (Fig.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Choice of Dynamics:</head><p>Floating point computation is very convenient, particularly for testing an algorithm, but it is rather expensive in hardware area and speed. According to simulations and recent publications <ref type="bibr">[ 171-[ 191, integer</ref> processing appears to be a consistent compromise and still allows parallelism on one chip. In typical applications for the range of machines considered, 8-bit states of neurons are deemed to be enough, while 8-bit synaptic coefficients are adequate for the resolution phase and are increased to 16 bits for learning. Extended precision computations are also available through several calls to the hardware primitives. 4) Reconfigurability: To limit the area, the multiplications are bit serialized. This allows us to organize the memory resources of one circuit in various ways. For instance, the state of neurons may appear to be coded on from 1 to 16 bits. The internal 1K synaptic coefficients then span from a 16 + 64 nodes layer (16-bit inputs) to a 256 -+ 4 one (binary inputs).</p><p>For up to 8-bit states of neurons, the neural state register may be regarded as two independent banks, and allows synchronous updating policies or pipelining the loading through the additional specific bus (Fig. <ref type="figure">2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Modularity:</head><p>The assembly of several circuits allows the computation of larger networks with the same level of performance, or even higher, with several circuits computing independent nodes at the same time. A 32 + 64 nodes layer requires two Lneuros, as a 16 -+ 128 nodes layer. Multilayer networks can be built by cascading layers of Lneuros. This architecture can be scaled at the circuit level as well as at the system level.</p><p>6) Virtual Neurons: A mechanism for fast transfer of blocks of data between Lneuro and its host Transputer, together with the possibility of loading coefficients while Lneuro 1.0 is computing, allows us to efficiently drive more "neurons" than the available set of PE's strictly contained in the circuits. This kind of "paging" yields a loss of performance (evaluated to be a factor 1.1 to 2 compared with a fully hardware mapped network), but widens the apparent size of the machine. The loss rate depends on the controller (I/O abilities), on the architecture of the board (control load), and on the algorithm (use of the circuit before paging).</p><p>7) Coprocessor for the Transputer: Lneuro 1.0 contains an address decoding mechanism making it entirely memory mapped for the host controller. Data 1/0 and control instructions are executed by reading or writing at the appropriate addresses. In addition, the timings and control signals match those of a 25 MHz INMOS Transputer (40 ns cycle time) without wait state, making Lneuro primarily a parallel coprocessor for Transputer-based machines. On Lneuro, 1/0 operations are three cycles long, with most others one cycle. The host processor is free after having started long microcoded instructions (such as the dot product, from one to 20 cycles), for example, for controlling other Lneuros. Nevertheless, the Transputer achieves an external I/O operation in four or five cycles at best (move block). The simplicity of interfacing allowed us to put the chip on a wrapped board and develop the very first application in one and a half months <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hardware Resources of Lneuro 1.0</head><p>The core of Lneuro 1.0, inspired by a discrete machine [21], is described in Fig. <ref type="figure">2</ref>. In relaxation phase, the data flow is from the left to the right of the diagram: synaptic RAM, learning registers, multipliers, tree of adders, then accumulation or output. In the learning phase, the synaptic weights are loaded from the RAM, updated inside the learning registers according to the learning function latches, and eventually stored back in the RAM. The main bus, BT, provides all 1/0 and control operations, while dedicated buses BNSR and BE give additional access to the circuit for advanced boards. Like the bus of the available Transputers at the time of the design, the main bus, BT, is multiplexed and allows some byte addressing.</p><p>The library RAM has separate input and output buses to reduce the capacitive load. This RAM is 32 bit wide, so the circuit is organized in four neuron modules, each containing the resources needed for local computations: a RAM for synaptic weights storage; a portion of the neural state register (in standard cells to allow word access for 1/0 and bit access for computation); and a data path. The data path achieves the AND multipliers, the first two stages of the tree of adders, and the learning registers; it enables us to master more efficiently the delays and silicon area in comparison with standard cells. Two blocks of standard cells implement the last stages of the tree of adders and the control structure.</p><p>The layout (Fig. <ref type="figure" target="#fig_1">3</ref> ) is composed of four mirrored modules at the periphery of the circuit. The synaptic RAM'S occupy the corners, the data paths art within, and the neural state registers (standard cells) are close to the center. The two standard cells block in the center implement the last stages of the tree of adders, the control logic and accumulation devices. Eight basic four-neuron modules were initially to be integrated on a 32neuron circuit. Area considerations and parasitic capacitances reduced the circuit to four modules. Lneuro 1.0 is 144mm2 for 16 neurons. The routing requires a significant fraction of the area.</p><p>The 15 first prototypes, delivered packaged and functional, were successfully tested at 25 MHz, showing a speed disparity of about 4%. The circuit can operate at more than 35 MHz with tuned control signals.</p><p>1) Implementation of the Resolution Primitive: Fig. <ref type="figure" target="#fig_2">4</ref> details the implementation of ( <ref type="formula" target="#formula_0">1</ref> The synaptic weights, stored in the learning registers, are multiplied by one bit of each state of neuron, and added together through the tree of adders working in 2's-complement signed mode. A 32-bit adder, and the following 1-bit shifter and accumulator, are sequenced by a finite state machine. For example, this can execute the microcode required by the serialization of computations for up to 8-bit neurons. The external bus, BE, permanently outputs the intermediate result or the value of the accumulator. The control bus, BT, also has access to both values. The preceding result is reset upon starting a new computation or is added.</p><p>2) Implementation of the Learning Primitive: Fig. <ref type="figure" target="#fig_3">5</ref> shows the part of the data path implementing (2). The synaptic weights are obtained by two accesses to the RAM, possibly chained automatically. The eight most significant bits are fed to the input of the multipliers.</p><p>The coefficients are updated one bit at a time in a way similar to that for the resolution primitive. An 8-bit sequencer is also available, performing the required shifting on the current increment A. At each step, the computations for each PE (*A or 0) are executed according to the corresponding two bits output of the learning function. These two fields can independently reflect either the learning function latches or a truth table that depends on the current bit of the neural state register.</p><p>3) Control: A SIMD control mode has been added to the address decoder in the circuit, for up to 32 sets of Lneuros for each host controller. In addition to being decoded separately, each circuit may be decoded by its software number if it matches the corresponding part of the instruction. To avoid the risk of bus contention, only write operations support this mode. Most control instructions can be achieved by a write operation. This is particularly suitable when several circuits compute parts of the same layer, as well as for initialization, with negligible overhead for driving several circuits at the same time.</p><p>The microcoded automata can relieve the host of tedious tasks, such as serial computations. Two additional buses enable similar automata at the circuit level through an external microcontroller and a local thresholding mechanism (lookup table, etc.) to update a complete circuit with one command from the host or to realize fast dedicated boards. These are steps toward a hierarchical control flow, where the control tasks get lighter as their level increases. The study of the efficiency of these possibilities on practical cases will help to determine which steps to embed for increasing the abilities of the microcode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. A Range of Simulation Machines</head><p>Lneuro could also be controlled by a DSP to take advantage of the Harvard architecture <ref type="bibr">[22]</ref>, leading to faster control. The TMS320C40, from Texas Instruments, is a promising candidate for the job, with improved communication paths compared with the Transputer: accesses through two 32-bit nonmultiplexed buses and six 8-bit ports (mainly for inter-TMS320C40 communication) are announced each cycle (40 ns), without much load for the CPU thanks to advanced DMA abilities <ref type="bibr">[23]</ref>. This would be an easy way of increasing the effective speed of Lneuro 1.0 by at least a factor of 4 (for 1 / 0 as well as for control operations), and to approach the computing limits of one chip without the need of a dedicated, perhaps less expensive microcontroller, or for medium-scale machines (a few TMS320C40's for a few dozen Lneuros). For larger machines, the benefit of the INMOS T9000 [24] at an upper level, with hardware packet router and virtual channel processor, may lead to more tractable systems.</p><p>Additional information concerning the Lneuro 1.0 instruction set, the inner structure, and the operative modes, together with a survey of certain possibilities to gather several circuits in a machine and system level evaluations, may be found in ~5 1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iv. TESTS ON AN LNEURO-BASED SIMULATOR</head><p>The computing primitives can be used for a variety of neuromimetic paradigms. An image compression by principal components analysis was first demonstrated on one Lneuro <ref type="bibr">[20]</ref>. Multilayer perceptron networks, the very structure of Lneuro, are also under study <ref type="bibr">[26]</ref>.</p><p>In order to evaluate the behavior of this architecture with different sizes of problems and machines, a one-layer-like problem easy to tailor has been selected: Kohonen selforganizing feature maps <ref type="bibr">[l]</ref>. This paradigm has proved to be of wide interest, both when used alone (e.g. vector quantization <ref type="bibr">[27]</ref>) and in combination with supervised methods <ref type="bibr" target="#b0">[28]</ref>. It also provides a way for rapidly relearning additional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Description of the Test Device</head><p>The tests hereafter have been conducted on a 16 Lneuro 1.0 system, composed of four boards controlled by a 25 MHz T800 Transputer with 128 Kbytes of three cycle access time RAM, plugged in a PC compatible via an INMOS BO08 board (Fig. <ref type="figure" target="#fig_4">6</ref>). Being compatible with INMOS TRAM'S (Transputer modules with power and link connectors standardized), the same system can also be plugged in a VME bus of a workstation, via an INMOS BO14 board. It can also be connected to any machine having an INMOS-compatible link interface. The system services are handled on a root TRAM (20 MHz, 2 Mbyte of four cycle access time RAM), and an additionnal 20 MHz TRAM has been used as router (Fig. <ref type="figure">7</ref>). The timing unit is one period (40 ns on Lneuro boards).</p><p>The software has been developed with INMOS TDT tool kit (OCCAM compiler, postmortem debugger). The use of an interactive Lneuros monitor, using a protocol compatible with the development of a remotely controlled application from the root TRAM, has been shown to be useful for tuning an algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation of Additional Computing Primitives</head><p>In addition to the primitives directly implemented in hardware (eqs. ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula">2</ref> Lneuro), other vectorial operators can be derived from them. For example, initialization, module computation, and rescaling vectors of various dimensions have been optimized through Transputer assembly language routines using Lneuro features. In addition, ways to get around the hardware dynamic limits have been programmed, to allow the use of 1 to 16 bit (or more) neuron states and 8 to 16 bit (or more) synaptic coefficients with these primitives.</p><p>As a preliminary remark, the prototype boards are not completely tuned: some microcoded instructions cannot be used, leading to a loss of performance compared with the first wrapped board (corresponding values given in parentheses). The following timings stand for primitives at the test board level, called with a control overhead similar to an application environment. Better use of the Transputer assembly language might lead to marginal improvements, and the values are rounded off to the next ten: initialization of a complete circuit to a column pattern: 450 cycles (200); ' copy of the complete synaptic memory from three cycle RAM to the circuit: 1000 cycles; from the circuit to external RAM: 1300 cycles (1000); exchanging data between two synaptic columns and the neural state register: 200 cycles; computing the square module of an 8 bit, 16 element vector: 120 cycles; of a 16 bit one: 540 cycles; resolution (eq. ( <ref type="formula" target="#formula_0">1</ref>)) with 8 bit neuron states and 8 bit synaptic coefficients: 100 cycles; with 16 bit synaptic coefficients: 220 cycles; learning (eq. ( <ref type="formula">2</ref>)) 16 bit synaptic coefficients from 8 bit neuron states: 170 cycles (90); renormalizing step, by which each element of the vector is multiplied by a 16 bit integer A and shifted right by  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance of a Test Application: Kohonen Feature Maps</head><p>This application was not intended to be relevant from the point of view of neural algorithms, or to reach competitive results, but rather to test the behavior of the system. Different ways of allocating resources may be tried to find the bottlenecks of each approach.</p><p>The simulation of a Kohonen feature map programmed here performs the classification of 16 by 16 bit-map images (handwritten characters) in a nonsupervised way. The data base consists of 35 sets of 36 characters, split in a 360 prototype learning set and a 900 element evaluation set. The neigbhorhood is one-dimensional here, but other neighborhood definitions (dimension 2, spanning tree [29], etc.) may be implemented with the same efficiency, as long as this upper level PE labeling can be managed in a similar hierarchical way. The algorithm was programmed at first with 32 bit integers on the root Transputer, to test various sizes of the parameters; the allowed discretization is probably problem dependent.</p><p>The digitized algorithm does not behave as smoothly as the original one, particularly if small dynamic coefficients are used, which might lead to the need for more nodes than a floating point implementation, and careful tuning of the learning parameters (gain, neighborhood evolution laws, etc.). With 8 bit synaptic weights, which is the representation mode for faster computations, the results obtained were satisfactory enough to start programming Lneuro. An extension of the algorithm for controlling the number of learned prototypes by each PE has been proposed <ref type="bibr" target="#b24">[30]</ref>, and may improve the behavior of the network, particularly when the scalar product is employed instead of the Euclidian distance (slower on Lneuro 1.0) for the election criterion.</p><p>Initialization procedures based on supervised centroid computation with the knowledge of the classes of the prototypes have also been developed taking advantage of Lneuro primitives (eq. (l)), and may allow us to reduce the starting number of neighbors.</p><p>The Kohonen feature maps algorithm is implemented in the following way. For each prototype, the resolution primitive (eq. ( <ref type="formula" target="#formula_0">1</ref>)) is used to compare it with all nodes, while the learning one (eq. ( <ref type="formula">2</ref>) ) is used only to update the neighbors of the elected node. Equation (2) might also be used in the evaluation phase if one uses as election criterion the minimal Euclidian distance between nodes and prototypes, instead of the maximal dot product. In the evaluation phase, each Transputer evaluates and propagates up the tree of clusters the dot product with the current prototype and identification number of the winner node among the corresponding subtree; the global winner identifier is broadcast back.</p><p>The application is evaluated without the use of Lneuro, then with one circuit, one cluster (four Lneuros controlled by a Transputer), and finally the complete 16-Lneuro system (Table <ref type="table" target="#tab_0">I</ref>). The conditions of a larger, SuperNode-based machine have been simulated by connecting the boards in a ternary tree configuration, with the intermediate nodes endowed or not endowed with Lneuros. A part of the memory is reserved for locally paging the synaptic coefficients needed beyond the capacity of the Lneuros in each case. To reduce the link usage and keep most of the control task local, the neighborhood and data base presentation order are managed at the board level. The maximum size of the simulated networks depends on the number of clusters, the memory available at the cluster level, and the speed requirement. Networks with more than 2000 nodes (256 inputs each) can be contained in this machine. Considering the size of the data base, networks ranging from 64 to 512 nodes were tested. This allows us to store a complete copy of the data base on each board, ready for loading into the Lneuros (8 bit instead of binary bit maps), to reach the best performance level. In the case of a larger data base, the current prototype could be broadcast to each board before comparison with the local nodes, in a compressed format to reduce the communication overhead (less than 2 Mbyte/s between two adjacent Transputers; faster channel communications have also been proposed <ref type="bibr" target="#b25">[31]</ref>). The overhead arising from transmission is shown to be negligible in our case, as we maintain most of the control task local to each cluster.</p><p>A significant improvement in performance is reached from the use of one Lneuro: roughly eight times faster than a bare Transputer running at the same frequency (Table <ref type="table" target="#tab_0">I</ref>). The use of several circuits avoids some loss of speed when it suppresses paging (around 10% in our case). It also gives a significant increase owing to SIMD control mode. The speed approximately doubled with the use of four Lneuros per Transputer, especially because of the evaluation phase, which was implemented with fewer Read (non-SIMD) Lneuro operations. The choice of input parallelization minimizes the links traffic. It enables us to maintain these speedup factors when increasing the number of boards, as compared with bare Transputer structures for a suitable algorithm <ref type="bibr" target="#b26">[32]</ref>. The speedup factor reaches 80 for the four test boards versus the root TRAM (Fig. <ref type="figure">8</ref>). For instance, a 256 --f 512 network is evaluated in 2.5 s and updated in 11.4 s, compared with 142 s and 614 s with the Transputer alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Considering the results of the tests, a scalable digital neurocomputer based on a parallel coprocessor seems an efficient answer to the neural network simulation problem, producing a linear speedup factor with the machine size. The peak and the actually obtained performances with the Lneuro prototype (Table <ref type="table" target="#tab_1">11</ref>) can be put into perspective with a survey of neural network simulations on various machines reported in the literature (Table <ref type="table" target="#tab_2">111</ref>). This cost-effective architecture appears to be relevant, particularly if compared with large machines which reach the announced performance on a problem of specific size <ref type="bibr">[5]</ref>.</p><p>This architecture would also be well suited for implementing algorithms exploiting parallelism on the prototype data base, as in [5], which allows a delocalized control structure and leads to a straightforward scalable speedup.</p><p>Particular attention should be paid to the software environment, with design tools for helping to allocate and configure the resources of large machines (possibly based on a high-level language for an easy description of the neuronal problem) and with graphical interfaces to clearly show the behavior of the system for debugging. A TNode-based machine (derived from SuperNode) is to be built with 100 Lneuro 1.0 circuits for testing the architecture on a wider scale. This also benefits from a parallel interactive debugger for the Transputers through a dedicated control bus. Moreover, a board dedicated to a three-layer perceptron using additional buses and a hardware threshold function is to be tested on back-propagation. It may establish the efficiency of this communication path, making use of a microcontroller. These systems will help to demonstrate the modularity of the architecture, based on the same building block: Lneuro 1.0.</p><p>The next generation of Lneuro is under study. The hardware primitives will be refined from the behavior of the current machines and from system-level simulations. An efficient architecture should support more hierarchical data and control flows. It also should deal more efficiently with sparse matrices, for instance using less strongly coupled synapses <ref type="bibr" target="#b27">[33]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by ANRT #460/88. N. Mauduit is with the Department of Electrical and Computer Engineering, University of California at San Diego, La Jolla, CA 92093-0407. M. Duranton, J. Gobert, and J.-A. Sirat are with the Laboratoire d'Electronique Philips (LEP), B.P. 15, 22 avenue Descartes 94453 Limeil-Brevannes Cedex, France. IEEE Log Number 9106980.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 2. Block diagram of Lneuro 1.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Hardware implementation of the resolution primitive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Functional diagram of the learning latches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Sixteen-LNeuro 1.0 test system in a PC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig. 7. Sixteen-Lneuro 1.0 machine (four clusters).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>MAUDUIT</head><label></label><figDesc>ef al.: LNEURO 1.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I EPOCHS</head><label>I</label><figDesc>(IN SECONDS) FOR NETWORKS OF DIFFERENT SIZES</figDesc><table><row><cell>Type of Simulation</cell><cell>0 Lneuro</cell><cell>1 Lneuro</cell><cell>4 Lneuros</cell><cell>16 Lneuros</cell></row><row><cell>64 nodes resolution</cell><cell>19.3</cell><cell>2.5</cell><cell>1.3</cell><cell>0.33</cell></row><row><cell>64 nodes learning</cell><cell>79.0</cell><cell>9.3</cell><cell>5.9</cell><cell>1.47</cell></row><row><cell>64 nodes complete 64 nodes complete (without paging)</cell><cell>96.2 -</cell><cell>11.8 -</cell><cell>7.0 -</cell><cell>1.74 1.47</cell></row><row><cell>512 nodes resolution</cell><cell>142</cell><cell>22</cell><cell>10</cell><cell>2.5</cell></row><row><cell>512 nodes learning</cell><cell>614</cell><cell>83</cell><cell>45</cell><cell>11.4</cell></row><row><cell>512 nodes complete</cell><cell>758</cell><cell>105</cell><cell>55</cell><cell>13.8</cell></row><row><cell cols="3">Measures scaled to 25 MHz for the bare Transputer (root) by a factor of 0.8.</cell><cell></cell><cell></cell></row><row><cell cols="2">up to eight positions, for 16 bit synaptic coefficients: 300</cell><cell></cell><cell></cell><cell></cell></row><row><cell>cycles.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 11 PERFORMANCES</head><label>11</label><figDesc>OF THE CONSIDERED COMPONENTS</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>~</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Device</cell><cell>Type</cell><cell>MCPS</cell><cell>MCUPS</cell></row><row><cell></cell><cell></cell><cell cols="3">1 20 MHz Transputer</cell><cell>RISC</cell><cell>0.4</cell><cell>0.08</cell></row><row><cell></cell><cell></cell><cell cols="3">1 Lneuro limit (1 bit neurons)</cell><cell>Dedicated ASIC</cell><cell>100</cell><cell>160</cell></row><row><cell></cell><cell></cell><cell cols="3">1 Lneuro limit (8 bit neurons)</cell><cell>Dedicated ASIC</cell><cell>26</cell><cell>32</cell></row><row><cell></cell><cell></cell><cell cols="3">16 Lneuro 1.0 prototype</cell><cell>4 Dedicated boards</cell><cell>19</cell><cell>4.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(measured)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MCPS: million connections per second</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">MCUPS: million connections update per second</cell></row><row><cell cols="3">,..d: Epoch length (sec.)</cell><cell></cell><cell cols="2">/Epoch length (sec.)</cell></row><row><cell cols="3">4 clusters (1 6 Lneuros)</cell><cell></cell><cell cols="2">one 20 MHz Transputer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-1 600</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>800</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-4400</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-200</cell></row><row><cell>I 64</cell><cell>1 ; s</cell><cell>I</cell><cell>I 2hj</cell><cell cols="2">I I 512 Numberofnodes -100 -</cell></row><row><cell></cell><cell cols="4">Fig. 8. Scalability of the speedup.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 111 PERFORMANCES</head><label>111</label><figDesc>OF SOME COMPUTERS ON NEURONAL SIMULATIONS</figDesc><table><row><cell>Name</cell><cell>Type of Machine</cell><cell>MCPS</cell></row><row><cell></cell><cell></cell><cell>~</cell></row><row><cell>PCIAT</cell><cell>Serial</cell><cell>0.025</cell></row><row><cell>Ridge 32</cell><cell>Serial</cell><cell>0.05</cell></row><row><cell>VAX 750</cell><cell>Serial</cell><cell>0.1</cell></row><row><cell>SUN 3</cell><cell>Serial</cell><cell>0.25</cell></row><row><cell>IBM 3090</cell><cell>Vectorial machine</cell><cell>0.4</cell></row><row><cell>MARK I11</cell><cell>Bus architecture</cell><cell>0.5</cell></row><row><cell>Convex C-1</cell><cell>Par a 11 e 1</cell><cell>1.8</cell></row><row><cell>16K CM-I</cell><cell>Array Processor</cell><cell>2.6</cell></row><row><cell></cell><cell>in Hypertorus</cell><cell></cell></row><row><cell>10 Warp</cell><cell>Linear Array</cell><cell>17</cell></row><row><cell></cell><cell>Processor</cell><cell></cell></row><row><cell>AAP-2</cell><cell>Massively Parallel</cell><cell>18</cell></row><row><cell></cell><cell>Cellular Array</cell><cell></cell></row><row><cell>Odyssey</cell><cell>Bus Architecture</cell><cell>20</cell></row><row><cell>SAIC Delta2</cell><cell>Dedicated DSP</cell><cell>22</cell></row><row><cell></cell><cell>Board</cell><cell></cell></row><row><cell>20 Warp</cell><cell>Linear Array</cell><cell>32</cell></row><row><cell></cell><cell>Processor</cell><cell></cell></row><row><cell>Cray XMP 1-2</cell><cell>Vectorial Computer</cell><cell>50</cell></row><row><cell>Caltech</cell><cell>Dedicated CCD</cell><cell>500</cell></row><row><cell></cell><cell>Circuit</cell><cell></cell></row><row><cell>64K CM-2</cell><cell>Array Processor</cell><cell>1300</cell></row><row><cell></cell><cell>in Hypertorus</cell><cell></cell></row><row><cell cols="2">MCPS: million connections per second.</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The authors wish to thank A. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural net and traditional classifiers</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Info. Processing Syst., D. Anderson, Ed. NY: American Institute of Physics</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="387" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">J. Laaksonen. 0. Simula. and 0. Venta</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kanpas</surname></persName>
		</author>
		<author>
			<persName><surname>Kohonen</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Variant REFERENCES</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-Organisation and Associative Memory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Springer Verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><surname>Research Group</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">VLSI architectures for neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Treleaven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vellasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="page" from="8" to="27" />
			<date type="published" when="1989-12">Dec. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The Connection Machine</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Hillis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting the inherent parallelism of artificial neural networks to achieve 1300 million interconnects per</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf: I.C.N.N. (Paris)</title>
		<meeting>Conf: I.C.N.N. (Paris)</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parallel implementations of neural networks simulations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Petrowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Personnaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dreyfus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Girault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hypercube and Distributed Computers</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Verjus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Andre</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>North-Holland</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The implementation of multi-layer perceptrons on Transputer-based networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Beynon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dodd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Occam Users Group</title>
		<meeting>7th Occam Users Group<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Design, fabrication and evaluation of a 5-inch wafer scale neural network LSI composed of 576 digital neurons</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="527" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A VLSI architecture for high-performance, low-cost, on-chip learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hammerstrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="537" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An 1 1-million transistor neural network execution engine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Knorpp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pinkham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISSCC</title>
		<meeting>ISSCC</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="18" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fully digital CMOS integrated feedback network including the learning process</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weinfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dreyfus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Johannet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Personnaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Networks for Computing (Snowbird)</title>
		<meeting>Neural Networks for Computing (Snowbird)</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Organisation ofBehavior</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">0</forename><surname>Hebb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1949">1949</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A fast generation of neuro-ASICs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saucier</surname></persName>
		</author>
		<editor>Proc. I.C.N.N</editor>
		<imprint>
			<date type="published" when="1990-07">July 1990</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="563" to="567" />
			<pubPlace>Paris</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Three constructive algorithms foi network learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Ann</title>
		<meeting>8th Ann</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="662" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning in feedforward networks: The tiling algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mezard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nemork Computation in Neural Systems</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">A I P</forename><surname>Sirat</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1989-10">1989. Oct. 1990</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">423438</biblScope>
		</imprint>
	</monogr>
	<note>Neural trees: A new tool for classification</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Paulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Hollis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Network</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">399</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>suppl. l</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Better speed through integers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Tveter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Expert</title>
		<imprint>
			<biblScope unit="page" from="40" to="46" />
			<date type="published" when="1990-11">Nov. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weight representation and network complexity reduction in the digital VLSI implementation of neural nets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Note RNi</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
		<respStmt>
			<orgName>University College of London Int</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Design and realization of a 128 node neuro-computer</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Theeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Duranton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mauduit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Sirat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">P</forename><surname>Peretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Zurk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mougin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Programmable DSP architectures&quot; (parts 1 and 2)JEEE ASSP Magazine</title>
		<meeting><address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<publisher>Texas Instruments Ltd</publisher>
			<date type="published" when="1988-01">Feb. 1990. 1988. Oct. 1988. Jan. 1989. 1991. 1989. 1991</date>
			<biblScope unit="page" from="4" to="14" />
		</imprint>
	</monogr>
	<note>The Transputer Databook. and The 79000 Product Overview, 3INMOS Ltd. INMOS server e-mail address archive-server@inmos.com</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Study of a modular machine for neural network simulation: Design of a reconfigurable VLSI circuit with learning primitive</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mauduit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>UniversitC PARIS XI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. report</note>
	<note>in French. postscript files available at: nm@ief:iefgaris-sud. fr</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LNeuro boards: Implementing some applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aglan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf: Microelectronics of Neural Networks</title>
		<meeting>2nd Int. Conf: Microelectronics of Neural Networks<address><addrLine>Munich, Germany; Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">Oct. 16-18, 1991. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">of self-organizing maps</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="517" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adding a conscience to competitive learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Desieno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf I.C.N.N. (San Diego)</title>
		<meeting>Conf I.C.N.N. (San Diego)</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="117" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Higher speed transputer communication using shared memory</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Boianov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microprocessors and Microsystems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="67" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parallel implementation of Kohonen algorithm on Transputers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Auger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computing with Parallel Architectures</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Tnode</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gassilloud</surname></persName>
		</editor>
		<editor>
			<persName><surname>Grossetie</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="215" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SMART How to simulate huge networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf INNC 90 (Paris)</title>
		<meeting>Conf INNC 90 (Paris)</meeting>
		<imprint>
			<date type="published" when="1990">July 9-13, 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Fogelman-Soulic</surname></persName>
		</author>
		<title level="m">Connexionism: A Tutorial</title>
		<meeting><address><addrLine>Neuro-Nimes</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural network simulation at WARP speed: How we got 17 million connections per second</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Gusciora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf: I.C.N.N</title>
		<meeting>Conf: I.C.N.N<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural network simulation on a massively parallel cellular array processor: AAP-2</title>
		<author>
			<persName><forename type="first">T</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
		<meeting>IJCNN</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="155" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The creation of DELTA: A new concept in ANS processing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Works</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN (San Diego)</title>
		<meeting>IJCNN (San Diego)</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="159" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Implementation of back-propagation on a VLSI asynchronous cellular architecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Faure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mazarc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf: I.C.N.N. (Paris)</title>
		<meeting>Conf: I.C.N.N. (Paris)</meeting>
		<imprint>
			<date type="published" when="1990-07">July 1990</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="631" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A CCD based neural network integrated circuit with 64K analog programmable synapses</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Agranat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yariv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN (San Diego)</title>
		<meeting>IJCNN (San Diego)</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="577" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Informatique d&apos;Entreprise. He holds an MSc. degree in computer science from Paris VI University and a Ph.D. degree in electrical engineering from Paris XI University</title>
	</analytic>
	<monogr>
		<title level="m">He is presently a Research Physicist at the University of California at San Diego. His research interests include neural networks, parallel computer architecture, new forms of information processing, optoelectronic V U 1 architecture, and CAD</title>
		<imprint/>
	</monogr>
	<note>Nicolas Mauduit is</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">He then received the Dr. degree in information processing from the Institut National Polytechnique de Grenoble, France, in 1985 and the Ing. degree in computer science from the Ecole National Superieure d&apos;hformatique et de Mathematiques Appliquees de Grenoble</title>
	</analytic>
	<monogr>
		<title level="s">Ecole Nationale Superieure d&apos;Electronique et de Radioelectricite de Grenoble</title>
		<imprint>
			<date type="published" when="1984">1984. 1986</date>
			<pubPlace>France; France</pubPlace>
		</imprint>
	</monogr>
	<note>He joined the Laboratoires d&apos;Electronique Philips in 1987. His research interests are in neural networks, massively parallel computers and processors, and threaded code languages. He is involved in the European ESPRIT I1 project P 5293 &quot;Galatea&quot; (neurocomputing) on the realization of a general-purpose neurocomputer</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
