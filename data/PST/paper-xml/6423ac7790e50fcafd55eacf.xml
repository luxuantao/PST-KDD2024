<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pre-training Transformers for Knowledge Graph Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-28">28 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sanxing</forename><surname>Chen</surname></persName>
							<email>sanxing.chen@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
							<email>yangfeng@virginia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">?</forename><surname>Duke University</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">?</forename><surname>Microsoft</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Ads</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pre-training Transformers for Knowledge Graph Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-28">28 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.15682v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning transferable representation of knowledge graphs (KGs) is challenging due to the heterogeneous, multi-relational nature of graph structures. Inspired by Transformerbased pretrained language models' success on learning transferable representation for texts, we introduce a novel inductive KG representation model (iHT) for KG completion by large-scale pre-training. iHT consists of a entity encoder (e.g., BERT) and a neighboraware relational scoring function both parameterized by Transformers. We first pre-train iHT on a large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art results on matched evaluations, with a relative improvement of more than 25% in mean reciprocal rank over previous SOTA models. When further fine-tuned on smaller KGs with either entity and relational shifts, pre-trained iHT representations are shown to be transferable, significantly improving the performance on FB15K-237 and WN18RR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As a fundamental component of human intelligence, relational knowledge plays a crucial role in imitating human cognitive abilities with machine learning <ref type="bibr" target="#b10">(Halford et al., 2010)</ref>. Knowledge graphs (KGs) are the most widely used representation of relational knowledge, with well-known examples such as Freebase <ref type="bibr" target="#b1">(Bollacker et al., 2008)</ref>, <ref type="bibr">YAGO (Suchanek et al., 2007), and</ref><ref type="bibr">Wikidata (Vrande?i? and</ref><ref type="bibr" target="#b30">Kr?tzsch, 2014)</ref>. KG is also a key ingredient for many natural language understanding tasks, such as language modeling <ref type="bibr" target="#b19">(Peters et al., 2019)</ref>, question answering <ref type="bibr">(Sun et al., 2018a)</ref>, and commonsense reasoning <ref type="bibr" target="#b3">(Bosselut et al., 2019)</ref>. Despite significant human efforts in constructing KGs, they are still far from being complete, which motivates the development of automatic KG completion models.</p><p>A long line of past work has tried to explore generalizable representation for KG completion. Conventional embedding approaches focus on transductive generalizability, i.e., learning entity representation from the structure of KGs to infer new facts among existing entities <ref type="bibr" target="#b31">(Wang et al., 2017)</ref>. On the other hand, inductive approaches have also been studied to accommodate new entities that are not present in the training KGs, e.g., by building compositional representation of entities based on their textual descriptions <ref type="bibr" target="#b33">(Wang et al., 2014;</ref><ref type="bibr" target="#b35">Xie et al., 2016)</ref>. However, as the research in both direction only leverages in-domain data, the knowledge obtained is limited within a fixed body of KG data that are available in the domain of interest.</p><p>Inspired by the success of transfer learning in natural language processing (NLP), recent work exploits knowledge stored in large amount of unstructured texts, leveraging large-scale pre-trained language models (LMs) to improve the performance of KG completion models <ref type="bibr" target="#b37">(Yao et al., 2019;</ref><ref type="bibr" target="#b6">Clouatre et al., 2021)</ref>. Co-training LMs and KG completion models has been shown to be effective in improving the performance of downstream knowledgeintensive NLP tasks, but not so much for the KG completion task itself <ref type="bibr" target="#b32">(Wang et al., 2021;</ref><ref type="bibr" target="#b38">Yasunaga et al., 2022)</ref>. Despite the progress on transferring knowledge between structured KGs and unstructured texts, the generalization from one KG to another is still an open problem that is rarely studied <ref type="bibr">(Kocijan and Lukasiewicz, 2021)</ref>.</p><p>In this paper, we propose to pre-train a Transformer-based inductive KG representation model for KG completion. Our goal is to learn transferable knowledge representations with richer semantics from both unstructured texts (by initialization from pre-trained LMs) and structured KGs (by pre-training on a large-scale KG, Wiki-data5M). We first introduce iHT as our pre-training backbone. iHT employs the typical Transformer encoder for processing entity surface forms and is augmented with a Transformer-based relational scoring module <ref type="bibr" target="#b4">(Chen et al., 2021)</ref>, which is proven to be more expressive and versatile than conventional approaches, to capture various KG contexts. The hope is, via a unified Transformer architecture that underlies recent successes in transfer learning, the pre-trained model can be continually adapted to different KGs and downstream tasks.</p><p>We pre-train iHT on the Wikidata5M dataset, which is an encyclopedic KG containing millions of entities and triplets for KG completion and show that iHT outperforms previous state-of-the-art approaches by more than 25% relatively in terms of MRR. Upon further analysis, we carefully examine several critical design choices and discuss important factors such as model parameterization and negative sample size in building large-scale knowledge representation models for pre-training. We then test iHT on two standard KG completion benchmarks (FB15K-237 and WN18RR) by continual training the pre-trained model and show that large-scale pre-training significantly improves the performance of iHT on KGs of different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">iHT</head><p>Similar to conventional knowledge graph embedding (KGE) approaches, we train iHT on the KG completion task. Given a KG representing as a set of fact triplets (G = {(e s , r p , e o ), . . . }), where each triplet consists of two entities (subject entity e s and object entity e o ) and a predicate r p , the task basically holds out one of the entities and requires the model to recover it based on the rest information. For example, without loss of generality, when e o is missing the model will be using (e s , r p , ) as input to recover it in the output.</p><p>Our proposed model has two major components, i.e., an entity encoder to map the surface form of an entity to a continuous embedding space; and a context encoder to contextualize the entity embedding with a relational query and the entity's graph neighborhood. We implement both encoders using the Transformer architecture with the hope to take advantage of the architecture's scalability and transferability as shown in pretrained language models. We detail their design in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Entity Transformer</head><p>Conventional knowledge embedding approaches treat an entity as the atomic unit. As a result, the parameter size increases linearly with the number of entities. Scaling up to large-scale KGs with millions of entities in this way is challenging and computationally inefficient. Moreover, due to this transductive setting assuming all entities are seen during training, such approaches can not adapt to new entities.</p><p>However, entities in KGs of common domains such as encyclopedia are often associated with surface forms (e.g., entity description, name and alias). These textual representations leverage compositional semantics to depict any entity with a small vocabulary (typically tens of thousands) in natural language, connecting the textual semantics of an entity to its symbolic representation. If we can learn an entity encoder to uniquely identify an entity based on its surface forms, we can not only solve the scalability problem but also can potentially enable compositional knowledge sharing across entities. Our entity Transformer is designed to achieve this goal with a BERT-like encoder. Specifically, we follow the same input format as specified in <ref type="bibr" target="#b9">Devlin et al. (2019)</ref>, we tokenize the entity's surface form into subwords e = (e<ref type="foot" target="#foot_0">1</ref> , . . . , e L ) and pad the subword sequence with special tokens. 1</p><formula xml:id="formula_0">Seq BERT (e) = [CLS]e[SEP]</formula><p>(1)</p><p>We take the final hidden state corresponding to the [CLS] token from the entity Transformer as the entity embedding.</p><formula xml:id="formula_1">Embed ENT (e) = ET [CLS] (Seq BERT (e))<label>(2)</label></formula><p>As our entity Transformer shares the same architecture with BERT-like language models, it is natural to use pretrained language model weights to initialize the entity Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Context Transformer</head><p>Recent Transformer-based approaches for KG completion often employ relational scoring functions from conventional KG embedding approaches, such as TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> and Dist-Mult <ref type="bibr" target="#b36">(Yang et al., 2015)</ref>. However, these simple and often non-trainable scoring functions can only incorporate the relational query, ignoring rich graph contexts in KGs and leaving all the heavy lifting in learning and storing relational knowledge to the entity encoder. We instead draw on recent successes in using Transformer as both contextualizer and relational scorer <ref type="bibr" target="#b4">(Chen et al., 2021)</ref> to build our context Transformer.</p><p>There are two major types of contexts that we want to incorporate, i.e., the relational query r p and a set of directly linked entities N G (e s ) = {(r 1 , e n 1 ), . . . , } in the neighborhood of the source entity, where r i is the relation type between e s and e n i . We use the summation of the embeddings of neighborhood entity Embed ENT (e n i ) and the embeddings of the corresponding relation type as a relation-dependent entity representation for each neighborhood entity.<ref type="foot" target="#foot_1">2</ref> These relation-dependent entity representations are concatenated with the relational query r p , the source entity, and a special [GCLS] token as the inputs to the context Transformer. We also add type embeddings to distinguish different types of inputs, which eliminates the need for additional position embeddings. The output corresponding to the [GCLS] token can be seen as a context-dependent entity embedding for the source entity Embed CTX (e s ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Link Prediction</head><p>We train the iHT model with a link prediction objective. Given the positive (e s , r p , e o ) triplet, we sample a set of target entities uniformly to construct negative triplets. Empirically we find that model performance improves with the increasing number of negative samples. In order to achieve the largest possible negative sample size under memory constraints, we use in-batch negative sampling where negative targets are shared across all examples in the same batch and encoded only once. We reuse the entity Transformer to get the embeddings of every negative target entity and the only positive target entity e o . We then compute the dot-product similarity between each of them and Embed CTX (e s ). The loss is defined as the cross entropy between the softmax normalized distribution and the ground truth one-hot distribution. During the inference stage, such link prediction task is performed over all possible triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we examine the ability of iHT to handle a large-scale KG and generalize in-domain to unseen facts of known entities (transductive setting) and unseen facts with unseen entities (inductive setting).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Table <ref type="table" target="#tab_1">1</ref> shows the statistics of the three datasets used in our experiments. The experiments of this section are conducted on the Wikidata5M dataset <ref type="bibr" target="#b32">(Wang et al., 2021)</ref>, which is the largest of the three. The dataset, based on Wikidata and the English Wikipedia, consists of nearly five million entities and twenty million triplets covering a wide range of encyclopedic knowledge, mostly about people, places, and things such as movies.</p><p>Wikidata5M has two versions of dataset splits, i.e., a transductive split where models are evaluated for their performance in predicting missing links between known nodes (entities) that have been seen during training, and an inductive split where nodes for evaluation are unseen during training. Conventional KGE approaches are generally designed for transductive evaluation, thus unsuitable for inductive evaluation. As Wikidata5M is much larger than standard KG completion datasets (e.g., FB15K-237 and WN18RR in Table <ref type="table" target="#tab_1">1</ref>), a prohibitive increase in parameter size limits those traditional approaches to be applicable for the transductive setting here.</p><p>We report results of ranking metrics, i.e., mean reciprocal rank (MRR) and Hits@k, k ? 1, 3, 10 under the standard filtered setting <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>. When there are ties during ranking, we treat the ground truth as the last in the tie, which is the worst-case ranking result as done in Sun et al.  (2020b). 3   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Settings</head><p>We adopt a nine-layer entity Transformer and a three-layer context Transformer. Each layer is parameterized in the same way of BERT BASE <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>. We use the entity descriptions released with Wikidata5M as the entity surface forms, tokenized by BERT's uncased subword tokenizer, with a maximum length of 27. We randomly sam-3 Ties are rare (fewer than 5%) in our model predictions.</p><p>ple five neighbor nodes of the source entity from the training graph as the graph context for the transductive setting. In the inductive setting, we cannot make use of this graph context because the nodes used in evaluation are disconnected to the training graph. Although we observe that including such context can help for the transductive case, we leave how to incorporate graph neighborhood for the inductive setting to future work. <ref type="foot" target="#foot_2">4</ref> To distinguish the two iHT models trained with and without graph contexts, we call the former iHT G and the latter single-entity model iHT E .</p><p>We use the AdamW <ref type="bibr" target="#b15">(Loshchilov and Hutter, 2019)</ref> optimizer with a learning rate of 5e-4. For models initialized with pre-trained BERT weights, we set a lower learning rate of 3e-5 for the loaded weights. Learning rates linearly warm up for the first 10% of total steps, then decay to zero. We allocate a batch of 128 examples to each GPU with an effective total batch size of 2048 on 16 GPUs. The in-batch negative sample size for each GPU is set to 1000. We run each model once with a fixed set of seeds. Training for 10 epochs usually takes about 60 hours on 16 NVIDIA Tesla V100 GPUs using PyTorch's DDP trainer. Evaluation is done with the last epoch checkpoints in an RTX 2080. Details of hyperparameter tuning are described in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Results</head><p>We compare our iHT model against two groups of baselines: (1) conventional KGE methods such as Model Params MRR? Hits? @1 @3 @10 TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> 2.4T .253 .170 .311 .392 DistMult <ref type="bibr" target="#b36">(Yang et al., 2015)</ref> 2. <ref type="bibr">4T .253 .208 .278 .334 ComplEx (Trouillon et al., 2016)</ref> 2 Model MRR? Hits? @1 @3 @10 MLMLM <ref type="bibr" target="#b6">(Clouatre et al., 2021)</ref> .284 .226 .285 .348 KEPLER <ref type="bibr" target="#b32">(Wang et al., 2021)</ref> .402 .222 .514 .730 BLP-SimplE <ref type="bibr" target="#b7">(Daza et al., 2021)</ref> .493 .289 .639 .866 iHT E (Ours)</p><p>.634 .517 .703 .871</p><p>Table <ref type="table">3</ref>: Link prediction results on the inductive split of Wikidata5M. Numbers in bold represent the best results.</p><p>TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, DistMult <ref type="bibr" target="#b36">(Yang et al., 2015)</ref>. These methods store entity embeddings in a large lookup table and use simple relation-aware scoring functions to compute the similarity between entities. As mentioned earlier, the size of the lookup table accounts for the large parameter size of these methods, and they only work for the transductive setting;</p><p>(2) compositional knowledge representation approaches, including KEPLER <ref type="bibr" target="#b32">(Wang et al., 2021)</ref>, MLMLM <ref type="bibr" target="#b6">(Clouatre et al., 2021)</ref>, and KGT5 <ref type="bibr" target="#b21">(Saxena et al., 2022)</ref>. These methods all use a Transformer to encode entity surface forms, but they differ in the way they decode target entities. MLMLM and KGT5 both leverage distribution of the language modeling objective to estimate the likelihood of target entities, while KEPLER uses a TransE-like scoring function. In contrast, we use the context Transformer as a decoder to directly predict the target entity.</p><p>Our iHT model outperforms all prior methods by a large margin, under both the transductive (Table 2) and inductive (Table <ref type="table">3</ref>) settings on Wiki-data5M, establishing a new state-of-the-art result. In the transductive setting, we see a clear advantage of compositional knowledge representation approaches over traditional KGE methods in terms of model size. However, previous compositional knowledge representation approaches are still on par with best performing KGE methods such as SimpIE in terms of link prediction performance, demonstrating the difficulty in building compositional representation from entity surface forms. iHT surpasses all baselines and even beats the ensemble of a T5-like Transformer model and transductive ComplEx embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablations and Analyses</head><p>To better understand the design choices we made and what contributes to the performance, we further provide ablations and analyses. Due to computational resource constraints, all ablation and analytical experiments are run for five epochs with other training details the same as in the main experiments. As shown in Table <ref type="table" target="#tab_3">4</ref>, the full models retrained for five epochs are only slightly worse.</p><p>Support Set Support sets consist of a source entity's neighbors in the training graph. This compositional feature relieves the model from the burden of memorizing the local graph context of the source entity <ref type="bibr" target="#b4">(Chen et al., 2021)</ref>, and could help the model identify entities when their surface forms are ambiguous. The superior performance of iHT G over iHT E on the transductive setting demonstrates the helpfulness of support sets. More evidence will be Entity Transformer Initialization To better utilize the compositional information of entity surface forms, we initialize the entity Transformer with pretrained BERT weights. As anticipated, the model performance improves when using pretrained BERT weights compared to random initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Early/Late Fusion of Relation Embeddings</head><p>Recall that in iHT design, all the relation embeddings (either from predicate or neighbors) are directly fed into the context Transformer, skipping the entity Transformer. It is possible that the entity Transformer could benefit from the relational information. The ablation results show that this is indeed the case, but the improvement is not significant. We view this as a trade-off between the expressive power of the entity Transformer and computational efficiency as the entity Transformer has a much larger input size than the context Transformer and thus adding one more token to the input would increase the O(N 2 ) computation cost more.</p><p>Entity Surface Forms When replacing conventional lookup entity embeddings with compositional entity embeddings produced from entity surface forms, the choice of the type of surface forms and the corresponding encoder is conceivably important for modeling meaningful representations of entities. As we can see in are not as harmful in the transductive setting, where the model can see and memorize all entity names in the training graph.</p><p>Shallow Entity Transformer From Table <ref type="table" target="#tab_5">5</ref>, we can see consistent improvements coming from increasing the proportion of bottom entity Transformer layers in the overall architecture, backing our design choice of the deep entity Transformer, and shallow context Transformer architecture. 5 This further suggests that encoding entity surface forms requires non-trivial modeling expressiveness.</p><p>Negative Sample Size Negative sampling is widely adopted in KG representation learning to distinguish the right target entity from others in the intractable entity space during training. It has been shown to have a huge impact on the quality of learned representations <ref type="bibr" target="#b13">(Kotnis and Nastase, 2017)</ref>.</p><p>The bottom part of Table <ref type="table" target="#tab_5">5</ref> shows that increasing the number of negatives from 100 to 1000 consistently improves link prediction results and further improvements seem to be marginal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Transfer Experiments</head><p>In this section, we transfer Wikidata5M pre-trained models to two other KGs by continual training and evaluate the out-of-distribution adaptability of iHT.</p><p>5 Further increases of entity Transformer size are difficult due to GPU memory constraint as entity Transformer usually has a much larger input size than that of the context Transformer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use the FB15K-237 dataset <ref type="bibr" target="#b28">(Toutanova and Chen, 2015)</ref> and the WN18RR dataset <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref> as the target KGs. As another encyclopedic KG, FB15K-237 comes from arguably the same domain as Wikidata5M, 94% of the entities in FB15K-237 are also in Wikidata5M, but they focus on different relation sets. We observe that about 80% of the unique (e s , e o ) pairs in FB15K-237 are not present in Wikidata5M, suggesting a distributional shift. WN18RR is a subset of Word-Net <ref type="bibr" target="#b17">(Miller, 1995)</ref> that contains facts about word relations, such as synonyms, antonyms, hypernyms, etc., which presents a much larger domain shift than FB15K-237. Besides the difference in domains, we also want to evaluate the ability of iHT to deal with entities unseen in the training graph. To this end, we additionally adopt inductive versions of the two datasets <ref type="bibr" target="#b7">(Daza et al., 2021)</ref>, where all triplets in the dev and test sets contain at least one novel entity. Details of the datasets are provided in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>The experimental settings are mostly the same as in the previous section with the following exceptions. Here, besides using an initialization with pre-trained BERT weights, we have another option to initialize the model with iHT model pretrained on Wikidata5M. When using this option, all weights are initialized with the pre-trained iHT model, except for the relation embeddings, which are randomly initialized. We denote this model iHT-Wiki to distinguish it from the model initialized with BERT weights. The entity surface forms for FB15K-237 and WN18RR are provided by <ref type="bibr" target="#b7">Daza et al. (2021)</ref>, used by both inductive and transductive settings. As the entity surface forms for WN18RR are generally shorter than those for encyclopedic knowledge, we cap the maximum number of subwords to 12 for WN18RR, which consequently allows us to use a larger negative sampling size of 1500. We train the model for five epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results</head><p>As we can see from the results in Table <ref type="table" target="#tab_6">6</ref>, models initialized with Wikidata5M pre-trained checkpoints consistently outperform models initialized with pre-trained BERT weights across the board.</p><p>For both datasets, we observe more pronounced improvements for models initialized from pretrained iHT under the inductive setting, which is likely more challenging than the transductive one. In section 3.4, we have seen the benefit of support sets under the transductive setting of Wikidata5M.</p><p>Here we can see that the support sets are also helpful for the transductive setting of WN18RR but not FB15K-237, which is consistent with the observation in <ref type="bibr" target="#b4">Chen et al. (2021)</ref>. We also note that randomly initialized models are hard to converge for these two datasets, suggesting that a good initialization is critical for smaller datasets given the instability of large Transformer models.</p><p>To investigate the transferability of iHT in lowresource scenarios, we further conduct experiments on the standard transductive setting of FB15K-237 and WN18RR with different training data sizes. From Figure <ref type="figure">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Conventional approaches to KG completion focus on learning a low-dimensional embedding for each entity and relation based purely on the structure of KGs <ref type="bibr" target="#b31">(Wang et al., 2017;</ref><ref type="bibr" target="#b11">Ji et al., 2021)</ref>. The generalizability of such transductive approaches is limited within learning new relations between a pre-defined set of entities that appear during training. Moreover, as they heavily rely on the embeddings of entities to store relational knowledge, they are not scalable to large-scale KGs without excessive memory consumption and engineering efforts <ref type="bibr" target="#b14">(Lerer et al., 2019)</ref>, which further limits the amount of knowledge they can access. To address the above limitations, inductive approaches abandon the notion of entity embeddings and instead represent entities with their textual information <ref type="bibr" target="#b33">(Wang et al., 2014;</ref><ref type="bibr" target="#b35">Xie et al., 2016)</ref> or rely purely on the multi-relational structure of KGs to perform rule induction <ref type="bibr" target="#b27">(Teru et al., 2020)</ref>, thus enabling the generalization to new entities that are not connected to the training graph. Nevertheless, the extended generalizability is still limited to a single KG.</p><p>Inspired by the success of Transformer in transfer learning <ref type="bibr" target="#b9">(Devlin et al., 2019)</ref>, Transformerbased KG completion models are proposed to capture the compositional semantics of entities and utilize the transferable knowledge in pre-trained LMs <ref type="bibr" target="#b37">(Yao et al., 2019;</ref><ref type="bibr" target="#b3">Bosselut et al., 2019;</ref><ref type="bibr" target="#b7">Daza et al., 2021;</ref><ref type="bibr" target="#b6">Clouatre et al., 2021)</ref>. A long line of work considers jointly training a Transformer model with both KG completion and language modeling objectives <ref type="bibr" target="#b32">(Wang et al., 2021;</ref><ref type="bibr" target="#b38">Yasunaga et al., 2022)</ref>. iHT keeps the notion of entity representation for relational scoring as opposed to modified token-based scoring <ref type="bibr" target="#b6">(Clouatre et al., 2021;</ref><ref type="bibr" target="#b21">Saxena et al., 2022)</ref> to facilitate potential application with entity representations. iHT can be initialized with pre-trained LMs like BERT but further training is only performed on the KG completion task.</p><p>Transferring knowledge between different KGs is less explored in literature, largely focused on KGs with shared components that can be aligned <ref type="bibr">(Sun et al., 2020a)</ref>, such as multilingual KGs <ref type="bibr" target="#b5">(Chen et al., 2020)</ref>. Most closely related to the concept of large-scale pre-training on KGs, Kocijan and Lukasiewicz (2021) adopts an RNN to encode textual representations of entities and conventional relational scoring functions such as <ref type="bibr">Balazevic et al. (2019, TuckER)</ref> for KG completion. Their encoder is first pre-trained on a large knowledge base, and then fine-tuned to generate entity representations on target knowledge bases. The pre-training improves model performance on small-scale OKBC datasets but not on standard KG completion datasets such as FB15K-237 and WN18RR. iHT employs a much more expressive Transformer model and offers consistent improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present iHT, a Transformer model for compositional knowledge graph representation learning. When evaluated on the KG completion task, iHT achieves state-of-the-art results on Wikidata5M for both inductive and transductive settings. Even given such a large-scale knowledge graph containing millions of entities and triplets, iHT can be successfully trained with regular memory footprints. We further show that iHT learns transferable representations from Wikidata5M, which is able to boost the model performance on other knowledge graphs from different domains.</p><p>As an inductive approach, iHT relies on the textual information of entities to learn their representations. Despite the wide availability of textual representations for popular KGs such as encyclopedic ones like Wikidata, iHT is not applicable to KGs that do not have textual information for entities and it can suffer from poor quality of textual information as demonstrated in the ablation study replacing entity descriptions with shorter entity names (Table <ref type="table" target="#tab_3">4</ref>). iHT keeps the notion of entity representation to support potential entity-centric applications, but it is consequently slower during inference than the token-based scoring methods <ref type="bibr" target="#b6">(Clouatre et al., 2021;</ref><ref type="bibr" target="#b21">Saxena et al., 2022)</ref> that do not encode entities separately at all. Future work is needed to explore iHT's effectiveness in a more diverse set of KGs and downstream tasks beyond KG completion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of three knowledge graph completion datasets. Numbers of entities in inductive training sets are generally smaller than those under the transductive setting, meaning more unseen entities in their dev and test sets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Transductive</cell><cell></cell><cell></cell><cell>Inductive</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">Wikidata5M FB15K-237 WN18RR Wikidata5M FB15K-237 WN18RR</cell></row><row><cell>Train</cell><cell>relation entity</cell><cell>822 4,594,485</cell><cell>237 14,505</cell><cell>11 40,559</cell><cell>822 4,579,609</cell><cell>237 11,633</cell><cell>11 32,755</cell></row><row><cell></cell><cell>triplet</cell><cell>20,614,279</cell><cell>272,115</cell><cell>86,835</cell><cell>20,496,514</cell><cell>215,082</cell><cell>69,585</cell></row><row><cell></cell><cell>relation</cell><cell>822</cell><cell>223</cell><cell>11</cell><cell>199</cell><cell>225</cell><cell>10</cell></row><row><cell>Dev</cell><cell>entity</cell><cell>4,594,485</cell><cell>9,809</cell><cell>5,173</cell><cell>7,374</cell><cell>8,965</cell><cell>9,736</cell></row><row><cell></cell><cell>triplet</cell><cell>5,163</cell><cell>17,535</cell><cell>3,034</cell><cell>6,699</cell><cell>42,164</cell><cell>11,381</cell></row><row><cell></cell><cell>relation</cell><cell>822</cell><cell>224</cell><cell>11</cell><cell>201</cell><cell>232</cell><cell>10</cell></row><row><cell>Test</cell><cell>entity</cell><cell>4,594,485</cell><cell>10,348</cell><cell>5,323</cell><cell>7,475</cell><cell>10,645</cell><cell>10,223</cell></row><row><cell></cell><cell>triplet</cell><cell>5,133</cell><cell>20,466</cell><cell>3,134</cell><cell>6,894</cell><cell>52,870</cell><cell>12,037</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison between the proposed method and baseline methods on Wikidata5M under the transductive setting. Results of transductive KGE methods (first section) are taken from<ref type="bibr" target="#b32">Wang et al. (2021)</ref>. Numbers in bold represent the best results.</figDesc><table><row><cell>.4T</cell><cell>.281 .228 .310 .373</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of different model variants on dev sets of Wikidata5M. Models trained under the inductive setting do not have a support set of neighbor entities. provided in Section 4 on the transfer experiments. Since the entities in the inductive setting are disconnected from the training graph, we cannot use support sets in the inductive setting.</figDesc><table><row><cell>Model</cell><cell cols="2">Inductive</cell><cell cols="2">Transductive</cell></row><row><cell></cell><cell cols="4">MRR H@10 MRR H@10</cell></row><row><cell>iHT G</cell><cell>-</cell><cell>-</cell><cell>36.4</cell><cell>44.1</cell></row><row><cell>iHT E</cell><cell>61.2</cell><cell>84.4</cell><cell>34.6</cell><cell>42.0</cell></row><row><cell>Early fusion</cell><cell>62.9</cell><cell>85.0</cell><cell>36.9</cell><cell>44.8</cell></row><row><cell>Random init</cell><cell>52.5</cell><cell>78.0</cell><cell>33.3</cell><cell>40.7</cell></row><row><cell>Entity name</cell><cell>25.0</cell><cell>40.1</cell><cell>29.0</cell><cell>36.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>, when using</cell></row><row><cell>entity names, the model performance on link pre-</cell></row><row><cell>diction decreases drastically. Since entity names</cell></row><row><cell>are generally shorter, more memorization and less</cell></row><row><cell>generalization of the model are expected. This also</cell></row><row><cell>coincides with the observation that entity names</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results of the Shallow Entity Transformer and Negative Sample Size experiments on the dev set of Wikidata5M's inductive split. TnBm stands for iHT with a n-layer context Transformer and a m-layer entity Transformer.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Our test results on two regular-scale datasets for transductive and inductive KG completion. The first part of the table presents the results of the state-of-the-art models from previous work, while the second/third parts show the results of our iHT models using different initialization methods. WD stands for Wikidata5M pretraining.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Transductive</cell><cell></cell><cell></cell><cell cols="2">Inductive</cell><cell></cell></row><row><cell></cell><cell cols="2">FB15K-237</cell><cell cols="2">WN18RR</cell><cell cols="2">FB15K-237</cell><cell cols="2">WN18RR</cell></row><row><cell>Model</cell><cell cols="8">MRR H@10 MRR H@10 MRR H@10 MRR H@10</cell></row><row><cell>HittER (Chen et al., 2021)</cell><cell>37.3</cell><cell>55.8</cell><cell>50.3</cell><cell>58.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BLP-TransE (Daza et al., 2021)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>19.5</cell><cell>36.3</cell><cell>28.5</cell><cell>58.0</cell></row><row><cell>iHT E</cell><cell>34.5</cell><cell>53.0</cell><cell>44.8</cell><cell>67.5</cell><cell>23.0</cell><cell>39.1</cell><cell>30.9</cell><cell>51.4</cell></row><row><cell>iHT E -WD</cell><cell>37.6</cell><cell>56.2</cell><cell>52.1</cell><cell>74.0</cell><cell>27.5</cell><cell>44.7</cell><cell>39.5</cell><cell>60.5</cell></row><row><cell>iHT G</cell><cell>34.4</cell><cell>52.8</cell><cell>50.0</cell><cell>71.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>iHT G -WD</cell><cell>37.4</cell><cell>56.3</cell><cell>55.0</cell><cell>74.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>, we can see that the performance of the model scales almost logarithmically with the</figDesc><table><row><cell></cell><cell>0.0 0.2</cell><cell>0.2% 7.8% iHT -WD iHT</cell><cell>15.6% 22.5%</cell><cell>25.7% 30.2%</cell><cell>37.5% 34.8%</cell></row><row><cell>Dev MRR</cell><cell>0.0 0.2 0.4</cell><cell cols="3">0.1% 5.8% Proportion of training data used 1.0% 10.0% 12.4% 0.0% 0.1% 8.8% 30.3%</cell><cell>100% 51.4% 43.8%</cell></row><row><cell cols="6">Figure 2: Transfer performance of iHT E -WD (large-</cell></row><row><cell cols="6">scale pretrained) and iHT E (BERT initialized) trained</cell></row><row><cell cols="6">on FB15K-237 (top) and WN18RR (bottom) with dif-</cell></row><row><cell cols="4">ferent training data sizes.</cell><cell></cell></row><row><cell cols="6">training data size. The pre-training on Wikidata5M</cell></row><row><cell cols="6">(inductive split) provides consistent improvements</cell></row><row><cell cols="6">over the model initialized with only BERT weights.</cell></row><row><cell cols="6">With only 10% of the training data, the Wikidata5M</cell></row><row><cell cols="6">pre-trained model is able to achieve 70-86% of the</cell></row><row><cell cols="6">performance of the BERT-initialized model trained</cell></row><row><cell cols="6">on the full training set, suggesting that the large-</cell></row><row><cell cols="6">scale pre-training helps to reduce the need for more</cell></row><row><cell cols="3">training data.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Position embeddings and type embeddings (one type) are also added as in BERT.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>As noted in<ref type="bibr" target="#b32">Wang et al. (2021)</ref>, the relation descriptions in Wikidata5M are unspecific, which leads to worse performance, thus we use lookup embeddings for relation types. Future work can replace them with compositional representations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p><ref type="bibr" target="#b16">Markowitz et al. (2022)</ref> sample a set of nodes from the test graph as the graph context during evaluation. We do not adopt this practice since it is not standard and cannot compare to results in other works.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Details</head><p>The FB15K-237 dataset <ref type="bibr" target="#b28">(Toutanova and Chen, 2015)</ref> is under a Microsoft Research Data License Agreement which allows uses for noncommercial or research purposes. The WN18RR dataset <ref type="bibr" target="#b8">(Dettmers et al., 2018)</ref> is under an MIT License. The inductive versions 6 of the two datasets <ref type="bibr" target="#b7">(Daza et al., 2021)</ref> are also under an MIT License. Wikidata5M 7 <ref type="bibr" target="#b32">(Wang et al., 2021)</ref> is publicly available but not clearly licensed. Our research use of these datasets are aligned with their intended purposes. Datasets derived from Freebase and Wikidata contain information directly associated with public figures. Such information is largely non-private and moderated by the Wikimedia community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Setting Details</head><p>We implement our proposed iHT model with PyTorch <ref type="bibr" target="#b18">(Paszke et al., 2019)</ref>. We follow <ref type="bibr" target="#b9">Devlin et al. (2019)</ref> to configure each of our 6 https://github.com/dfdazac/blp 7 https://deepgraphlearning.github.io/project/wikidata5m</p><p>Transformer layers and initialize the bottom entity transformer with pretrained BERT weights (bert-base-uncased) from huggingface's transformers library <ref type="bibr" target="#b34">(Wolf et al., 2020)</ref>. KG completion evaluation is performed with the LibKGE <ref type="bibr" target="#b20">(Ruffinelli et al., 2020)</ref>.</p><p>We perform simple hyperparameter tuning on</p><p>? the number of negative samples used: { 100, 400, 700, 1000 }</p><p>? layer combinations of the two Transformer blocks: { T6B6, T5B7, T4B8, T3B9 }</p><p>These results are present in Table <ref type="table">5</ref>. Other hyperparameters such as learning rates, learning rate scheduler, batch size, random seeds are manually determined without tuning.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TuckER: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1522</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5185" to="5194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">COMET: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1470</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4762" to="4779" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HittER: Hierarchical transformers for knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Sanxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.812</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online and Punta Cana, Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10395" to="10407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multilingual knowledge graph completion via ensemble knowledge transfer</title>
		<author>
			<persName><forename type="first">Xuelu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changjun</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.290</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3227" to="3238" />
		</imprint>
	</monogr>
	<note>Ankith Uppunda, Yizhou Sun, and Carlo Zaniolo</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MLMLM: Link prediction with mean likelihood masked language model</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Clouatre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Trempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amal</forename><surname>Zouaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.378</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4321" to="4331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive entity representations from text via link prediction</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Daza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Groth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="798" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Relational knowledge: The foundation of higher cognition</title>
		<author>
			<persName><forename type="first">Graeme</forename><forename type="middle">S</forename><surname>Halford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">H</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="497" to="505" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey on knowledge graphs: Representation, acquisition, and applications</title>
		<author>
			<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="494" to="514" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Advances in neural information processing systems, 31. Vid Kocijan and Thomas Lukasiewicz. 2021. Knowledge base completion meets transfer learning</title>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazemi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.524</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6521" to="6533" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Simple embedding for link prediction in knowledge graphs</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Analysis of the impact of negative sampling on link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Bhushan</forename><surname>Kotnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivi</forename><surname>Nastase</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06816</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pytorch-biggraph: A large scale graph embedding system</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothee</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Peysakhovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="120" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">StATIK: Structure and text for inductive knowledge graph completion</title>
		<author>
			<persName><forename type="first">Elan</forename><surname>Markowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrnoosh</forename><surname>Mirtaheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murali</forename><surname>Annavaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steeg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: NAACL 2022</title>
		<imprint>
			<publisher>Seattle, United States. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="604" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You can teach an old dog new tricks! on training knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence-to-sequence knowledge graph completion and question answering</title>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Kochsiek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.201</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2814" to="2828" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName><forename type="first">Gjergji</forename><surname>Fabian M Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Open domain question answering using early fusion of knowledge bases and text</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1455</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4231" to="4242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A benchmarking study of embedding-based entity alignment for knowledge graphs</title>
		<author>
			<persName><forename type="first">Zequn</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farahnaz</forename><surname>Akrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A reevaluation of knowledge graph completion methods</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.489</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5516" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inductive relation prediction by subgraph reasoning</title>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Komal K Teru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-4007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00360</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="176" to="194" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knowledge graph and text jointly embedding</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1167</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1591" to="1601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Liang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengsheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03193</idno>
		<title level="m">Kgbert: Bert for knowledge graph completion</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep bidirectional language-knowledge graph pretraining</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
