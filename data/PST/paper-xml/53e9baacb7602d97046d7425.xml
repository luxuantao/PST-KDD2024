<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Future Generation Computer Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2011-07-27">27 July 2011</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nitesh</forename><surname>Maheshwari</surname></persName>
							<email>nitesh.maheshwari@research.iiit.ac.in</email>
						</author>
						<author>
							<persName><forename type="first">Radheshyam</forename><surname>Nanduri</surname></persName>
							<email>radheshyam.nanduri@research.iiit.ac.in</email>
						</author>
						<author>
							<persName><forename type="first">Vasudeva</forename><surname>Varma</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Search and Information Extraction Lab, Language Technologies Research Centre (LTRC)</orgName>
								<orgName type="institution">International Institute of Information Technology</orgName>
								<address>
									<settlement>Hyderabad</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IIIT Hyderabad)</orgName>
								<address>
									<postCode>500032</postCode>
									<settlement>Gachibowli, Hyderabad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Search and Information Extraction Lab</orgName>
								<orgName type="institution">International Institute of Information Technology</orgName>
								<address>
									<addrLine>Block B2, &apos;&apos;Vind-hya&apos;&apos;</addrLine>
									<postCode>Hyderabad -500 032</postCode>
									<settlement>Gachibowli</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Future Generation Computer Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2011-07-27">27 July 2011</date>
						</imprint>
					</monogr>
					<idno type="MD5">98F572BA330A005A14D8A4A1BF6262F8</idno>
					<idno type="DOI">10.1016/j.future.2011.07.001</idno>
					<note type="submission">Received 18 January 2011 Received in revised form 15 July 2011 Accepted 16 July 2011</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Energy efficiency MapReduce Cloud computing</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the recent emergence of cloud computing based services on the Internet, MapReduce and distributed file systems like HDFS have emerged as the paradigm of choice for developing large scale data intensive applications. Given the scale at which these applications are deployed, minimizing power consumption of these clusters can significantly cut down operational costs and reduce their carbon footprint-thereby increasing the utility from a provider's point of view. This paper addresses energy conservation for clusters of nodes that run MapReduce jobs. The algorithm dynamically reconfigures the cluster based on the current workload and turns cluster nodes on or off when the average cluster utilization rises above or falls below administrator specified thresholds, respectively. We evaluate our algorithm using the GridSim toolkit and our results show that the proposed algorithm achieves an energy reduction of 33% under average workloads and up to 54% under low workloads.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most enterprises today are focusing their attention on energy efficient computing, motivated by high operational costs for their large scale clusters and warehouses. This power related cost includes investment, operating expenses, cooling costs and environmental impacts. The US Environmental Protection Agency (EPA) datacenter report <ref type="bibr" target="#b0">[1]</ref> mentions that the energy consumed by datacenters has doubled in the period of 2000 and 2006 and estimates another two fold increase from 2007 to 2011 if the servers are not used in an improved operational scenario.</p><p>A substantial percentage of these datacenters run large scale data intensive applications and MapReduce <ref type="bibr" target="#b1">[2]</ref> has emerged as an important paradigm for building such applications. MapReduce framework, by design, incorporates mechanisms to ensure reliability, load balancing, fault tolerance, etc. but such mechanisms can have an impact on energy efficiency as even idle machines remain powered on to ensure data availability. The Server and Energy Efficiency Report <ref type="bibr" target="#b2">[3]</ref> states that more than 15% of the servers are run without being used actively on a daily basis.</p><p>Consider the case of a website for which the traffic is not uniform throughout the year. The website owners can either invest in buying more servers or move to a cloud platform. Moving to cloud is clearly a better alternative for the website owners as they can add resources based on the traffic according to a pay-per-use model. But for cloud computing to be efficient, the individual servers that make up the datacenter cloud will need to be used optimally. None of the major cloud providers release their server utilization data, so it is not possible to know how efficient cloud computing actually is. We have also observed that most applications developed today fail to exploit the low power modes supported by the underlying framework.</p><p>In this paper, we address the issue of power conservation for clusters of nodes that run MapReduce jobs, as there is no separate power controller in MapReduce frameworks such as Hadoop <ref type="bibr" target="#b3">[4]</ref>. Although we focus on one specific framework, our approach can also be applied to other application frameworks where the framework's power controller module can be connected to its foundational software. Our key contribution is an algorithm that dynamically reconfigures the cluster based on the current workload. It scales up (down) the number of nodes in the cluster when the average cluster utilization rises above (falls below) a threshold specified by the cluster administrator. By doing this the nodes in the cluster that are underutilized can be turned off to save power. We remodel the cluster's data placement policy to place the data on the cluster and reconfigure the cluster depending on the workload experienced. The advantage of such a modification to the framework is that the applications developed for these clusters need not be fine-tuned to be energy aware.</p><p>To verify the efficacy of our algorithm, we simulated the Hadoop Distributed File System (HDFS) <ref type="bibr" target="#b4">[5]</ref> using the GridSim toolkit <ref type="bibr" target="#b5">[6]</ref>. We studied the behavior of our algorithm with the default Hadoop implementation as baseline and our results indicate an energy reduction of 33% under average workloads and up to 54% under low workloads.</p><p>A substantial amount of earlier research has dealt with optimizing the energy efficiency of servers at a component level like processors, memories and disks <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>. Most literature in this field focuses on solving the problem of energy efficiency using local techniques like Dynamic Voltage Scaling (DVS), request batching and multi-speed disks <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. In our work, we employ the use of a cluster wide technique where the global state of the cluster is used to dynamically scale the cluster to handle the workload imposed on it.</p><p>The remainder of this paper is organized as follows. We give a brief overview of HDFS rack aware data placement policy and the key idea behind our chosen approach in Section 2. Next, we present our energy efficient algorithm for cluster reconfiguration and data placement in Section 3. Section 4 describes the methodology and simulation models used in evaluating our algorithm and the results of evaluation. We then compare our work with existing work in Section 5. Finally, we conclude by summarizing the key contributions, and touching on future topics in this area in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Need for a power controller in MapReduce</head><p>With the emergence of cloud computing in the past few years, MapReduce <ref type="bibr" target="#b1">[2]</ref> has seen tremendous growth especially for largescale data intensive computing <ref type="bibr" target="#b13">[14]</ref>. A large segment of this MapReduce workload is managed by Hadoop <ref type="bibr" target="#b3">[4]</ref>, Amazon Elastic MapReduce <ref type="bibr" target="#b14">[15]</ref> and Google's in house MapReduce implementation. MapReduce was designed for deployment on clusters running inexpensive commodity hardware. Hence, even idle nodes remain powered on to ensure data availability. However, there is no separate power controller in MapReduce frameworks such as Hadoop as yet.</p><p>Datacenters are known to be expensive to operate and they consume huge amounts of electric power <ref type="bibr" target="#b15">[16]</ref>. Google's server utilization and energy consumption study <ref type="bibr" target="#b16">[17]</ref> reports that the energy efficiency peaks at full utilization and significantly drops as the utilization level decreases (Fig. <ref type="figure">1</ref>). Hence, the power consumption at zero utilization is still considerably high (around 50%). Essentially, even an idle server consumes about half its maximum power. We also observe that the energy efficiency of these servers lies in the range of 20%-60% when operating under 20%-50% utilization. Thus Fig. <ref type="figure">1</ref> indicates that dynamically reconfiguring the cluster by scaling it while running the active nodes at higher utilization is the best decision from a power management point of view.</p><p>The MapReduce framework implemented by Hadoop relies heavily on the performance and reliability of the underlying HDFS. It divides applications into numerous smaller blocks of work and application data into smaller data blocks. It then allows parallel execution for applications in a fault tolerant way by creating redundant copies of both data and computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">HDFS data layout</head><p>HDFS is based on Google File System (GFS) <ref type="bibr" target="#b17">[18]</ref> and has a master-slave architecture with the master called as NameNode and the slaves as DataNodes. The NameNode is responsible for storing the HDFS namespace and records changes to the file system metadata. The DataNodes are spread across multiple racks and store the HDFS data in their local file systems as shown in Fig. <ref type="figure" target="#fig_0">2</ref>.</p><p>Fig. <ref type="figure">1</ref>. Server power usage and energy efficiency at varying utilization levels, from idle to peak performance. Even an energy-efficient server still consumes about half its full power when doing virtually no work. Source: <ref type="bibr" target="#b16">[17]</ref>. The DataNodes are connected via a network and intrarack network bandwidth between machines is usually higher than interrack network bandwidth.</p><p>Each file stored on HDFS is split into smaller chunks of size 64 MB called data blocks and these blocks are distributed across the cluster. Each block is replicated to ensure data availability even in case of connectivity failure to nodes or even complete racks. For the same, Hadoop implements a rack-aware data block replication policy for the data that is stored on HDFS. For the default replication factor of three, HDFS's replica placement policy is to put one replica of the block on one node in the local rack, another on a different node in the same rack, and the third on a node in some other rack.</p><p>Each DataNode periodically sends a heartbeat message to the NameNode in order to inform the NameNode about its current state including the block report of the blocks it stores. The NameNode assumes DataNodes without recent heartbeat messages to be dead and stops forwarding any more requests to them. The NameNode also re-replicates the data lost because of dead nodes, corrupt blocks, hard disk failures, etc. If some DataNodes are turned off without informing the NameNode then the cluster might enter what can be called as a panic phase where the Name-Node tries to replicate the blocks earlier stored on these Data-Nodes. This process might end up generating heavy network traffic <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Replica placement in Hadoop</head><p>In the context of high-volume data processing, the limiting factor is the rate at which data can be transferred between nodes. Hadoop uses the bandwidth between the nodes as a measure of distance between them. Hence, the cost of data transfer between nodes in the same rack is less than that of nodes in different racks, which in turn is less than that of nodes on different datacenters. Replication pipeline for a replication factor of three is shown in Fig. <ref type="figure" target="#fig_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed algorithm</head><p>We attempt to reduce the energy consumption of datacenters that run MapReduce jobs by reconfiguring the cluster. In this section, we explain the approach followed in our proposed algorithm for cluster reconfiguration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Approach</head><p>We implement a channel between the cluster's power controller and HDFS that scales the number of nodes in the cluster dynamically to meet the current service demands (Fig. <ref type="figure" target="#fig_2">4</ref>). We start with a subset of nodes powered on and intelligently accept job requests by checking if they can be served using the current state of the cluster. We add more nodes to the cluster if the resources available are not enough for serving these requests. We power them off again when they are not being used optimally.</p><p>When the NameNode receives a request to create a file F k of size S k with replication factor of R k , it calculates the number of HDFS blocks that will be needed to store it using the following formula: </p><formula xml:id="formula_0">⌈S k /B⌉ × R k (1)</formula><p>where B is the HDFS block size. The block size is configurable per file with a default value of 64 MB. The space needed to store F k on HDFS is:</p><formula xml:id="formula_1">S k × R k .<label>(2)</label></formula><p>The algorithm checks if the there is enough space available on HDFS to store the file and turns on more nodes if needed. At all times, we keep some space reserved on the DataNodes so that write operations do not block or fail waiting for more space to be provisioned while new DataNodes are being powered on. After the nodes are powered on, the cluster is rebalanced as there is no data on the newly activated DataNodes while other DataNodes are already filled substantially. The NameNode gives instructions to the DataNodes to create blocks in a rack-aware manner on HDFS and updates its namespace and metadata.</p><p>When the NameNode receives a request to delete a file, it gives instructions to the DataNodes to delete blocks corresponding to that file. This communication with the DataNodes happens by means of heartbeat messages that are exchanged periodically between the NameNode and the DataNodes. If the cluster becomes underutilized after deleting the data, we move the data from the least utilized nodes to other nodes and turn them off.</p><p>We shall make use of the following notation in the rest of the paper:</p><p>• n = Number of currently active nodes on the cluster  there is a need to scale up the cluster to accommodate the current request. Similarly, when some data is deleted from the cluster, the NameNode consults the Power Controller if some nodes can be deactivated to save power. The Power Controller module responds with cluster reconfiguration information which has details about the nodes to be activated or deactivated. It decides which nodes to activate or deactivate based on scale up threshold U u , scale down threshold U d and the current average utilization of active nodes in the cluster U.</p><formula xml:id="formula_2">• C θ = Disk capacity of DataNode θ • U θ = Current utilization of DataNode θ • R θ = Set of active nodes in DataNode θ's rack • U = Average utilization of active nodes in the cluster • U(R) = Average utilization of active nodes in rack R • U u = Scale up threshold • U d = Scale</formula><p>Once this cluster reconfiguration information is received by the NameNode, it performs scaling and rebalancing operations depending on the average utilization U(R i ) of active nodes in racks. A detailed description of these operations is covered in subsequent sections. The latest version of Hadoop (0.21.0) comes with a pluggable interface to place replicas of data blocks in HDFS and has support for an API that allows a module external to HDFS to specify how data blocks should be placed <ref type="bibr" target="#b19">[20]</ref>. This will ease the integration of placement policy we suggest in Hadoop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cluster reconfiguration</head><p>We consider the following types of cluster reconfigurations in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Scaling up</head><p>Fig. <ref type="figure" target="#fig_4">5</ref> presents our algorithm for scaling up the cluster when the current state of resources are not enough to serve write requests. Scaling up of the cluster is triggered when the average utilization of the active nodes (U) exceeds the administrator specified threshold U u .</p><p>When the trigger condition is reached, we select a set of Data-Nodes (Θ) to be powered on in the function ActivateDataNodes(Θ). We choose the least recently deactivated nodes to avoid hotspots caused by the same set of machines running all the time. It is advantageous to activate nodes from different racks because HDFS follows a rack-aware replica placement policy where replicas of Fig. <ref type="figure">6</ref>. Algorithm for scaling down reconfiguration. Upon reaching the trigger condition, we perform intrarack transfer and interrack transfer from the nodes to be removed. a data block are spread across multiple racks. Also, activating multiple nodes in the same iteration saves time as these nodes can be booted in parallel.</p><p>Addition of new nodes while scaling up will result in more storage space on the cluster. As a result of this, the average utilization level of the cluster will drop and the data distribution across nodes will become skewed. Therefore, we balance the racks where new nodes were recently activated. For each newly activated DataNode (θ dst ) we first perform intrarack transfer of blocks (Fig. <ref type="figure" target="#fig_4">5</ref>, lines 4-10). If the utilization of a node (θ src ) in the rack exceeds U by the transfer threshold (τ ) specified by the administrator, we transfer the excess amount of data from θ src to θ dst . If the intrarack transfer does not sufficiently fill up the storage on θ dst , interrack transfer of blocks to θ dst is performed (Fig. <ref type="figure" target="#fig_4">5</ref>, lines <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. Since interrack transfer is costlier than intrarack transfer, the amount of excess data to be moved in case of interrack transfer is hindered by a value of δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Scaling down</head><p>Fig. <ref type="figure">6</ref> presents our algorithm for scaling down the cluster. Scaling down of the cluster is triggered when the average utilization of the active nodes (U) falls below the administrator specified threshold U d .</p><p>If the trigger condition is reached, it is likely that some nodes in the cluster are underutilized. Time of activation and current utilization of nodes are considered in function CandidateSet() while selecting the set of candidate nodes (Θ) that can potentially be powered off. We perform two checks on each candidate node (θ src ) before deactivating it. Firstly, we do not deactivate a node if some node from its rack has been deactivated recently. Secondly, we keep a percentage of nodes (µ) activated at all times in all the racks so that the cluster can still serve write requests that are received in bursts.</p><p>For each candidate node θ src , we calculate the expected average utilization of its rack (m ′ ) if θ src was to be powered off. We then perform intrarack transfer of blocks to nodes in the same rack (Fig. <ref type="figure">6</ref>, lines <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. If the difference between U θ dst and m ′ is less than the transfer threshold specified (τ ) by the administrator, we transfer the excess amount of data from θ src to θ dst . If the intrarack transfer does not completely vacate the storage on θ src , interrack transfer of blocks from θ src is performed (Fig. <ref type="figure">6</ref>, lines <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. As in the case of scaling up reconfiguration, the amount of excess data to be moved in case of interrack transfer is hindered by a value of δ. When all the data blocks from θ src have been moved, we add it to the set of nodes to be deactivated (Σ ). Finally, after considering all the candidate nodes, we deactivate the nodes in Σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Avoiding jitter effect</head><p>While performing these scaling operations of activating and deactivating the nodes in the cluster, the cluster can experience a jitter effect if the load variability is high. Consider a case when by the time new nodes were being activated, the load dissipates resulting in shutting down some already activate nodes, at which time the load was increasing again. The parameters for our algorithm can be configured in a way that this jitter effect does not occur. The difference in the values of scale up threshold U u and scale down threshold U d should be made sufficiently large so that the cluster does not experience a jitter effect of activating and deactivating nodes due to small changes in average utilization of active nodes in the cluster U.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cluster rebalancing</head><p>The level of network activity depends on the techniques used for rebalancing the cluster; hence they are very crucial for our algorithm to perform well. The cluster needs to be rebalanced after scaling up the number of nodes in the cluster in order to distribute the extra workload from overutilized nodes to the newly added nodes. Similarly, we need to perform rebalancing operations before scaling down the number of nodes in the cluster so that workload from underutilized nodes can be transferred to already active nodes before they can be turned off. We put an upper limit on the bandwidth used for rebalancing operations so that it does not congest the network. This also ensures that the other ongoing file operations remain unaffected while the cluster is being rebalanced.</p><p>We employ a simple heuristic for rebalancing where we try to move the maximum amount of data to nodes in the same rack as network bandwidth is greater for intrarack transfers. While moving data blocks across nodes, we do not violate the rack-aware replica placement policy followed by HDFS. We also do not move data that is currently being used by some job already running on the cluster. This ensures that we do not compromise on the fault tolerance and runtime of already running jobs on the cluster while performing rebalancing operations. A data block is removed from the source DataNode only after all the running tasks using that data block are completed. In order to respect the rack aware block replication policy followed by Hadoop, we make the following checks before moving a data block within the same rack and across racks (Fig. <ref type="figure" target="#fig_5">7</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1</head><p>Data transferred (p) from θ src to θ dst .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scaling up Scaling down a</head><p>Intrarack transfer</p><formula xml:id="formula_3">(U θsrc -U) × C θsrc (m ′ +δ-U θ dst )×C θ dst Interrack transfer (U θsrc -U -δ) × C θsrc (m ′ -U θ dst ) × C θ dst a Where m ′ = U(R θsrc )×|R θsrc | |R θsrc |-1</formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interrack transfer :</head><p>Interrack transfer means transferring a data block from a node to one of the nodes in some other rack. In the case of interrack transfer, a data is moved from a node to another node only if a replica of that block does not exist on any nodes in the destination rack.</p><p>The subroutines IntrarackTransfer(θ src , θ dst , p) and Interrack Transfer(θ src , θ dst , p) in the algorithms of Figs. <ref type="figure" target="#fig_6">5</ref> and<ref type="figure">6</ref> transfer p amount of data from node θ src to node θ dst . Table <ref type="table">1</ref> summarizes the amount of data transferred from θ src to θ dst for intrarack and interrack transfers while scaling up and scaling down operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation and results</head><p>We use the GridSim toolkit to simulate HDFS architecture and perform our experiments. We verify the efficacy of our algorithm and study its behavior with the default HDFS implementation as baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Simulation model</head><p>The NameNode and DataNode classes in our simulations are extended from the GridSim toolkit's DataGridResource class. These DataNodes are distributed across racks and a metadata of these mappings is maintained at the NameNode. We implement the default rack-aware replica placement policy followed by HDFS. We then incorporate the cluster reconfiguration decisions suggested by our algorithm to dynamically scale the number of nodes in the cluster. We demonstrate the scale up and scale down operations of our algorithm and their corresponding energy savings.</p><p>The parameters used in our simulation runs along with their default values are specified in Table <ref type="table" target="#tab_2">2</ref>. These parameters are kept constant at their default values between different runs while comparing the results. All values reported in these results are averaged over ten independent runs of varying workloads, unless otherwise specified. We vary the number, size and replication factor of the files created on the HDFS in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Cluster reconfiguration</head><p>To verify whether our cluster reconfiguration algorithm is able to scale the cluster in accordance to the workload imposed on it, we   perform basic experiments for scale up and scale down operations. These experiments were performed with 50 nodes spread across 5 racks, with each rack having 10 nodes. We plot the number of active nodes and their average utilization for the default HDFS implementation and our algorithm, when presented with the same workload (Fig. <ref type="figure" target="#fig_7">8</ref>(a) and (b)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Scaling up</head><p>Fig. <ref type="figure" target="#fig_7">8</ref>(a) summarizes the results of our scale up experiments.</p><p>We start with a percentage of nodes (λ = 50%) turned off initially and scale up the cluster when the average utilization rises above the scale up threshold (U u = 80%). The boxed area in the plot represents the scaling up operation where more nodes are added to the cluster whenever the cluster utilization (U) rises above the value U u . Adding more nodes to the cluster brings down the average utilization of the nodes in the cluster and it accepts incoming requests until the average utilization reaches U u again, as demonstrated by the peaks in the curve (Fig. <ref type="figure" target="#fig_7">8</ref>(a)). Our algorithm tries to maintain the utilization levels of the nodes at U u affirming that the nodes are being used in the region of higher energy efficiency, as specified in Fig. <ref type="figure">1</ref>. In this case, the average number of inactive nodes during the experiments denote energy savings of about 36%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Scaling down</head><p>Fig. <ref type="figure" target="#fig_7">8</ref>(b) summarizes the results of our scale down experiments. Initially all the nodes in the cluster are being used and we scale down the cluster when the average utilization falls below the scale down threshold (U d = 50%). The boxed area in the plot represents the scaling down operation where nodes are deactivated from the cluster whenever the cluster utilization (U) falls below the value U d . Deactivating nodes from the cluster brings up the average utilization of the nodes and our algorithm scales down the cluster till the percentage of active nodes is more than the value specified by µ, as demonstrated by the curve in Fig. <ref type="figure" target="#fig_7">8(b)</ref>. The energy savings denoted by the experiments in this case is about 27%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Energy savings</head><p>Next, we perform scale up and scale down experiments over a range of workloads to calculate the average energy savings with our algorithm. These experiments were performed with 120 nodes spread across 8 racks, with each rack having 15 nodes. In our model, the amount of energy saved is proportional to the number of deactive nodes. We plot the energy savings for the cases when the cluster is imposed with low and high workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Workload imposed</head><p>The number of files is varied from 200 for low workloads to 400 for high workloads and the minimum file size is varied from 3000 MB for low workloads to 15 000 MB for high workloads. The maximum file size is set to 25 000 MB across all the experiments, whereas the number of replicas of these files is selected randomly from 1 to 3. We observe that the cluster intelligently reconfigures itself based on the workload imposed, proving the effectiveness of our algorithm. As expected, the energy conserved in case of low workloads is significantly higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Scaling up and the effect of λ</head><p>We calculate the average amount of energy saved during the scale up operation by performing experiments over a range of various workloads (Fig. <ref type="figure" target="#fig_8">9</ref>). We analyze the behavior of the plots for various values of λ, which is the percentage of nodes kept  inactive at the start of each experiment. The average energy savings by these experiments is about 36%. make the observations the plots:</p><p>• Increasing the value of λ increases the of energy</p><p>• conserved for low workloads can reach up to 54% for higher values of λ. • Even for high workloads and lower values of λ, the energy conserved is about 20%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Scaling down and the effect of µ</head><p>We calculate the average amount of energy saved during the scale down operation in a similar manner over a range of various workloads (Fig. <ref type="figure" target="#fig_9">10</ref>). We analyze the behavior of the plots for various values of µ, which is the minimum percentage of nodes that must be kept active in each rack at all times. The average energy savings denoted by these experiments is about 30%.</p><p>We then make the following observations from the plots:</p><p>• Increasing the value of µ decreases the percentage of energy conserved.</p><p>• Energy conserved for low workloads can reach up to 48% for lower values of µ. • Even for high workloads and higher values of µ, the energy conserved is about 15%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related work</head><p>Lowering the energy usage of datacenters is a challenging and complex issue because computing applications and data are growing so quickly that increasingly larger servers and disks are needed to process them fast enough within the required time period <ref type="bibr" target="#b20">[21]</ref>. Fan et al. report that the opportunities for power and energy savings significant at the cluster-level and hence the systems need to be energy efficient across their activity range <ref type="bibr" target="#b21">[22]</ref>.</p><p>For MapReduce frameworks like Hadoop Vasić et al. present a design for energy aware MapReduce and HDFS where they leverage the sleeping state of machines to save power <ref type="bibr" target="#b18">[19]</ref>. Their work also proposes a model for collaborative power management where a common control platform acts as a communication channel between the cluster power management and services running on the cluster. Leverich et al. present a modified design for Hadoop that allows scale-down of operational cluster <ref type="bibr" target="#b22">[23]</ref>. They propose the notion of covering subset during block replication that at least one replica of a data block must be stored in the covering subset. This ensures data availability, even when all the nodes not in the covering subset are turned off. Also, the system configuration parameters used for MapReduce clusters have a high impact on their energy efficiency <ref type="bibr" target="#b23">[24]</ref>.</p><p>An important difference between the works mentioned above and our work is that the techniques employed in these works compromise on the replication factor of data blocks stored on the cluster. Our algorithm attempts to keep the cluster utilized to its maximum allowed potential and accordingly scale the number of nodes without compromising on the replication factor for the data blocks.</p><p>Our work is inspired from earlier works by Pinheiro et al. where they present the problem of energy efficiency using cluster reconfiguration at the application level for clusters of nodes, and at the operating system level for standalone servers <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. We present the same problem for a cluster of machines running MapReduce frameworks such as Hadoop. Work by Pérez et al. in <ref type="bibr" target="#b26">[27]</ref> presents a mathematical formalism to achieve dynamic reconfiguration with the use of storage groups for data-based clusters. Work by Duy et al. also presents this problem using machine learning based approach by applying the use of neural network predictors to predict future load demand based on historical demand <ref type="bibr" target="#b27">[28]</ref>.</p><p>The technique of Dynamic Voltage Scaling (DVS) has been employed to provide power-aware scheduling algorithms to minimize the power consumption <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. DVS is a power management technique where undervolting (decreasing the voltage) is done to conserve power and overvolting (increasing the voltage) is done to increase computing performance. Hsu et al. apply a variation of DVS called Dynamic Voltage Frequency Scaling (DVFS) by operating servers at various CPU voltage and frequency levels to reduce overall power consumption <ref type="bibr" target="#b30">[31]</ref>.</p><p>Berl et al. propose virtualization with cloud computing as a way forward to identify the main sources of energy consumption <ref type="bibr" target="#b31">[32]</ref>. Live migration and placement optimizations of virtual machines (VM) have been used in earlier works to provide a mechanism to achieve energy efficiency <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref>. Power aware provisioning and scheduling of VMs has also been used with DVFS techniques to reduce the overall power consumption <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. Although virtualization helps reduce datacenter's power consumption, the ease of provisioning virtualized servers can lead to uncontrolled growth and more unused servers, a phenomenon called virtual server sprawl <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and future work</head><p>In this paper we addressed the problem of energy conservation for large datacenters that run MapReduce jobs. We also discussed that servers are most efficient when used at a higher utilization level. In this context, we proposed an energy efficient data placement and cluster reconfiguration algorithm that dynamically scales the cluster in accordance with the workload imposed on it. However, the efficacy of this approach would depend on how quickly a node can be activated. We evaluated our algorithm by performing simulations and the results show energy savings of as high as 54% under low workloads and about 33% under average workloads.</p><p>Future directions of our research include applying machine learning approaches to study the resource usage patterns of jobs and model their power usage characteristics to efficiently schedule them on the cluster. Upon submission of a job, make use of the knowledge about the dataset to predict the data layout for its data blocks and optimally schedule the tasks making use of data locality. We would also like to study the performance of our algorithm when presented with different types of workloads such as processor intensive, memory intensive, disk intensive and network intensive. We would also like to study the impact of node's boot time and ACPI <ref type="bibr" target="#b37">[38]</ref> power states available in the hardware on the efficiency of our algorithm.</p><p>We plan to find optimal values of MapReduce configuration parameters so that our algorithm can provide even better energy savings. We also plan to make use of multiple heuristics like access time and minimum replication factor of a file, while rebalancing the cluster to avoid overhead on the network. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Architecture of Hadoop Distributed File System (HDFS). The communication between the master node (NameNode) and the worker nodes (DataNodes) is shown by means of heartbeat messages. The files to be stored on HDFS are split in 64 MB chunks and distributed across racks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A typical replica pipeline in Hadoop for a replication factor of 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.4. Proposed approach: Interaction of power controller module with the NameNode. The NameNode performs rebalancing and scaling operations once it receives the reconfiguration information from the power controller.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>down threshold • τ = Transfer threshold (in percentage) • δ = Dampening factor for interrack transfer (in percentage) • λ = Percentage of initially inactive nodes per rack • µ = Minimum percentage of active nodes per rack. All the U values mentioned above are in percentage. As demonstrated in Fig. 4, when the NameNode receives a request to create a file, it checks with the Power Controller if</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Algorithm for scaling up reconfiguration. Upon reaching the trigger condition, we perform intrarack transfer and interrack transfer to all the newly added nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Examples of valid and invalid movement of data blocks while performing intrarack and interrack transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( a )</head><label>a</label><figDesc>Basic scale up operation, with U u = 80% and λ = 50%. (b) Basic scale down operation, with U d = 50% and µ = 50%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Basic scale up and scale down operations for a cluster of 50 nodes spread across 5 racks, with each rack having 10 nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The effect of varying λ on energy savings while scaling up.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The effect of varying µ on energy savings while scaling down.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Radheshyam</head><label></label><figDesc>Nanduri received B.Tech. degree in Information Technology from Vellore Institute of Technology, Vellore, India, 2009. He is currently an M.S. by Research student in the department of Computer Science and Engineering at International Institute of Information Technology, Hyderabad, India. Since December 2009, he has been working as a research student at Search and Information Extraction Lab at IIIT Hyderabad. His primary research interests are in cloud computing, provisioning and scheduling algorithms for heterogeneous distributed systems. Vasudeva Varma is a faculty member at International Institute of Information Technology, Hyderabad since 2002. He is heading Search and Information Extraction Lab and Software Engineering Research Lab at IIIT Hyderabad. His research interests include search (information retrieval), information extraction, information access, knowledge management, cloud computing and software engineering. He published a book on Software Architecture (Pearson Education) and over seventy technical papers in journals and conferences. In 2004, he obtained young scientist award and grant from Department of Science and Technology, Government of India, for his proposal on personalized search engines. In 2007, he was given Research Faculty Award by AOL Labs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>explains this approach by means of examples):</figDesc><table><row><cell>Intrarack transfer :</cell></row></table><note><p>Intrarack transfer means transferring a data block from a node to some other node in the same rack. While performing intrarack transfer, we move a data block from a node to another node only if a replica of that block does not exist on the destination node. Nodes in the same rack are connected by a higher bandwidth link and hence intrarack transfers are cheaper as compared to interrack transfers.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Simulation parameters.</figDesc><table><row><cell>Parameter</cell><cell>Description</cell></row><row><cell>Number of racks</cell><cell>UniformRandom(5, 10) [Default = 8]</cell></row><row><cell>Number of nodes per rack</cell><cell>UniformRandom(10, 20) [Default = 15]</cell></row><row><cell>Storage capacity of nodes (GB)</cell><cell>UniformRandom(100, 500)</cell></row><row><cell>HDFS default block size, B</cell><cell>64 MB</cell></row><row><cell>Scale up threshold, U u</cell><cell>80%</cell></row><row><cell>Scale down threshold, U d</cell><cell>50%</cell></row><row><cell>Transfer threshold, τ</cell><cell>5%</cell></row><row><cell>Dampening factor for interrack transfer, δ</cell><cell>5%</cell></row><row><cell>Percentage of initially inactive nodes per rack, λ</cell><cell>30%-70% [Default = 50]</cell></row><row><cell>Minimum percentage of active nodes per rack, µ</cell><cell>30%-70% [Default = 50]</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partly supported by a grant from Yahoo! India R&amp;D, under the Nurture an area: Cloud Computing project. We would like to thank Chidambaran Kollengode of Yahoo! and Reddyraja Annareddy of Pramati Technologies for their continual guidance throughout this work. We would also like to thank Jaideep Dhok, Sudip Datta and Manisha Verma for their constant feedback on our work.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nitesh Maheshwari received his B.Tech. degree in Information and Communication Technology from DA-IICT, Gandhinagar, India, in 2006. He is currently pursuing M.S. by Research in Computer Science and Engineering from International Institute of Information Technology, Hyderabad, India. He works as part of the Search and Information Extraction Lab at IIIT Hyderabad. His research interests are in the area of cloud computing with emphasis on data placement algorithms and scheduling policies for energy efficient computing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">EPA Report to Congress on Server and Data Center Energy Efficiency, Tech. Rep</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>US Environmental Protection Agency</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mapreduce: simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<idno type="DOI">10.1145/1327452.1327492</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Server energy and efficiency report</title>
		<imprint>
			<date type="published" when="2009">1E, 2009</date>
		</imprint>
	</monogr>
	<note>Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apache</forename><surname>Hadoop</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="http://hadoop.apache.org/hdfs/" />
		<title level="m">Hadoop distributed file system</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gridsim: a toolkit for the modeling and simulation of distributed resource management and scheduling for grid computing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Murshed</surname></persName>
		</author>
		<idno type="DOI">10.1002/cpe.710</idno>
	</analytic>
	<monogr>
		<title level="j">Concurr. Comput. Pract. Exp</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1175" to="1220" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scheduling for reduced CPU energy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI&apos;94: Proceedings of the 1st USENIX Conference on Operating Systems Design and Implementation, USENIX Association</title>
		<meeting><address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compiler-directed frequency and voltage scaling for a multiple clock domain microarchitecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rangasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Srikant</surname></persName>
		</author>
		<idno type="DOI">10.1145/1366230.1366267</idno>
	</analytic>
	<monogr>
		<title level="m">CF&apos;08: Proceedings of the 5th Conference on Computing Frontiers</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Power aware page allocation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Lebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ellis</surname></persName>
		</author>
		<idno type="DOI">10.1145/378993.379007</idno>
	</analytic>
	<monogr>
		<title level="m">ASPLOS-IX: Proceedings of the Ninth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="105" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive disk spin-down for mobile computers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Helmbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Sconyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sherrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mobile Netw. Appl</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="285" to="297" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Energy conservation policies for web servers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elnozahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kistler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajamony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USITS&apos;03: Proceedings of the 4th Conference on USENIX Symposium on Internet Technologies and Systems, USENIX Association</title>
		<meeting><address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Conserving disk energy in network servers</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Carrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianchini</surname></persName>
		</author>
		<idno type="DOI">10.1145/782814.782829</idno>
	</analytic>
	<monogr>
		<title level="m">ICS&apos;03: Proceedings of the 17th Annual International Conference on Supercomputing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="86" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DRPM: dynamic speed control for power management in server class disks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gurumurthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sivasubramaniam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Franke</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCA.2003.1206998</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Architecture, International Symposium on</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page">169</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning based opportunistic admission control algorithm for MapReduce as a service</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dhok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Varma</surname></persName>
		</author>
		<idno type="DOI">10.1145/1730874.1730903</idno>
	</analytic>
	<monogr>
		<title level="m">ISEC&apos;10: Proceedings of the 3rd India Software Engineering Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<ptr target="http://aws.amazon.com/elasticmapreduce/" />
		<title level="m">Amazon elastic MapReduce</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cloud computing and emerging it platforms: vision, hype, and reality for delivering computing as the 5th utility</title>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Broberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Brandic</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.future.2008.12.001</idno>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="599" to="616" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The case for energy-proportional computing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Holzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="33" to="37" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The google file system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Leung</surname></persName>
		</author>
		<idno type="DOI">10.1145/1165389.945450</idno>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="29" to="43" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Making cluster applications energy-aware</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vasić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barisits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Salzgeber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kostic</surname></persName>
		</author>
		<idno type="DOI">10.1145/1555271.1555281</idno>
	</analytic>
	<monogr>
		<title level="m">ACDC&apos;09: Proceedings of the 1st Workshop on Automated Control for Datacenters and Clouds</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<idno>HDFS-385</idno>
		<ptr target="https://issues.apache.org/jira/browse/HDFS-385" />
		<title level="m">Design a pluggable interface to place replicas of blocks in HDFS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Energy-efficient management of data center resources for cloud computing: a vision, architectural elements, and open challenges</title>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beloglazov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Abawajy</surname></persName>
		</author>
		<idno>CoRR abs/1006.0308</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Power provisioning for a warehousesized computer</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-D</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<idno type="DOI">10.1145/1250662.1250665</idno>
	</analytic>
	<monogr>
		<title level="m">ISCA&apos;07: Proceedings of the 34th Annual International Symposium on Computer Architecture</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the energy (in)efficiency of hadoop clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leverich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<idno type="DOI">10.1145/1740390.1740405</idno>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="65" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy efficiency of MapReduce</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Load balancing and unbalancing for power and performance in cluster-based systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Carrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Dynamic cluster reconfiguration for power and performance, in: Power and Energy Management for Server Systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Carrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Design and Performance of Networks for Super-, Cluster-, and Grid-Computing Part I</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Robles</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jpdc.2005.04.018</idno>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1134" to="1145" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>A new formalism for dynamic reconfiguration of data servers in a cluster</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Performance evaluation of a green scheduling algorithm for energy savings in cloud computing</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">V T</forename><surname>Duy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Inoguchi</surname></persName>
		</author>
		<idno type="DOI">10.1109/IPDPSW.2010.5470908</idno>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing, Workshops and Ph.D. Forum, IPDPSW, 2010 IEEE International Symposium on</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Power aware scheduling of bag-of-tasks applications with deadline constraints on DVS-enabled clusters</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/CCGRID.2007.85</idno>
	</analytic>
	<monogr>
		<title level="m">Seventh IEEE International Symposium on</title>
		<imprint>
			<date type="published" when="2007">2007. 2007. 2007</date>
			<biblScope unit="page" from="541" to="548" />
		</imprint>
	</monogr>
	<note>Cluster Computing and the Grid</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Minimizing energy consumption for precedenceconstrained applications using dynamic voltage scaling</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
		<idno type="DOI">10.1109/CCGRID.2009.16</idno>
	</analytic>
	<monogr>
		<title level="m">CCGRID&apos;09: Proceedings of the 2009 9th IEEE/ACM International Symposium on Cluster Computing and the Grid</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="92" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1109/SC.2005.3</idno>
		<title level="m">A power-aware run-time system for high-performance computing, in: SC&apos;05: Proceedings of the 2005 ACM/IEEE Conference on Supercomputing</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Energy-efficient cloud computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gelenbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Digirolamo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giuliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Demeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pentikousis</surname></persName>
		</author>
		<idno type="DOI">10.1093/comjnl/bxp080</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. J</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1045" to="1051" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Greencloud: a new architecture for green data center</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/1555312.1555319</idno>
	</analytic>
	<monogr>
		<title level="m">ICAC-INDST&apos;09: Proceedings of the 6th International Conference Industry Session on Autonomic Computing and Communications Industry Session</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Magnet: a novel scheduling policy for power reduction in cluster with virtual machines</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CLUSTR.2008.4663751</idno>
	</analytic>
	<monogr>
		<title level="m">Cluster Computing</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Power-aware management in cloud data centers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Castro-Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blakley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cloud Computing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Jaatun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Rong</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5931</biblScope>
			<biblScope unit="page" from="668" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Power-aware provisioning of cloud resources for real-time services</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beloglazov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Buyya</surname></persName>
		</author>
		<idno type="DOI">10.1145/1657120.1657121</idno>
	</analytic>
	<monogr>
		<title level="m">MGC&apos;09: Proceedings of the 7th International Workshop on Middleware for Grids, Clouds and e-Science</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Power-aware scheduling of virtual machines in DVFS-enabled clusters</title>
		<author>
			<persName><forename type="first">G</forename><surname>Laszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Younge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/CLUSTR.2009.5289182</idno>
	</analytic>
	<monogr>
		<title level="m">Cluster Computing and Workshops</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<ptr target="http://www.acpi.info/" />
		<title level="m">Advanced Configuration and Power Interface (ACPI)</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
