<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-01">1 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sravanti</forename><surname>Addepalli</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arya</forename><surname>Baburaj</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gaurang</forename><surname>Sriramanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computational and Data Sciences</orgName>
								<orgName type="laboratory">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-01">1 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.00306v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As humans, we inherently perceive images based on their predominant features, and ignore noise embedded within lower bit planes. On the contrary, Deep Neural Networks are known to confidently misclassify images corrupted with meticulously crafted perturbations that are nearly imperceptible to the human eye. In this work, we attempt to address this problem by training networks to form coarse impressions based on the information in higher bit planes, and use the lower bit planes only to refine their prediction. We demonstrate that, by imposing consistency on the representations learned across differently quantized images, the adversarial robustness of networks improves significantly when compared to a normally trained model. Present stateof-the-art defenses against adversarial attacks require the networks to be explicitly trained using adversarial samples that are computationally expensive to generate. While such methods that use adversarial training continue to achieve the best results, this work paves the way towards achieving robustness without having to explicitly train on adversarial samples. The proposed approach is therefore faster, and also closer to the natural learning process in humans.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Neural Networks have been used to achieve remarkable performance in many Computer Vision tasks, such as Classification <ref type="bibr" target="#b14">[15]</ref>, Segmentation <ref type="bibr" target="#b19">[20]</ref> and Object recognition <ref type="bibr" target="#b24">[25]</ref>. While these networks achieve near-human accuracy on many benchmark datasets, they are far from being as robust as the human visual system. Deep Networks are known to be vulnerable to carefully crafted imperceptible noise known as Adversarial Perturbations <ref type="bibr" target="#b31">[32]</ref>, which could have disastrous implications in critical applications such as autonomous navigation and surveillance systems. The compelling need of securing these systems, coupled with the goal of improving the worst-case robustness of * Equal contribution Deep Networks has propelled research in the area of Adversarial Robustness over the last few years. While adversarial training methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">42]</ref> have led to significant progress in improving adversarial robustness, these methods are computationally expensive and also non-intuitive when compared to the learning process in humans.</p><p>Humans perceive images based on features of large magnitude and use finer details only to enhance their impressions <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30]</ref>. This background knowledge of giving higher importance to information present in higher bit planes naturally equips the human visual system to develop resistance towards adversarial perturbations, which are of relatively lower magnitude. On the contrary, these adversarial perturbations can arbitrarily flip the predictions of Deep Networks to completely unrelated classes, suggesting that such background knowledge of giving hierarchical importance to different bit planes is missing in these networks. In this work, we propose to equip Deep Networks with such knowledge, and demonstrate that this improves their robustness to adversarial examples.</p><p>We propose a novel Bit Plane Feature Consistency (BPFC) regularizer, which can significantly improve adversarial robustness of models, without exposure to adversarial samples during training. The proposed method is considerably faster than methods that require multi-step adversarial samples for training <ref type="bibr" target="#b21">[22]</ref>, and is therefore scalable to large datasets such as ImageNet. Through this work, we hope to pave the path towards training robust Deep Networks without using adversarial samples, similar to the learning process that exists in human beings.</p><p>The organization of this paper is as follows: The subsequent section presents a discussion on the existing literature related to our work. Section-3 lays out the preliminaries related to notation and threat model. This is followed by details and analysis of our proposed approach in Section-4. We present the experiments performed and an analysis on the results in Section-5, followed by our concluding remarks in Section-6.</p><p>The code and pretrained models are available at: https://github.com/val-iisc/BPFC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works 2.1. Adversarial Training methods</head><p>The most popular methods of improving adversarial robustness of Deep Networks involve Adversarial Training (AT), where clean data samples are augmented with adversarial samples during training. Early formulations such as FGSM-AT <ref type="bibr" target="#b9">[10]</ref> proposed training on adversarial samples generated using single-step optimization, that assumes a first-order linear approximation to the loss function. This was later shown to be ineffective against multi-step attacks by Kurakin et al. <ref type="bibr" target="#b16">[17]</ref>, wherein the effect of gradient masking was identified. Gradient masking, first identified by Papernot et al. <ref type="bibr" target="#b23">[24]</ref>, is the phenomenon where the trained network yields masked gradients, thereby resulting in the generation of weak adversaries, leading to a false sense of robustness. In a wide variety of settings, numerous countermeasures have been developed that can produce strong adversaries to circumvent gradient masking <ref type="bibr" target="#b32">[33]</ref>.</p><p>Madry et al. <ref type="bibr" target="#b21">[22]</ref> proposed Projected Gradient Descent (PGD) based training, that employs an iterative procedure for finding strong adversaries that maximise training loss under the given norm constraint. Crucially, PGD trained models are robust against various gradient-based iterative attacks, as well as several variants of non-gradient based attacks. However, the process of PGD Adversarial Training (PGD-AT) is computationally expensive.</p><p>In order to address this, Vivek et al. <ref type="bibr" target="#b36">[37]</ref> revisited singlestep adversarial training, and introduced a regularizer that helps mitigate the effect of gradient masking. This regularizer penalizes the 2 distance between logits of images perturbed with FGSM and R-FGSM <ref type="bibr" target="#b32">[33]</ref> attacks. In the proposed method, we achieve adversarial robustness without using adversarial samples during training, and hence achieve a further reduction in computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Attempts of Adversary-Free Training</head><p>In this section, we discuss existing training methods that do not utilize adversarial samples during training. Works such as Mixup <ref type="bibr" target="#b40">[41]</ref> and Manifold-Mixup <ref type="bibr" target="#b35">[36]</ref> propose training methods to learn better feature representations. In Mixup, the network is trained to map a random convex combination of the input data to the corresponding convex combination of their one-hot encodings. This work is extended further in Manifold Mixup, where the network is trained to map a convex combination of intermediate hidden-layers generated by two different data points to the corresponding convex combination of their one-hot encodings. Hence, these methods encourage the network to behave in a linearized manner between input data points, or between hidden-layers deeper in the network. While these methods resulted in improved performance against singlestep FGSM attacks, they were susceptible to stronger multi-step attacks.</p><p>Another attempt of adversary-free training to achieve robustness utilized input transformations for defense. In the work by Guo et al. <ref type="bibr" target="#b10">[11]</ref>, the effect of various input transformations such as bit-depth reduction, JPEG compression, total variation minimisation and image quilting was studied. The robustness from these techniques primarily originated from the non-differentiable pre-processing steps, in order to possibly thwart gradient-based iterative attacks. This method, along with a few others <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b28">29]</ref>, were broken in the work by Athalye et al. <ref type="bibr" target="#b0">[1]</ref>, where it was identified that obfuscated gradients do not provide reliable security against adversaries.</p><p>Yet another avenue pursued was towards the detection of adversarial samples. Feature Squeezing, proposed by Xu et al. <ref type="bibr" target="#b39">[40]</ref>, used transformations such as reduction of color bit depth, spatial smoothing with a median filter and a combination of both, in order to generate a feature-squeezed image from a given input image. By thresholding the 1 distance between logits of an input image and its featuresqueezed counterpart, the image was classified to be either adversarial or legitimate in nature. However, in the work by He et al. <ref type="bibr" target="#b12">[13]</ref>, it was shown that an adaptive attacker cognizant of this defense strategy could fool the model by constructing attacks that retain adversarial properties even after feature-squeezing is applied, thereby evading detection.</p><p>While we use the concept of quantization to defend against adversarial attacks in this work, we do not introduce any pre-processing blocks that lead to obfuscated or shattered gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation</head><p>In this paper, we consider f (.) as the function mapping of a classifier C, from an image x, to its corresponding softmax output f (x). The predicted class label, which is an argmax over the softmax output, is denoted by c(x). The ground truth label corresponding to x is denoted by y. The image is said to be correctly classified when c(x) = y. The pre-softmax output of the classifier C is denoted by g(x). We define A(x) to be the set of all Adversarial Samples corresponding to x. A specific adversarial sample corresponding to a clean sample x is denoted by x .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Threat Model</head><p>In this paper, we consider the task of improving the worst-case robustness of Deep Networks. The goal of an adversary is to cause an error in the prediction of the classifier. We define an Adversarial Sample x , as one that causes the output of the network to be different from the ground truth label y. We do not restrict the adversary to flip labels from a specific source class, or to a specific target class. We restrict x to be in the ∞ -ball of radius ε around x. The set of Adversarial Samples can be formally defined as follows:</p><formula xml:id="formula_0">A(x) = {x : c(x ) = y, x − x ∞ ≤ ε}<label>(1)</label></formula><p>We therefore impose a constraint that any individual pixel in the image x cannot be perturbed by more than ε.</p><p>Since the goal of this work is to improve worst-case robustness, we do not impose any restrictions on the access to the adversary. We consider that the adversary has complete knowledge of the model architecture, weights and the defense mechanism employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head><p>In this section, we first present the motivation behind our proposed method, followed by a detailed discussion of the proposed algorithm. We further describe local properties of networks trained using the proposed regularizer, which lead to improved robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hierarchical Importance of Bit Planes</head><p>Bit planes of an image are the spatial maps (of the same dimension as the image) corresponding to a given bit position. For an n-bit representation of an image, bit plane n−1 corresponds to the most significant bit (MSB), and bit plane 0 corresponds to the least significant bit (LSB). An n-bit image can be considered as the sum of n bit planes weighted by their relative importance. The importance of features embedded within lower bit planes is significantly lower than that of features embedded within higher bit planes, both in terms of pixel value, and information content <ref type="bibr" target="#b26">[27]</ref>. Steganography methods <ref type="bibr" target="#b8">[9]</ref> utilize lower bit planes to embed crucial copyright information that needs to be visually imperceptible. However, information content in natural images decreases from the most significant bit (MSB) to the least significant bit (LSB). A weighted sum of the five least significant bit planes of the image in Fig. <ref type="figure" target="#fig_0">1</ref>(a) is shown in Fig. <ref type="figure" target="#fig_3">1(e)</ref>, from which it is evident that lower bit planes contribute only to fine details. Fig. <ref type="figure" target="#fig_3">1(b), (c</ref>) and (d) show images ranging from fine to coarse structure, with different levels of quantization. The difference between Fig. <ref type="figure" target="#fig_0">1</ref>(a) and Fig. <ref type="figure" target="#fig_3">1(b</ref>) is Fig. <ref type="figure" target="#fig_3">1(e)</ref>. While the addition of Fig. <ref type="figure" target="#fig_0">1</ref>(e) certainly improves the information content, it is not as crucial as the higher bit planes for interpreting the image. The human visual system is known to give higher importance to global information when compared to fine details <ref type="bibr" target="#b29">[30]</ref>. Sugase et al. <ref type="bibr" target="#b30">[31]</ref> demonstrate that global information is used for coarse classification in early parts of the neural response, while information related to fine details is perceived around 51ms later. This demonstrates a hierarchical classification mechanism, where the response to an image containing both coarse and fine information is aligned with that containing only coarse information.</p><p>We take motivation from this aspect of the human visual system, and enforce Deep Networks to maintain consistency across decisions based on features in high bit planes alone (quantized image) and all bit planes (normal image). Such a constraint will ensure that Deep Networks give more importance to high bit planes when compared to lower bit planes, similar to the human visual system. Adversarial examples constrained to the ∞ -ball utilize low bit planes to transmit information which is inconsistent with that of higher bit planes. The fact that Deep Networks are susceptible to such adversarial noise demonstrates the weakness of these networks, which emanates from the lack of consistency between predictions corresponding to coarse information and fine details. Therefore, enforcing feature consistency across bit planes results in a significant improvement in adversarial robustness when compared to conventionally trained networks.</p><p>While we use the base-2 (binary) representation of an image to illustrate the concept of ignoring low magnitude additive noise, the same can be formulated in terms of any other representation (in any other base) as well. Secondly, low magnitude noise does not always reside in low bit planes. It can overflow to MSBs as well, based on the pixel values in the image. We introduce pre-quantization noise in our proposed approach to mitigate these effects. This is illustrated in the following section, where we explain our proposed method in greater detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Proposed Training Algorithm</head><p>We present the proposed training method in Algorithm-1. Motivated by the need to learn consistent representations for coarse and fine features of an image, we introduce a regularizer that imposes feature consistency between each image and its quantized counterpart.  The probability P of a pixel i being assigned to the quantized values q(i) = 48, 80 and 112 is shown here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Quantization</head><p>The steps followed for generating a coarse image are described in this section. The input image x i is assumed to be represented using n-bit quantization. The intensity of pixels is hence assumed to be in the range [0, 2 n ). We generate an n−k +1 bit image using the quantization process described here. The allowed range of k is between 1 and n − 1.</p><p>• Pre-quantization noise: Initially, uniform noise sampled independently from U(−2 k−<ref type="foot" target="#foot_0">2</ref> , 2 k−2 ) is added to each pixel in the image x i , to generate x pre .</p><p>• Quantization step: Next, each pixel is quantized to n − k bits, by setting the last k bits to 0.</p><p>• Range Shift: The intensity of all pixels is shifted up by 2 k−1 . This shifts the range of quantization error (w.r.t.</p><formula xml:id="formula_1">x pre ) from [0, 2 k ) to [−2 k−1 , 2 k−1 ).</formula><p>• Clip: Finally, the quantized image is clipped to the original range, [0, 2 n ).</p><p>Fig. <ref type="figure" target="#fig_2">2</ref> illustrates the probability of a pixel i being assigned to different quantization levels using the above method, when n = 8 and k = 5. As shown in the figure, addition of pre-quantization noise introduces randomness in the quantized value of each pixel. The probability of being assigned the nearest quantization level is 1 when the input pixel intensity is close to the quantized value, and decays linearly to 0 after a fixed distance. This helps mitigate unwanted non-linear effects at the edges of each quantization bin, due to specific pixel intensity values of the original image.</p><p>We do an ablation experiment (in Section-5) called Simple Quantization, where the pre-quantization noise is not added. Simple Quantization can be also viewed as addition of correlated low magnitude (quantization) noise, where the noise pattern depends on local pixel intensity levels. This noise is the difference between the original image, and the image subject to simple quantization. Since the pixel intensities can be assumed to be locally correlated in space, the noise is also correlated locally. The correlated nature of noise differentiates quantization noise from random noise, and also brings it closer to the properties of adversarial perturbations. We also consider an ablation experiment of replacing the quantization step with addition of random noise sampled from a uniform distribution in Section-5.</p><p>While pre-quantization noise disturbs the local correlation properties of the quantization noise for some of the pixels, it is crucial to mitigate the bin edge effects discussed above. We demonstrate through experiments that the proposed solution is better than both the ablation experiments discussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Bit Plane Feature Consistency Regularizer</head><p>The loss function used for training is shown below:</p><formula xml:id="formula_2">L = 1 M M i=1 ce(f (x i ), y i ) + λ g(x i ) − g(q(x i ))<label>2 2</label></formula><p>(2) For a given image x i , the first term of Eq. ( <ref type="formula" target="#formula_2">2</ref>) is the cross-entropy (ce) loss obtained from the softmax output of the network f (x i ), and the corresponding ground truth label y i . The second term is the squared 2 distance between the pre-softmax activation of the image x i , and that of the corresponding quantized image q(x i ) (generated using the process described in Section-4.2.1). We call this squared</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Local Properties of BPFC Trained Networks</head><p>In this section, we examine local properties of the function g(.) learned using the proposed BPFC regularizer.</p><p>Let x i denote an n-bit image sampled from the data distribution P D with pixel intensities in the range [0, 2 n ), and let q(x i ) denote a quantized image corresponding to x i . We assume that q(x i ) is not identically equal to x i . For a fixed value of λ, let Θ g(λ) denote the set of parameters corresponding to a family of functions that lead to the crossentropy term in Eq. ( <ref type="formula" target="#formula_2">2</ref>) being below a certain threshold. Minimization of BPFC loss among the family of functions parameterized by Θ g(λ) is shown in Eq. ( <ref type="formula">3</ref>):</p><formula xml:id="formula_3">min θg∈Θ g(λ) E xi∼P D E q(xi) g(x i ) − g(q(x i )) 2 2</formula><p>(3)</p><formula xml:id="formula_4">min θg∈Θ g(λ) E xi∼P D E q(xi) g(x i ) − g(q(x i )) 2 2 x i − q(x i ) 2 2<label>(4)</label></formula><p>The expression in Eq. ( <ref type="formula">3</ref>) can be lower bounded by the expression in Eq. ( <ref type="formula" target="#formula_4">4</ref>), which is equivalent to minimizing the local Lipschitz constant of the network at each sample x i . The denominator of the objective function in Eq. ( <ref type="formula" target="#formula_4">4</ref>) is the 2 -norm between each image and its quantized counterpart, and is thus independent of θ g . Therefore, minimization of BPFC loss in Eq. ( <ref type="formula">3</ref>) can be viewed as minimization of the local Lipschitz constant at each sample x i , weighted by an 2 -norm of its deviation from the quantized image. An expectation of this 2 -norm term over all q(x i ) (with differently sampled pre-quantization noise) converges to a constant value for all samples, thereby establishing an equivalence between the minimization of BPFC loss and the minimization of local Lipschitz constant of the network.</p><p>Hence, imposing BPFC regularizer encourages the network to be locally Lipschitz continuous with a reduced Lipschitz constant. While the BPFC regularizer imposes local smoothness, the cross-entropy term in Eq. (2) requires g(.) to be a complex mapping for better accuracy on clean images. The final selection of θ g would depend on λ, which is typically selected based on the amount by which clean accuracy can be traded-off for adversarial accuracy <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b33">34]</ref>. During the initial epochs of training, the function learned is relatively smooth. Hence, we start with a low value of λ and step it up during training.</p><p>Therefore, the BPFC formulation leads to functions with improved local properties, which is closely related to adversarial robustness as explained by Szegedy et al. <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Analysis</head><p>In this section, we discuss the experiments done to verify the robustness of our proposed approach. We first describe the datasets used and details on the training methodology in Section-5.1, followed by an overview of the experiments conducted in Section-5.2. We further present details on each experiment and our analysis on the results in Sections-5.3 to 5.6. We follow the guidelines laid out by Athalye et al. <ref type="bibr" target="#b0">[1]</ref> and Carlini et al. <ref type="bibr" target="#b2">[3]</ref> to ascertain the validity of our claim on the achieved robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Preliminaries</head><p>We use the benchmark datasets, CIFAR-10 [14], Fashion-MNIST (F-MNIST) <ref type="bibr" target="#b37">[38]</ref> and MNIST <ref type="bibr" target="#b17">[18]</ref> for validating our proposed approach. CIFAR-10 is a ten-class dataset with RGB images of dimension 32 × 32. The number of images in the training set and test set are 50, 000 and 10, 000 respectively. These images are equally distributed across all classes. We set aside 10, 000 images from the training set as the validation set. Fashion-MNIST and MNIST are ten-class datasets with gray-scale images of dimension 28 × 28. The datasets are composed of 60, 000 training samples and 10, 000 test samples each. We further split each of the training datasets into 50, 000 training samples and 10, 000 validation samples.</p><p>We use ResNet-18 <ref type="bibr" target="#b11">[12]</ref> architecture for CIFAR-10, and a modified LeNet (M-LeNet) <ref type="bibr" target="#b18">[19]</ref> architecture with two additional convolutional layers (details in Table-S1 of the Supplementary) for MNIST and Fashion-MNIST. We train CIFAR-10 models for 100 epochs, MNIST and Fashion-MNIST models for 50 epochs each. The minibatch size is set to 128 for CIFAR-10 and 64 for Fashion-MNIST and MNIST. We use SGD optimizer with momentum of 0.9 and weight decay of 5e-4. We use an initial learning rate of 0.1 for CIFAR-10, and 0.01 for MNIST and Fashion-MNIST. We reduce the initial learning rate by a factor of 5, three times during the training process. We use early stopping based on I-FGSM <ref type="bibr" target="#b15">[16]</ref> accuracy on the validation split in the last 20 epochs for CIFAR-10, and last 30 epochs for MNIST and Fashion-MNIST.</p><p>The hyperparameters to be selected for training are: k, which is the number of bits to be eliminated during the quantization step in Section-4.2.1, and λ, which is the weighting factor for BPFC loss in Eq. ( <ref type="formula" target="#formula_2">2</ref>). We set k to 5 for CIFAR-10, 6 for Fashion-MNIST and 7 for MNIST. The value of k can be selected based on the ε value of the attack to be defended. λ is selected to achieve the desired tradeoff between accuracy on clean and adversarial samples (details in Section-S1.2 of the Supplementary). As explained in Section-4.3, we start with a low value of λ and step it up over epochs. This helps achieve better accuracy on clean samples. For CIFAR-10, we start with λ of 1 and step it up by a factor of 9 every 25 epochs. Since the clean accuracy on Fashion-MNIST and MNIST datasets increases within very few epochs, we use high λ values from the beginning (without a step-up factor). We thus use a λ value of 30 for MNIST and 25 for Fashion-MNIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Overview of Experiments</head><p>We compare the proposed approach with Normal Training (NT), FGSM-AT <ref type="bibr" target="#b9">[10]</ref>, PGD-AT <ref type="bibr" target="#b21">[22]</ref> and Regularized Single-Step Adversarial Training (RSS-AT) <ref type="bibr" target="#b36">[37]</ref> across all three datasets. We report results on single-step (FGSM) and multi-step (I-FGSM, PGD) attacks, epsilon-bounded and unbounded (DeepFool <ref type="bibr" target="#b22">[23]</ref>, Carlini-Wagner (C&amp;W) <ref type="bibr" target="#b3">[4]</ref>) attacks, untargeted and targeted attacks, and gradient-free attacks (random attacks, SPSA <ref type="bibr" target="#b34">[35]</ref>). We consider attacks in white-box and black-box settings. We also consider adaptive attacks that are specific to the defense mechanism used.</p><p>As explained in Section-3.2, we restrict the adversary to be in the ∞ -ball of radius ε around each data point. We refer to the work by Madry et al. <ref type="bibr" target="#b21">[22]</ref> for attack parameters and number of iterations for PGD attack. For an image with pixel intensities in the range [0, 1], we consider an ε value of 8/255 for CIFAR-10, 0.3 for MNIST and 0.1 for Fashion-MNIST. We consider ε step to be 2/255, 0.01 and 0.01 for CIFAR-10, MNIST and Fashion-MNIST respectively. These restrictions do not apply to the unbounded attacks, DeepFool and C&amp;W.</p><p>We present our experiments, results and analysis for each attack in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Performance against White-box Attacks</head><p>As explained in Section-3.2, we consider that the adversary has access to the network architecture and weights. In this scenario, white-box attacks are expected to be stronger than black-box attacks (unless the model merely appears to be robust due to gradient masking). In this section, we consider the following types of white-box attacks: untargeted and targeted ε-bounded attacks, and unbounded attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Bounded Attacks: Untargeted</head><p>The results of various single-step and multi-step whitebox attacks for CIFAR-10 dataset are presented in Table-1. FGSM-AT achieves the best robustness to single-step attacks. However, it is not robust to multi-step attacks as explained by Kurakin et al. <ref type="bibr" target="#b16">[17]</ref>. PGD-AT and RSS-AT show the best accuracy of around 45% for 1000-step PGD attack.  As explained in Section-4.2.1, we consider ablation experiments of Simple Quantization (A1) and addition of Uniform Noise (A2). The proposed method (BPFC) achieves an improvement over these two baselines, indicating the significance of the proposed formulation. Adding uniform random noise in the range (−8/255, 8/255) produces an effect similar to that of quantization by reducing the importance given to LSBs for the classification task. Hence, we see comparable results even for this ablation experiment.</p><p>We also consider an ablation experiment of using 1norm instead of 2 -norm in Eq. <ref type="bibr" target="#b1">(2)</ref>. While the results using 1 -norm (Table-1) show an improvement over the proposed method, the 500-step worst-case PGD accuracy goes down from 37.5% to 24.8% with 100 random restarts (over 1000 test samples represented equally across all classes), indicating that it achieves robustness due to gradient masking. For the proposed approach, the PGD accuracy with 50 steps (34.68%) is similar to that with 1000 steps (34.44%). Hence, we check 50-step PGD accuracy with 1000 random restarts (for 1000 test samples) and find that the drop in accuracy over multiple random restarts is negligible. The accuracy drops from 35.6% to 34.9% over 1000 random restarts, verifying that the robustness in the proposed approach is not due to gradient masking.</p><p>Table-3 shows the consolidated white-box results for all datasets. The proposed method has significantly better robustness to multi-step attacks when compared to methods that do not use Adversarial training (NT and Mixup). We also achieve results comparable to PGD-AT and RSS-AT, while being significantly faster. Detailed results with Fashion-MNIST and MNIST datasets are reported in Tables-S2 and S3 of the Supplementary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Bounded attacks: Targeted</head><p>We evaluate the robustness of BPFC trained models against targeted attacks of two types, as discussed in this section.</p><p>In the first attack (Least Likely target), we set the target class to be the least likely predicted class of a given image.</p><p>In the second variant (Random target), we assign random targets to each image. We use 1000-step PGD attacks for both these evaluations, and compare the robustness against an untargeted PGD attack in Table-4. As expected, model trained using the proposed approach is more robust to targeted attacks, when compared to an untargeted attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Unbounded Attacks</head><p>We evaluate robustness of BPFC trained models to the unbounded attacks, DeepFool and Carlini-Wagner (C&amp;W).</p><p>The goal here is to find the lowest 2 -norm bound on perturbations that can result in 100% fooling rate for all sam- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Performance against Black-box Attacks</head><p>We report accuracy against FGSM black-box attacks in Table-6. We consider two source models for generation of black-box attacks on each dataset; the first is a model with a different architecture, and the second is a model with the same architecture as that of the target model. In both cases, and across all datasets, black-box accuracies are significantly better with the proposed approach when compared to other non-adversarial training methods (NT and Mixup). Further, our results are comparable with those of adversarial training methods. Results on multi-step black-box attacks are presented in Section-S2.2 of the Supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Performance against Gradient-free Attacks</head><p>We check the robustness of our proposed approach against the following gradient-free attacks on the CIFAR-10 dataset, to ensure that there is no gradient masking: attack with random noise <ref type="bibr" target="#b2">[3]</ref> and SPSA attack <ref type="bibr" target="#b34">[35]</ref>.</p><p>For the attack with random noise, we consider a random sample of 1000 images from the test set of CIFAR-10, such that all ten classes are equally represented. We randomly select 10 5 samples from an ∞ -ball of radius ε around each data point (each pixel is an i.i.d. sample from a Uniform distribution) and compute the accuracy of these samples. We find that the accuracy on these random samples is 79.76%, which is slightly less than the accuracy on clean samples (82.4%). We run another experiment to verify that every image that is robust to PGD attack is also robust to random noise. We run PGD attack for 50 steps and 100 random restarts and identify the images that are robust to the attack. We attack these images with 10 5 random noiseperturbations each, and find that we achieve the expected accuracy of 100%. Hence, we conclude that the attack with random noise is not stronger than a gradient-based attack.</p><p>The SPSA attack <ref type="bibr" target="#b34">[35]</ref> is a gradient-free attack that computes a numerical approximation of the gradient along multiple random directions and approximates the final gradient to be an average over these gradients. The attack becomes stronger as more directions are used. We use the following hyperparameters to generate the attack: δ = 0.01, learning rate = 0.01, batch size = 128 and iterations = 5. We get an accuracy of 70.5% against the SPSA attack using the proposed approach. For the same attack, the accuracy of a PGD trained model is 70.8%.</p><p>Therefore, we verify that gradient-based attacks are stronger than gradient-free attacks, thereby confirming the absence of gradient masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Performance against Adaptive Attacks</head><p>In this section, we consider methods that utilize knowledge of the defense mechanism in creating stronger attacks. We explore maximizing loss functions that are different from the standard cross-entropy loss for generating adversarial samples. We consider the CIFAR-10 dataset for this experiment. Maximizing the same loss that is used for training, with the same hyperparameters, gives a slightly lower accuracy (34.52%) when compared to PGD (34.68%) for a 50-step attack. However, this difference is not statistically significant, and it may be due to the random nature of the PGD attack. The worst accuracy across different hyperparameters in the loss function is 34.41%.</p><p>We also explore adding another term to the loss that is maximized during a PGD attack. In addition to maximizing the training loss, we minimize the magnitude of k (= 5) LSBs in the generated samples. This would encourage the adversaries to have low magnitude LSBs, which could possibly be the samples where the defense was less effective. However, even with this change, we get the same accuracy as that of a standard PGD attack.</p><p>Therefore, the adaptive attacks are only as strong as a PGD attack. We include more details on adaptive attacks in Section-S2.3 of the Supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Basic Sanity Checks to Verify Robustness</head><p>In this section, we present results on the basic sanity checks listed by Athalye et al. <ref type="bibr" target="#b0">[1]</ref> to ensure that the model's robustness is not due to gradient masking.</p><p>• Results in Table-1 illustrate that iterative attacks (PGD and I-FGSM) are stronger than an FGSM attack.</p><p>• White-box attacks are stronger than black-box attacks based on results in Tables-3 and 6.</p><p>• We note that unbounded attacks reach 100% success rate, and increasing distortion bound increases the success rate of the attack (Fig. <ref type="figure" target="#fig_2">S2</ref> of the Supplementary).</p><p>• As discussed in Section-5.5, gradient-based attacks are stronger than gradient-free attacks.</p><p>• We note that cross-entropy loss on FGSM samples increases monotonically with an increase in perturbation size. (Fig. <ref type="figure">S3</ref> of the Supplementary).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Scalability of the Proposed Method to ImageNet</head><p>We present results on ImageNet <ref type="bibr" target="#b4">[5]</ref>, which is a 1000class dataset with 1.2 million images in the training set and 50, 000 images in the validation set. The accuracy on a targeted PGD 20-step attack is 32.91% with the proposed approach and 43.43% with a PGD-AT model <ref type="bibr" target="#b7">[8]</ref>. The trend in robustness when compared to PGD-AT is similar to that of CIFAR-10 (Table-1), thereby demonstrating the scalability of the proposed approach to large scale datasets. We present detailed results in Section-S2.5 of the Supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have proposed a novel Bit Plane Feature Consistency (BPFC) regularizer, which improves the adversarial robustness of models using a normal training regime. Results obtained using the proposed regularizer are significantly better than existing non-adversarial training methods, and are also comparable to adversarial training methods. Since the proposed method does not utilize adversarial samples, it is faster than adversarial training methods. We demonstrate through extensive experiments that the robustness achieved is indeed not due to gradient masking. Motivated by human vision, the proposed regularizer leads to improved local properties, which results in better adversarial robustness. We hope this work would lead to further improvements on the front of non-adversarial training methods to achieve adversarial robustness in Deep Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material S1. Details on Architecture and Training</head><p>In this section, we present details related to the architecture of models used and the impact of change in hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.1. Architecture details for Fashion-MNIST and MNIST</head><p>We use a modified LeNet architecture for all our experiments on Fashion-MNIST and MNIST datasets. This architecture has two additional convolutional layers when compared to the standard LeNet architecture <ref type="bibr" target="#b18">[19]</ref>. Architecture details are presented in Table-S1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1.2. Impact of change in Hyperparameters</head><p>In this section, we study the effect of variation in the hyperparameter λ (Eq. ( <ref type="formula" target="#formula_2">2</ref>) in main paper). For CIFAR-10 dataset, we set the initial value of λ to be 1, and multiply this by a constant factor every 25 epochs (3 times over 100 epochs). We present the results obtained by changing the rate of increase in λ for CIFAR-10 dataset in Fig- <ref type="figure" target="#fig_3">S1</ref>. As the rate increases, accuracy on clean samples reduces, and accuracy on adversarial samples increases. The clean accuracy saturates to about 70%, and accuracy on adversarial samples saturates to approximately 40%. The best trade-off between both is obtained at a rate of 15, where the clean accuracy is 75.28% and adversarial accuracy is 40.6%. However, for a fair comparison with PGD training and other existing methods, we select the rate at which clean accuracy matches with that of PGD-AT. Hence, the selected hyperparameter is 9.</p><p>We use a similar methodology for hyperparameter selection in MNIST and Fashion-MNIST datasets as well. For these datasets, we set a fixed value of λ and do not increase it over epochs. The value of λ is selected such that the accuracy on clean samples matches with that of a PGD trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2. Details on Experimental Results</head><p>In this section, we present additional experimental results to augment our observations and results presented in the main paper.  <ref type="formula">9</ref>) is highlighted using a cross mark.</p><p>proposed method achieves significantly better robustness to multi-step adversarial attacks when compared to Normal training (NT), FGSM-AT and Mixup. The robustness to multi-step attacks using the proposed approach is comparable to that of PGD-AT and RSS-AT models, while being faster than both approaches. We run the PGD attack with multiple random restarts on a random sample of 1000 test set images, equally distributed across all classes. This experiment is done to ensure that the achieved robustness is not due to gradient masking. The results with random restarts are presented in Table-S4.  Here, the overall accuracy is computed as an average over the worst-case per-sample accuracy, as suggested by Carlini et al. <ref type="bibr" target="#b2">[3]</ref>. A 50-step PGD attack is performed on CIFAR-10 dataset, and a 100-step attack is performed on Fashion-MNIST and MNIST datasets. The degradation from 100 random restarts to 1000 random restarts is insignificant across all datasets, indicating the absence of gradient masking. Degradation from a single run to 100 random restarts is also insignificant for CIFAR-10 and Fashion-MNIST. However, the degradation is larger for MNIST, similar to the trend observed with PGD-AT and RSS-AT models. It is to be noted that the results corresponding to this experiment may not coincide with those reported in Table-1 in the main paper, and Tables-S2 and S3 in the Supplementary, as we consider only a sample of the test set for this experiment. The results with unbounded attacks (DeepFool <ref type="bibr" target="#b22">[23]</ref> and Carlini-Wagner (C&amp;W) <ref type="bibr" target="#b3">[4]</ref>) for Fashion-MNIST and MNIST datasets are presented in Tables-S5 and S6 respectively. We select the following hyperparameters for C&amp;W attack on Fashion-MNIST and MNIST datasets: search steps = 9, max iterations = 500, learning rate = 0.01. For DeepFool attack, we set the number of steps to 100 for both Fashion-MNIST and MNIST datasets. The average 2 -norm of the generated perturbations to achieve approximately 100% fooling rate using C&amp;W attack is higher with the proposed approach when compared to most other approaches, with the exception of PGD-AT, whose average 2 -norm is marginally higher. DeepFool attack does not achieve 100% fooling rate for Fashion-MNIST and MNIST datasets, as was the case with CIFAR-10 (ref: Section-5.3.3 of main paper). However, since the fooling rates of the proposed approach are comparable to, or greater than that of PGD-AT and RSS-AT, we can make a fair comparison between the required 2 -norm for achieving the given fooling rate across these approaches. We observe that the proposed approach is more robust to DeepFool at- tack, when compared to both of these approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.2. Black-box attacks</head><p>Multi-step attacks such as I-FGSM are known to show weak transferability across models in a black-box setting <ref type="bibr" target="#b16">[17]</ref>. Dong et al. <ref type="bibr" target="#b6">[7]</ref> introduced a momentum term in the optimization process of I-FGSM, so as to increase the transferability of the generated adversarial samples. This attack is referred to as the Momentum Iterative FGSM (MI-FGSM) attack.</p><p>The results corresponding to black-box multi-step PGD and MI-FGSM attacks are presented in Tables-S7 and S8 respectively. We consider two source models for black-box attacks on each of the models trained: one with the same architecture as the target model, and second with a different architecture. For Fashion-MNIST and MNIST, the architecture of the second model (Net-A) is presented in Table-S1. For CIFAR-10, we consider a second model with VGG-19 <ref type="bibr" target="#b27">[28]</ref> architecture. The proposed approach achieves a significant improvement in robustness to adversarial samples with respect to Normal Training (NT) and Mixup, and comparable results with respect to the adversarial training methods, across all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.3. Adaptive attacks</head><p>In this section, we explain the adaptive attacks used in this paper in greater detail. We utilize information related to the proposed regularizer to construct potentially stronger attacks when compared to a standard PGD attack. We maximize the following loss function to generate an adaptive attack corresponding to each data sample x i :</p><formula xml:id="formula_5">L i = λ ce ce(f (x i ), y i ) + λ g g(x i ) − g(q(x i )) 2 2 − λ LSB x i − q(x i ) 2 2 (S1)</formula><p>The quantized image corresponding to x i is denoted by q(x i ). We consider f (.) as the function mapping of the trained network, from an image x i , to its corresponding softmax output f (x i ). The corresponding pre-softmax output of the network is denoted by g(x i ). The ground truth label corresponding to x i is denoted by y i . The first term in the above equation is the cross-entropy loss, the second term is the BPFC regularizer proposed in this paper, and the third term is an 2 penalty term on the magnitude of k LSBs. We consider the value of k to be the same as that used for training the models (ref: Section-5.1 in the main paper). The coefficients of these loss terms are denoted by λ ce , λ g and λ LSB respectively.</p><p>Maximizing the cross-entropy term leads to finding samples that are misclassified by the network. Maximizing the BPFC loss results in finding samples which do not comply with the BPFC regularizer imposed during training. Minimizing the third term would help find samples with low magnitude LSBs, which are possibly the points where the defense is less effective. The objective of an adversary is to cause misclassification, which can be achieved by maximizing only the first term in Eq. (S1). However, the proposed defense mechanism could lead to masking of the true solution, thereby resulting in a weak attack. Thus, the role of the remaining terms, which take into account the defense mechanism, is to aid the optimization process in finding such points, if any. The remainder of the algorithm used is similar to that proposed by Madry et al. <ref type="bibr" target="#b21">[22]</ref>.</p><p>The results with adaptive attacks for CIFAR-10, Fashion-MNIST and MNIST datasets are presented in Tables-S9, S10 and S11 respectively. We consider the following coefficients in Eq. (S1) to find a strong adaptive attack:</p><p>• λ ce = 1, λ g = 0, λ LSB = 0</p><p>This corresponds to a standard PGD attack <ref type="bibr" target="#b21">[22]</ref>, which serves as a baseline in this table. The goal of the remaining experiments is to find an attack stronger than this.</p><p>• λ ce = 1, λ g = variable, λ LSB = 0 This case corresponds to using the training loss directly to find adversarial samples. We find that lower Table <ref type="table">S9</ref>: CIFAR-10: Recognition accuracy (%) of the model trained using the proposed approach on adversarial samples generated using adaptive attacks. values of λ g lead to stronger attacks, while still not being significantly stronger than baseline. This indicates that addition of the BPFC regularizer does not help in the generation of a stronger attack.</p><p>• λ ce = 0, λ g = variable, λ LSB = 0 For CIFAR-10 dataset, this case is able to generate attacks which are as strong as PGD, without using the cross-entropy term. This indicates that the BPFC loss term is relevant in the context of generating adversarial samples. However, addition of this to the cross-entropy term does not generate a stronger attack, as the defense is not masking gradients that prevents generation of stronger adversaries. However, for Fashion-MNIST and MNIST datasets, this attack is weaker than PGD.</p><p>• λ ce = 1, λ g = variable, λ LSB = variable Next, we consider the case of introducing the third term that imposes a penalty on high magnitude LSBs. Addition of this term with or without the BPFC term does not help generate a stronger attack, indicating that this training regime does not create isolated points in the ∞ -ball around each sample, which correspond to points with low magnitude LSBs. This can be attributed to the addition of pre-quantization noise.</p><p>Overall, the adaptive attacks constructed based on the knowledge of the defense mechanism do not lead to stronger attacks. This leads to the conclusion that the proposed defense does not merely make the process of finding adversaries harder, but results in learning models that are truly robust.</p><p>Table <ref type="table" target="#tab_0">S10</ref>: Fashion-MNIST: Recognition accuracy (%) of the model trained using the proposed approach on adversarial samples generated using adaptive attacks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.4. Basic Sanity Checks to verify Robustness</head><p>In this section, we present details related to Section-5.7 in the main paper. The plots of accuracy verses perturbation size in Fig. <ref type="figure" target="#fig_2">S2</ref> demonstrate that unbounded attacks are able to reach 100% success rate. It can be observed that increasing the distortion bound increases the success rate of the attack. Fig- <ref type="figure">S3</ref> shows a plot of the average loss on FGSM samples generated on the test set, versus perturbation size of</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Original 8-bit image (b) Weighted sum of (higher) bit planes 7, 6 and 5 (c) Weighted sum of (higher) bit planes 7 and 6 (d) Bit plane 7 -Most significant bit plane (e) Weighted sum of (lower) bit planes 4, 3, 2, 1 and 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Bit Plane Feature Consistency Input: Network f with parameters θ, fixed weight λ, training data D = {(x i , y i )} of n-bit images, quantization parameter k, learning rate η, minibatch size M for minibatch B ⊂ D do Set L = 0 for i = 1 to M do x pre = x i + U(−2 k−2 , 2 k−2 ) // Add noise xq = xpre − xpre mod 2 k // Quantization xq = xq + 2 k−1 // Range Shift xq = min(max(xq, 0), 2 n − 1) // Clip L = L + ce(f (xi), yi) + λ g(xi) − g(xq) 2 2 end for θ = θ − 1 M • η • ∇ θ L /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Quantization of a given 8-bit pixel to 3-bits (n = 8, k = 5): The probability P of a pixel i being assigned to the quantized values q(i) = 48, 80 and 112 is shown here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>S2. 1 .</head><label>1</label><figDesc>Figure S1: Plot of recognition accuracy (%) on clean samples and PGD samples versus the rate of increase in hyperparameter(λ) used for BPFC training. The selected setting (9) is highlighted using a cross mark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-7.png" coords="15,50.11,243.51,494.99,139.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>CIFAR-10: Recognition accuracy (%) of models in a white-box attack setting.</figDesc><table><row><cell cols="3">Training method Clean FGSM</cell><cell cols="3">IFGSM PGD (n-steps) 7 steps 7 20 1000</cell></row><row><cell>FGSM-AT</cell><cell>92.9</cell><cell>96.9</cell><cell>0.8</cell><cell>0.4</cell><cell>0.0 0.0</cell></row><row><cell>RSS-AT</cell><cell>82.3</cell><cell>55.0</cell><cell>50.9</cell><cell cols="2">50.0 46.2 45.8</cell></row><row><cell>PGD-AT</cell><cell>82.7</cell><cell>54.6</cell><cell>51.2</cell><cell cols="2">50.4 47.4 47.0</cell></row><row><cell>NT</cell><cell>92.3</cell><cell>16.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0 0.0</cell></row><row><cell>Mixup</cell><cell>90.3</cell><cell>27.4</cell><cell>1.6</cell><cell>0.6</cell><cell>0.1 0.0</cell></row><row><cell>BPFC (Ours)</cell><cell>82.4</cell><cell>50.1</cell><cell>44.1</cell><cell cols="2">41.7 35.7 34.4</cell></row><row><cell></cell><cell cols="5">Ablations of the proposed approach (BPFC)</cell></row><row><cell cols="2">A1: Simple quant 82.6</cell><cell>49.2</cell><cell>41.4</cell><cell cols="2">38.8 31.6 30.1</cell></row><row><cell cols="2">A2: Uniform noise 82.6</cell><cell>48.7</cell><cell>42.3</cell><cell cols="2">40.0 33.3 31.9</cell></row><row><cell>A3: 1 norm 1</cell><cell>92.1</cell><cell>68.3</cell><cell>60.8</cell><cell cols="2">57.1 46.8 35.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Computational complexity measured in terms of absolute training time per epoch (seconds) and ratio w.r.t. the proposed method (BPFC). This experiment is run on a single Nvidia Titan-X GPU card.Mixup<ref type="bibr" target="#b40">[41]</ref> does not use Adversarial Training and achieves an improvement over Normal Training (NT) in robustness towards FGSM attack. However, it is not robust to PGD attacks. The proposed method achieves a significant improvement over Normal Training and Mixup in robustness to both single-step and multi-step attacks, despite not being exposed to adversarial samples during training. As shown in Table-2, the proposed method is faster than methods that are robust to multi-step attacks (PGD-AT and RSS-AT).</figDesc><table><row><cell>Training</cell><cell cols="2">CIFAR-10</cell><cell cols="2">F-MNIST</cell><cell cols="2">MNIST</cell></row><row><cell>method</cell><cell cols="6">seconds ratio seconds ratio seconds ratio</cell></row><row><cell>RSS-AT</cell><cell>127.2</cell><cell>1.8</cell><cell>23.8</cell><cell>2.0</cell><cell>24.1</cell><cell>1.7</cell></row><row><cell>PGD-AT</cell><cell>257.8</cell><cell>3.7</cell><cell>199.6</cell><cell>16.9</cell><cell>199.2</cell><cell>14.2</cell></row><row><cell>NT</cell><cell>39.6</cell><cell>0.6</cell><cell>9.3</cell><cell>0.8</cell><cell>8.9</cell><cell>0.6</cell></row><row><cell>BPFC (Ours)</cell><cell>69.4</cell><cell>1.0</cell><cell>11.8</cell><cell>1.0</cell><cell>14.0</cell><cell>1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>White-box setting: Recognition accuracy (%) of different models on clean samples and adversarial samples generated using PGD-1000 step attack.</figDesc><table><row><cell>Training</cell><cell cols="2">CIFAR-10</cell><cell cols="2">F-MNIST</cell><cell cols="2">MNIST</cell></row><row><cell>method</cell><cell cols="6">Clean PGD Clean PGD Clean PGD</cell></row><row><cell>FGSM-AT</cell><cell>92.9</cell><cell>0.0</cell><cell>93.1</cell><cell>15.1</cell><cell>99.4</cell><cell>3.7</cell></row><row><cell>RSS-AT</cell><cell>82.3</cell><cell>45.8</cell><cell>87.7</cell><cell>71.8</cell><cell>99.0</cell><cell>90.4</cell></row><row><cell>PGD-AT</cell><cell>82.7</cell><cell>47.0</cell><cell>87.5</cell><cell>79.1</cell><cell>99.3</cell><cell>94.1</cell></row><row><cell>NT</cell><cell>92.3</cell><cell>0.0</cell><cell>92.0</cell><cell>0.3</cell><cell>99.2</cell><cell>0.0</cell></row><row><cell>Mixup</cell><cell>90.3</cell><cell>0.0</cell><cell>91.0</cell><cell>0.0</cell><cell>99.4</cell><cell>0.0</cell></row><row><cell>BPFC (Ours)</cell><cell>82.4</cell><cell>34.4</cell><cell>87.2</cell><cell>67.7</cell><cell>99.1</cell><cell>85.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Recognition accuracy (%) of the proposed method (BPFC) on different 1000-step PGD attacks.</figDesc><table><row><cell>Attack</cell><cell cols="3">CIFAR-10 F-MNIST MNIST</cell></row><row><cell>Untargeted</cell><cell>34.4</cell><cell>67.7</cell><cell>85.7</cell></row><row><cell>Targeted (Least Likely target)</cell><cell>65.2</cell><cell>85.5</cell><cell>95.6</cell></row><row><cell>Targeted (Random target)</cell><cell>63.1</cell><cell>83.5</cell><cell>94.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>DeepFool</figDesc><table><row><cell>Training</cell><cell cols="2">DeepFool</cell><cell cols="2">C&amp;W</cell></row><row><cell>method</cell><cell>FR (%)</cell><cell>Mean 2</cell><cell>FR (%)</cell><cell>Mean 2</cell></row><row><cell>FGSM-AT</cell><cell>95.12</cell><cell>0.306</cell><cell>100</cell><cell>0.078</cell></row><row><cell>PGD-AT</cell><cell>90.78</cell><cell>1.098</cell><cell>100</cell><cell>0.697</cell></row><row><cell>RSS-AT</cell><cell>89.75</cell><cell>1.362</cell><cell>100</cell><cell>0.745</cell></row><row><cell>NT</cell><cell>94.66</cell><cell>0.176</cell><cell>100</cell><cell>0.108</cell></row><row><cell>Mixup</cell><cell>93.37</cell><cell>0.168</cell><cell>100</cell><cell>0.104</cell></row><row><cell>BPFC (Ours)</cell><cell>89.51</cell><cell>2.755</cell><cell>100</cell><cell>0.804</cell></row></table><note>and C&amp;W attacks (CIFAR-10): Average 2 norm of the generated adversarial perturbations is reported. Higher 2 norm implies better robustness. Fooling rate (FR) represents percentage of test set samples that are misclassified.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Black-box setting: Recognition accuracy (%) of different models on FGSM black-box adversaries. Columns represent the source model used for generating the attack. For DeepFool attack, we set the number of steps as 100. With these settings, we achieve 100% fooling rate for C&amp;W attack with all the training methods. DeepFool does not achieve 100% fooling rate for any of the methods. However, these results are consistent with the results reported in literature<ref type="bibr" target="#b25">[26]</ref>. The performance of these models is measured in terms of average</figDesc><table><row><cell>Training</cell><cell cols="2">CIFAR-10</cell><cell cols="2">Fashion-MNIST</cell><cell>MNIST</cell></row><row><cell>method</cell><cell cols="5">VGG19 ResNet18 Net-A M-LeNet Net-A M-LeNet</cell></row><row><cell>FGSM-AT</cell><cell>78.67</cell><cell>77.58</cell><cell>94.36</cell><cell>90.76</cell><cell>87.99 85.68</cell></row><row><cell>RSS-AT</cell><cell>79.80</cell><cell>79.99</cell><cell>84.99</cell><cell>84.16</cell><cell>95.28 95.19</cell></row><row><cell>PGD-AT</cell><cell>80.24</cell><cell>80.53</cell><cell>84.99</cell><cell>85.68</cell><cell>95.75 95.36</cell></row><row><cell>NT</cell><cell>36.11</cell><cell>15.97</cell><cell>34.71</cell><cell>16.67</cell><cell>29.94 16.60</cell></row><row><cell>Mixup</cell><cell>42.67</cell><cell>43.41</cell><cell>54.65</cell><cell>66.31</cell><cell>58.47 69.46</cell></row><row><cell cols="2">BPFC (Ours) 78.92</cell><cell>78.98</cell><cell>81.38</cell><cell>83.46</cell><cell>94.17 94.56</cell></row><row><cell cols="6">ples. We select the following hyperparameters for C&amp;W</cell></row><row><cell cols="6">attack: search steps = 9, max iterations = 200, learning rate</cell></row><row><cell>= 0.01.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>2 -norm of the generated perturbations. A higher value on the bound implies that the model has better robustness. The results on CIFAR-10 are presented in Table-5. It can be observed that the BPFC trained model is more robust to C&amp;W attack when compared to all other methods, including PGD-AT. The DeepFool results of the proposed method can be compared directly only with PGD-AT and RSS-AT, as they achieve similar fooling rates. The proposed method achieves significantly improved robustness when compared to both these methods. Results on Fashion-MNIST and MNIST datasets are presented in Section-S2.1.2 of the Supplementary.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S2 :</head><label>S2</label><figDesc>Fashion-MNIST: Recognition accuracy (%) of models in a white-box attack setting.</figDesc><table><row><cell>Training method</cell><cell cols="2">Clean FGSM</cell><cell>IFGSM 40 steps</cell><cell cols="3">PGD (n-steps) 40 100 1000</cell></row><row><cell>FGSM-AT</cell><cell>93.0</cell><cell>89.9</cell><cell>25.3</cell><cell>15.5</cell><cell cols="2">15.1 15.0</cell></row><row><cell>RSS-AT</cell><cell>87.7</cell><cell>81.2</cell><cell>77.5</cell><cell>72.0</cell><cell cols="2">71.8 71.8</cell></row><row><cell>PGD-AT</cell><cell>87.4</cell><cell>81.4</cell><cell>80.2</cell><cell>79.1</cell><cell cols="2">79.0 79.0</cell></row><row><cell>NT</cell><cell>92.0</cell><cell>16.6</cell><cell>2.4</cell><cell>0.3</cell><cell>0.3</cell><cell>0.3</cell></row><row><cell>Mixup</cell><cell>91.0</cell><cell>37.7</cell><cell>0.1</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell cols="2">BPFC (Ours) 87.1</cell><cell>73.1</cell><cell>70.2</cell><cell>68.0</cell><cell cols="2">67.7 67.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S3 :</head><label>S3</label><figDesc>MNIST: Recognition accuracy (%) of models in a white-box attack setting.</figDesc><table><row><cell>Training method</cell><cell cols="2">Clean FGSM</cell><cell>IFGSM 40 steps</cell><cell cols="3">PGD (n-steps) 40 100 1000</cell></row><row><cell>FGSM-AT</cell><cell>99.4</cell><cell>89.6</cell><cell>29.4</cell><cell>13.8</cell><cell>4.9</cell><cell>3.7</cell></row><row><cell>RSS-AT</cell><cell>99.0</cell><cell>96.4</cell><cell>93.1</cell><cell>93.0</cell><cell cols="2">90.9 90.4</cell></row><row><cell>PGD-AT</cell><cell>99.3</cell><cell>96.2</cell><cell>94.9</cell><cell>95.4</cell><cell cols="2">94.3 94.1</cell></row><row><cell>NT</cell><cell>99.2</cell><cell>82.7</cell><cell>0.5</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Mixup</cell><cell>99.4</cell><cell>58.1</cell><cell>0.2</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell cols="2">BPFC (Ours) 99.1</cell><cell>94.4</cell><cell>92.0</cell><cell>91.5</cell><cell cols="2">86.6 85.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S4 :</head><label>S4</label><figDesc>PGD attack with multiple random restarts:</figDesc><table><row><cell>Recognition accuracy (%) of different models on PGD ad-</cell></row><row><cell>versarial samples with multiple random restarts in a white-</cell></row><row><cell>box setting.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S5 :</head><label>S5</label><figDesc>DeepFool and C&amp;W attacks (Fashion-MNIST): Average 2 norm of the generated adversarial perturbations is reported. Higher 2 norm implies better robustness. Fooling rate (FR) represents percentage of test set samples that are misclassified.</figDesc><table><row><cell>Training</cell><cell cols="2">DeepFool</cell><cell cols="2">C&amp;W</cell></row><row><cell>method</cell><cell>FR (%)</cell><cell>Mean 2</cell><cell>FR (%)</cell><cell>Mean 2</cell></row><row><cell>FGSM-AT</cell><cell>94.34</cell><cell>1.014</cell><cell>100.0</cell><cell>0.715</cell></row><row><cell>PGD-AT</cell><cell>90.70</cell><cell>3.429</cell><cell>100.0</cell><cell>2.142</cell></row><row><cell>RSS-AT</cell><cell>91.22</cell><cell>2.762</cell><cell>99.9</cell><cell>1.620</cell></row><row><cell>NT</cell><cell>94.07</cell><cell>0.467</cell><cell>100.0</cell><cell>0.406</cell></row><row><cell>Mixup</cell><cell>92.22</cell><cell>0.226</cell><cell>100.0</cell><cell>0.186</cell></row><row><cell>BPFC (Ours)</cell><cell>90.94</cell><cell>3.620</cell><cell>100.0</cell><cell>1.789</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S6 :</head><label>S6</label><figDesc>DeepFool and C&amp;W attacks (MNIST): Average 2 norm of the generated adversarial perturbations is reported. Higher 2 norm implies better robustness. Fooling rate (FR) represents percentage of test set samples that are misclassified.</figDesc><table><row><cell>Training</cell><cell cols="2">DeepFool</cell><cell cols="2">C&amp;W</cell></row><row><cell>method</cell><cell>FR (%)</cell><cell>Mean 2</cell><cell>FR (%)</cell><cell>Mean 2</cell></row><row><cell>FGSM-AT</cell><cell>99.36</cell><cell>3.120</cell><cell>100.0</cell><cell>1.862</cell></row><row><cell>PGD-AT</cell><cell>95.97</cell><cell>5.316</cell><cell>100.0</cell><cell>3.053</cell></row><row><cell>RSS-AT</cell><cell>94.41</cell><cell>4.894</cell><cell>98.2</cell><cell>2.725</cell></row><row><cell>NT</cell><cell>99.15</cell><cell>1.601</cell><cell>100.0</cell><cell>1.427</cell></row><row><cell>Mixup</cell><cell>91.82</cell><cell>0.518</cell><cell>100.0</cell><cell>0.498</cell></row><row><cell>BPFC (Ours)</cell><cell>97.47</cell><cell>6.289</cell><cell>100.0</cell><cell>3.041</cell></row><row><cell cols="3">S2.1.2 Unbounded attacks</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table S7 :</head><label>S7</label><figDesc>PGD Black-box attacks: Recognition accuracy (%) of different models on PGD black-box adversaries. Columns represent source model used for generating the attack. 7-step attack is used for CIFAR-10 and 40-step attack is used for Fashion-MNIST and MNIST</figDesc><table><row><cell>Training</cell><cell cols="2">CIFAR-10</cell><cell cols="2">Fashion-MNIST</cell><cell cols="2">MNIST</cell></row><row><cell>method</cell><cell cols="6">VGG19 ResNet18 Net-A M-LeNet Net-A M-LeNet</cell></row><row><cell>FGSM-AT</cell><cell>85.85</cell><cell>85.61</cell><cell>94.27</cell><cell>91.52</cell><cell>79.8</cell><cell>74.11</cell></row><row><cell>RSS-AT</cell><cell>80.92</cell><cell>80.82</cell><cell>84.71</cell><cell>83.91</cell><cell cols="2">95.19 96.27</cell></row><row><cell>PGD-AT</cell><cell>81.37</cell><cell>81.22</cell><cell>85.16</cell><cell>85.71</cell><cell cols="2">96.52 96.69</cell></row><row><cell>NT</cell><cell>16.86</cell><cell>0</cell><cell>27.10</cell><cell>0.33</cell><cell>4.64</cell><cell>0.03</cell></row><row><cell>Mixup</cell><cell>30.16</cell><cell>29.53</cell><cell>49.07</cell><cell>60.71</cell><cell>31.4</cell><cell>58.25</cell></row><row><cell cols="2">BPFC (Ours) 80.42</cell><cell>80.15</cell><cell>81.45</cell><cell>83.00</cell><cell cols="2">95.31 95.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table S8 :</head><label>S8</label><figDesc>MI-FGSM<ref type="bibr" target="#b6">[7]</ref> Black-box attacks: Recognition accuracy (%) of different models on MI-FGSM black-box adversaries. Columns represent source model used for generating the attack. 7-step attack is used for CIFAR-10 and 40-step attack is used for Fashion-MNIST and MNIST</figDesc><table><row><cell>Training</cell><cell cols="2">CIFAR-10</cell><cell cols="2">Fashion-MNIST</cell><cell cols="2">MNIST</cell></row><row><cell>method</cell><cell cols="6">VGG19 ResNet18 Net-A M-LeNet Net-A M-LeNet</cell></row><row><cell>FGSM-AT</cell><cell>76.44</cell><cell>74.22</cell><cell>94.61</cell><cell>92.11</cell><cell cols="2">79.95 73.92</cell></row><row><cell>RSS-AT</cell><cell>80.21</cell><cell>80.10</cell><cell>84.61</cell><cell>84.02</cell><cell cols="2">96.11 95.28</cell></row><row><cell>PGD-AT</cell><cell>80.47</cell><cell>80.59</cell><cell>84.98</cell><cell>85.58</cell><cell cols="2">95.56 95.34</cell></row><row><cell>NT</cell><cell>12.98</cell><cell>0.04</cell><cell>28.28</cell><cell>4.69</cell><cell>12.48</cell><cell>1.93</cell></row><row><cell>Mixup</cell><cell>35.74</cell><cell>25.22</cell><cell>50.60</cell><cell>63.32</cell><cell cols="2">43.72 62.98</cell></row><row><cell cols="2">BPFC (Ours) 79.04</cell><cell>79.04</cell><cell>81.33</cell><cell>82.70</cell><cell cols="2">94.03 94.46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table S11 :</head><label>S11</label><figDesc>MNIST: Recognition accuracy (%) of the model trained using the proposed approach on adversarial samples generated using adaptive attacks.</figDesc><table><row><cell>Adaptive attack</cell><cell cols="6">Loss coefficients n-step Adaptive attack</cell></row><row><cell></cell><cell cols="3">λ ce λ g λ LSB</cell><cell>40</cell><cell>100</cell><cell>500</cell></row><row><cell>PGD</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell cols="3">68.03 67.75 67.71</cell></row><row><cell>Variation in λ g</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell cols="3">69.41 69.22 69.19</cell></row><row><cell>(λ ce = 1 and</cell><cell>1</cell><cell>10</cell><cell>0</cell><cell cols="3">76.72 76.44 76.46</cell></row><row><cell>λ LSB = 0)</cell><cell>1</cell><cell>25</cell><cell>0</cell><cell>78.95</cell><cell>78.8</cell><cell>78.79</cell></row><row><cell>Variation in λ g (λ ce = 0 and λ LSB = 0)</cell><cell>0 0 0 0</cell><cell>1 10 25 50</cell><cell>0 0 0 0</cell><cell cols="3">80.45 80.23 80.46 80.24 80.22 80.2 80.46 80.22 80.18 80.46 80.22 80.18</cell></row><row><cell>Variation in λ LSB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(λ ce = 1 and</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>68.2</cell><cell cols="2">67.98 67.95</cell></row><row><cell>λ g = 0)</cell><cell>1</cell><cell>0</cell><cell>10</cell><cell cols="3">71.32 70.98 70.98</cell></row><row><cell>Variation in λ LSB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(λ ce = 1 and</cell><cell>1</cell><cell>25</cell><cell>1</cell><cell>78.96</cell><cell>78.8</cell><cell>78.77</cell></row><row><cell>λ g = 25)</cell><cell>1</cell><cell>25</cell><cell>10</cell><cell cols="3">78.91 78.74 78.76</cell></row><row><cell>Adaptive attack</cell><cell cols="6">Loss coefficients n-step Adaptive attack</cell></row><row><cell></cell><cell cols="3">λ ce λ g λ LSB</cell><cell>40</cell><cell>100</cell><cell>500</cell></row><row><cell>PGD</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>91.49</cell><cell>86.6</cell><cell>85.63</cell></row><row><cell>Variation in λ g</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell cols="2">92.99 89.13</cell><cell>88.2</cell></row><row><cell>(λ ce = 1 and</cell><cell>1</cell><cell>10</cell><cell>0</cell><cell cols="3">94.58 91.75 91.04</cell></row><row><cell>λ LSB = 0)</cell><cell>1</cell><cell>30</cell><cell>0</cell><cell cols="3">94.74 91.99 91.26</cell></row><row><cell>Variation in λ g (λ ce = 0 and λ LSB = 0)</cell><cell>0 0 0 0</cell><cell>1 10 30 50</cell><cell>0 0 0 0</cell><cell cols="3">94.8 94.8 94.79 91.98 91.38 91.96 91.34 91.97 91.34 94.79 91.97 91.35</cell></row><row><cell>Variation in λ LSB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(λ ce = 1 and</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell cols="3">91.56 86.96 86.01</cell></row><row><cell>λ g = 0)</cell><cell>1</cell><cell>0</cell><cell>10</cell><cell cols="3">93.51 90.11 89.41</cell></row><row><cell>Variation in λ LSB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(λ ce = 1 and</cell><cell>1</cell><cell>30</cell><cell>1</cell><cell cols="3">94.72 91.98 91.33</cell></row><row><cell>λ g = 30)</cell><cell>1</cell><cell>30</cell><cell>10</cell><cell>94.7</cell><cell cols="2">91.97 91.36</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">loss term as the Bit Plane Feature Consistency (BPFC) regularizer, as it ensures that the network learns consistent feature representations across the original image as well as the coarse quantized image. The loss for each minibatch of size M is an average over all samples in the minibatch.The cross-entropy term on original images ensures that a combination of coarse and fine features is used to learn the overall function mapping g(.). This helps preserve the accuracy on clean images, while the BPFC regularizer helps improve the adversarial robustness of the model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>This work was supported by RBCCPS, IISc and Uchhatar Avishkar Yojana (UAY) project (IISC 10), MHRD, Govt. of India. We would like to extend our gratitude to all the reviewers for their valuable suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the FGSM attack. It can be observed that the loss increases monotonically with an increase in perturbation size. These two plots confirm that there is no gradient masking effect <ref type="bibr" target="#b0">[1]</ref> in the models trained using the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2.5. Results on ImageNet</head><p>We report results on ImageNet dataset using the proposed method and PGD-AT in Table-S12. The architecture used for both methods is ResNet-50 <ref type="bibr" target="#b11">[12]</ref>. We use the PGD-AT pre-trained model from <ref type="bibr" target="#b7">[8]</ref> for comparison. We train the proposed method for 125 epochs and decay learning rate by a factor of 10 at epochs 35, 70 and 95. Similar to CIFAR-10, we start with a λ of 1 and step it up by a factor of 9 at epochs 35 and 70. We use a higher step-up factor of 20 at epoch 95 to improve robustness. Since training ImageNet models is computationally intensive, we report results using similar hyperparameters as that of CIFAR-10. However, tuning hyperparameters specifically for ImageNet can lead to improved results. Accuracy on black-box FGSM attack is 47.39% for PGD-AT and 40.41% for the BPFC trained model. We note that the trend in robustness when compared to PGD-AT is similar to that of CIFAR-10, thereby demonstrating the scalability of the proposed approach to large scale datasets.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00420</idno>
		<imprint>
			<date type="published" when="2005">2018. 2, 5</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06705</idno>
		<title level="m">On evaluating adversarial robustness</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic activation pruning for robust adversarial defense</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guneet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">D</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Kossaifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note>and Dimitris Tsipras. Robustness (python library</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reliable detection of lsb steganography in color and grayscale images</title>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Goljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 Workshop on Multimedia and Security: New Challenges</title>
				<meeting>the 2001 Workshop on Multimedia and Security: New Challenges</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adversarial example defense: Ensembles of weak defenses are not strong</title>
		<author>
			<persName><forename type="first">Warren</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th U SEN IX Workshop on Offensive Technologies (W OOT )</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cifar-10 (canadian institute for advanced research</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/.5" />
		<title level="m">The mnist database of handwritten digits</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Characterizing adversarial subspaces using local intrinsic dimensionality</title>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">M</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsipras</forename><surname>Dimitris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018. 1, 2, 5, 6, 13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Asia Conference on Computer and Communications Security</title>
				<meeting>the ACM Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Decoupling direction and norm for efficient gradient-based l2 adversarial attacks and defenses</title>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luiz</forename><forename type="middle">S</forename><surname>Luiz G Hafemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismail</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image retrieval based on bitplane distribution entropy</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Hai-Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Science and Software Engineering (CSSE)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pixeldefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Representing the forest before the trees: a global advantage effect in monkey inferotemporal cortex</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">R</forename><surname>Sripati</surname></persName>
		</author>
		<author>
			<persName><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Global and fine information coded by single neurons in the temporal visual cortex</title>
		<author>
			<persName><forename type="first">Yasuko</forename><surname>Sugase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigeru</forename><surname>Yamane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoogo</forename><surname>Ueno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Kawano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">400</biblScope>
			<biblScope unit="issue">6747</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12152</idno>
		<title level="m">Robustness may be at odds with accuracy</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adversarial risk and the dangers of evaluating against weak attacks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05666</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05236</idno>
		<title level="m">Manifold mixup: Better representations by interpolating hidden states</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regularizer to mitigate gradient masking effect during single-step adversarial training</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arya</forename><surname>Baburaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fashionmnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01155</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08573</idno>
		<title level="m">Theoretically principled trade-off between robustness and accuracy</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
