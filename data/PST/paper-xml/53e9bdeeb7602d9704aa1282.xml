<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Maintaining a Large Matching and a Small Vertex Cover</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Krzysztof</forename><surname>Onak</surname></persName>
							<email>konak@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MIT</orgName>
								<orgName type="institution" key="instit2">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Mit</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MIT</orgName>
								<orgName type="institution" key="instit2">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ronitt</forename><surname>Rubinfeld</surname></persName>
							<email>ronitt@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MIT</orgName>
								<orgName type="institution" key="instit2">Tel Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Maintaining a Large Matching and a Small Vertex Cover</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">122430B6C8742A64F6A2547B564BBEE5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graphs and Networks</term>
					<term>F.2.2 [Analysis of Algorithms and Problem Complexity]: Nonnumerical Algorithms and Problems</term>
					<term>G.2.2 [Graph Theory]: Graph Algorithms dynamic algorithms, data structures, maximum matching, vertex cover</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of maintaining a large matching and a small vertex cover in a dynamically changing graph. Each update to the graph is either an edge deletion or an edge insertion. We give the first randomized data structure that simultaneously achieves a constant approximation factor and handles a sequence of K updates in K • polylog(n) time, where n is the number of vertices in the graph. Previous data structures require a polynomial amount of computation per update.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Suppose one is given the task of solving a combinatorial problem, such as vertex cover or maximum matching, for a very large and constantly changing graph. In this setting, it is natural to ask, does one need to recompute the solution from scratch after every update? Such questions have been asked before for various combinatorial quantities-examples include minimum spanning tree, shortest path length, min-cut, and many others (some examples include <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b14">14]</ref>). Classic works for these problems have shown update times that are sublinear in the input size. For the problem of maximum matching, Sankowski <ref type="bibr" target="#b13">[13]</ref> shows that it can be maintained with O(n 1.495 ) computation per update (n is the number of vertices in the graph), which for dense graphs is sublinear in the number of edges.</p><p>For very large graphs, it may be crucial to maintain the maximum matching with much faster, even polylogarithmic, update time. Note that this may be hard for maximum matching, since obtaining o( √ n) update time, even in the case when only insertions are allowed, would improve on the 30-year-old algorithm of running time O(m √ n) due to Micali and Vazirani <ref type="bibr" target="#b11">[11]</ref>, where m is the number of edges in the graph. Therefore, some kind of approximation may be unavoidable. Following similar considerations, Ivković and Lloyd <ref type="bibr" target="#b7">[7]</ref> give a factor-2 approximation to both vertex cover and maximum matching, by maintaining a maximal matching (which is well known to give the desired approximation for maximum matching and minimum vertex cover). Their update time is nevertheless still polynomial in n. More precisely, it is O((n + m) 0.7072 ), which is o(n) for sparse graphs.</p><p>In this paper, we concentrate on the setting in which slightly weaker, but still O(1), approximation factors are acceptable, and in which it is crucial that update times be extremely fast, in particular, polylogarithmic in the graph size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Problem Statement and Our Results</head><p>Recall that in the maximum matching problem, one wants to find the largest subset of vertex disjoint edges. In the vertex cover problem, one wants to find the smallest set of vertices such that each edge of the graph is incident to at least one vertex in the set.</p><p>Our goal here is to design a data structure that handles edge removals and edge insertions. The data structure provides access to a list of edges that constitute a large matching or a list of vertices that constitute a small vertex cover. We assume that we always start with an empty graph, and n is known in advance.</p><p>The main result of the paper is the following:</p><p>There is a randomized data structure for maximum matching and vertex cover that Furthermore, the first step in our presentation is a deterministic data structure for vertex cover. The data structure keeps a vertex cover that gives an O(log n) approximation to the minimum vertex cover. The amortized update time of the data structure is O(log 2 n). Though the approximation factor achieved by this algorithm is relatively weak, the algorithm may be of independent interest because of its relative simplicity and efficient update time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Overview of Our Techniques</head><p>We present our main result in two stages.</p><p>A Deterministic O(log n)-Approximation Data Structure: We construct a data structure that makes use of a carefully designed partition of vertices into a logarithmic number of subsets. The partition is inspired by a simple distributed algorithm of Parnas and Ron <ref type="bibr" target="#b12">[12]</ref>. In <ref type="bibr" target="#b12">[12]</ref>, the first subset in the partition corresponds to removing vertices of degree approximately n. The second subset corresponds to removing vertices of degree close to n/4 from the modified graph. In general, the i-th subset is a set of vertices that are approximately n/4 i-1 in the graph with all previous subsets of vertices removed. Finally, after a logarithmic number of steps, the remaining graph has no edges. This implies that the union of all subsets removed so far constitutes a vertex cover. For each of the removed subsets, it is easy to show that the subset size is bounded by O(VC(G)), where VC(G) is the size of the minimum vertex cover. Hence the total vertex cover is bounded by O(VC(G) • log n).</p><p>The main idea behind our data structure is to modify the partition of Parnas and Ron in order to allow efficient maintenance of this partition, under edge insertions and deletions. While this is not possible in the partition of Parnas and Ron, it is possible in our relaxed version of it. As edges are inserted and removed, we want to move vertices between subsets. In order to determine whether to move a vertex, we associate a potential function with every vertex, and we allow a vertex to jump from one set to another only if it has collected enough potential. To do this, we set two thresholds τ1 &lt; τ2 for each subset. A vertex can move into the subset from a subset corresponding to a lower degree if its number of neighbors in a specific graph is at least τ2. Then the vertex can move back to a subset corresponding to a lower degree only if the number of edges decreases to τ1 in the same graph. A slight technical difficulty is presented by the fact that moving vertices may increase the potential of other vertices. We overcome this obstacle by carefully selecting constants in the potential function so that the potential of the vertex that moves is spent on increasing the potential of its neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Randomized O(1)-Approximation Data Structure:</head><p>In this case, we redesign the partition, building upon the previous one. In the process of defining the partition, whenever we remove a large subset W of vertices of degree approximately n/4 i , we also show the existence of a matching M which is smaller than W by at most a constant factor. To build the next set of the partition, we not only remove W but also all vertices matched in M . In this way we achieve a matching and a vertex cover of sizes that are within a constant factor of each other. Therefore, both give a constant factor approximation to their respective optimal solutions.</p><p>Efficient maintenance of the new partition is more involved, as we are sometimes forced to recompute a matching. This can happen, for instance, when many edges in the matching are deleted from the graph. Unfortunately, the creation of a new matching is expensive, since we have modified the set of the vertices matched in M that are deleted together with W . If the edges in the matching are deleted too quickly, we have to create a new matching often, in which case we do not know how to maintain small update time. Fortunately, by picking a large random matching, we can ensure that it is unlikely that many edges from the matching get deleted in a short span of time. Thus, by the time the matching gets deleted, we are likely to have collected enough potential to pay for the creation of a new matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Other Related Work</head><p>A sequence of papers <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b4">4]</ref> considers computing a large matching or a large weight matching (in the weighted case) in the semi-streaming model. The stream is a sequence of edges, and the goal of an algorithm is to compute a large matching in a small number of passes over the stream, using O(n • log O(1) n) space, and preferably at most polylog(n) update time. Results in this model correspond to results for dynamically changing graphs in which only edge insertions occur, except that the matching is only output once, at the end of the processing. To the best of our knowledge, it is not known how to achieve a better approximation factor than 2 in one pass for the maximum matching problem.</p><p>Lotker, Patt-Shamir, and Rosén <ref type="bibr" target="#b9">[9]</ref> show how to maintain a large matching in a distributed network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRELIMINARIES</head><p>We assume that all the necessary simple set operations (insert, remove, find, . . . ) on ordered sets of size t can be implemented in O(log t) time. A number of tree based dictionaries (AVL trees, red-black trees, etc.) have this property (see for instance the textbook of Cormen et al. <ref type="bibr" target="#b1">[1]</ref>). We also assume that the first s items in a set can be accessed in O(s) time, which can usually easily be achieved by augmenting a given data structure with additional links.</p><p>Throughout the paper, α is a fixed integer greater than 1. We write kα to denote log α n + 2. Moreover, VC(G) is the minimum vertex cover size in G, and MM(G) is the maximum matching size in G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Basic Facts</head><p>Fact 1. Let G be a graph, and let M be a matching in G. Then, VC(G) ≥ |M |.</p><p>Lemma 2. Let G be a graph of maximum degree d. Let V be a subset of vertices such that every vertex in V has degree between d/γ and d. The following holds:</p><p>• There is a matching M of size at least |V |/(4γ) with each edge incident to a vertex in V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• |V | ≤ 4γ • VC(G).</head><p>Proof. There are at least X </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Simple O(d)-Update-Time Data Structure</head><p>We now describe a straightforward data structure for maintaining a maximal matching in a graph of maximum degree bounded by d. To the best of our knowledge, the data structure was first described by Ivković and Lloyd <ref type="bibr" target="#b7">[7]</ref>.</p><p>For every vertex, the data structure maintains information indicating whether it is matched. Whenever an edge is inserted, the data structure checks if its endpoints are matched or not. If none of them are, the edge is added to the matching. Whenever an edge e in the maximal matching is removed, the data structure checks whether the remaining matching may be extended by adding edges incident to the endpoints of e. To do this, the data structure goes over O(d) edges that were adjacent to e, and greedily tries to extend the matching with each of them.</p><p>It is easy to show that the matching held by the data structure is maximal. It can be used for obtaining a 2approximation of both the minimum vertex cover (use the endpoints of edges in the matching) and the maximum matching (use the maximal matching itself). The Insert operation takes O(1) time, and the Delete operation requires O(d) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WARMUP: DETERMINISTIC</head><p>O(log n)-APPROXIMATION FOR VERTEX COVER</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Sequential Algorithm</head><p>Consider first the sequential Algorithm 1. The algorithm is a modification of a simple distributed algorithm for vertex cover that was used by Parnas and Ron <ref type="bibr" target="#b12">[12]</ref>.</p><formula xml:id="formula_0">Algorithm 1: A sequential O(log n)-approximation al- gorithm for vertex cover Input: graph G, integer α &gt; 1 kα := log α n + 2 1 G kα := G 2 for i := kα downto 1 do 3 Vi := {vertices of degree ≥ α i-1 in Gi} 4 ∪ {arbitrary subset of vertices of degree 5 in [α i-2 , α i-1 ) in Gi} 6 Gi-1 := Gi with vertices in Vi removed 7 return S kα i=1 Vi 8 Lemma 3.</formula><p>Let α be an integer greater than 1. The size of each set Vi in Algorithm 1 is bounded by</p><formula xml:id="formula_1">4α 2 VC(G). Algo- rithm 1 computes a vertex cover of size ≤ 4α 2 • kα • VC(G).</formula><p>Proof. The algorithm repeatedly removes vertices and their adjacent edges from the original graph, and adds the removed vertices to the cover. To see that the algorithm computes a vertex cover, note that the final graph G0 has no edges, which means that all edges of G have been covered by the output of the algorithm.</p><p>Let i be any integer between 1 and kα. The maximum degree of Gi is bounded by α i . By Lemma 2, |Vi| ≤ 4α 2 • VC(Gi) ≤ 4α 2 • VC(G). This implies that the size of the cover returned by the algorithm is at most kα•4α 2 VC(G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Data Structure</head><p>We design a data structure that keeps a partition of vertices into a logarithmic number of sets Vi, 0 ≤ i ≤ kα. The partition is one that could potentially be created in an execution of Algorithm 1. We refer to sets Vi as buckets. The sets Vi with i &gt; 0 are sets of vertices removed in consecutive executions of the loop of Algorithm 1, and V0 is the set of vertices that are not removed from the graph by the algorithm. For i &gt; 0, each Vi consists of vertices that at the time of removal, have degree between α i-2 and α i . The union of Vi over i &gt; 0 is the current vertex cover.</p><p>For every vertex v, we maintain the following variables:</p><formula xml:id="formula_2">index[v]: the index of the set Vi that contains v. neighbors[v, j] for j ≥ index[v]: the set of neighbors of v that that belong to Vj. below[v, j] for j ≥ index[v]: the total number of all neigh- bors of v in sets V0 through Vj. lower-neighbors[v]: the set of neighbors of v that belong to Vi for i &lt; index[v].</formula><p>We call the collection of vertices involving v the structures of v.</p><p>Initially, the graph is empty, so all sets of neighbors are empty, and index[v] = 0 and below[v, j] = 0 for all v and j.</p><p>We maintain the following invariants for each vertex v after each update to the graph:</p><p>• To ensure that v's bucket number index[v] is not too high, i.e., that it has enough edges to nodes in lower buckets, we ask that if index</p><formula xml:id="formula_3">[v] &gt; 0, then below[v, index[v]] &gt; α index[v]-2 .</formula><p>• On the other hand, to ensure that v's bucket number is not too low, i.e., that it should not have been placed in a higher bucket, we ask that for each i ∈ {index</p><formula xml:id="formula_4">[v]+ 1, . . . , kα}, below[v, i] &lt; α i-1 .</formula><p>Note that if this is the case, then the sets Vi, 1 ≤ i ≤ kα defined as Vi = {v ∈ V : index[v] = i} could potentially be created by the non-deterministic Algorithm 1.</p><p>As a result of edge removals and insertions, the invariants may no longer hold. We first design a procedure Restabilize that given a set of vertices for which the invariant may not hold (we call such vertices dirty), attempts to fix the partition given by index[•]. As long as there is a dirty vertex v, the procedure does the following. Finally, the procedure marks v as no longer dirty. In this case, the procedure does not change the status of v. It still remains dirty, since the procedure may have to decrease index[v] further 1 .</p><formula xml:id="formula_5">• If there is an i &gt; index[v], such that below[v, i] ≥ α i-1 ,</formula><formula xml:id="formula_6">• Otherwise, if index[v] &gt; 0, and below[v, index[v]] ≤ α index[v]-2 ,</formula><p>• If none of the previous cases occurred, v is already in the right bucket, and there is no need to move it. The procedure marks the vertex as no longer dirty.</p><p>It is not immediately clear that the above procedure Restabilize always stops. We show that this is the case in Section 3.  where</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Complexity Analysis</head><formula xml:id="formula_7">Φ1(v) def = 0, if index[v] = 0, Φ1(v) def = 8 • min  max n α index[v]-1 -below[v, index[v]], 0 o , α index[v]-1 -α index[v]-2 ff for index[v] &gt; 0,<label>and</label></formula><formula xml:id="formula_8">Φ2(v) def = 12 • kα X i=index[v]+1 max n below[v, i] -α i-2 , 0 o .</formula><p>Φ1(v) corresponds to losing neighbors. When v loses sufficiently many of them, there is enough potential to pay for decreasing index</p><formula xml:id="formula_9">[v]. Φ2(v) is related to the number of neigh- bors u with index[u] &gt; index[v]. We only increase index[v]</formula><p>if there are sufficiently many of them, and then Φ2(v) provides enough potential to conduct the operation. For the initial empty graph, all Φ(v) = 0.</p><p>Each unit of the potential corresponds to O(log n) computation. Inserting or removing an edge can only change the potential of the endpoints of the edge, and the change is bounded by O(kα), because each of O(kα) terms can only change by a constant. We show that fixing the invariant of the data structure, i.e., executing the procedure Restabilize, is almost entirely paid for by potentials of vertices. More precisely, we show that the amortized complexity of Restabilize is the number of vertices that are initially marked as dirty times O(log n). Assuming this, the amortized cost of both Insert and Delete is O(kα</p><formula xml:id="formula_10">• log n) = O(log 2 n).</formula><p>Consider the case when Restabilize increases index</p><formula xml:id="formula_11">[v] for a vertex v. If this happens, index[v] becomes t such that below[v, t] ≥ α t-1 . We can use up to 12• `below[v, t] -α t-2 únits</formula><p>of the potential of v. This comes from the decrease in Φ(v), and more precisely in Φ2(v). Once index[v] is set to t, Φ1(v) becomes 0, and Φ2(v) only equals</p><formula xml:id="formula_12">kα X i=t+1 max n below[v, i] -α i-2 , 0 o .</formula><p>Restabilize updates structures for all neighbors u of v such that index[u] ≤ t. It also marks some of them as dirty, and the potential has to pay also for checking later whether the invariant holds for them. This costs at most </p><formula xml:id="formula_13">O(log n) • below[v, t], that is, below[v,</formula><formula xml:id="formula_14">= 9 • below[v, t] below[v, t] -α t-2 • `below[v, t] -α t-2 ≤ 9 • α t-1 α t-1 -α t-2 • `below[v, t] -α t-2 = 9α α -1 • `below[v, t] -α t-2 = 12 • `below[v, t] -α t-2 ´,</formula><p>which is not more than the available budget. Consider now the other case when Restabilize decreases index[v] by one for a vertex v. Let t be the new index[v], i.e., the old index[v] minus one. Note that the old Φ1(v) equals 8 • (α t -α t-1 ). Restabilize updates structures for at most α t-1 neighbors u of v, and it also marks some of them as dirty. This costs at most α t-1 units of potential together with checking later whether the invariant holds for them. Moving v may also increase below[u, t] by one for all of them, and this may increase their potential Φ2. The total increase is at most 12 • α t-1 units of potential. Furthermore, the new potential Φ1(v) can still be positive, but it can be bounded by 8•(α t-1 -α t-2 ). Finally, we pay 1 for reverifying if the invariant holds for v after the modification of index <ref type="bibr">[v]</ref>. The total expense is bounded by</p><formula xml:id="formula_15">α t-1 + 12 • α t-1 + 8 • (α t-1 -α t-2 ) + 1 ≤ " 13 α -1 + 8 α + 1 α t -α t-1 « • (α t -α t-1 ) ≤ " 13 3 + 2 + 4 3 « • (α t -α t-1 ) ≤ 8 • (α t -α t-1 ),</formula><p>which is the old Φ1(v).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RANDOMIZED O(1)-APPROXIMATION</head><p>We now describe a new data structure that maintains an O(1) approximation for maximum matching and vertex cover. The data structure handles a sequence of updates in • polylog(n) time. In Section 4.1, we describe a new method of creating a partition of the vertices along with the properties that the partition satisifies. In Section 4.2, we describe how to generate a matching and vertex cover from the partition and bound the approximation factor. In Section 4.4, we give the implementation details which allow one to maintain the partition along with the required properties through the successive updates. In Section 4.5, we describe the amortized analysis of the update time.</p><p>In the following description, we use sufficiently large positive constants C1, C2, and CT. We require that C2 C1 and C2</p><p>CT. As before, we assume that the α we use equals 4. Recall also that kα = log α n + 2 , that is, kα = O(log n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The new partition and its properties</head><p>As in the partition of Algorithm 1, we partition the vertices into a logarithmic number of sets. We remove a set of vertices at each of logarithmically many phases based on the degrees of the vertices being considered. However, whereas in Algorithm 1, only high degree vertices were removed at each phase, here we may remove additional vertices. In the following, we mainly focus on describing differences from the partition constructed by Algorithm 1.</p><p>Let ∆ def = 2+ log α 2kα . For all i ∈ {1, . . . , kα}, recall that Gi is the graph remaining in the i-th loop of Algorithm 1, and Vi is the set of vertices of Gi. We define Ei as the edges incident to vertices in Vi in graph Gi. The new partition differs as follows:</p><p>1. We stop partitioning the graph when i ≤ ∆ (i.e., step 3 of Algorithm 1 is changed to "for i := kα to ∆ + 1").</p><p>The graph G∆ has degree bounded by α ∆ ≤ 2α 3 kα = O(log n), and we use the simple data structure of Section 2.2 to maintain a maximal matching M in that graph. Each update costs O(log n).</p><p>2. For each i &gt; ∆, we select a set Vi in the same way as in Algorithm 1. Each i &gt; ∆ is either heavy or light. In general, when |Vi| is sufficiently large, then i is heavy, and when |Vi| is sufficiently small, then i is light, but there is a range of |Vi| for which either alternative may be the case. In Section 4.4, we describe how the choice is made so that we can bound the amortized complexity of the data structure.</p><p>For each i &gt; ∆, one of the following two is the case in the partition: </p><formula xml:id="formula_16">heavy i: Apart from Vi,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approximating the Maximum Matching and the Minimum Vertex Cover</head><p>We now describe how the above partition is used to maintain a constant-factor approximation to both the maximum matching and the minimum vertex cover.</p><p>The current matching and the current vertex cover kept by the data structure are the following:</p><p>• At the end of every Insert and Delete operation, we compute a matching M light that matches at least one vertex in Vi for each light i with non-empty Vi. We pick one vertex from each such Vi. There are at most kα such vertices, and each of them has degree greater than α ∆ /α 2 ≥ 2kα. This means that considering them in any order and going over the list of their neighbors, we eventually find a neighbor that has not yet been matched. This way, we get a matching of size at least half the number of light i. The whole procedure takes at most O(log 2 n) time, because we consider at most O(log 2 n) vertices, and for each of them we check whether it is already matched in M light .</p><p>We write M heavy to denote the union of all Mi for i heavy. Note that M heavy ∪ M is a matching as well.</p><p>Combining M heavy ∪ M with M light gives a graph of degree at most 2, and we use the simple data structure of Section 2.2 to maintain a matching f M of size at least 1  2 max{|M light |, |M heavy |+|M |}. f M is the current matching.</p><p>• The current vertex cover e V is the union of all Vi for i &gt; ∆, V i for heavy i &gt; ∆, and the vertices matched in M .</p><formula xml:id="formula_17">Since | f M | ≤ MM(G) ≤ VC(G) ≤ | e V | (see Fact 1), it suffices to show that | f M | ≥ | e V |/C, for some constant C to prove that f</formula><p>M and e V are constant factor approximations to maximum matching and vertex cover, respectively. Note that</p><formula xml:id="formula_18">˛[ light i Vi ˛≤ 2 • α 2 • CT • |M light |, ˛[ heavy i `Vi ∪ V i ´˛≤ C1 • |M heavy |, |{endpoints of edges in M }| ≤ 2 • |M |.</formula><p>Therefore,</p><formula xml:id="formula_19">˛e V ˛≤ 2α 2 CT • |M light | + C1 • |M heavy | + 2 • |M | ≤ 2α 2 CT • |M light | + 2C1 • (|M heavy | + |M |) ≤ 2α 2 C1CT • |M light | + 2α 2 C1CT • (|M heavy | + |M |) ≤ 4α 2 C1CT • max{|M light |, |M heavy | + |M |} = 8α 2 C1CT • 1 2 max{|M light |, |M heavy | + |M |} ≤ 8α 2 C1CT • | f M |.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Selecting a Large Matching Mi of Vi</head><p>We now describe how to select a large matching Mi for i heavy. The lemma states properties of our procedure, which we will use in the next section.</p><p>Lemma 5. Let Gi be a graph of maximum degree α i . Let n be the number of vertices in Gi. Let Vi be a set of vertices in Gi, each with degree in [α i-2 , α i ]. Let Ei be the set of edges in Gi incident to at least one vertex in Vi, and let |Ei| ≥ CT • α i .</p><p>Let S be the distribution on subsets of Ei created by independently selecting every edge in Ei with probability</p><formula xml:id="formula_20">p def = 1/(C2α i ).</formula><p>There is an algorithm A that with the following properties:</p><p>• A selects a subset E of edges from a distribution S on subsets of Ei such that the statistical distance between S and S is at most 1/1000.</p><formula xml:id="formula_21">• The size of E is at most 21 20 • p|Ei|. • With probability 998/1000, A outputs a matching M that is a subset of E and |M | ≥ 9 10 • p|Ei|. The running time of A is O(|Ei| log n).</formula><p>Proof. The set E is selected as follows. The algorithm goes over all edges in Ei, and selects each edge independently with probability p. If at some point the number of selected edges reaches 21 20 • p|Ei|, the procedure stops selecting new edges. If CT and C2 are large enough, then this does not happen with probability greater than 1/1000 (via the Chernoff bound), so the statistical distance between S and S is at most 1/1000.</p><p>Suppose for now that E is selected according to S, not S . If C2 is large enough, then the probability that a given edge in E intersects with another edge in E is small. Let M be the set of all those edges in E that do not intersect with other edges in E . For sufficiently large C2 and CT such that C2</p><p>CT, the size of M is close to its expectation via the Chernoff bound, and in particular |M | ≥ 9  10 p|Ei| with probability 999/1000. Since the statistical distance between S and S is at most 1/1000, |M | ≥ 9  10 p|Ei| with probability at least 998/1000, even if E is selected from S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Maintaining the Partition</head><p>We now describe how the partition is maintained. As before the data structure keep a value index[v] for each vertex v. This time index[v] can only belong to the set {∆, ∆ + 1, ∆ + 2, . . . , kα -1, kα}. The value ∆ corresponds to v remaining in the graph G∆, for which a separate copy of the simple data structure is kept. Additionally, for each vertex v with index[v] &gt; ∆, there is an additional Boolean</p><formula xml:id="formula_22">variable promoted[v]. If v belongs to V index[v] in the par- tition, then promoted[v] = false. Otherwise, if v belongs to V index[v] , then promoted[v] = true. Initially, each j ∈ {∆ + 1, ∆ + 2, . . . , kα -1, kα} is set to light.</formula><p>We wish that all vertices v obey the invariant that there be no j &gt; index <ref type="bibr">[v]</ref> such that the number of neighbors u of v with index[u] ≤ j is at least α j-1 . Furthermore, we require that for v with promoted[v] = false and index[v] &gt; ∆, the number of neighbors u with index[u] ≤ index[v] be greater than α index[v]-2 . Note that the difference from the previous data structures is that some vertices, namely those with promoted[v] = true, do not obey the second invariant.</p><p>As before, some vertices will be marked as dirty in the course of the execution of the algorithm. When an edge is removed or added, we mark its endpoints as dirty and update its structures. Then our algorithm considers consecutive j starting with kα and goes down to ∆ + 1. For a given i &gt; ∆: C2. Finally, for endpoints v of edges in the new Mi that have index[v] &lt; i, we set index[v] = i and promoted[v] = true, and update all the structures for them and their neighbors accordingly, marking some of them as dirty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">If i is light:</head><p>If |Ei| &gt; CT • α i , we make i heavy and construct Mi in the same way as for heavy i.</p><p>Otherwise, if |Ei| ≤ CT • α i , and we do nothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Complexity Analysis</head><p>We use almost the same potential functions for vertices as before. The only difference is that we multiply all potentials by a constant factor so that when an edge is inserted into some set Ei or removed from it, we can pass one unit of potential to a special fund. We will use this fund to pay for the cost of recomputing matchings Mi. We will show that with large probability the fund deficit is small.</p><p>Whenever the data structure artificially moves a vertex v, creating or destroying some V i , one has to cover the cost associated with changing index <ref type="bibr">[v]</ref>. In the deterministic data structure the cost of moving vertices around was covered by the collected potential. Here, we have to find another source of funding.</p><p>Suppose that we want to create a matching Mi for a heavy i. We run the algorithm of Lemma 5, charging its running time to the fund. If Mi is sufficiently large, we spend O(|Vi|α i ) units of potential the fund on moving vertices in V from other buckets (where the constant hidden in the big O notation is very small). Later, moving vertices in V back to their buckets will cost approximately the same, so we can assume that we charge this cost to the fund in advance. If Mi that has been generated is too small and the data structure has to rerun the process generating Mi, we say that we lose.</p><p>The matching Mi is relatively large compared to |Vi ∪ V i | right after we create it. The matching requires recomputation if it becomes relatively small compared to |Vi ∪ V i |. For this to happen, at least one of the following two must be the case:</p><p>• A constant fraction of edges in Mi have been removed from Ei.</p><p>• The number of vertices in Vi must have grown by a constant factor.</p><p>Consider first the latter case. Since each new vertex in Vi contributes to the special fund at least α i-1 units of potential, it is easy to set constants so that we can afford to pay for moving vertices in V i (if their number is sufficiently small, which is the case if C2 is large) and for the initial execution of the algorithm of Lemma 5, which requires only Θ(|Vi|α i ) units of potential. We can also set the constants in the data structure such that in fact, we are left with a surplus of potential. We make sure that we collect a lot of potential. We say that we win in this case. The former case requires probabilistic analysis, which we now describe. Recall that a well chosen Mi is at least a 4/5fraction of a subset E of edges Ei. Therefore, to delete at least half the edges of the initial Mi, one has to delete at least a 2/5-fraction of E . We claim that with probability at least 3/4, one has to delete at least a 1/100-fraction of Ei in order to delete a 2/5-fraction of E . We now sketch a proof of this claim.</p><p>Suppose to the contrary that one can delete at least a 2/5-fraction of E by deleting at most a 1/100-fraction of Ei with some probability greater than 1/4. Then, given how E is selected in Lemma 5, one can delete at least |Ei|/(5C2α i ) edges of E with probability at least 1/4 -1/100 (for well chosen constants) by selecting a subset of Ei of size |Ei|/100, where E is created by independently selecting each edge of Ei with probability 1/(C2α i ). Expressing the last sentence in a slightly different way, the sum of |Ei|/100 independent variables Xj is at least |Ei|/(5C2α i ) with probability at least 24/100, where each Xj is 1 with probability 1/(C2α i ), and 0, otherwise. Using the Chernoff bound, one can show that this is not the case for well chosen constants.</p><p>The claim implies that with probability at least 3/4, we collect a lot of potential before a large fraction of Mi gets deleted, and also in this case, we say that we win. Otherwise, when the edges are deleted very quickly, we say that we lose.</p><p>Summarizing, we win with probability at least 2/3, in which case we collect a lot of potential (a large constant times the invested potential), which goes to the fund. We lose with probability at most 1/3, and in this case, the operation is paid by the fund. Consider a logarithmic number of ranges [2 j , 2 j+1 ) corresponding to different sizes of |Ei| that may appear in the data structure. For a given range, we can assume that whenever we play, we lose at most C • 2 j , for some constant C, and we win at least 10 • C • 2 j . We initially provide the fund with enough potential to pay for the first t def = C • log log n δ times we play the game, for every j of interest, where C is a sufficiently large constant. Then, the probability that we ever spend more than we gain for a given j is bounded by δ 2 log n . To prove this, it suffices to give an upper bound pt on the probability that we spend more than win in t games. Using the Chernoff bound, one can show pt's such that pt+1 ≤ pt • c, for t ≥ t , where c is a constant in (0, 1). Using the Chernoff bound again, one can show that if C is sufficiently large, pt ≤ (1 -C) δ 2 log n , and P ∞ t=t pt ≤ δ 2 log n . So by the union bound, the probability that we ever spend more than we gain for any j is bounded by δ.</p><p>Recall that K is the total number of graph operations, which gives a bound on the maximum size of Ei that can appear. The above analysis implies that for every j such that 2 j ≤ n 2 and K ≥ 2 j , we can subsidize the fund with O(2 j • log log n δ ) units of potential to make sure that with probability 1 -δ, the fund's balance is always non-negative. In total, the aid for the fund is bounded by O(min{K, n 2 } • log log n δ ). Therefore, the total potential P spent by the al- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">OPEN PROBLEMS</head><p>The two main questions left open by our paper are:</p><p>• Our approximation factors are large constants. How small can they be made with polylogarithmic update time? Can they be made 2? Can the approximation constant be made smaller than 2 for maximum matching?</p><p>• Is there a deterministic data structure that achieves a constant approximation factor with polylogarithmic update time?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) achieves a constant approximation factor,(b) runs in O " min{K, n 2 } • log n • log 1 δ + K • log 2 n« time for a fixed sequence of K updates with probability 1 -δ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>vertices in V . Each such edge is adjacent to at most Y def = 2(d-1) other such edges. This implies that G has a matching of size at least X/(Y + 1) ≥ |V |/(4γ). By Fact 1, |V | ≤ 4γ • VC(G).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 .</head><label>3</label><figDesc>It is easy to implement operations Insert and Delete that are responsible for inserting and removing an edge by using Restabilize. It suffices to modify first the corresponding below[u, •], neighbors[u, •], and lower-neighbors[u] for each of the edge's endpoints u (this can be done in O(log n) time), mark the endpoints as dirty, and run Restabilize to fix the partition of vertices if necessary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 4 .</head><label>4</label><figDesc>The amortized complexity of the operations Insert and Delete in the deterministic data structure is O(log 2 n) for α = 4. Proof. The use the following potential function. The potential of a vertex v equals Φ(v) def = Φ1(v) + Φ2(v), 1 One could immediately decrease index[v] to the right value, but it is easier to analyze the complexity of this version of the procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>«Corollary 6 .</head><label>6</label><figDesc>with probability 1-δ. Recall that each unit of potential corresponds to O(log n) computation. Other operations, which include computing M light and combining the three matchings, do not take more than O(log 2 n) time per update to the graph. Summarizing, we prove the following claim. For any sequence of K updates, the randomized data structure runs inO " min{K, n 2 } • log n • log 1 δ + K • log 2 n« time with probability 1 -δ, where δ ∈ (0, 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Therefore, the procedure marks all of them as dirty.Next, the procedure updates neighbors[u], below[u, •], and lower-neighbors[u] for all neighbors u of v in buckets V0 through Vt. Then the procedure updates the structures for v as well. Note that updating all the structures takes at most O(below[v, t] • log n) time, because this requires at most a constant number of set operations per each of the neighbors in consideration, and for each vertex u, the array below[u, •] can be updated in O(kα) = O(log n) time.</figDesc><table /><note><p><p><p>the procedure sets index</p>[v]  </p>to the highest such i. (This could happen if many edges adjacent to v have been added to the graph, or if many edges have been deleted from v's neighbors that were previously in higher buckets than the i-th, causing them to be demoted to lower buckets.) Let t and t be the new and old value of index[v], respectively. The move of v from Vt to Vt may invalidate the invariant for neighbors of v in buckets Vt to Vt -1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>the procedure decreases index[v] by one. (This could happen if many edges adjacent to v have been deleted, or if many edges are added to v's neighbors that were previously in lower buckets than v, causing them to jump to higher buckets.</figDesc><table /><note><p><p><p><p><p>)</p>Let t be the new value of index</p>[v]</p>. The move of v can affect the invariant for neighbors of v in sets V0 through Vt-1, so the procedure marks all of them as dirty.</p>The procedure also updates neighbors[u], below[u, •], and lower-neighbors[u] for all neighbors u of v in buckets V0 to Vt+1. Next it does the same for the structures of v. In total, this takes O(below[v, t + 1] • log n) time, since at most a constant number of set operations per each of the neighbors in consideration is necessary, and for each of them below[•, •] can be updated in O(kα) = O(log n) time.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>1 .</head><label>1</label><figDesc>The algorithm checks if there are dirty vertices v with index[v] &lt; i such that the number of neighbors u with index[u] ≤ i is at least α i-1 . Those vertices have index[v] set to i, and the algorithms updates structures below[v, •], lower-neighbors[v], and neighbors[v, •] for them and for their neighbors u with index[u] ≤ i accordingly, as we did for the deterministic data structures. Furthermore, if there are vertices v with index[v] = i and promoted[v] = true with the same property of the number of neighbors, the algorithm sets promoted[v] = false for them. Finally, if there are vertices v with index[v] = i, promoted[v] = false, and the number of neighbors u with index[u] ≤ i is at most α i-2 , the algorithm sets index[v] = i -1 and updates all the structures accordingly, also marking specific neighbors as dirty whenever necessary.The algorithm checks if the old matching is large enough, and if not, the algorithm deletes it, and computes a new matching. More specifically, if it is no longer the case that |Mi| ≥ |Vi ∪ V i |/C1, the algorithm goes over all v with index[v] = i and promoted[v] = true. For each such v, we set promoted[v] to false. Moreover, for those v with the number of neighbors u with index[u] ≤ i at most α i-2 , we set index[v] = i -1 and mark them as dirty.If now |Ei| is at most CT • α i , we make i light. Otherwise, we use the procedure of Lemma 5 to select a new matching Mi. We set V i to the set of vertices matched by Mi that do not belong to Vi. As long as |Mi| &lt;9  10 |Ei|/(C2α i ), we keep repeating the procedure of Lemma 5 until we succeed. For well chosen constants,9  10 |Ei|/(C2α i ) ≥ |Vi ∪ V i |/C1, since C1</figDesc><table><row><cell>2. If i is heavy:</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Supported by NSF grants 0732334 and 0728645. † Supported by NSF grants 0732334 and 0728645, Marie Curie Reintegration grant PIRG03-GA-2008-231077, and the Israel Science Foundation grant nos. 1147/09 and 1675/09.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>The MIT Press and McGraw-Hill Book Company</publisher>
		</imprint>
	</monogr>
	<note>Introduction to Algorithms, Second Edition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Dynamic graph algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eppstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Galil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Italiano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sparsification-a technique for speeding up dynamic graph algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eppstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Galil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Italiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nissenzweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="669" to="696" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved approximation guarantees for weighted matching in the semi-streaming model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Segev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STACS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On graph problems in a semi-streaming model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feigenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="207" to="216" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Randomized fully dynamic graph algorithms with polylogarithmic time per operation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="502" to="516" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fully dynamic maintenance of vertex cover</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ivković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WG</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="99" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A fully dynamic approximation scheme for shortest paths in planar graphs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="249" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distributed approximate matching</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lotker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Patt-Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODC</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Finding graph matchings in data streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcgregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">APPROX-RANDOM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="170" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An O( p |V | • |E|) algorithm for finding maximum matching in general graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Micali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Vazirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="17" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximating the minimum vertex cover in sublinear time and a connection to distributed algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Parnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="183" to="196" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Faster dynamic matchings and vertex connectivity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sankowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully-dynamic min-cut</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thorup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="224" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Worst-case update times for fully-dynamic all-pairs shortest paths</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thorup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Weighted matching in the semi-streaming model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zelke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STACS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="669" to="680" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
