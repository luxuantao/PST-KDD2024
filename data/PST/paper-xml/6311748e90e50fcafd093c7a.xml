<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Pretraining of Graph Neural Networks for the Retrieval of Related Mathematical Expressions in Scientific Articles</title>
				<funder ref="#_g3dxjxt">
					<orgName type="full">Federal Ministry of Education and Research of Germany</orgName>
				</funder>
				<funder ref="#_g7rf9xV">
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-08-22">22 Aug 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lukas</forename><surname>Pfahler</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Katharina</forename><surname>Morik</surname></persName>
						</author>
						<title level="a" type="main">Self-Supervised Pretraining of Graph Neural Networks for the Retrieval of Related Mathematical Expressions in Scientific Articles</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-08-22">22 Aug 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2209.00446v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given the increase of publications, search for relevant papers becomes tedious. In particular, search across disciplines or schools of thinking is not supported. This is mainly due to the retrieval with keyword queries: technical terms differ in different sciences or at different times. Relevant articles might better be identified by their mathematical problem descriptions. Just looking at the equations in a paper already gives a hint to whether the paper is relevant. Hence, we propose a new approach for retrieval of mathematical expressions based on machine learning. We design an unsupervised representation learning task that combines embedding learning with self-supervised learning. Using graph convolutional neural networks we embed mathematical expression into low-dimensional vector spaces that allow efficient nearest neighbor queries. To train our models, we collect a huge dataset with over 29 million mathematical expressions from over 900,000 publications published on arXiv.org. The math is converted into an XML format, which we view as graph data. Our empirical evaluations involving a new dataset of manually annotated search queries show the benefits of using embedding models for mathematical retrieval.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine learning has contributed to many success stories of search engines. Unfortunately, the search is most often based on words or text. Technical terms in different disciplines, however, may have different meanings or the same meaning may be referred to by different terms. For instance, various usages of the Bayes' law occur in different scientific fields and can be found under different titles. For instance in astrophysics, it is known as information field theory <ref type="bibr" target="#b9">(En?lin et al., 2009)</ref>. Without knowing physics and even if the name Bayes were not mentioned, it is easily recognized by the formula P (d|s) = P (d, s)/P (s) in the paper. Another example is Ising's paper in a physics journal from 1925 under the title Ferromagnetismus. Today, the Ising model is also popular in machine learning, but is referred to first as Hopfield network and later as Boltzmann machine. This illustrates the aspect of time: words for particular topics change over time. The language of Ising's paper is German, the paper introducing Jensen's inequality in 1906 is written in French. Again, the inequality f ((a + b)/2) ? f (a)/2 + f (b)/2 can be easily understood, anyhow. We conclude that the most compact and comprehensive way to transport the main ideas of scientific manuscripts in disciplines like computer science or physics are the equations used. Thus it should also be the way we formulate our search queries when searching for scientific manuscripts. In order to judge the relevance of mathematical expressions for a search query, a system has to generalize between different notations and match the parts of equations, that describe the same concepts, even if they appear in a different form. A human reader resorts to domain knowledge acquired over years of training in his field to judge the relevance. We wonder how machine learning models with access to vast amounts of mathematical content can help automatize this process.</p><p>In this work, we propose to use graph convolutional neural networks to learn a representation of mathematical expressions that captures semantic relatedness. To this end, we design two unsupervised learning tasks, one classic embedding learning task based on contextual similarity and one self-supervised learning task inspired by masked-language models. We curate a dataset of over 28.9 million equations from over 900,000 papers from arXiv.org and represent the equations as graphs with one-hot encoded features. Then we train our models on this large collection of equations. We compile an evaluation dataset with annotated search queries from several different disciplines and showcase the usefulness of our approach for deploying a search engine for mathematical expressions.</p><p>The rest of this paper is structured as follows: We begin by reviewing related work on math search and on our machine learning approaches. In Section 3 we describe the dataset of papers and equations sourced from arXiv.org for our study and present our pre-processing choices. Then we present the graph convolutional neural network we use for embedding equations and describe our two unsupervised learning tasks in Section 4. We begin a statistical analysis of our problem in Section 5 before presenting an extensive empirical validation in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Math Search and KDD</head><p>Mining and indexing mathematical expressions in document collections is a challenging task, mostly tackled in the information retrieval community <ref type="bibr" target="#b12">(Guidi &amp; Coen, 2016;</ref><ref type="bibr" target="#b35">Zhong &amp; Zanibbi, 2019)</ref>. We outline how the problem of math search is treated with the tools from knowledge discovery and data mining and present related work on the machine learning methods we chose for our approach.</p><p>Representation The first question we have to consider is how to represent mathematical expression. Choices can be divided into two categories: those for visually representing and those for semantically representing math. The former category is focused on the layout of an expression. The most prominent choices are LaTex, a Turing-complete language used in the publications on arXiv.org, as well as Presentation MathML 1 , an XML dialect for displaying math on the web that we chose in this work. The latter category includes Content MathML and OpenMath, two similar XML dialects that focus on semantic rather than layout, but also domain-specific languages for symbolic math solvers like Mathematica, that also allow to manipulate and transform formulas. To the best of our knowledge, no large, public collection of semantic math expressions exists and, unfortunately, converting math from a displayrepresentation, where data is available in large quantities, to a semantic representation which seems more appropriate for searching, is a non-trivial task. Available solutions either use rules and heuristics, e.g. the converter ml2om that translates LaTeX to OpenMath <ref type="bibr">(Timoney, 1999)</ref>, or also apply machine learning <ref type="bibr" target="#b31">(Wang et al., 2018)</ref>. We chose to apply machine learning methods directly on the Presentation MathML representation. The bottom line of the representation question is that math is expressed in trees, either XML or other parse trees. Our previous work <ref type="bibr" target="#b26">(Pfahler et al., 2019)</ref> may be the notable exception to this: We chose to represent equations as fixed-size bitmaps. While one could argue that this is an unsuitable choice, the multitude of machine-learning or computer-vision approaches 1 https://www.w3.org/TR/MathML3/ that successfully transform images of typeset <ref type="bibr" target="#b5">(Deng et al., 2017)</ref> or hand-written <ref type="bibr" target="#b0">( ?lvaro et al., 2014;</ref><ref type="bibr" target="#b18">Mahdavi et al., 2019)</ref> math back to tree-based representations suggests that bitmap representations preserve all required information of tree-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarity Measure</head><p>The second question is how we compute similarity between formulas. Zanibbi et al. distinguish text-based, tree-based and spectral approaches <ref type="bibr">(Zanibbi et al., 2016a)</ref>. Text-based approaches transform tree-structured math into a sequence, for instance by preorder traversal, and then estimate the similarity using methods for sequences like cosine similarities of bags-of-words or the length of the largest common substring. Tree-based approaches focus on matching trees or subtrees. Typically computing similarities involves solving dynamicprogramming problems. Spectral approaches work on paths or partial subtrees in the trees. An example is ap-proach0 <ref type="bibr" target="#b35">(Zhong &amp; Zanibbi, 2019)</ref>, that indexes root-leaf paths of operator trees. From matches of the root-leaf paths, they compute the largest common subexpression to score the similarity of two equations. To convert math from La-TeX to the semantic representation of operator trees, the authors use ca. 100 handwritten grammar rules.</p><p>A new trend is to use machine learning to learn a similarity measure. A machine learning model maps an equation to a dense, low-dimensional vector. The similarity between these so-called embeddings can be computed via their inner product, which enables fast indexing using a variety of index structures, including faiss and annoy, designed for efficiently handling millions of these dense, lowdimensional vectors. <ref type="bibr" target="#b19">Mansouri et al. (2019)</ref> propose to embed equations using fastText, a method originally designed for computing word embeddings, while in our previous work <ref type="bibr" target="#b26">(Pfahler et al., 2019)</ref> we compute embeddings with a similar embedding learning task and convolutional neural networks (see Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Convolutional Neural Networks In this work</head><p>we propose an embedding model based on graph convolutional neural networks. Like classic convolutional neural networks for image processing, they compute feature maps based on local neighborhoods. While in CNNs, we have features associated with each pixel in the pixel grid and neighborhoods are defined by this grid, in graph CNNs we have features associated with each node of the graph and neighborhoods are defined by the edges in the graph. We define graph structures x = (X, E) as a tuple of nodefeatures X and edges E. Let |x| denote the number of nodes in x. We assume that X ? R |x|?d where X i are the features of the i-th node. A graph CNN maps an input graph to an output with transformed feature vectors in a d ?dimensional output space but with identical edge structure.</p><p>It is defined by composing different layers. Borrowing the notation of <ref type="bibr" target="#b23">Morris et al. (2019)</ref>, an abstract graph network layer is defined by its output x ? i for the i-th node</p><formula xml:id="formula_0">x ? i = ? x i , j?NE (i) ? (x i , x j , e ij )</formula><p>where ?, ? are (sub-)differentiable operators such as linear transformations or multi-layer perceptrons, denotes a sub-differentiable, permutation invariant function like sum, mean or max and N E (i) denotes the set of all neighboring nodes of i in the graph with edges E. We optionally use information about the edges in the form of vectorial edge-features e ij . As long as all layers in a graph neural network are (sub-)differentiable operations, we can train the network via backpropagation. Efficient software libraries for training models with GPU-support are available, e.g. we use torch-geometric <ref type="bibr">(Fey &amp; Lenssen, 2019)</ref>. Graph CNNs have been applied in many contexts, for instance for classifying molecules <ref type="bibr" target="#b8">(Duvenaud et al., 2015)</ref> or classification and segmentation of point-clouds <ref type="bibr" target="#b32">(Wang et al., 2019)</ref>. In this work we apply them to learn similarities between mathematical expressions, where we view an XMLrepresentation as graph-structured data.</p><p>Self-Supervised Learning We further draw influence from a recently proposed class of representation learning tasks called self-supervised learning. Self-supervised learning tasks are unsupervised learning tasks, where parts of the inputs are used to construct proxy tasks. The representations learned in these proxy-tasks can then be used in downstream tasks. For instance, we can rotate images and train a model to predict the rotation angle, as proposed by <ref type="bibr" target="#b11">Gidaris et al. (2018)</ref>. Using massive amounts of unlabeled data readily available, we can fit models that solve a task like this.</p><p>We are particularly interested in masking tasks, where parts of the input are hidden from a model and the model's task is to predict the hidden parts. This was made popular by the BERT model for pretraining natural language representations <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> and has since then been adopted to other inputs, for instance as pretraining for image classification with convolutional neural networks <ref type="bibr" target="#b27">(Trinh et al., 2019)</ref>. We construct a masking task for mathematical expressions and use graph convolutional neural networks to predict the masked parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Data</head><p>We outline how we gather data from arxiv.org and transform them to graph structured data for our graph convolutional neural network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We are working on data obtained from arxiv.org, a service where scientists can upload their manuscripts or pre-prints without reviewing process. We have downloaded all the LaTeX sources of publications up to April 2019 from the official bulk data repositories<ref type="foot" target="#foot_1">2</ref> . This way we have obtained 934,287 papers. As we can see in Figure <ref type="figure">1</ref>, the large majority of these papers are from disciplines where mathematical expressions are an important part of publications. The most prominent subject areas are astrophysics, condensedmatter physics, computer science, mathematics, and high energy physics.</p><p>From all publications, we extract mathematical expressions by using regular expressions for the most common mathenvironments like 'equation', 'align', etc. We do not use inline math snippets but focus on expressions that stand on their own, as they tend to describe more important concepts. Furthermore we extract user-defined commands and macros. Using the library Katex<ref type="foot" target="#foot_2">3</ref> we compile the raw LaTeX-equations to the XML-based MathML format. Out of all papers downloaded, 760,041 papers contain at least one equation that we were able to convert to MathML. In total we have a dataset of 28,973,591 MathML equations. Furthermore we have used regular expressions to find arXiv-ids in the bibliographies of the paper to build a citation graph. In total, 540,892 papers have an outgoing edge, with a total number of edges of 4,553,297. Since we only detect those references that use an arXiv-id, for instance in an url, our citation graph is only a subgraph of the true citation graph.</p><p>To ensure reproducibility we provide the scripts used for processing the public arXiv data dump, extracting the mathematical expressions and converting them to MathML as well as collecting meta-data and citations at https://github.com/Whadup/arxiv_library. We also share our citation graph, which might be interesting in other applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data-Representation</head><p>In order to feed the MathML to a graph convolutional neural network, we have to convert it to a graph with vectorial node features. The MathML standard defines around 30 different XML-tags like &lt;mi&gt; for math identifiers or &lt;mo&gt; for math operators. Some of these tags use attributes, for instance to change font or spacing. Leaf nodes contain text like numbers, parenthesis or letters (greek, latin, etc...).</p><p>We view the XML-structure as a tree and use its nodes and edges and derive features based on tags, attributes and text.</p><p>For each node we use one-hot encoded feature vectors of dimensionality 256. The first 32 features are used to encode the type of the XML-tag. The next 32 features are used to encode optional attributes, most commonly changes of the font to bold or calligraphy fonts. The remaining 192 dimensions are used to encode the most frequent characters used in leaf-nodes. In Figure <ref type="figure" target="#fig_0">2</ref> we see that the most frequent characters are opening and closing parenthesis, followed by a variety of numbers, latin or greek letters and mathematical operators. For both attribute and character features, we introduce special unknown symbols for all rare attributes/characters. In addition to the one-hot encoded features, we store the position of the node with respect to the parent node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning to Find Related Equations</head><p>In this section we will introduce the graph convolutional neural network used for computing embeddings and present two unsupervised learning tasks used for training the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Graph-Convolutional Model for Equations</head><p>We define a graph convolutional neural network for the task of embedding mathematical expressions into a lowdimensional vector space. The raw MathML is converted to graphs with vectorial features as described in Section 3.2. We propose to use a special first layer that combines the one-hot encoded information at a node with the decimal position attribute. Following <ref type="bibr" target="#b30">Vaswani et al. (2017)</ref>, we encode the position of the i-th node p i ? N using po-sitional embeddings. We use fixed sinusoid embeddings <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> denoted by E(p j ), but to still allow the model to control the influence of the positional embeddings, we introduce a learnable scaling coefficient ? initialized to 1.</p><p>x</p><formula xml:id="formula_1">(1) i = max ? ? 0, j?N (i)?i W (1) x j + ?E(p j ) + b (1) ? ?</formula><p>The first layer is followed by 3 fully-connected graph convolution layers of width 512, where the l-th layer is defined by</p><formula xml:id="formula_2">x (l) i = max ? ? 0, j?N (i)?i W (l) x (l-1) j + b (l) ? ?</formula><p>which linearly transforms all nodes using a weight matrix W (l) , adds a bias term b (l) , aggregates by computing the sum over all neighborhoods and applies the ReLU activation component-wise. All graph convolution layers output feature maps with 512 dimensions. We apply batchnormalization before the first and third graph convolution layer. For the remainder of this paper, let ?(x) ? R |x|?512 denote the output of the last graph convolution layer given the input x. To obtain a single embedding for an input graph, we compute the mean of all node features. This mean is transformed in another linear layer to reduce the dimensionality to 64. For the remainder of this paper, let ?(x) ? R 64 denote this embedding of x.</p><p>When scoring similarities between embeddings with margin losses, we need to control the norm of the embeddings, otherwise the notion of adherence to a margin becomes meaningless. Among others, <ref type="bibr" target="#b7">Ding et al. (2015)</ref> propose to normalize all embeddings to unit length. We propose a softer normalization inspired by batch normalization <ref type="bibr" target="#b14">(Ioffe &amp; Szegedy, 2015)</ref> that also allows to obtain embeddings with norms smaller than 1. For every training batch of graphs, we compute the mean of the norm as well as its standard deviation. Then we inversely scale each embedding by the mean plus the standard deviation. This way, most embeddings have norm smaller than 1. We keep a running average of the means and standard deviations. At inference time, we use these running averages for scaling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Representation Learning Tasks</head><p>We propose to train our embeddings using two selfsupervised learning tasks simultaneously by adding their respective losses.</p><p>Contextual Similarity For learning relations between equations, we rely on the established contextual similarity task that was first made popular by word embeddings <ref type="bibr" target="#b21">(Mikolov et al., 2013)</ref> and has hence been used in many representation learning approaches, including our approach <ref type="bibr" target="#b26">(Pfahler et al., 2019)</ref> for learning similarities between equations. The main idea is that objects that frequently appear in shared contexts are related. We define the context of mathematical expressions as the paper containing the equation and conjecture that two equations are related if they appear in the same paper, as originally proposed in <ref type="bibr" target="#b26">(Pfahler et al., 2019)</ref>. We extend this approach and further define two equations as related if one paper references the other using a citation graph. This way we hope to connect equations that describe the same context but use different notation. In addition, we discriminate between sampling expressions from the same paper and from the same section.</p><p>We hope that within sections, equations are more related to each other. For obtaining positive examples of related equations, we 1. sample a paper uniformly at random and select an expression from this paper uniformly at random.</p><p>2. randomly select whether we sample from the same section, same paper or along a citation, 3. sample a positive example using that method. When we cannot find a positive example using that method, we jump back to (1).</p><p>For learning similarities we also require negative examples.</p><p>To obtain these, we sample a paper uniformly at random and select an expression from this paper uniformly at random. The random process that generates these weak labels for similarity learning introduces a lot of noise, as many equations we claim are related are unrelated and some of the pairs we say are unrelated are related. We leave the investigation of more advanced sampling schemes to future work.</p><p>Using the sampled equations x with positive x + and negative partners x -, we apply similarity learning. We have to choose a suitable loss function and investigate two different losses: Triplet and Histogram. The triplet loss <ref type="bibr" target="#b1">(Balntas et al., 2016)</ref> we have previously used <ref type="bibr" target="#b26">(Pfahler et al., 2019)</ref>, contrasts the similarity between a positive pair of examples and a negative pair of examples and demands that the similar pair has a higher similarity by a user-defined margin ?, usually set to 1.</p><formula xml:id="formula_3">? t (x, x + , x -) = max(0, ? -?(x), ?(x + ) + ?(x), ?(x -) )<label>(1)</label></formula><p>In this paper, we propose to use the histogram loss as proposed by <ref type="bibr" target="#b28">Ustinova and Lempitsky (2016)</ref>. It does not work on a triplet of equations, but on a mini-batch of size m positive pairs X + and a batch of negative pairs X -with respect to anchor examples X. We collect all similarities between positive pairs in a vector s + = ( ?(x i ), ?(x + i ) ) i=1,...,m and of all negative pairs in s -. We divide the interval [-1, 1] into R-1 equally-sized bins with boundaries -1 = t 1 , t 2 , ..., t R = 1 and width ? = 2/(R -1) and build histograms for the positive similarities and the negative similarities. Now we demand that the positive histogram leans more toward the +1 similarity than the negative histogram. We formalize this intuition as</p><formula xml:id="formula_4">? h (s + , s -) = 1 m 2 R r=1 r r ? =1 m i=1 ? r [s - i ] m i=1 ? r ? [s + i ]</formula><p>where instead of hard assignments, we use the triangular kernel</p><formula xml:id="formula_5">? r [s] = ? ? ? ? ? (s -t r-1 )/? if s ? [t r-1 , t r ] (t r-1 -s)/? if s ? [t r , t r+1 ] 0 otherwise</formula><p>to put similarities into bins. This way we obtain a differentiable loss function. We hope that histogram loss is more robust with regard to the massive noise in our labels as each positive example is contrasted with all negative examples.</p><p>Masking Task We propose to extend the contextual similarity task by another tasks and optimize the sum of both tasks for training our embedding models. The main idea of our second task is, that the symbols in mathematical expressions do not appear independent from each other, but have strong dependencies. Thus if we hide a fraction of the symbols in an equation, we should be able to approximately reconstruct the hidden symbols from the remaining symbols. This task is reminiscent of masked language modeling tasks made popular by BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> for natural language processing. In order to successfully solve this task, a model has to learn about the frequencies of symbols and their dependencies from the data, as is illustrated in Figure <ref type="figure" target="#fig_1">3</ref>.</p><p>More formally, we proceed as follows: For each input graph x with features X, we randomly set the feature vector of 15% of the nodes to all zero obtaining the graph x . Then we compute ?(x ) ? R |x|?512 . Now for each masked node, we want to solve three separate classification tasks: Given ? i (x ), predict the right XML-tag, predict the right XML-attributes (or no-attribute if no attributes where used) and predict the right character (or no-character if no character was used). We solve these tasks using a single linear layer of dimensionality 32,32+1 and 192+1 respectively with soft-max activation and compute the cross-  attr) , X i,33:64 )</p><formula xml:id="formula_6">? tag,i = ?(softmax(W (tag) ? i (x ) + b (tag) , X i,1:32 ) ? attr,i = ?(softmax(W (attr) ? i (x ) + b (</formula><formula xml:id="formula_7">? char,i = ?(softmax(W (char) ? i (x ) + b (char) , X i,65:256 )</formula><p>The overall loss of the masking task is defined as the mean of all three classification losses ? mask,i = 1 3 (? tag,i + ? attr,i + ? char,i ). The loss is only evaluated for the masked tokens and we compute the mean over all masked tokens to obtain a loss value for x .</p><p>Adding this task to the contextual similarity task has the additional advantage that we now learn a representation that not only captures context information, but also preserves information about the raw input symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data-Augmentation</head><p>Data augmentation eases the generalization of machine learning models and is particularly popular for image classification tasks where we can augment images by randomly rotating, scaling, padding, etc. For mathematical expressions, we propose the following random data augmentation: Since we know that a renaming of symbols in equations rarely changes the semantic, we propose to randomly permute the character features of all nodes that correspond to a math identifier, encoded in &lt;mi&gt; tags according to the MathML standard. For each equation we process, we sample a number of flips from a Poisson distribution with expected value 32. Then starting with the identity permutation that does not change the order of our 192 features, we construct a permutation with the desired number of flips by incrementally exchanging two random characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Hyperparameter Choices</head><p>We train all our models for 20 epochs with Adam optimization, batch size of 128, an initial learning rate of 0.0001 that we decrease linearly. We use R = 64 bins for the histogram loss and margin ? = 1 for the triplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Statistical Analysis</head><p>Before we present empirical results on our embedding approach, we want to discuss their statistical significance using concentration inequalities. We begin by discussing the assumptions we use for our discussion that go beyond the usual iid. assumptions. Then we talk about testing of embedding models on hold-out data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Assumptions</head><p>In usual classification settings <ref type="bibr" target="#b29">(Vapnik, 2000)</ref>, as well as metric learning settings <ref type="bibr" target="#b4">(Cl?menc ?on et al., 2016;</ref><ref type="bibr" target="#b20">Maurer, 2008)</ref>, we assume that we have access to independent and identically distributed training data. In our case of embedding learning for mathematical expressions, we would require to a sample of independent equations, or to be more precise, independent pairs of equations with (weak) similarity labels. However, this assumption surely does not hold, as two equations that appear in the same paper do not appear independently from each other. An example illustrates this: Let X 1 and X 2 denote two equations. If it is revealed to you that X 1 shows Heisenberg's uncertainty principle X 1 = {? x ? p ? /2}, and that X 2 appears in the same paper, the probability of X 2 being related to quantum mechanics increases. This illustrates that P (X 2 = x 2 | X 1 = x 1 ) = P (X 2 = x 2 ) and thus X 1 and X 2 are not independent. For the purpose of our analysis, we assume that (A1) we have an iid. sample of N papers where the i-th paper contains n i equations (A2) the number of equations is n = N i=1 n i (A3) equations in the i-th paper are independent of equations in all other papers.</p><p>Surely papers do not appear independently of one another, but this we will ignore in our statistical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Confidence Intervals for Hold-Out Data</head><p>Now we carry out our statistical analysis for measuring scores on hold-out data where the model ? is fixed and independent of the hold-out data. In the common iid setting, we can apply concentration inequalities like Hoeffding's inequality to bound the gap between performance evaluations like accuracy or loss measured on a finite hold-out sample and the true expected value.</p><p>Since we do not have iid. data, we have to use a refined approach. We use U -statistics <ref type="bibr" target="#b13">(Hoeffding, 1948)</ref> s to analyze our models. In our case, they are defined as expectations over functions of triples of equations V (x 1 , x 2 , x 3 ) called kernel.</p><formula xml:id="formula_8">s = E x1,x2,x3 V (x 1 , x 2 , x 3 ).</formula><p>For instance we can define the ranking score, i.e. the fraction of triplets where the positive pair has a higher similarity than the negative pair. For convenience, we define P (x) as the set of possible positive examples for an expression x.</p><p>Then we can write the ranking score as</p><formula xml:id="formula_9">V ranking (x 1 , x 2 , x 3 ) = ?(x 2 ? P (x 1 ) ? x 3 ? P (x 1 )) ? ?( ?(x 1 ), ?(x 2 ) &gt; ?(x 1 ), ?(x 3 ) )</formula><p>The histogram loss can also be expressed as a U -statistic:</p><formula xml:id="formula_10">V hist (x 1 , x 2 , x 3 ) = ?(x 2 ? P (x 1 ) ? x 3 ? P (x 1 )) ? R r=1 r r ? =1 ? r ? ( ?(x 1 ), ?(x 2 ) )? r ( ?(x 1 ), ?(x 3 ) )</formula><p>Note that whenever (x 1 , x 2 , x 3 ) is not a triple with a positive and a negative example, the triple contributes zero to the expectations. To simplify the analysis, we first consider so-called complete U -statistics, i.e. we estimate the true score using all possible triplets from a finite set of n expressions</p><formula xml:id="formula_11">{x i | i = 1, ..., n} s = n 3 -1 i,j,k V (x i , x j , x k ).<label>(2)</label></formula><p>Now the goal is to bound the difference between s and ? in high probability.</p><p>Theorem 1. If V (x 1 , x 2 , x 3 ) ? [0, 1], then for ? ? (0, 1) with probability at least 1? we have</p><formula xml:id="formula_12">s ? ? + 3 ln(1/?) 2N</formula><p>Proof. We proof this using Janson's concentration inequality for sums of partly dependent variables <ref type="bibr" target="#b16">(Janson, 2003)</ref>.</p><p>In Equation (2), we compute a sum over triplets, where the triplets are constructed from dependent samples. Thus the summands have dependencies. We construct a dependency graph, where each node corresponds to a triplet and two nodes have an edge if they are dependent. In our case two triplets are connected if one of the equations in the first triplet is from the same paper as any equation in the second triplet. In order to apply <ref type="bibr" target="#b16">(Janson, 2003)</ref>, Theorem 2.1, we have to bound the chromatic number ? * of this dependency graph. To this end, we consider an arbitrary node v in the graph with equations from paper i, j and k. Its degree is bounded by the number of equations in the papers deg</p><formula xml:id="formula_13">(v) ? (n i + n j + n k ) n-1 2 , hence ? * ? 3 n -1 2 max i n i .<label>(3)</label></formula><p>Then for t &gt; 0 we have</p><formula xml:id="formula_14">P(s -s ? t) (Janson, 2003) ? exp -2t 2 n 3 ? * (4) (3) ? exp -2t 2 n 9 max i n i ? exp -2t 2 N 9</formula><p>Solving for t yields the desired confidence interval.</p><p>For incomplete U -statistics, where we estimate the score by a subset of triplets D sampled independently with replacement(Cl?menc ?on et al., 2016) as</p><formula xml:id="formula_15">s ? = 1 |B| ? (xi,xj,x k )?D V (x i , x j , x k ),</formula><p>the bound (3) no longer holds. But we can compute an empirical bound ? using any greedy graph coloring algorithm. Then Janson's concentration inequality implies that with probability at least 1? s ? s ? + ln(1/?) ? 2|D|</p><p>.</p><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>In this section we perform an experimental evaluation of our embedding model. In particular, we focus on the usecase of a search engine for mathematical expressions. We begin by investigating the effects of the individual components of our model on a small, closed subset of the data.</p><p>Then we investigate the effectiveness of our method on all 29,9 million equations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Analysis on the Machine-Learning-Subset</head><p>We begin our analysis only on arXiv publications where the primary subject classification is machine learning (cs.LG). This is a natural choice, as we have some expertise to judge the quality of our results, a task which we are in no way equipped for across all subject fields.</p><p>Of these 9,936 publications, we sample two subsets, train and test of size 7,949 and 1,987 respectively with a total number of equations of 237,335 and 54,767 respectively.</p><p>In Section 5 we have seen that scores evaluated on hold-out data converge to true empirical scores in O(1/ ? N ) where N is the number of papers in the test set. In this regard, Bitmap CNN original <ref type="bibr" target="#b26">(Pfahler et al., 2019)</ref> 76.2 (-0.3) 71.9 (+14.2) 68.3 (+7.7) Bitmap CNN retrained 70.0 (-6.5) 50.0 (-7.7) 52.9 (-7.7) our test set is appropriately sized. We use the train-set for building our embedding models and use the test-set to investigate generalization properties.</p><p>For training, we sample 1 million triplets (x, x + , x -). Of these triples, 45.9% have a positive pair from the same section, 42.2% from the same paper and 13.9% along an edge in the citation graph. We sample 100k triplets for testing with similarly distributed positive examples.</p><p>We perform an ablation study on our proposed embedding model and compare it to prior work. This section investigates the influence of our design choices. We decided (a) to use the Histogram loss instead of the triplet loss, (b) to also add an masking task, (c) to data augmentation.</p><p>We measure Ranking score, i.e. the fraction of all triples in the training data where same-class pairs of equations have higher similarities than across-class pairs. As we see in Table 1, our evaluations indicate that all of our design choices contribute favorably to the overall performance on hold-out data, as deactivating any component decreases the score. We note that the biggest gain is achieved by switching form triplet-loss to histogram-loss. We believe that this is due to the massive noise in our labels.</p><p>We also compare with the our previous model <ref type="bibr" target="#b26">(Pfahler et al., 2019)</ref> and see that we beat this baseline by a small margin. However this comparison is not entirely fair, as their model was trained on a larger dataset of around 25k papers, probably including some of the papers in our test set. We use their code to re-train on our subset of equations and yield a substantial margin of 6.5 percentage points.</p><p>We also use our previous evaluation data <ref type="bibr" target="#b26">(Pfahler et al., 2019)</ref>. It consists of 103 equations labeled into 13 categories related to machine learning including k-means, LSTMs, empirical risk minimization, etc. Since only bitmaps are available, we transcribe the equations manually. There are three issues with this evaluation set: First, it is too small to produce significant numbers. Second, some equations in the dataset appear in the training data. This is not only the case for our subset, but also for the training data used in <ref type="bibr" target="#b26">(Pfahler et al., 2019)</ref>. Third, many equations  <ref type="bibr" target="#b26">(Pfahler et al., 2019)</ref> 71.9 68.3 within a category are obviously from the same paper, hence we have seen some of the pairs in our training data. Nevertheless we use the evaluation data. Indeed in our use-case of search engines, the crawled equations will always be in the training data and only the user queries will be unseen equations. In a way, we simulate this with the eval data.</p><p>Following the original experimental protocol, we measure the 1-nearest-neighbor accuracy obtained in leave-one-out validation (named Accuracy) as well as the above Ranking score. In Table <ref type="table" target="#tab_1">1</ref>, we again see that our model is only surpassed by the pre-trained model that uses a larger training dataset. This motivates the use of a much larger dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Large-Scale Experiments</head><p>For training on all the papers in our dataset, we sample two different sets of training triplets, one with 5 million triplets and one with 20 million triplets. We train our models on a Nvidia GTX1080 GPU with 8GB memory, which allows us to process mini-batches of 128 triplets or 384 equations.</p><p>During training, we process around 1,300 triplets per second, not counting the time for reading data from hard disk. In total, one of the 20 epochs of training on 20mio triplets takes 6:30h on our system. We use annoy to construct an index for approximate nearest neighbor retrieval. In total, our index uses 13GB of hard disk storage to manage all mathematical expressions in our dataset.</p><p>Before we evaluate our models in a search engine study, we again check the performance on the aforementioned evaluation data. The results in Table <ref type="table" target="#tab_2">2</ref> indicate the power of using large amounts of training data, although it is unclear if using 20mio training triplets is an advantage over using only 5mio. Our large-scale models beat all the models trained on smaller amounts of data. Even though the smaller models were trained on only machine-learning related data, we obtain better scores on the machine learning evaluation data by training on all disciplines.</p><p>Let us now inspect two example search queries. In Figures 4 and 5 we see two examples from the introduction, Bayes law and Ising models, and their respective nearest neighbors under our model trained on 5mio triplets. We see that we can find other definitions of Bayes' law as well as the related law of total probability. When we perform a query for the Ising Model and look at the first 20 results, we find papers where the model is called Boltzmann machine as well as papers that refer to the Ising model. This illustrates the power of querying for mathematical expressions instead of using keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Search Engine Study</head><p>Finally we want to study the usefulness of our embedding approach for a search engine application more systematically. Traditionally, validating search engines using measures like precision or recall, requires relevance scores for each result for each evaluation query. We see that this requires a lot of manual annotation work since we have to manually identify each relevant equation for each query. Unfortunately, we were not able to find available evaluation data. The best fit is the NTCIR-12 task evaluation data <ref type="bibr">(Zanibbi et al., 2016b</ref>) consisting of 37 annotated queries. However is not appropriate for our approach, as most queries are a combination of math as well as keywords. When we ignore the keywords, the remaining query become very generic, for instance x + y, which makes it very unlikely that we accurately find the articles labeled as relevant. In addition, the overall focus of the NTCIR-12 task is recovery of exact matches, whereas our focus is on retrieving related expressions.</p><p>Consequently, we curate and publish our own evaluation data. To reduce the manual annotation labour, we want to apply a heuristic for the relevance judgement. To this end, we have asked our colleagues, many from disciplines other than computer science and data science, to provide us with equations that we should query. For each equation, they provide a set of keywords or key-phrases that should appear in the section around the result. If one of the keywords is present, we count the result as correct. This way we can evaluate our search result without manually checking result lists. If a keyword has more than 10 characters, we also count it, if we find a substring that has a Levenshtein distance less than 2. In total, we have 53 evaluation queries publicly available and editable online 4 .</p><p>4 Crowd-sourced evaluation data can be accessed and edited here:</p><p>Query:</p><formula xml:id="formula_16">P (d | s) = P (d,s) P (s)</formula><p>1st Result: P (s | d) = P (d|s)P (s) We inspect two different information retrieval metrics that do not require to know the number of relevant documents in advance: Precision@k and unnormalized Mean Average Precision. Precision@k is defined as the fraction of relevant documents within the first k results. We report it for lists of 10, 100 and 1000 results and compute its mean over our evaluation queries.</p><formula xml:id="formula_17">P (d)<label>4th</label></formula><p>Unnormalized Mean Average Precision is derived of the standard mean average precision metric. Since we do not now the number of relevant documents in advance, we omit this term, limit the search to a maximum of 1000 results and obtain the following definition uMAP = 1000 k=1 P (k)? k where P (k) is Precision@k and ? k specifies if the k-th result is relevant. Again we compute the mean over all evaluation queries. In comparison to Precision@k, uMAP considers the order of the search results and rewards relevant results early in the result lists.</p><p>For reference, we include retrieval based on a bag-of-words representation. To this end, we use our data representation as in Section 3.2, but compute the sum over all nodes in the graph to obtain a single 256-dimensional vector of the whole tree. We retrieve the nearest neighbors using cosine similarity.</p><p>In Table <ref type="table" target="#tab_5">3</ref>, we see that our approach beats the bag-of-words margin, in particular for larger values of k. We see for Precision@10, the performance between BoW and our embedding model is very similar. This is because for many queries the top-10 results are mostly near-perfect matches which are easily identified even without machine learning. However when we look at more results, we are able to find almost 50% more relevant equations.</p><p>Overall the precision values seem very low. This is in part due to the experiment design where we rely on the annotated keywords. A closer inspection reveals that the queries achieve very different precision values. In  There are examples that achieve more than 90% precision, but many queries have precision lower than 1%. We note that the results are better if keywords are broader. Highly specific queries where the number of relevant documents is low perform poorly. We hope that in the future our collection of evaluation query grows further to allow more informative evaluations. In particular with more queries it may be helpful to split the evaluation data into difficulty levels according to the number of relevant documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Automatic Identification of Equalities</head><p>We have extracted equalities and inequalities in the test-set of our data using regular expressions. Using a simple heuristic, we filter the resulting (in-)equalities, such that left-hand-side (LHS) and right-hand-side (RHS) do not differ in length dramatically, thereby eliminating formulas like definitions, where the LHS is a only single symbol. We derive three different data sets, one with only equalities (LHS and RHS split at "="), one with inequalities (split at &lt; and ?) and one with mixed relations (split at =&lt;&gt;? and ?). This data allows us to use the LHS of the (in-)equalities as query in hopes of retrieving RHS. We make our finetuning-data available at https://whadup.github.io/arxiv_learning/ as well.</p><p>Following other machine-learning-based approaches for mathematical retrieval <ref type="bibr" target="#b19">(Mansouri et al., 2019;</ref><ref type="bibr" target="#b26">Pfahler et al., 2019;</ref><ref type="bibr" target="#b25">Pfahler &amp; Morik, 2020)</ref>, we use our models to encode formulas into a dense vector space and retrieve results using approximate nearest neighbor search <ref type="bibr" target="#b2">(Bernhardsson, 2018)</ref> in this vector space. We finetune our models on half of the available data and test on the remaining half.</p><p>Finetuning Task We propose to use contrastive learning to learn to identify the RHS given the LHS. The learning task in contrastive learning is identifying the right partner for each input in a minibatch of datapoints. Hence the representation learning problem is formulated as a classification problem. Let X l , X r ? R m?d contain the output embeddings of a minibatch of LHSs and RHSs. We normalize each embedding to unit-length and denote the normalized embeddings by Xl and Xr . We use the InfoNCE loss <ref type="bibr">(Oord et al.)</ref>, i.e. the negative log likelihood of softmax probabilities parameterized by the pairwise cosine similarities between the LHs and RHSs:</p><formula xml:id="formula_18">? ? ( Xl , Xr ) = m -1 m i=1 log exp( Xl i , Xr i /? ) j =i exp( Xl i , Xr j /? )<label>(6)</label></formula><p>where ? &gt; 0 is a hyperparameter that controls the temperature of the ouput probability distribution, which we set to 10 -2 . The contrastive learning task is more difficult for larger batchsizes m, as there are more candidate RHSs to choose from and thus the underlying classification problem becomes more difficult. But it has been shown that the utility of the model increases for larger batchsizes <ref type="bibr" target="#b3">(Chen et al., 2020;</ref><ref type="bibr" target="#b22">Misra &amp; van der Maaten, 2020)</ref>, which we also investigate in our application.</p><p>Baseline-Models In addition to our models we include several baseline models:</p><p>? First, we test a simple bag-of-words (BoW) model that is trained on a bag of MathML tree nodes. This model does not use a pre-training phase, but is only tuned on the finetuning data. The BoW model maps the sparse BoW representation to a d-dimensional vectorial embedding though a single matrix multiplication. In comparison to our BERT models, we do not restrict the vocabulary size of the inputs. The representation is Table <ref type="table">5</ref>. Results of the Mathematical Retrieval Experiment. We report recall@K for K ? {1, 10, 100}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Equalities (36864) Relations ( <ref type="formula">40960</ref>) Inequalities (13312) R@1 R@10 R@100 R@1 R@10 R@100 R@1 R@10 R@100  <ref type="bibr" target="#b19">(Mansouri et al., 2019)</ref>, hence we include it in our comparison. We finetune these embeddings by learning a linear mapping into a d-dimensional vector space, d ? {64, 128, 256} using the same contrastive learning task.</p><p>? Third, we also train a BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> transformer model on our mathematical expressions using the same two pretraining tasks: masked token prediction and same-paper prediction. This model has 6 million trainable parameters and uses 8 transformer layers with a hidden dimensionality of 256 and an intermediate dimensionality of 768. We use the same vocabulary as for our graph neural network.</p><p>We begin by training our models and the baseline-models with a minibatch-size of 1024. Then we also investigate the effect of varying the batch-size.</p><p>Results For testing, we compute embeddings for all LHSs and RHSs in the test-data and store them in an index structure. We use annoy <ref type="bibr" target="#b2">(Bernhardsson, 2018)</ref>, an indexing method for approximate nearest-neighbor search based on an ensemble of random projection trees. We use an ensemble of 16 trees with default hyperparameters, but we found that the results were very insensitive to our particular parameter choices.</p><p>Then we query the k-nearest neighbors, k ? {1, 10, 100} for each formula from the test-set and check if the corresponding other side of the (in-)equality is in the result set. This way we can compute recall values to measure the quality of our embeddings.</p><p>As we can see in Table <ref type="table">5</ref>, our Graph-CNN network substantially outperforms both baseline methods -bag-of-words and FastText -but also the transformer model. With one exception, we can report the best recall scores on all 3 subsets of formulas, often by quite substantial margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Outlook</head><p>Finding relevant literature even across disciplines is essential for scientific investigations. The search results should entail stimulating, relevant papers. Very often, a look at the formula in a paper gives a compact description of the problems and solutions discussed in the paper. Hence, the goal is to offer related papers based on the mathematical expressions. This task is different from mathematical information retrieval, but it shares the problem of determining the right representation of mathematical expressions.</p><p>In this paper, we have proposed and evaluated a new method for searching mathematical expressions based on machine learning. The problem is framed as representation learning on graph-structured data. A precise definition of this task with its assumptions is given. Using concentration inequalities for sums of partly dependent variables allowed us to analyze the performance of our embedding models with statistical assumptions that go beyond the usual i.i.d. over-simplifications. Further work could extend this into a PAC style analysis.</p><p>For the first time, we have applied unsupervised embedding learning with graph convolutional neural networks to learn a representation of math that allows efficient retrieval of semantically related expressions. Unlike existing work, our approach does not rely on hand-written rules, but learns embeddings purely data-driven in a combination of two unsupervised tasks: On the one hand, we train a contextual similarity tasks where labels are generated from the surrounding contexts of mathematical expressions, on the other hand we train a self-supervised masking task where labels are derived directly from the inputs. To illustrate our ideas, we have curated a huge dataset with over 29 million mathematical expressions based on over 900,000 papers hosted on arXiv.org. This allowed us to train graph convolutional neural networks with millions of equations and to carefully examine the impact of our design choices. We have shown the benefit of our search system using a new dataset of annotated math queries.</p><p>As of now, our method uses uniform sampling for positive and negative examples. In the future we want to explore more guided variants of sampling. It is still unclear what the tradeoffs between less noise in the labels and more bias in the sampling are.</p><p>Given the importance of the data samples and their labeling, we have set up a crowd-sourced evaluation procedure. Currently, there are more evaluation queries available than in the NTCIR dataset. This data collection will be continued and extended to user interactions with a running search engine.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>)Figure 2 .</head><label>2</label><figDesc>Figure 2. The 50 most frequent characters in math environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Example of the Masking Task with Fictional Values</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ablation Study</figDesc><table><row><cell>Influence Factor</cell><cell>Ranking Hold-Out</cell><cell>Ranking Eval</cell><cell>Accuracy Eval</cell></row><row><cell>Full Model</cell><cell cols="3">76.5 (?0.0) 57.7 (?0.0) 60.6 (?0.0)</cell></row><row><cell>No Histogram Loss</cell><cell cols="3">72.5 (-4.0) 49.6 (-8.1) 30.9 (-29.7)</cell></row><row><cell>No Masking</cell><cell cols="3">75.2 (-1.3) 54.3 (-3.4) 50.0 (-10.6)</cell></row><row><cell>No Augmentation</cell><cell cols="3">75.3 (-1.2) 53.6 (-4.1) 50.0 (-10.6)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Eval Scores</figDesc><table><row><cell>Dataset</cell><cell>Ranking Eval</cell><cell>Accuracy Eval</cell></row><row><cell>1mio ML-Subset Triplets</cell><cell>57.7</cell><cell>60.6</cell></row><row><cell>5mio Full ArXiv Triplets</cell><cell>76.2</cell><cell>80.9</cell></row><row><cell>20mio Full ArXiv Triplets</cell><cell>75.3</cell><cell>84.0</cell></row><row><cell>Bitmap CNN original</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Result: P (d) = P (d | s)P (s)ds Figure 4.Example: Bayes' law. We report the first result and the first result that does not show Bayes' law, but, in this case, the related law of total probability. The first result is from: R. H. Leike, T. A. En?lin, Charting nearby dust clouds using Gaia data only, 2019. Query: i&lt;j wij si sj + i ?i si 'Boltzmann' Result: E =i bi si -i&lt;j wij sisj . 'Ising' Result: H = -i&lt;j Cij Jij ?i?ji hi?i</figDesc><table><row><cell>Figure 5. Example: Ising Model. We find equations related to</cell></row><row><cell>both Ising Models and Boltzmann Machines. First result is from:</cell></row><row><cell>Weinstein, Learning the Einstein-Podolsky-Rosen correlations on</cell></row><row><cell>a Restricted Boltzmann Machine, 2017. Second result is from:</cell></row><row><cell>Ferrari et al., Finite size corrections to disordered systems on</cell></row><row><cell>Erd?s-R?nyi random graphs, 2013.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table. 4 we show</figDesc><table /><note><p>https://www.overleaf.com/8721648589nrjxgwmtzfvm.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">. Search Engine Performance</cell></row><row><cell></cell><cell cols="2">P@10 P@100 P@1000 uMAP</cell></row><row><cell>BoW</cell><cell>0.4567 0.3170</cell><cell>0.2083 106.17</cell></row><row><cell>5Mio</cell><cell>0.5038 0.3817</cell><cell>0.2984 165.04</cell></row><row><cell cols="2">20Mio 0.4547 0.3709</cell><cell>0.2897 156.51</cell></row><row><cell cols="3">Table 4. Sorted P@100 values per Query for 5Mio Triplets</cell></row><row><cell cols="2">P@100 Keywords</cell><cell></cell></row><row><cell cols="2">0.95 'policy gradient'</cell><cell></cell></row><row><cell cols="2">0.93 'convex', 'strongly convex'</cell><cell></cell></row><row><cell cols="3">0.91 'q-learning', 'reinforcement learning'</cell></row><row><cell cols="2">0.89 'chain complex', 'sequence'</cell><cell></cell></row><row><cell cols="2">0.81 'lipschitz', 'continuous'</cell><cell></cell></row><row><cell cols="3">0.80 'empirical risk', 'training objective', 'erm'</cell></row><row><cell cols="3">0.71 'lstm', 'recurrent neural network'</cell></row><row><cell cols="3">0.70 'neural network', 'hidden layer', ...</cell></row><row><cell cols="3">0.67 'eigenvalue', 'eigenvector',...</cell></row><row><cell cols="3">0.66 'received signal strength indication', 'rssi', ...</cell></row><row><cell>. . .</cell><cell></cell><cell></cell></row><row><cell cols="3">0.08 'jaccard similarity', 'min-hashing', ...</cell></row><row><cell cols="3">0.07 'fredholm integral', 'equation of the first kind', ...</cell></row><row><cell cols="3">0.07 'effective collection area', 'effective area'</cell></row><row><cell cols="2">0.07 'modified erlang', 'erlang'</cell><cell></cell></row><row><cell cols="3">0.07 'johnson-lindenstrauss', 'embedding'</cell></row><row><cell cols="3">0.06 'significance of detection', 'lima'</cell></row><row><cell cols="3">0.06 'log-odds alignment', 'pairwise dynamic prog.', ...</cell></row><row><cell cols="2">0.05 'proximal gradient'</cell><cell></cell></row><row><cell cols="3">0.04 'handshaking lemma', 'handshake lemma'</cell></row><row><cell cols="3">0.03 'single photon spectrum', 'multi-gaussian distr.', ...</cell></row><row><cell cols="3">0.02 'graphcnn', 'geometric deep learning', ...</cell></row><row><cell cols="2">0.01 'crosstalk probability'</cell><cell></cell></row><row><cell cols="3">the best and worst-performing queries of our 5Mio training</cell></row><row><cell cols="2">examples model.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Artificial Intelligence Group, TU Dortmund University, Dortmund, Germany.Correspondence to: Lukas Pfahler &lt;lukas.pfahler@tu-dortmund.de&gt;.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://arxiv.org/help/bulk_data_s3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://katex.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank our fellow scientist who contributed annotated equations to our evaluation dataset. This work has been supported by <rs type="funder">Deutsche Forschungsgemeinschaft (DFG)</rs> within the <rs type="programName">Collaborative Research Center SFB 876</rs>, "<rs type="projectName">Providing Information by Resource-Constrained Data Analysis</rs>", project <rs type="grantNumber">A1</rs>. http://sfb876.tu-dortmund.de. Parts of this work have been funded by the <rs type="funder">Federal Ministry of Education and Research of Germany</rs> as part of the competence center for machine learning ML2R (<rs type="grantNumber">01IS18038A</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_g7rf9xV">
					<idno type="grant-number">A1</idno>
					<orgName type="project" subtype="full">Providing Information by Resource-Constrained Data Analysis</orgName>
					<orgName type="program" subtype="full">Collaborative Research Center SFB 876</orgName>
				</org>
				<org type="funding" xml:id="_g3dxjxt">
					<idno type="grant-number">01IS18038A</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recognition of On-line Handwritten Mathematical Expressions Using 2D Stochastic Context-Free Grammars and Hidden Markov Models</title>
		<author>
			<persName><forename type="first">F</forename><surname>?lvaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-A</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Bened?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="issue">35</biblScope>
			<biblScope unit="page" from="58" to="67" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors with triplets and shallow convolutional neural networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">A P</forename><surname>Smith</surname></persName>
		</editor>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="119" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Annoy: Approximate Nearest Neighbors in C++/Python</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bernhardsson</surname></persName>
		</author>
		<ptr target="https://pypi.org/project/annoy/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/chen20j/chen20j.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning<address><addrLine>Vienna</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scaling-up Empirical Risk Minimization: Optimization of Incomplete U-statistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cl?menc ?on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Colin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<idno type="ISSN">15337928</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imageto-Markup Generation with Coarse-to-Fine Attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanervisto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="980" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019</title>
		<meeting>NAACL-HLT 2019</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person reidentification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Precessing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Information field theory for cosmological perturbation reconstruction and nonlinear signal analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>En?lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frommert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Kitaura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. D</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">105005</biblScope>
			<date type="published" when="2009-11">nov 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised Representation Learning by Predicting Image Rotations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey on retrieval of mathematical knowledge</title>
		<author>
			<persName><forename type="first">F</forename><surname>Guidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Coen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics in Computer Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="427" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A class of statistics with asymptotically normal distribution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hoeffding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large Deviations for Sums of Partly Dependent Random Variables</title>
		<author>
			<persName><forename type="first">S</forename><surname>Janson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random Structures &amp; Algorithms</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="234" to="248" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bag of Tricks for Efficient Text Classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.09249%5Cnhttps://arxiv.org/pdf/1604.00289v1.pdf%5Cnhttp://arxiv.or" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CROHME + TFD : Competition on Recognition of Handwritten Mathematical Expressions and Typeset Formula Detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Viard-Gaudin</surname></persName>
		</author>
		<author>
			<persName><surname>Icdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th IAPR International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tangent-CFT : An Embedding Model for Mathematical Formulas</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the 2019 ACM SIGIR International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning similarity with operator-valued largemargin classifiers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<idno type="ISSN">15324435</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1049" to="1082" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Representation Learning with Contrastive Predictive Coding</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1807.03748.pdf" />
		<imprint/>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic Search in Millions of Equations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pfahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Morik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Search for Equations -Learning to Identify Similarities between Mathematical Expressions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pfahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Morik</surname></persName>
		</author>
		<ptr target="https://www.maths.tcd.ie/$\sim$richardt/openmath/m" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases. ECML PKDD 2019</title>
		<imprint>
			<date type="published" when="1999">2019. 1999</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Selfie : Self-supervised Pretraining for Image Embedding</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning Deep Embeddings with Histogram Loss</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Precessing Systems</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Nature of Statistical Learning Theory</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNN.1997.641482</idno>
		<ptr target="http://portal.acm.org/citation.cfm?id=211359" />
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Translating Math Word Problem to Expression Tree</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium, Oc</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">tober 31 -November 4, 2018. 2018</date>
			<biblScope unit="page" from="1064" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Tompa</surname></persName>
		</author>
		<author>
			<persName><surname>Multi</surname></persName>
		</author>
		<title level="m">Stage Math Formula Search: Using Appearance-Based Similarity Metrics at Scale. In Proc. of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="145" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">NTCIR-12 MathIR Task Overview</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Topi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Davila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th NTCIR Conference on Evaluation of Information Access Technologies</title>
		<meeting>the 12th NTCIR Conference on Evaluation of Information Access Technologies<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structural Similarity Search for Formulas using Leaf-Root Paths in Operator Subtrees</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="116" to="129" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
